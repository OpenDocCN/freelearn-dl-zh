<html><head></head><body>
		<div><h1 id="_idParaDest-185" class="chapter-number"><a id="_idTextAnchor184"/><st c="0">9</st></h1>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor185"/><st c="2">Evaluating RAG Quantitatively and with Visualizations</st></h1>
			<p><st c="55">Evaluation plays a crucial role in building and maintaining </st><strong class="bold"><st c="116">retrieval-augmented generation</st></strong><st c="146"> (</st><strong class="bold"><st c="148">RAG</st></strong><st c="151">) pipelines. </st><st c="165">While you build the pipeline, you can use evaluation to identify areas for improvement, optimize the system’s performance, and systematically measure the impact of improvements. </st><st c="343">When your RAG system is deployed, evaluation can help ensure the effectiveness, reliability, and performance of </st><st c="455">the system.</st></p>
			<p><st c="466">In this chapter, we will cover the </st><st c="502">following topics:</st></p>
			<ul>
				<li><st c="519">Evaluating when building a </st><st c="547">RAG application</st></li>
				<li><st c="562">Evaluating a RAG application </st><st c="592">after deployment</st></li>
				<li><st c="608">Standardized </st><st c="622">evaluation frameworks</st></li>
				<li><st c="643">Ground truth</st></li>
				<li><st c="656">Code lab 9.1 – </st><st c="672">ragas</st></li>
				<li><st c="677">Additional evaluation techniques for </st><st c="715">RAG systems</st></li>
			</ul>
			<p><st c="726">Let’s start by talking about how evaluation can help during the initial stages of building your </st><st c="823">RAG system.</st></p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor186"/><st c="834">Technical requirements</st></h1>
			<p><st c="857">The code for this chapter is placed in the following GitHub </st><st c="918">repository: </st><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_09"><st c="930">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_09</st></a></p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor187"/><st c="1027">Evaluate as you build</st></h1>
			<p><st c="1049">Evaluation plays a crucial role throughout the development process of a RAG pipeline. </st><st c="1136">By continuously evaluating your system as you build it, you can identify areas that need improvement, optimize the system’s performance, and systematically measure the impact of any modifications or enhancements </st><st c="1348">you make.</st></p>
			<p><st c="1357">Evaluation is </st><a id="_idIndexMarker521"/><st c="1372">essential for understanding the trade-offs and limitations of different approaches within the RAG pipeline. </st><st c="1480">RAG pipelines often involve various technical choices, such as the vector store, the retrieval algorithm, and the language generation model. </st><st c="1621">Each of these components can have a significant impact on the overall performance of the system. </st><st c="1718">By systematically evaluating different combinations of these components, you can gain valuable insights into which approaches yield the best results for your specific tasks </st><st c="1891">and domain.</st></p>
			<p><st c="1902">For instance, you might experiment with different embedding models, such as local open source models that you can download for free or cloud service APIs that charge each time you convert text to an embedding. </st><st c="2113">You may need to understand whether the cloud API service is better than the free model, and if so, whether it is good enough to offset the additional cost. </st><st c="2269">Similarly, you can evaluate the performance of various language generation models, such as ChatGPT, Llama, </st><st c="2376">and Claude.</st></p>
			<p><st c="2387">This iterative evaluation process helps you make informed decisions about the most suitable architecture and components for your RAG pipeline. </st><st c="2531">By considering factors such as efficiency, scalability, and generalization ability, you can fine-tune your system to achieve optimal performance while minimizing computational costs and ensuring robustness across </st><st c="2744">different scenarios.</st></p>
			<p><st c="2764">Evaluation is essential for understanding the trade-offs and limitations of different approaches within the RAG pipeline. </st><st c="2887">But evaluation can also be useful after deployment, which we will talk </st><st c="2958">about next.</st></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor188"/><st c="2969">Evaluate after you deploy</st></h1>
			<p><st c="2995">Once your RAG </st><a id="_idIndexMarker522"/><st c="3010">system is deployed, evaluation remains a crucial aspect of ensuring its ongoing effectiveness, reliability, and performance. </st><st c="3135">Continuous monitoring and assessment of your deployed RAG pipeline are essential for maintaining its quality and identifying any potential issues or degradation </st><st c="3296">over time.</st></p>
			<p><st c="3306">There are numerous reasons why a RAG system’s performance might decline after deployment. </st><st c="3397">For example, the data used for retrieval may become outdated or irrelevant as new information emerges. </st><st c="3500">The language generation model may struggle to adapt to evolving user queries or changes in the target domain. </st><st c="3610">Additionally, the underlying infrastructure, such as hardware or software components, may experience performance issues </st><st c="3730">or failures.</st></p>
			<p><st c="3742">Imagine a situation where you are at a financial wealth management company that has a RAG-based application that helps users understand the most likely factors to impact their financial portfolio. </st><st c="3940">Your data sources might include all of the analyses published by major financial firms in the past five years covering all the financial assets represented by </st><st c="4099">your clientele.</st></p>
			<p><st c="4114">In financial markets, though, major (macro) events around the world can have a dramatic impact </st><a id="_idIndexMarker523"/><st c="4210">on those portfolios that have not been captured in the past five years of data. </st><st c="4290">Major catastrophes, political instability, or even a regional event for some stocks can set a whole new trajectory for their performance. </st><st c="4428">For your RAG application, this represents shifts in the value that the data can provide to your end user, and over time, that value can decrease rapidly without proper updates. </st><st c="4605">Users may start asking questions about those specific events that the RAG application will not be able to handle, such as </st><em class="italic"><st c="4727">“What impact will the Category 5 hurricane that just occurred have on my portfolio in the next year?”</st></em><st c="4828"> But with continual updates and monitoring, and particularly with more recent reports about the impacts of the hurricane, these issues will likely be </st><st c="4978">well addressed.</st></p>
			<p><st c="4993">To mitigate these risks, it is crucial to continuously monitor your RAG system, particularly at common failure points. </st><st c="5113">By continuously evaluating these critical components of your RAG pipeline, you can proactively identify and address any degradation in performance. </st><st c="5261">This may involve updating the retrieval corpus with fresh and relevant data, fine-tuning the language generation model on new data, or optimizing the system’s infrastructure to handle increased load or address </st><st c="5471">performance bottlenecks.</st></p>
			<p><st c="5495">Furthermore, it is essential to establish a feedback loop that allows users to report any issues or provide suggestions for improvement. </st><st c="5633">By actively soliciting and incorporating user feedback, you can continuously refine and enhance your RAG system to better meet the needs of its users. </st><st c="5784">This can also include monitoring aspects such as user interface usage, response times, and the relevance and usefulness of the generated outputs from the user’s perspective. </st><st c="5958">Conducting user surveys, analyzing user interaction logs, and monitoring user satisfaction metrics can provide valuable insights into how well your RAG system is meeting its intended purpose. </st><st c="6150">How you utilize this information depends heavily on what type of RAG application you have developed, but in general, these are the most common areas monitored for continual improvement of deployed </st><st c="6347">RAG applications.</st></p>
			<p><st c="6364">By regularly </st><a id="_idIndexMarker524"/><st c="6378">evaluating your deployed RAG system, you can ensure its long-term effectiveness, reliability, and performance. </st><st c="6489">Continuous monitoring, proactive issue detection, and a commitment to ongoing improvement are key to maintaining a high-quality RAG pipeline that delivers value to its users </st><st c="6663">over time.</st></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor189"/><st c="6673">Evaluation helps you get better</st></h1>
			<p><st c="6705">Why is evaluation so important? </st><st c="6738">Put simply, if you don’t measure where you are at, and then measure again after you have made improvements, it will be difficult to understand </st><a id="_idIndexMarker525"/><st c="6881">how or what improved (or hurt) the performance of your </st><st c="6936">RAG system.</st></p>
			<p><st c="6947">It is also difficult to understand what is going wrong when something does go wrong without something objective to compare against. </st><st c="7080">Was it your retrieval mechanism? </st><st c="7113">Was it the prompt? </st><st c="7132">Is it your LLM responses? </st><st c="7158">These are questions a good evaluation system can </st><st c="7207">help answer.</st></p>
			<p><st c="7219">Evaluation provides a systematic and objective way to measure the performance of your pipeline, identify areas for enhancement, and track the impact of any changes or improvements you make. </st><st c="7410">Without a robust evaluation framework, it becomes challenging to understand how your RAG system is progressing and where it needs </st><st c="7540">further refinement.</st></p>
			<p><st c="7559">By embracing evaluation as an integral part of your development process, you can continuously refine and optimize your RAG pipeline, ensuring that it delivers the best possible results and meets the evolving needs of </st><st c="7777">its users.</st></p>
			<p><st c="7787">Early in the RAG system development process, you have to start making decisions about what technical components you are going to consider. </st><st c="7927">At this point, you haven’t even installed </st><a id="_idIndexMarker526"/><st c="7969">anything, so you can’t evaluate your code yet, but you can still use </st><strong class="bold"><st c="8038">standardized evaluation frameworks</st></strong><st c="8072"> to narrow down what you are considering. </st><st c="8114">Let’s discuss these standardized evaluation frameworks for some of the most common RAG </st><st c="8201">system elements.</st></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor190"/><st c="8217">Standardized evaluation frameworks</st></h1>
			<p><st c="8252">Key technical components of your RAG system include the embedding model that makes </st><a id="_idIndexMarker527"/><st c="8336">your embeddings, the vector store, the vector search, and the LLM. </st><st c="8403">When you look at the different options for each technical component, there are a number of standardized metrics that are available for each that help you compare them against each other. </st><st c="8590">Here are some common metrics for </st><st c="8623">each category.</st></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor191"/><st c="8637">Embedding model benchmarks</st></h2>
			<p><st c="8664">The </st><strong class="bold"><st c="8669">Massive Text Embedding Benchmark</st></strong><st c="8701"> (</st><strong class="bold"><st c="8703">MTEB</st></strong><st c="8707">) Retrieval Leaderboard evaluates </st><a id="_idIndexMarker528"/><st c="8742">the performance of embedding models on various retrieval tasks across different datasets. </st><st c="8832">The MTEB leaderboard ranks models based on their average performance across many embedding and </st><a id="_idIndexMarker529"/><st c="8927">retrieval-related tasks. </st><st c="8952">You can visit the leaderboard using this </st><st c="8993">link: </st><a href="https://huggingface.co/spaces/mteb/leaderboard"><st c="8999">https://huggingface.co/spaces/mteb/leaderboard</st></a></p>
			<p><st c="9045">When visiting this web page, click on the </st><strong class="bold"><st c="9088">Retrieval</st></strong><st c="9097"> and </st><strong class="bold"><st c="9102">Retrieval w/Instructions</st></strong><st c="9126"> tabs for </st><a id="_idIndexMarker530"/><st c="9136">retrieval-specific embedding ratings. </st><st c="9174">To evaluate each of the models on the leaderboard, the model’s outputs are tested using a number of datasets that cover a wide range of domains, such as </st><st c="9327">the following:</st></p>
			<ul>
				<li><st c="9341">Argument </st><st c="9351">retrieval (</st><code><st c="9362">ArguAna</st></code><st c="9370">)</st></li>
				<li><st c="9372">Climate fact </st><st c="9385">retrieval (</st><code><st c="9396">ClimateFEVER</st></code><st c="9409">)</st></li>
				<li><st c="9411">Duplicate question </st><st c="9430">retrieval (</st><code><st c="9441">CQADupstackRetrieval</st></code><st c="9462">)</st></li>
				<li><st c="9464">Entity </st><st c="9471">retrieval (</st><code><st c="9482">DBPedia</st></code><st c="9490">)</st></li>
				<li><st c="9492">Fact extraction and </st><st c="9512">verification (</st><code><st c="9526">FEVER</st></code><st c="9532">)</st></li>
				<li><st c="9534">Financial </st><st c="9544">question-answering (</st><code><st c="9564">FiQA2018</st></code><st c="9573">)</st></li>
				<li><st c="9575">Multi-hop </st><st c="9585">question-answering (</st><code><st c="9605">HotpotQA</st></code><st c="9614">)</st></li>
				<li><st c="9616">Passage and document </st><st c="9637">ranking (</st><code><st c="9646">MSMARCO</st></code><st c="9654">)</st></li>
				<li><st c="9656">Fact-checking (</st><code><st c="9671">NFCorpus</st></code><st c="9680">)</st></li>
				<li><st c="9682">Open-domain </st><st c="9694">question-answering (</st><code><st c="9714">NQ</st></code><st c="9717">)</st></li>
				<li><st c="9719">Duplicate-question </st><st c="9738">detection (</st><code><st c="9749">QuoraRetrieval</st></code><st c="9764">)</st></li>
				<li><st c="9766">Scientific document </st><st c="9786">retrieval (</st><code><st c="9797">SCIDOCS</st></code><st c="9805">)</st></li>
				<li><st c="9807">Scientific claim </st><st c="9824">verification (</st><code><st c="9838">SciFact</st></code><st c="9846">)</st></li>
				<li><st c="9848">Argument </st><st c="9857">retrieval (</st><code><st c="9868">Touche2020</st></code><st c="9879">)</st></li>
				<li><st c="9881">COVID-19-related information </st><st c="9910">retrieval (</st><code><st c="9921">TRECCOVID</st></code><st c="9931">)</st></li>
			</ul>
			<p><st c="9933">The leaderboard </st><a id="_idIndexMarker531"/><st c="9949">ranks embedding models based on </st><a id="_idIndexMarker532"/><st c="9981">their average performance across these tasks, providing a comprehensive view of their strengths and weaknesses. </st><st c="10093">You can also click on any metric to order the board by that metric. </st><st c="10161">So, for example, if you are interested in a metric that is more focused on financial question-answering, look at what model scored top marks on the </st><st c="10309">FiQA2018 dataset.</st></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor192"/><st c="10326">Vector store and vector search benchmarks</st></h2>
			<p><strong class="bold"><st c="10368">ANN-Benchmarks</st></strong><st c="10383"> is a </st><a id="_idIndexMarker533"/><st c="10389">benchmarking tool that evaluates the performance </st><a id="_idIndexMarker534"/><st c="10438">of </st><strong class="bold"><st c="10441">approximate nearest neighbor</st></strong><st c="10469"> (</st><strong class="bold"><st c="10471">ANN</st></strong><st c="10474">) algorithms, which we discussed thoroughly in </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="10522">Chapter 8</st></em></a><st c="10531">. ANN-Benchmarks </st><a id="_idIndexMarker535"/><st c="10548">assesses the </st><a id="_idIndexMarker536"/><st c="10561">search </st><a id="_idIndexMarker537"/><st c="10568">accuracy, speed, and memory usage of different </st><a id="_idIndexMarker538"/><st c="10615">vector search tools on various datasets, including </st><a id="_idIndexMarker539"/><st c="10666">the vector search tools we mentioned in </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="10706">Chapter 8</st></em></a><st c="10715">—</st><strong class="bold"><st c="10717">Facebook AI Similarity Search</st></strong><st c="10746"> (</st><strong class="bold"><st c="10748">FAISS</st></strong><st c="10753">), </st><strong class="bold"><st c="10757">Approximate Nearest Neighbors Oh Yeah</st></strong><st c="10794"> (</st><strong class="bold"><st c="10796">ANNOY</st></strong><st c="10801">), and </st><strong class="bold"><st c="10809">Hierarchical Navigable Small </st></strong><strong class="bold"><st c="10838">Worlds</st></strong><st c="10844"> (</st><strong class="bold"><st c="10846">HNSW</st></strong><st c="10850">).</st></p>
			<p><strong class="bold"><st c="10853">Benchmarking IR</st></strong><st c="10869"> (</st><strong class="bold"><st c="10871">BEIR</st></strong><st c="10875">) is another </st><a id="_idIndexMarker540"/><st c="10889">useful </st><a id="_idIndexMarker541"/><st c="10896">resource for evaluating vector stores and search algorithms. </st><st c="10957">It provides a heterogeneous benchmark for zero-shot evaluation of information retrieval models across diverse domains, including question-answering, fact-checking, and entity retrieval. </st><st c="11143">We will further discuss what </st><em class="italic"><st c="11172">zero-shot</st></em><st c="11181"> means in </st><a href="B22475_13.xhtml#_idTextAnchor256"><em class="italic"><st c="11191">Chapter 13</st></em></a><st c="11201">, but basically, it means questions/user queries that do not have any examples included with them, which is a common situation in RAG. </st><st c="11336">BEIR offers a standardized evaluation framework and includes popular datasets such as </st><st c="11422">the following:</st></p>
			<ul>
				<li><code><st c="11436">MSMARCO</st></code><st c="11444">: A large-scale dataset derived from real-world queries and answers for evaluating deep learning models in search </st><st c="11559">and question-answering</st></li>
				<li><code><st c="11581">HotpotQA</st></code><st c="11590">: A question-answering dataset that features natural, multi-hop questions, with strong supervision for supporting facts and enabling more explainable </st><st c="11741">question-answering systems</st></li>
				<li><code><st c="11767">CQADupStack</st></code><st c="11779">: A benchmark dataset for </st><strong class="bold"><st c="11806">community question-answering</st></strong><st c="11834"> (</st><strong class="bold"><st c="11836">cQA</st></strong><st c="11839">) research, taken </st><a id="_idIndexMarker542"/><st c="11858">from 12 Stack Exchange subforums and annotated with duplicate </st><st c="11920">question information</st></li>
			</ul>
			<p><st c="11940">These </st><a id="_idIndexMarker543"/><st c="11947">datasets, along </st><a id="_idIndexMarker544"/><st c="11963">with others in the </st><a id="_idIndexMarker545"/><st c="11982">BEIR benchmark, cover a diverse range of domains and </st><a id="_idIndexMarker546"/><st c="12035">information retrieval tasks, allowing you to assess the performance of your retrieval system in different contexts and compare it against </st><st c="12173">state-of-the-art methods.</st></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor193"/><st c="12198">LLM benchmarks</st></h2>
			<p><st c="12213">The Artificial Analysis LLM Performance Leaderboard is a comprehensive resource for evaluating </st><a id="_idIndexMarker547"/><st c="12309">both open source and proprietary language models, such as </st><a id="_idIndexMarker548"/><st c="12367">ChatGPT, Claude, and Llama. </st><st c="12395">It </st><a id="_idIndexMarker549"/><st c="12398">assesses the models’ performance </st><a id="_idIndexMarker550"/><st c="12431">on a wide range of tasks. </st><st c="12457">For quality comparisons, it uses a number </st><st c="12499">of sub-leaderboards:</st></p>
			<ul>
				<li><strong class="bold"><st c="12519">General ability</st></strong><st c="12535">: </st><st c="12538">Chatbot Arena</st></li>
				<li><strong class="bold"><st c="12551">Reasoning </st></strong><strong class="bold"><st c="12562">and knowledge</st></strong><st c="12575">:</st><ul><li><strong class="bold"><st c="12577">Massive Multitask Language </st></strong><strong class="bold"><st c="12604">Understanding</st></strong><st c="12617"> (</st><strong class="bold"><st c="12619">MMLU</st></strong><st c="12623">)</st></li><li><strong class="bold"><st c="12625">Multi-turn Benchmark</st></strong><st c="12645"> (</st><strong class="bold"><st c="12647">MT Bench</st></strong><st c="12655">)</st></li></ul></li>
			</ul>
			<p><st c="12657">They also track speed and price and provide analysis to allow you to compare a balance of each of these areas. </st><st c="12768">By ranking the models based on their performance across these tasks, the leaderboard provides a holistic view of </st><st c="12881">their capabilities.</st></p>
			<p><st c="12900">It can be found </st><st c="12917">here: </st><a href="https://artificialanalysis.ai/"><st c="12923">https://artificialanalysis.ai/</st></a></p>
			<p><st c="12953">In addition to the general LLM leaderboard, there are specialized leaderboards that focus on specific aspects of LLM performance. </st><st c="13084">The Artificial Analysis LLM Performance Leaderboard </st><a id="_idIndexMarker551"/><st c="13136">evaluates the technical aspects of LLMs, such as inference speed, memory consumption, and scalability. </st><st c="13239">It </st><a id="_idIndexMarker552"/><st c="13242">includes metrics such as throughput (tokens processed per second), latency (time to generate a response), memory footprint, and scaling efficiency. </st><st c="13390">These metrics help you understand the computational requirements and performance characteristics of </st><st c="13490">different LLMs.</st></p>
			<p><st c="13505">The Open LLM Leaderboard tracks the performance of open source language models on various </st><a id="_idIndexMarker553"/><st c="13596">natural language understanding and generation tasks. </st><st c="13649">It includes benchmarks such as the </st><strong class="bold"><st c="13684">AI2 Reasoning Challenge</st></strong><st c="13707"> (</st><strong class="bold"><st c="13709">ARC</st></strong><st c="13712">) for complex scientific reasoning, HellaSwag for common-sense reasoning, MMLU for domain-specific performance, TruthfulQA for generating truthful and informative responses, WinoGrande </st><a id="_idIndexMarker554"/><st c="13898">for common-sense reasoning through pronoun disambiguation, and </st><strong class="bold"><st c="13961">Grade School Math 8K</st></strong><st c="13981"> (</st><strong class="bold"><st c="13983">GSM8K</st></strong><st c="13988">) for mathematical </st><st c="14008">reasoning abilities.</st></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor194"/><st c="14028">Final thoughts on standardized evaluation frameworks</st></h2>
			<p><st c="14081">Using standardized evaluation frameworks and benchmarks offers a valuable starting point for </st><a id="_idIndexMarker555"/><st c="14175">comparing the performance of different components in your RAG pipeline. </st><st c="14247">They cover a wide range of tasks and domains, allowing you to assess the strengths and weaknesses of various approaches. </st><st c="14368">By considering the results on these benchmarks, along with other factors such as computational efficiency and ease of integration, you can narrow down your options and make better-informed decisions when selecting the most suitable components for your specific </st><st c="14629">RAG application.</st></p>
			<p><st c="14645">However, it’s important to note that while these standardized evaluation metrics are helpful for initial component selection, they may not fully capture the performance of your specific RAG pipeline with your unique inputs and outputs. </st><st c="14882">To truly understand how well your RAG system performs in your particular use case, you need to set up your own evaluation framework tailored to your specific requirements. </st><st c="15054">This customized evaluation </st><a id="_idIndexMarker556"/><st c="15081">system will provide the most accurate and relevant insights into the performance of your </st><st c="15170">RAG pipeline.</st></p>
			<p><st c="15183">Next, we need to talk about one of the most important and often overlooked aspects of RAG evaluation, your </st><st c="15291">ground-truth data.</st></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor195"/><st c="15309">What is the ground truth?</st></h1>
			<p><st c="15335">Simply put, ground-truth data is data that represents the ideal responses you would expect if your RAG system was operating at </st><st c="15463">peak performance.</st></p>
			<p><st c="15480">As a practical example, if you had a RAG system focused on allowing someone to ask questions about </st><a id="_idIndexMarker557"/><st c="15580">the latest cancer research in veterinarian medicine for dogs, with your data source being all the latest research papers on the subject that have been submitted to PubMed, your ground truth would likely be questions and answers that could be asked and answered of that data. </st><st c="15855">You would want to use realistic questions that your target audience would really ask, and the answers should be what you consider to be the ideal answer expected from the LLM. </st><st c="16031">This could be somewhat objective, but nonetheless, having a set of ground-truth data to compare against the input and output of your RAG system is a critical way to help compare the impact of changes you make and ultimately make the system </st><st c="16271">more effective.</st></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor196"/><st c="16286">How to use the ground truth?</st></h2>
			<p><st c="16315">Ground-truth data serves as a benchmark to measure the performance of RAG systems. </st><st c="16399">By comparing </st><a id="_idIndexMarker558"/><st c="16412">the output generated by the RAG system to the ground truth, you can assess how well the system retrieves relevant information and generates accurate and coherent responses. </st><st c="16585">The ground truth helps quantify the effectiveness of different RAG approaches and identify areas </st><st c="16682">for improvement.</st></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor197"/><st c="16698">Generating the ground truth</st></h2>
			<p><st c="16726">Creating </st><a id="_idIndexMarker559"/><st c="16736">ground-truth data manually can be time-consuming. </st><st c="16786">If your company already has a dataset of ideal responses for specific queries or prompts, that can be a valuable resource. </st><st c="16909">However, if such data is not readily available, there are alternative methods to obtain the ground truth that we will look </st><st c="17032">into next.</st></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor198"/><st c="17042">Human annotation</st></h2>
			<p><st c="17059">You can </st><a id="_idIndexMarker560"/><st c="17068">employ human annotators to manually create ideal responses for a set of queries or prompts. </st><st c="17160">This ensures high-quality ground-truth data but can be costly and time-consuming, especially for </st><st c="17257">large-scale evaluations.</st></p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor199"/><st c="17281">Expert knowledge</st></h2>
			<p><st c="17298">In some </st><a id="_idIndexMarker561"/><st c="17307">domains, you may have access to </st><strong class="bold"><st c="17339">subject-matter experts</st></strong><st c="17361"> (</st><strong class="bold"><st c="17363">SMEs</st></strong><st c="17367">) who can provide ground-truth responses based on their </st><a id="_idIndexMarker562"/><st c="17424">expertise. </st><st c="17435">This can be particularly useful for specialized or technical domains where accurate information </st><st c="17531">is crucial.</st></p>
			<p><st c="17542">One common </st><a id="_idIndexMarker563"/><st c="17554">approach to help with this method is called </st><strong class="bold"><st c="17598">rule-based generation</st></strong><st c="17619">. With rule-based generation, for specific domains or tasks, you can define a set of rules or templates to generate the synthetic ground truth and utilize your SMEs to fill in the template. </st><st c="17809">By leveraging domain knowledge and predefined patterns, you can create responses that align with the expected format </st><st c="17926">and content.</st></p>
			<p><st c="17938">For example, if you are building a customer support chatbot to support mobile phones, you may have a template such as this: </st><code><st c="18063">To resolve [issue], you can try [solution]</st></code><st c="18105">. Your SMEs can fill in various issue-solution approaches where an issue might be </st><em class="italic"><st c="18187">battery drain</st></em><st c="18200"> and the solution </st><em class="italic"><st c="18218">reducing screen brightness and closing background apps</st></em><st c="18272">. This would be fed to the template (what we call hydrating) and the final output would be something such as this: </st><code><st c="18387">To resolve [battery drain], you can try [reducing screen brightness and closing </st></code><code><st c="18467">background apps]</st></code><st c="18483">.</st></p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor200"/><st c="18484">Crowdsourcing</st></h2>
			<p><st c="18498">Platforms </st><a id="_idIndexMarker564"/><st c="18509">such as Amazon Mechanical Turk and Figure Eight allow you to outsource the task of creating ground-truth data to a large pool of workers. </st><st c="18647">By providing clear instructions and quality control measures, you can obtain a diverse set </st><st c="18738">of responses.</st></p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor201"/><st c="18751">Synthetic ground truth</st></h2>
			<p><st c="18774">In cases where obtaining real ground-truth data is challenging or infeasible, generating the synthetic </st><a id="_idIndexMarker565"/><st c="18878">ground truth can be a viable </st><a id="_idIndexMarker566"/><st c="18907">alternative. </st><st c="18920">The synthetic ground truth involves using existing LLMs or techniques to automatically generate plausible responses. </st><st c="19037">Here are a </st><st c="19048">few approaches:</st></p>
			<ul>
				<li><strong class="bold"><st c="19063">Fine-tuned language models</st></strong><st c="19090">: You can fine-tune LLMs on a smaller dataset of </st><a id="_idIndexMarker567"/><st c="19140">high-quality responses. </st><st c="19164">By </st><a id="_idIndexMarker568"/><st c="19167">providing the model with examples of ideal responses, it can learn to generate similar responses for new queries or prompts. </st><st c="19292">The generated responses can serve as a synthetic </st><st c="19341">ground truth.</st></li>
				<li><strong class="bold"><st c="19354">Retrieval-based methods</st></strong><st c="19378">: If you have a large corpus of high-quality text data, you </st><a id="_idIndexMarker569"/><st c="19439">can use retrieval-based methods to find </st><a id="_idIndexMarker570"/><st c="19479">relevant passages or sentences that closely match the query or prompt. </st><st c="19550">These retrieved passages can be used as a proxy for </st><st c="19602">ground-truth responses.</st></li>
			</ul>
			<p><st c="19625">Obtaining the ground truth can be a challenging step in building your RAG system, but once you have obtained it, you will have a strong foundation for effective RAG evaluation. </st><st c="19803">In the next section, we have a code lab where we generate synthetic ground-truth data and then integrate a useful evaluation platform into our RAG system that will tell us how the hybrid search we used in the previous chapter impacted </st><st c="20038">our results.</st></p>
			<h1 id="_idParaDest-203"><a id="_idTextAnchor202"/><st c="20050">Code lab 9.1 – ragas</st></h1>
			<p><strong class="bold"><st c="20071">Retrieval-augmented generation assessment</st></strong><st c="20113"> (</st><strong class="bold"><st c="20115">ragas</st></strong><st c="20120">) is an evaluation platform designed </st><a id="_idIndexMarker571"/><st c="20158">specifically for RAG. </st><st c="20180">In this code lab, we will step through the implementation of ragas in your code, generating a synthetic ground truth, and then establishing a comprehensive set of metrics that you can integrate into your RAG system. </st><st c="20396">But evaluation systems are meant to evaluate something, right? </st><st c="20459">What will we evaluate in our </st><st c="20488">code lab?</st></p>
			<p><st c="20497">If you remember in </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="20517">Chapter 8</st></em></a><st c="20526">, we introduced a new search method for our retrieval stage called </st><strong class="bold"><st c="20593">hybrid search</st></strong><st c="20606">. In this </st><a id="_idIndexMarker572"/><st c="20616">code lab, we will both implement the original dense vector semantic-based search and then use ragas to evaluate the impact of using the hybrid search method. </st><st c="20774">This will give you a real-world working example of how a comprehensive evaluation system can be implemented in your </st><st c="20890">own code!</st></p>
			<p><st c="20899">Before we dive </st><a id="_idIndexMarker573"/><st c="20915">into how to use ragas, it is important to note that it is a highly evolving project. </st><st c="21000">New features and API changes are happening often with new releases, so be sure to refer to the documentation website when walking through code </st><st c="21143">examples: </st><a href="https://docs.ragas.io/"><st c="21153">https://docs.ragas.io/</st></a></p>
			<p><st c="21175">This code lab picks up right where we left off in the last chapter when we added </st><code><st c="21257">EnsembleRetriever</st></code><st c="21274"> from LangChain (</st><em class="italic"><st c="21291">Code </st></em><em class="italic"><st c="21297">lab 8.3</st></em><st c="21304">):</st></p>
			<ol>
				<li><st c="21307">Let’s start with some new packages </st><st c="21343">to install:</st><pre class="source-code">
<strong class="bold"><st c="21354">$ pip install ragas</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="21374">$ pip install tqdm -q –user</st></strong></pre><pre class="source-code">
<code><st c="21515">tqdm</st></code><st c="21519"> package, which is used by ragas, is a popular Python library used for creating progress bars and displaying progress information for iterative processes. </st><st c="21674">You have probably come across the </st><code><st c="21708">matplotlib</st></code><st c="21718"> package before, as it is a widely used plotting library for Python. </st><st c="21787">We will be using it to provide visualizations for our evaluation </st><st c="21852">metric results.</st></p></li>
				<li><st c="21867">Next, we need to add several imports related to what we </st><st c="21924">just installed:</st><pre class="source-code">
<strong class="bold"><st c="21939">import tqdm as notebook_tqdm</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="21968">import pandas as pd</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="21988">import matplotlib.pyplot as plt</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22020">from datasets import Dataset</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22049">from ragas import evaluate</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22076">from ragas.testset.generator import TestsetGenerator</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22129">from ragas.testset.evolutions import (</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22168">    simple, reasoning, multi_context)</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22202">from ragas.metrics import (</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22230">    answer_relevancy,</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22248">    faithfulness,</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22262">    context_recall,</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22278">    context_precision,</st></strong></pre><pre class="source-code">
<strong class="bold"/><strong class="bold"><st c="22297">answer_correctness,</st></strong></pre><pre class="source-code">
<strong class="bold"><st c="22317">    answer_similarity</st></strong></pre><pre class="source-code">
<code><st c="22347">tqdm</st></code><st c="22351"> will give our ragas platform the ability to use progress bars during the time-consuming processing tasks it implements. </st><st c="22472">We are going to use the </st><a id="_idIndexMarker574"/><st c="22496">popular pandas data manipulation and analysis library to pull our data into DataFrames as part of our analysis. </st><st c="22608">The </st><code><st c="22612">matplotlib.pyplot as plt</st></code><st c="22636"> import gives us the ability to add visualizations (charts in this case) for our metric results. </st><st c="22733">We also import </st><code><st c="22748">Dataset</st></code><st c="22755"> from </st><code><st c="22761">datasets</st></code><st c="22769">. The </st><code><st c="22775">datasets</st></code><st c="22783"> library is an open source library developed and maintained by Hugging Face. </st><st c="22860">The </st><code><st c="22864">datasets</st></code><st c="22872"> library provides </st><a id="_idIndexMarker575"/><st c="22890">a standardized interface for accessing and manipulating a wide variety of datasets, typically focused on the field of </st><code><st c="23157">from ragas import evaluate</st></code><st c="23184">: The </st><code><st c="23191">evaluate</st></code><st c="23199"> function takes a dataset in the ragas format, along with optional metrics, language models, embeddings, and other configurations, and runs the evaluation on the RAG pipeline. </st><st c="23375">The </st><code><st c="23379">evaluate</st></code><st c="23387"> function returns a </st><code><st c="23407">Result</st></code><st c="23413"> object containing the scores for each metric, providing a convenient way to assess the performance of RAG pipelines using various metrics </st><st c="23552">and configurations.</st></li><li><code><st c="23571">from ragas.testset.generator import TestsetGenerator</st></code><st c="23624">: The </st><code><st c="23631">TestsetGenerator</st></code><st c="23647"> class is used to generate synthetic ground-truth datasets for evaluating RAG pipelines. </st><st c="23736">It takes a set of documents and generates question-answer pairs along with the corresponding contexts. </st><st c="23839">One key aspect of </st><code><st c="23857">TestsetGenerator</st></code><st c="23873"> is that it allows the customization of the test data distribution by specifying the </st><a id="_idIndexMarker576"/><st c="23958">proportions of different question types (e.g., simple, multi-context, or reasoning) using the </st><code><st c="24052">distributions</st></code><st c="24065"> parameter. </st><st c="24077">It supports generating test sets using both LangChain and LlamaIndex </st><st c="24146">document loaders.</st></li><li><code><st c="24163">from ragas.testset.evolutions import simple, reasoning, multi_context</st></code><st c="24233">: These imports represent different types of question evolutions used in the test dataset generation process. </st><st c="24344">These evolutions help create a diverse and comprehensive test dataset that covers various types of questions encountered in </st><st c="24468">real-world scenarios:</st><ul><li><code><st c="24807">from ragas.metrics import…()</st></code><st c="24836">: This </st><code><st c="24844">import</st></code><st c="24850"> statement brings in various evaluation metrics provided by the ragas library. </st><st c="24929">The metrics imported include </st><code><st c="24958">answer_relevancy</st></code><st c="24974">, </st><code><st c="24976">faithfulness</st></code><st c="24988">, </st><code><st c="24990">context_recall</st></code><st c="25004">, </st><code><st c="25006">context_precision</st></code><st c="25023">, </st><code><st c="25025">answer_correctness</st></code><st c="25043">, and </st><code><st c="25049">answer_similarity</st></code><st c="25066">. There are currently two more component-wise metrics (context relevancy and context entity recall) that we can import, but to reduce the complexity of this, we will skip over them here. </st><st c="25253">We will talk about additional metrics you can use toward the end of the code lab. </st><st c="25335">These metrics assess different aspects of the RAG pipeline’s performance that relate to the retrieval and generation and, overall, all the end-to-end stages of the </st><st c="25499">active pipeline.</st></li></ul></li>
			</ol>
			<p><st c="25515">Overall, these </st><a id="_idIndexMarker577"/><st c="25531">imports from the ragas library provide a comprehensive set of tools for generating synthetic test datasets, evaluating RAG pipelines using various metrics, and analyzing the </st><st c="25705">performance results.</st></p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor203"/><st c="25725">Setting up LLMs/embedding models</st></h2>
			<p><st c="25758">Now, we are going to upgrade how we handle our LLM and embedding services. </st><st c="25834">With ragas, we </st><a id="_idIndexMarker578"/><st c="25849">are introducing more complexity to the number of LLMs that we use; we want to better </st><a id="_idIndexMarker579"/><st c="25934">manage that by setting our initializations upfront for both the embedding service and the LLM services. </st><st c="26038">Let’s look at </st><st c="26052">the code:</st></p>
			<pre class="source-code"><st c="26061">
embedding_ada = "text-embedding-ada-002"
model_gpt35="gpt-3.5-turbo"
model_gpt4="gpt-4o-mini"
embedding_function = OpenAIEmbeddings(
    model=embedding_ada, openai_api_key=openai.api_key)
llm = ChatOpenAI(model=model_gpt35,
    openai_api_key=openai.api_key, temperature=0.0)
generator_llm = ChatOpenAI(model=model_gpt35,
    openai_api_key=openai.api_key, temperature=0.0)
critic_llm = ChatOpenAI(model=model_gpt4,
    openai_api_key=openai.api_key, temperature=0.0)</st></pre>
			<p><st c="26514">Note that while we still only use one embedding service, we now have two different LLMs to call. </st><st c="26612">The main goal of this, though, is to establish the </st><em class="italic"><st c="26663">primary</st></em><st c="26670"> LLM that we want to use directly for LLMs (</st><code><st c="26714">llm</st></code><st c="26718">), and then two additional LLMs that are designated for the evaluation process (</st><code><st c="26799">generator_llm</st></code> <st c="26813">and </st><code><st c="26818">critic_llm</st></code><st c="26828">).</st></p>
			<p><st c="26831">We have the benefit of having a more advanced LLM available, ChatGPT-4o-mini, which we can use as the critic LLM, which, in theory, means it can be more effective at evaluating input that we feed to it. </st><st c="27035">This may not always be the case, or you may have an LLM </st><a id="_idIndexMarker580"/><st c="27091">that you fine-tune specifically for the task of evaluation. </st><st c="27151">Either way, breaking these LLMs out into specialized designations shows you how different LLMs can be used for different </st><a id="_idIndexMarker581"/><st c="27272">purposes within a RAG system. </st><st c="27302">You can remove the following line from the previous code that was initializing the LLM object that we were </st><st c="27409">originally using:</st></p>
			<pre class="source-code"><st c="27426">
llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)</st></pre>
			<p><st c="27484">Next, we will add a new RAG chain for running the similarity search (which is what we were originally running with just the </st><st c="27609">dense embeddings):</st></p>
			<pre class="source-code">
<strong class="bold"><st c="27627">rag_chain_similarity</st></strong><st c="27648"> = RunnableParallel(
        {"context": dense_retriever,
        "question": RunnablePassthrough()
}).assign(answer=rag_chain_from_docs)</st></pre>
			<p><st c="27769">To make things clearer, we will update the hybrid RAG chain with </st><st c="27835">this name:</st></p>
			<pre class="source-code">
<strong class="bold"><st c="27845">rag_chain_hybrid</st></strong><st c="27862"> = RunnableParallel(
        {"context": ensemble_retriever,
        "question": RunnablePassthrough()
}).assign(answer=rag_chain_from_docs)</st></pre>
			<p><st c="27986">Note the change of the variable that is bolded, which used to be </st><code><st c="28052">rag_chain_with_source</st></code><st c="28073">. It is now called </st><code><st c="28092">rag_chain_hybrid</st></code><st c="28108">, representing the hybrid </st><st c="28134">search aspect.</st></p>
			<p><st c="28148">Now we are going to update our original code for submitting a user query, only this time, we are going to use both the similarity and hybrid </st><st c="28290">search versions.</st></p>
			<p><st c="28306">Create the </st><st c="28318">similarity version:</st></p>
			<pre class="source-code"><st c="28337">
user_query = "What are Google's environmental initiatives?"
</st><st c="28398">result = rag_chain_similarity.invoke(user_query)
retrieved_docs = result['context']
print(f"Original Question to Similarity Search: {user_query}\n")
print(f"Relevance Score: {result['answer']['relevance_score']}\n")
print(f"Final Answer:\n{result['answer']['final_answer']}\n\n")
print("Retrieved Documents:")
for i, doc in enumerate(retrieved_docs, start=1):
    print(f"Document {i}: Document ID: {doc.metadata['id']}
        source: {doc.metadata['source']}")
    print(f"Content:\n{doc.page_content}\n")</st></pre>
			<p><st c="28889">Now, create </st><a id="_idIndexMarker582"/><st c="28902">the </st><a id="_idIndexMarker583"/><st c="28906">hybrid version:</st></p>
			<pre class="source-code"><st c="28921">
user_query = "What are Google's environmental initiatives?"
</st><st c="28982">result = rag_chain_hybrid.invoke(user_query)
retrieved_docs = result['context']
print(f"Original Question to Dense Search:: {user_query}\n")
print(f"Relevance Score: {result['answer']['relevance_score']}\n")
print(f"Final Answer:\n{result['answer']['final_answer']}\n\n")
print("Retrieved Documents:")
for i, doc in enumerate(retrieved_docs, start=1):
    print(f"Document {i}: Document ID: {doc.metadata['id']}
        source: {doc.metadata['source']}")
    print(f"Content:\n{doc.page_content}\n")</st></pre>
			<p><st c="29465">The </st><a id="_idIndexMarker584"/><st c="29470">primary difference between these two sets of code is that they show the use of the different </st><a id="_idIndexMarker585"/><st c="29563">RAG chains we have created, </st><code><st c="29591">rag_chain_similarity</st></code> <st c="29611">and </st><code><st c="29616">rag_chain_hybrid</st></code><st c="29632">.</st></p>
			<p><st c="29633">First, let’s take a look at the output from the </st><st c="29682">similarity search:</st></p>
			<pre class="source-code"><st c="29700">
Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, water stewardship, and promoting a circular economy. </st><st c="29942">They have implemented sustainability features in products like Google Maps, Google Nest thermostats, and Google Flights to help individuals make more sustainable choices. </st><st c="30113">Google also supports various environmental organizations and initiatives, such as the iMasons Climate Accord, ReFED, and The Nature Conservancy, to accelerate climate action and address environmental challenges. </st><st c="30325">Additionally, Google is involved in public policy advocacy and is committed to reducing its environmental impact through its operations and value chain.</st></pre>
			<p><st c="30477">Next is the output from the </st><st c="30506">hybrid search:</st></p>
			<pre class="source-code"><st c="30520">
Google's environmental initiatives include empowering individuals to take action, working together with partners and customers, operating sustainably, achieving net-zero carbon emissions, focusing on water stewardship, promoting a circular economy, engaging with suppliers to reduce energy consumption and greenhouse gas emissions, and reporting environmental data. </st><st c="30887">They also support public policy and advocacy for low-carbon economies, participate in initiatives like the iMasons Climate Accord and ReFED, and support projects with organizations like The Nature Conservancy. </st><st c="31097">Additionally, Google is involved in initiatives with the World Business Council for Sustainable Development and the World Resources Institute to improve well-being for people and the planet. </st><st c="31288">They are also working on using technology and platforms to organize information about the planet and make it actionable to help partners and customers create a positive impact.</st></pre>
			<p><st c="31464">Saying which is better may be subjective, but if you look back at the code from </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="31545">Chapter 8</st></em></a><st c="31554">, the retrieval mechanism for each of these chains returns a different set of data for the </st><a id="_idIndexMarker586"/><st c="31645">LLM to use as the basis for answering the user query. </st><st c="31699">You can see these differences reflected </st><a id="_idIndexMarker587"/><st c="31739">in the preceding responses, where each has slightly different information and highlights slightly different aspects of </st><st c="31858">that information.</st></p>
			<p><st c="31875">So far, we have set up our RAG system to have the ability to use two different RAG chains, one focused on using just the similarity/dense search and the other using the hybrid search. </st><st c="32060">This sets the foundation for applying ragas to our code to establish a more objective approach to evaluating the results we are getting from either of </st><st c="32211">these chains.</st></p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor204"/><st c="32224">Generating the synthetic ground truth</st></h2>
			<p><st c="32262">As we mentioned in the previous section, ground truth is a key element for us to conduct </st><a id="_idIndexMarker588"/><st c="32352">this evaluation analysis. </st><st c="32378">But we have no ground truth—oh no! </st><st c="32413">No problem, we can use ragas to generate synthetic data for </st><st c="32473">this purpose.</st></p>
			<p class="callout-heading"><st c="32486">WARNING</st></p>
			<p class="callout"><st c="32494">The ragas library uses your LLM API extensively. </st><st c="32544">The analysis that ragas will provide is LLM-assisted evaluation, meaning every time a ground-truth example is generated or evaluated, an LLM is called (sometimes multiple times for one metric) and an API charge is incurred. </st><st c="32768">If you generate 100 ground-truth examples, which includes the generation of both questions and answers, and then run six different evaluation metrics, the number of LLM API calls you make multiplies substantially, well into the thousands of calls. </st><st c="33016">It is recommended to use it sparingly until you have a good grasp on how often the calls are being made. </st><st c="33121">These are cost-incurring API calls and they have the potential to run up your LLM API bills! </st><st c="33214">At the time of this writing, I was incurring a cost of about $2 to $2.50 every time I ran through the entire code lab with just 10 ground examples and 6 metrics. </st><st c="33376">If you have a larger dataset or set </st><code><st c="33412">test_size</st></code><st c="33421"> for your </st><code><st c="33431">testset</st></code><st c="33438"> generator to generate more than 10 examples, the costs will </st><st c="33499">increase substantially.</st></p>
			<p><st c="33522">We start </st><a id="_idIndexMarker589"/><st c="33532">with creating an instance of our generator that we will use to generate our </st><st c="33608">ground-truth dataset:</st></p>
			<pre class="source-code"><st c="33629">
generator = TestsetGenerator.from_langchain(
    generator_llm,
    critic_llm,
    embedding_function
)
documents = [Document(page_content=chunk) for chunk in splits]
testset = generator.generate_with_langchain_docs(
    documents,
    test_size=10,
    distributions={
        simple: 0.5,
        reasoning: 0.25,
        multi_context: 0.25
    }
)
testset_df = testset.to_pandas()
testset_df.to_csv(
    os.path.join('testset_data.csv'), index=False)
print("testset DataFrame saved successfully in the local directory.")</st></pre>
			<p><st c="34099">As you can see in this code, we are using both </st><code><st c="34147">generator_llm</st></code><st c="34160"> and </st><code><st c="34165">critic_llm</st></code><st c="34175">, as well as </st><code><st c="34188">embedding_function</st></code><st c="34206">. As the previous</st><em class="italic"><st c="34223">WARNING</st></em><st c="34231"> box states, be careful about this! </st><st c="34267">That is three different APIs that can generate substantial costs if you are not careful with the settings in this code. </st><st c="34387">In this code, we also take our splits of data generated earlier in the code and preprocess them to work more effectively with ragas. </st><st c="34520">Each </st><a id="_idIndexMarker590"/><st c="34525">chunk in splits is assumed to be a string representing a portion of a document. </st><st c="34605">The </st><code><st c="34609">Document</st></code><st c="34617"> class is from the LangChain library and is a convenient way to represent a document with </st><st c="34707">its content.</st></p>
			<p><code><st c="34719">testset</st></code><st c="34727"> uses the </st><code><st c="34737">generator_with_langchain_docs</st></code><st c="34766"> function from our generator object to generate a synthetic test. </st><st c="34832">This function takes the documents list as input. </st><st c="34881">The </st><code><st c="34885">test_size</st></code><st c="34894"> parameter sets the desired number of test questions to be generated (in this case, 10). </st><st c="34983">The </st><code><st c="34987">distributions</st></code><st c="35000"> parameter defines the distribution of question types, with simple questions comprising 50% of the dataset, reasoning questions 25%, and multi-context questions 25%, in this example. </st><st c="35183">We then convert </st><code><st c="35199">testset</st></code><st c="35206"> into a pandas DataFrame, which we can use to view the results, and save it as a file. </st><st c="35293">Given the costs we just mentioned, saving the data at this point to a CSV that can persist in your file directory offers the added convenience of only having to run this </st><st c="35463">code once!</st></p>
			<p><st c="35473">Now let’s pull our saved dataset back up and look </st><st c="35524">at it!</st></p>
			<pre class="source-code"><st c="35530">
saved_testset_df = pd.read_csv(os.path.join('testset_data.csv'))
print("testset DataFrame loaded successfully from local directory.")
saved_testset_df.head(5)</st></pre>
			<p><st c="35689">The output should look something like what you see here in </st><em class="italic"><st c="35749">Figure 9</st></em><em class="italic"><st c="35757">.1</st></em><st c="35759">:</st></p>
			<div><div><img src="img/B22475_09_01.jpg" alt="Figure 9.1 – DataFrame showing synthesized ground-truth data"/><st c="35761"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="36615">Figure 9.1 – DataFrame showing synthesized ground-truth data</st></p>
			<p><st c="36675">In this dataset, you see questions and answers (</st><code><st c="36724">ground_truth</st></code><st c="36737">) that have been generated by the </st><code><st c="36772">generator_llm</st></code><st c="36785"> instance you initialized earlier. </st><st c="36820">You now have your ground truth! </st><st c="36852">The LLM will attempt to generate 10 different question-and-answer pairs for our ground truth, but in some cases, a failure will occur that limits this generation. </st><st c="37015">This will result </st><a id="_idIndexMarker591"/><st c="37032">in fewer ground-truth examples than you had set in the </st><code><st c="37087">test_size</st></code><st c="37096"> variable. </st><st c="37107">In this case, the generation resulted in 7 examples, rather than 10. </st><st c="37176">Overall, you will likely want to generate more than 10 examples for a thorough test of your RAG system. </st><st c="37280">We are going to accept seven examples for this simple example though, primarily to keep your API </st><st c="37377">costs down!</st></p>
			<p><st c="37388">Next, let’s prepare the </st><st c="37413">similarity dataset:</st></p>
			<pre class="source-code"><st c="37432">
saved_testing_data = \
    saved_testset_df.astype(str).to_dict(orient='list')
saved_testing_dataset = Dataset.from_dict(saved_testing_data)
saved_testing_dataset_sm = saved_testing_dataset.remove_columns(
    ["evolution_type", "episode_done"])</st></pre>
			<p><st c="37670">Here, we are performing some more data conversion to make formats compatible with other parts of our code (for ragas input in this case). </st><st c="37809">We convert the </st><code><st c="37824">saved_testset_df</st></code><st c="37840"> DataFrame into a dictionary format using the </st><code><st c="37886">to_dict()</st></code><st c="37895"> method with </st><code><st c="37908">orient='list'</st></code><st c="37921">, after converting all columns to the string type using </st><code><st c="37977">astype(str)</st></code><st c="37988">. The resulting </st><code><st c="38004">saved_testing_data</st></code><st c="38022"> dictionary is then used to create a </st><code><st c="38059">Dataset</st></code><st c="38066"> object called </st><code><st c="38081">saved_testing_dataset</st></code><st c="38102"> using the </st><code><st c="38113">from_dict()</st></code><st c="38124"> method from the </st><code><st c="38141">datasets</st></code><st c="38149"> library. </st><st c="38159">We create a new dataset called </st><code><st c="38190">saved_testing_dataset_sm</st></code><st c="38214"> representing a smaller section of the data containing just the columns </st><st c="38286">we need.</st></p>
			<p><st c="38294">In this case, we remove the </st><code><st c="38323">evolution_type</st></code><st c="38337"> and </st><code><st c="38342">episode_done</st></code><st c="38354"> columns using the </st><code><st c="38373">remove_columns()</st></code><st c="38389"> method. </st><st c="38398">Let’s take a look by adding this code in a </st><st c="38441">separate cell:</st></p>
			<pre class="source-code"><st c="38455">
saved_testing_dataset_sm</st></pre>
			<p><st c="38480">The output should look </st><st c="38504">like this:</st></p>
			<pre class="source-code"><st c="38514">
Dataset({
    features: ['question', 'contexts', 'ground_truth', 'metadata'],
    num_rows: 7
})</st></pre>
			<p><st c="38603">If you </st><a id="_idIndexMarker592"/><st c="38611">have more ground-truth examples, the </st><code><st c="38648">num_rows</st></code><st c="38656"> variable will reflect that, but the rest should be the same. </st><st c="38718">The </st><code><st c="38722">Dataset</st></code><st c="38729"> object indicates the “features” we have, representing the columns we passed into it, and then this indicates that we have seven rows </st><st c="38863">of data.</st></p>
			<p><st c="38871">Next, we will set up a function to run the RAG chains we pass to it, and then add some additional formatting that enables it to work </st><st c="39005">with ragas:</st></p>
			<pre class="source-code"><st c="39016">
def generate_answer(question, ground_truth, rag_chain):
    result = rag_chain.invoke(question)
    return {
        "question": question,
        "answer": result["answer"]["final_answer"],
        "contexts": [doc.page_content for doc in result["context"]],
        "ground_truth": ground_truth
    }</st></pre>
			<p><st c="39275">This block defines a </st><code><st c="39297">generate_answer()</st></code><st c="39314"> function that takes a question, the </st><code><st c="39351">ground_truth</st></code><st c="39363"> data, and </st><code><st c="39374">rag_chain</st></code><st c="39383"> as inputs. </st><st c="39395">This function is flexible in that it accepts either of the chains that we provide to it, which will come in handy when we want to generate an analysis of both the similarity and hybrid chains. </st><st c="39588">The first step in this function is to invoke the </st><code><st c="39637">rag_chain</st></code><st c="39646"> input that has been passed to it with the given question </st><a id="_idIndexMarker593"/><st c="39704">and retrieve the result. </st><st c="39729">The second step is to return a dictionary containing the question, the final answer from the result, the contexts extracted from the result, and the </st><st c="39878">ground truth.</st></p>
			<p><st c="39891">Now we are ready to prep our datasets more to work </st><st c="39943">with ragas:</st></p>
			<pre class="source-code"><st c="39954">
testing_dataset_similarity = saved_testing_dataset_sm.map(
    lambda x: generate_answer(x["question"],
        x["ground_truth"], rag_chain_similarity),
    remove_columns=saved_testing_dataset_sm.column_names)
testing_dataset_hybrid = saved_testing_dataset_sm.map(
    lambda x: generate_answer(x["question"],
        x["ground_truth"], rag_chain_hybrid),
    remove_columns=saved_testing_dataset_sm.column_names)</st></pre>
			<p><st c="40338">In this code, we create two new datasets, </st><code><st c="40381">testing_dataset_similarity</st></code><st c="40407"> and </st><code><st c="40412">testing_dataset_hybrid</st></code><st c="40434">, by applying the </st><code><st c="40452">generate_answer()</st></code><st c="40469"> function to each row of </st><code><st c="40494">saved_testing_dataset_sm</st></code><st c="40518"> for each of our RAG chains (similarity and hybrid) using the </st><code><st c="40580">map()</st></code><st c="40585"> method. </st><code><st c="40594">rag_chain_similarity</st></code><st c="40614"> and </st><code><st c="40619">rag_chain_hybrid</st></code><st c="40635"> are used as the </st><code><st c="40652">rag_chain</st></code><st c="40661"> argument in the respective dataset creations. </st><st c="40708">The original columns of </st><code><st c="40732">saved_testing_dataset_sm</st></code><st c="40756"> are removed </st><st c="40769">using </st><code><st c="40775">remove_columns=saved_testing_dataset_sm.column_names</st></code><st c="40827">.</st></p>
			<p><st c="40828">And finally, let’s run ragas on the two datasets. </st><st c="40879">Here is the code for applying ragas to our similarity </st><st c="40933">RAG chain:</st></p>
			<pre class="source-code"><st c="40943">
score_similarity = evaluate(
    testing_dataset_similarity,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness,
        answer_similarity
    ]
)
similarity_df = score_similarity.to_pandas()</st></pre>
			<p><st c="41164">Here, we </st><a id="_idIndexMarker594"/><st c="41174">apply ragas to evaluate </st><code><st c="41198">testing_dataset_similarity</st></code><st c="41224"> using the </st><code><st c="41235">evaluate()</st></code><st c="41245"> function from the ragas library. </st><st c="41279">The evaluation is performed using the specified metrics, which include </st><code><st c="41350">faithfulness</st></code><st c="41362">, </st><code><st c="41364">answer_relevancy</st></code><st c="41380">, </st><code><st c="41382">context_precision</st></code><st c="41399">, </st><code><st c="41401">context_recall</st></code><st c="41415">, </st><code><st c="41417">answer_correctness</st></code><st c="41435">, and </st><code><st c="41441">answer_similarity</st></code><st c="41458">. The evaluation results are stored in the </st><code><st c="41501">score_similarity</st></code><st c="41517"> variable, which is then converted to a pandas DataFrame, </st><code><st c="41575">similarity_df</st></code><st c="41588">, using the </st><code><st c="41600">to_pandas()</st></code><st c="41611"> method.</st></p>
			<p><st c="41619">We will do the same with the </st><st c="41649">hybrid dataset:</st></p>
			<pre class="source-code"><st c="41664">
score_hybrid = evaluate(
    testing_dataset_hybrid,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall,
        answer_correctness,
        answer_similarity
    ]
)
hybrid_df = score_hybrid.to_pandas()</st></pre>
			<p><st c="41869">Once </st><a id="_idIndexMarker595"/><st c="41875">you have reached this point, the use of ragas is done! </st><st c="41930">We have now performed a full evaluation of our two chains using ragas, and within these two DataFrames, </st><code><st c="42034">similarity_df</st></code><st c="42047"> and </st><code><st c="42052">hybrid_df</st></code><st c="42061">, we have all of our metrics data. </st><st c="42096">All we have left to do is analyze the data </st><st c="42139">ragas provided.</st></p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor205"/><st c="42154">Analyzing the ragas results</st></h2>
			<p><st c="42182">We will spend the rest of this code lab formatting the data so that we can first save and persist it (because again, this can be a more expensive part of our RAG system). </st><st c="42354">The rest of this </st><a id="_idIndexMarker596"/><st c="42371">code can be reused in the future to pull the data from the </st><code><st c="42430">.csv</st></code><st c="42434"> files if you save them, preventing you from having to re-run this potentially expensive </st><st c="42523">evaluation process.</st></p>
			<p><st c="42542">Let’s start with setting up some important variables and then saving the data we’ve collected to </st><code><st c="42640">csv</st></code><st c="42643"> files:</st></p>
			<pre class="source-code"><st c="42650">
key_columns = [
    'faithfulness',
    'answer_relevancy',
    'context_precision',
    'context_recall',
    'answer_correctness',
    'answer_similarity'
]
similarity_means = similarity_df[key_columns].mean()
hybrid_means = hybrid_df[key_columns].mean()
comparison_df = pd.DataFrame(
        {'Similarity Run': similarity_means,
        'Hybrid Run': hybrid_means})
comparison_df['Difference'] = comparison_df['Similarity Run'] \
        - comparison_df['Hybrid Run']
similarity_df.to_csv(
    os.path.join('similarity_run_data.csv'), index=False)
hybrid_df.to_csv(
    os.path.join('hybrid_run_data.csv'), index=False)
comparison_df.to_csv(os.path.join('comparison_data.csv'), index=True)
print("Dataframes saved successfully in the local directory.")</st></pre>
			<p><st c="43350">In this </st><a id="_idIndexMarker597"/><st c="43359">code, we first define a </st><code><st c="43383">key_columns</st></code><st c="43394"> list containing the names of the key columns to be used for comparison. </st><st c="43467">We then calculate the mean scores for each key column in </st><code><st c="43524">similarity_df</st></code><st c="43537"> and </st><code><st c="43542">hybrid_df</st></code><st c="43551"> using the </st><code><st c="43562">mean()</st></code><st c="43568"> method and store them in </st><code><st c="43594">similarity_means</st></code><st c="43610"> and </st><code><st c="43615">hybrid_means</st></code><st c="43627">, respectively.</st></p>
			<p><st c="43642">Next, we create a new DataFrame called </st><code><st c="43682">comparison_df</st></code><st c="43695"> that compares the mean scores of the similarity run and the hybrid run. </st><st c="43768">The </st><code><st c="43772">Difference</st></code><st c="43782"> column is added to </st><code><st c="43802">comparison_df</st></code><st c="43815">, calculated as the difference between the mean scores of the similarity run and the hybrid run. </st><st c="43912">And finally, we save the </st><code><st c="43937">similarity_df</st></code><st c="43950">, </st><code><st c="43952">hybrid_df</st></code><st c="43961">, and </st><code><st c="43967">comparison_df</st></code><st c="43980"> DataFrames as </st><code><st c="43995">.csv</st></code><st c="43999"> files. </st><st c="44007">We will save the files again, and we can work from these files in the future without having to go back and re-generate everything again. </st></p>
			<p><st c="44144">Also, keep in mind that this is just one way to conduct the analysis. </st><st c="44214">This is where you will want to get creative and adjust this code to conduct an analysis that focuses on the aspects you find important in your specific RAG system. </st><st c="44378">For example, you may be focused solely on improving your retrieval mechanisms. </st><st c="44457">Or you could be applying this to data that is streaming from a deployed environment, in which case you likely have no ground truth and will want to focus on the metrics that can work without a ground truth (see the </st><em class="italic"><st c="44672">ragas founder insights</st></em><st c="44694"> section later in this chapter for more information about </st><st c="44752">that concept).</st></p>
			<p><st c="44766">Moving </st><a id="_idIndexMarker598"/><st c="44774">on with this analysis, though, we now want to pull the files we saved back up to complete our analysis, and then print out our analysis of each of the stages of our RAG system across the two </st><st c="44965">different chains:</st></p>
			<pre class="source-code"><st c="44982">
sem_df = pd.read_csv(os.path.join('similarity_run_data.csv'))
rec_df = pd.read_csv(os.path.join('hybrid_run_data.csv'))
comparison_df = pd.read_csv(
    os.path.join('comparison_data.csv'), index_col=0)
print("Dataframes loaded successfully from the local directory.")
print("Performance Comparison:")
print("\n**Retrieval**:")
print(comparison_df.loc[['context_precision', 'context_recall']])
print("\n**Generation**:")
print(comparison_df.loc[['faithfulness', 'answer_relevancy']])
print("\n**End-to-end evaluation**:")
print(comparison_df.loc[['answer_correctness', 'answer_similarity']])</st></pre>
			<p><st c="45570">This section of the code will generate a set of metrics that we will examine further as follows. </st><st c="45668">We first load DataFrames from the CSV files we generated in the previous code block. </st><st c="45753">We then apply an analysis that consolidates everything into </st><st c="45813">easier-to-read scores.</st></p>
			<p><st c="45835">We continue on, using variables we defined in the previous code block to help generate plots </st><st c="45929">with </st><code><st c="45934">matplotlib</st></code><st c="45944">:</st></p>
			<pre class="source-code"><st c="45946">
fig, axes = plt.subplots(3, 1, figsize=(12, 18), sharex=False)
bar_width = 0.35
categories = ['Retrieval', 'Generation', 'End-to-end evaluation']
metrics = [
    ['context_precision', 'context_recall'],
    ['faithfulness', 'answer_relevancy'],
    ['answer_correctness', 'answer_similarity']
]</st></pre>
			<p><st c="46229">Here, we </st><a id="_idIndexMarker599"/><st c="46239">are creating subplots for each category with </st><st c="46284">increased spacing.</st></p>
			<p><st c="46302">Next, we will iterate over each of those categories and plot the </st><st c="46368">corresponding metrics:</st></p>
			<pre class="source-code"><st c="46390">
for i, (category, metric_list) in enumerate(zip(categories, metrics)):
    ax = axes[i]
    x = range(len(metric_list))
    similarity_bars = ax.bar(
    x, comparison_df.loc[metric_list, 'Similarity Run'],
    width=bar_width, label='Similarity Run',
    color='#D51900')
    for bar in similarity_bars:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            height, f'{height:.1%}', ha='center',
            va='bottom', fontsize=10)
    hybrid_bars = ax.bar(
        [i + bar_width for i in x],
        comparison_df.loc[metric_list, 'Hybrid Run'],
        width=bar_width, label='Hybrid Run',
        color='#992111')
    for bar in hybrid_bars:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2,
            height, f'{height:.1%}', ha='center',
            va='bottom', fontsize=10)
    ax.set_title(category, fontsize=14, pad=20)
    ax.set_xticks([i + bar_width / 2 for i in x])
    ax.set_xticklabels(metric_list, rotation=45,
        ha='right', fontsize=12)
    ax.legend(fontsize=12, loc='lower right',
        bbox_to_anchor=(1, 0))</st></pre>
			<p><st c="47334">Most </st><a id="_idIndexMarker600"/><st c="47340">of this code is focused on formatting our visualizations, including plotting bars for both the similarity and hybrid runs, as well as adding values to those bars. </st><st c="47503">We give the bars some color and even add some hashing to improve accessibility for the </st><st c="47590">visually impaired.</st></p>
			<p><st c="47608">We have just a few more improvements to make to </st><st c="47657">the visualization:</st></p>
			<pre class="source-code"><st c="47675">
fig.text(0.04, 0.5, 'Scores', va='center',
    rotation='vertical', fontsize=14)
fig.suptitle('Performance Comparison', fontsize=16)
plt.tight_layout(rect=[0.05, 0.03, 1, 0.95])
plt.subplots_adjust(hspace=0.6, top=0.92)
plt.show()</st></pre>
			<p><st c="47902">In this code, we add labels and the title to our visualization. </st><st c="47967">We also adjust the spacing between the subplots and increase the top margin. </st><st c="48044">Then, finally, we </st><code><st c="48062">use plt.show()</st></code><st c="48076"> to display the visualization within the </st><st c="48117">notebook interface.</st></p>
			<p><st c="48136">Overall, the code in this section will generate a text-based analysis that shows you results </st><a id="_idIndexMarker601"/><st c="48230">from both chains, and then it generates a visualization in the form of bar charts comparing the results. </st><st c="48335">While the code will generate all of this together, we are going to break this up and discuss each part of the output as it relates to the main stages of our </st><st c="48492">RAG system.</st></p>
			<p><st c="48503">As we discussed in previous chapters, RAG has two primary stages of action when it is engaged: retrieval and generation. </st><st c="48625">When evaluating a RAG system, you can break down your evaluation by those two categories as well. </st><st c="48723">Let’s first talk about </st><st c="48746">evaluating retrieval.</st></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor206"/><st c="48767">Retrieval evaluation</st></h2>
			<p><st c="48788">Ragas </st><a id="_idIndexMarker602"/><st c="48795">provides metrics </st><a id="_idIndexMarker603"/><st c="48812">for evaluating each stage of the RAG pipeline in </st><a id="_idIndexMarker604"/><st c="48861">isolation. </st><st c="48872">For retrieval, ragas has two metrics, called </st><strong class="bold"><st c="48917">context precision</st></strong><st c="48934"> and </st><strong class="bold"><st c="48939">context recall</st></strong><st c="48953">. You </st><a id="_idIndexMarker605"/><st c="48959">can see this here in this part of the output </st><st c="49004">and charts:</st></p>
			<pre class="source-code"><st c="49015">
Performance Comparison:
**Retrieval**:
                   Similarity Run  Hybrid Run  Difference
context_precision        0.906113    0.841267    0.064846
context_recall           0.950000    0.925000    0.025000</st></pre>
			<p><st c="49178">You can see the chart for the retrieval metrics in </st><em class="italic"><st c="49230">Figure 9</st></em><em class="italic"><st c="49238">.2</st></em><st c="49240">:</st></p>
			<div><div><img src="img/B22475_09_02.jpg" alt="Figure 9.2 – Chart showing retrieval performance comparison between similarity search and hybrid search"/><st c="49242"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49254">Figure 9.2 – Chart showing retrieval performance comparison between similarity search and hybrid search</st></p>
			<p><st c="49357">Retrieval </st><a id="_idIndexMarker606"/><st c="49368">evaluation is focused on assessing the accuracy and relevance of the documents that were retrieved. </st><st c="49468">We do this with ragas using these two metrics, as described on the ragas </st><st c="49541">documentation website:</st></p>
			<ul>
				<li><code><st c="49632">context_precision</st></code><st c="49649"> is a metric that evaluates whether all of the ground-truth-relevant items present in the contexts are ranked higher or not. </st><st c="49774">Ideally, all the relevant chunks must appear at the top ranks. </st><st c="49837">This metric is computed using the question, </st><code><st c="49881">ground_truth</st></code><st c="49893">, and </st><code><st c="49899">contexts</st></code><st c="49907">, with values ranging between </st><code><st c="49937">0</st></code><st c="49938"> and </st><code><st c="49943">1</st></code><st c="49944">, where higher scores indicate </st><st c="49975">better precision.</st></li>
				<li><code><st c="50088">context_recall</st></code><st c="50102"> measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. </st><st c="50217">It is computed based on the ground truth and the retrieved context, and the values range between </st><code><st c="50314">0</st></code><st c="50315"> and </st><code><st c="50320">1</st></code><st c="50321">, with higher values indicating </st><st c="50353">better performance.</st></li>
			</ul>
			<p><st c="50372">If you come from a traditional data science or information retrieval background, you may recognize the terms </st><em class="italic"><st c="50482">precision</st></em><st c="50491"> and </st><em class="italic"><st c="50496">recall</st></em><st c="50502"> and be wondering whether there is any relation to those terms. </st><st c="50566">The context precision and context recall metrics used in ragas are conceptually similar to those traditional precision and </st><st c="50689">recall metrics.</st></p>
			<p><st c="50704">In traditional terms, precision measures the proportion of retrieved items that are relevant, while recall measures the proportion of relevant items that </st><st c="50859">are retrieved.</st></p>
			<p><st c="50873">Similarly, context precision evaluates the relevance of the retrieved context by assessing whether </st><a id="_idIndexMarker607"/><st c="50973">the ground-truth-relevant items are ranked higher, while context recall measures the extent to which the retrieved context covers the relevant information required to answer </st><st c="51147">the question.</st></p>
			<p><st c="51160">However, there are some key differences </st><st c="51201">to note.</st></p>
			<p><st c="51209">Traditional precision and recall are typically computed based on a binary relevance judgment (relevant or not relevant) for each item, whereas context precision and recall in ragas consider the ranking and alignment of the retrieved context with respect to the ground-truth answer. </st><st c="51492">Additionally, context precision and recall are specifically designed to evaluate the retrieval performance in the context of question-answering tasks, taking into account the specific requirements of retrieving relevant information to answer a </st><st c="51736">given question.</st></p>
			<p><st c="51751">When looking at the results from our analysis, we need to keep in mind that we are using a small dataset for our ground truth. </st><st c="51879">In fact, the original dataset our entire RAG system is based on is small and that could also impact our results. </st><st c="51992">Therefore, I wouldn’t read into the numbers you are seeing here too much. </st><st c="52066">But what this does show you is how you can use ragas to run an analysis and then provide a very informative representation of what is happening in the retrieval stage of our RAG system. </st><st c="52252">This code lab is primarily to demonstrate real-world challenges you will likely encounter when building a RAG system, where you have to consider different metrics in the context of your specific use case, the trade-offs between those different metrics, and having to decide which approach fits your needs in the most </st><st c="52569">effective way.</st></p>
			<p><st c="52583">Next, we will review a similar analysis in the generation stage of our </st><st c="52655">RAG system.</st></p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor207"/><st c="52666">Generation evaluation</st></h2>
			<p><st c="52688">As </st><a id="_idIndexMarker608"/><st c="52692">mentioned, ragas provides metrics for evaluating each stage of the RAG pipeline in isolation. </st><st c="52786">For the generation stage, ragas has two metrics, called </st><code><st c="52842">faithfulness</st></code><st c="52854"> and </st><code><st c="52859">answer relevancy</st></code><st c="52875">, as you see here in this part of the output and the </st><st c="52928">following chart:</st></p>
			<pre class="source-code"><st c="52944">
**Generation**:
                  Similarity Run  Hybrid Run  Difference
faithfulness            0.977500    0.945833    0.031667
answer_relevancy        0.968222    0.965247    0.002976</st></pre>
			<p><st c="53081">The generation metrics can be seen in the chart in </st><em class="italic"><st c="53133">Figure 9</st></em><em class="italic"><st c="53141">.3</st></em><st c="53143">:</st></p>
			<div><div><img src="img/B22475_09_03.jpg" alt="Figure 9.3 – Chart showing generation performance comparison between similarity search and hybrid search"/><st c="53145"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="53175">Figure 9.3 – Chart showing generation performance comparison between similarity search and hybrid search</st></p>
			<p><st c="53279">Generation evaluation measures the appropriateness of the response generated by the system when the context is provided. </st><st c="53401">We do this with ragas using the following two metrics, as described in the </st><st c="53476">ragas documentation:</st></p>
			<ul>
				<li><code><st c="53496">faithfullness</st></code><st c="53510">: How factually accurate is the generated answer? </st><st c="53561">This measures the factual consistency of the generated answer against the given context. </st><st c="53650">It is calculated from the answer and retrieved context. </st><st c="53706">The answer is scaled to a (</st><code><st c="53733">0-1</st></code><st c="53737">) range, with a higher score </st><st c="53767">being better.</st></li>
				<li><code><st c="53780">answer_relevancy</st></code><st c="53797">: How relevant is the generated answer to the question? </st><st c="53854">Answer relevancy focuses on assessing how pertinent the generated answer is to the given prompt. </st><st c="53951">A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. </st><st c="54086">This metric is computed using the question, the context, and </st><st c="54147">the answer.</st></li>
			</ul>
			<p><st c="54158">Again, I want </st><a id="_idIndexMarker609"/><st c="54173">to reiterate that we are using a small dataset for our ground truth and dataset, which likely makes these results less reliable. </st><st c="54302">But you can see here how these results form the foundation for providing a very informative representation of what is happening in the generation stage of our </st><st c="54461">RAG system.</st></p>
			<p><st c="54472">This leads us to our next set of metrics, the end-to-end evaluation metrics, which we </st><st c="54559">discuss next.</st></p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor208"/><st c="54572">End-to-end evaluation</st></h1>
			<p><st c="54594">Beyond providing the metrics for evaluating each stage of the RAG pipeline in isolation, ragas </st><a id="_idIndexMarker610"/><st c="54690">provides metrics for the entire RAG system, called end-to-end </st><a id="_idIndexMarker611"/><st c="54752">evaluation. </st><st c="54764">For the generation stage, ragas has two </st><a id="_idIndexMarker612"/><st c="54804">metrics, called </st><strong class="bold"><st c="54820">answer correctness</st></strong><st c="54838"> and </st><strong class="bold"><st c="54843">answer similarity</st></strong><st c="54860">, as you see here in the last part of the output </st><st c="54909">and charts:</st></p>
			<pre class="source-code"><st c="54920">
**End-to-end evaluation**:
                    Similarity Run  Hybrid Run  Difference
answer_correctness        0.776018    0.717365    0.058653
answer_similarity         0.969899    0.969170    0.000729</st></pre>
			<p><st c="55075">The chart in </st><em class="italic"><st c="55089">Figure 9</st></em><em class="italic"><st c="55097">.4</st></em><st c="55099"> shows the visualization for </st><st c="55128">these results:</st></p>
			<div><div><img src="img/B22475_09_04.jpg" alt="Figure 9.4 – Chart showing end-to-end performance comparison between similarity search and hybrid search"/><st c="55142"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="55186">Figure 9.4 – Chart showing end-to-end performance comparison between similarity search and hybrid search</st></p>
			<p><st c="55290">End-to-end </st><a id="_idIndexMarker613"/><st c="55302">metrics are for evaluating the end-to-end performance of a pipeline, gauging the overall experience of using the pipeline. </st><st c="55425">Combining these metrics provides a comprehensive evaluation of the RAG pipeline. </st><st c="55506">We do this with ragas using the following two metrics, as described in the </st><st c="55581">ragas documentation:</st></p>
			<ul>
				<li><code><st c="55601">answer_correctness</st></code><st c="55620">: Gauges the accuracy of the generated answer when compared to the ground truth. </st><st c="55702">The assessment of answer correctness involves gauging the accuracy of the generated answer when compared to the ground truth. </st><st c="55828">This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. </st><st c="55920">A higher score indicates a closer alignment between the generated answer and the ground truth, signifying </st><st c="56026">better correctness.</st></li>
				<li><code><st c="56045">answer_similarity</st></code><st c="56063">: Assesses the semantic resemblance between the generated answer and the ground truth. </st><st c="56151">The concept of answer semantic similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. </st><st c="56299">This evaluation is based on the ground truth and the answer, with values falling within the range of </st><code><st c="56400">0</st></code><st c="56401"> to </st><code><st c="56405">1</st></code><st c="56406">. A higher score signifies a better alignment between the generated answer and the </st><st c="56489">ground truth.</st></li>
			</ul>
			<p><st c="56502">Evaluating </st><a id="_idIndexMarker614"/><st c="56514">the end-to-end performance of a pipeline is also crucial, as it directly affects the user experience and helps to ensure a </st><st c="56637">comprehensive evaluation.</st></p>
			<p><st c="56662">To keep this code lab simple, we left out a couple more metrics you might also consider in your analysis. </st><st c="56769">Let’s talk about those </st><st c="56792">metrics next.</st></p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor209"/><st c="56805">Other component-wise evaluation</st></h1>
			<p><st c="56837">Component-wise evaluation involves evaluating individual components of the pipeline, such as the </st><a id="_idIndexMarker615"/><st c="56935">retrieval and generation stages, to gain insights into their effectiveness and identify areas for improvement. </st><st c="57046">We already shared two metrics for each of these stages, but here are a couple more that are available in the </st><st c="57155">ragas platform:</st></p>
			<ul>
				<li><code><st c="57338">(0-1)</st></code><st c="57343">, with higher values indicating </st><st c="57375">better relevancy.</st></li>
				<li><code><st c="57531">ground_truth</st></code><st c="57543"> data and </st><code><st c="57553">contexts</st></code><st c="57561"> data relative to the number of entities present in the </st><code><st c="57617">ground_truth</st></code><st c="57629"> data alone. </st><st c="57642">Simply put, it is a measure of what fraction of entities are recalled from </st><code><st c="57717">ground_truth</st></code><st c="57729"> data. </st><st c="57736">This metric is particularly useful in fact-based use cases such as a tourism help desk and historical Q&amp;A. </st><st c="57843">This metric can help evaluate the retrieval mechanism for entities, based on comparison with entities present in </st><code><st c="57956">ground_truth</st></code><st c="57968"> data, because in cases where entities matter, we need the contexts that </st><st c="58041">cover them.</st></li>
				<li><strong class="bold"><st c="58052">Aspect critique</st></strong><st c="58068">: Aspect critique is designed to assess submissions based on predefined aspects such as harmlessness and correctness. </st><st c="58187">Additionally, users have the flexibility to define their own aspects for evaluating submissions according to their specific criteria. </st><st c="58321">The output of aspect critiques is binary, indicating whether the submission aligns with the defined aspect or not. </st><st c="58436">This evaluation is performed using the “</st><em class="italic"><st c="58476">answer</st></em><st c="58483">” </st><st c="58486">as input.</st></li>
			</ul>
			<p><st c="58495">These additional component-wise evaluation metrics offer further granularity in assessing the retrieved context and the generated answers. </st><st c="58635">To finish off this code lab, we are going </st><a id="_idIndexMarker616"/><st c="58677">to bring in some insights that one of the founders of ragas provided directly to help you with your </st><st c="58777">RAG evaluation.</st></p>
			<p class="callout-heading"><st c="58792">Founder’s perspective</st></p>
			<p class="callout"><st c="58814">In preparation for this chapter, we had a chance to talk with one of the founders of ragas, Shahul Es, to gain additional insights into the platform and how you can better utilize it for RAG development and evaluation. </st><st c="59034">Ragas is a young platform, but as you have seen in the code lab, it already has a solid foundation of metrics that you can implement to evaluate your RAG system. </st><st c="59196">But this also means ragas has a lot of room for growth, this platform that is built specifically for RAG implementations will continue to evolve. </st><st c="59342">Shahul provided some helpful tips and insights that we will summarize and share with you here. </st><st c="59437">We share notes from that discussion in the </st><st c="59480">following section.</st></p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor210"/><st c="59498">Ragas founder insights</st></h2>
			<p><st c="59521">The following </st><a id="_idIndexMarker617"/><st c="59536">is a list of notes taken from a discussion with ragas co-founder Shahul Es, discussing how ragas can be used for </st><st c="59649">RAG evaluation:</st></p>
			<ul>
				<li><strong class="bold"><st c="59664">Synthetic data generation</st></strong><st c="59690">: The first roadblock people typically face with RAG evaluation is not having enough testing ground-truth data. </st><st c="59803">Ragas’ main focus is to create an algorithm that could create a test dataset that covers a wide variety of question types, resulting in their synthetic data generation capabilities. </st><st c="59985">Once you have used ragas to synthesize your ground truth, it is helpful to vet the ground truth generated and pick out any questions that </st><st c="60123">don’t belong.</st></li>
				<li><strong class="bold"><st c="60136">Feedback metrics</st></strong><st c="60153">: Something that is currently being emphasized in their development is incorporating various feedback loops into the evaluation from both performance and user feedback, where there are explicit metrics (something went wrong) and implicit metrics (levels of satisfaction, thumbs up/down, and similar mechanisms). </st><st c="60466">Any kind of interaction with the user can potentially be implicit. </st><st c="60533">Implicit feedback can be noisy (from a data standpoint), but can still be useful if </st><st c="60617">used properly.</st></li>
				<li><strong class="bold"><st c="60631">Reference and reference-free metrics</st></strong><st c="60668">: Shahul categorized the metrics into reference metrics and reference-free metrics, where reference means it requires a ground truth to process. </st><st c="60814">The ragas team place emphasis on building reference-free metrics in their work, which you can read more about in the ragas paper (</st><a href="https://arxiv.org/abs/2309.15217"><st c="60944">https://arxiv.org/abs/2309.15217</st></a><st c="60977">). </st><st c="60981">For many fields, where the ground truth is difficult to collect, this is an important point, as this makes at least </st><a id="_idIndexMarker618"/><st c="61097">some of the evaluation still possible. </st><st c="61136">Faithfulness and answer relevance were reference-free metrics </st><st c="61198">Shahul mentioned.</st></li>
				<li><strong class="bold"><st c="61215">Deployment evaluation</st></strong><st c="61237">: Reference-free evaluation metrics are also ideal for deployment evaluation, where you are less likely to have a ground </st><st c="61359">truth available.</st></li>
			</ul>
			<p><st c="61375">These are some key insights, and it will be exciting to see where ragas development goes in the future to help us all continually improve our RAG systems. </st><st c="61531">You can find the latest ragas documentation </st><st c="61575">here: </st><a href="https://docs.ragas.io/en/stable/"><st c="61581">https://docs.ragas.io/en/stable/</st></a></p>
			<p><st c="61613">That concludes our evaluation code lab using ragas. </st><st c="61666">But ragas is not the only evaluation tool that is used for RAG evaluation; there are many more! </st><st c="61762">Next, we will discuss some other approaches you </st><st c="61810">can consider.</st></p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor211"/><st c="61823">Additional evaluation techniques</st></h1>
			<p><st c="61856">Ragas is just one of many evaluation tools and techniques available to evaluate your RAG system. </st><st c="61954">This is not an exhaustive list, but in the following subsections, we will discuss some of the more popular techniques you can use to evaluate the performance of your RAG system, once you have obtained or generated </st><st c="62168">ground-truth data.</st></p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor212"/><st c="62186">Bilingual Evaluation Understudy (BLEU)</st></h2>
			<p><st c="62225">BLEU measures the overlap of n-grams between the generated response and the ground-truth </st><a id="_idIndexMarker619"/><st c="62315">response. </st><st c="62325">It provides a score indicating the similarity between the two. </st><st c="62388">In the context of RAG, BLEU can be used to evaluate the quality of the generated answers by comparing them to the ground-truth answers. </st><st c="62524">By calculating the n-gram overlap, BLEU assesses how closely the generated answers match the reference answers in terms of word choice and phrasing. </st><st c="62673">However, it’s important to note that BLEU is more focused on surface-level similarity and may not capture the semantic meaning or relevance of the </st><st c="62820">generated answers.</st></p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor213"/><st c="62838">Recall-Oriented Understudy for Gisting Evaluation (ROUGE)</st></h2>
			<p><st c="62896">ROUGE assesses the quality of the generated response by comparing it to the ground truth </st><a id="_idIndexMarker620"/><st c="62986">in terms of recall. </st><st c="63006">It measures how much of the ground truth is captured in the generated response. </st><st c="63086">For RAG evaluation, ROUGE can be used to evaluate the coverage and completeness of the generated answers. </st><st c="63192">By calculating the recall between the generated answers and the ground-truth answers, ROUGE assesses how well the generated answers capture the key information and details present in the reference answers. </st><st c="63398">ROUGE is particularly useful when the ground-truth answers are longer or more detailed, as it focuses on the overlap of information rather than exact </st><st c="63548">word matches.</st></p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor214"/><st c="63561">Semantic similarity</st></h2>
			<p><st c="63581">Metrics </st><a id="_idIndexMarker621"/><st c="63590">such as cosine similarity or </st><strong class="bold"><st c="63619">semantic textual similarity</st></strong><st c="63646"> (</st><strong class="bold"><st c="63648">STS</st></strong><st c="63651">) can be used to evaluate the semantic relevance between the generated response and the ground truth. </st><st c="63754">These metrics capture the meaning and context beyond exact word matches. </st><st c="63827">In RAG evaluation, semantic similarity metrics can be used to assess the semantic coherence and relevance of the generated answers. </st><st c="63959">By comparing the semantic representations of the generated answers and the ground-truth answers, these metrics evaluate how well the generated answers capture the underlying meaning and context of the reference answers. </st><st c="64179">Semantic similarity metrics are particularly useful when the generated answers may use different words or phrasing but still convey the same meaning as the </st><st c="64335">ground truth.</st></p>
			<h2 id="_idParaDest-216"><a id="_idTextAnchor215"/><st c="64348">Human evaluation</st></h2>
			<p><st c="64365">While automated metrics provide a quantitative assessment, human evaluation remains important </st><a id="_idIndexMarker622"/><st c="64460">for assessing the coherence, fluency, and overall quality of the generated responses compared to the ground truth. </st><st c="64575">In the context of RAG, human evaluation involves having human raters assess the generated answers based on various criteria. </st><st c="64700">These criteria may include relevance to the question, factual correctness, clarity of the answer, and overall coherence. </st><st c="64821">Human evaluators can provide qualitative feedback and insights that automated metrics may not capture, such as the appropriateness of the answer tone, the presence of any inconsistencies or contradictions, and the overall user experience. </st><st c="65060">Human evaluation can complement automated metrics by providing a more comprehensive and nuanced assessment of the RAG </st><st c="65178">system’s performance.</st></p>
			<p><st c="65199">When evaluating a RAG system, it’s often beneficial to use a combination of these evaluation techniques to obtain a holistic view of the system’s performance. </st><st c="65359">Each technique has its strengths and limitations, and using multiple metrics can provide a more robust and comprehensive evaluation. </st><st c="65492">Additionally, it’s important to consider the specific requirements and goals of your RAG application when selecting the appropriate evaluation techniques. </st><st c="65647">Some applications may prioritize factual correctness, while others may focus more on the fluency and coherence of the generated answers. </st><st c="65784">By aligning the evaluation techniques with your specific needs, you can effectively assess the performance of your RAG system and identify areas </st><st c="65929">for improvement.</st></p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor216"/><st c="65945">Summary</st></h1>
			<p><st c="65953">In this chapter, we explored the key role that evaluation plays in building and maintaining RAG pipelines. </st><st c="66061">We discussed how evaluation helps developers identify areas for improvement, optimize system performance, and measure the impact of modifications throughout the development process. </st><st c="66243">We also highlighted the importance of evaluating the system after deployment to ensure ongoing effectiveness, reliability, </st><st c="66366">and performance.</st></p>
			<p><st c="66382">We introduced standardized evaluation frameworks for various components of a RAG pipeline, such as embedding models, vector stores, vector search, and LLMs. </st><st c="66540">These frameworks provide valuable benchmarks for comparing the performance of different models and components. </st><st c="66651">We emphasized the significance of ground-truth data in RAG evaluation and discussed methods for obtaining or generating the ground truth, including human annotation, expert knowledge, crowdsourcing, and synthetic </st><st c="66864">ground-truth generation.</st></p>
			<p><st c="66888">The chapter included a hands-on code lab where we integrated the ragas evaluation platform into our RAG system. </st><st c="67001">We generated synthetic ground-truth data and established a comprehensive set of metrics to evaluate the impact of using hybrid search compared to the original dense vector semantic-based search. </st><st c="67196">We explored the different stages of RAG evaluation, including retrieval evaluation, generation evaluation, and end-to-end evaluation, and analyzed the results obtained from our evaluation. </st><st c="67385">The code lab provided a real-world example of implementing a comprehensive evaluation system in a RAG pipeline, demonstrating how developers can leverage evaluation metrics to gain insights and make data-driven decisions to improve their RAG pipelines. </st><st c="67638">We were also able to share key insights from one of the founders of ragas to help your RAG evaluation efforts </st><st c="67748">even further.</st></p>
			<p><st c="67761">In the next chapter, we will start our discussion about how to utilize LangChain with the key components of RAG systems in the most </st><st c="67894">effective way.</st></p>
			<h1 id="_idParaDest-218"><a id="_idTextAnchor217"/><st c="67908">References</st></h1>
			<p><code><st c="67919">MSMARCO</st></code><st c="67927">: </st><a href="https://microsoft.github.io/msmarco/"><st c="67930">https://microsoft.github.io/msmarco/</st></a></p>
			<p><code><st c="67966">HotpotQA</st></code><st c="67975">: </st><a href="https://hotpotqa.github.io/"><st c="67978">https://hotpotqa.github.io/</st></a></p>
			<p><code><st c="68005">CQADupStack</st></code><st c="68017">: </st><a href="http://nlp.cis.unimelb.edu.au/resources/cqadupstack/"><st c="68020">http://nlp.cis.unimelb.edu.au/resources/cqadupstack/</st></a></p>
			<p><strong class="bold"><st c="68072">Chatbot </st></strong><strong class="bold"><st c="68081">Arena</st></strong><st c="68086">: </st><a href="https://chat.lmsys.org/?leaderboard"><st c="68089">https://chat.lmsys.org/?leaderboard</st></a></p>
			<p><strong class="bold"><st c="68124">MMLU</st></strong><st c="68129">: </st><a href="https://arxiv.org/abs/2009.03300"><st c="68132">https://arxiv.org/abs/2009.03300</st></a></p>
			<p><strong class="bold"><st c="68164">MT </st></strong><strong class="bold"><st c="68168">Bench</st></strong><st c="68173">: </st><a href="https://arxiv.org/pdf/2402.14762"><st c="68176">https://arxiv.org/pdf/2402.14762</st></a></p>
		</div>
	<div></body></html>