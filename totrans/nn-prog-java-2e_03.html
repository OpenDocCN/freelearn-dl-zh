<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 3. Perceptrons and Supervised Learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch03" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 3. Perceptrons and Supervised Learning</h1></div></div></div><p class="calibre11">In this chapter, we are going to explore in more detail supervised learning, which is very useful in finding relations between two datasets. Also, we introduce perceptrons, a very popular neural network architecture that implements supervised learning. This chapter also presents their extended generalized version, the so-called multi-layer perceptrons, as well as their features, learning algorithms, and parameters. Also, the reader will learn how to implement them in Java and how to use them in solving some basic problems. This chapter will cover the following topics:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Supervised learning</li><li class="listitem">Regression tasks</li><li class="listitem">Classification tasks</li><li class="listitem">Perceptrons</li><li class="listitem">Linear separation</li><li class="listitem">Limitations: the XOR problem</li><li class="listitem">Multilayer perceptrons</li><li class="listitem">Generalized delta rule – backpropagation algorithm</li><li class="listitem">Levenberg–Marquardt algorithm</li><li class="listitem">Single hidden layer neural networks</li><li class="listitem">Extreme learning machines</li></ul></div><div class="calibre2" title="Supervised learning – teaching the neural net"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec25" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Supervised learning – teaching the neural net</h1></div></div></div><p class="calibre11">In the previous<a id="id147" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> chapter, we introduced the learning paradigms that apply to neural networks, where supervised learning implies that there is a goal or a defined target to reach. In practice, we present a set of input data X, and a set of desired output data YT, then we evaluate a cost function whose aim is to reduce the error between the neural output Y and the target output YT.</p><p class="calibre11">In supervised <a id="id148" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>learning, there are two major categories of tasks involved, which are detailed as follows: classification and regression.</p><div class="calibre2" title="Classification – finding the appropriate class"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec33" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Classification – finding the appropriate class</h2></div></div></div><p class="calibre11">Neural networks <a id="id149" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>also work with categorical data. Given a<a id="id150" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> list of classes and a dataset, one wishes to classify them according to a historical dataset containing records and their respective class. The following table shows an example of this dataset, considering the subjects' average grades between <span class="strong1"><strong class="calibre12">0</strong></span> and <span class="strong1"><strong class="calibre12">10</strong></span>:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th rowspan="2" valign="bottom" class="calibre25">
<p class="calibre26">Student Id</p>
</th><th colspan="8" valign="bottom" class="calibre80">
<p class="calibre26">Subjects</p>
</th><th rowspan="2" valign="bottom" class="calibre25">
<p class="calibre26">Profession</p>
</th></tr><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">English</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Math</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Physics</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Chemistry</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Geography</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">History</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Literature</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Biology</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">89543</p>
</td><td class="calibre29">
<p class="calibre26">7.82</p>
</td><td class="calibre29">
<p class="calibre26">8.82</p>
</td><td class="calibre29">
<p class="calibre26">8.35</p>
</td><td class="calibre29">
<p class="calibre26">7.45</p>
</td><td class="calibre29">
<p class="calibre26">6.55</p>
</td><td class="calibre29">
<p class="calibre26">6.39</p>
</td><td class="calibre29">
<p class="calibre26">5.90</p>
</td><td class="calibre29">
<p class="calibre26">7.03</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Electrical Engineer</strong></span>
</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">93201</p>
</td><td class="calibre29">
<p class="calibre26">8.33</p>
</td><td class="calibre29">
<p class="calibre26">6.75</p>
</td><td class="calibre29">
<p class="calibre26">8.01</p>
</td><td class="calibre29">
<p class="calibre26">6.98</p>
</td><td class="calibre29">
<p class="calibre26">7.95</p>
</td><td class="calibre29">
<p class="calibre26">7.76</p>
</td><td class="calibre29">
<p class="calibre26">6.98</p>
</td><td class="calibre29">
<p class="calibre26">6.84</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Marketer</strong></span>
</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">95481</p>
</td><td class="calibre29">
<p class="calibre26">7.76</p>
</td><td class="calibre29">
<p class="calibre26">7.17</p>
</td><td class="calibre29">
<p class="calibre26">8.39</p>
</td><td class="calibre29">
<p class="calibre26">8.64</p>
</td><td class="calibre29">
<p class="calibre26">8.22</p>
</td><td class="calibre29">
<p class="calibre26">7.86</p>
</td><td class="calibre29">
<p class="calibre26">7.07</p>
</td><td class="calibre29">
<p class="calibre26">9.06</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Doctor</strong></span>
</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">94105</p>
</td><td class="calibre29">
<p class="calibre26">8.25</p>
</td><td class="calibre29">
<p class="calibre26">7.54</p>
</td><td class="calibre29">
<p class="calibre26">7.34</p>
</td><td class="calibre29">
<p class="calibre26">7.65</p>
</td><td class="calibre29">
<p class="calibre26">8.65</p>
</td><td class="calibre29">
<p class="calibre26">8.10</p>
</td><td class="calibre29">
<p class="calibre26">8.40</p>
</td><td class="calibre29">
<p class="calibre26">7.44</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Lawyer</strong></span>
</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">96305</p>
</td><td class="calibre29">
<p class="calibre26">8.05</p>
</td><td class="calibre29">
<p class="calibre26">6.75</p>
</td><td class="calibre29">
<p class="calibre26">6.54</p>
</td><td class="calibre29">
<p class="calibre26">7.20</p>
</td><td class="calibre29">
<p class="calibre26">7.96</p>
</td><td class="calibre29">
<p class="calibre26">7.54</p>
</td><td class="calibre29">
<p class="calibre26">8.01</p>
</td><td class="calibre29">
<p class="calibre26">7.86</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">School Principal</strong></span>
</p>
</td></tr><tr class="calibre37"><td class="calibre29">
<p class="calibre26">92904</p>
</td><td class="calibre29">
<p class="calibre26">6.95</p>
</td><td class="calibre29">
<p class="calibre26">8.85</p>
</td><td class="calibre29">
<p class="calibre26">9.10</p>
</td><td class="calibre29">
<p class="calibre26">7.54</p>
</td><td class="calibre29">
<p class="calibre26">7.50</p>
</td><td class="calibre29">
<p class="calibre26">6.65</p>
</td><td class="calibre29">
<p class="calibre26">5.86</p>
</td><td class="calibre29">
<p class="calibre26">6.76</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Programmer</strong></span>
</p>
</td></tr></tbody></table></div><p class="calibre11">One example is the prediction of profession based on scholar grades. Let's consider a dataset of former students who are now working. We compile a data set containing each student's average grade on each subject and his/her current profession. Note that the output would be the name of professions, which neural networks are not able to give directly. Instead, we need to make one column (one output) for each known profession. If that student chose a certain profession, the column corresponding to that profession would have the value one, otherwise it would be zero:</p><div class="mediaobject"><img src="Images/B05964_03_20.jpg" alt="Classification – finding the appropriate class" class="calibre81"/></div><p class="calibre11">Now we want<a id="id151" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to find a model - based on a neural network - to <a id="id152" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>predict which profession a student will be likely to choose based on his/her grades. To that end, we structure a neural network containing the number of scholar subjects as the input and the number of known professions as the output, and an arbitrary number of hidden neurons in the hidden layer:</p><div class="mediaobject"><img src="Images/B05964_03_01.jpg" alt="Classification – finding the appropriate class" class="calibre82"/></div><p class="calibre11">For classification problems, there is usually only one class for each data point. So in the output layer, the neurons are fired to produce either zero or one, it being better to use activation functions that are output bounded between these two values. However, we must consider the case in which more than one neuron fires, giving two classes for a record. There are a number of mechanisms to prevent this case, such as the softmax function or the winner-takes-all algorithm, for example. These mechanisms are going to be detailed in the practical application in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6. Classifying Disease Diagnosis">Chapter 6</a>, <span class="strong1"><em class="calibre16">Classifying Disease Diagnosis</em></span>.</p><p class="calibre11">After being <a id="id153" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>trained, the neural network has learned what will be<a id="id154" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the most probable profession for a given student given his/her grades.</p></div><div class="calibre2" title="Regression – mapping real inputs to outputs"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec34" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Regression – mapping real inputs to outputs</h2></div></div></div><p class="calibre11">Regression<a id="id155" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> consists in finding some function that maps a set <a id="id156" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of inputs to a set of outputs. The following table shows how a dataset containing k records of m independent inputs X are known to be bound to n dependent outputs:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th colspan="4" valign="bottom" class="calibre80">
<p class="calibre26">Input independent data</p>
</th><th colspan="4" valign="bottom" class="calibre80">
<p class="calibre26">Output dependent data</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">X1</p>
</td><td class="calibre29">
<p class="calibre26">X2</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">XM</p>
</td><td class="calibre29">
<p class="calibre26">T1</p>
</td><td class="calibre29">
<p class="calibre26">T2</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">TN</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">x1[0]</p>
</td><td class="calibre29">
<p class="calibre26">x2[0]</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">xm[0]</p>
</td><td class="calibre29">
<p class="calibre26">t1[0]</p>
</td><td class="calibre29">
<p class="calibre26">t2[0]</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">tn[0]</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">x1[1]</p>
</td><td class="calibre29">
<p class="calibre26">x2[1]</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">xm[1]</p>
</td><td class="calibre29">
<p class="calibre26">t1[1]</p>
</td><td class="calibre29">
<p class="calibre26">t2[1]</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">tn[1]</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td></tr><tr class="calibre28"><td class="calibre29">
<p class="calibre26">x1[k]</p>
</td><td class="calibre29">
<p class="calibre26">x2[k]</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">xm[k]</p>
</td><td class="calibre29">
<p class="calibre26">t1[k]</p>
</td><td class="calibre29">
<p class="calibre26">t2[k]</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">tn[k]</p>
</td></tr></tbody></table></div><p class="calibre11">The preceding table can be compiled in matrix format:</p><div class="mediaobject"><img src="Images/B05964_03_01_01.jpg" alt="Regression – mapping real inputs to outputs" class="calibre83"/></div><p class="calibre11">Unlike the <a id="id157" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>classification, the output values are numerical instead<a id="id158" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of labels or classes. There is also a historical database containing records of some behavior we would like the neural network to learn. One example is the prediction of bus ticket prices between two cities. In this example, we collect information from a list of cities and the current ticket prices of buses departing from one and arriving to another. We structure the city features as well as the distance and/or time between them as the input and the bus ticket price as the output:</p><div class="mediaobject"><img src="Images/B05964_03_02.jpg" alt="Regression – mapping real inputs to outputs" class="calibre84"/></div><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th colspan="3" valign="bottom" class="calibre80">
<p class="calibre26">Features city of origin</p>
</th><th colspan="3" valign="bottom" class="calibre80">
<p class="calibre26">Features city of destination</p>
</th><th colspan="3" valign="bottom" class="calibre80">
<p class="calibre26">Features of the way between</p>
</th><th rowspan="2" valign="bottom" class="calibre25">
<p class="calibre26">Ticket fare</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Population</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">GDP</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Routes</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Population</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">GDP</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Routes</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Distance</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Time</strong></span>
</p>
</td><td class="calibre29">
<p class="calibre26">
<span class="strong1"><strong class="calibre12">Stops</strong></span>
</p>
</td><td class="auto-generated"> </td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">500,000</p>
</td><td class="calibre29">
<p class="calibre26">4.5</p>
</td><td class="calibre29">
<p class="calibre26">6</p>
</td><td class="calibre29">
<p class="calibre26">45,000</p>
</td><td class="calibre29">
<p class="calibre26">1.5</p>
</td><td class="calibre29">
<p class="calibre26">5</p>
</td><td class="calibre29">
<p class="calibre26">90</p>
</td><td class="calibre29">
<p class="calibre26">1,5</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">15</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">120,000</p>
</td><td class="calibre29">
<p class="calibre26">2.6</p>
</td><td class="calibre29">
<p class="calibre26">4</p>
</td><td class="calibre29">
<p class="calibre26">500,000</p>
</td><td class="calibre29">
<p class="calibre26">4.5</p>
</td><td class="calibre29">
<p class="calibre26">6</p>
</td><td class="calibre29">
<p class="calibre26">30</p>
</td><td class="calibre29">
<p class="calibre26">0,8</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">10</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">30,000</p>
</td><td class="calibre29">
<p class="calibre26">0.8</p>
</td><td class="calibre29">
<p class="calibre26">3</p>
</td><td class="calibre29">
<p class="calibre26">65,000</p>
</td><td class="calibre29">
<p class="calibre26">3.0</p>
</td><td class="calibre29">
<p class="calibre26">3</p>
</td><td class="calibre29">
<p class="calibre26">103</p>
</td><td class="calibre29">
<p class="calibre26">1,6</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">20</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">35,000</p>
</td><td class="calibre29">
<p class="calibre26">1.4</p>
</td><td class="calibre29">
<p class="calibre26">3</p>
</td><td class="calibre29">
<p class="calibre26">45,000</p>
</td><td class="calibre29">
<p class="calibre26">1.5</p>
</td><td class="calibre29">
<p class="calibre26">5</p>
</td><td class="calibre29">
<p class="calibre26">7</p>
</td><td class="calibre29">
<p class="calibre26">0.4</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">5</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td></tr><tr class="calibre28"><td class="calibre29">
<p class="calibre26">120,000</p>
</td><td class="calibre29">
<p class="calibre26">2.6</p>
</td><td class="calibre29">
<p class="calibre26">4</p>
</td><td class="calibre29">
<p class="calibre26">12,000</p>
</td><td class="calibre29">
<p class="calibre26">0.3</p>
</td><td class="calibre29">
<p class="calibre26">3</p>
</td><td class="calibre29">
<p class="calibre26">37</p>
</td><td class="calibre29">
<p class="calibre26">0.6</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">7</p>
</td></tr></tbody></table></div><p class="calibre11">Having<a id="id159" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> structured the dataset, we define a neural network containing<a id="id160" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the exact number of features (multiplied by two, provided two cities) plus the route features in the input, one output, and an arbitrary number of neurons in the hidden layer. In the case presented in the preceding table, there would be nine inputs. Since the output is numerical, there is no need to convert output data.</p><p class="calibre11">This neural network would give an estimate price for a route between two cities, which currently is not served by any bus transportation company.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="A basic neural architecture – perceptrons"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec26" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>A basic neural architecture – perceptrons</h1></div></div></div><p class="calibre11">Perceptron is<a id="id161" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the most simple neural network <a id="id162" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>architecture. Projected by <span class="strong1"><em class="calibre16">Frank Rosenblatt</em></span> in 1957, it has just one layer of neurons, receiving a set of inputs and producing another set of outputs. This<a id="id163" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> was one of the first representations <a id="id164" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of neural networks to gain attention, especially because of their simplicity:</p><div class="mediaobject"><img src="Images/B05964_03_03.jpg" alt="A basic neural architecture – perceptrons" class="calibre85"/></div><p class="calibre11">In our Java implementation, this is illustrated with one neural layer (the output layer). The following code creates a perceptron with three inputs and two outputs, having the linear function at the output layer:</p><div class="calibre2"><pre class="programlisting">int numberOfInputs=3;
int numberOfOutputs=2;

Linear outputAcFnc = new Linear(1.0);
NeuralNet perceptron = new NeuralNet(numberOfInputs,numberOfOutputs,
                outputAcFnc);</pre></div><div class="calibre2" title="Applications and limitations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec35" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Applications and limitations</h2></div></div></div><p class="calibre11">However, scientists <a id="id165" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>did not take long to conclude that a perceptron neural <a id="id166" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>network could only be applied to simple tasks, according to that simplicity. At that time, neural networks were being used for simple classification problems, but perceptrons usually failed when faced with more complex datasets. Let's illustrate this with a very basic example (an <code class="literal">AND</code> function) to understand better this issue.</p></div><div class="calibre2" title="Linear separation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec36" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Linear separation</h2></div></div></div><p class="calibre11">The example <a id="id167" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>consists of an AND function that takes two <a id="id168" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>inputs, <span class="strong1"><strong class="calibre12">x1</strong></span> and <span class="strong1"><strong class="calibre12">x2</strong></span>. That function can be plotted in a two-dimensional chart as follows:</p><div class="mediaobject"><img src="Images/B05964_03_04.jpg" alt="Linear separation" class="calibre86"/></div><p class="calibre11">And now let's examine how the neural network evolves the training using the perceptron rule, considering a pair of two weights, <span class="strong1"><strong class="calibre12">w1</strong></span> and <span class="strong1"><strong class="calibre12">w2</strong></span>, initially <span class="strong1"><strong class="calibre12">0.5</strong></span>, and bias valued <span class="strong1"><strong class="calibre12">0.5</strong></span> as well. Assume learning rate η equals <span class="strong1"><strong class="calibre12">0.2</strong></span>:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Epoch</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">x1</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">x2</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">w1</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">w2</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">b</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">y</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">t</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">E</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Δw1</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Δw2</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Δb</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.5</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.1</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.4</p>
</td><td class="calibre29">
<p class="calibre26">0.9</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.9</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.18</p>
</td><td class="calibre29">
<p class="calibre26">-0.18</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.32</p>
</td><td class="calibre29">
<p class="calibre26">0.22</p>
</td><td class="calibre29">
<p class="calibre26">0.72</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.72</p>
</td><td class="calibre29">
<p class="calibre26">-0.144</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.144</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.356</p>
</td><td class="calibre29">
<p class="calibre26">0.32</p>
</td><td class="calibre29">
<p class="calibre26">0.076</p>
</td><td class="calibre29">
<p class="calibre26">0.752</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.248</p>
</td><td class="calibre29">
<p class="calibre26">0.0496</p>
</td><td class="calibre29">
<p class="calibre26">0.0496</p>
</td><td class="calibre29">
<p class="calibre26">0.0496</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.406</p>
</td><td class="calibre29">
<p class="calibre26">0.370</p>
</td><td class="calibre29">
<p class="calibre26">0.126</p>
</td><td class="calibre29">
<p class="calibre26">0.126</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.126</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">-0.025</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.406</p>
</td><td class="calibre29">
<p class="calibre26">0.370</p>
</td><td class="calibre29">
<p class="calibre26">0.100</p>
</td><td class="calibre29">
<p class="calibre26">0.470</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.470</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">-0.094</p>
</td><td class="calibre29">
<p class="calibre26">-0.094</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.406</p>
</td><td class="calibre29">
<p class="calibre26">0.276</p>
</td><td class="calibre29">
<p class="calibre26">0.006</p>
</td><td class="calibre29">
<p class="calibre26">0.412</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.412</p>
</td><td class="calibre29">
<p class="calibre26">-0.082</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">-0.082</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.323</p>
</td><td class="calibre29">
<p class="calibre26">0.276</p>
</td><td class="calibre29">
<p class="calibre26">-0.076</p>
</td><td class="calibre29">
<p class="calibre26">0.523</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.477</p>
</td><td class="calibre29">
<p class="calibre26">0.095</p>
</td><td class="calibre29">
<p class="calibre26">0.095</p>
</td><td class="calibre29">
<p class="calibre26">0.095</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">89</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.562</p>
</td><td class="calibre29">
<p class="calibre26">-0.312</p>
</td><td class="calibre29">
<p class="calibre26">-0.312</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.312</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.062</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">89</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.562</p>
</td><td class="calibre29">
<p class="calibre26">-0.25</p>
</td><td class="calibre29">
<p class="calibre26">0.313</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.313</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.063</p>
</td><td class="calibre29">
<p class="calibre26">-0.063</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">89</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.500</p>
</td><td class="calibre29">
<p class="calibre26">-0.312</p>
</td><td class="calibre29">
<p class="calibre26">0.313</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.313</p>
</td><td class="calibre29">
<p class="calibre26">-0.063</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.063</p>
</td></tr><tr class="calibre28"><td class="calibre29">
<p class="calibre26">89</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.562</p>
</td><td class="calibre29">
<p class="calibre26">0.500</p>
</td><td class="calibre29">
<p class="calibre26">-0.375</p>
</td><td class="calibre29">
<p class="calibre26">0.687</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.313</p>
</td><td class="calibre29">
<p class="calibre26">0.063</p>
</td><td class="calibre29">
<p class="calibre26">0.063</p>
</td><td class="calibre29">
<p class="calibre26">0.063</p>
</td></tr></tbody></table></div><p class="calibre11">After <span class="strong1"><strong class="calibre12">89</strong></span> epochs, we find the network to produce values near to the desired output. Since in this <a id="id169" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>example the outputs are binary (zero or one), we can assume <a id="id170" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that any value produced by the network that is below <span class="strong1"><strong class="calibre12">0.5</strong></span> is considered to be <span class="strong1"><strong class="calibre12">0</strong></span> and any value above <span class="strong1"><strong class="calibre12">0.5</strong></span> is considered to be <span class="strong1"><strong class="calibre12">1</strong></span>. So, we can draw a function <span class="inlinemediaobject"><img src="Images/B05964_03_05_01.jpg" alt="Linear separation" class="calibre87"/></span>, with the final weights and bias found by the learning algorithm <span class="strong1"><em class="calibre16">w1=0.562</em></span>, <span class="strong1"><em class="calibre16">w2=0.5</em></span> and <span class="strong1"><em class="calibre16">b=-0.375</em></span>, defining the linear boundary in the chart:</p><div class="mediaobject"><img src="Images/B05964_03_05.jpg" alt="Linear separation" class="calibre88"/></div><p class="calibre11">This boundary is a definition of all classifications given by the network. You can see that the boundary is linear, given that the function is also linear. Thus, the perceptron network is really<a id="id171" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> suitable<a id="id172" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> for problems whose patterns are linearly separable.</p></div><div class="calibre2" title="The XOR case"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec37" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The XOR case</h2></div></div></div><p class="calibre11">Now let's<a id="id173" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> analyze <a id="id174" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <code class="literal">XOR</code> case:</p><div class="mediaobject"><img src="Images/B05964_03_06.jpg" alt="The XOR case" class="calibre89"/></div><p class="calibre11">We see that in two dimensions, it is impossible to draw a line to separate the two patterns. What would happen if we tried to train a single layer perceptron to learn this function? Suppose we tried, let's see what happened in the following table:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Epoch</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">x1</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">x2</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">w1</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">w2</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">b</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">y</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">t</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">E</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Δw1</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Δw2</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Δb</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.5</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.1</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.4</p>
</td><td class="calibre29">
<p class="calibre26">0.9</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.02</p>
</td><td class="calibre29">
<p class="calibre26">0.02</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.5</p>
</td><td class="calibre29">
<p class="calibre26">0.52</p>
</td><td class="calibre29">
<p class="calibre26">0.42</p>
</td><td class="calibre29">
<p class="calibre26">0.92</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.08</p>
</td><td class="calibre29">
<p class="calibre26">0.016</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.016</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.516</p>
</td><td class="calibre29">
<p class="calibre26">0.52</p>
</td><td class="calibre29">
<p class="calibre26">0.436</p>
</td><td class="calibre29">
<p class="calibre26">1.472</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-1.472</p>
</td><td class="calibre29">
<p class="calibre26">-0.294</p>
</td><td class="calibre29">
<p class="calibre26">-0.294</p>
</td><td class="calibre29">
<p class="calibre26">-0.294</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.222</p>
</td><td class="calibre29">
<p class="calibre26">0.226</p>
</td><td class="calibre29">
<p class="calibre26">0.142</p>
</td><td class="calibre29">
<p class="calibre26">0.142</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.142</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">-0.028</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.222</p>
</td><td class="calibre29">
<p class="calibre26">0.226</p>
</td><td class="calibre29">
<p class="calibre26">0.113</p>
</td><td class="calibre29">
<p class="calibre26">0.339</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.661</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.132</p>
</td><td class="calibre29">
<p class="calibre26">0.132</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.222</p>
</td><td class="calibre29">
<p class="calibre26">0.358</p>
</td><td class="calibre29">
<p class="calibre26">0.246</p>
</td><td class="calibre29">
<p class="calibre26">0.467</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.533</p>
</td><td class="calibre29">
<p class="calibre26">0.107</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.107</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.328</p>
</td><td class="calibre29">
<p class="calibre26">0.358</p>
</td><td class="calibre29">
<p class="calibre26">0.352</p>
</td><td class="calibre29">
<p class="calibre26">1.038</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-1.038</p>
</td><td class="calibre29">
<p class="calibre26">-0.208</p>
</td><td class="calibre29">
<p class="calibre26">-0.208</p>
</td><td class="calibre29">
<p class="calibre26">-0.208</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29">
<p class="calibre26">…</p>
</td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td><td class="calibre29"> </td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">127</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.250</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">127</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">-0.250</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td><td class="calibre29">
<p class="calibre26">0.500</p>
</td><td class="calibre29">
<p class="calibre26">0.375</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.125</p>
</td><td class="calibre29">
<p class="calibre26">0.125</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">127</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.250</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.375</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0.125</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.125</p>
</td></tr><tr class="calibre28"><td class="calibre29">
<p class="calibre26">127</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td><td class="calibre29">
<p class="calibre26">0.000</p>
</td><td class="calibre29">
<p class="calibre26">0.750</p>
</td><td class="calibre29">
<p class="calibre26">0.625</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">-0.625</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td><td class="calibre29">
<p class="calibre26">-0.125</p>
</td></tr></tbody></table></div><p class="calibre11">The perceptron <a id="id175" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>just could not find any pair of weights that would drive the <a id="id176" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>following error 0.625. This can be explained mathematically as we already perceived from the chart that this function cannot be linearly separable in two dimensions. So what if we add another dimension? Let's see the chart in three dimensions:</p><div class="mediaobject"><img src="Images/B05964_03_07.jpg" alt="The XOR case" class="calibre90"/></div><p class="calibre11">In three dimensions, it is possible to draw a plane that would separate the patterns, provided that this additional dimension could properly transform the input data. Okay, but now there is an additional problem: how could we derive this additional dimension since we have only two input variables? One obvious, but also workaround, answer would be adding a third variable as a derivation from the two original ones. And being this third variable a (derivation), our neural network would probably get the following shape:</p><div class="mediaobject"><img src="Images/B05964_03_08.jpg" alt="The XOR case" class="calibre91"/></div><p class="calibre11">Okay, now<a id="id177" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the perceptron has three inputs, one of them being a composition <a id="id178" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the other. This also leads to a new question: how should that composition be processed? We can see that this component could act as a neuron, so giving the neural network a nested architecture. If so, there would another new question: how would the weights of this new neuron be trained, since the error is on the output neuron?</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Multi-layer perceptrons"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec27" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Multi-layer perceptrons</h1></div></div></div><p class="calibre11">As we can<a id="id179" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> see, one simple example in which the patterns are not linearly separable has led us to more and more issue using the perceptron architecture. That need led to the application of multilayer perceptrons. In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Getting Started with Neural Networks">Chapter 1</a>, <span class="strong1"><em class="calibre16">Getting Started with Neural Networks</em></span> we dealt with the fact that the natural neural network is structured in layers as well, and each layer captures pieces of information from a specific environment. In artificial neural networks, layers of neurons act in this way, by extracting and abstracting information from data, transforming them into another dimension or shape.</p><p class="calibre11">In the XOR example, we found the solution to be the addition of a third component that would make possible a linear separation. But there remained a few questions regarding how that third component would be computed. Now let's consider the same solution as a two-layer perceptron:</p><div class="mediaobject"><img src="Images/B05964_03_09.jpg" alt="Multi-layer perceptrons" class="calibre92"/></div><p class="calibre11">Now we have<a id="id180" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> three neurons instead of just one, but in the output the information transferred by the previous layer is transformed into another dimension or shape, whereby it would be theoretically possible to establish a linear boundary on those data points. However, the question on finding the weights for the first layer remains unanswered, or can we apply the same training rule to neurons other than the output? We are going to deal with this issue in the Generalized delta rule section.</p><div class="calibre2" title="MLP properties"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec38" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>MLP properties</h2></div></div></div><p class="calibre11">Multi-layer perceptrons <a id="id181" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can have any number of layers and also any number of neurons in each layer. The activation functions may be different on any layer. An MLP network is usually composed of at least two layers, one for the output and one hidden layer.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip08" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">There are also some references that consider the input layer as the nodes that collect input data; therefore, for those cases, the MLP is considered to have at least three layers. For the purpose of this book, let's consider the input layer as a special type of layer which has no weights, and as the effective layers, that is, those enabled to be trained, we'll consider the hidden and output layers.</p></div></div><p class="calibre11">A hidden layer is called that because it actually <code class="literal">hides</code> its outputs from the external world. Hidden layers can be connected in series in any number, thus forming a deep neural network. However, the more layers a neural network has, the slower would be both training and running, and according to mathematical foundations, a neural network with one or two hidden layers at most may learn as well as deep neural networks with dozens of hidden layers. But it depends on several factors.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="tip09" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre17">It is really <a id="id182" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>recommended for the activation functions to be nonlinear in the hidden layers, especially if in the output layer the activation function is linear. According to linear algebra, having a linear activation function in all layers is equivalent to having only one output layer, provided that the additional variables introduced by the layers would be mere linear combinations of the previous ones or the inputs. Usually, activation functions such as hyperbolic tangent or sigmoid are used, because they are derivable.</p></div></div></div><div class="calibre2" title="MLP weights"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec39" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>MLP weights</h2></div></div></div><p class="calibre11">In an MLP<a id="id183" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> feedforward network, one particular neuron <span class="strong1"><em class="calibre16">i</em></span> receives data from a neuron j of the previous layer and forwards its output to a neuron k of the next layer:</p><div class="mediaobject"><img src="Images/B05964_03_10.jpg" alt="MLP weights" class="calibre93"/></div><p class="calibre11">The mathematical description of a neural network is recursive:</p><div class="mediaobject"><img src="Images/B05964_03_21_Replace.jpg" alt="MLP weights" class="calibre94"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre16">yo</em></span> is the<a id="id184" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> network output (should we have multiple outputs, we can replace <span class="strong1"><em class="calibre16">yo</em></span> with Y, representing a vector); <span class="strong1"><em class="calibre16">fo</em></span> is the activation function of the output; <span class="strong1"><em class="calibre16">l</em></span> is the number of hidden layers; <span class="strong1"><em class="calibre16">nhi</em></span> is the number of neurons in the hidden layer <span class="strong1"><em class="calibre16">i</em></span>; <span class="strong1"><em class="calibre16">wi</em></span> is the weight connecting the <span class="strong1"><em class="calibre16">i</em></span> th neuron of the last hidden layer to the output; <span class="strong1"><em class="calibre16">fi</em></span> is the activation function of the neuron <span class="strong1"><em class="calibre16">i</em></span>; and <span class="strong1"><em class="calibre16">bi</em></span> is the bias of the neuron <span class="strong1"><em class="calibre16">i</em></span>. It can be seen<a id="id185" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that this equation gets larger as the number of layers increases. In the last summing operation, there will be the inputs <span class="strong1"><em class="calibre16">xi</em></span>.</p></div><div class="calibre2" title="Recurrent MLP"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec40" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Recurrent MLP</h2></div></div></div><p class="calibre11">The neurons <a id="id186" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>on an MLP may feed signals not only to <a id="id187" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neurons in the next layers (feedforward network), but also to neurons in the same or previous layers (feedback or recurrent). This behavior allows the neural network to maintain state on some data sequence, and this feature is especially exploited when dealing with time series or handwriting recognition. Recurrent networks are usually harder to train, and eventually the computer may run out of memory while executing them. In addition, there are recurrent network architectures better than MLPs, such as Elman, Hopfield, Echo state, Bidirectional RNNs (recurrent neural networks). But we are not going to dive deep into these architectures, because this book focuses on the simplest applications for those who have minimal experience in programming. However, we recommend good literature on recurrent networks for those who are interested in it.</p></div><div class="calibre2" title="Coding an MLP"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec41" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Coding an MLP</h2></div></div></div><p class="calibre11">Bringing these<a id="id188" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> concepts into the OOP point of view, we can review the classes already designed so far:</p><div class="mediaobject"><img src="Images/B05964_03_11.jpg" alt="Coding an MLP" class="calibre95"/></div><p class="calibre11">One can see<a id="id189" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that the neural network structure is hierarchical. A neural network is composed of layers that are composed of neurons. In the MLP architecture, there are three types of layers: input, hidden, and output. So suppose that in Java, we would like to define a neural network consisting of three inputs, one output (linear activation function) and one hidden layer (<code class="literal">sigmoid</code> function) containing five neurons. The resulting code would be as follows:</p><div class="calibre2"><pre class="programlisting">int numberOfInputs=3;
int numberOfOutputs=1;
int[] numberOfHiddenNeurons={5};
        
Linear outputAcFnc = new Linear(1.0);
Sigmoid hiddenAcFnc = new Sigmoid(1.0);
NeuralNet neuralnet = new NeuralNet(numberOfInputs, numberOfOutputs, numberOfHiddenNeurons, hiddenAcFnc, outputAcFnc);</pre></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Learning in MLPs"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec28" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning in MLPs</h1></div></div></div><p class="calibre11">The multi-layer perceptron network learns based on the Delta Rule, which is also inspired by the<a id="id190" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> gradient descent optimization method. The gradient method is broadly applied to find minima or maxima of a given function:</p><div class="mediaobject"><img src="Images/B05964_03_12.jpg" alt="Learning in MLPs" class="calibre96"/></div><p class="calibre11">This method is applied at <span class="strong1"><em class="calibre16">walking</em></span> the direction where the function's output is higher or lower, depending on the criteria. This concept is explored in the Delta Rule:</p><div class="mediaobject"><img src="Images/B05964_03_12_01.jpg" alt="Learning in MLPs" class="calibre97"/></div><p class="calibre11">The function that the Delta Rule wants to minimize is the error between the neural network output<a id="id191" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> and the target output, and the parameters to be found are the neural weights. This is an enhanced learning algorithm compared to the perceptron rule, because it takes into account the activation function derivative <span class="strong1"><em class="calibre16">g'(h)</em></span>, which in mathematical terms indicates the direction where the function is decreasing most.</p><div class="calibre2" title="Backpropagation algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec42" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Backpropagation algorithm</h2></div></div></div><p class="calibre11">Although the Delta Rule works well for the neural networks having only output and input layers, for the MLP networks, the pure Delta Rule cannot be applied because of the hidden layer neurons. To<a id="id192" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> overcome this <a id="id193" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>issue, in the 1980s, <span class="strong1"><em class="calibre16">Rummelhart</em></span> and others proposed a new algorithm, also inspired by the gradient method, called backpropagation.</p><p class="calibre11">This algorithm is indeed a generalization of the Delta Rule for MLPs. The benefits of having additional layers to abstract more data from the environment have motivated the development of a training algorithm that could properly adjust the weights of the hidden layer. Based on the gradient method, the error from output would be (back) propagated to the previous layers, so making possible the weight update using the same equation as the Delta Rule.</p><p class="calibre11">The algorithm runs as follows:</p><div class="mediaobject"><img src="Images/B05964_03_13.jpg" alt="Backpropagation algorithm" class="calibre98"/></div><p class="calibre11">The second step is the backpropagation itself. What it does is to find the weight variation according to the gradient, which is the base for the Delta Rule:</p><div class="mediaobject"><img src="Images/B05964_03_13_01.jpg" alt="Backpropagation algorithm" class="calibre99"/></div><p class="calibre11">Here, E is<a id="id194" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the error, <span class="strong1"><em class="calibre16">wji</em></span> is the weight<a id="id195" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> between the neurons <span class="strong1"><em class="calibre16">i</em></span> and <span class="strong1"><em class="calibre16">j</em></span>, <span class="strong1"><em class="calibre16">oi</em></span> is the output of the <span class="strong1"><em class="calibre16">ith</em></span> neuron, and <span class="strong1"><em class="calibre16">hi</em></span> is the weighted sum of that neuron's inputs before passing to activation function. Remember that <span class="strong1"><em class="calibre16">oi=f(hi)</em></span>, <span class="strong1"><em class="calibre16">f</em></span> being the activation function.</p><p class="calibre11">For updating in the hidden layers, it is a bit more complicated as we consider the error as function of all neurons between the weight to be updated and the output. To facilitate this process, we should compute the sensibility or backpropagation error:</p><div class="mediaobject"><img src="Images/B05964_03_14_01.jpg" alt="Backpropagation algorithm" class="calibre100"/></div><p class="calibre11">And the weight update is as follows:</p><div class="mediaobject"><img src="Images/B05964_03_14_02.jpg" alt="Backpropagation algorithm" class="calibre101"/></div><p class="calibre11">The <a id="id196" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculation of the backpropagation <a id="id197" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>error varies for the output and for the hidden layers:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Backpropagation for the output layer:<div class="mediaobject1"><img src="Images/B05964_03_14_03.jpg" alt="Backpropagation algorithm" class="calibre102"/></div><div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Here, oi is the <span class="strong1"><em class="calibre16">ith</em></span> output, <span class="strong1"><em class="calibre16">ti</em></span> is the desired <span class="strong1"><em class="calibre16">ith</em></span> output, <span class="strong1"><em class="calibre16">f'(hi)</em></span> is the derivative of the output activation function, and <span class="strong1"><em class="calibre16">hi</em></span> is the weighted sum of the <span class="strong1"><em class="calibre16">ith</em></span> neuron inputs</li></ul></div></li><li class="listitem">Backpropagation for the hidden layer:<div class="mediaobject1"><img src="Images/B05964_03_14_04.jpg" alt="Backpropagation algorithm" class="calibre103"/></div><div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Here, <span class="strong1"><em class="calibre16">l</em></span> is a neuron of the layer ahead and <span class="strong1"><em class="calibre16">wil</em></span> is the weight that connects the current neuron to the lth neuron of the layer immediately ahead.</li></ul></div></li></ul></div><p class="calibre11">For simplicity reasons, we did not demonstrate fully how the backpropagation equation was developed. Anyway, if you are interested in the details, you may consult the book neural <a id="id198" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>networks – a comprehensive<a id="id199" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> foundation by Simon Haykin.</p></div><div class="calibre2" title="The momentum"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec43" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The momentum</h2></div></div></div><p class="calibre11">Like any<a id="id200" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> gradient-based method, there is a risk of falling <a id="id201" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>into a local minimum. To mitigate this risk, we can add another term to the weight update rule called momentum, which takes into consideration the last variation of weight:</p><div class="mediaobject"><img src="Images/B05964_03_14_05.jpg" alt="The momentum" class="calibre104"/></div><p class="calibre11">Here, μ is a momentum rate and <span class="inlinemediaobject"><img src="Images/B05964_03_14_06.jpg" alt="The momentum" class="calibre105"/></span> is the last delta weight. This gives an additional step to the update, therefore attenuating the oscillations in the error hyperspace.</p></div><div class="calibre2" title="Coding the backpropagation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec44" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Coding the backpropagation</h2></div></div></div><p class="calibre11">Let's define the class backpropagation in the package <code class="literal">edu.packt.neural.learn</code>. Since this learning <a id="id202" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithm is a <a id="id203" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>generalization of the <code class="literal">DeltaRule</code>, this class may inherit and override the features already defined in Delta Rule. Three additional attributes included in this class are the momentum rate, the delta neuron, and the last delta weight arrays:</p><div class="calibre2"><pre class="programlisting">public class Backpropagation extends DeltaRule {
    private double MomentumRate=0.7;
    public ArrayList&lt;ArrayList&lt;Double&gt;&gt; deltaNeuron;
    public ArrayList&lt;ArrayList&lt;ArrayList&lt;Double&gt;&gt;&gt; lastDeltaWeights;
  …
}</pre></div><p class="calibre11">The constructor will have the same arguments as for the <code class="literal">DeltaRule</code> class, adding the calls to methods for initialization of the <code class="literal">deltaNeuron</code> and <code class="literal">lastDeltaWeights</code> arrays:</p><div class="calibre2"><pre class="programlisting">public Backpropagation(NeuralNet _neuralNet, NeuralDataSet _trainDataSet, DeltaRule.LearningMode _learningMode){
    super(_neuralNet,_trainDataSet,_learningMode);
    initializeDeltaNeuron();
    initializeLastDeltaWeights();
}</pre></div><p class="calibre11">The <code class="literal">train()</code> method will work in a similar way as in the <code class="literal">DeltaRule</code> class; the additional component is the backward step, whereby the error is <code class="literal">backpropagated</code> throughout the neural <a id="id204" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>layers up to the<a id="id205" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> input:</p><div class="calibre2"><pre class="programlisting">@Override
public void train() throws NeuralException{
    neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.TRAINING);
    epoch=0; // initialization of epoch
    int k=0; // first training record
    currentRecord=0; // this attribute keeps track of which record
                     // is currently under processing in the training
    forward();  // initial forward step to determine the error
    forward(k); // forward for backpropagation of first record error
    while(epoch&lt;MaxEpochs &amp;&amp; overallGeneralError&gt;MinOverallError){
        backward(); // backward step
        switch(learningMode){
            case BATCH:
                if(k==trainingDataSet.numberOfRecords-1)
                    applyNewWeights(); // batch update
                break;
            case ONLINE:
                applyNewWeights(); //online update
        }
        currentRecord=++k; // moving on to the next record
        if(k&gt;=trainingDataSet.numberOfRecords){ //if it was the last
            k=0;  
            currentRecord=0; // reset to the first
            epoch++;         // and increase the epoch
        }
        forward(k); // forward the next record
    }
    neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.RUN);
}</pre></div><p class="calibre11">The role of the backward step is to determine the delta weights by means of error backpropagation, from the output layer down to the first hidden layer:</p><div class="calibre2"><pre class="programlisting">public void backward(){
  int numberOfLayers=neuralNet.getNumberOfHiddenLayers();
  for(int l=numberOfLayers;l&gt;=0;l--){
    int numberOfNeuronsInLayer=deltaNeuron.get(l).size();
    for(int j=0;j&lt;numberOfNeuronsInLayer;j++){
      for(int i=0;i&lt;newWeights.get(l).get(j).size();i++){
 
 // get the current weight of the neuron
        double currNewWeight = this.newWeights.get(l).get(j).get(i);
       //if it is the first epoch, get directly from the neuron
        if(currNewWeight==0.0 &amp;&amp; epoch==0.0)
          if(l==numberOfLayers)
            currNewWeight=neuralNet.getOutputLayer().getWeight(i, j);
          else
            currNewWeight=neuralNet.getHiddenLayer(l).
                               getWeight(i, j);
       // calculate the delta weight
        double deltaWeight=calcDeltaWeight(l, i, j);
       // store the new calculated weight for subsequent update
        newWeights.get(l).get(j).set(i,currNewWeight+deltaWeight);
      }
    }
  }
}</pre></div><p class="calibre11">The<a id="id206" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> backpropagation step is <a id="id207" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>performed in the method <code class="literal">calcDeltaWeight()</code>. The momentum will be added only before updating the weights because it should recall the last delta weight determined:</p><div class="calibre2"><pre class="programlisting">public Double calcDeltaWeight(int layer,int input,int neuron) {
  Double deltaWeight=1.0;
  NeuralLayer currLayer;
  Neuron currNeuron;
  double _deltaNeuron;
  if(layer==neuralNet.getNumberOfHiddenLayers()){ //output layer
    currLayer=neuralNet.getOutputLayer();
    currNeuron=currLayer.getNeuron(neuron);
    _deltaNeuron=error.get(currentRecord).get(neuron)
                   *currNeuron.derivative(currLayer.getInputs());
  }
  else{ //hidden layer
    currLayer=neuralNet.getHiddenLayer(layer);
    currNeuron=currLayer.getNeuron(neuron);
    double sumDeltaNextLayer=0;
    NeuralLayer nextLayer=currLayer.getNextLayer();
    for(int k=0;k&lt;nextLayer.getNumberOfNeuronsInLayer();k++){
      sumDeltaNextLayer+=nextLayer.getWeight(neuron, k)
                           *deltaNeuron.get(layer+1).get(k);
    }
    _deltaNeuron=sumDeltaNextLayer*
                      currNeuron.derivative(currLayer.getInputs());
            
  }
        
  deltaNeuron.get(layer).set(neuron, _deltaNeuron);
  deltaWeight*=_deltaNeuron;
  if(input&lt;currNeuron.getNumberOfInputs()){
            deltaWeight*=currNeuron.getInput(input);
  }
        
  return deltaWeight;
}</pre></div><p class="calibre11">Note the <a id="id208" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculation of the <code class="literal">_deltaNeuron</code> is different for the output and the hidden layers, but for both of them the<a id="id209" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> derivative is used. To facilitate this task, we've added the <code class="literal">derivative()</code> method to the class <code class="literal">Neuron</code>. Details can be found in <span class="strong1"><em class="calibre16">Annex III</em></span> documentation. At the end, the input corresponding to the weight is multiplied to the delta weight calculated.</p><p class="calibre11">The weight update is performed by the method <code class="literal">applyNewWeights()</code>. To save space, we are not going to write here the whole method body, but only the core where the weight update is performed:</p><div class="calibre2"><pre class="programlisting">HiddenLayer hl = this.neuralNet.getHiddenLayer(l);
  Double lastDeltaWeight=lastDeltaWeights.get(l).get(j).get(i);
  // determine the momentum
  double momentum=MomentumRate*lastDeltaWeight;
  //the momentum is then added to the new weight
  double newWeight=this.newWeights.get(l).get(j).get(i)
                              -momentum;
  this.newWeights.get(l).get(j).set(i,newWeight);
  Neuron n=hl.getNeuron(j);
  // save the current delta weight for the next step
  double deltaWeight=(newWeight-n.getWeight(i));
  lastDeltaWeights.get(l).get(j).set(i,(double)deltaWeight);
  // finally the weight is updated
  hl.getNeuron(j).updateWeight(i, newWeight);</pre></div><p class="calibre11">In the code listing, <code class="literal">l</code> represents the layer, <code class="literal">j</code> the neuron, and <code class="literal">i</code> the input to the weight. For the output layer, l will be equal to the number of hidden layers (exceeding the Java array limits), so the <code class="literal">NeuralLayer</code> called is as follows:</p><div class="calibre2"><pre class="programlisting">OutputLayer ol = this.neuralNet.getOutputLayer();
  Neuron n=ol.getNeuron(j);
  ol.getNeuron(j).updateWeight(i, newWeight);</pre></div><p class="calibre11">This class can be used exactly the same way as <code class="literal">DeltaRule</code>:</p><div class="calibre2"><pre class="programlisting">int numberOfInputs=2;
int numberOfOutputs=1;
        
int[] numberOfHiddenNeurons={2};
    
Linear outputAcFnc = new Linear(1.0);
Sigmoid hdAcFnc = new Sigmoid(1.0);
IActivationFunction[] hiddenAcFnc={hdAcFnc };

NeuralNet mlp = new NeuralNet(numberOfInputs,numberOfOutputs
                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);
        
Backpropagation backprop = new Backpropagation(mlp,neuralDataSet
                ,LearningAlgorithm.LearningMode.ONLINE);</pre></div><p class="calibre11">At the<a id="id210" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> end of this chapter, we <a id="id211" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will make a comparison of the Delta Rule for the perceptrons with the backpropagation with a multi-layer perceptron, trying to solve the XOR problem.</p></div><div class="calibre2" title="Levenberg-Marquardt algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec45" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Levenberg-Marquardt algorithm</h2></div></div></div><p class="calibre11">The<a id="id212" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> backpropagation algorithm, like<a id="id213" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> all gradient-based methods, usually presents slow convergence, especially when it falls in a zig-zag situation, when the weights are changed to almost the same value every two iterations. This drawback was studied in problems such as curve-fitting interpolation by Kenneth Levenberg in 1944, and later by Donald Marquart in 1963, who developed a method for finding coefficients based on the Gauss Newton algorithm and the gradient descent, so therefrom comes the name of the algorithm.</p><p class="calibre11">The LM algorithm deals with some optimization terms which are beyond the scope of this book, but in the references section, the reader will find good resources to learn more about these concepts, so we will present the method in a simpler way. Let's suppose we have a list of inputs x and outputs <span class="strong1"><em class="calibre16">t</em></span>:</p><div class="mediaobject"><img src="Images/B05964_03_14_07.jpg" alt="Levenberg-Marquardt algorithm" class="calibre106"/></div><p class="calibre11">We have seen that a neural network has the property to map inputs to outputs just like a nonlinear function f with coefficients <span class="strong1"><em class="calibre16">W</em></span> (weights and bias):</p><div class="mediaobject"><img src="Images/B05964_03_14_08.jpg" alt="Levenberg-Marquardt algorithm" class="calibre107"/></div><p class="calibre11">The<a id="id214" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> nonlinear function will produce<a id="id215" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> values different than the outputs T; that's because we marked the variable Y in the equation. The Levenberg-Marquardt algorithm works over a Jacobian matrix, which is a matrix of all partial derivatives in regard to each weight and bias for each data row. So the Jacobian matrix has the following format:</p><div class="mediaobject"><img src="Images/B05964_03_14_09.jpg" alt="Levenberg-Marquardt algorithm" class="calibre108"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre16">k</em></span> is the total number of data points and p is the total number of weights and bias. In the Jacobian matrix, all weights and bias are stored serially in a single row. The elements of the Jacobian Matrix are calculated from the gradients:</p><div class="mediaobject"><img src="Images/B05964_03_14_10.jpg" alt="Levenberg-Marquardt algorithm" class="calibre109"/></div><p class="calibre11">The partial derivative of the error E in relation to each weight is calculated in the backpropagation algorithm, so this algorithm is going to run the backpropagation step as well.</p><p class="calibre11">In every optimization problem, one wishes to minimize the total error:</p><div class="mediaobject"><img src="Images/B05964_03_14_11.jpg" alt="Levenberg-Marquardt algorithm" class="calibre110"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre16">W</em></span> (weights and bias in the NN case) are the variables to optimize. The optimization algorithm <a id="id216" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>updates W by adding ΔW. By <a id="id217" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>applying some algebra, the last equation can be extended to this one:</p><div class="mediaobject"><img src="Images/B05964_03_14_12.jpg" alt="Levenberg-Marquardt algorithm" class="calibre111"/></div><p class="calibre11">Converting to the vector and notation:</p><div class="mediaobject"><img src="Images/B05964_03_14_13.jpg" alt="Levenberg-Marquardt algorithm" class="calibre112"/></div><p class="calibre11">Finally, by setting the error E to zero, we get the Levenberg-Marquardt equation after some manipulation:</p><div class="mediaobject"><img src="Images/B05964_03_14_14.jpg" alt="Levenberg-Marquardt algorithm" class="calibre113"/></div><p class="calibre11">This is the weight update rule. As can be seen, it involves matrix operations such as transposition and inversion. The Greek letter λ is the damping factor, an equivalent for the learning rate.</p></div><div class="calibre2" title="Coding the Levenberg-Marquardt with matrix algebra"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec46" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Coding the Levenberg-Marquardt with matrix algebra</h2></div></div></div><p class="calibre11">In order to <a id="id218" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>effectively<a id="id219" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> implement the LM algorithm, it is very useful to work with matrix algebra. To address that, we defined a class called <code class="literal">Matrix</code> in the package <code class="literal">edu.packt.neuralnet.math</code>, including all the matrix operations, such as multiplication, inverse, and LU decomposition, among others. The reader<a id="id220" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> may refer to the<a id="id221" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> documentation to find out more about this class.</p><p class="calibre11">The Levenberg-Marquardt algorithm uses many features of the backpropagation algorithm; that's why we inherit this class from backpropagation. Some new attributes are included:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><strong class="calibre12">Jacobian matrix</strong></span>: This is<a id="id222" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the matrix containing the partial derivatives to each weight for all training records</li><li class="listitem"><span class="strong1"><strong class="calibre12">Damping factor</strong></span></li><li class="listitem"><span class="strong1"><strong class="calibre12">Error backpropagation</strong></span>: This array has the same function of <code class="literal">deltaNeuron</code>, but its <a id="id223" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculation differs a little to each neural output; that's why we defined it in a separate attribute</li><li class="listitem"><span class="strong1"><strong class="calibre12">Error LMA</strong></span>: The <a id="id224" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>error in the matrix form:<div class="calibre2"><pre class="programlisting1">public class LevenbergMarquardt extends Backpropagation {
    
    private Matrix jacobian = null;
    private double damping=0.1;
    
    private ArrayList&lt;ArrayList&lt;ArrayList&lt;Double&gt;&gt;&gt; errorBackpropagation;
    private Matrix errorLMA;
    
    public ArrayList&lt;ArrayList&lt;ArrayList&lt;Double&gt;&gt;&gt; lastWeights;
}</pre></div></li></ul></div><p class="calibre11">Basically, the train function is the same as that of the backpropagation, except for the following calculation of the Jacobian and error matrices and the damping update:</p><div class="calibre2"><pre class="programlisting">@Override
public void train() throws NeuralException{
  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.TRAINING);
  forward();
  double currentOverallError=overallGeneralError;
  buildErrorVector(); // copy the error values to the error matrix
  while(epoch&lt;MaxEpochs &amp;&amp; overallGeneralError&gt;MinOverallError
           &amp;&amp; damping&lt;=10000000000.0){ // to prevent the damping from growing up to infinity
    backward(); // to determine the error backpropgation
    calculateJacobian(); // copies the derivatives to the jacobian matrix
      applyNewWeights(); //update the weights
      forward(); //forward all records to evaluate new overall error
      if(overallGeneralError&lt;currentOverallError){
        if the new error is less than current
        damping/=10.0; // the damping factor reduces
        currentOverallError=overallGeneralError;
      }
      else{ // otherwise, the damping factor grows
        damping*=10.0;
        restoreLastWeights(); // the last weights are recovered
        forward();
      }
      buildErrorVector(); reevaluate the error matrix
  }
  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.RUN);

}</pre></div><p class="calibre11">The loop<a id="id225" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> where it goes over<a id="id226" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the training dataset calls the method <code class="literal">calculateJacobian</code>. This method works on the error backpropagation evaluated in the method backward:</p><div class="calibre2"><pre class="programlisting">double input;
if(p==numberOfInputs)
  input=1.0;
else
  input = n.getInput(p);
double deltaBackprop = errorBackpropagation.get(m).get(l).get(k);
jacobian.setValue(i, j++, deltaBackprop*input);</pre></div><p class="calibre11">In the code listing, <code class="literal">p</code> is the input connecting to the neuron (when it is equal to the number of neuron inputs, it represents the bias), <code class="literal">k</code> is the neuron, l is the layer, m is the neural output, <code class="literal">i</code> is a sequential index of the record, and <code class="literal">j</code> is the sequential index of the weight or bias, according to the layer and neuron in which it is located. Note that after setting the value in the Jacobian matrix, <code class="literal">j</code> is incremented.</p><p class="calibre11">The weight update is performed by means of determining the <code class="literal">deltaWeight</code> matrix:</p><div class="calibre2"><pre class="programlisting">Matrix jacob=jacobian.subMatrix(rowi, rowe, 0, numberOfWeights-1);
Matrix errorVec = errorLMA.subMatrix(rowi, rowe, 0, 0);
Matrix pseudoHessian=jacob.transpose().multiply(jacob);
Matrix miIdent = new IdentityMatrix(numberOfWeights)
.multiply(damping);
Matrix inverseHessianMi = pseudoHessian.add(miIdent).inverse();
Matrix deltaWeight = inverseHessianMi.multiply(jacob.transpose())
   .multiply(errorVec);</pre></div><p class="calibre11">The<a id="id227" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> previous code refers to the<a id="id228" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> matrix algebra shown in the section presenting the algorithm. The matrix <code class="literal">deltaWeight</code> contains the steps for each weight in the neural network. In the following code, <code class="literal">k</code> is the neuron, <code class="literal">j</code> is the input, and <code class="literal">l</code> is the layer:</p><div class="calibre2"><pre class="programlisting">Neuron n=nl.getNeuron(k);
double currWeight=n.getWeight(j);
double newWeight=currWeight+deltaWeight.getValue(i++,0);
newWeights.get(l).get(k).set(j,newWeight);
lastWeights.get(l).get(k).set(j,currWeight);
n.updateWeight(j, newWeight);</pre></div><p class="calibre11">Note that the weights are saved in the <code class="literal">lastWeights</code> array, so they can be recovered if the error gets worse.</p></div><div class="calibre2" title="Extreme learning machines"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch03lvl2sec47" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Extreme learning machines</h2></div></div></div><p class="calibre11">Taking advantage of the matrix algebra, <span class="strong1"><strong class="calibre12">extreme learning machines</strong></span> (<span class="strong1"><strong class="calibre12">ELMs</strong></span>) are able to converge<a id="id229" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learning very fast. This<a id="id230" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> learning algorithm has one limitation, since it is applied only on neural networks containing one single hidden layer. In practice, one hidden layer works pretty fine for most applications.</p><p class="calibre11">Representing the neural network in matrix algebra, for the following neural network:</p><div class="mediaobject"><img src="Images/B05964_03_14.jpg" alt="Extreme learning machines" class="calibre114"/></div><p class="calibre11">We have the corresponding equations:</p><div class="mediaobject"><img src="Images/B05964_03_15_01.jpg" alt="Extreme learning machines" class="calibre115"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre16">H</em></span> is the <a id="id231" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output of the hidden <a id="id232" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>layer, <span class="strong1"><em class="calibre16">g()</em></span> is the activation function of the hidden layer, <span class="strong1"><em class="calibre16">Xi</em></span> is the <span class="strong1"><em class="calibre16">ith</em></span> input record, <span class="strong1"><em class="calibre16">Wj</em></span> is the weight vector for the <span class="strong1"><em class="calibre16">jth</em></span> hidden neuron, <span class="strong1"><em class="calibre16">bj</em></span> is the bias of the <span class="strong1"><em class="calibre16">jth</em></span> hidden neuron, βp is the weight vector for the output <span class="strong1"><em class="calibre16">p</em></span>, and <span class="strong1"><em class="calibre16">Y</em></span> is the output generated by the neural network.</p><p class="calibre11">In the ELM algorithm, the hidden layer weights are generated randomly, while the output weights are adjusted according to a least squares approach:</p><div class="mediaobject"><img src="Images/B05964_03_15_02.jpg" alt="Extreme learning machines" class="calibre116"/></div><div class="mediaobject"><img src="Images/B05964_03_15_03.jpg" alt="Extreme learning machines" class="calibre117"/></div><p class="calibre11">Here, T is the target output training dataset.</p><p class="calibre11">This<a id="id233" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> algorithm is implemented in a <a id="id234" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>class called <code class="literal">ELM</code> in the same package as the other training algorithms. This class will inherit from <code class="literal">DeltaRule</code>, which has all the basic properties for supervised learning algorithms:</p><div class="calibre2"><pre class="programlisting">public class ELM extends DeltaRule {
    
  private Matrix H;
  private Matrix T;
    
  public ELM(NeuralNet _neuralNet,NeuralDataSet _trainDataSet){
    super(_neuralNet,_trainDataSet);
    learningMode=LearningMode.BATCH;
    initializeMatrices();
  }
}</pre></div><p class="calibre11">In this class, we define the matrices <span class="strong1"><em class="calibre16">H</em></span> and <span class="strong1"><em class="calibre16">T</em></span>, which will be later used for output weight calculation. The constructor is similar to the other training algorithms, except for the fact that this algorithm works only on batch mode.</p><p class="calibre11">Since this training algorithm takes only one epoch, the train method forwards all training records to build the <span class="strong1"><em class="calibre16">H</em></span> matrix. Then, it calculates the output weights:</p><div class="calibre2"><pre class="programlisting">@Override
public void train() throws NeuralException{
  if(neuralNet.getNumberOfHiddenLayers()!=1)
    throw new NeuralException("The ELM learning algorithm can be performed only on Single Hidden Layer Neural Network");
  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.TRAINING);
  int k=0;
  int N=trainingDataSet.numberOfRecords;
  currentRecord=0;
  forward();
  double currentOverallError=overallGeneralError;
  while(k&lt;N){
    forward(k);
    buildMatrices();
    currentRecord=++k;
  }
  applyNewWeights();
  forward();
  currentOverallError=overallGeneralError;
  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.RUN);
}</pre></div><p class="calibre11">The <code class="literal">buildMatrices</code> method only places the output of the hidden layer to its corresponding row<a id="id235" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> in the <span class="strong1"><em class="calibre16">H</em></span> matrix. The output <a id="id236" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>weights are adjusted in the <code class="literal">applyNewWeights</code> method:</p><div class="calibre2"><pre class="programlisting">@Override
public void applyNewWeights(){
  Matrix Ht = H.transpose();
  Matrix HtH = Ht.multiply(H);
  Matrix invH = HtH.inverse();
  Matrix invHt = invH.multiply(Ht);
  Matrix beta = invHt.multiply(T);
        
  OutputLayer ol = this.neuralNet.getOutputLayer();
  HiddenLayer hl = (HiddenLayer)ol.getPreviousLayer();
  int h = hl.getNumberOfNeuronsInLayer();
  int n = ol.getNumberOfNeuronsInLayer();
  for(int i=0;i&lt;=h;i++){
    for(int j=0;j&lt;n;j++){
      if(i&lt;h || outputBiasActive)
        ol.getNeuron(j).updateWeight(i, beta.getValue(i, j));
      }
    }
  }</pre></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Practical example 1 – the XOR case with delta rule and backpropagation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec29" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Practical example 1 – the XOR case with delta rule and backpropagation</h1></div></div></div><p class="calibre11">Now let's <a id="id237" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see the multilayer perceptron in action. We coded the example <code class="literal">XORTest.java</code>, which basically creates two neural networks with the following features:</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Neural Network</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Perceptron</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Multi-layer Percepetron</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Inputs</p>
</td><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">2</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">Outputs</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Hidden Layers</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">Hidden Neurons in each layer</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">2</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Hidden Layer Activation Function</p>
</td><td class="calibre29">
<p class="calibre26">Non</p>
</td><td class="calibre29">
<p class="calibre26">Sigmoid</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">Output Layer Activation Function</p>
</td><td class="calibre29">
<p class="calibre26">Linear</p>
</td><td class="calibre29">
<p class="calibre26">Linear</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Training Algorithm</p>
</td><td class="calibre29">
<p class="calibre26">Delta Rule</p>
</td><td class="calibre29">
<p class="calibre26">Backpropagation</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">Learning Rate</p>
</td><td class="calibre29">
<p class="calibre26">0.1</p>
</td><td class="calibre29">
<p class="calibre26">0.3</p>
<p class="calibre26">Momentum 0.6</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">Max Epochs</p>
</td><td class="calibre29">
<p class="calibre26">4000</p>
</td><td class="calibre29">
<p class="calibre26">4000</p>
</td></tr><tr class="calibre37"><td class="calibre29">
<p class="calibre26">Min. overall error</p>
</td><td class="calibre29">
<p class="calibre26">0.1</p>
</td><td class="calibre29">
<p class="calibre26">0.01</p>
</td></tr></tbody></table></div><p class="calibre11">In Java, this is coded as follows:</p><div class="calibre2"><pre class="programlisting">public class XORTest {
  public static void main(String[] args){
    RandomNumberGenerator.seed=0;
       
    int numberOfInputs=2;
    int numberOfOutputs=1;
        
    int[] numberOfHiddenNeurons={2};
      
    Linear outputAcFnc = new Linear(1.0);
    Sigmoid hdAcFnc = new Sigmoid(1.0);
    IActivationFunction[] hiddenAcFnc={hdAcFnc};
        
    NeuralNet perceptron = new NeuralNet(numberOfInputs,
         numberOfOutputs,outputAcFnc);

    NeuralNet mlp = new NeuralNet(numberOfInputs,numberOfOutputs
                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);
  }
}</pre></div><p class="calibre11">Then, we<a id="id238" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> define the dataset and the learning algorithms:</p><div class="calibre2"><pre class="programlisting">Double[][] _neuralDataSet = {
            {0.0 , 0.0 , 1.0 }
        ,   {0.0 , 1.0 , 0.0 }
        ,   {1.0 , 0.0 , 0.0 }
        ,   {1.0 , 1.0 , 1.0 }
        };
        
int[] inputColumns = {0,1};
int[] outputColumns = {2};
        
NeuralDataSet neuralDataSet = new NeuralDataSet(_neuralDataSet,inputColumns,outputColumns);
        
DeltaRule deltaRule=new DeltaRule(perceptron,neuralDataSet
                ,LearningAlgorithm.LearningMode.ONLINE);
        
deltaRule.printTraining=true;
deltaRule.setLearningRate(0.1);
deltaRule.setMaxEpochs(4000);
deltaRule.setMinOverallError(0.1);
        
Backpropagation backprop = new Backpropagation(mlp,neuralDataSet
                ,LearningAlgorithm.LearningMode.ONLINE);
        backprop.printTraining=true;
        backprop.setLearningRate(0.3);
        backprop.setMaxEpochs(4000);
        backprop.setMinOverallError(0.01);
        backprop.setMomentumRate(0.6);</pre></div><p class="calibre11">The<a id="id239" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> training is then performed for both algorithms. As expected, the XOR case is not linearly separable by one single layer perceptron. The neural network runs the training but unsuccessfully:</p><div class="calibre2"><pre class="programlisting">deltaRule.train();</pre></div><div class="mediaobject"><img src="Images/B05964_03_15.jpg" alt="Practical example 1 – the XOR case with delta rule and backpropagation" class="calibre118"/></div><p class="calibre11">But the backpropagation algorithm for the multilayer perceptron manages to learn the XOR function after 39 epochs:</p><div class="calibre2"><pre class="programlisting">backprop.train();</pre></div><div class="mediaobject"><img src="Images/B05964_03_16.jpg" alt="Practical example 1 – the XOR case with delta rule and backpropagation" class="calibre119"/></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Practical example 2 – predicting enrolment status"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec30" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Practical example 2 – predicting enrolment status</h1></div></div></div><p class="calibre11">In Brazil, one<a id="id240" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the ways for a person to enter university consists of taking an exam and if he/she achieves the minimum grade for the course that he/she is seeking, then he/she can enroll. To demonstrate the backpropagation algorithm, let us consider this scenario. The data shown in the table was collected from a university database. The second column represents the person's gender (one means female and zero means male); the third column has grades scaled by 100 and the last column is formed by two neurons (1,0 means performed enrollment and 0,1 means waiver enrollment):</p><div class="informaltable"><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Sample</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Gender</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Grade</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Enrollment</p>
<p class="calibre26">Status</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre31"><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.73</p>
</td><td class="calibre29">
<p class="calibre26">1  0</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">2</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.81</p>
</td><td class="calibre29">
<p class="calibre26">1  0</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">3</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.86</p>
</td><td class="calibre29">
<p class="calibre26">1  0</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">4</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.65</p>
</td><td class="calibre29">
<p class="calibre26">1  0</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">5</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.45</p>
</td><td class="calibre29">
<p class="calibre26">1  0</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">6</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.70</p>
</td><td class="calibre29">
<p class="calibre26">0  1</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">7</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.51</p>
</td><td class="calibre29">
<p class="calibre26">0  1</p>
</td></tr><tr class="calibre34"><td class="calibre29">
<p class="calibre26">8</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.89</p>
</td><td class="calibre29">
<p class="calibre26">0  1</p>
</td></tr><tr class="calibre31"><td class="calibre29">
<p class="calibre26">9</p>
</td><td class="calibre29">
<p class="calibre26">1</p>
</td><td class="calibre29">
<p class="calibre26">0.79</p>
</td><td class="calibre29">
<p class="calibre26">0  1</p>
</td></tr><tr class="calibre37"><td class="calibre29">
<p class="calibre26">10</p>
</td><td class="calibre29">
<p class="calibre26">0</p>
</td><td class="calibre29">
<p class="calibre26">0.54</p>
</td><td class="calibre29">
<p class="calibre26">0  1</p>
</td></tr></tbody></table></div><div class="calibre2"><pre class="programlisting">Double[][] _neuralDataSet = {
            {1.0,   0.73,   1.0,    -1.0}
        ,   {1.0,   0.81,   1.0,    -1.0}
        ,   {1.0,   0.86,   1.0,    -1.0}
        ,   {0.0,   0.65,   1.0,    -1.0}
        ,   {0.0,   0.45,   1.0,    -1.0}
        ,   {1.0,   0.70,   -1.0,    1.0}
        ,   {0.0,   0.51,   -1.0,    1.0}
        ,   {1.0,   0.89,   -1.0,    1.0}
        ,   {1.0,   0.79,   -1.0,    1.0}
        ,   {0.0,   0.54,   -1.0,    1.0}

        };
        
int[] inputColumns = {0,1};
int[] outputColumns = {2,3};
       
NeuralDataSet neuralDataSet = new NeuralDataSet(_neuralDataSet,inputColumns,outputColumns);</pre></div><p class="calibre11">We create a<a id="id241" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> neural network containing three neurons in the hidden layer, as shown in the following figure:</p><div class="mediaobject"><img src="Images/B05964_03_17.jpg" alt="Practical example 2 – predicting enrolment status" class="calibre120"/></div><div class="calibre2"><pre class="programlisting">int numberOfInputs = 2;
int numberOfOutputs = 2;
int[] numberOfHiddenNeurons={5};
   
Linear outputAcFnc = new Linear(1.0);
Sigmoid hdAcFnc = new Sigmoid(1.0);
IActivationFunction[] hiddenAcFnc={hdAcFnc  };
       
NeuralNet nnlm = new NeuralNet(numberOfInputs,numberOfOutputs
                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);
        
NeuralNet nnelm = new NeuralNet(numberOfInputs,numberOfOutputs
                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);</pre></div><p class="calibre11">We've also <a id="id242" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>set up the learning algorithms Levenberg-Marquardt and extreme learning machines:</p><div class="calibre2"><pre class="programlisting">LevenbergMarquardt lma = new LevenbergMarquardt(nnlm,
neuralDataSet,
LearningAlgorithm.LearningMode.BATCH);
lma.setDamping(0.001);
lma.setMaxEpochs(100);
lma.setMinOverallError(0.0001);
        
ELM elm = new ELM(nnelm,neuralDataSet);
elm.setMinOverallError(0.0001);
elm.printTraining=true;</pre></div><p class="calibre11">Running the training, we find that the training was successful. For the Levenberg-Marquardt algorithm, the minimum satisfied error was found after nine epochs:</p><div class="mediaobject"><img src="Images/B05964_03_18.jpg" alt="Practical example 2 – predicting enrolment status" class="calibre121"/></div><p class="calibre11">And the <a id="id243" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>extreme learning machine found an error near to zero:</p><div class="mediaobject"><img src="Images/B05964_03_19.jpg" alt="Practical example 2 – predicting enrolment status" class="calibre122"/></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch03lvl1sec31" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've seen how perceptrons can be applied to solve linear separation problems, but also their limitations in classifying nonlinear data. To suppress those limitations, we presented <span class="strong1"><strong class="calibre12">multi-layer perceptrons</strong></span> (<span class="strong1"><strong class="calibre12">MLPs</strong></span>) and new training algorithms: backpropagation, Levenberg-Marquardt, and extreme learning machines. We've also seen some classes of problems which MLPs can be applied to, such as classification and regression. The Java implementation explored the power of the backpropagation algorithm in updating the weights both in the output layer and the hidden layer. Two practical applications were shown to demonstrate the MLPs for the solution of problems with the three learning algorithms.</p></div></div>



  </body></html>