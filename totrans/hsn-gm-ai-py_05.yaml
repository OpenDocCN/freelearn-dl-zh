- en: Temporal Difference Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间差分学习
- en: In our previous discussion on the history of reinforcement learning, we covered
    the two main threads, trial and error and **Dynamic Programming** (**DP**), which
    came together to derive current modern **Reinforcement Learning** (**RL**). As
    we mentioned in earlier chapters, there is also a third thread that arrived late
    called **Temporal Difference Learning** (**TDL**). In this chapter, we will explore
    TDL and how it solves the **Temporal Credit Assignment** (**TCA**) problem. From
    there, we will explore how TD differs from **Monte Carlo** (**MC**) and how it
    evolves to full Q-learning. After that, we will explore the differences between
    on-policy and off-policy learning and then, finally, work on a new example RL
    environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前关于强化学习历史的讨论中，我们涵盖了两个主要线索，试错和**动态规划**（**DP**），它们结合在一起推导出现代**强化学习**（**RL**）。正如我们在前面的章节中提到的，还有一个后来到达的第三线索，称为**时间差分学习**（**TDL**）。在本章中，我们将探讨TDL以及它是如何解决**时间信用分配**（**TCA**）问题的。从那里，我们将探讨TD与**蒙特卡洛**（**MC**）的不同之处以及它是如何演变成完整的Q-learning的。然后，我们将探讨在策略学习和离策略学习之间的差异，最后，我们将尝试一个新的RL环境示例。
- en: 'For this chapter, we will introduce TDL and how it improves on the previous
    techniques we looked at in previous chapters. Here are the main topics we will
    cover in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将介绍TDL以及它是如何改进我们在前几章中查看的先前技术的。以下是本章我们将涵盖的主要主题：
- en: Understanding the TCA problem
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解TCA问题
- en: Introducing TDL
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍TDL
- en: Applying TDL to Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将TDL应用于Q-learning
- en: Exploring TD(0) in Q-learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Q-learning中探索TD(0)
- en: Running off-policy versus on-policy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行离策略与在策略
- en: This chapter introduces TDL and Q-learning in detail. Therefore, it is worth
    reviewing and understanding the material well. Knowing the foundational material
    well will only ease your learning later.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了TDL和Q-learning。因此，值得仔细回顾和理解这些材料。对基础材料的深入了解将有助于你后来的学习。
- en: Understanding the TCA problem
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解TCA问题
- en: The credit assignment problem is described as the task of understanding what
    actions you need to take to receive the most credit or, in the case of RL, rewards.
    RL solves the credit assignment problem by allowing an algorithm or agent to find
    the optimum set of actions to maximize the rewards. In all of our previous chapters,
    we have seen how variations of this can be done with DP and MC methods. However,
    both of these previous methods are offline, so they cannot learn while performing
    a task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 信用分配问题被描述为理解你需要采取哪些行动来获得最多的信用或，在RL的情况下，奖励。RL通过允许算法或智能体找到最大化奖励的最佳动作集来解决信用分配问题。在我们之前的所有章节中，我们都看到了如何使用DP和MC方法实现这一点的变体。然而，这两种先前的方法都是离线的，因此它们在执行任务时无法学习。
- en: The TCA problem is differentiated from the credit assignment CA problem in that
    it needs to be solved across time; that is, an algorithm needs to find the best
    policy across time steps instead of learning after an episode, in the case of
    MC, or needing to plan before, as DP does. This also means that an algorithm that
    solves the CA problem across time can also or should be able to learn in real
    time, that is, be able to make updates to a policy during the progression of the
    task, rather than before or after as we have seen in earlier chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TCA问题与信用分配CA问题的区别在于它需要在时间上解决；也就是说，算法需要在时间步长中找到最佳策略，而不是在MC的情况下在事件后学习，或者在DP的情况下需要提前规划。这也意味着，一个在时间上解决CA问题的算法也能够或应该能够实时学习，即在任务进行过程中能够更新策略，而不是像我们在前面的章节中看到的那样在之前或之后。
- en: By introducing the concept of time or a progression of events, we also allow
    our agent to learn the importance of event timing. Previously, our agents would
    have no awareness of time-critical events such as hitting a moving target or timing
    a running jump just right. TDL, on the other hand, allows an agent to understand
    event timing and take appropriate action. We will introduce the concept and intuition
    of TDL in the next section.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入时间或事件进展的概念，我们也允许我们的智能体学习事件时间的重要性。以前，我们的智能体对时间关键事件，如击中移动目标或准确计时跳跃，没有任何意识。另一方面，TDL允许智能体理解事件时间并采取适当的行动。我们将在下一节介绍TDL的概念和直觉。
- en: Introducing TDL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍TDL
- en: TDL was introduced by the father of RL himself, Dr. Richard Sutton, in 1988\.
    Sutton had developed the method as an improvement to MC/DP but, as we will see,
    the method itself led to the development of Q-learning by Chris Watkins in 1989\.
    The method itself is model-free and does not require episode completion before
    an agent learns. This makes this method very powerful for exploring unknown environments
    in real time, as we will see.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TDL是由强化学习之父理查德·萨顿博士在1988年提出的。萨顿将此方法作为MC/DP的改进，但正如我们将看到的，该方法本身导致了1989年由克里斯·沃特金斯（Chris
    Watkins）提出的Q-learning的发展。这种方法本身是无模型的，不需要在智能体学习之前完成一个回合。这使得这种方法在实时探索未知环境时非常强大，正如我们将看到的。
- en: Before we get into discovering the updated mathematics to this approach, it
    may be helpful to look at the backup diagrams of all of the methods covered so
    far in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨更新此方法的新数学方法之前，查看下一节中涵盖的所有方法的备份图可能会有所帮助。
- en: Bootstrapping and backup diagrams
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自举和备份图
- en: 'TDL can learn during an episode by approximating the updated value function
    given previous experience. This allows the algorithm to learn while it is in an
    episode and hence make corrections as needed. To understand the differences further,
    let''s review a composite of the backup diagrams for DP, MC, and TD in the following
    diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TDL可以通过近似给定先前经验更新的价值函数来在回合期间学习。这使得算法在回合中进行学习，并根据需要做出修正。为了进一步了解差异，让我们回顾以下图中DP、MC和TD的备份图的组合：
- en: '![](img/730cb028-e6f9-469a-9d2e-5fdb56020163.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/730cb028-e6f9-469a-9d2e-5fdb56020163.png)'
- en: Backup diagrams for DP, MC, and TDL
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: DP、MC和TDL的备份图
- en: The diagram was taken from *An Introduction to Reinforcement Learning* by Barto
    and Sutton (2018). In the diagram, you can see our previous two methods, DP and
    MC, as well as TDL. The shaded area (red or black) denotes the algorithm's learning
    space. That is the area the agent will need to explore before being able to update
    its value function and, hence, policy. Notice how, as the diagrams progress from
    DP to TDL, the shaded area becomes smaller—that is, for each advancing algorithm,
    that agent needs to explore less and less area initially or during an episode
    before learning. As we will see, this allows an agent to learn before even finishing
    an episode.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该图取自巴托（Barto）和萨顿（Sutton）的《强化学习导论》（2018年）。在图中，你可以看到我们之前两种方法，DP和MC，以及TDL。阴影区域（红色或黑色）表示算法的学习空间。也就是说，智能体在能够更新其价值函数和策略之前需要探索的区域。注意，随着图从DP到TDL的进展，阴影区域变得较小——也就是说，对于每个进步的算法，智能体在开始学习之前或在学习过程中需要探索的面积越来越少。正如我们将看到的，这允许智能体在学习完成回合之前就开始学习。
- en: Before we look at the code, we should take a look at how this new approach revises
    the math of our value function in the next section.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看代码之前，我们应该看看这种新的方法如何在下一节中修改我们的价值函数的数学。
- en: Applying TD prediction
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用TD预测
- en: Throughout this book, we will explore methods for allowing an algorithm to predict
    and control an agent to complete a task. Prediction and control are at the heart
    of RL, and previously we had both methods separate. That is, they either ran before
    (DP) or after (MC). Now, for an agent to learn in real time, we need an online
    update rule that will update the value function after a designated time step.
    In TDL, this is called the TD update rule.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将探讨允许算法预测和控制智能体完成任务的方法。预测和控制是强化学习（RL）的核心，之前我们有两种方法分开。也就是说，它们要么在（DP）之前运行，要么在（MC）之后运行。现在，为了使智能体能够实时学习，我们需要一个在线更新规则，该规则将在指定的时间步之后更新价值函数。在TDL中，这被称为TD更新规则。
- en: 'The rule is shown here in equation form:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 规则在此以方程形式展示：
- en: '![](img/956df4fc-01db-4559-b254-7ad3413a6414.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/956df4fc-01db-4559-b254-7ad3413a6414.png)'
- en: 'In the previous equation, we have the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下内容：
- en: '![](img/7fc168a7-ace6-409c-97eb-86a90e1489e8.png): The value function for the
    current state'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/7fc168a7-ace6-409c-97eb-86a90e1489e8.png): 当前状态的价值函数'
- en: '![](img/023a6a65-0a6a-4a35-9459-bdc579e13a2e.png): Alpha, the learning rate'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/023a6a65-0a6a-4a35-9459-bdc579e13a2e.png): 学习率α'
- en: '![](img/373c9f31-ae6e-46bf-a01c-72958e5a7f97.png): The reward of the next state'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/373c9f31-ae6e-46bf-a01c-72958e5a7f97.png): 下一个状态的重奖'
- en: '![](img/c76bca53-77df-4b0b-b907-7f32c82761b1.png): A discount factor'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/c76bca53-77df-4b0b-b907-7f32c82761b1.png): 折扣因子'
- en: '![](img/14dc93fa-c8d1-42f7-83a1-cfc226215e99.png): The value of the next state'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![图片](img/14dc93fa-c8d1-42f7-83a1-cfc226215e99.png): 下一个状态的价值'
- en: Hence, we can say that the value of the current state is equal to the value
    of the current state plus alpha, times the summation of the next reward, plus
    a discount factor, gamma, times the difference between the next state value and
    the current state value.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以说当前状态的价值等于当前状态的价值加上alpha，乘以下一个奖励的总和，再加上一个折现因子，gamma，乘以下一个状态价值与当前状态价值的差。
- en: To understand this better, let's look at a code example in the next section.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们在下一节中查看一个代码示例。
- en: TD(0) or one-step TD
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD(0)或一步TD
- en: One thing we should identify before getting too far ahead is that the method
    we look at here is for one-step TD or what we refer to as TD(0). Remember, as
    programmers, we start counting at 0, so TD(0) essentially means TD one-step. We
    will look at multiple-step TD in [Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml),
    *Exploring SARSA*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入之前，我们应该确定的是，我们这里查看的方法是针对一步TD或我们所说的TD(0)。记住，作为程序员，我们从0开始计数，所以TD(0)本质上意味着一步TD。我们将在[第5章](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml)中查看多步TD，*探索SARSA*。
- en: 'For now, though, we will look at an example of using one-step TD in the next
    exercise:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看使用一步TD在下一练习中的示例：
- en: 'Open the `Chapter_4_1.py` source code example, as seen in the following code:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开`Chapter_4_1.py`源代码示例，如下所示：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a straight code example demonstrating how value updates work and that
    uses no RL environment. The first section we will focus on is where we initialize
    our parameters just after the imports. Here, we initialize the learning rate,
    `alpha` (`0.5`); discount factor, `gamma` (`0.5`); the size of the environment,
    `gridSize` (`4`); a list of state `terminations`; list of `actions`; and finally,
    `episodes`. Actions represent the movement vector and terminations represent the
    grid square where an episode will terminate.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个直接展示价值更新工作原理的代码示例，并且不使用RL环境。我们将重点关注的第一个部分是在导入之后初始化我们的参数。在这里，我们初始化学习率`alpha`（`0.5`）、折现因子`gamma`（`0.5`）、环境大小`gridSize`（`4`）、状态`terminations`列表、`actions`列表，最后是`episodes`。动作代表移动向量，终止代表一个episod将终止的网格方块。
- en: Next, we initialize the value function, `V`, with all zeros using the `numpy`
    function, `zeros`. We then create three lists, `returns`, `deltas`, and `states`
    using Python list comprehensions. These lists hold values for later retrieval
    and notice how this relates to a DP technique.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用`numpy`函数`zeros`将值函数`V`初始化为零。然后，我们使用Python列表推导式创建了三个列表，`returns`、`deltas`和`states`。这些列表用于后续检索，并注意这一点如何与动态规划技术相关。
- en: 'Then, we define some utility functions, `generateInitialState`, `generateNextAction`,
    and `takeAction`. The first two functions are self-explanatory but let''s focus
    on the `takeAction` function:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义了一些效用函数，`generateInitialState`、`generateNextAction`和`takeAction`。前两个函数是自解释的，但让我们专注于`takeAction`函数：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding function takes the current `state` and `action` as inputs. It
    then determines whether the current state is terminal; if it is, it returns. Otherwise,
    it calculates the next state using simple vector math to get `finalState`.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前面的函数接受当前`state`和`action`作为输入。然后，它确定当前状态是否是终端状态；如果是，则返回。否则，它使用简单的向量数学计算下一个状态以获得`finalState`。
- en: Then, we enter the `for` loop that starts episodic training. Note that, although
    the agent explores the environment episodically, since the environment has a beginning
    and an end, it still learns temporally. That is, it learns after every time step.
    The `tqdm` library is a helper enumerator, which prints a status bar when we run
    a `for` loop.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们进入开始episodic训练的`for`循环。注意，尽管代理是episodically探索环境的，但由于环境有开始和结束，它仍然在时间上学习。也就是说，它在每个时间步之后学习。`tqdm`库是一个辅助枚举器，当我们在`for`循环中运行时，它会打印一个状态栏。
- en: 'The first thing that happens at the start of the `for` loop is the agent''s
    state is initialized randomly. After that, it enters a `while` loop that runs
    one entire episode. Most of this code is self-explanatory, aside from the implementation
    of the value update equation, as shown here:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`for`循环开始时发生的第一件事是代理的状态被随机初始化。之后，它进入一个`while`循环，运行整个一个episodic。大部分代码都是自解释的，除了这里所示的价值更新方程的实现：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The block of code is an implementation of the previous value update function.
    Notice the use of the learning rate, `alpha`, and discount factor, `gamma`.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码块是之前价值更新函数的实现。注意学习率`alpha`和折现因子`gamma`的使用。
- en: 'Run the code as you normally would and notice the output of the `Value` function:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行代码，并注意`Value`函数的输出：
- en: '![](img/19cbf135-fb34-45ec-8d9c-867c47f23945.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19cbf135-fb34-45ec-8d9c-867c47f23945.png)'
- en: Running example in Chapter_4_1.py
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在第_4_1章中运行示例
- en: Now, the function is less than optimal and that has more to do with the training
    or hyperparameters we used. We will look at the importance of tuning hyperparameters
    in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，函数不是最优的，这更多与我们的训练或使用的超参数有关。我们将在下一节探讨调整超参数的重要性。
- en: Tuning hyperparameters
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整超参数
- en: The collection of training parameters that we investigated at the top of the
    last code example are called hyperparameters, so named to differentiate them from
    normal parameters or weights we use in deep learning. We have yet to look at deep
    learning in detail, but it is important to understand why they are called **hyperparameters**.
    Previously, we played around with the concept of a learning rate and discount
    factor but now we need to formalize them and understand their effect across methods
    and environments.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一个代码示例顶部调查的训练参数集合被称为超参数，这样命名是为了将它们与我们在深度学习中使用的正常参数或权重区分开来。我们尚未详细探讨深度学习，但了解为什么它们被称为**超参数**是很重要的。之前，我们围绕学习率和折扣因子进行了实验，但现在我们需要正式化它们并理解它们在方法和环境中的影响。
- en: 'In our last example, both the learning rate (alpha) and discount factor (gamma)
    were set to .5\. What we need to understand is what effect these parameters have
    on training. Let''s open up sample code `Chapter_4_2.py` and follow the next exercise:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们最后的例子中，学习率（alpha）和折扣因子（gamma）都被设置为 .5。我们需要理解这些参数对训练的影响。让我们打开示例代码 `Chapter_4_2.py`
    并跟随下一个练习：
- en: '`Chapter_4_2.py` is almost identical to the previous example, aside from a
    few minor differences. The first of these is we `import matplotlib` here to be
    able to view some results later.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Chapter_4_2.py` 几乎与上一个示例相同，除了几个细微的差异。其中第一个差异是我们在这里`import matplotlib`以便稍后能够查看一些结果。'
- en: 'Recall, you can install `matplotlib` from a Python console with the following
    command:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请记住，你可以使用以下命令从 Python 控制台安装 `matplotlib`：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We use `matplotlib` to render the results of our training efforts. As we progress
    through this book, we will look at more advanced methods later.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用 `matplotlib` 来渲染我们训练努力的成果。随着我们继续阅读本书，我们将在后面探讨更高级的方法。
- en: 'Next, we see that the hyperparameters, alpha and gamma, have been modified
    to values of `0.1`:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们看到超参数 alpha 和 gamma 已经被修改为 `0.1` 的值：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, as you often refine training, you likely will only want to modify a single
    parameter at a time. This will give you more control and understanding of the
    effect a parameter may have.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，由于你经常对训练进行微调，你很可能只想一次只修改一个参数。这将使你能够更好地控制并理解参数可能产生的影响。
- en: 'Finally, at the end of the file, we see the code to output the training values
    or change in training values. Recall the list we created earlier called `deltas`.
    Captured in this list are all of the deltas or changes made during training. This
    can be extremely useful to visualize, as we''ll see:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在文件末尾，我们看到输出训练值或改变训练值的代码。回想一下我们之前创建的名为 `deltas` 的列表。在这个列表中捕获了训练期间所做的所有 delta
    或变化。这可以非常有助于可视化，正如我们将看到的：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code just loops through the list of changes made during training for each
    episode. What we expect is that over training, the amount of change will reduce.
    Reducing the amount of change allows the agent to converge to some optimal value
    function and hence policy later.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码只是遍历每个一集中训练期间所做的变化列表。我们期望的是，随着训练的进行，变化量将减少。减少变化量允许代理收敛到某个最优值函数和策略。
- en: 'Run the code as you normally would and notice how the output of the value function
    has changed substantially but we can also see how the agent''s training progressed:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式运行代码，注意值函数的输出已经发生了显著变化，但我们也可以看到代理的训练进展：
- en: '![](img/734a9474-69fe-4ded-a419-51551aa206c1.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/734a9474-69fe-4ded-a419-51551aa206c1.png)'
- en: Example output from Chapter_4_2.py
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `Chapter_4_2.py` 的示例输出
- en: On the plot, you can now see how the training converges over time. The plot
    is represented over the number of steps in an episode. Notice how the amount of
    change or delta is greater to the left of the plot, with fewer steps in an episode,
    and then it decreases over time with more steps. This convergence assures us that
    the agent is indeed learning using the provided hyperparameters.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表上，你现在可以看到训练如何随时间收敛。图表表示的是一集中步骤的数量。注意，图表左侧的变化量或 delta 更大，因为一集中的步骤更少，然后随着时间的推移，随着步骤的增加而减少。这种收敛确保代理确实在使用提供的超参数进行学习。
- en: Tuning hyperparameters is foundational to deep learning and deep reinforcement
    learning. Many consider the tuning practice to be where all of the work is done
    and, for the most part, that is true. You may often spend days, weeks, or months
    tuning hyperparameters of a single network model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 调整超参数是深度学习和深度强化学习的基础。许多人认为调整实践是所有工作的核心，在大多数情况下，这是真的。你可能经常需要花费数天、数周或数月来调整单个网络模型的超参数。
- en: Feel free to explore playing with the hyperparameters on your own and see what
    effect each has on training convergence. In the next section, we look at how TD
    is combined with Q-learning.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 随意探索调整超参数，看看每个参数对训练收敛的影响。在下一节中，我们将探讨TD如何与Q-learning结合。
- en: Applying TDL to Q-learning
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将TDL应用于Q-learning
- en: Q-learning is considered one of the most popular and often used foundational
    RL methods . The method itself was developed by Chris Watkins in 1989 as part
    of his thesis, *Learning from Delayed Rewards*. Q-learning or rather Deep Q-learning,
    which we will cover in [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml),
    *Going Deep with DQN*, became so popular because of its use by DeepMind (Google)
    to play classic Atari games better than a human. What Watkins did was show how
    an update could be applied across state-action pairs using a learning rate and
    discount factor gamma.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning被认为是最受欢迎且经常使用的强化学习基础方法之一。该方法本身由Chris Watkins于1989年作为其论文《从延迟奖励中学习》的一部分开发。Q-learning或更确切地说，深度Q-learning（我们将在第6章中介绍，即《使用DQN深入探索》），因其被DeepMind（谷歌）用于玩经典Atari游戏并优于人类而变得非常流行。Watkins所做的是展示了如何使用学习率α和折扣因子γ在状态-动作对之间应用更新。
- en: 'This improved the update equation into a Q or quality of state-action update
    equation, as shown in the following formula:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这将更新方程改进为Q或状态-动作质量更新方程，如下公式所示：
- en: '![](img/4cc882f2-cb7a-4fb0-b25b-d2e7fd76b4d3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![超参数示例](img/4cc882f2-cb7a-4fb0-b25b-d2e7fd76b4d3.png)'
- en: 'In the previous equation, we have the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，我们有以下内容：
- en: '![](img/8b4a101d-da73-40ed-adae-292972e81996.png)The current state-action quality
    being updated'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![正在更新的当前状态-动作质量](img/8b4a101d-da73-40ed-adae-292972e81996.png)'
- en: '![](img/a570960a-0217-47dc-9587-489391546791.png)The learning rate'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![学习率](img/a570960a-0217-47dc-9587-489391546791.png)'
- en: '![](img/54152573-c230-41cb-bbe8-ba63f0480176.png)The reward for the next state'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![下一状态奖励](img/54152573-c230-41cb-bbe8-ba63f0480176.png)'
- en: '![](img/04ee51c9-7562-452e-ad22-3a08624e7d44.png)Gamma, the discount factor'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![折扣因子Gamma](img/04ee51c9-7562-452e-ad22-3a08624e7d44.png)'
- en: '![](img/2334cd52-d080-486f-b3e4-82e33576e28b.png)Take the max best or greedy
    action'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![采取最佳或贪婪动作](img/2334cd52-d080-486f-b3e4-82e33576e28b.png)'
- en: This equation allows us to update state-action pairs based on learned future
    state-action pairs. It also does not require a model as the algorithm explores
    by trial and error and can learn during an episode since updates are run during
    the episode.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此方程允许我们根据学习到的未来状态-动作对来更新状态-动作对。它也不需要模型，因为算法通过试错进行探索，并且可以在一个回合中学习，因为更新是在回合中运行的。
- en: This method can now solve the temporal credit assignment problem and we will
    look at a code example in the next section.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法现在可以解决时间信用分配问题，我们将在下一节中查看一个代码示例。
- en: Exploring TD(0) in Q-learning
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Q-learning中的TD(0)
- en: 'TDL for first step or TD(0) then essentially simplifies to Q-learning. To do
    a full comparison of this method against DP and MC, we will first revisit the
    FrozenLake environment from Gym. Open up example code `Chapter_4_4.py` and follow
    the exercise:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: TDL对于第一步或TD(0)实际上简化为Q-learning。为了全面比较此方法与DP和MC，我们首先回顾Gym中的FrozenLake环境。打开示例代码`Chapter_4_4.py`并遵循练习：
- en: 'The full listing of code is too large to show. Instead, we will review the
    code in sections starting with the imports:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码的完整列表太大，无法展示。相反，我们将从导入部分开始分节审查代码：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We have seen all of these imports before, so there is nothing new here. Next,
    we cover the initialization of the environment and outputting some initial environment
    variables:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们之前已经看到所有这些导入，所以这里没有什么新内容。接下来，我们将介绍环境初始化和输出一些初始环境变量的过程：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There''s nothing new here either. Next, we introduce the concept of a Q table
    or quality table that now defines our policy in terms of `state-action` pairs.
    We set this to an equal quality for each `state-action` pair by dividing one by
    the total number of actions in a state (`action-size`):'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里也没有什么新内容。接下来，我们介绍Q表或质量表的概念，它现在以`状态-动作`对的形式定义了我们的策略。我们通过将每个`状态-动作`对的质量设置为等于该状态下动作的总数（`action-size`）来设置这个值：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we can see a section of hyperparameters:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以看到超参数的一部分：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: There are two new parameters here called `play_game_test_episode` and `max_steps`.
    `max_steps` determine the number of maximum steps our algorithm may run in an
    episode. We do this to limit the agent from getting into possible endless loops.
    `play_game_test_episode` sets the episode number to show a preview of the agent
    playing based on the current best Q table.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里有两个新的参数，称为 `play_game_test_episode` 和 `max_steps`。`max_steps` 决定了我们的算法在一个场景中可能运行的最大步数。我们这样做是为了限制智能体陷入可能的无穷循环。`play_game_test_episode`
    设置场景编号，以显示基于当前最佳 Q 表的智能体预览。
- en: 'Next, we introduce an entirely new set of parameters that have to deal with
    exploration and exploitation:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们介绍一组全新的参数，这些参数必须处理探索和利用：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Recall that we discussed the exploration versus exploitation dilemma in RL in
    [Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding Rewards-Based
    Learning*. In this section, we introduce `epsilon`, `max_epsilon`, `min_epsilon`,
    and `decay_rate`. These hyperparameters control the exploration rate of the agent
    while it explores, where `epsilon` is the current probability of an agent exploring
    during a time step. Maximum and minimum epsilon represent the limits of how much
    or little the agent explores, with `decay_rate` controlling how much the `epsilon`
    value decays from time step to step.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回想一下，我们在 [第 1 章](5553d896-c079-4404-a41b-c25293c745bb.xhtml) 中讨论了 RL 中的探索与利用的困境，*理解基于奖励的学习*。在本节中，我们介绍了
    `epsilon`、`max_epsilon`、`min_epsilon` 和 `decay_rate`。这些超参数控制智能体在探索时的探索率，其中 `epsilon`
    是智能体在时间步长内探索的概率。最大和最小 ε 代表智能体探索的多少或多少的限制，`decay_rate` 控制从时间步到时间步 `epsilon` 值衰减的程度。
- en: 'Understanding the exploration versus exploitation dilemma is essential to RL
    so we will pause here and let you run the example. The following is an example
    of the agent playing on the FrozenLake environment:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解探索与利用的困境对于 RL 是至关重要的，因此我们将在这里暂停，让你运行示例。以下是在 FrozenLake 环境中智能体玩游戏的示例：
- en: '![](img/8ef78387-241b-43d5-87a5-0d066bee52dd.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8ef78387-241b-43d5-87a5-0d066bee52dd.png)'
- en: Example output from Chapter_4_4.py
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Chapter_4_4.py 的示例输出
- en: Watch how the agent plays the game in the later episodes, after 40,000, and
    you will see that the agent can move around the holes and find the goal in short
    order, usually. The reason it may not do this has to do with exploration/exploitation
    and something we will review again in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 观察智能体在后续场景（40,000 之后）如何玩游戏，你会发现智能体可以迅速在洞周围移动并找到目标，通常情况下。它可能不这样做的原因与探索/利用有关，我们将在下一节再次回顾。
- en: Exploration versus exploitation revisited
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视探索与利用
- en: For several chapters now, we have always assumed our agent to be greedy. That
    is, it always chooses the best action given a choice in policy. However, as we
    have seen, this does not always provide the best path to optimum reward. Instead,
    what we find is that by allowing an agent to randomly explore early and then over
    time reduce the chance of exploration, there is a substantial improvement in learning.
    Except, if the environment is too large or complex, an agent may need more exploration
    time compared to a much smaller environment. If we maintained high exploration
    in a small environment, our agent would just waste time exploring. This is the
    trade-off you need to balance and it is often tied to the task required to perform.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们一直假设我们的智能体是贪婪的。也就是说，它总是根据策略选择最佳行动。然而，正如我们所看到的，这并不总是提供最佳的最优奖励路径。相反，我们发现，通过允许智能体在早期随机探索，然后随着时间的推移减少探索的机会，学习效果有显著提高。除非环境太大或太复杂，智能体可能需要比小得多环境更多的探索时间。如果我们在一个小环境中保持高探索，我们的智能体就会浪费时间探索。这是你需要平衡的权衡，这通常与需要执行的任务相关。
- en: 'The method of exploration we are using in this example is called e-greedy or
    epsilon-greedy. It''s so named because we start out greedy with high epsilon and
    then decrease it over time. There are several exploration methods we may use and
    a list of the more common versions and a description of each is shown here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用的探索方法是称为 ε-贪婪或 ε-greedy。之所以这样命名，是因为我们一开始以高 ε 值贪婪，然后随着时间的推移逐渐减少它。我们可以使用几种探索方法，这里展示了一些更常见的版本及其描述：
- en: '**Random**: This is the always random method, which can be an effective baseline
    test to perform on a new environment.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机**：这是一种始终随机的策略，可以作为一个有效的基线测试，在新环境中执行。'
- en: '**Greedy**: This is always taking the greedy or best action in a state. As
    we have seen, this can have bad consequences.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贪婪**：这始终是在状态中采取贪婪或最佳动作。正如我们所见，这可能会产生不良后果。'
- en: '**E-greedy**: Epsilon greedy allows for a method to balance exploration rate
    over time by decreasing epsilon (exploration rate) by a factor during each time
    step or episode.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ε-贪婪**：ε-贪婪允许通过在每个时间步或回合期间通过一个因子减少ε（探索率）来平衡随时间变化的探索率。'
- en: '**Bayesian or Thompson sampling**: This uses statistics and probability to
    best choose an action over a random distribution of sampled actions. Essentially,
    the action is chosen from across action distributions. For example, if a state
    has a bag of actions to choose from, each bag then also stores previous rewards
    for each action. A new action is chosen by taking a random choice from each action
    bag and comparing it to all of the other choices. The best action, the one giving
    the highest value, is selected. We don''t actually store all of the rewards for
    all previous actions but, rather, we determine the sample distribution that describes
    these returned rewards.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**贝叶斯或汤普森采样**：这种方法利用统计学和概率来从一系列随机采样的动作中选择最佳动作。本质上，动作是从动作分布中选择出来的。例如，如果一个状态有一系列动作可供选择，那么每个袋子也会存储每个动作的先前奖励。通过从每个动作袋中随机选择一个动作并与所有其他选择进行比较，选择一个新的动作。选择最佳动作，即给出最高价值的动作。我们实际上并不存储所有先前动作的所有奖励，而是确定描述这些返回奖励的样本分布。'
- en: If the concepts of statistics and probability we just discussed are foreign
    to you, then you should engage in some free online learning of these topics. These
    concepts will be covered over and over again in-depth in later chapters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们刚才讨论的统计学和概率概念对你来说很陌生，那么你应该参与一些关于这些主题的免费在线学习。这些概念将在后面的章节中反复深入探讨。
- en: There are other methods that provide additional options for strategies on how
    to pick the best action for a Q-learner. For now, though, we will stick to e-greedy
    as it is relatively simple to implement and is quite effective.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他方法提供了额外的选项，用于选择Q-学习者的最佳动作的策略。然而，现在我们将坚持使用ε-贪婪，因为它相对简单易行且相当有效。
- en: Now, if you go back to example `Chapter_4_4.py` and watch closely, you will
    see the agent may reach the goal, or it may not. In fact, the FrozenLake environment
    is more treacherous than we give it credit for. What that means is that rewards
    in that environment are more sparse and it often takes considerably longer to
    train with trial and error techniques. Instead, this type of learning method performs
    better in environments with continuous rewards. That is, when an agent receives
    a reward during an episode and not just at termination.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你回到示例`Chapter_4_4.py`并仔细观察，你会看到智能体可能达到目标，也可能不会。事实上，FrozenLake环境比我们给予它的评价要危险得多。这意味着该环境中的奖励更加稀疏，并且通常需要相当长的时间才能通过试错技术进行训练。相反，这种学习方法在具有连续奖励的环境中表现更好。也就是说，当智能体在回合期间而不是仅在终止时收到奖励时。
- en: Fortunately, Gym has plenty of environments we can play with that will allow
    us more continuous rewards and we will explore a fun example in the next section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Gym提供了大量的环境，我们可以玩这些环境，这将使我们获得更多的连续奖励，我们将在下一节中探索一个有趣的例子。
- en: Teaching an agent to drive a taxi
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教智能体驾驶出租车
- en: OpenAI Gym has plenty of fun environments that allow us to switch out and test
    new environments very easily. This, as we have seen, allows us to compare results
    of algorithms far easier. However, as we have also seen, there are limitations
    to various algorithms, and the new environment we explore in this section introduces
    the limitation of time. That is, it places a time limit on the agent as part of
    the goal. In doing this, our previous algorithms, DP and MC, become unable to
    solve such a problem, which makes this a good example to also introduce time-based
    or time-critical rewards.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI Gym提供了许多有趣的环境，使我们能够轻松切换和测试新的环境。正如我们所见，这使我们能够更容易地比较算法的结果。然而，正如我们所见，各种算法都有局限性，本节中我们探索的新环境引入了时间的限制。也就是说，它将时间限制作为目标的一部分。通过这样做，我们之前的算法，DP和MC，无法解决这样的问题，这使得这是一个介绍基于时间或时间敏感奖励的好例子。
- en: What better way to introduce time-dependent learning than to think of a time-dependent
    task? There are plenty of tasks, but one that works well is an example of a taxi.
    That is, the agent is a taxi driver and must pick up passengers and drop them
    off at their correct destinations but promptly. In the Gym Taxi-v2 environment,
    it is the goal of the agent to pick up a passenger at a location and then drop
    them off at a goal. The agent gets a reward of +20 points for a successful drop
    off and -1 point for every time step it takes. Hence, the agent needs to pick
    up and drop off passengers as quickly as possible.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 想要介绍时间依赖性学习，还有什么比考虑一个时间依赖性任务更好的方法呢？有很多任务，但其中一个效果很好的例子是出租车。也就是说，智能体是出租车司机，必须迅速接乘客并送他们到正确的目的地。在Gym
    Taxi-v2环境中，智能体的目标是接乘客到某个位置，然后将其送到目标。如果智能体成功送达，将获得+20分的奖励，每经过一个时间步长将扣除1分。因此，智能体需要尽可能快地接乘客并送达。
- en: 'An example of an agent playing this environment is shown here:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了智能体在这个环境中玩耍的一个示例：
- en: '![](img/f7020394-2df8-466f-bbac-28cb0be8e588.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f7020394-2df8-466f-bbac-28cb0be8e588.png)'
- en: Example output from the Taxi-v2 Gym environment
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Taxi-v2 Gym环境的示例输出
- en: In the screenshot, the car is green, meaning it has picked up a passenger from
    one of the symbols. The current goal is for the agent to drop off the passenger
    at their designated goal (symbol) highlighted. Let's now go back to the code,
    this time, to `Chapter_4_5.py`, which is an updated version of our last example
    now using the Taxi-v2 environment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在截图上，汽车是绿色的，这意味着它已经从其中一个符号接过了一名乘客。当前的目标是让智能体将乘客送到指定的目标（符号）处，该目标被高亮显示。现在让我们回到代码，这次是`Chapter_4_5.py`，这是我们上一个示例的更新版本，现在使用的是Taxi-v2环境。
- en: 'After you have the code opened, follow the exercise:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在打开代码后，请按照以下练习进行操作：
- en: 'This code example is virtually identical to `Chapter_4_4.py`, aside from the
    initialization of the environment shown:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个代码示例几乎与`Chapter_4_4.py`相同，除了显示的环境初始化不同：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will use all of the same hyperparameters as before, so there''s no need
    to look at those again. Instead, skip down to the `play_game` function, as shown
    in the following code block:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用之前相同的超参数，因此没有必要再次查看它们。相反，跳到下面的`play_game`函数，如下面的代码块所示：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `play_game` function essentially uses the `qtable` list, which is essentially
    the generated policy of qualities for state-action pairs. The code should be comfortable
    now and one detail to notice is how the agent selects an action from the `qtable`
    list using the following code:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`play_game`函数本质上使用的是`qtable`列表，这实际上是状态-动作对的生成策略。现在代码应该很熟悉了，一个需要注意的细节是智能体如何使用以下代码从`qtable`列表中选择动作：'
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The `play_game` function here takes the role of our agent test function previously.
    This function will allow you to see the agent play the game as it progresses through
    training. This is accomplished by setting `render_game` to `play_game` to `True`.
    Doing this allows you to visualize the agent playing an episode of the game.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里的`play_game`函数扮演了我们之前提到的智能体测试函数的角色。这个函数将允许你在智能体训练过程中看到它玩游戏。这是通过将`render_game`设置为`play_game`为`True`来实现的。这样做可以让你可视化智能体在游戏中的一个回合。
- en: You will often want to watch how your agent is training, at least initially.
    This can provide you with clues as to possible errors in your implementation or
    the agent finding possible cheats in new environments. We have found agents to
    be very good cheaters or at least be able to find cheats quite readily.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常会想观察你的智能体是如何训练的，至少最初是这样。这可以为你提供关于实现中可能出现的错误或智能体在新环境中找到可能的作弊方法的线索。我们发现智能体是非常好的作弊者，或者至少能够很容易地找到作弊方法。
- en: 'Next, we jump down to the next for loop that iterates through the training
    episodes and trains the `qtable`. When a threshold set by `play_game_test_episode`
    of episodes has elapsed, we allow the agent to play a visible game. Doing this
    allows us to visualize the overall training progress. However, it is important
    to remember that this is only a single episode and the agent could be doing extensive
    exploration. So, it is important to remember that, when watching the agent, they
    may just occasionally randomly explore. The code shows how we loop through the
    episodes:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们跳到下一个for循环，该循环遍历训练回合并训练`qtable`。当经过`play_game_test_episode`设定的回合数阈值后，我们允许智能体玩一个可见的游戏。这样做可以让我们可视化整体训练进度。然而，重要的是要记住，这只是一个回合，智能体可能正在进行广泛的探索。因此，当观察智能体时，它们可能只是偶尔随机探索。代码展示了我们如何遍历回合：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'First, inside the episode loop, we handle the exploration-exploitation dilemma
    by sampling a random value and then comparing it to epsilon. If it is greater
    than the greedy action, it is selected; otherwise, a random exploratory action
    is selected, as shown in the code:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在情节循环内部，我们通过采样一个随机值并将其与epsilon进行比较来处理探索-利用困境。如果它大于贪婪动作，则选择该动作；否则，选择一个随机的探索性动作，如代码所示：
- en: '[PRE15]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, the next line is where the step is taken with the selected action. After
    that, `qtable` is updated based on the previous Q-learning equation. There is
    a lot going on in this single line of code, so make sure you understand it:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，下一行是执行所选动作的地方。之后，根据之前的Q-learning方程更新`qtable`。这一行代码中发生了很多事情，所以请确保你理解它：
- en: '[PRE16]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After that, we check whether the episode is done with the `done` flag. If it
    is, we terminate and continue with another episode. Otherwise, we update the value
    of `epsilon` with the following code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们通过`done`标志检查情节是否结束。如果是，我们终止并继续下一个情节。否则，我们使用以下代码更新`epsilon`的值：
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, the remainder of the code is shown:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，剩余的代码如下：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The last piece of code resets and then tests the environment with the trained
    `qtable` for `total_test_episodes` and then outputs the average score or reward
    for an episode.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一段代码重置并使用训练好的`qtable`测试环境，进行`total_test_episodes`次，然后输出一个情节的平均得分或奖励。
- en: 'Finally, run the code as you normally would and observe the output carefully.
    Pay particular attention to how the taxi picks up and drops off passengers in
    later episodes. Sample output from the training is shown here:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，像平时一样运行代码，并仔细观察输出结果。特别关注在后续的情节中出租车如何接载和放下乘客。以下是训练过程中的示例输出：
- en: '![](img/7ca331b0-d030-4496-a285-75ec1f378d60.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7ca331b0-d030-4496-a285-75ec1f378d60.png)'
- en: The final output from sample Chapter_4_5.py
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 样本输出来自Chapter_4_5.py
- en: In this example, you will clearly see how the agent progresses in training from
    doing nothing to picking up and dropping off passengers in short order. The agent
    will actually perform remarkably better in this environment than in other apparently
    simpler environments such as the FrozenLake. This has more to do with the method
    of learning and the related task and suggests that we need to be careful as to
    what methods we use for which problems. You may find, in some cases, that certain
    advanced algorithms perform poorly on simple problems and that the converse may
    happen. That is, simpler algorithms such as Q-learning when paired with other
    technologies can become far more powerful as we will see in [Chapter 6](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml),
    *Going Deeper with DQN*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，你可以清楚地看到代理在训练中的进步，从什么都不做到迅速接载和放下乘客。实际上，代理在这个环境中会比在其他看似更简单的环境中（如FrozenLake）表现得更好。这更多与学习方法以及相关任务有关，这表明我们需要谨慎选择针对不同问题的方法。在某些情况下，你可能会发现某些高级算法在简单问题上的表现不佳，反之亦然。也就是说，像Q-learning这样的简单算法，当与其他技术结合时，可以变得非常强大，正如我们将在[第6章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)“深入DQN”中看到的。
- en: In the last section of this chapter, we look at how the previous Q-learning
    method could be improved.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将探讨如何改进之前的Q-learning方法。
- en: Running off- versus on-policy
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行离线策略与在线策略
- en: We covered the terms on- and off-policy previously when we looked at MC training
    in [Chapter 2](5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml), *Monte Carlo Methods*.
    Recall that the agent didn't update its policy until after an episode. Hence,
    this defines the TD(0) method of learning in the last example as an off-policy
    learner. In our last example, it may seem that the agent is learning online but
    it still, in fact, trains a policy or Q table externally. That is, the agent needs
    to build up a policy before it can learn to make decisions and play the game.
    Ideally, we want our agent to learn or improve its policy as it plays through
    an episode. After all, we don't learn offline nor does any other biological animal.
    Instead, our goal will be to understand how an agent can learn using on-policy
    learning. On-policy learning will be covered in [Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml),
    *Exploring SARSA*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看 [第2章](5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml) 中的 MC 训练时，我们之前已经讨论了
    on-policy 和 off-policy 的术语，*蒙特卡洛方法*。回想一下，代理直到一个回合结束后才更新其策略。因此，这在上一个例子中将 TD(0)
    学习方法定义为 off-policy 学习者。在我们的上一个例子中，可能看起来代理是在在线学习，但实际上它仍然在外部训练策略或 Q 表。也就是说，代理在学会做出决策和玩游戏之前需要建立策略。理想情况下，我们希望代理在玩一个回合的过程中学习或改进其策略。毕竟，我们不是离线学习，任何其他生物动物也不是。相反，我们的目标将是了解代理如何使用
    on-policy 学习来学习。on-policy 学习将在 [第5章](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml)
    *探索 SARSA* 中介绍。
- en: Exercises
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'As we progress through this book, the exercises at the end of each chapter
    will be more directed toward providing you with agent training experience. Training
    RL agents not only requires a fair amount of patience but also intuition on how
    to spot whether something is wrong or right. That only comes with training experience,
    so use the following exercises to learn that:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们这本书的进展，每章末尾的练习将更多地针对为你提供代理训练经验。训练 RL 代理不仅需要相当多的耐心，还需要对如何判断对错有直觉。这只有通过训练经验才能获得，所以请使用以下练习来学习：
- en: Open example `Chapter_4_2.py` and change the `gridSize` variable to see what
    effect this has on convergence.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开示例 `Chapter_4_2.py` 并将 `gridSize` 变量更改，以查看这将对收敛产生什么影响。
- en: Open example `Chapter_4_2.py` and tune the hyperparameters for alpha and gamma.
    Try to find the optimum values for both. This will require you to run the example
    multiple times.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开示例 `Chapter_4_2.py` 并调整 alpha 和 gamma 的超参数。尝试找到两者的最佳值。这需要你多次运行示例。
- en: Open example `Chapter_4_2.py` and change the number of episodes, up or down.
    See what effect a large number of episodes, such as 100,000 or 1,000,000, has
    on training.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开示例 `Chapter_4_2.py` 并更改回合数，增加或减少。看看大量回合，如 100,000 或 1,000,000，对训练有什么影响。
- en: Tune the `learning_rate` and `gamma` hyperparameters in example `Chapter_4_4.py`.
    Can they be improved upon?
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在示例 `Chapter_4_4.py` 中调整 `learning_rate` 和 `gamma` 超参数。它们是否可以改进？
- en: Adjust the exploration (`epsilon`, `max_epsilon`, `min_epsilon`, and `decay_rate`)
    hyperparameters from example `Chapter_4_4.py`. How does changing these values
    affect training performance or lack thereof?
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从示例 `Chapter_4_4.py` 中调整探索（`epsilon`、`max_epsilon`、`min_epsilon` 和 `decay_rate`）超参数。改变这些值会如何影响训练性能或缺乏性能？
- en: Tune the `learning_rate` and `gamma` hyperparameters in example `Chapter_4_5.py`.
    Can they be improved upon?
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在示例 `Chapter_4_5.py` 中调整 `learning_rate` 和 `gamma` 超参数。它们是否可以改进？
- en: Adjust the exploration (`epsilon`, `max_epsilon`, `min_epsilon`, and `decay_rate`)
    hyperparameters from example `Chapter_4_5.py`. How does changing these values
    affect training performance or lack thereof?
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从示例 `Chapter_4_5.py` 中调整探索（`epsilon`、`max_epsilon`、`min_epsilon` 和 `decay_rate`）超参数。改变这些值会如何影响训练性能或缺乏性能？
- en: Add the ability to track the deltas or change in Q values during training to
    examples `Chapter_4_4.py` or `Chapter_4_5.py`. Recall how we tracked and output
    the deltas to a plot in example `Chapter_4_2.py`.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将跟踪训练期间 delta 或 Q 值变化的能力添加到示例 `Chapter_4_4.py` 或 `Chapter_4_5.py` 中。回想一下，我们在示例
    `Chapter_4_2.py` 中是如何跟踪和输出 delta 到图的。
- en: Add the ability to render plots of the training performance to example `Chapter_4_4.py`
    or `Chapter_4_5.py`. This will require you to complete the previous exercise as
    well.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将绘制训练性能图的能力添加到示例 `Chapter_4_4.py` 或 `Chapter_4_5.py` 中。这需要你完成之前的练习。
- en: Use the code in example `Chapter_4_5.py` and try the Q-learner on other environments.
    The Mountain Car or Cart Pole environments from Gym are interesting and ones we
    will be exploring soon.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用示例 `Chapter_4_5.py` 中的代码，并在其他环境中尝试 Q-learner。Gym 中的 Mountain Car 或 Cart Pole
    环境很有趣，我们很快就会探索。
- en: 'At this stage in your RL training career, it is important to get how hyperparameters
    work. The right or wrong hyperparameters can make or truly break an experiment.
    This leaves you with two options: read a lot of boring math-induced papers or
    just do it. Since this is a hands-on book, we are expecting you prefer the latter.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的RL训练生涯的这个阶段，了解超参数的工作方式非常重要。正确或错误的选择超参数可能会使实验成功或彻底失败。这给你留下了两个选择：阅读大量枯燥的数学诱导论文，或者只是动手实践。由于这是一本实践性书籍，我们期待你更倾向于后者。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed how temporal difference learning, the third thread
    of RL, combined to develop TD(0) and Q-learning. We did that by first exploring
    the temporal credit assignment problem and how it differed from the credit assignment
    problem. From that, we learned how TD learning works and how TD(0) or first step
    TD can be reduced to Q-learning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何将强化学习（RL）的第三条线索——时序差分学习（Temporal Difference Learning）结合，以发展出TD(0)和Q-learning。我们首先探讨了时序信用分配问题及其与信用分配问题的区别。从那里，我们了解了TD学习的工作原理以及如何将TD(0)或第一步TD简化为Q-learning。
- en: After that, we again played on the FrozenLake environment to understand how
    the new algorithm compared to our past efforts. Using model-free off-policy Q-learning
    allowed us to tackle the more difficult Taxi environment problem. This is where
    we learned how to tune hyperparameters and finally looked at the difference between
    off- and on-policy learning. In the next chapter, we continue where we left off
    with on- versus off-policy as we explore SARSA.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们再次在FrozenLake环境中进行实验，以了解新算法与我们的过去努力相比如何。使用无模型离策略Q-learning使我们能够解决更困难的出租车环境问题。这就是我们学习如何调整超参数，并最终查看离策略和在线策略学习之间的差异的地方。在下一章中，我们将继续探讨在线策略与离策略，当我们探索SARSA时。
