- en: Temporal Difference Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our previous discussion on the history of reinforcement learning, we covered
    the two main threads, trial and error and **Dynamic Programming** (**DP**), which
    came together to derive current modern **Reinforcement Learning** (**RL**). As
    we mentioned in earlier chapters, there is also a third thread that arrived late
    called **Temporal Difference Learning** (**TDL**). In this chapter, we will explore
    TDL and how it solves the **Temporal Credit Assignment** (**TCA**) problem. From
    there, we will explore how TD differs from **Monte Carlo** (**MC**) and how it
    evolves to full Q-learning. After that, we will explore the differences between
    on-policy and off-policy learning and then, finally, work on a new example RL
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, we will introduce TDL and how it improves on the previous
    techniques we looked at in previous chapters. Here are the main topics we will
    cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the TCA problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing TDL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying TDL to Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring TD(0) in Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running off-policy versus on-policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter introduces TDL and Q-learning in detail. Therefore, it is worth
    reviewing and understanding the material well. Knowing the foundational material
    well will only ease your learning later.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the TCA problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The credit assignment problem is described as the task of understanding what
    actions you need to take to receive the most credit or, in the case of RL, rewards.
    RL solves the credit assignment problem by allowing an algorithm or agent to find
    the optimum set of actions to maximize the rewards. In all of our previous chapters,
    we have seen how variations of this can be done with DP and MC methods. However,
    both of these previous methods are offline, so they cannot learn while performing
    a task.
  prefs: []
  type: TYPE_NORMAL
- en: The TCA problem is differentiated from the credit assignment CA problem in that
    it needs to be solved across time; that is, an algorithm needs to find the best
    policy across time steps instead of learning after an episode, in the case of
    MC, or needing to plan before, as DP does. This also means that an algorithm that
    solves the CA problem across time can also or should be able to learn in real
    time, that is, be able to make updates to a policy during the progression of the
    task, rather than before or after as we have seen in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: By introducing the concept of time or a progression of events, we also allow
    our agent to learn the importance of event timing. Previously, our agents would
    have no awareness of time-critical events such as hitting a moving target or timing
    a running jump just right. TDL, on the other hand, allows an agent to understand
    event timing and take appropriate action. We will introduce the concept and intuition
    of TDL in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TDL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TDL was introduced by the father of RL himself, Dr. Richard Sutton, in 1988\.
    Sutton had developed the method as an improvement to MC/DP but, as we will see,
    the method itself led to the development of Q-learning by Chris Watkins in 1989\.
    The method itself is model-free and does not require episode completion before
    an agent learns. This makes this method very powerful for exploring unknown environments
    in real time, as we will see.
  prefs: []
  type: TYPE_NORMAL
- en: Before we get into discovering the updated mathematics to this approach, it
    may be helpful to look at the backup diagrams of all of the methods covered so
    far in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping and backup diagrams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TDL can learn during an episode by approximating the updated value function
    given previous experience. This allows the algorithm to learn while it is in an
    episode and hence make corrections as needed. To understand the differences further,
    let''s review a composite of the backup diagrams for DP, MC, and TD in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/730cb028-e6f9-469a-9d2e-5fdb56020163.png)'
  prefs: []
  type: TYPE_IMG
- en: Backup diagrams for DP, MC, and TDL
  prefs: []
  type: TYPE_NORMAL
- en: The diagram was taken from *An Introduction to Reinforcement Learning* by Barto
    and Sutton (2018). In the diagram, you can see our previous two methods, DP and
    MC, as well as TDL. The shaded area (red or black) denotes the algorithm's learning
    space. That is the area the agent will need to explore before being able to update
    its value function and, hence, policy. Notice how, as the diagrams progress from
    DP to TDL, the shaded area becomes smallerâ€”that is, for each advancing algorithm,
    that agent needs to explore less and less area initially or during an episode
    before learning. As we will see, this allows an agent to learn before even finishing
    an episode.
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at the code, we should take a look at how this new approach revises
    the math of our value function in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Applying TD prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we will explore methods for allowing an algorithm to predict
    and control an agent to complete a task. Prediction and control are at the heart
    of RL, and previously we had both methods separate. That is, they either ran before
    (DP) or after (MC). Now, for an agent to learn in real time, we need an online
    update rule that will update the value function after a designated time step.
    In TDL, this is called the TD update rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rule is shown here in equation form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/956df4fc-01db-4559-b254-7ad3413a6414.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7fc168a7-ace6-409c-97eb-86a90e1489e8.png): The value function for the
    current state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/023a6a65-0a6a-4a35-9459-bdc579e13a2e.png): Alpha, the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/373c9f31-ae6e-46bf-a01c-72958e5a7f97.png): The reward of the next state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/c76bca53-77df-4b0b-b907-7f32c82761b1.png): A discount factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/14dc93fa-c8d1-42f7-83a1-cfc226215e99.png): The value of the next state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, we can say that the value of the current state is equal to the value
    of the current state plus alpha, times the summation of the next reward, plus
    a discount factor, gamma, times the difference between the next state value and
    the current state value.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this better, let's look at a code example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: TD(0) or one-step TD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One thing we should identify before getting too far ahead is that the method
    we look at here is for one-step TD or what we refer to as TD(0). Remember, as
    programmers, we start counting at 0, so TD(0) essentially means TD one-step. We
    will look at multiple-step TD in [Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml),
    *Exploring SARSA*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, though, we will look at an example of using one-step TD in the next
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `Chapter_4_1.py` source code example, as seen in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is a straight code example demonstrating how value updates work and that
    uses no RL environment. The first section we will focus on is where we initialize
    our parameters just after the imports. Here, we initialize the learning rate,
    `alpha` (`0.5`); discount factor, `gamma` (`0.5`); the size of the environment,
    `gridSize` (`4`); a list of state `terminations`; list of `actions`; and finally,
    `episodes`. Actions represent the movement vector and terminations represent the
    grid square where an episode will terminate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we initialize the value function, `V`, with all zeros using the `numpy`
    function, `zeros`. We then create three lists, `returns`, `deltas`, and `states`
    using Python list comprehensions. These lists hold values for later retrieval
    and notice how this relates to a DP technique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we define some utility functions, `generateInitialState`, `generateNextAction`,
    and `takeAction`. The first two functions are self-explanatory but let''s focus
    on the `takeAction` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function takes the current `state` and `action` as inputs. It
    then determines whether the current state is terminal; if it is, it returns. Otherwise,
    it calculates the next state using simple vector math to get `finalState`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we enter the `for` loop that starts episodic training. Note that, although
    the agent explores the environment episodically, since the environment has a beginning
    and an end, it still learns temporally. That is, it learns after every time step.
    The `tqdm` library is a helper enumerator, which prints a status bar when we run
    a `for` loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first thing that happens at the start of the `for` loop is the agent''s
    state is initialized randomly. After that, it enters a `while` loop that runs
    one entire episode. Most of this code is self-explanatory, aside from the implementation
    of the value update equation, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The block of code is an implementation of the previous value update function.
    Notice the use of the learning rate, `alpha`, and discount factor, `gamma`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and notice the output of the `Value` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/19cbf135-fb34-45ec-8d9c-867c47f23945.png)'
  prefs: []
  type: TYPE_IMG
- en: Running example in Chapter_4_1.py
  prefs: []
  type: TYPE_NORMAL
- en: Now, the function is less than optimal and that has more to do with the training
    or hyperparameters we used. We will look at the importance of tuning hyperparameters
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The collection of training parameters that we investigated at the top of the
    last code example are called hyperparameters, so named to differentiate them from
    normal parameters or weights we use in deep learning. We have yet to look at deep
    learning in detail, but it is important to understand why they are called **hyperparameters**.
    Previously, we played around with the concept of a learning rate and discount
    factor but now we need to formalize them and understand their effect across methods
    and environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our last example, both the learning rate (alpha) and discount factor (gamma)
    were set to .5\. What we need to understand is what effect these parameters have
    on training. Let''s open up sample code `Chapter_4_2.py` and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Chapter_4_2.py` is almost identical to the previous example, aside from a
    few minor differences. The first of these is we `import matplotlib` here to be
    able to view some results later.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recall, you can install `matplotlib` from a Python console with the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We use `matplotlib` to render the results of our training efforts. As we progress
    through this book, we will look at more advanced methods later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we see that the hyperparameters, alpha and gamma, have been modified
    to values of `0.1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now, as you often refine training, you likely will only want to modify a single
    parameter at a time. This will give you more control and understanding of the
    effect a parameter may have.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, at the end of the file, we see the code to output the training values
    or change in training values. Recall the list we created earlier called `deltas`.
    Captured in this list are all of the deltas or changes made during training. This
    can be extremely useful to visualize, as we''ll see:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code just loops through the list of changes made during training for each
    episode. What we expect is that over training, the amount of change will reduce.
    Reducing the amount of change allows the agent to converge to some optimal value
    function and hence policy later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the code as you normally would and notice how the output of the value function
    has changed substantially but we can also see how the agent''s training progressed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/734a9474-69fe-4ded-a419-51551aa206c1.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_4_2.py
  prefs: []
  type: TYPE_NORMAL
- en: On the plot, you can now see how the training converges over time. The plot
    is represented over the number of steps in an episode. Notice how the amount of
    change or delta is greater to the left of the plot, with fewer steps in an episode,
    and then it decreases over time with more steps. This convergence assures us that
    the agent is indeed learning using the provided hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters is foundational to deep learning and deep reinforcement
    learning. Many consider the tuning practice to be where all of the work is done
    and, for the most part, that is true. You may often spend days, weeks, or months
    tuning hyperparameters of a single network model.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore playing with the hyperparameters on your own and see what
    effect each has on training convergence. In the next section, we look at how TD
    is combined with Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: Applying TDL to Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning is considered one of the most popular and often used foundational
    RL methods . The method itself was developed by Chris Watkins in 1989 as part
    of his thesis, *Learning from Delayed Rewards*. Q-learning or rather Deep Q-learning,
    which we will cover in [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml),
    *Going Deep with DQN*, became so popular because of its use by DeepMind (Google)
    to play classic Atari games better than a human. What Watkins did was show how
    an update could be applied across state-action pairs using a learning rate and
    discount factor gamma.
  prefs: []
  type: TYPE_NORMAL
- en: 'This improved the update equation into a Q or quality of state-action update
    equation, as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cc882f2-cb7a-4fb0-b25b-d2e7fd76b4d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous equation, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b4a101d-da73-40ed-adae-292972e81996.png)The current state-action quality
    being updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a570960a-0217-47dc-9587-489391546791.png)The learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/54152573-c230-41cb-bbe8-ba63f0480176.png)The reward for the next state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/04ee51c9-7562-452e-ad22-3a08624e7d44.png)Gamma, the discount factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2334cd52-d080-486f-b3e4-82e33576e28b.png)Take the max best or greedy
    action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This equation allows us to update state-action pairs based on learned future
    state-action pairs. It also does not require a model as the algorithm explores
    by trial and error and can learn during an episode since updates are run during
    the episode.
  prefs: []
  type: TYPE_NORMAL
- en: This method can now solve the temporal credit assignment problem and we will
    look at a code example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring TD(0) in Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TDL for first step or TD(0) then essentially simplifies to Q-learning. To do
    a full comparison of this method against DP and MC, we will first revisit the
    FrozenLake environment from Gym. Open up example code `Chapter_4_4.py` and follow
    the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full listing of code is too large to show. Instead, we will review the
    code in sections starting with the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We have seen all of these imports before, so there is nothing new here. Next,
    we cover the initialization of the environment and outputting some initial environment
    variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There''s nothing new here either. Next, we introduce the concept of a Q table
    or quality table that now defines our policy in terms of `state-action` pairs.
    We set this to an equal quality for each `state-action` pair by dividing one by
    the total number of actions in a state (`action-size`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can see a section of hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There are two new parameters here called `play_game_test_episode` and `max_steps`.
    `max_steps` determine the number of maximum steps our algorithm may run in an
    episode. We do this to limit the agent from getting into possible endless loops.
    `play_game_test_episode` sets the episode number to show a preview of the agent
    playing based on the current best Q table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we introduce an entirely new set of parameters that have to deal with
    exploration and exploitation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Recall that we discussed the exploration versus exploitation dilemma in RL in
    [Chapter 1](5553d896-c079-4404-a41b-c25293c745bb.xhtml), *Understanding Rewards-Based
    Learning*. In this section, we introduce `epsilon`, `max_epsilon`, `min_epsilon`,
    and `decay_rate`. These hyperparameters control the exploration rate of the agent
    while it explores, where `epsilon` is the current probability of an agent exploring
    during a time step. Maximum and minimum epsilon represent the limits of how much
    or little the agent explores, with `decay_rate` controlling how much the `epsilon`
    value decays from time step to step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding the exploration versus exploitation dilemma is essential to RL
    so we will pause here and let you run the example. The following is an example
    of the agent playing on the FrozenLake environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8ef78387-241b-43d5-87a5-0d066bee52dd.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_4_4.py
  prefs: []
  type: TYPE_NORMAL
- en: Watch how the agent plays the game in the later episodes, after 40,000, and
    you will see that the agent can move around the holes and find the goal in short
    order, usually. The reason it may not do this has to do with exploration/exploitation
    and something we will review again in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration versus exploitation revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For several chapters now, we have always assumed our agent to be greedy. That
    is, it always chooses the best action given a choice in policy. However, as we
    have seen, this does not always provide the best path to optimum reward. Instead,
    what we find is that by allowing an agent to randomly explore early and then over
    time reduce the chance of exploration, there is a substantial improvement in learning.
    Except, if the environment is too large or complex, an agent may need more exploration
    time compared to a much smaller environment. If we maintained high exploration
    in a small environment, our agent would just waste time exploring. This is the
    trade-off you need to balance and it is often tied to the task required to perform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The method of exploration we are using in this example is called e-greedy or
    epsilon-greedy. It''s so named because we start out greedy with high epsilon and
    then decrease it over time. There are several exploration methods we may use and
    a list of the more common versions and a description of each is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random**: This is the always random method, which can be an effective baseline
    test to perform on a new environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Greedy**: This is always taking the greedy or best action in a state. As
    we have seen, this can have bad consequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**E-greedy**: Epsilon greedy allows for a method to balance exploration rate
    over time by decreasing epsilon (exploration rate) by a factor during each time
    step or episode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bayesian or Thompson sampling**: This uses statistics and probability to
    best choose an action over a random distribution of sampled actions. Essentially,
    the action is chosen from across action distributions. For example, if a state
    has a bag of actions to choose from, each bag then also stores previous rewards
    for each action. A new action is chosen by taking a random choice from each action
    bag and comparing it to all of the other choices. The best action, the one giving
    the highest value, is selected. We don''t actually store all of the rewards for
    all previous actions but, rather, we determine the sample distribution that describes
    these returned rewards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the concepts of statistics and probability we just discussed are foreign
    to you, then you should engage in some free online learning of these topics. These
    concepts will be covered over and over again in-depth in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: There are other methods that provide additional options for strategies on how
    to pick the best action for a Q-learner. For now, though, we will stick to e-greedy
    as it is relatively simple to implement and is quite effective.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you go back to example `Chapter_4_4.py` and watch closely, you will
    see the agent may reach the goal, or it may not. In fact, the FrozenLake environment
    is more treacherous than we give it credit for. What that means is that rewards
    in that environment are more sparse and it often takes considerably longer to
    train with trial and error techniques. Instead, this type of learning method performs
    better in environments with continuous rewards. That is, when an agent receives
    a reward during an episode and not just at termination.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, Gym has plenty of environments we can play with that will allow
    us more continuous rewards and we will explore a fun example in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Teaching an agent to drive a taxi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym has plenty of fun environments that allow us to switch out and test
    new environments very easily. This, as we have seen, allows us to compare results
    of algorithms far easier. However, as we have also seen, there are limitations
    to various algorithms, and the new environment we explore in this section introduces
    the limitation of time. That is, it places a time limit on the agent as part of
    the goal. In doing this, our previous algorithms, DP and MC, become unable to
    solve such a problem, which makes this a good example to also introduce time-based
    or time-critical rewards.
  prefs: []
  type: TYPE_NORMAL
- en: What better way to introduce time-dependent learning than to think of a time-dependent
    task? There are plenty of tasks, but one that works well is an example of a taxi.
    That is, the agent is a taxi driver and must pick up passengers and drop them
    off at their correct destinations but promptly. In the Gym Taxi-v2 environment,
    it is the goal of the agent to pick up a passenger at a location and then drop
    them off at a goal. The agent gets a reward of +20 points for a successful drop
    off and -1 point for every time step it takes. Hence, the agent needs to pick
    up and drop off passengers as quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of an agent playing this environment is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7020394-2df8-466f-bbac-28cb0be8e588.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from the Taxi-v2 Gym environment
  prefs: []
  type: TYPE_NORMAL
- en: In the screenshot, the car is green, meaning it has picked up a passenger from
    one of the symbols. The current goal is for the agent to drop off the passenger
    at their designated goal (symbol) highlighted. Let's now go back to the code,
    this time, to `Chapter_4_5.py`, which is an updated version of our last example
    now using the Taxi-v2 environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have the code opened, follow the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This code example is virtually identical to `Chapter_4_4.py`, aside from the
    initialization of the environment shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use all of the same hyperparameters as before, so there''s no need
    to look at those again. Instead, skip down to the `play_game` function, as shown
    in the following code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The `play_game` function essentially uses the `qtable` list, which is essentially
    the generated policy of qualities for state-action pairs. The code should be comfortable
    now and one detail to notice is how the agent selects an action from the `qtable`
    list using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `play_game` function here takes the role of our agent test function previously.
    This function will allow you to see the agent play the game as it progresses through
    training. This is accomplished by setting `render_game` to `play_game` to `True`.
    Doing this allows you to visualize the agent playing an episode of the game.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will often want to watch how your agent is training, at least initially.
    This can provide you with clues as to possible errors in your implementation or
    the agent finding possible cheats in new environments. We have found agents to
    be very good cheaters or at least be able to find cheats quite readily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we jump down to the next for loop that iterates through the training
    episodes and trains the `qtable`. When a threshold set by `play_game_test_episode`
    of episodes has elapsed, we allow the agent to play a visible game. Doing this
    allows us to visualize the overall training progress. However, it is important
    to remember that this is only a single episode and the agent could be doing extensive
    exploration. So, it is important to remember that, when watching the agent, they
    may just occasionally randomly explore. The code shows how we loop through the
    episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'First, inside the episode loop, we handle the exploration-exploitation dilemma
    by sampling a random value and then comparing it to epsilon. If it is greater
    than the greedy action, it is selected; otherwise, a random exploratory action
    is selected, as shown in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the next line is where the step is taken with the selected action. After
    that, `qtable` is updated based on the previous Q-learning equation. There is
    a lot going on in this single line of code, so make sure you understand it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we check whether the episode is done with the `done` flag. If it
    is, we terminate and continue with another episode. Otherwise, we update the value
    of `epsilon` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the remainder of the code is shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The last piece of code resets and then tests the environment with the trained
    `qtable` for `total_test_episodes` and then outputs the average score or reward
    for an episode.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, run the code as you normally would and observe the output carefully.
    Pay particular attention to how the taxi picks up and drops off passengers in
    later episodes. Sample output from the training is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7ca331b0-d030-4496-a285-75ec1f378d60.png)'
  prefs: []
  type: TYPE_IMG
- en: The final output from sample Chapter_4_5.py
  prefs: []
  type: TYPE_NORMAL
- en: In this example, you will clearly see how the agent progresses in training from
    doing nothing to picking up and dropping off passengers in short order. The agent
    will actually perform remarkably better in this environment than in other apparently
    simpler environments such as the FrozenLake. This has more to do with the method
    of learning and the related task and suggests that we need to be careful as to
    what methods we use for which problems. You may find, in some cases, that certain
    advanced algorithms perform poorly on simple problems and that the converse may
    happen. That is, simpler algorithms such as Q-learning when paired with other
    technologies can become far more powerful as we will see in [Chapter 6](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml),
    *Going Deeper with DQN*.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this chapter, we look at how the previous Q-learning
    method could be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Running off- versus on-policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered the terms on- and off-policy previously when we looked at MC training
    in [Chapter 2](5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml), *Monte Carlo Methods*.
    Recall that the agent didn't update its policy until after an episode. Hence,
    this defines the TD(0) method of learning in the last example as an off-policy
    learner. In our last example, it may seem that the agent is learning online but
    it still, in fact, trains a policy or Q table externally. That is, the agent needs
    to build up a policy before it can learn to make decisions and play the game.
    Ideally, we want our agent to learn or improve its policy as it plays through
    an episode. After all, we don't learn offline nor does any other biological animal.
    Instead, our goal will be to understand how an agent can learn using on-policy
    learning. On-policy learning will be covered in [Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml),
    *Exploring SARSA*.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we progress through this book, the exercises at the end of each chapter
    will be more directed toward providing you with agent training experience. Training
    RL agents not only requires a fair amount of patience but also intuition on how
    to spot whether something is wrong or right. That only comes with training experience,
    so use the following exercises to learn that:'
  prefs: []
  type: TYPE_NORMAL
- en: Open example `Chapter_4_2.py` and change the `gridSize` variable to see what
    effect this has on convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open example `Chapter_4_2.py` and tune the hyperparameters for alpha and gamma.
    Try to find the optimum values for both. This will require you to run the example
    multiple times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open example `Chapter_4_2.py` and change the number of episodes, up or down.
    See what effect a large number of episodes, such as 100,000 or 1,000,000, has
    on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the `learning_rate` and `gamma` hyperparameters in example `Chapter_4_4.py`.
    Can they be improved upon?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the exploration (`epsilon`, `max_epsilon`, `min_epsilon`, and `decay_rate`)
    hyperparameters from example `Chapter_4_4.py`. How does changing these values
    affect training performance or lack thereof?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the `learning_rate` and `gamma` hyperparameters in example `Chapter_4_5.py`.
    Can they be improved upon?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the exploration (`epsilon`, `max_epsilon`, `min_epsilon`, and `decay_rate`)
    hyperparameters from example `Chapter_4_5.py`. How does changing these values
    affect training performance or lack thereof?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to track the deltas or change in Q values during training to
    examples `Chapter_4_4.py` or `Chapter_4_5.py`. Recall how we tracked and output
    the deltas to a plot in example `Chapter_4_2.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to render plots of the training performance to example `Chapter_4_4.py`
    or `Chapter_4_5.py`. This will require you to complete the previous exercise as
    well.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the code in example `Chapter_4_5.py` and try the Q-learner on other environments.
    The Mountain Car or Cart Pole environments from Gym are interesting and ones we
    will be exploring soon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At this stage in your RL training career, it is important to get how hyperparameters
    work. The right or wrong hyperparameters can make or truly break an experiment.
    This leaves you with two options: read a lot of boring math-induced papers or
    just do it. Since this is a hands-on book, we are expecting you prefer the latter.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed how temporal difference learning, the third thread
    of RL, combined to develop TD(0) and Q-learning. We did that by first exploring
    the temporal credit assignment problem and how it differed from the credit assignment
    problem. From that, we learned how TD learning works and how TD(0) or first step
    TD can be reduced to Q-learning.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we again played on the FrozenLake environment to understand how
    the new algorithm compared to our past efforts. Using model-free off-policy Q-learning
    allowed us to tackle the more difficult Taxi environment problem. This is where
    we learned how to tune hyperparameters and finally looked at the difference between
    off- and on-policy learning. In the next chapter, we continue where we left off
    with on- versus off-policy as we explore SARSA.
  prefs: []
  type: TYPE_NORMAL
