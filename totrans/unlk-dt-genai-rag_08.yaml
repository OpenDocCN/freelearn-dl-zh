- en: <st c="0">8</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="0">8</st>
- en: <st c="2">Similarity Searching with Vectors</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2">基于向量的相似性搜索</st>
- en: <st c="35">This chapter is all about the</st> **<st c="66">R</st>** <st c="67">or</st>
    **<st c="71">retrieval</st>** <st c="80">part of</st> **<st c="89">retrieval-augmented
    generation</st>** <st c="119">(</st>**<st c="121">RAG</st>**<st c="124">).</st>
    <st c="128">Specifically, we are going to talk about four areas related to similarity
    searches:</st> **<st c="212">indexing</st>**<st c="220">,</st> **<st c="222">distance
    metrics</st>**<st c="238">,</st> **<st c="240">similarity algorithms</st>**<st
    c="261">, and</st> **<st c="267">vector search services</st>**<st c="289">. With
    this in mind, in this chapter, we will cover</st> <st c="341">the following:</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35">本章全部关于检索-augmented generation</st> **<st c="66">R</st>** <st c="67">或</st>
    **<st c="71">检索</st> **<st c="80">部分</st> **<st c="89">检索增强生成</st> **<st c="119">(**<st
    c="121">RAG</st>**<st c="124">).</st> <st c="128">具体来说，我们将讨论与相似性搜索相关的四个领域：**<st
    c="212">索引</st>**<st c="220">，**<st c="222">距离度量</st>**<st c="238">，**<st c="240">相似性算法</st>**<st
    c="261">，以及**<st c="267">向量搜索服务</st>**<st c="289">。考虑到这一点，在本章中，我们将涵盖以下内容：</st>
    <st c="341">以下内容：</st>
- en: <st c="355">Distance metrics versus similarity algorithms versus</st> <st c="409">vector
    search</st>
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="355">距离度量与相似性算法与</st> <st c="409">向量搜索</st>
- en: <st c="422">Vector space</st>
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="422">向量空间</st>
- en: <st c="435">Code lab 8.1 – Semantic</st> <st c="460">distance metrics</st>
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="435">代码实验室8.1 – 语义</st> <st c="460">距离度量</st>
- en: <st c="476">Different search paradigms – sparse, dense,</st> <st c="521">and
    hybrid</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="476">不同的搜索范式 – 稀疏的、密集的，以及混合的</st>
- en: <st c="531">Code lab 8.2 – Hybrid search with a</st> <st c="568">custom function</st>
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="531">代码实验室8.2 – 使用自定义函数进行</st> <st c="568">混合搜索</st>
- en: <st c="583">Code lab 8.3 – Hybrid search with</st> <st c="618">LangChain’s EnsembleRetriever</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="583">代码实验室8.3 – 使用LangChain的EnsembleRetriever进行混合搜索</st> <st c="618">混合搜索</st>
- en: <st c="647">Semantic search algorithms such as k-NN</st> <st c="688">and ANN</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="647">如k-NN</st> <st c="688">和ANN</st> <st c="716">的语义搜索算法</st>
- en: <st c="695">Indexing techniques that enhance ANN</st> <st c="733">search efficiency</st>
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="695">增强ANN搜索效率的索引技术</st> <st c="733">搜索效率</st>
- en: <st c="750">Vector</st> <st c="758">search options</st>
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="750">向量</st> <st c="758">搜索选项</st>
- en: <st c="772">By the end of this chapter, you should have a comprehensive understanding
    of how vector-based similarity searching operates and why it’s instrumental for
    the retrieval component in</st> <st c="954">RAG systems.</st>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="772">在本章结束时，你应该对基于向量的相似性搜索如何运作以及为什么它在RAG系统中的检索组件中起着至关重要的作用有一个全面的理解。</st>
    <st c="954">RAG系统。</st>
- en: <st c="966">Technical requirements</st>
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="966">技术要求</st>
- en: <st c="989">The code for this chapter is placed in the following GitHub</st>
    <st c="1050">repository:</st> [<st c="1062">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_08</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_08
    )
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="989">本章的代码放置在以下GitHub</st> <st c="1050">仓库中：</st> [<st c="1062">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_08</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_08
    )
- en: <st c="1159">Individual file names for each code lab are mentioned in the</st>
    <st c="1221">respective sections.</st>
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1159">每个代码实验室的单独文件名在各自的章节中提到。</st>
- en: <st c="1241">Distance metrics versus similarity algorithms versus vector search</st>
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1241">距离度量与相似性算法与向量搜索</st>
- en: <st c="1308">First, let’s distinguish the difference between distance metrics,
    similarity algorithms, and vector search.</st> <st c="1417">A similarity algorithm
    can use different distance metrics, whereas a vector search can use different
    similarity algorithms.</st> <st c="1541">They are all different concepts that
    ultimately form the retrieval component of your RAG system.</st> <st c="1638">It
    is important to make the distinction between these concepts serving different
    purposes if you are going to understand how to properly implement and optimize
    your retrieval solution.</st> <st c="1823">You can think of this as a hierarchy,
    as shown in</st> *<st c="1873">Figure 8</st>**<st c="1881">.1</st>*<st c="1883">:</st>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1308">首先，让我们区分距离度量、相似性算法和向量搜索之间的区别。</st> <st c="1417">相似性算法可以使用不同的距离度量，而向量搜索可以使用不同的相似性算法。</st>
    <st c="1541">它们都是不同的概念，最终构成了你的RAG系统的检索组件。</st> <st c="1638">如果你打算理解如何正确实施和优化你的检索解决方案，区分这些服务于不同目的的概念是很重要的。</st>
    <st c="1823">你可以将其视为一个层次结构，如图**<st c="1873">图8</st>**<st c="1881">.1</st>**<st
    c="1883">所示：</st>
- en: '![Figure 8.1 – Vector store, similarity algorithm, and distance metric hierarchy
    for two options each](img/B22475_08_01.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 每个选项的向量存储、相似性算法和距离度量层次结构](img/B22475_08_01.jpg)'
- en: <st c="2007">Figure 8.1 – Vector store, similarity algorithm, and distance metric
    hierarchy for two options each</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2007">图8.1 – 每个选项的向量存储、相似性算法和距离度量层次结构</st>
- en: <st c="2106">In</st> *<st c="2110">Figure 8</st>**<st c="2118">.1</st>*<st c="2120">,
    we are</st> <st c="2128">only demonstrating two options for each, where each vector
    search has two different options for similarity algorithms, and then each similarity
    algorithm has two different options for distance metrics.</st> <st c="2331">In
    reality, though, there are many more options at</st> <st c="2382">each level.</st>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2106">在</st> *<st c="2110">图8</st>**<st c="2118">.1</st>*<st c="2120">中，我们只展示了每个选项的两个选项，其中每个向量搜索都有两种不同的相似性算法选项，然后每个相似性算法都有两种不同的距离度量选项。</st>
    <st c="2331">然而，实际上在每个层面上都有更多的选项。</st>
- en: <st c="2393">The key point here is that these terms are often used interchangeably
    or together as if they are the same thing, but they are very different parts of
    the overall similarity search mechanism.</st> <st c="2585">If you make the mistake
    of confusing them, it makes it much more difficult to understand the overall concepts
    behind</st> <st c="2702">similarity search.</st>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2393">这里的要点是，这些术语通常被互换使用或一起使用，好像它们是同一件事，但实际上它们是整体相似性搜索机制中非常不同的部分。</st>
    <st c="2585">如果你犯了一个混淆它们的错误，那么理解相似性搜索背后的整体概念就会变得困难得多。</st> <st c="2702">相似性搜索。</st>
- en: <st c="2720">Now that we have cleared that up, we will talk about another concept
    that can help you understand the underpinnings of how similarity search works,
    the</st> <st c="2873">vector space.</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2720">现在我们已经澄清了这一点，我们将讨论另一个可以帮助你理解相似性搜索工作基础的概念，即</st> <st c="2873">向量空间。</st>
- en: <st c="2886">Vector space</st>
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="2886">向量空间</st>
- en: <st c="2899">The</st> <st c="2904">concept of a</st> **<st c="2917">vector space</st>**
    <st c="2929">is highly related to the vector similarity search, as the search
    is conducted within the vector space represented by the vectors.</st> <st c="3060">Technically,
    a vector space is a mathematical construct that represents a collection of vectors
    in a high-dimensional space.</st> <st c="3185">The dimensions of the vector space
    correspond to the number of features or attributes associated with each vector.</st>
    <st c="3300">In this space, the vectors of text that are most similar have more
    similar embeddings and, therefore, are located closer to each other in the space.</st>
    <st c="3449">You will hear the concept of vector space referred to often when
    talking in more technical ways about similarity searches.</st> <st c="3572">Other</st>
    <st c="3578">common</st> <st c="3585">names for this</st> *<st c="3600">space</st>*
    <st c="3605">are</st> **<st c="3610">embedding space</st>** <st c="3625">or</st>
    **<st c="3629">latent space</st>**<st c="3641">.</st>
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2899">向量空间的概念与向量相似性搜索高度相关，因为搜索是在由向量表示的向量空间内进行的。</st> <st c="2929">技术上讲，向量空间是一个数学结构，它表示一个高维空间中向量的集合。</st>
    <st c="3185">向量空间的维度对应于与每个向量关联的特征或属性的数量。</st> <st c="3300">在这个空间中，最相似的文本向量具有更相似的嵌入，因此它们在空间中彼此更接近。</st>
    <st c="3449">当以更技术性的方式讨论相似性搜索时，你经常会听到向量空间的概念。</st> <st c="3572">其他</st> <st c="3578">常见的</st>
    <st c="3585">这个</st> *<st c="3600">空间</st>* <st c="3605">的</st> <st c="3610">名称</st>
    <st c="3625">是**<st c="3629">嵌入空间</st>** <st c="3641">或**<st c="3629">潜在空间</st>**。</st>
- en: <st c="3642">The concept of vector space can be helpful in visualizing how the
    distance algorithms that find the nearest vectors to our user query embedding
    are working.</st> <st c="3800">Ignoring the fact that these vectors are sometimes
    thousands of dimensions, we can picture them in a 2D space with its outer limits
    defined by the vectors in them, and the data points (little dots can be seen in
    the free PDF version) representing each of those vectors (see</st> <st c="4074">Figure
    8</st><st c="4082">.2).</st> <st c="4087">There are little clusters of data points
    (small dots) in various places representing semantic similarities across the different
    data points.</st> <st c="4228">When a search happens, a new query (X) appears
    in this imaginary space based on the user query vector dimensions, and the data
    points (little dots) that are closest to that query (X) are going to be the results
    of our retriever-orchestrated similarity search.</st> <st c="4488">We take all
    of the data points (small dots) that we are going to retrieve in the search result,
    and we turn them into query results (</st><st c="4621">large dots):</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3642">向量空间的概念有助于可视化寻找与用户查询嵌入最近的向量距离算法的工作方式。</st> <st c="3800">忽略这些向量有时是数千维的事实，我们可以将它们想象在一个二维空间中，其外限由其中的向量定义，并且数据点（在免费PDF版本中可以看到的小点）代表每个向量（见图8.2）。</st>
    <st c="4074">在各个地方都有代表不同数据点之间语义相似性的小数据点（小点）簇。</st> <st c="4228">当发生搜索时，一个新的查询（X）根据用户查询向量的维度出现在这个想象空间中，并且与该查询（X）最近的那些数据点（小点）将成为我们检索器协调的相似性搜索的结果。</st>
    <st c="4488">我们检索搜索结果中所有将要检索的数据点（小点），并将它们转换为查询结果（</st><st c="4621">大点）：</st>
- en: "![Figure 8.2 – \uFEFF2D representation of embeddings in a vector space with\
    \ the X representing a query and large dots representing the closest embeddings\
    \ from the dataset](img/B22475_08_02.jpg)"
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![Figure 8.2 – 2D representation of embeddings in a vector space with the X
    representing a query and large dots representing the closest embeddings from the
    dataset](img/B22475_08_02.jpg)'
- en: <st c="4719">Figure 8.2 – 2D representation of embeddings in a vector space
    with the X representing a query and large dots representing the closest embeddings
    from the dataset</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4719">图8.2 – 2D表示向量空间中的嵌入，其中X代表查询，大点代表数据集中最近的嵌入</st>
- en: <st c="4881">Let’s talk</st> <st c="4893">through what we see here.</st> <st
    c="4919">There are four results from the query (the large dots).</st> <st c="4975">From
    our vantage point, in this 2D space, it looks like there are data points (small
    dots) that are closer to the query (X) than the query results (large dots).</st>
    <st c="5136">Why is that?</st> <st c="5149">You may remember that these dots were
    originally in a 1,536D space.</st> <st c="5217">So if you imagine just adding
    one more dimension (height), where the dots spread toward you right out of this
    page, those query results (large dots) may actually be closer because they are
    all much higher than the data points (small dots) that seem closer.</st> <st c="5474">Looking
    straight down at them, some data points (small dots) may appear closer, but it
    is a mathematical certainty that the query results (large dots) are the closer
    ones when taking all dimensions into account.</st> <st c="5686">Expand your space
    to all 1,536 dimensions, and this situation becomes even more likely.</st>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4881">让我们来谈谈我们在这里看到的内容。</st> <st c="4893">查询（大点）有四个结果。</st> <st c="4975">从我们的视角来看，在这个二维空间中，看起来有一些数据点（小点）比查询结果（大点）更接近查询（X）。</st>
    <st c="5136">为什么是这样呢？</st> <st c="5149">你可能记得这些点最初是在一个1536维空间中。</st> <st c="5217">所以，如果你想象只是增加一个维度（高度），其中点从页面中向外扩散到你这里，那些查询结果（大点）实际上可能更近，因为它们都远高于看起来更近的数据点（小点）。</st>
    <st c="5474">直接从上方看它们，一些数据点（小点）可能看起来更近，但数学上可以确定，当考虑所有维度时，查询结果（大点）是更接近的。</st> <st
    c="5686">将你的空间扩展到所有1536维，这种情况变得更加可能。</st>
- en: <st c="5774">Semantic versus keyword search</st>
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="5774">语义搜索与关键词搜索</st>
- en: <st c="5804">As we’ve already</st> <st c="5821">said many times, vectors capture
    the meaning behind our data in a mathematical representation.</st> <st c="5917">To
    find data points similar in meaning to a user query, we can search and retrieve
    the closest objects in a vector space such as the one we just showed.</st> <st
    c="6070">This is known as</st> **<st c="6087">semantic</st>** <st c="6095">or</st>
    **<st c="6099">vector search</st>**<st c="6112">. A semantic</st> <st c="6125">search,
    as opposed to keyword matching, is searching for documents that have similar semantic
    meaning, rather than just the same words.</st> <st c="6261">As humans, we can
    say the same or similar things in so many different ways!</st> <st c="6337">Semantic
    search can capture that aspect of our language because it assigns similar mathematical
    values to similar concepts, whereas keyword search focuses on specific word matching
    and often misses similar semantic meanings partially</st> <st c="6571">or entirely.</st>
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6583">From a technical standpoint, semantic search utilizes the meaning
    of the documents we have vectorized that is mathematically embedded in the vector
    that represents it.</st> <st c="6752">For math enthusiasts, you have to recognize
    the beauty of using a mathematical solution to solve</st> <st c="6849">linguistic
    challenges!</st>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6871">Let’s walk through an example to highlight how semantic</st> <st
    c="6928">search works.</st>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6941">Semantic search example</st>
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="6965">Think of a</st> <st c="6977">simple example of semantic similarity,
    such as a review of a blanket product online where one customer says</st> <st
    c="7085">the following:</st>
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="7099">This blanket does such a great job maintaining a high cozy temperature</st>`
    `<st c="7171">for me!</st>`'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7178">Another customer</st> <st c="7196">says this:</st>
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="7206">I am so much warmer and snug using</st>` `<st c="7242">this spread!</st>`'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7254">While they are saying relatively similar things semantically, a
    keyword search would not grade this nearly as similar as a semantic search would.</st>
    <st c="7401">Here, we introduce a third sentence representing a random comment
    online</st> <st c="7474">for comparison:</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="7489">Taylor Swift was 34 years old</st>` `<st c="7520">in 2024.</st>`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7528">The semantics of this random online comment are considered quite
    different from either of the last two sentences.</st> <st c="7643">But let’s not
    take my word for it, let’s do the math in a notebook!</st> <st c="7711">In the
    following code, we will review some of the most common distance metrics used as
    a foundational element of</st> <st c="7824">semantic search.</st>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7840">Code lab 8.1 – Semantic distance metrics</st>
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="7881">The file you need to access from the GitHub repository is</st>
    <st c="7940">titled</st> `<st c="7947">CHAPTER8-1_DISTANCEMETRICS.ipynb</st>`<st
    c="7979">.</st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7980">Our first code</st> <st c="7995">lab in this chapter will focus
    on the different ways you can calculate the distance between vectors, giving you
    a hands-on view of the difference between each of these approaches.</st> <st c="8176">We
    will use a brand new notebook called</st> `<st c="8216">CHAPTER8-1_DISTANCEMETRICS.ipynb</st>`
    <st c="8248">that has separate code from what we have used up to this point.</st>
    <st c="8313">We will install and import the packages we need, create the embeddings
    for the sentences we discussed, and then we will step through three types of distance
    metric formulas that are very common in NLP, generative AI, and</st> <st c="8534">RAG
    systems.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7980">本章的第一个代码</st> <st c="7995">实验室将专注于您计算向量之间距离的不同方法，让您亲身体验这些方法之间的差异。</st>
    <st c="8176">我们将使用一个全新的笔记本</st> `<st c="8216">CHAPTER8-1_DISTANCEMETRICS.ipynb</st>`
    <st c="8248">，其中包含与我们迄今为止使用的代码不同的代码。</st> <st c="8313">我们将安装并导入所需的包，为所讨论的句子创建嵌入，然后我们将逐步介绍三种在NLP、生成式AI和</st>
    <st c="8534">RAG系统中非常常见的距离度量公式。</st>
- en: <st c="8546">We first install the open source</st> `<st c="8580">sentence_transformers</st>`
    <st c="8601">library that will set up our</st> <st c="8631">embedding algorithm:</st>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8546">我们首先安装开源的</st> `<st c="8580">sentence_transformers</st>` <st c="8601">库，这将设置我们的</st>
    <st c="8631">嵌入算法：</st>
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="8696">The</st> `<st c="8701">sentence_transformers</st>` <st c="8722">package
    provides an easy way to compute dense vector representations for sentences and
    paragraphs.</st> <st c="8822">Next, we import some select packages that will aid
    our efforts to</st> <st c="8888">measure distance:</st>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8696">`<st c="8701">sentence_transformers</st>` <st c="8722">包提供了一个简单的方法来计算句子和段落的密集向量表示。</st>
    <st c="8822">接下来，我们导入一些有助于我们</st> <st c="8888">测量距离</st> <st c="8890">的精选包：</st>
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="8977">Here, we add the popular NumPy library that will provide the mathematical
    operations we need to perform our analysis of distances.</st> <st c="9109">As
    mentioned previously,</st> `<st c="9134">sentence_transformers</st>` <st c="9155">is
    imported so that we can create dense vector representations for our text.</st>
    <st c="9233">This will give us the ability to create instances of a pre-trained</st>
    <st c="9300">embedding model.</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8977">在这里，我们添加了流行的NumPy库，它将提供我们执行距离分析所需的数学运算。</st> <st c="9109">如前所述，</st>
    `<st c="9134">sentence_transformers</st>` <st c="9155">被导入，以便我们可以为我们的文本创建密集向量表示。</st>
    <st c="9233">这将使我们能够创建预训练嵌入模型的实例。</st>
- en: <st c="9316">In this next line, we define the transformer model we want</st>
    <st c="9376">to use:</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9316">在下一行中，我们定义了我们想要</st> <st c="9376">使用的转换器模型：</st>
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="9438">This</st> `<st c="9444">'paraphrase-MiniLM-L6-v2'</st>` <st c="9469">model
    is one of the smaller models available through this package, which will hopefully
    make it more compatible across more computer environments that you may be using
    this code on.</st> <st c="9652">If you want something more powerful, try the</st>
    `<st c="9697">'all-mpnet-base-v2'</st>` <st c="9716">model, where the semantic
    search performance scores around</st> <st c="9776">50% higher.</st>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9438">这个</st> `<st c="9444">'paraphrase-MiniLM-L6-v2'</st>` <st c="9469">模型是此包中可用的较小模型之一，希望它能与您可能在此代码上使用的更多计算机环境兼容。</st>
    <st c="9652">如果您需要更强大的功能，请尝试</st> `<st c="9697">'all-mpnet-base-v2'</st>` <st
    c="9716">模型，其语义搜索性能评分大约</st> <st c="9776">高出50%。</st>
- en: <st c="9787">We will take the sentences we mentioned previously and add them
    to a list we can reference in</st> <st c="9882">our code:</st>
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9787">我们将把之前提到的句子添加到我们可以在</st> <st c="9882">代码中引用的列表中：</st>
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="10048">We then</st> <st c="10056">encode the sentences using our</st>
    <st c="10088">SentenceTransformer model:</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10048">然后</st> <st c="10056">我们使用我们的</st> <st c="10088">SentenceTransformer模型</st>
    <st c="10096">对句子进行编码：</st>
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="10182">The</st> `<st c="10187">model.encode</st>` <st c="10199">function
    takes a list of strings and converts them into a list of embeddings.</st> <st
    c="10278">Our output shows us the mathematical representations (vectors) of</st>
    <st c="10344">our sentences:</st>
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10182">`<st c="10187">model.encode</st>` <st c="10199">函数接受一个字符串列表，并将其转换为嵌入列表。</st>
    <st c="10278">我们的输出显示了我们的句子</st> <st c="10344">的数学表示（向量）：</st>
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="10585">You’ll notice</st> `<st c="10600">(3, 384)</st>` <st c="10608">coming
    from the</st> `<st c="10625">embedding.shape</st>` <st c="10640">function.</st>
    <st c="10651">Do you remember what that is telling us?</st> <st c="10692">It says
    we have three vectors, all of which are 384 dimensions.</st> <st c="10756">So
    now we know this particular SentenceTransformer model provides vectors</st> <st
    c="10830">in 384D!</st>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10585">你会注意到</st> `<st c="10600">(3, 384)</st>` <st c="10608">来自</st>
    `<st c="10625">embedding.shape</st>` <st c="10640">函数。</st> <st c="10651">你还记得那是什么意思吗？</st>
    <st c="10692">它告诉我们我们有三个向量，它们都是384维。</st> <st c="10756">所以现在我们知道这个特定的SentenceTransformer模型提供的是384D的向量！</st>
- en: <st c="10838">Fun fact</st>
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10838">有趣的事实</st>
- en: <st c="10847">You may be wondering whether you can use the</st> `<st c="10893">sentence_transformers</st>`
    <st c="10914">library to generate embeddings for your RAG vector store as we have
    been doing with OpenAI’s embedding API.</st> <st c="11023">The answer is a resounding
    yes!</st> <st c="11055">This is a free alternative to using OpenAI’s embeddings
    API and the embeddings, especially if generated from the larger</st> `<st c="11175">'all-mpnet-base-v2'</st>`
    <st c="11194">model.</st> <st c="11202">You can use the</st> `<st c="11415">ada</st>`
    <st c="11418">model is ranked 65</st><st c="11437">th</st> <st c="11440">and their
    “best” model, the</st> `<st c="11469">'text-embedding-3-large'</st>` <st c="11493">model
    is ranked 14</st><st c="11512">th</st><st c="11515">. You can also fine-tune these
    models with your own data and potentially make it more effective for your RAG
    system than any paid API embedding service.</st> <st c="11668">Finally, for any
    API service, you are reliant on it being available, which is not always the case.</st>
    <st c="11767">Using the</st> `<st c="11777">sentence_transformers</st>` <st c="11798">model
    locally makes it always available and 100% reliable.</st> <st c="11858">Take a
    look at MTEB to find even better models that you can download and use in a</st>
    <st c="11940">similar way.</st>
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="10847">你可能想知道是否可以使用</st> `<st c="10893">sentence_transformers</st>` <st
    c="10914">库为你的RAG向量存储生成嵌入，就像我们使用OpenAI的嵌入API所做的那样。</st> <st c="11023">答案是响亮的肯定！</st>
    <st c="11055">这是使用OpenAI的嵌入API的免费替代方案，特别是如果使用更大的</st> `<st c="11175">'all-mpnet-base-v2'</st>`
    <st c="11194">模型生成的嵌入。</st> <st c="11202">你可以使用</st> `<st c="11415">ada</st>`
    <st c="11418">模型，排名第65位，以及他们的“最佳”模型，即</st> `<st c="11469">'text-embedding-3-large'</st>`
    <st c="11493">模型，排名第14位。你还可以使用自己的数据微调这些模型，并可能使其比任何付费API嵌入服务更有效地适用于你的RAG系统。</st>
    <st c="11668">最后，对于任何API服务，你依赖于它始终可用，而这并不总是事实。</st> <st c="11767">使用</st> `<st
    c="11777">sentence_transformers</st>` <st c="11798">模型本地使用使其始终可用且100%可靠。</st>
    <st c="11858">查看MTEB以找到更好的模型，你可以下载并在类似方式中使用。</st>
- en: <st c="11952">Okay, we now</st> <st c="11966">have an environment for us to
    start exploring</st> <st c="12012">distance measures.</st>
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11952">好的，我们现在</st> <st c="11966">有一个环境可以开始探索</st> <st c="12012">距离度量。</st>
- en: <st c="12030">There are many ways to calculate the distance between vectors.</st>
    <st c="12094">Euclidean distance (L2), dot product, and cosine distance are the
    most common distance metrics used</st> <st c="12194">in NLP.</st>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12030">计算向量之间距离的方法有很多。</st> <st c="12094">欧几里得距离（L2）、点积和余弦距离是NLP中最常用的距离度量。</st>
- en: <st c="12201">Let’s start with</st> **<st c="12219">Euclidean</st>** **<st c="12229">distance</st>**
    <st c="12237">(</st>**<st c="12239">L2</st>**<st c="12241">).</st>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12201">让我们从</st> **<st c="12219">欧几里得</st>** **<st c="12229">距离</st>**
    <st c="12237">(</st>**<st c="12239">L2</st>**<st c="12241">).</st>
- en: <st c="12244">Euclidean distance (L2)</st>
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="12244">欧几里得距离（L2）</st>
- en: <st c="12268">Euclidean distance</st> <st c="12288">calculates the shortest
    distance between</st> <st c="12329">two vectors.</st> <st c="12342">When using
    this to score the distance, keep in mind that we are looking for what is closer,
    so a lower value indicates higher similarity (closeness in distance).</st> <st
    c="12504">Let us calculate the Euclidean distance between</st> <st c="12552">two
    vectors:</st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12268">欧几里得距离</st> <st c="12288">计算两个向量之间的最短距离。</st> <st c="12329">当使用此方法来评分距离时，请记住，我们正在寻找更接近的，因此较低值表示更高的相似度（距离上的接近）。</st>
    <st c="12504">让我们计算两个向量之间的欧几里得距离：</st>
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="12635">In this function, we are calculating the Euclidean distance between
    two vectors,</st> `<st c="12717">vec1</st>` <st c="12721">and</st> `<st c="12726">vec2</st>`<st
    c="12730">. We first perform element-wise subtraction between the two vectors,
    and then we use NumPy’s</st> `<st c="12823">linalg.norm()</st>` <st c="12836">function
    to calculate the Euclidean norm (also known as L2 norm) of the vector.</st> <st
    c="12917">This function takes the square root of the sum of the squares of the
    vector elements.</st> <st c="13003">Combined, these give us the Euclidean distance
    between the</st> <st c="13062">two vectors.</st>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12635">在这个函数中，我们计算两个向量</st> `<st c="12717">vec1</st>` <st c="12721">和</st>
    `<st c="12726">vec2</st>`<st c="12730">之间的欧几里得距离。</st> <st c="12823">我们首先对两个向量进行逐元素减法，然后使用NumPy的</st>
    `<st c="12836">linalg.norm()</st>` <st c="12836">函数来计算向量的欧几里得范数（也称为L2范数）。</st>
    <st c="12917">此函数计算向量元素平方和的平方根。</st> <st c="13003">结合这些，我们得到了两个向量之间的欧几里得距离。</st>
- en: <st c="13074">We call this function here for each of</st> <st c="13114">the
    embeddings:</st>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13074">我们为每个</st> <st c="13114">嵌入</st>调用此函数：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: <st c="13645">Take a moment</st> <st c="13660">and look around you for the closest</st>
    *<st c="13696">thing</st>* <st c="13701">to you.</st> <st c="13710">Then look
    for something further away.</st> <st c="13748">The thing that is closest to you
    is measured</st> <st c="13793">in a smaller distance.</st> <st c="13816">One foot
    is closer than two feet, so in this case, when you want it to be closer,</st>
    `<st c="13898">1</st>` <st c="13899">is a better score than</st> `<st c="13923">2</st>`<st
    c="13924">. When it comes to distance in semantic search, closer means it is more
    similar.</st> <st c="14005">For these results, we want to see a lower score to
    say it is more similar.</st> `<st c="14080">Review 1</st>` <st c="14088">and</st>
    `<st c="14093">Review 2</st>` <st c="14101">have a Euclidean distance of</st>
    `<st c="14131">4.6202903</st>`<st c="14140">. Both reviews are significantly further
    from</st> `<st c="14186">Random Comment</st>`<st c="14200">. This shows how math
    is used to determine how semantically similar or dissimilar these texts are.</st>
    <st c="14299">But as with most things in data science, we have several options
    for how to calculate these distances.</st> <st c="14402">Let’s take a look at
    another approach,</st> **<st c="14441">dot product</st>**<st c="14452">.</st>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13645">花点时间</st> <st c="13660">环顾四周，寻找离你最近的</st> *<st c="13696">东西</st>*
    <st c="13701">。</st> <st c="13710">然后寻找更远一点的东西。</st> <st c="13748">离你最近的东西测量的距离更小。</st>
    <st c="13793">一英尺比两英尺近，所以在这种情况下，当你想要它更近时，</st> `<st c="13898">1</st>` <st c="13899">比</st>
    `<st c="13923">2</st>`<st c="13924">是一个更好的分数。当涉及到语义搜索中的距离时，更近意味着更相似。</st> <st
    c="14005">对于这些结果，我们希望看到一个更低的分数来说明它更相似。</st> `<st c="14080">评论1</st>` <st c="14088">和</st>
    `<st c="14093">评论2</st>` <st c="14101">的欧几里得距离为</st> `<st c="14131">4.6202903</st>`<st
    c="14140">。这两个评论都显著地远离</st> `<st c="14186">随机评论</st>`<st c="14200">。这显示了数学是如何用来确定这些文本在语义上相似或不同。</st>
    <st c="14299">但就像数据科学中的大多数事情一样，我们有几种计算这些距离的方法。</st> <st c="14402">让我们看看另一种方法，</st>
    **<st c="14441">点积</st>**<st c="14452">。</st>
- en: <st c="14453">Dot product (also called inner product)</st>
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="14453">点积（也称为内积）
- en: <st c="14493">The</st> <st c="14498">dot product is not technically a distance
    metric, as it measures the magnitude of the</st> <st c="14584">projection of one
    vector onto the other, which indicates similarity rather than distance.</st> <st
    c="14674">However, it is a metric used for similar purposes as the other metrics
    mentioned here.</st> <st c="14761">Since we are talking about magnitude and not
    closeness, a higher positive dot product value indicates more similarity.</st>
    <st c="14880">And so, as the value goes lower, or even negative, this indicates
    less similarity.</st> <st c="14963">Here we will print out the dot product of
    each of our</st> <st c="15017">text strings:</st>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14493">点积</st> <st c="14498">实际上不是一个距离度量，因为它衡量的是</st> <st c="14584">一个向量在另一个向量上的投影的大小，这表明的是相似性而不是距离。</st>
    <st c="14674">然而，它是一种与其他提到的度量具有相似目的的度量。</st> <st c="14761">由于我们谈论的是大小而不是接近程度，一个更高的正点积值表示更大的相似性。</st>
    <st c="14880">因此，随着值的降低，甚至变为负值，这表明相似性更小。</st> <st c="14963">在这里，我们将打印出我们每个</st>
    <st c="15017">文本字符串</st>的点积：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="15282">In this code, we</st> <st c="15300">are using a NumPy function
    that does all the dot product computations for us.</st> <st c="15378">The output
    is</st> <st c="15392">as follows:</st>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="15282">在这段代码中，我们使用了一个NumPy函数，它为我们完成了所有的点积计算。</st> <st c="15378">输出如下：</st>
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="15552">In our</st> <st c="15560">first comparison,</st> `<st c="15578">Review</st>`
    `<st c="15584">1</st>` <st c="15586">and</st> `<st c="15591">Review 2</st>`<st
    c="15599">, we see a score of</st> `<st c="15619">12.270497</st>`<st c="15628">.
    The positive magnitude of the dot product (</st>`<st c="15673">12.270497</st>`<st
    c="15683">) suggests a relatively high similarity between</st> `<st c="15732">Review
    1</st>` <st c="15740">and</st> `<st c="15745">Review 2</st>`<st c="15753">. When
    we compare</st> `<st c="15771">Review</st>` `<st c="15777">1</st>` <st c="15779">with</st>
    `<st c="15785">Random Comment</st>`<st c="15799">, we see a score of</st> `<st
    c="15819">-0.7654616</st>`<st c="15829">, and</st> `<st c="15835">Review 2</st>`
    <st c="15843">versus</st> `<st c="15851">Random Comment</st>` <st c="15865">gives
    us a</st> `<st c="15877">0.95240986</st>` <st c="15887">dot product.</st> <st
    c="15901">These low and negative values indicate that there is dissimilarity or
    misalignment between the two vectors.</st> <st c="16009">These scores tell us
    that</st> `<st c="16035">Review 1</st>` <st c="16043">and</st> `<st c="16048">Review
    2</st>` <st c="16056">are more similar to each other compared to their similarity
    with</st> `<st c="16122">Random Comment</st>`<st c="16136">.</st>
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16137">Let’s look at our last distance metric,</st> **<st c="16178">cosine
    distance</st>**<st c="16193">.</st>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16194">Cosine distance</st>
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16210">Cosine distance</st> <st c="16227">measures the difference in
    directionality between</st> <st c="16276">the vectors.</st> <st c="16290">Given
    that this is another distance metric, we consider a lower value to indicate closer,
    more similar vectors.</st> <st c="16402">First, we set up a function to calculate
    the cosine distance between</st> <st c="16471">two vectors:</st>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="16612">Notice that the formula for cosine distance contains elements
    from both of our previous metrics.</st> <st c="16710">First, we use</st> `<st
    c="16724">np.dot(vec1, vec2)</st>` <st c="16742">to calculate the dot product
    between the two vectors.</st> <st c="16797">Then, we divide by the product of
    the magnitudes, using the same NumPy function we used for Euclidean distance to
    calculate the Euclidean norm.</st> <st c="16941">In this case, though, we are
    calculating the Euclidean norm of each of the vectors (rather than the difference
    between</st> <st c="17059">the vectors as we did with Euclidean distance) and</st>
    <st c="17111">then multiplying them.</st> <st c="17134">Combined, we get the cosine
    similarity, which is then subtracted as an absolute value from</st> `<st c="17225">1</st>`
    <st c="17226">to get the cosine distance.</st> <st c="17255">Here, we call</st>
    <st c="17269">this function:</st>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="17574">And this is what we see in</st> <st c="17602">the output:</st>
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="17798">Just like with Euclidean distance, a lower distance value means
    closer, which means more similar.</st> <st c="17897">Once again, the value measuring
    the distance between the two reviews indicates much closer and similar semantics
    compared to either of the reviews and the random comment.</st> <st c="18068">However,
    it should be noted that</st> `<st c="18101">0.4523802399635315</st>` <st c="18119">suggests
    more of a moderate similarity between</st> `<st c="18167">Review 1</st>` <st c="18175">and</st>
    `<st c="18180">Review 2</st>`<st c="18188">. But the other two scores,</st> `<st
    c="18216">1.0295443572103977</st>` <st c="18234">and</st> `<st c="18239">0.9542623348534107</st>`<st
    c="18257">, indicate a high dissimilarity between</st> <st c="18297">the vectors.</st>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17798">再次强调，衡量两个评论之间距离的值表明，与任何一个评论和随机评论相比，这两个评论的语义更加接近和相似。</st> <st c="17897">然而，需要注意的是</st>
    `<st c="18101">0.4523802399635315</st>` <st c="18119">表明在<st c="18167">评论1</st>和<st
    c="18175">评论2</st>之间有更多的**中等相似性**。</st> <st c="18188">但其他两个分数，<st c="18216">1.0295443572103977</st>
    <st c="18234">和<st c="18239">0.9542623348534107</st>`<st c="18257">，表明向量之间有很高的**不相似性**。</st>
- en: <st c="18309">Sorry Taylor Swift, mathematically speaking, we have ample proof
    that you are not the semantic equivalent of a</st> <st c="18421">warm blanket!</st>
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18309">对不起泰勒·斯威夫特，从数学上来说，我们有充分的证据证明你不是一个**温暖毯子**的语义等价物！</st> <st c="18421">就像欧几里得距离一样，距离值越低意味着越接近，也就是越相似。</st>
- en: <st c="18434">Keep in mind that there are many other distance metrics and similarity
    scores you can use for text</st> <st c="18534">embeddings, including</st> **<st
    c="18556">Lin similarity</st>**<st c="18570">,</st> **<st c="18572">Jaccard similarity</st>**<st
    c="18590">,</st> **<st c="18592">Hamming distance</st>**<st c="18608">,</st> **<st
    c="18610">Manhattan distance</st>**<st c="18628">, and</st> **<st c="18634">Levenshtein
    distance</st>**<st c="18654">. However, the</st> <st c="18669">three</st> <st
    c="18674">metrics</st> <st c="18682">listed</st> <st c="18689">previously are
    the most commonly used for NLP and should give you a good start in understanding
    how these metrics</st> <st c="18804">are calculated.</st>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18434">请记住，还有许多其他可用于文本嵌入的距离度量标准和相似度分数，包括</st> **<st c="18556">Lin相似度</st>**<st
    c="18570">、**<st c="18572">Jaccard相似度</st>**<st c="18590">、**<st c="18592">汉明距离</st>**<st
    c="18608">、**<st c="18610">曼哈顿距离</st>**<st c="18628">和**<st c="18634">Levenshtein距离</st>**<st
    c="18654">。然而，之前列出的三个度量标准是NLP中最常用的，应该能帮助您了解这些度量标准是如何计算的。</st> <st c="18669">三个</st>
    <st c="18674">度量标准</st> <st c="18682">列出的</st> <st c="18689">之前</st> <st c="18804">应该能帮助您了解这些度量标准是如何计算的。</st>
- en: <st c="18819">Up to this point, we have discussed dense vectors, which represent
    semantic meaning, but not all models represent semantic meaning.</st> <st c="18952">Some
    are literally just a count of words in the data we provide to it.</st> <st c="19023">These
    vectors are called sparse vectors.</st> <st c="19064">Let’s talk about the differences
    between these types of vectors and how we can use those differences to our advantage</st>
    <st c="19182">in RAG.</st>
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="18819">到目前为止，我们讨论了密集向量，它们代表语义意义，但并非所有模型都代表语义意义。</st> <st c="18952">有些模型实际上只是对我们提供的数据中单词的计数。</st>
    <st c="19023">这些向量被称为稀疏向量。</st> <st c="19064">让我们来谈谈这些类型向量之间的差异，以及我们如何利用这些差异在RAG中占得优势。</st>
    <st c="19182">在RAG中。</st>
- en: <st c="19189">Different search paradigms – sparse, dense, and hybrid</st>
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="19189">不同的搜索范式——稀疏、密集和混合</st>
- en: <st c="19244">There are different types of vectors, and this difference is important
    to this discussion because you need to use different types of vector searches
    depending on the type of vector you are searching.</st> <st c="19445">Let’s talk
    in depth about the differences between these types</st> <st c="19507">of vectors.</st>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="19244">存在不同类型的向量，这种差异对于本次讨论非常重要，因为您需要根据所搜索的向量类型使用不同类型的向量搜索。</st> <st
    c="19445">让我们深入探讨这些类型向量之间的差异。</st> <st c="19507">的向量。</st>
- en: <st c="19518">Dense search</st>
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="19518">密集搜索</st>
- en: '**<st c="19531">Dense search</st>** <st c="19544">(semantic search) uses</st>
    <st c="19567">vector embedding representation</st> <st c="19599">of data to perform
    search.</st> <st c="19627">As we have talked about previously, this type of search
    allows you to capture and return semantically similar objects.</st> <st c="19746">It
    relies on the meaning of the data in order to perform that query.</st> <st c="19815">This
    sounds great in theory, but there are some limitations.</st> <st c="19876">If
    the model we are using was trained on a completely different domain, the accuracy
    of our queries would be poor.</st> <st c="19991">It is very dependent on the data
    it was</st> <st c="20031">trained on.</st>'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="19531">密集搜索</st>** <st c="19544">（语义搜索）使用</st> <st c="19567">数据向量的嵌入表示</st>
    <st c="19599">来进行搜索。</st> <st c="19627">正如我们之前讨论的，这种搜索类型允许你捕捉并返回语义相似的对象。</st>
    <st c="19746">它依赖于数据的意义来执行查询。</st> <st c="19815">在理论上听起来很棒，但也有一些局限性。</st> <st
    c="19876">如果我们使用的模型是在一个完全不同的领域上训练的，我们的查询准确性会很低。</st> <st c="19991">它非常依赖于它所</st>
    <st c="20031">训练的数据。</st>'
- en: <st c="20042">Searching for data that is a reference to something (such as serial
    numbers, codes, IDs, and even people’s names) will also yield poor results.</st>
    <st c="20187">This is because there isn’t a lot of meaning in text like this,
    so no meaning is captured in the embeddings, and no meaning can be used to compare
    embeddings.</st> <st c="20346">When searching for specific references like that,
    it is better for string or word matching.</st> <st c="20438">We call this type
    of search keyword search or sparse search, and we will discuss</st> <st c="20519">this
    next.</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="20042">搜索引用某些事物（如序列号、代码、ID甚至人名）的数据也会产生较差的结果。</st> <st c="20187">这是因为这种文本中的意义不大，所以在嵌入中没有捕捉到任何意义，也无法使用意义来比较嵌入。</st>
    <st c="20346">当搜索此类特定引用时，字符串或词匹配会更好。</st> <st c="20438">我们称这种类型的搜索为关键词搜索或稀疏搜索，我们将在下一节讨论。</st>
- en: <st c="20529">Sparse search</st>
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="20529">稀疏搜索</st>
- en: '**<st c="20543">Sparse search</st>** <st c="20557">allows</st> <st c="20564">you
    to utilize</st> <st c="20579">keyword matching across all of your content.</st>
    <st c="20625">It is called</st> **<st c="20638">sparse embedding</st>** <st c="20654">because</st>
    <st c="20663">text is embedded into vectors by counting how many times every unique
    word in your vocabulary occurs in the query and stored sentences.</st> <st c="20799">This
    vector has mostly zeros because the likelihood of any given sentence containing
    every word in your vocabulary is low.</st> <st c="20922">In mathematical terms,
    if an embedding contains mostly zeros, it is</st> <st c="20990">considered sparse.</st>'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="20543">稀疏搜索</st>** <st c="20557">允许</st> <st c="20564">你在所有内容中利用</st>
    <st c="20579">关键词匹配。</st> <st c="20625">它被称为</st> **<st c="20638">稀疏嵌入</st>**
    <st c="20654">，因为</st> <st c="20663">文本是通过统计词汇表中每个唯一词在查询和存储句子中出现的次数来嵌入向量的。</st>
    <st c="20799">这个向量大部分是零，因为任何给定句子包含你词汇表中所有词的可能性很低。</st> <st c="20922">从数学的角度来看，如果一个嵌入向量大部分是零，它被认为是</st>
    <st c="20990">稀疏的。</st>'
- en: <st c="21008">One example could be</st> <st c="21030">using a</st> **<st c="21038">bag-of-words</st>**<st
    c="21050">. A bag-of-words approach is where you count how many times a word occurs
    in the query and the data vector and then return objects with the highest matching
    word frequency.</st> <st c="21223">This is the easiest way to do</st> <st c="21253">keyword
    matching.</st>
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21008">一个例子可能是</st> <st c="21030">使用一个</st> **<st c="21038">词袋</st>**<st
    c="21050">。词袋方法是指你统计查询和数据向量中每个词出现的次数，然后返回匹配词频率最高的对象。</st> <st c="21223">这是进行</st>
    <st c="21253">关键词匹配</st>的最简单方法。
- en: <st c="21270">A good example of a keyword-based</st> <st c="21305">algorithm
    is the</st> **<st c="21322">Best Matching 25</st>** <st c="21338">(</st>**<st
    c="21340">BM25</st>**<st c="21344">) algorithm.</st> <st c="21358">This very popular
    model performs really well when it comes to searching across many keywords.</st>
    <st c="21452">The idea behind BM25 is that it counts the number of words within
    the phrase that you are passing in and then those that appear more than often
    are weighted as less important when the match occurs.</st> <st c="21650">Words
    that are rare will score much higher.</st> <st c="21694">Does this concept sound
    familiar?</st> <st c="21728">It uses TF-IDF, one of the models we reviewed in
    the</st> <st c="21781">last chapter!</st>
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21270">基于关键词的</st> <st c="21305">算法的一个好例子是</st> **<st c="21322">最佳匹配
    25</st>** <st c="21338">(**<st c="21340">BM25</st>**<st c="21344">) 算法。</st> <st
    c="21358">这个非常流行的模型在搜索多个关键词时表现非常出色。</st> <st c="21452">BM25 的理念是，它计算您传递的短语中的单词数量，然后那些出现频率较高的单词在匹配发生时被赋予较低的权重。</st>
    <st c="21650">罕见的单词将获得更高的分数。</st> <st c="21694">这个概念听起来熟悉吗？</st> <st c="21728">它使用了
    TF-IDF，这是我们上一章中讨论过的模型之一！</st>
- en: '<st c="21794">Having these two options brings up a challenging question, though:
    which one do we use?</st> <st c="21883">What if we need semantic matching and
    keyword matching?</st> <st c="21939">The great news is we do not have to choose;
    we can use both in what is called</st> **<st c="22017">hybrid searching</st>**<st
    c="22033">! We will review that</st> <st c="22055">concept next.</st>'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21794">虽然这两个选项提出了一个具有挑战性的问题：我们应该使用哪一个？</st> <st c="21883">如果我们需要语义匹配和关键词匹配怎么办？</st>
    <st c="21939">好消息是，我们不必做出选择；我们可以在所谓的</st> **<st c="22017">混合搜索</st>**<st c="22033">中使用两者！我们将在下一节中回顾这个概念。</st>
- en: <st c="22068">Hybrid search</st>
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="22068">混合搜索</st>
- en: <st c="22082">Hybrid search</st> <st c="22096">allows you to make the most of
    both dense and sparse search techniques and then fuse the return rank results
    together.</st> <st c="22216">With hybrid search, you</st> <st c="22240">are performing
    both vector/dense search and keyword/sparse search and then you combine</st> <st
    c="22327">the results.</st>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22082">混合搜索</st> <st c="22096">允许您充分利用密集和稀疏搜索技术，然后将返回的排名结果融合在一起。</st>
    <st c="22216">使用混合搜索，您</st> <st c="22240">执行了向量/密集搜索和关键词/稀疏搜索，然后您将</st> <st c="22327">结果合并。</st>
- en: <st c="22339">This combination can be done based on a scoring system that measures
    how well each object matches the query using both dense and sparse searches.</st>
    <st c="22486">What better way to illustrate how this approach works than to walk
    through a code lab with it?</st> <st c="22581">In the next section, we will introduce
    you to BM25 to conduct your keyword/sparse search and then combine it with our
    existing retriever to form a</st> <st c="22728">hybrid search.</st>
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22339">这种组合可以通过一个评分系统来完成，该系统衡量每个对象使用密集和稀疏搜索与查询匹配的程度。</st> <st c="22486">还有什么比通过一个代码实验室来展示这种方法如何工作更好的方式吗？</st>
    <st c="22581">在下一节中，我们将向您介绍 BM25 以进行关键词/稀疏搜索，然后将其与我们的现有检索器结合形成</st> <st c="22728">混合搜索。</st>
- en: <st c="22742">Code lab 8.2 – Hybrid search with a custom function</st>
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="22742">代码实验室 8.2 – 带自定义函数的混合搜索</st>
- en: <st c="22794">The</st> <st c="22798">file you need to access from the GitHub
    repository is</st> <st c="22853">titled</st> `<st c="22860">CHAPTER8-2_HYBRID_CUSTOM.ipynb</st>`<st
    c="22890">.</st>
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22794">您需要从 GitHub 仓库访问的文件名为</st> <st c="22798">titled</st> `<st c="22860">CHAPTER8-2_HYBRID_CUSTOM.ipynb</st>`<st
    c="22890">。</st>
- en: <st c="22891">In this code lab, we are going to start with the notebook from</st>
    [*<st c="22955">Chapter 5</st>*](B22475_05.xhtml#_idTextAnchor095)<st c="22964">:</st>
    `<st c="22967">CHAPTER5-3_BLUE_TEAM_DEFENDS.ipynb</st>`<st c="23001">. Note that
    we are not using the</st> [*<st c="23034">Chapter 6</st>*](B22475_06.xhtml#_idTextAnchor114)
    <st c="23043">or</st> *<st c="23047">7</st>* <st c="23048">code, which has a lot
    of miscellaneous code we won’t use going forward.</st> <st c="23121">There is
    an added bonus in this code lab though; we are going to introduce some new elements
    that will carry us through the next couple of chapters, such as a new type of
    document loader for PDFs rather than web pages, a new larger document with more
    data to search, and a new text splitter.</st> <st c="23413">We will also clean
    out any code we no longer need as a result of</st> <st c="23478">these changes.</st>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22891">在这个代码实验室中，我们将从第5章的笔记本开始</st> [*<st c="22955">第5章</st>*](B22475_05.xhtml#_idTextAnchor095)<st
    c="22964">：</st> `<st c="22967">CHAPTER5-3_BLUE_TEAM_DEFENDS.ipynb</st>`<st c="23001">。请注意，我们不会使用</st>
    [*<st c="23034">第6章</st>*](B22475_06.xhtml#_idTextAnchor114) <st c="23043">或</st>
    *<st c="23047">第7章</st> * <st c="23048">的代码，其中包含我们以后不会使用的很多杂乱代码。</st> <st c="23121">然而，在这个代码实验室中有一个额外的奖励；我们将介绍一些新元素，这些元素将贯穿接下来的几章，例如用于PDF而不是网页的新类型文档加载器，一个包含更多数据以供搜索的新大型文档，以及一个新的文本分割器。</st>
    <st c="23413">我们还将清理掉由于这些更改而不再需要的任何代码。</st>
- en: <st c="23492">Once we have updated the code for these changes, we can focus
    on the task at hand, which is to use BM25 to generate our sparse vectors, combining
    those vectors with the dense vectors we have already used to form a hybrid search
    approach.</st> <st c="23731">We will use our previous vectorizer to generate our
    dense vectors.</st> <st c="23798">Then, we will search using both sets of vectors,
    rerank the results accounting for documents that appear in both retrievals, and
    provide a final hybrid result.</st> <st c="23958">BM25 has been around for several
    decades, but it is still a very effective bag-of-words algorithm based on TF-IDF,
    which we reviewed in the last chapter.</st> <st c="24112">It is also very quick</st>
    <st c="24134">to compute.</st>
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23492">一旦我们为这些更改更新了代码，我们就可以专注于手头的任务，即使用BM25生成我们的稀疏向量，将这些向量与我们已经使用的密集向量结合起来，形成一个混合搜索方法。</st>
    <st c="23731">我们将使用我们之前的向量器来生成我们的密集向量。</st> <st c="23798">然后，我们将使用这两组向量进行搜索，重新排名考虑出现在两次检索中的文档，并提供最终的混合结果。</st>
    <st c="23958">BM25已经存在了几十年，但它仍然是一个基于TF-IDF的非常有效的词袋算法，我们在上一章中已经讨论过。</st> <st c="24112">它也非常快</st>
    <st c="24134">地计算。</st>
- en: <st c="24145">One interesting aspect of combining the results from the two retrievers
    begs the question, how does it rank results from two relatively different search
    mechanisms?</st> <st c="24311">Our dense vector search uses cosine similarity
    and provides a similarity score.</st> <st c="24391">Our sparse vector is based
    on TF-IDF and using TF and IDF scores, which we reviewed in the previous chapter.</st>
    <st c="24500">These are not comparable scores.</st> <st c="24533">As it turns
    out, there are numerous algorithms we can use to perform the ranking among these
    two retrievers.</st> <st c="24642">The one we will use is called</st> <st c="24672">the</st>
    **<st c="24676">Reciprocal Rank Fusion</st>** <st c="24698">(</st>**<st c="24700">RRF</st>**<st
    c="24703">) algorithm.</st> <st c="24717">This lab primarily focuses on building
    a function that mimics the ranking approach that the RRF takes, so that you can
    walk through and understand these</st> <st c="24870">calculations yourself.</st>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24145">将两个检索器的结果结合起来有一个有趣的方面，那就是如何对来自两种相对不同的搜索机制的结果进行排名？</st> <st c="24311">我们的密集向量搜索使用余弦相似度并提供相似度分数。</st>
    <st c="24391">我们的稀疏向量基于TF-IDF，并使用TF和IDF分数，这些我们在上一章中已经讨论过。</st> <st c="24500">这些分数是不可比较的。</st>
    <st c="24533">实际上，我们可以使用许多算法来在这两个检索器之间进行排名。</st> <st c="24642">我们将使用的是</st> <st
    c="24672">**互逆排名融合**</st> <st c="24698">(</st>**<st c="24700">RRF</st>**<st c="24703">)
    算法。</st> <st c="24717">这个实验室主要关注构建一个模拟RRF排名方法的函数，这样你就可以亲自走一遍并理解这些</st> <st c="24870">计算。</st>
- en: <st c="24892">We no longer need this package focused on parsing web pages, since
    we are changing from processing a web page to parsing a PDF.</st> <st c="25021">Let’s
    start with removing</st> <st c="25047">that code:</st>
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24892">由于我们正在从处理网页转换为解析PDF，我们不再需要这个专注于解析网页的包。</st> <st c="25021">让我们从移除</st>
    <st c="25047">这段代码：</st>
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="25085">We do</st> <st c="25091">need to install a new package for parsing
    PDFs, as we need a new package that will let us use the BM25 model with LangChain
    to generate the</st> <st c="25232">sparse embeddings:</st>
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25085">我们需要安装一个新的包来解析PDF，因为我们需要一个新包来让我们使用BM25模型与LangChain生成</st> <st
    c="25232">稀疏嵌入：</st>
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="25302">That will load both of these packages into our environment.</st>
    <st c="25363">Remember to restart your kernel after</st> <st c="25401">the installation!</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25302">这将把这两个包都加载到我们的环境中。</st> <st c="25363">记住在安装后重启您的内核！</st> <st c="25401">安装后！</st>
- en: <st c="25418">Next, remove this code from</st> <st c="25447">the imports:</st>
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25418">接下来，从</st> <st c="25447">导入中删除此代码：</st>
- en: '[PRE15]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="25598">As mentioned earlier, we no longer need the code for parsing web
    pages.</st> <st c="25671">We are also removing our text splitter and will replace
    it with a</st> <st c="25737">new one.</st>
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25598">如前所述，我们不再需要解析网页的代码。</st> <st c="25671">我们还将移除我们的文本分割器，并用一个新的替换它。</st>
- en: <st c="25745">Add this code to</st> <st c="25763">the imports:</st>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25745">将此代码添加到</st> <st c="25763">导入中：</st>
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: <st c="25977">Here, we add</st> `<st c="25991">PdfReader</st>` <st c="26000">for
    PDF extraction.</st> <st c="26021">We add the</st> `<st c="26032">RecursiveCharacterTextSplitter</st>`
    <st c="26062">text splitter, which will replace</st> `<st c="26097">SemanticChunker</st>`<st
    c="26112">. We add a new class that will help us manage and process our documents
    when dealing with LangChain.</st> <st c="26213">Last, we add the</st> `<st c="26230">BM25Retriever</st>`
    <st c="26243">loader that acts as a</st> <st c="26266">LangChain retriever.</st>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25977">在这里，我们添加</st> `<st c="25991">PdfReader</st>` <st c="26000">用于PDF提取。</st>
    <st c="26021">我们添加了</st> `<st c="26032">RecursiveCharacterTextSplitter</st>` <st
    c="26062">文本分割器，它将替换</st> `<st c="26097">SemanticChunker</st>`<st c="26112">。我们添加了一个新类，它将帮助我们管理并处理与LangChain相关的文档。</st>
    <st c="26213">最后，我们添加了</st> `<st c="26230">BM25Retriever</st>` <st c="26243">加载器，它充当</st>
    <st c="26266">LangChain检索器。</st>
- en: <st c="26286">Let’s next remove the web</st> <st c="26313">parsing code:</st>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26286">接下来，让我们删除网页解析代码：</st>
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: <st c="26532">We</st> <st c="26535">are going to take the cell where we are
    defining our OpenAI variables and expand it to define all of the variables we
    use across the code; add this to the bottom of</st> <st c="26701">that cell:</st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26532">我们将把定义OpenAI变量的单元格扩展到定义代码中使用的所有变量；将此添加到该单元格的底部：</st>
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="26847">This sets up some variables that we will discuss further when
    we use them in the following code.</st> <st c="26945">Now, let’s add our code
    for processing</st> <st c="26984">a PDF:</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26847">这设置了一些变量，我们将在使用以下代码时进一步讨论。</st> <st c="26945">现在，让我们添加处理PDF的代码：</st>
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="27091">We will talk more about LangChain document loading in</st> [*<st
    c="27146">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)<st c="27156">, but
    for now, we wanted to introduce you to an alternative to just loading web pages.</st>
    <st c="27243">Given the popularity of PDFs, this is probably going to be a common
    situation for you.</st> <st c="27330">The catch is that you need to have the</st>
    `<st c="27369">google-2023-environmental-report.pdf</st>` <st c="27405">file available
    in the same directory as your notebook.</st> <st c="27461">You can download that
    file from the same repo you access all of the other code in this book.</st> <st
    c="27554">This code pulls that file up and extracts the text across the pages,
    concatenating the text back together so that there is no text loss across</st>
    <st c="27697">the pages.</st>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27091">我们将在第11章中更详细地讨论LangChain文档加载，但现在，我们想向您介绍一种除了加载网页之外的替代方法。</st>
    [*<st c="27146">第11章</st>*](B22475_11.xhtml#_idTextAnchor229)<st c="27156">，但就目前而言，我们希望向您介绍一种除了加载网页之外的替代方法。</st>
    <st c="27243">鉴于PDF的普及，这可能是您常见的场景。</st> <st c="27330">需要注意的是，您需要将</st> `<st c="27369">google-2023-environmental-report.pdf</st>`
    <st c="27405">文件放在与您的笔记本相同的目录中。</st> <st c="27461">您可以从访问本书中所有其他代码的同一仓库中下载该文件。</st>
    <st c="27554">此代码将提取该文件并提取跨页面的文本，将文本重新连接起来，以确保页面之间没有文本丢失。</st>
- en: <st c="27707">At this</st> <st c="27715">point, we have a very large string
    representing all of the text in the PDF.</st> <st c="27792">We now need to use
    a splitter to break the text into manageable chunks.</st> <st c="27864">This is
    where we are going to switch from</st> `<st c="27906">SemanticChunker</st>` <st
    c="27921">to</st> `<st c="27925">RecursiveCharacterTextSplitter</st>`<st c="27955">.
    This gives you a chance to work with a different LangChain splitter as well, which
    is another topic we will expand on in</st> [*<st c="28078">Chapter 11</st>*](B22475_11.xhtml#_idTextAnchor229)<st
    c="28088">. First, remove</st> <st c="28104">this one:</st>
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27707">在这个</st> <st c="27715">阶段，我们有一个表示PDF中所有文本的非常大的字符串。</st> <st c="27792">我们现在需要使用一个分割器将文本分割成可管理的块。</st>
    <st c="27864">这就是我们将从</st> `<st c="27906">SemanticChunker</st>` <st c="27921">切换到</st>
    `<st c="27925">RecursiveCharacterTextSplitter</st>`<st c="27955">的地方。</st> <st
    c="27955">这给了你使用不同的LangChain分割器的机会，这也是我们将在</st> [*<st c="28078">第11章</st>*](B22475_11.xhtml#_idTextAnchor229)<st
    c="28088">中展开的另一个主题。</st> <st c="28088">首先，移除</st>
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: <st c="28210">Then, add</st> <st c="28221">this one:</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28210">然后，添加</st> <st c="28221">这个：</st>
- en: '[PRE21]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`<st c="28407">RecursiveCharacterTextSplitter</st>` <st c="28438">is a commonly
    used splitter that also saves us the cost of using the</st> `<st c="28508">OpenAI</st>`<st
    c="28514">embeddings API associated with</st> `<st c="28546">the SemanticChunker</st>`
    <st c="28565">splitter object.</st> <st c="28583">Combined with the larger PDF
    document we are now uploading, this splitter will give us more chunks to work
    with when looking at vector spaces and retrieval maps in the</st> <st c="28751">next
    chapter.</st>'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="28407">RecursiveCharacterTextSplitter</st>` <st c="28438">是一个常用的分割器，同时也节省了我们使用与</st>
    `<st c="28508">OpenAI</st>`<st c="28514">embeddings API相关的</st> `<st c="28546">SemanticChunker</st>`
    <st c="28565">分割器对象。</st> <st c="28583">结合我们现在上传的更大的PDF文档，这个分割器将在下一章查看向量空间和检索映射时给我们提供更多的块来工作。</st>'
- en: <st c="28764">With new data and a new splitter, we also need to update our retriever-related
    code.</st> <st c="28850">Let’s start with prepping</st> <st c="28876">our documents:</st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28764">使用新的数据和新的分割器，我们还需要更新我们的检索器相关代码。</st> <st c="28850">让我们从准备</st>
    <st c="28876">我们的文档：</st>
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <st c="28991">Then, the retriever code needs to</st> <st c="29026">be removed:</st>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28991">然后，检索器代码需要</st> <st c="29026">被移除：</st>
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: <st c="29160">Replace it</st> <st c="29172">with</st> <st c="29177">this code:</st>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29160">用这个</st> <st c="29172">替换</st> <st c="29177">这段代码：</st>
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: <st c="29500">This code takes a moment to load, but once it does, we are setting
    up our Chroma DB vector store to better manage the documents we get from our PDF,
    with ID metadata added.</st> <st c="29674">The original retriever is now called</st>
    `<st c="29711">dense_retriever</st>`<st c="29726">, which is a more descriptive
    and accurate name for it since it interacts with dense embeddings.</st> <st c="29823">The
    new retriever,</st> `<st c="29842">sparse_retriever</st>`<st c="29858">, is based
    on BM25, which is conveniently available through LangChain as a retriever, giving
    us similar functionality to any other LangChain instance of retriever.</st> <st
    c="30022">In both instances, we are ensuring that we are getting 10 results back
    by setting</st> `<st c="30104">k</st>` <st c="30105">to</st> `<st c="30109">10</st>`<st
    c="30111">. Also, note that the</st> `<st c="30133">vectorstore</st>` <st c="30144">object
    is using the</st> `<st c="30165">collection_name</st>` <st c="30180">string we
    defined in the variables earlier in</st> <st c="30227">the code.</st>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29500">这段代码需要一点时间来加载，但一旦加载完成，我们正在设置我们的Chroma DB向量存储库，以更好地管理我们从PDF中获取的文档，并添加了ID元数据。</st>
    <st c="29674">原始的检索器现在被称为</st> `<st c="29711">dense_retriever</st>`<st c="29726">，这是一个更描述性和准确的名称，因为它与密集嵌入交互。</st>
    <st c="29823">新的检索器</st>，`<st c="29842">sparse_retriever</st>`<st c="29858">，基于BM25，它通过LangChain作为检索器方便地可用，为我们提供了与其他任何LangChain实例的检索器类似的功能。</st>
    <st c="30022">在这两种情况下，我们通过将</st> `<st c="30104">k</st>` <st c="30105">设置为</st>
    `<st c="30109">10</st>`<st c="30111">来确保我们得到10个结果。</st> <st c="30111">此外，请注意，</st>
    `<st c="30133">vectorstore</st>` <st c="30144">对象正在使用我们在代码中较早定义的</st> `<st c="30165">collection_name</st>`
    <st c="30180">字符串。</st>
- en: <st c="30236">Fun fact</st>
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30236">有趣的事实</st>
- en: <st c="30245">You should note though that we are not storing our sparse embeddings
    in a Chroma DB vector store like we are with the dense embeddings.</st> <st c="30382">We
    pull the documents directly into the retriever, which keeps them in memory while
    we use them in our code.</st> <st c="30491">In a more sophisticated application,
    we would likely want to handle this more thoroughly and store the embeddings in
    a more permanent vector store for future retrieval.</st> <st c="30660">Even our
    Chroma DB is ephemeral in this code, which means we will lose it if we shut down
    the notebook kernel.</st> <st c="30771">You can improve this situation using</st>
    `<st c="30808">vectorstore.persist()</st>`<st c="30829">, which will store the
    Chroma DB database locally in a</st> `<st c="30884">sqlite</st>` <st c="30890">file.</st>
    <st c="30897">These are advanced techniques that aren’t needed for this code lab,
    but look them up if you want to build a more robust vector store environment for
    your</st> <st c="31051">RAG pipeline!</st>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30245">不过，请注意，我们并没有像处理密集嵌入那样，将稀疏嵌入存储在Chroma DB向量存储中。</st> <st c="30382">我们直接将文档拉入检索器，在我们使用它们时将它们保存在内存中。</st>
    <st c="30491">在更复杂的应用中，我们可能会想要更彻底地处理这个问题，并将嵌入存储在一个更持久的向量存储中，以便将来检索。</st> <st c="30660">在这个代码中，我们的Chroma
    DB也是临时的，这意味着如果我们关闭笔记本内核，我们将丢失它。</st> <st c="30771">您可以使用</st> `<st c="30808">vectorstore.persist()</st>`<st
    c="30829">来改善这种情况，它将Chroma DB数据库本地存储在一个</st> `<st c="30884">sqlite</st>` <st c="30890">文件中。</st>
    <st c="30897">这些是高级技术，对于这个代码实验室不是必需的，但如果您想要为您的</st> <st c="31051">RAG管道</st>构建一个更健壮的向量存储环境，可以查找它们！
- en: <st c="31064">In a moment, we</st> <st c="31081">will introduce you to the function
    that performs a hybrid search for you, so that you can step through and see what
    is happening.</st> <st c="31211">Before reviewing it, though, let’s discuss how
    to approach it.</st> <st c="31274">Keep in mind that this is a quick stab at trying
    to replicate the ranking algorithm that LangChain uses in its hybrid search mechanism.</st>
    <st c="31410">The idea here is that this will let you walk through what is going
    on under the hood when you are doing a hybrid search using LangChain.</st> <st
    c="31547">LangChain actually provides a</st> <st c="31576">mechanism that will
    do all of this in one line of code!</st> <st c="31633">This is</st> `<st c="31660">EnsembleRetriever</st>`
    <st c="31677">performs a hybrid search in the same way our function does, but
    it employs a sophisticated ranking algorithm called the RRF algorithm.</st> <st
    c="31813">This algorithm does the heavy lifting of determining how to rank all
    of the results, similar to how we just discussed our</st> <st c="31935">function
    operation.</st>
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31064">在片刻之后，我们将</st> <st c="31081">向您介绍一个为您执行混合搜索的功能，这样您可以逐步查看发生了什么。</st>
    <st c="31211">不过，在回顾它之前，让我们讨论一下如何接近它。</st> <st c="31274">请记住，这只是一个快速尝试来复制LangChain在其混合搜索机制中使用的排名算法。</st>
    <st c="31410">这里的想法是，这将让您在用LangChain进行混合搜索时，了解底层发生了什么。</st> <st c="31547">LangChain实际上提供了一个</st>
    <st c="31576">机制，只需一行代码就能完成所有这些操作！</st> <st c="31633">这是</st> `<st c="31660">EnsembleRetriever</st>`
    <st c="31677">以与我们函数相同的方式执行混合搜索，但它采用了一种称为RRF算法的复杂排名算法。</st> <st c="31813">该算法负责确定如何对所有结果进行排名，类似于我们刚才讨论的</st>
    <st c="31935">函数操作。</st>
- en: <st c="31954">We will step through the next function discussing each point and
    how that relates to the RFF algorithm that LangChain uses for the same purpose.</st>
    <st c="32100">This is by far the biggest function we have used so far, but it
    is worth the effort!</st> <st c="32185">Keep in mind that this is one function,
    which you can see in the code altogether.</st> <st c="32267">Let’s start with
    the</st> <st c="32288">function definition:</st>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31954">我们将逐步查看下一个函数，讨论每个点以及它与LangChain用于相同目的的RFF算法之间的关系。</st> <st c="32100">这是迄今为止我们使用的最大的函数，但这是值得的！</st>
    <st c="32185">请记住，这是一个函数，您可以在代码中一起看到它。</st> <st c="32267">让我们从</st> <st c="32288">函数定义</st>开始：
- en: '[PRE25]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: <st c="32377">Initially, we are going to take the weights for the dense and
    sparse results separately.</st> <st c="32467">This matches the</st> `<st c="32484">EnsembleRetriever</st>`
    <st c="32501">weight parameters from LangChain, which we will review in a moment,
    but this sets up this function to act exactly like that type of retriever.</st>
    <st c="32645">We also have a</st> `<st c="32660">k</st>` <st c="32661">value,
    indicating the total results we want the function to return.</st> <st c="32730">The
    default of</st> `<st c="32745">k</st>` <st c="32746">matches what the retrievers
    are set to return when they were initialized earlier in</st> <st c="32831">the
    code.</st>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32377">最初，我们将分别获取密集和稀疏结果的权重。</st> <st c="32467">这符合</st> `<st c="32484">EnsembleRetriever</st>`
    <st c="32501">权重参数来自LangChain，我们将在稍后进行回顾，但这将此函数设置为与那种类型的检索器完全一样。</st> <st c="32645">我们还有一个</st>
    `<st c="32660">k</st>` <st c="32661">值，表示函数要返回的总结果数。</st> <st c="32730">`<st c="32745">k</st>`
    <st c="32746">的默认值与检索器在代码中初始化时设置的返回值相匹配。</st>
- en: <st c="32840">Our first step within the function is focused on retrieving the
    top-</st>`<st c="32909">k</st>` <st c="32911">documents from both types</st> <st
    c="32938">of retrievers:</st>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32840">在函数内的第一步是专注于从两种类型的检索器中检索前<st c="32909">k</st> <st c="32911">个文档：</st>
- en: '[PRE26]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: <st c="33491">We start our</st> <st c="33504">retrieval process by retrieving
    the top-</st>`<st c="33545">k</st>` <st c="33547">documents from both dense search
    and sparse search.</st> <st c="33600">Just like RRF, we start with retrieving
    the top documents from both dense search and sparse search based on their respective
    scoring mechanisms.</st> <st c="33745">We also want to assign IDs to our content
    so that we can compare the results across retrievers, remove duplicates across
    results (by converting them to a set that removes all duplicates), and then create
    two dictionaries to store the reciprocal ranks of</st> <st c="33999">each document.</st>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33491">我们的检索过程从检索密集搜索和稀疏搜索的前<st c="33545">k</st> <st c="33547">个文档开始。</st>
    <st c="33600">就像RRF一样，我们根据各自的评分机制从密集搜索和稀疏搜索中检索前几个文档。</st> <st c="33745">我们还希望为我们的内容分配ID，以便我们可以跨检索器比较结果，通过将它们转换为去除所有重复项的集合来删除结果中的重复项，然后创建两个字典来存储每个文档的互信息排名。</st>
    <st c="33999">。</st>
- en: <st c="34013">Next, we are going to calculate the reciprocal rank for</st> <st
    c="34070">each document:</st>
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34013">接下来，我们将计算每个文档的互信息排名：</st>
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: <st c="34266">This code will</st> <st c="34282">calculate the reciprocal rank
    for each document in dense and sparse search results and store them in the dictionaries
    we just created.</st> <st c="34417">For each document, we calculate its reciprocal
    rank in each ranked list.</st> <st c="34490">The reciprocal rank is the inverse
    of the document’s position in the ranked list (e.g., 1/rank).</st> <st c="34587">The
    reciprocal rank is calculated as</st> `<st c="34624">1.0</st>` <st c="34627">divided
    by the position of the document in the respective search results (1-based index).</st>
    <st c="34718">Note that the similarity scores are not involved in this calculation.</st>
    <st c="34788">As you might remember from previous discussions, our semantic search
    is ranking based on distance and BM25 is ranking based on relevance.</st> <st
    c="34926">But RRF does not require these scores, which means we don’t need to
    worry about normalizing scores from the different retrieval methods to be on the
    same scale or directly comparable.</st> <st c="35110">With RFF, it relies on the
    rank positions, making it easier to combine results from different scoring mechanisms.</st>
    <st c="35224">It is important to note the impact this will have on your search,
    though.</st> <st c="35298">You may have a scenario where from a semantic standpoint,
    you have a really</st> *<st c="35374">close</st>* <st c="35379">score (distance-wise)
    in your semantic search, but the highest-ranked result from your keyword search
    is still not that similar.</st> <st c="35509">Using RFF with equal weights will
    result in these results having equal rankings and, therefore, equal value from
    the ranking standpoint, even though you would want the semantic result to have
    more weight.</st> <st c="35714">You can adjust this using the</st> `<st c="35744">dense_weight</st>`
    <st c="35756">and</st> `<st c="35761">sparse_weight</st>` <st c="35774">parameters,
    but what if you have the reverse situation?</st> <st c="35831">This is a downside
    of using RRF and hybrid search in general, which is why you will want to test
    to make sure this is the best solution for your</st> <st c="35976">particular
    needs.</st>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34266">此代码将</st> <st c="34282">计算密集和稀疏搜索结果中每个文档的互逆排名，并将它们存储在我们刚刚创建的字典中。</st>
    <st c="34417">对于每个文档，我们计算它在每个排名列表中的互逆排名。</st> <st c="34490">互逆排名是文档在排名列表中位置的倒数（例如，1/rank）。</st>
    <st c="34587">互逆排名的计算方法是</st> `<st c="34624">1.0</st>` <st c="34627">除以文档在相应搜索结果中的位置（基于1的索引）。</st>
    <st c="34718">请注意，相似度分数不涉及此计算。</st> <st c="34788">正如您可能从之前的讨论中记得的那样，我们的语义搜索是基于距离排名，而BM25是基于相关性排名。</st>
    <st c="34926">但是RRF不需要这些分数，这意味着我们不需要担心将来自不同检索方法的分数进行归一化，以便它们处于相同的尺度或直接可比。</st>
    <st c="35110">使用RFF，它依赖于排名位置，这使得结合来自不同评分机制的搜索结果变得更容易。</st> <st c="35224">尽管如此，重要的是要注意这将对您的搜索产生的影响。</st>
    <st c="35298">您可能有一个场景，从语义角度来看，您在语义搜索中有一个非常</st> *<st c="35374">接近</st>* <st c="35379">的分数（从距离的角度看），但您关键词搜索的最高排名结果仍然不太相似。</st>
    <st c="35509">使用RFF并赋予相同的权重将导致这些结果具有相同的排名和因此，从排名的角度看，具有相同的价值，尽管您可能希望语义结果具有更大的权重。</st>
    <st c="35714">您可以使用</st> `<st c="35744">dense_weight</st>` <st c="35756">和</st>
    `<st c="35761">sparse_weight</st>` <st c="35774">参数进行调整，但您如果遇到相反的情况怎么办？</st> <st
    c="35831">这是使用RRF和混合搜索的一般缺点，这就是为什么您想要测试以确保这是最适合您</st> <st c="35976">特定需求的最佳解决方案。</st>
- en: <st c="35993">Here, we sum the reciprocal ranks of each document across the
    ranked lists from dense search and</st> <st c="36091">sparse search:</st>
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35993">在这里，我们计算从密集搜索和</st> <st c="36091">稀疏搜索的排名列表中每个文档的互逆排名：</st>
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: <st c="36332">The RFF approach hinges on the idea that documents that are ranked
    highly by both retrieval methods are more likely to be relevant to the query.</st>
    <st c="36478">By using reciprocal ranks, RRF gives more weight to documents that
    appear at the top of the ranked lists.</st> <st c="36584">Note that we are</st>
    <st c="36601">weighting the sums using the weights we collected in the parameters.</st>
    <st c="36670">That means this is the place where we can make a particular set
    of embeddings (dense or sparse) more influential in the</st> <st c="36790">search
    results.</st>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36332">RFF方法的核心思想是，那些被检索方法高度排名的文档更有可能对查询相关。</st> <st c="36478">通过使用互逆排名，RFF给排名列表顶部出现的文档赋予更大的权重。</st>
    <st c="36584">请注意，我们</st> <st c="36601">正在使用我们在参数中收集的权重来加权总和。</st> <st c="36670">这意味着这是我们可以使特定的一组嵌入（密集或稀疏）在搜索结果中更具影响力的地方。</st>
    <st c="36790"></st>
- en: <st c="36805">This next line sorts the document IDs based on their combined
    reciprocal rank scores in</st> <st c="36894">descending order:</st>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36805">下一行根据它们的组合倒数排名分数在</st> <st c="36894">降序中排序文档ID：</st>
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: <st c="37016">Descending order is indicated by</st> `<st c="37050">reverse=True</st>`<st
    c="37062">. It uses the</st> `<st c="37076">sorted()</st>` <st c="37084">function
    with a</st> `<st c="37101">key</st>` <st c="37104">function that retrieves the
    combined reciprocal rank for each</st> <st c="37167">document ID.</st>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37016">降序由</st> `<st c="37050">reverse=True</st>`<st c="37062">表示。它使用</st>
    `<st c="37076">sorted()</st>` <st c="37084">函数，并使用一个</st> `<st c="37101">key</st>`
    <st c="37104">函数来检索每个</st> <st c="37167">文档ID的组合倒数。</st>
- en: <st c="37179">Our next step is to iterate over the sorted document IDs and retrieve
    the corresponding documents from the dense and sparse</st> <st c="37304">search
    results:</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37179">我们的下一步是遍历排序后的文档ID，并从密集和稀疏</st> <st c="37304">搜索结果中检索相应的文档：</st>
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: <st c="37822">We use</st> <st c="37830">this to indicate the source retriever,
    giving us a better sense of how each of our retrievers is impacting the results.</st>
    <st c="37950">Retrieve the documents based on the sorted document IDs.</st> <st
    c="38007">The resulting ranked list represents the hybrid search results, where
    documents that appear higher in both dense and sparse search rankings will have
    higher</st> <st c="38164">combined scores.</st>
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37822">我们使用</st> <st c="37830">此来指示源检索器，这让我们更好地了解每个检索器如何影响结果。</st> <st
    c="37950">根据排序后的文档ID检索文档。</st> <st c="38007">生成的排名列表代表混合搜索结果，其中在密集和稀疏搜索排名中都较高的文档将具有更高的</st>
    <st c="38164">组合分数。</st>
- en: <st c="38180">Finally, we return</st> <st c="38200">the results:</st>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38180">最后，我们返回</st> <st c="38200">结果：</st>
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '<st c="38235">Note that</st> `<st c="38246">k</st>` <st c="38247">was used
    for both retrievers, giving us twice as many results as we are asking for here.</st>
    <st c="38337">So this is taking those results and cutting them in half, returning
    just the top-</st>`<st c="38418">k</st>`<st c="38420">. What this does in practice
    is if there are results in the lower half of these retrievers, such as rank #8,
    but they are in both results, it is likely to push those results into</st> <st
    c="38599">the top-</st>`<st c="38607">k</st>`<st c="38609">.</st>'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38235">请注意，</st> `<st c="38246">k</st>` <st c="38247">被用于两个检索器，因此我们得到的结果是我们请求的两倍。</st>
    <st c="38337">因此，这是将那些结果减半，只返回</st>`<st c="38418">k</st>`<st c="38420">。实际上，如果这些检索器的下半部分有结果，例如排名#8，但它们同时出现在两个结果中，那么这些结果很可能会被推到</st>
    <st c="38599">前</st>`<st c="38607">k</st>`<st c="38609">。</st>
- en: <st c="38610">Next, we have to account for this new retriever mechanism in our
    LangChain chain.</st> <st c="38693">Update the</st> `<st c="38704">rag_chain_with_source</st>`
    <st c="38725">chain to use the</st> `<st c="38743">hybrid_search</st>` <st c="38756">function
    to return</st> `<st c="38776">context</st>` <st c="38783">like this:</st>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38610">接下来，我们必须在我们的LangChain链中考虑这个新的检索器机制。</st> <st c="38693">更新</st>
    `<st c="38704">rag_chain_with_source</st>` <st c="38725">链，使用</st> `<st c="38743">hybrid_search</st>`
    <st c="38756">函数来返回</st> `<st c="38776">context</st>` <st c="38783">如下：</st>
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: <st c="38935">This completes the code changes for the RAG pipeline to use hybrid
    search.</st> <st c="39011">But we added all of this extra metadata that we want
    to show in our output and analysis.</st> <st c="39100">An extra benefit of building
    this function ourselves is that it lets us print output that you normally wouldn’t
    be able to see if using LangChain’s</st> `<st c="39248">EnsembleRetriever</st>`<st
    c="39265">. Let’s take advantage and replace this cell where we call to the RAG
    pipeline.</st> <st c="39345">Rather than using the final code in the past code
    labs, when processing our RAG pipeline, use</st> <st c="39439">this code:</st>
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38935">这完成了为使用混合搜索对RAG管道进行代码更改的工作。</st> <st c="39011">但我们添加了所有这些额外的元数据，我们希望在输出和分析中显示这些数据。</st>
    <st c="39100">自己构建这个函数的额外好处是，它让我们能够打印出通常在使用LangChain的</st> `<st c="39248">EnsembleRetriever</st>`<st
    c="39265">时无法看到的信息。</st> 让我们利用这个机会，替换掉调用RAG管道的这个单元格。</st> <st c="39345">在处理我们的RAG管道时，我们不是使用过去代码实验室中的最终代码，而是使用</st>
    <st c="39439">以下代码：</st>
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: <st c="40234">This code carries over</st> <st c="40257">what we’ve used in previous
    chapters, such as the relevance score that we used in our security response.</st>
    <st c="40363">We added a printout of each of the results from our retriever and
    the metadata we collected on them.</st> <st c="40464">Here is a sample output
    with the first couple</st> <st c="40510">of results:</st>
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40234">此代码继承了我们在前几章中使用的内容，例如我们在安全响应中使用的相关性分数。</st> <st c="40257">我们添加了从我们的检索器中获取的每个结果的打印输出以及我们收集的元数据。</st>
    <st c="40363">以下是前几个结果的示例输出：</st>
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: <st c="42038">While we are retrieving documents, we print out the document IDs
    so that we can see how many overlap.</st> <st c="42141">Then, for each result,
    we print out the document ID, the ranking score, the rank, and what retriever
    produced that result (including</st> `<st c="42274">both</st>` <st c="42278">if
    both retrieved it).</st> <st c="42302">Note</st> <st c="42306">that I cut off
    the full content here to only show the first 3 results of 10, as it was considerably
    longer in the output.</st> <st c="42429">But if you run this in the notebook,
    you can see the</st> <st c="42482">full output.</st>
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42038">当我们检索文档时，我们会打印出文档ID，以便我们可以看到有多少重叠。</st> <st c="42141">然后，对于每个结果，我们会打印出文档ID、排名分数、排名以及产生该结果的检索器（包括</st>
    `<st c="42274">两者</st>` <st c="42278">如果两者都检索到了）。</st> <st c="42302">注意</st> <st
    c="42306">，我在这里截断了完整内容，只显示10个结果中的前3个，因为输出相当长。</st> <st c="42429">但如果你在笔记本中运行这个，你可以看到</st>
    <st c="42482">完整的输出。</st>
- en: <st c="42494">If you look through the 10 results, the source retriever is</st>
    `<st c="42555">sparse</st>`<st c="42561">,</st> `<st c="42563">dense</st>`<st
    c="42568">,</st> `<st c="42570">both</st>`<st c="42574">,</st> `<st c="42576">sparse</st>`<st
    c="42582">,</st> `<st c="42584">dense</st>`<st c="42589">,</st> `<st c="42591">sparse</st>`<st
    c="42597">,</st> `<st c="42599">dense</st>`<st c="42604">,</st> `<st c="42606">dense</st>`<st
    c="42611">,</st> `<st c="42613">sparse</st>`<st c="42619">, and</st> `<st c="42625">sparse</st>`<st
    c="42631">. This is a relatively even distribution across the different searching
    mechanisms, including one result that came from both, pushing it further up the
    ranking.</st> <st c="42792">The ranking scores were</st> `<st c="42816">0.5</st>`<st
    c="42819">,</st> `<st c="42821">0.5</st>`<st c="42824">,</st> `<st c="42826">0.29</st>`<st
    c="42830">,</st> `<st c="42832">0.25</st>`<st c="42836">, and</st> `<st c="42842">0.25</st>`<st
    c="42846">,</st> `<st c="42848">0.17</st>`<st c="42852">,</st> `<st c="42854">0.125</st>`<st
    c="42859">,</st> `<st c="42861">0.1</st>`<st c="42864">,</st> `<st c="42866">0.1</st>`<st
    c="42869">,</st> `<st c="42871">0.83</st>`<st c="42875">.</st>
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42494">如果你查看这10个结果，源检索器是</st> `<st c="42555">稀疏</st>`<st c="42561">,</st>
    `<st c="42563">密集</st>`<st c="42568">,</st> `<st c="42570">两者</st>`<st c="42574">,</st>
    `<st c="42576">稀疏</st>`<st c="42582">,</st> `<st c="42584">密集</st>`<st c="42589">,</st>
    `<st c="42591">稀疏</st>`<st c="42597">,</st> `<st c="42599">密集</st>`<st c="42604">,</st>
    `<st c="42606">密集</st>`<st c="42611">,</st> `<st c="42613">稀疏</st>`<st c="42619">，以及</st>
    `<st c="42625">稀疏</st>`<st c="42631">。这在不同的搜索机制中是一个相对均匀的分布，包括一个来自两者的结果，这进一步提高了它的排名。</st>
    <st c="42792">排名分数是</st> `<st c="42816">0.5</st>`<st c="42819">,</st> `<st c="42821">0.5</st>`<st
    c="42824">,</st> `<st c="42826">0.29</st>`<st c="42830">,</st> `<st c="42832">0.25</st>`<st
    c="42836">，以及</st> `<st c="42842">0.25</st>`<st c="42846">,</st> `<st c="42848">0.17</st>`<st
    c="42852">,</st> `<st c="42854">0.125</st>`<st c="42859">,</st> `<st c="42861">0.1</st>`<st
    c="42864">,</st> `<st c="42866">0.1</st>`<st c="42869">,</st> `<st c="42871">0.83</st>`<st
    c="42875">。</st>
- en: <st c="42876">This is the response we saw when we were just using</st> `<st
    c="42929">dense</st>` <st c="42934">embeddings:</st>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="42876">这是我们仅使用</st> `<st c="42929">密集</st>` <st c="42934">嵌入时看到的响应：</st>
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: <st c="43767">At this point, judging which version is better is a little subjective,
    but we will cover a more objective approach in</st> [*<st c="43886">Chapter 9</st>*](B22475_09.xhtml#_idTextAnchor184)
    <st c="43895">when we talk about RAG evaluation.</st> <st c="43931">In the meantime,
    let’s just look at a few things that stand out.</st> <st c="43996">Our hybrid
    search version seems to have broader coverage of the</st> <st c="44060">different
    initiatives.</st>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43767">在这个阶段，判断哪个版本更好有点主观，但当我们谈到RAG评估时，我们会介绍一个更客观的方法。</st> <st c="43886">[第9章</st]*](B22475_09.xhtml#_idTextAnchor184)
    <st c="43895">我们将讨论。</st> <st c="43931">同时，让我们看看一些突出的事情。</st> <st c="43996">我们的混合搜索版本似乎对不同倡议的覆盖范围更广。</st>
- en: <st c="44082">This is the hybrid</st> <st c="44102">search approach:</st>
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44082">这是混合</st> <st c="44102">搜索方法：</st>
- en: '[PRE36]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: <st c="44704">This is the</st> <st c="44717">dense</st> <st c="44723">search
    approach:</st>
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44704">这是</st> <st c="44717">密集</st> <st c="44723">搜索方法：</st>
- en: '[PRE37]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: <st c="44992">You might say the dense search approach focuses on more precise
    details, but whether that is a good or bad thing is subjective.</st> <st c="45121">For
    example, you do not see anything about the one billion people goal in the hybrid
    search, but you see it here in the</st> <st c="45241">dense search:</st>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44992">您可能会说密集搜索方法更关注精确的细节，但这是好是坏是主观的。</st> <st c="45121">例如，在混合搜索中，您看不到关于十亿人目标的任何内容，但在这里的</st>
    <st c="45241">密集搜索中可以看到：</st>
- en: '[PRE38]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: <st c="45447">The hybrid search took a more general approach, saying</st> <st
    c="45503">the following:</st>
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45447">混合搜索采用了更通用的方法，说</st> <st c="45503">以下内容：</st>
- en: '[PRE39]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: <st c="45674">You can run this code with other questions and see how they compare
    across the different</st> <st c="45764">search approaches.</st>
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45674">您可以用其他问题运行此代码，看看它们在不同</st> <st c="45764">搜索方法中的比较情况。</st>
- en: <st c="45782">Okay, we did a lot of work to set up this function, but now we
    are going to look at what LangChain is offering and replace our</st> <st c="45910">function
    completely.</st>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45782">好的，我们为设置这个函数做了很多工作，但现在我们将查看LangChain提供的内容，并完全替换我们的</st> <st c="45910">函数。</st>
- en: <st c="45930">Code lab 8.3 – Hybrid search with LangChain’s EnsembleRetriever
    to replace our custom function</st>
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="45930">代码实验室8.3 – 使用LangChain的EnsembleRetriever进行混合搜索以替换我们的自定义函数</st>
- en: <st c="46025">The file you need to access from the GitHub repository is</st>
    <st c="46084">titled</st> `<st c="46091">CHAPTER8-3_HYBRID-ENSEMBLE.ipynb</st>`<st
    c="46123">.</st>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46025">您需要从GitHub仓库访问的文件名为</st> <st c="46084">“</st> <st c="46091">CHAPTER8-3_HYBRID-ENSEMBLE.ipynb</st>”</st>
    <st c="46123">。</st>
- en: <st c="46124">We</st> <st c="46128">continue this code from the last lab starting
    with the</st> `<st c="46183">CHAPTER8-2_HYBRID-CUSTOM.ipynb</st>` <st c="46213">file.</st>
    <st c="46220">The complete code for this code lab is</st> `<st c="46259">CHAPTER8-3_HYBRID-ENSEMBLE.ipynb</st>`<st
    c="46291">. First, we need to import the retriever from LangChain; add this to</st>
    <st c="46360">your imports:</st>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46124">我们从上一个实验室的代码继续，从</st> `<st c="46183">CHAPTER8-2_HYBRID-CUSTOM.ipynb</st>`
    <st c="46213">文件开始。</st> <st c="46220">此代码实验室的完整代码为</st> `<st c="46259">CHAPTER8-3_HYBRID-ENSEMBLE.ipynb</st>`<st
    c="46291">。首先，我们需要从LangChain导入检索器；将其添加到</st> <st c="46360">您的导入中：</st>
- en: '[PRE40]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: <st c="46424">This adds</st> `<st c="46435">EnsembleRetriever</st>` <st c="46452">from
    LangChain to be used as a third retriever that combines the other two retrievers.</st>
    <st c="46540">Note that previously, in</st> *<st c="46565">Code lab 8.2</st>*<st
    c="46577">, we added</st> `<st c="46588">k=10</st>` <st c="46592">to each of the
    two retrievers to make sure we got enough responses to be similar to the</st>
    <st c="46681">other response.</st>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46424">这添加了LangChain中的</st> `<st c="46435">EnsembleRetriever</st>` <st
    c="46452">，用作第三个检索器，结合其他两个检索器。</st> <st c="46540">注意，在之前，在</st> *<st c="46565">代码实验室8.2</st>*<st
    c="46577">中，我们为每个检索器添加了</st> `<st c="46588">k=10</st>` <st c="46592">，以确保我们得到足够的响应，与</st>
    <st c="46681">其他响应相似。</st>
- en: <st c="46696">In the past, we just had one set of documents that we defined
    as</st> `<st c="46762">documents</st>`<st c="46771">, but here we want to change
    the name of those documents to</st> `<st c="46831">dense_documents</st>`<st c="46846">,
    and then add a second set of documents</st> <st c="46887">called</st> `<st c="46894">sparse_documents</st>`<st
    c="46910">:</st>
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46696">在过去，我们只有一套文档，我们将其定义为</st> `<st c="46762">documents</st>`<st c="46771">，但在这里我们想将这些文档的名称更改为</st>
    `<st c="46831">dense_documents</st>`<st c="46846">，然后添加第二套文档</st> <st c="46887">，称为</st>
    `<st c="46894">sparse_documents</st>`<st c="46910">：</st>
- en: '[PRE41]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: <st c="47164">This allowed me to tag the dense documents with the</st> `<st
    c="47217">"dense"</st>` <st c="47224">source and the sparse documents with the</st>
    `<st c="47266">"sparse"</st>` <st c="47274">source in their metadata.</st> <st
    c="47301">We pass this through to the final results and can use it to show the
    source for each document.</st> <st c="47396">This is not as effective as the approach
    we used in our custom function though, because when the content comes from both
    sources, it does not indicate both.</st> <st c="47553">This highlights an advantage
    of creating our</st> <st c="47598">own function.</st>
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47164">这使我能够在密集文档的元数据中标记</st> `<st c="47217">"dense"</st>` <st c="47224">源，并在稀疏文档中标记</st>
    `<st c="47266">"sparse"</st>` <st c="47274">源。</st> <st c="47301">我们将这些传递到最终结果中，并可以使用它来显示每份文档的来源。</st>
    <st c="47396">但这不如我们在自定义函数中使用的方法有效，因为当内容来自两个来源时，它不会指示两个来源。</st> <st c="47553">这突出了我们创建自己的函数的优势。</st>
- en: <st c="47611">We then want to add our new type of retriever,</st> `<st c="47659">EnsembleRetriever</st>`<st
    c="47676">, which we will add to the bottom of the cell where we define the other</st>
    <st c="47748">two retrievers:</st>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47611">然后我们想要添加我们新的检索器类型，</st> `<st c="47659">EnsembleRetriever</st>`<st
    c="47676">，我们将将其添加到定义其他</st> <st c="47748">两个检索器的单元格底部：</st>
- en: '[PRE42]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`<st c="47876">ensemble_retriever</st>` <st c="47895">takes both retrievers,
    weights for how to emphasize them, and a</st> `<st c="47960">c</st>` <st c="47961">value.</st>
    <st c="47969">The</st> `<st c="47973">c</st>` <st c="47974">value is described
    as a constant added to the rank, controlling the balance</st> <st c="48051">between
    the importance of high-ranked items and the consideration given to lower-ranked
    items.</st> <st c="48146">The default is</st> `<st c="48161">60</st>`<st c="48163">,
    but I set it to</st> `<st c="48181">0</st>`<st c="48182">. We don’t have a</st>
    `<st c="48200">c</st>` <st c="48201">parameter in our function, so that would
    make it difficult to compare results!</st> <st c="48281">But that can be a handy
    parameter if you want more IDs to float up from</st> <st c="48353">the bottom.</st>'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="47876">ensemble_retriever</st>` <st c="47895">接受两个检索器，如何强调它们的权重，以及一个</st>
    `<st c="47960">c</st>` <st c="47961">值。</st> <st c="47969">`<st c="47973">c</st>`
    <st c="47974">值被描述为一个添加到排名中的常数，控制着高排名项的重要性和对低排名项考虑之间的平衡。</st> <st c="48051">默认值是</st>
    `<st c="48161">60</st>`<st c="48163">，但我将其设置为</st> `<st c="48181">0</st>`<st c="48182">。在我们的函数中没有`<st
    c="48200">c</st>` <st c="48201">参数，这会使比较结果变得困难！</st> <st c="48281">但如果你想让更多ID从底部浮上来，这可以是一个很有用的参数。</st>'
- en: <st c="48364">You can remove our</st> `<st c="48384">hybrid_search</st>` <st
    c="48397">function altogether.</st> <st c="48419">Delete the entire cell that
    starts with</st> <st c="48459">this code:</st>
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48364">您可以完全删除我们的</st> `<st c="48384">hybrid_search</st>` <st c="48397">函数。</st>
    <st c="48419">删除以</st> <st c="48459">此代码</st> 开头的整个单元格：</st>
- en: '[PRE43]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: <st c="48538">Next, we update the</st> `<st c="48559">"context"</st>` <st c="48568">input
    from</st> `<st c="48580">rag_chain_with_source</st>` <st c="48601">with the</st>
    <st c="48611">new retriever:</st>
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48538">接下来，我们更新</st> `<st c="48559">"上下文"</st>` <st c="48568">输入</st>
    `<st c="48580">rag_chain_with_source</st>` <st c="48601">，使用</st> <st c="48611">新的检索器：</st>
- en: '[PRE44]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: <st c="48771">Now our code for output has to change because we no longer have
    all that metadata we were able to add with the</st> <st c="48883">custom function:</st>
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48771">现在我们的输出代码必须更改，因为我们不再拥有我们能够通过</st> <st c="48883">自定义函数</st> 添加的所有元数据：</st>
- en: '[PRE45]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: <st c="49493">The</st> <st c="49498">output looks</st> <st c="49511">like this:</st>
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="49493">输出</st> <st c="49498">看起来像这样：</st>
- en: '[PRE46]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: <st c="50612">The result is</st> <st c="50626">almost exactly the same as our
    function, but with the dense search results winning any ties in the order (in
    our function, the sparse result is winning), which is pretty minor, but something
    you can easily address by changing the weights.</st> <st c="50866">Remember that</st>
    `<st c="50880">c</st>` <st c="50881">value though?</st> <st c="50896">If you change
    that, you see big changes in the results.</st> <st c="50952">With more time, we
    should go back and add a</st> `<st c="50996">c</st>` <st c="50997">value to our
    function, but</st> <st c="51025">I digress!</st>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50612">结果是</st> <st c="50626">几乎与我们的函数完全相同，但密集搜索结果在顺序中赢得了任何平局（在我们的函数中，稀疏结果是获胜的），这是一个很小的变化，但你可以通过改变权重轻松解决。</st>
    <st c="50866">不过，记得那个</st> `<st c="50880">c</st>` <st c="50881">值吗？</st> <st c="50896">如果你改变它，你会看到结果中的重大变化。</st>
    <st c="50952">如果有更多时间，我们应该回到我们的函数中添加一个</st> `<st c="50996">c</st>` <st c="50997">值，但我跑题了！
- en: <st c="51035">Building our own function certainly gave us more flexibility and
    allowed us to see and change the inner workings of the function.</st> <st c="51166">With
    the LangChain</st> `<st c="51185">EnsembleRetriever</st>`<st c="51202">, we cannot
    change any steps in the search or ranking to better fit our needs and we have
    little quirks such as the</st> `<st c="51318">"source"</st>` <st c="51326">metadata
    issue where we don’t know whether or when it is coming from both sources.</st>
    <st c="51410">It is difficult to judge what is a better approach from this small
    example.</st> <st c="51486">The reality is that everything you do will need consideration
    and you will have to decide for yourself what works in</st> <st c="51603">your
    situation.</st>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51035">构建我们自己的函数当然给了我们更多的灵活性，并允许我们查看和改变函数的内部工作原理。</st> <st c="51166">使用
    LangChain 的 `<st c="51185">EnsembleRetriever</st>`<st c="51202">，我们无法更改搜索或排名中的任何步骤以更好地满足我们的需求，并且存在一些小问题，例如</st>
    `<st c="51318">"source"</st>` <st c="51326">元数据问题，我们不知道它是否或何时来自两个来源。</st> <st
    c="51410">从这个小例子中很难判断哪种方法更好。</st> <st c="51486">现实是，你做的每一件事都需要考虑，你必须自己决定在你的情况下什么有效。</st>
- en: <st c="51618">If hybrid search is important, you may want to consider a vector
    database or vector search service that gives you more features and flexibility
    in defining your hybrid search.</st> <st c="51795">LangChain provides weights
    that allow you to emphasize one of the search mechanisms over the other, but as
    of right now, you can only use the built-in ranking mechanism in the RRF.</st>
    <st c="51976">Weaviate, for example, lets you pick from two different ranking
    algorithms.</st> <st c="52052">This is yet another consideration to take into
    account when making decisions on what infrastructure to use in your</st> <st c="52167">RAG
    pipeline.</st>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51618">如果混合搜索很重要，你可能想要考虑一个提供更多功能和灵活性的向量数据库或向量搜索服务，以便定义你的混合搜索。</st> <st
    c="51795">LangChain 提供了权重，允许你强调一种搜索机制而不是另一种，但截至目前，你只能使用 RRF 中的内置排名机制。</st> <st
    c="51976">例如，Weaviate 允许你从两种不同的排名算法中选择。</st> <st c="52052">这是在决定在你的 RAG 管道中使用什么基础设施时需要考虑的另一个因素。</st>
- en: <st c="52180">Next, let’s talk about the algorithms that use</st> <st c="52228">these
    distances.</st>
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52180">接下来，让我们谈谈使用这些距离的算法。</st>
- en: <st c="52244">Semantic search algorithms</st>
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="52244">语义搜索算法</st>
- en: <st c="52271">We have discussed the concept of a semantic search in depth.</st>
    <st c="52333">Our next step is to walk through the different approaches we can
    take to conduct a semantic search.</st> <st c="52433">These are the actual search
    algorithms that use things such as the distance metrics we’ve already discussed
    (Euclidean distance, dot product, and cosine similarity) to conduct their search
    of dense embeddings.</st> <st c="52643">We start with</st> **<st c="52657">k-nearest</st>**
    **<st c="52667">neighbors</st>** <st c="52676">(</st>**<st c="52678">k-NN</st>**<st
    c="52682">).</st>
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52271">我们已经深入讨论了语义搜索的概念。</st> <st c="52333">我们的下一步是探讨我们可以采取的不同方法来进行语义搜索。</st>
    <st c="52433">这些是实际使用的搜索算法，它们使用我们之前讨论过的距离度量（欧几里得距离、点积和余弦相似度）来对其密集嵌入进行搜索。</st>
    <st c="52643">我们从</st> **<st c="52657">k-最近</st>** **<st c="52667">邻域</st>** <st
    c="52676">(</st>**<st c="52678">k-NN</st>**<st c="52682">).</st>
- en: <st c="52685">k-NN</st>
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="52685">k-NN</st>
- en: <st c="52690">One way to</st> <st c="52701">find</st> <st c="52707">similar
    vectors is through brute force.</st> <st c="52747">With brute force, you find
    the distances between the query and all the data vectors.</st> <st c="52832">Then,
    you sort the distances from closest to furthest, returning a</st> <st c="52898">certain
    number of results.</st> <st c="52926">You can cut off the results based on a threshold,
    or you can define a set number to return, such as</st> `<st c="53026">5</st>`<st
    c="53027">. The set number is called</st> `<st c="53054">k</st>`<st c="53055">,
    so you would say</st> `<st c="53074">k=5</st>`<st c="53077">. This is known in
    classical machine learning as the k-NN algorithm.</st> <st c="53146">This is a
    straightforward algorithm, but its performance degrades as the dataset grows.</st>
    <st c="53234">The increase in computational cost for this algorithm is linear
    based on the amount of data you are querying.</st> <st c="53344">The time complexity
    is represented by</st> `<st c="53382">O(n * d)</st>`<st c="53390">, where</st>
    `<st c="53398">n</st>` <st c="53399">is the number of instances in the training
    dataset and</st> `<st c="53455">d</st>` <st c="53456">is the dimensionality of
    the data.</st> <st c="53492">This means that if your data doubles, the query time
    will double.</st> <st c="53558">For large datasets that reach into the millions
    or even billions of data points, brute force comparison between every pair of
    items can become</st> <st c="53701">computationally infeasible.</st>
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52690">一种找到</st> <st c="52701">相似向量</st>的方法是通过暴力搜索。</st> <st c="52747">使用暴力搜索，你将查询向量与所有数据向量之间的距离计算出来。</st>
    <st c="52832">然后，你将距离从最近到最远进行排序，返回一定数量的结果。</st> <st c="52898">你可以根据阈值截断结果，或者定义一个返回的固定数量，例如</st>
    `<st c="53026">5</st>`<st c="53027">。这个固定数量被称为</st> `<st c="53054">k</st>`<st
    c="53055">，所以你会说</st> `<st c="53074">k=5</st>`<st c="53077">。这在经典机器学习中被称为k-NN算法。</st>
    <st c="53146">这是一个直接的算法，但随着数据集的增长，其性能会下降。</st> <st c="53234">该算法的计算成本增加与你要查询的数据量呈线性关系。</st>
    <st c="53344">时间复杂度用</st> `<st c="53382">O(n * d)</st>`<st c="53390">表示，其中</st>
    `<st c="53398">n</st>` <st c="53399">是训练数据集中的实例数量，而</st> `<st c="53455">d</st>`
    <st c="53456">是数据的维度。</st> <st c="53492">这意味着如果你的数据量加倍，查询时间也会加倍。</st> <st c="53558">对于包含数百万甚至数十亿数据点的庞大数据集，对每一对项目进行暴力比较可能变得</st>
    <st c="53701">在计算上不可行。</st>
- en: <st c="53728">If you have a relatively small dataset, it may be worth considering
    k-NN, as it is considered to be more accurate than the next approach we discuss.</st>
    <st c="53878">What constitutes</st> *<st c="53895">small</st>* <st c="53900">can
    be dependent on your data and embedding dimensions, but I have used k-NN successfully
    for projects with 25,000 to 30,000 embeddings and 256 dimensions.</st> <st c="54057">I’ve
    seen a 2–6% improvement in the retrieval evaluation metrics we will talk about
    in</st> [*<st c="54144">Chapter 9</st>*](B22475_09.xhtml#_idTextAnchor184)<st
    c="54153">, which is significant enough for me to offset the small increase in</st>
    <st c="54222">computation cost.</st>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="53728">如果你有一个相对较小的数据集，考虑k-NN可能是有价值的，因为它被认为比我们接下来要讨论的下一个方法更准确。</st> <st
    c="53878">什么构成</st> *<st c="53895">小</st>* <st c="53900">可能取决于你的数据和嵌入维度，但我已经成功地在具有25,000到30,000个嵌入和256维度的项目中使用了k-NN。</st>
    <st c="54057">我观察到在即将讨论的[第9章](B22475_09.xhtml#_idTextAnchor184)中提到的检索评估指标上提高了2-6%，这对于我来说足以抵消计算成本的小幅增加。</st>
- en: <st c="54239">But what about all the distance metrics we just discussed; where
    do those come in with k-NN?</st> <st c="54333">k-NN can use any of those distance
    metrics to determine the similarity between the query vector and the vectors in
    the dataset.</st> <st c="54461">The most common distance metric used in k-NN is
    Euclidean distance.</st> <st c="54529">Other distance metrics, such as Manhattan
    distance (also known as city block distance) or cosine similarity, can also be
    used, depending on the nature of the data and the problem at hand.</st> <st c="54717">The
    choice of distance metric can significantly impact the performance of the k-NN
    algorithm.</st> <st c="54811">Once the distances between the query vector and
    all the vectors in the dataset are calculated, k-NN sorts the distances and</st>
    <st c="54934">selects the</st> `<st c="54947">k</st>` <st c="54948">nearest neighbors
    based on the chosen</st> <st c="54987">distance metric.</st>
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54239">但是，我们刚才讨论的所有距离度量；在 k-NN 中它们是如何应用的呢？</st> <st c="54333">k-NN 可以使用这些距离度量中的任何一个来确定查询向量与数据集中的向量之间的相似性。</st>
    <st c="54461">在 k-NN 中最常用的距离度量是欧几里得距离。</st> <st c="54529">根据数据的性质和问题的性质，也可以使用其他距离度量，例如曼哈顿距离（也称为城市街区距离）或余弦相似度。</st>
    <st c="54717">距离度量的选择可以显著影响 k-NN 算法的性能。</st> <st c="54811">一旦计算了查询向量与数据集中所有向量的距离，k-NN
    就会根据所选的</st> <st c="54934">距离度量对距离进行排序，并</st> <st c="54947">选择</st> `<st c="54948">k</st>`
    <st c="54948">个最近邻。</st>
- en: <st c="55003">If you find your dataset has outgrown k-NN, there are many other
    algorithms that allow us to find the nearest vectors in a more efficient way.</st>
    <st c="55147">In general, we call this</st> **<st c="55172">Approximate Nearest
    Neighbors</st>** <st c="55201">(</st>**<st c="55203">ANN</st>**<st c="55206">),
    which we will</st> <st c="55224">discuss next.</st>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55003">如果你发现你的数据集已经超过了 k-NN 的处理能力，有许多其他算法可以更有效地找到最近向量。</st> <st c="55147">通常，我们称之为</st>
    **<st c="55172">近似最近邻</st>** <st c="55201">(</st>**<st c="55203">ANN</st>**<st
    c="55206">)，我们将在下一节中讨论。</st>
- en: <st c="55237">ANN</st>
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="55237">ANN</st>
- en: <st c="55241">ANN is a</st> <st c="55251">family of</st> <st c="55260">algorithms
    designed to address the scalability limitations of k-NN while still providing
    satisfactory results.</st> <st c="55372">ANN algorithms aim to find the most similar
    vectors to a query vector in a more efficient manner, sacrificing some accuracy
    for</st> <st c="55500">improved performance.</st>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55241">ANN 是一组</st> <st c="55251">算法，旨在解决 k-NN 的可扩展性限制，同时仍然提供令人满意的结果。</st>
    <st c="55372">ANN 算法旨在以更有效的方式找到与查询向量最相似的向量，牺牲一些准确性以换取</st> <st c="55500">改进的性能。</st>
- en: <st c="55521">Compared to k-NN, which performs an exhaustive search by calculating
    the distances between the query vector and all data vectors, ANN algorithms employ
    various techniques to reduce the search space and speed up the retrieval process.</st>
    <st c="55756">These techniques include indexing, partitioning, and approximation
    methods that allow ANN algorithms to focus on a subset of the data points that
    are likely to be the</st> <st c="55923">nearest neighbors.</st>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55521">与通过计算查询向量与所有数据向量之间的距离进行穷举搜索的 k-NN 相比，ANN 算法采用各种技术来减少搜索空间并加快检索过程。</st>
    <st c="55756">这些技术包括索引、分区和近似方法，这些方法允许 ANN 算法专注于可能成为</st> <st c="55923">最近邻的数据点的子集。</st>
- en: <st c="55941">One key difference between k-NN and ANN is the trade-off between
    accuracy and efficiency.</st> <st c="56032">While k-NN guarantees finding the
    exact</st> `<st c="56072">k</st>` <st c="56073">nearest neighbors, it becomes
    computationally expensive as the dataset grows.</st> <st c="56152">On the other
    hand, ANN algorithms prioritize efficiency by approximating the nearest neighbors,
    accepting the possibility of missing some of the true nearest neighbors in exchange
    for faster</st> <st c="56343">retrieval times.</st>
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55941">k-NN 和 ANN 之间的一个关键区别在于准确性和效率之间的权衡。</st> <st c="56032">虽然 k-NN
    保证找到精确的</st> `<st c="56072">k</st>` <st c="56073">最近邻，但随着数据集的增长，计算成本变得很高。</st>
    <st c="56152">另一方面，ANN 算法通过近似最近邻来优先考虑效率，以换取更快的</st> <st c="56343">检索时间。</st>
- en: <st c="56359">ANN algorithms often</st> <st c="56380">leverage</st> **<st c="56390">indexing
    structures</st>** <st c="56409">such as</st> **<st c="56418">hierarchical trees</st>**
    <st c="56436">(e.g., KD-trees, Ball trees),</st> **<st c="56467">hashing techniques</st>**
    <st c="56485">(e.g.,</st> **<st c="56493">Locality-Sensitive Hashing</st>** <st
    c="56519">(</st>**<st c="56521">LSH</st>**<st c="56524">)), or</st> **<st c="56532">graph-based
    methods</st>** <st c="56551">(e.g.,</st> **<st c="56559">Hierarchical Navigable
    Small World</st>** <st c="56593">(</st>**<st c="56595">HNSW</st>**<st c="56599">))
    to</st> <st c="56606">organize</st> <st c="56614">the data</st> <st c="56624">points
    in a way that facilitates efficient</st> <st c="56666">search.</st> <st c="56675">These
    indexing structures allow ANN algorithms to quickly narrow down the search space
    and identify candidate neighbors without exhaustively comparing the query vector
    to every data point.</st> <st c="56864">We will talk in more depth about indexing
    approaches for ANN in the</st> <st c="56932">next section.</st>
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56359">ANN算法通常</st> <st c="56380">利用</st> **<st c="56390">索引结构</st>**
    <st c="56409">，例如</st> **<st c="56418">层次树</st>** <st c="56436">(例如，KD树、球树)，</st>
    **<st c="56467">哈希技术</st>** <st c="56485">(例如，</st> **<st c="56493">局部敏感哈希</st>**
    <st c="56519">(**<st c="56521">LSH</st>**<st c="56524">))，或</st> **<st c="56532">基于图的方法</st>**
    <st c="56551">(例如，</st> **<st c="56559">层次可导航小世界</st>** <st c="56593">(**<st c="56595">HNSW</st>**<st
    c="56599">))来</st> <st c="56606">组织</st> <st c="56614">数据点</st> <st c="56624">，以便于高效</st>
    <st c="56666">搜索。</st> <st c="56675">这些索引结构允许ANN算法快速缩小搜索空间并识别候选邻居，而无需将查询向量与每个数据点进行详尽的比较。</st>
    <st c="56864">我们将在下一节中更深入地讨论ANN的索引方法。</st>
- en: <st c="56945">The time complexity of ANN algorithms varies depending on the
    specific algorithm and indexing technique used.</st> <st c="57056">However, in
    general, ANN algorithms aim to achieve sublinear search times, meaning that the
    query time grows more slowly than the size of the dataset.</st> <st c="57207">This
    makes ANN algorithms more suitable for large-scale datasets where the computational
    cost of k-NN</st> <st c="57309">becomes prohibitive.</st>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56945">ANN算法的时间复杂度因具体算法和索引技术而异。</st> <st c="57056">然而，一般来说，ANN算法旨在实现亚线性搜索时间，这意味着查询时间增长速度低于数据集的大小。</st>
    <st c="57207">这使得ANN算法更适合大规模数据集，其中k-NN的计算成本变得过高。</st>
- en: <st c="57329">Again, what</st> <st c="57341">about those distance metrics?</st>
    <st c="57372">Well, like k-NN, ANN algorithms rely on distance metrics to measure</st>
    <st c="57440">the similarity between the query vector and the vectors in the dataset.</st>
    <st c="57512">The choice of distance metric depends on the nature of the data
    and the problem at hand.</st> <st c="57601">Common distance metrics used in ANN
    include Euclidean distance, Manhattan distance, and cosine similarity.</st> <st
    c="57708">However, unlike k-NN, which calculates distances between the query vector
    and all data vectors, ANN algorithms employ indexing structures and approximation
    techniques to reduce the number of distance calculations.</st> <st c="57922">These
    techniques allow ANN algorithms to quickly identify a subset of candidate neighbors
    that are likely to be close to the query vector.</st> <st c="58061">The distance
    metric is then applied to this subset to determine the approximate nearest neighbors,
    rather than computing distances for the entire dataset.</st> <st c="58216">By
    minimizing the number of distance calculations, ANN algorithms can significantly
    speed up the retrieval process while still providing</st> <st c="58353">satisfactory
    results.</st>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57329">再次，那些距离度量标准是什么？</st> <st c="57341">嗯，就像k-NN一样，ANN算法依赖于距离度量标准来衡量</st>
    <st c="57440">查询向量和数据集中向量之间的相似度。</st> <st c="57512">距离度量标准的选择取决于数据的性质和待解决的问题。</st>
    <st c="57601">在ANN中常用的距离度量标准包括欧几里得距离、曼哈顿距离和余弦相似度。</st> <st c="57708">然而，与k-NN不同，k-NN计算查询向量和所有数据向量之间的距离，ANN算法采用索引结构和近似技术来减少距离计算的次数。</st>
    <st c="57922">这些技术允许ANN算法快速识别出可能接近查询向量的候选邻居子集。</st> <st c="58061">然后，将距离度量标准应用于这个子集，以确定近似最近邻，而不是计算整个数据集的距离。</st>
    <st c="58216">通过最小化距离计算的次数，ANN算法可以显著加快检索过程，同时仍然提供</st> <st c="58353">令人满意的结果。</st>
- en: <st c="58374">It’s important to note that the choice between k-NN and ANN depends
    on the specific requirements of the application.</st> <st c="58492">If exact nearest
    neighbors are critical and the dataset is relatively small, k-NN may still be
    a viable option.</st> <st c="58604">However, when dealing with massive datasets
    or when near real-time retrieval is required, ANN algorithms provide a practical
    solution by striking a balance between accuracy</st> <st c="58777">and efficiency.</st>
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58374">需要注意的是，k-NN和ANN之间的选择取决于应用的特定要求。</st> <st c="58492">如果精确的最近邻至关重要且数据集相对较小，k-NN可能仍然是一个可行的选项。</st>
    <st c="58604">然而，当处理大规模数据集或需要近实时检索时，ANN算法通过在准确性和效率之间取得平衡，提供了一个实用的解决方案。</st> <st
    c="58777">和效率。</st>
- en: <st c="58792">In summary, ANN algorithms can offer a more scalable and efficient
    alternative to k-NN for finding similar vectors in large datasets.</st> <st c="58927">By
    employing indexing techniques and approximation methods, ANN algorithms can significantly
    reduce the search space and retrieval times, making them suitable for applications
    that require fast and scalable</st> <st c="59134">similarity search.</st>
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58792">总之，ANN算法可以为在大数据集中找到相似向量提供比k-NN更可扩展和高效的替代方案。</st> <st c="58927">通过采用索引技术和近似方法，ANN算法可以显著减少搜索空间和检索时间，使其适用于需要快速和可扩展</st>
    <st c="59134">相似性搜索的应用。</st>
- en: <st c="59152">While it is important to understand what ANN is, it is just as
    important to know that the real benefit is in all the ways you can enhance it.</st>
    <st c="59295">Let’s review some of those</st> <st c="59322">techniques next.</st>
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59152">虽然了解ANN是什么很重要，但同样重要的是要知道真正的益处在于所有可以增强它的方式。</st> <st c="59295">接下来，让我们回顾一些</st>
    <st c="59322">这些技术。</st>
- en: <st c="59338">Enhancing search with indexing techniques</st>
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="59338">使用索引技术增强搜索</st>
- en: <st c="59380">ANN and k-NN search</st> <st c="59401">are fundamental solutions
    in computer science and machine learning, with applications in various domains
    such as image retrieval, recommendation systems, and similarity search.</st> <st
    c="59578">While search algorithms play a crucial role in ANN and k-NN, indexing
    techniques and data structures are equally important for enhancing the efficiency
    and performance of</st> <st c="59749">these algorithms.</st>
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59380">ANN和k-NN搜索</st> <st c="59401">是计算机科学和机器学习中的基本解决方案，在图像检索、推荐系统和相似性搜索等各个领域都有应用。</st>
    <st c="59578">虽然搜索算法在ANN和k-NN中起着至关重要的作用，但索引技术和数据结构对于提高这些算法的效率和性能同样重要。</st>
- en: <st c="59766">These</st> <st c="59773">indexing techniques are used to optimize
    the search process by reducing the number of vectors that need to be compared
    during the search.</st> <st c="59911">They help in quickly identifying a smaller
    subset of candidate vectors that are likely to be similar to the query vector.</st>
    <st c="60033">The search algorithms (such as k-NN, ANN, or other similarity search
    algorithms) can then operate on this reduced set of candidate vectors to find
    the actual nearest neighbors or</st> <st c="60212">similar vectors.</st>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="59766">这些</st> <st c="59773">索引技术用于通过减少搜索过程中需要比较的向量数量来优化搜索过程。</st> <st
    c="59911">它们有助于快速识别出可能类似于查询向量的较小候选向量子集。</st> <st c="60033">然后，搜索算法（如k-NN、ANN或其他相似性搜索算法）可以在这个减少的候选向量集上操作，以找到实际的最近邻或</st>
    <st c="60212">相似向量。</st>
- en: <st c="60228">All these techniques aim to improve the efficiency and scalability
    of similarity search by reducing the search space and enabling faster retrieval
    of relevant vectors.</st> <st c="60397">However, each has its own advantages and
    trade-offs in terms of indexing time, search time, memory usage, and accuracy.</st>
    <st c="60517">The choice of technique depends on the specific requirements of
    the application, such as the dimensionality of the vectors, the desired level
    of accuracy, and the available</st> <st c="60690">computational resources.</st>
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="60228">所有这些技术旨在通过减少搜索空间并允许更快地检索相关向量来提高相似性搜索的效率和可扩展性。</st> <st c="60397">然而，每种技术都有其自己的优势以及在索引时间、搜索时间、内存使用和准确性方面的权衡。</st>
    <st c="60517">技术的选择取决于应用的特定要求，例如向量的维度、所需的精度水平和可用的</st> <st c="60690">计算资源。</st>
- en: <st c="60714">In practice, these techniques can be used independently or in
    combination to achieve the best performance for a given vector search task.</st>
    <st c="60853">Some libraries and frameworks, such as Facebook AI Similarity Search
    (FAISS) and pgvector, provide implementations of multiple indexing techniques,
    including PQ (product quantization), HNSW, and LSH, allowing users to choose the
    most suitable technique for their specific</st> <st c="61125">use case.</st>
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这些技术可以独立使用或组合使用，以达到给定向量搜索任务的最佳性能。<st c="60714">一些库和框架，如Facebook AI相似性搜索（FAISS）和pgvector，提供了多种索引技术的实现，包括PQ（产品量化）、HNSW和LSH，使用户能够为他们的特定用例选择最合适的技巧。</st>
- en: <st c="61134">Before we dive in, let’s review where we are so far.</st> <st
    c="61188">There are distance/similarity metrics (e.g., cosine similarity, Euclidean
    distance, and dot product) used by the search algorithms.</st> <st c="61320">These
    search algorithms include k-NN, ANN, and others.</st> <st c="61375">The search
    algorithms can use indexing techniques such as LSH, KD-trees, Ball trees, PQ,
    and HNSW to improve their efficiency</st> <st c="61501">and scalability.</st>
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入之前，让我们回顾一下到目前为止我们所处的位置。<st c="61134">搜索算法使用距离/相似度度量（例如，余弦相似度、欧几里得距离和点积）。</st>这些搜索算法包括k-NN、ANN和其他算法。<st
    c="61320">这些搜索算法可以使用LSH、KD树、球树、PQ和HNSW等索引技术来提高它们的效率和可扩展性。</st>
- en: <st c="61517">Okay, are we all caught up?</st> <st c="61546">Great!</st> <st
    c="61553">Let’s talk more about several indexing techniques that complement search
    algorithms and improve the overall efficiency of</st> <st c="61675">ANN search:</st>
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们都跟上了吗？<st c="61517">太好了！</st>让我们更深入地讨论一些补充搜索算法并提高ANN搜索整体效率的索引技术：
- en: '**<st c="61686">LSH</st>**<st c="61690">: LSH is an</st> <st c="61703">indexing
    technique that maps similar</st> <st c="61740">vectors to the same hash buckets
    with high probability.</st> <st c="61796">The goal of LSH is to quickly identify
    potential candidate vectors for similarity search by reducing the search space.</st>
    <st c="61915">It achieves this by dividing the</st> <st c="61947">vector</st>
    <st c="61955">space into regions using hash functions, where similar items are
    more likely to be hashed to the</st> <st c="62052">same bucket.</st>'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="61686">LSH</st>**<st c="61690">：LSH是一种将相似向量以高概率映射到相同哈希桶的索引技术。<st c="61703">LSH的目标是通过减少搜索空间来快速识别潜在候选向量以进行相似性搜索。</st>它通过使用哈希函数将向量空间划分为区域来实现这一点，其中相似的项目更有可能被哈希到相同的桶中。'
- en: <st c="62064">LSH offers a</st> <st c="62078">trade-off between accuracy and
    efficiency.</st> <st c="62121">By using LSH as a preprocessing step, the set of
    vectors that need to be examined by the search algorithm can be significantly
    narrowed down.</st> <st c="62263">This reduces the computational overhead and
    improves the overall</st> <st c="62328">search performance.</st>
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LSH提供了一种在准确性和效率之间的折衷方案。<st c="62064">通过将LSH作为预处理步骤，可以显著缩小需要由搜索算法检查的向量集。</st>这减少了计算开销并提高了整体搜索性能。
- en: '**<st c="62347">Tree-based indexing</st>**<st c="62367">: Tree-based indexing</st>
    <st c="62389">techniques</st> <st c="62400">organize vectors into hierarchical
    structures based on their spatial properties.</st> <st c="62482">Two popular tree-based
    indexing techniques</st> <st c="62525">are</st> **<st c="62529">KD-trees</st>**
    <st c="62537">and</st> **<st c="62542">Ball trees</st>**<st c="62552">.</st>'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="62347">基于树的索引</st>**<st c="62367">：基于树的索引技术根据向量的空间属性将向量组织成层次结构。<st
    c="62389">两种流行的基于树的索引技术</st>是**<st c="62529">KD树</st>**和**<st c="62542">球树</st>**。'
- en: <st c="62553">KD-trees</st> <st c="62562">are binary space partitioning</st>
    <st c="62592">trees used for organizing points in a</st> *<st c="62631">k</st>*<st
    c="62632">-dimensional space.</st> <st c="62652">They recursively divide the space
    into subregions based on the dimensions of the vectors.</st> <st c="62742">During
    the search process, KD-trees enable efficient nearest neighbor search by pruning
    irrelevant branches of</st> <st c="62853">the tree.</st>
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: KD-trees是用于在*<st c="62631">k</st>维空间中组织点的二叉空间划分树。<st c="62553">它们根据向量的维度递归地将空间划分为子区域。</st>在搜索过程中，KD树通过剪枝树的不相关分支来启用高效的最近邻搜索。
- en: <st c="62862">Ball trees, on</st> <st c="62877">the other hand, partition the
    data points</st> <st c="62919">into nested hyperspheres.</st> <st c="62946">Each
    node in the tree represents a hypersphere that encapsulates a subset of the data
    points.</st> <st c="63040">Ball trees are particularly effective for nearest-neighbor
    search in</st> <st c="63109">high-dimensional spaces.</st>
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="62862">另一方面，球树将数据点</st> <st c="62877">划分为嵌套的超球体。</st> <st c="62919">树中的每个节点代表一个包含数据点子集的超球体。</st>
    <st c="62946">球树特别适用于高维空间中的最近邻搜索。</st> <st c="63040">球树特别适用于高维空间中的最近邻搜索。</st>
- en: <st c="63133">Both KD-trees and Ball trees provide a way to efficiently navigate
    through possible candidates and speed up the</st> <st c="63246">search process.</st>
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="63133">KD树和球树都提供了一种高效导航可能候选的方法，从而加快搜索过程。</st> <st c="63246">搜索过程。</st>
- en: '**<st c="63261">PQ</st>**<st c="63264">: PQ is a</st> <st c="63274">compression
    and indexing technique that quantizes</st> <st c="63324">vectors into a set of
    sub-vectors and represents them using code books.</st> <st c="63397">Do you remember
    the earlier discussion about quantizing vectors?</st> <st c="63462">We use those
    same concepts here.</st> <st c="63495">PQ allows for compact</st> <st c="63516">storage
    and efficient distance computation by approximating the distances between the
    query vector and the</st> <st c="63624">quantized vectors.</st>'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="63261">PQ</st>**<st c="63264">：PQ是一种压缩和索引技术，它将向量量化为一组子向量，并使用代码簿来表示它们。</st>
    <st c="63324">你还记得之前关于量化向量的讨论吗？</st> <st c="63397">我们在这里使用相同的概念。</st> <st c="63462">我们在这里使用那些相同的概念。</st>
    <st c="63495">PQ通过近似查询向量与量化向量之间的距离，允许紧凑的存储和高效的距离计算。</st> <st c="63516">PQ通过近似查询向量与量化向量之间的距离，允许紧凑的存储和高效的距离计算。</st>'
- en: <st c="63642">PQ is particularly</st> <st c="63661">effective for high-dimensional
    vectors and has been widely used in applications like image retrieval and recommendation
    systems.</st> <st c="63791">By compressing the vectors and approximating distances,
    PQ reduces the memory footprint and computational cost of</st> <st c="63905">similarity
    search.</st>
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="63642">PQ特别适用于高维向量，并在图像检索和推荐系统等应用中被广泛使用。</st> <st c="63661">通过压缩向量和近似距离，PQ减少了相似性搜索的内存占用和计算成本。</st>
    <st c="63791">通过压缩向量和近似距离，PQ减少了相似性搜索的内存占用和计算成本。</st>
- en: '**<st c="63923">HNSW</st>**<st c="63928">: HNSW is a</st> <st c="63940">graph-based
    indexing technique that</st> <st c="63977">builds a hierarchical structure of
    interconnected nodes to enable fast approximate nearest neighbor search.</st>
    <st c="64085">It creates a multi-layer graph where each layer represents a different
    level of abstraction, allowing for efficient traversal and retrieval of approximate</st>
    <st c="64240">nearest neighbors.</st>'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="63923">HNSW</st>**<st c="63928">：HNSW是一种基于图的索引技术，它构建了一个相互连接的节点层次结构，以实现快速近似最近邻搜索。</st>
    <st c="63940">它创建了一个多层图，其中每一层代表不同的抽象级别，允许高效地遍历和检索近似最近邻。</st> <st c="63977">它通过创建一个多层的图结构，每一层代表不同的抽象级别，从而实现高效的遍历和检索近似最近邻。</st>
    <st c="64085">它创建了一个多层图，其中每一层代表不同的抽象级别，允许高效地遍历和检索近似最近邻。</st> <st c="64240">它通过创建一个多层图，每一层代表不同的抽象级别，允许高效地遍历和检索近似最近邻。</st>'
- en: <st c="64258">HNSW is highly</st> <st c="64274">scalable and solves the runtime
    complexity issues of brute-force KNN search.</st> <st c="64351">It is offered
    by the most advanced vector databases and has gained popularity due to its high
    performance and scalability, especially for</st> <st c="64489">high-dimensional
    data.</st>
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="64258">HNSW具有高度的</st> <st c="64274">可扩展性，并解决了暴力KNN搜索的运行时复杂性问题。</st> <st
    c="64351">它由最先进的向量数据库提供，由于其高性能和可扩展性而受到欢迎，尤其是在处理</st> <st c="64489">高维数据时。</st>
- en: <st c="64511">The NSW part of HNSW works by finding vectors that are well-positioned
    among many other vectors (closeness-wise) relative to other vectors.</st> <st
    c="64652">These vectors become the starting points for the search.</st> <st c="64709">The
    number of connections counted among nodes can be defined, allowing for the selection
    of the best-positioned nodes to connect to a large number of</st> <st c="64859">other
    nodes.</st>
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: <st c="64511">HNSW中的NSW部分通过寻找相对于其他向量（在接近度方面）位置良好的向量来工作。</st> <st c="64652">这些向量成为搜索的起点。</st>
    <st c="64709">可以在节点之间定义连接的数量，允许选择最佳位置的节点连接到大量其他节点。</st> <st c="64859">可以在节点之间定义连接的数量，允许选择最佳位置的节点连接到大量其他节点。</st>
- en: <st c="64871">During a query, the</st> <st c="64892">search algorithm starts
    with a random entry node and moves toward the nearest neighbor to the query vector.</st>
    <st c="65000">For each node that gets closer, the distance from the user query
    node to the current node is recalculated, and the next node that gets the closest
    among the current node’s network connections is selected.</st> <st c="65205">This
    process traverses across nodes, skipping large parts of the data, making it</st>
    <st c="65286">significantly faster.</st>
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="64871">在查询过程中，</st> <st c="64892">搜索算法从随机入口节点开始，向查询向量的最近邻移动。</st> <st
    c="65000">对于每个越来越接近的节点，都会重新计算用户查询节点到当前节点的距离，并选择当前节点网络连接中距离最近的下一个节点。</st> <st c="65205">这个过程跨越节点，跳过了大量数据，使其</st>
    <st c="65286">显著更快。</st>
- en: <st c="65307">The hierarchical (H) part of HNSW adds several layers of navigable
    small worlds on top of each other.</st> <st c="65410">This can be imagined as
    traveling to a place by taking a plane to the nearest airport, then catching a
    train to a town, and finally searching within a much smaller set of</st> *<st
    c="65581">node</st>* <st c="65585">locations to find the</st> <st c="65608">desired
    location.</st>
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65307">HNSW中的分层（H）部分在每一层上叠加了几个可导航的小世界。</st> <st c="65410">这可以想象成通过乘坐飞机到达最近的机场，然后乘坐火车到达一个城镇，最后在更小的一组<st
    c="65581">节点</st> <st c="65585">位置中搜索以找到</st> <st c="65608">目标位置。</st>
- en: <st c="65625">Fun fact</st>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65625">有趣的事实</st>
- en: <st c="65634">HNSW is inspired by the observed phenomenon in human social networks
    where everyone is closely connected, such</st> <st c="65745">as the</st> **<st
    c="65753">six degrees of separation</st>** <st c="65778">concept.</st> *<st c="65788">Six
    degrees of separation</st>* <st c="65813">says that any two individuals are on
    average separated by six acquaintance links.</st> <st c="65896">This concept was
    originally inspired by a 1929 short story by Frigyes Karinthy that described a
    group of people playing a game of trying to connect any person in the world to
    themselves by a chain of five others.</st> <st c="66109">This chain of connections
    can be made to connect any two people in the world, in theory, by a maximum of
    six steps.</st> <st c="66225">It is also known</st> <st c="66242">as the</st>
    **<st c="66249">six</st>** **<st c="66253">handshakes rule</st>**<st c="66268">.</st>
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="65634">HNSW受到了人类社交网络中观察到的现象的启发，即每个人都紧密相连，例如</st> <st c="65745">六度分隔</st>
    <st c="65753">概念。</st> *<st c="65788">六度分隔</st> * <st c="65813">表示任何两个人平均通过六个熟人链接相隔。</st>
    <st c="65896">这个概念最初是由Frigyes Karinthy在1929年的一篇短篇小说中受到启发，描述了一群人玩一个游戏，试图通过五个人链将世界上任何一个人连接到他们自己。</st>
    <st c="66109">理论上，通过最多六步的连接，可以将世界上任何两个人连接起来。</st> <st c="66225">它也被称为</st> <st
    c="66249">六</st> **<st c="66253">握手规则</st>**<st c="66268">。</st>
- en: <st c="66269">All of these</st> <st c="66283">indexing techniques play a vital
    role in enhancing the efficiency and performance of ANN search algorithms.</st>
    <st c="66391">LSH, tree-based indexing, PQ, and HNSW are some of the prominent
    indexing techniques</st> <st c="66476">used in conjunction with search algorithms.</st>
    <st c="66520">By leveraging these indexing techniques, the search space can be
    reduced, irrelevant candidates can be pruned, and the overall search process can
    be accelerated.</st> <st c="66682">Indexing techniques provide a way to organize
    and structure the data, enabling efficient retrieval and similarity search in</st>
    <st c="66806">high-dimensional spaces.</st>
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些<st c="66269">索引技术</st>在提高人工神经网络搜索算法的效率和性能方面发挥着至关重要的作用。<st c="66283">局部敏感哈希（LSH）、基于树的索引、PQ和HNSW是一些与搜索算法结合使用的突出索引技术</st>
    <st c="66476">。</st>通过利用这些索引技术，可以减少搜索空间，剪枝无关候选者，并加速整体搜索过程。<st c="66520">索引技术提供了一种组织和结构化数据的方法，使得在</st>
    <st c="66806">高维空间中进行高效检索和相似性搜索成为可能。</st>
- en: <st c="66830">Now that we have added the indexing techniques to our repertoire,
    we still have another important aspect to talk about before we can start actually
    implementing these capabilities.</st> <st c="67012">ANN and k-NN are not services
    you can just sign up for; they are search algorithm approaches that services and
    software packages use.</st> <st c="67146">So, next, we need to learn what those
    packages are so that you can actually put them to use.</st> <st c="67239">Let’s
    talk about</st> <st c="67256">vector search!</st>
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="66830">现在我们已经将索引技术添加到我们的技能库中，在我们开始实际实施这些功能之前，我们还有另一个重要方面需要讨论。</st> <st
    c="67012">ANN和k-NN不是你可以随意注册的服务；它们是服务和软件包使用的搜索算法方法。</st> <st c="67146">因此，接下来，我们需要了解这些包是什么，这样你才能真正使用它们。</st>
    <st c="67239">让我们来谈谈</st> <st c="67256">向量搜索！</st>
- en: <st c="67270">Vector search options</st>
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="67270">向量搜索选项</st>
- en: <st c="67292">In basic terms, vector search</st> <st c="67323">is the process
    of finding the vectors most similar to the query vector within the vector store.</st>
    <st c="67419">The ability to quickly identify relevant vectors is crucial for
    the system’s overall performance, as it determines which pieces of information
    will be used by the LLM for generating responses.</st> <st c="67612">This component
    bridges the gap between the raw user query and the data-rich inputs needed for
    high-quality generation.</st> <st c="67731">There are numerous offerings and numerous
    types of offerings in the marketplace that you can use to conduct your vector
    search.</st> <st c="67859">We have talked a lot so far about the components and
    concepts that make a good vector search.</st> <st c="67953">You can apply that
    knowledge to selecting the best vector search option for your specific project
    needs.</st> <st c="68058">Services are evolving quickly with new start-ups emerging
    every day, so it is worth your effort to do some deep research before deciding
    on an option.</st> <st c="68209">In the next few subsections, we will look at
    the few options that can give you an idea of the breadth of what</st> <st c="68319">is
    available.</st>
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="67292">在基本术语中，向量搜索</st> <st c="67323">是找到向量存储中与查询向量最相似的向量的过程。</st> <st
    c="67419">快速识别相关向量的能力对于系统的整体性能至关重要，因为它决定了LLM将使用哪些信息来生成响应。</st> <st c="67612">这个组件在原始用户查询和高质量生成所需的大量数据输入之间架起了桥梁。</st>
    <st c="67731">市场上提供了众多服务和多种类型的服务，你可以使用它们来进行向量搜索。</st> <st c="67859">到目前为止，我们已经讨论了很多关于构成良好向量搜索的组件和概念。</st>
    <st c="67953">你可以将这方面的知识应用到选择最适合你特定项目需求的最佳向量搜索选项上。</st> <st c="68058">服务正在快速发展，每天都有新的初创公司出现，所以在决定选择之前做一些深入研究是值得的。</st>
    <st c="68209">在接下来的几个小节中，我们将探讨一些可以给你一个关于可用的广泛性的想法的选项。</st>
- en: <st c="68332">pgvector</st>
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="68332">pgvector</st>
- en: <st c="68341">pgvector is an</st> <st c="68357">open source vector similarity
    search</st> <st c="68394">extension for PostgreSQL, a popular relational database
    management system.</st> <st c="68469">It allows you to store and search vectors
    directly within PostgreSQL, leveraging its robust features and ecosystem.</st>
    <st c="68585">pgvector supports various distance metrics and indexing algorithms,
    including L2 distance and cosine distance.</st> <st c="68696">pgvector is one
    of the few services that supports both exact k-NN search and approximate k-NN
    search using ANN algorithms.</st> <st c="68819">Indexing options</st> <st c="68836">include</st>
    **<st c="68844">Inverted File Index</st>** <st c="68863">(</st>**<st c="68865">IVF</st>**<st
    c="68868">) and HNSW to accelerate the search process.</st> <st c="68914">pgvector
    can be used to perform a k-NN search by specifying the desired number of nearest
    neighbors and choosing between exact or approximate search.</st> <st c="69064">We’ve
    discussed HNSW thoroughly, but IVF is an indexing technique commonly used in combination
    with vector stores.</st> <st c="69179">IVF is designed to efficiently identify
    a subset of vectors that are likely to be similar to a given query vector, reducing
    the number of distance calculations required during the search process.</st> <st
    c="69375">IVF can be used in combination with HNSW, improving efficiencies and
    speed even further.</st> <st c="69464">pgvector integrates seamlessly with existing
    PostgreSQL tools and libraries, making it easy to incorporate vector search into
    applications that already use PostgreSQL.</st> <st c="69632">pgvector is a good
    fit if you are already using PostgreSQL and want to add vector search capabilities
    without introducing a separate system.</st> <st c="69773">It benefits from the
    reliability, scalability, and transaction support</st> <st c="69844">of PostgreSQL.</st>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="68341">pgvector是一个</st> <st c="68357">开源的向量相似度搜索</st> <st c="68394">扩展，用于PostgreSQL，这是一个流行的关系型数据库管理系统。</st>
    <st c="68469">它允许你在PostgreSQL中直接存储和搜索向量，利用其强大的特性和生态系统。</st> <st c="68585">pgvector支持各种距离度量指标和索引算法，包括L2距离和余弦距离。</st>
    <st c="68696">pgvector是少数几个支持使用ANN算法进行精确k-NN搜索和近似k-NN搜索的服务之一。</st> <st c="68819">索引选项</st>
    <st c="68836">包括</st> **<st c="68844">倒排文件索引</st>** <st c="68863">(</st>**<st
    c="68865">IVF</st>**<st c="68868">) 和HNSW以加速搜索过程。</st> <st c="68914">pgvector可以通过指定所需的最近邻数量并在精确或近似搜索之间进行选择来执行k-NN搜索。</st>
    <st c="69064">我们已经详细讨论了HNSW，但IVF是一种常与向量存储结合使用的索引技术。</st> <st c="69179">IVF旨在高效地识别一组可能与给定查询向量相似的向量子集，从而减少搜索过程中所需的距离计算数量。</st>
    <st c="69375">IVF可以与HNSW结合使用，进一步提高效率和速度。</st> <st c="69464">pgvector与现有的PostgreSQL工具和库无缝集成，使得将向量搜索集成到已使用PostgreSQL的应用程序中变得容易。</st>
    <st c="69632">如果你已经在使用PostgreSQL并且想要添加向量搜索功能而不引入一个单独的系统，pgvector是一个很好的选择。</st>
    <st c="69773">它受益于PostgreSQL的可靠性、可扩展性和事务支持。</st>
- en: <st c="69858">Elasticsearch</st>
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="69858">Elasticsearch</st>
- en: <st c="69872">Elasticsearch</st> <st c="69886">is a popular open source search
    and analytics</st> <st c="69932">engine that supports vector similarity search.</st>
    <st c="69980">It is widely adopted and has a large ecosystem of plugins and integrations.</st>
    <st c="70056">Elasticsearch uses ANN algorithms, particularly HNSW, for efficient
    vector similarity search.</st> <st c="70150">It does not explicitly use k-NN,
    but the similarity search functionality can be used to find the</st> *<st c="70247">k</st>*<st
    c="70248">-nearest neighbors.</st> <st c="70268">Elasticsearch offers advanced
    search capabilities, including full-text search, aggregations, and geospatial
    search, as well as a distributed architecture that allows for high scalability
    and fault tolerance.</st> <st c="70476">It integrates well with LangChain and
    offers robust scalability, distributed architecture, and a wide range of features.</st>
    <st c="70597">Elasticsearch is a good fit if you already have experience with
    it or need its advanced search and analytics capabilities.</st> <st c="70720">However,
    it may require more setup and configuration compared to some other</st> <st c="70796">vector
    stores.</st>
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="69872">Elasticsearch</st> <st c="69886">是一种流行的开源搜索和分析</st> <st c="69932">引擎，支持向量相似度搜索。</st>
    <st c="69980">它被广泛采用，并拥有庞大的插件和集成生态系统。</st> <st c="70056">Elasticsearch使用ANN算法，特别是HNSW，进行高效的向量相似度搜索。</st>
    <st c="70150">它不明确使用k-NN，但相似度搜索功能可以用来找到</st> *<st c="70247">k</st>*<st c="70248">-最近邻。</st>
    <st c="70268">Elasticsearch提供高级搜索功能，包括全文搜索、聚合和地理空间搜索，以及允许高可扩展性和容错性的分布式架构。</st>
    <st c="70476">它与LangChain集成良好，提供强大的可扩展性、分布式架构和广泛的功能。</st> <st c="70597">如果你已经熟悉Elasticsearch或需要其高级搜索和分析功能，它是一个不错的选择。</st>
    <st c="70720">然而，与一些其他</st> <st c="70796">向量存储相比，它可能需要更多的设置和配置。</st>
- en: <st c="70810">FAISS</st>
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="70810">FAISS</st>
- en: '**<st c="70816">FAISS</st>** <st c="70822">is a</st> <st c="70827">library
    for efficient similarity search and clustering of</st> <st c="70886">dense vectors,
    developed by Facebook.</st> <st c="70924">It is known for its exceptional performance
    and ability to handle billion-scale vector datasets.</st> <st c="71021">FAISS
    heavily relies on ANN algorithms for efficient similarity search offering a wide
    range of ANN indexing and search algorithms, including IVF, PQ, and HNSW.</st>
    <st c="71182">FAISS can be used to perform a k-NN search by retrieving the</st>
    *<st c="71243">k</st>* <st c="71244">most similar vectors to a query vector.</st>
    <st c="71285">FAISS offers a wide range of indexing and search algorithms, including
    quantization-based methods for compact vector representation, and provides GPU
    support for accelerated similarity search.</st> <st c="71478">It can be used as
    a vector store and integrates with LangChain.</st> <st c="71542">FAISS is a good
    choice if you have high-performance requirements and are comfortable working with
    a lower-level library.</st> <st c="71663">However, it may require more manual
    setup and management compared to</st> <st c="71732">managed services.</st>'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="70816">FAISS</st>** <st c="70822">是一个由Facebook开发的</st> <st c="70827">用于高效相似度搜索和密集向量聚类的库。</st>
    <st c="70924">它以其卓越的性能和能够处理数十亿规模向量数据集的能力而闻名。</st> <st c="71021">FAISS高度依赖于ANN算法进行高效的相似度搜索，提供包括IVF、PQ和HNSW在内的广泛ANN索引和搜索算法。</st>
    <st c="71182">FAISS可以通过检索查询向量的</st> *<st c="71243">k</st> <st c="71244">个最相似向量来执行k-NN搜索。</st>
    <st c="71285">FAISS提供广泛的索引和搜索算法，包括基于量化的紧凑向量表示方法，并提供GPU支持以加速相似度搜索。</st> <st c="71478">它可以作为向量存储使用，并与LangChain集成。</st>
    <st c="71542">如果你对高性能有要求并且习惯于使用底层库，FAISS是一个不错的选择。</st> <st c="71663">然而，与托管服务相比，它可能需要更多的手动设置和管理。</st>'
- en: <st c="71749">Google Vertex AI Vector Search</st>
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="71749">Google Vertex AI Vector Search</st>
- en: <st c="71780">Google Vertex AI Vector Search</st> <st c="71811">is a fully</st>
    <st c="71822">managed vector similarity search service offered by GCP that includes
    both vector storage and search capabilities.</st> <st c="71938">Google Vertex
    AI Vector Search</st> <st c="71969">uses ANN algorithms under the hood to enable
    fast and scalable vector similarity search.</st> <st c="72058">The specific ANN
    algorithms used are not disclosed, but it likely employs state-of-the-art techniques
    based on Google ScaNN (Scalable, Channel-Aware Nearest Neighbors).</st> <st c="72227">It
    can be used to perform a k-NN search by specifying the desired number of nearest
    neighbors.</st> <st c="72322">When you use Vertex AI Vector Search, the vectors
    are stored within the managed service itself.</st> <st c="72418">It</st> <st c="72421">integrates
    seamlessly with other Google Cloud services, such as BigQuery and Dataflow, enabling
    efficient data processing pipelines.</st> <st c="72554">Vertex AI Vector Search
    offers features such as online updates, allowing you to incrementally add or delete
    vectors without rebuilding the entire index.</st> <st c="72707">It integrates
    with LangChain and provides scalability, high availability, and easy integration
    with other Google Cloud services.</st> <st c="72836">If you are already using
    Google Cloud and want a managed solution with minimal setup, Vertex AI Vector
    Search is a good option.</st> <st c="72964">However, it may be more expensive
    compared to</st> <st c="73010">self-hosted solutions.</st>
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="71780">Google Vertex AI 向量搜索</st> <st c="71811">是一项由 GCP 提供的完全</st> <st
    c="71822">管理的向量相似度搜索服务，它包括向量存储和搜索功能。</st> <st c="71938">Google Vertex AI 向量搜索</st>
    <st c="71969">在底层使用 ANN 算法来实现快速和可扩展的向量相似度搜索。</st> <st c="72058">使用的具体 ANN 算法未公开，但它可能采用了基于
    Google ScaNN（可扩展、通道感知最近邻）的最先进技术。</st> <st c="72227">可以通过指定所需的最近邻数量来执行 k-NN 搜索。</st>
    <st c="72322">当您使用 Vertex AI 向量搜索时，向量存储在托管服务内部。</st> <st c="72418">它</st> <st
    c="72421">与其他 Google Cloud 服务无缝集成，例如 BigQuery 和 Dataflow，从而实现高效的数据处理管道。</st> <st
    c="72554">Vertex AI 向量搜索提供在线更新等功能，允许您在不重建整个索引的情况下增量添加或删除向量。</st> <st c="72707">它与
    LangChain 集成，提供可扩展性、高可用性，以及与其他 Google Cloud 服务轻松集成的功能。</st> <st c="72836">如果您已经在使用
    Google Cloud，并希望有一个设置最少的托管解决方案，Vertex AI 向量搜索是一个不错的选择。</st> <st c="72964">然而，与自托管解决方案相比，它可能更昂贵。</st>
- en: <st c="73032">Azure AI Search</st>
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="73032">Azure AI 搜索</st>
- en: <st c="73048">Azure AI Search is a</st> <st c="73069">fully managed search service
    provided</st> <st c="73108">by Microsoft Azure.</st> <st c="73128">It supports
    vector similarity search alongside traditional keyword-based search.</st> <st
    c="73209">Azure AI Search utilizes ANN algorithms for efficient vector similarity
    search.</st> <st c="73289">The exact ANN algorithms used are not specified, but
    it leverages advanced indexing techniques to enable fast retrieval of similar
    vectors.</st> <st c="73429">It can be used to perform a k-NN search by querying
    for the</st> *<st c="73489">k</st>* <st c="73490">nearest neighbors.</st> <st
    c="73510">Azure AI Search offers features such as synonyms, autocomplete, and
    faceted navigation.</st> <st c="73598">It integrates with Azure Machine Learning
    for seamless deployment of machine learning models.</st> <st c="73692">Azure AI
    Search is a good choice if you are already using Azure services and want a managed
    solution with advanced search capabilities.</st> <st c="73828">However, it may
    have a steeper learning curve compared to some</st> <st c="73891">other options.</st>
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="73048">Azure AI 搜索是</st> <st c="73069">由 Microsoft Azure 提供的完全</st> <st
    c="73108">管理的搜索服务。</st> <st c="73128">它支持向量相似度搜索以及传统的基于关键词的搜索。</st> <st c="73209">Azure
    AI 搜索利用 ANN 算法进行高效的向量相似度搜索。</st> <st c="73289">使用的具体 ANN 算法未指定，但它利用先进的索引技术来实现快速检索相似向量。</st>
    <st c="73429">可以通过查询最近的</st> *<st c="73489">k</st> <st c="73490">个邻居</st> 来执行
    k-NN 搜索。</st> <st c="73510">Azure AI 搜索提供同义词、自动完成和分面导航等功能。</st> <st c="73598">它与
    Azure Machine Learning 集成，以实现机器学习模型的无缝部署。</st> <st c="73692">如果您已经在使用 Azure 服务，并希望有一个具有高级搜索功能的托管解决方案，Azure
    AI 搜索是一个不错的选择。</st> <st c="73828">然而，与一些其他选项相比，它可能有一个更陡峭的学习曲线。</st>
- en: <st c="73905">Approximate Nearest Neighbors Oh Yeah</st>
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="73905">近似最近邻，是的</st>
- en: '**<st c="73943">Approximate Nearest Neighbors Oh Yeah</st>** <st c="73981">(</st>**<st
    c="73983">ANNOY</st>**<st c="73988">) is an open source library developed</st>
    <st c="74027">by Spotify</st> <st c="74038">for ANN search.</st> <st c="74054">It
    is known for its fast indexing and querying speed, as well as its ability to handle
    high-dimensional vectors efficiently.</st> <st c="74179">It uses a combination
    of random projections and binary space partitioning to build a forest of trees
    that enables a fast similarity search.</st> <st c="74319">ANNOY can be used to
    perform a k-NN search by retrieving the</st> *<st c="74380">k</st>* <st c="74381">approximate
    nearest neighbors.</st> <st c="74413">ANNOY</st> <st c="74419">uses a combination
    of random projections and binary space partitioning to build a forest of trees
    that enables a fast similarity search.</st> <st c="74556">It has a simple and
    intuitive API, making it easy to integrate into existing projects.</st> <st c="74643">ANNOY
    is a good fit if you prioritize speed and have a smaller dataset.</st> <st c="74715">However,
    it may not scale as well as some other options for extremely</st> <st c="74785">large
    datasets.</st>'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="73943">近似最近邻，是的</st>** <st c="73981">(**<st c="73983">ANNOY</st>**<st
    c="73988">) 是由 Spotify 开发的开源库，用于 ANN 搜索。<st c="74027">它以其快速的索引和查询速度以及高效处理高维向量的能力而闻名。</st>
    <st c="74054">它通过结合随机投影和二进制空间划分来构建一棵树森林，从而实现快速相似性搜索。</st> <st c="74319">ANNOY
    可以通过检索<st c="74380">k</st> <st c="74381">近似最近邻来执行 k-NN 搜索。</st> <st c="74413">ANNOY
    使用随机投影和二进制空间划分的组合来构建一棵树森林，从而实现快速相似性搜索。</st> <st c="74556">它具有简单直观的API，使其易于集成到现有项目中。</st>
    <st c="74643">如果你优先考虑速度并且数据集较小，ANNOY 是一个很好的选择。</st> <st c="74715">然而，对于极大数据集，它可能不如其他一些选项扩展得那么好。</st>'
- en: <st c="74800">Pinecone</st>
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="74800">Pinecone</st>
- en: <st c="74809">Pinecone is a</st> <st c="74823">fully managed vector database
    designed</st> <st c="74862">specifically for machine learning applications.</st>
    <st c="74911">It offers high-performance vector similarity search and supports
    both dense and sparse vector representations.</st> <st c="75022">Pinecone employs
    ANN algorithms to achieve its high-performance vector similarity search.</st>
    <st c="75112">It supports various ANN indexing algorithms, including HNSW, to
    enable efficient retrieval of similar vectors.</st> <st c="75223">Pinecone can
    be used to perform a k-NN search by querying for the</st> *<st c="75289">k</st>*<st
    c="75290">-nearest neighbors.</st> <st c="75310">Pinecone provides features such
    as real-time updates, horizontal scaling, and multi-region replication for high
    availability.</st> <st c="75436">It has a simple API and integrates seamlessly
    with various machine learning frameworks and libraries.</st> <st c="75538">Pinecone
    is a good choice if you want a dedicated vector database with a focus on machine
    learning use cases.</st> <st c="75648">However, it may be more expensive compared
    to some open source or</st> <st c="75714">self-hosted solutions.</st>
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="74809">松果数据库</st>是一个<st c="74823">专门为机器学习应用设计的</st> <st c="74862">完全托管向量数据库</st>。它提供高性能向量相似性搜索，并支持密集和稀疏向量表示。<st
    c="74911">松果数据库采用ANN算法来实现其高性能向量相似性搜索。</st> <st c="75022">它支持各种ANN索引算法，包括HNSW，以实现相似向量的有效检索。</st>
    <st c="75223">松果数据库可以通过查询<st c="75289">k</st><st c="75290">-最近邻来执行k-NN搜索。</st>
    <st c="75310">松果数据库提供实时更新、水平扩展和多区域复制等特性，以确保高可用性。</st> <st c="75436">它具有简单的API，并且可以无缝集成到各种机器学习框架和库中。</st>
    <st c="75538">如果你需要一个专注于机器学习用例的专用向量数据库，松果数据库是一个不错的选择。</st> <st c="75648">然而，与一些开源或自托管解决方案相比，它可能更昂贵。</st>
- en: <st c="75736">Weaviate</st>
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="75736">Weaviate</st>
- en: <st c="75745">Weaviate is an</st> <st c="75760">open source vector search engine
    that enables</st> <st c="75807">efficient similarity search and data exploration.</st>
    <st c="75857">It supports multiple vector indexing algorithms, including HNSW,
    and offers a GraphQL-based API for easy integration.</st> <st c="75975">Weaviate
    utilizes ANN algorithms, particularly HNSW, for efficient vector similarity search.</st>
    <st c="76068">It leverages the NSW structure of HNSW to enable fast retrieval
    of similar vectors.</st> <st c="76152">Weaviate can be used to perform a k-NN
    search by specifying the desired number of nearest neighbors.</st> <st c="76253">Weaviate
    provides features such as schema management, data validation, and real-time updates.</st>
    <st c="76347">It can be self-hosted or used as a managed service.</st> <st c="76399">Weaviate
    is a good fit if you prefer an open source solution with a focus on data exploration
    and graph-like queries.</st> <st c="76517">However, it may require more setup
    and configuration compared to fully</st> <st c="76588">managed services.</st>
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="75745">Weaviate是一个</st> <st c="75760">开源向量搜索引擎，它能够实现</st> <st c="75807">高效的相似性搜索和数据探索。</st>
    <st c="75857">它支持多种向量索引算法，包括HNSW，并提供基于GraphQL的API以实现易于集成。</st> <st c="75975">Weaviate利用ANN算法，特别是HNSW，进行高效的向量相似性搜索。</st>
    <st c="76068">它利用HNSW的NSW结构来实现相似向量的快速检索。</st> <st c="76152">Weaviate可以通过指定所需的最近邻数量来执行k-NN搜索。</st>
    <st c="76253">Weaviate提供诸如模式管理、数据验证和实时更新等功能。</st> <st c="76347">它可以自行托管或用作托管服务。</st>
    <st c="76399">如果你更喜欢一个专注于数据探索和图状查询的开源解决方案，Weaviate是一个不错的选择。</st> <st c="76517">然而，与完全</st>
    <st c="76588">托管服务相比，它可能需要更多的设置和配置。</st>
- en: <st c="76605">Chroma</st>
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="76605">Chroma</st>
- en: <st c="76612">This is what</st> <st c="76625">you have been using so far for
    vector search in</st> <st c="76674">most of the code in this book.</st> <st c="76705">Chroma
    is an open source embedded vector database designed for easy integration with
    existing tools and frameworks.</st> <st c="76821">Chroma supports ANN algorithms,
    including HNSW, for fast and efficient vector similarity search.</st> <st c="76918">It
    can be used to perform a k-NN search by retrieving the</st> *<st c="76976">k</st>*<st
    c="76977">-nearest neighbors to a query vector.</st> <st c="77015">Chroma provides
    a simple and intuitive Python API for storing and searching vectors, making it
    particularly convenient for machine learning and data science workflows.</st>
    <st c="77183">That is the primary reason we selected it to showcase in the code
    in this book.</st> <st c="77263">Chroma supports various indexing algorithms,
    including HNSW, and offers features such as dynamic filtering and metadata storage.</st>
    <st c="77392">It can be used as an in-memory database or persist data to disk
    for longer-term storage.</st> <st c="77481">Chroma is a good choice if you want
    a lightweight and easy-to-use vector database that can be embedded directly into
    your Python applications.</st> <st c="77624">However, it may not have the same
    level of scalability and advanced features as some of the more mature vector</st>
    <st c="77735">search solutions.</st>
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="76612">这就是</st> <st c="76625">你迄今为止在</st> <st c="76674">本书的大部分代码中用于向量搜索的内容。</st>
    <st c="76705">Chroma是一个开源的嵌入式向量数据库，旨在与现有工具和框架轻松集成。</st> <st c="76821">Chroma支持ANN算法，包括HNSW，以实现快速高效的向量相似性搜索。</st>
    <st c="76918">它可以通过检索查询向量的*<st c="76976">k</st>*<st c="76977">-最近邻来执行k-NN搜索。</st>
    <st c="77015">Chroma提供了一个简单直观的Python API，用于存储和搜索向量，这使得它在机器学习和数据科学工作流程中特别方便。</st>
    <st c="77183">这就是我们选择在本书的代码中展示它的主要原因。</st> <st c="77263">Chroma支持包括HNSW在内的各种索引算法，并提供如动态过滤和元数据存储等功能。</st>
    <st c="77392">它可以作为内存数据库使用，或将数据持久化到磁盘进行长期存储。</st> <st c="77481">如果你想要一个轻量级且易于使用的向量数据库，可以直接嵌入到你的Python应用程序中，Chroma是一个不错的选择。</st>
    <st c="77624">然而，它可能不具备一些更成熟向量搜索解决方案相同的可扩展性和高级功能。</st>
- en: <st c="77752">Summary</st>
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="77752">摘要</st>
- en: <st c="77760">In this chapter, we covered a wide range of topics related to
    similarity searching with vectors, a crucial component of RAG systems.</st> <st
    c="77894">We explored the concept of a vector space, discussed the differences
    between semantic and keyword searches, and covered various distance metrics used
    to compare the similarity between embeddings, providing code examples to demonstrate</st>
    <st c="78129">their calculation.</st>
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="77760">在本章中，我们涵盖了与向量相似性搜索相关的广泛主题，这是RAG系统的一个关键组件。</st> <st c="77894">我们探讨了向量空间的概念，讨论了语义搜索和关键词搜索之间的区别，并介绍了用于比较嵌入相似性的各种距离度量，提供了代码示例来演示</st>
    <st c="78129">它们的计算。</st>
- en: <st c="78147">We reviewed code that implemented hybrid search using the BM25
    algorithm for sparse search and a dense retriever for semantic search, showcasing
    how to combine and rerank the results.</st> <st c="78332">We also discussed semantic
    search algorithms, focusing on k-NN and ANN, and covered indexing techniques that
    enhance the efficiency of ANN search, such as LSH, tree-based indexing, PQ,</st>
    <st c="78517">and HNSW.</st>
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了使用BM25算法进行稀疏搜索和密集检索器进行语义搜索的混合搜索代码实现，展示了如何组合和重新排序结果。</st> <st c="78332">我们还讨论了语义搜索算法，重点关注k-NN和ANN，并介绍了增强ANN搜索效率的索引技术，如LSH、基于树的索引、PQ</st>
    <st c="78517">和HNSW。</st>
- en: <st c="78526">Finally, we provided an overview of several vector search options
    available in the market, discussing their key features, strengths, and considerations
    to help you make an informed decision when selecting a vector search solution
    for your specific</st> <st c="78775">project needs.</st>
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="78526">最后，我们概述了市场上可用的几种向量搜索选项，讨论了它们的关键特性、优势和考虑因素，以帮助您在选择针对特定</st> <st
    c="78775">项目需求的向量搜索解决方案时做出明智的决定。</st>
- en: <st c="78789">In the next chapter, we will take a deep look at ways to visualize
    and evaluate your</st> <st c="78875">RAG pipeline.</st>
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="78789">在下一章中，我们将深入探讨可视化和分析您的</st> <st c="78875">RAG管道的方法。</st>
