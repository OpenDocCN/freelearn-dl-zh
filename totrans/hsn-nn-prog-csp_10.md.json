["```py\nvar net = new Net<double>();\n```", "```py\nnet.AddLayer(new InputLayer(1, 1, 2));\n```", "```py\nnet.AddLayer(new FullyConnLayer(20));\n```", "```py\nnet.AddLayer(new ReluLayer());\n```", "```py\nnet.AddLayer(new FullyConnLayer(10));\n```", "```py\nnet.AddLayer(new SoftmaxLayer(10));\nvar x = BuilderInstance.Volume.From(new[] { 0.3, -0.5 }, new Shape(2));\n```", "```py\nvar prob = net.Forward(x);\n```", "```py\nConsole.WriteLine(\"probability that x is class 0: \" + prob.Get(0));\n```", "```py\nvar trainer = new SgdTrainer(net)\n{\nLearningRate = 0.01, L2Decay = 0.001\n};\ntrainer.Train(x,BuilderInstance.Volume.From(new[]{ 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 }, new Shape(1, 1, 10, 1)));\nvar prob2 = net.Forward(x);\nConsole.WriteLine(\"probability that x is class 0: \" + prob2.Get(0));\n```", "```py\nvar cns = new ConvNetSharp<float>();\n```", "```py\nOp<float> cost;\nOp<float> fun;\nif (File.Exists(\"test.graphml\"))\n{\nConsole.WriteLine(\"Loading graph from disk.\");\nvar ops = SerializationExtensions.Load<float>(\"test\", true);\nfun = ops[0];\ncost = ops[1];\n}\nelse\n{\nvar x = cns.PlaceHolder(\"x\");\nvar y = cns.PlaceHolder(\"y\");\nvar W = cns.Variable(1.0f, \"W\", true);\nvar b = cns.Variable(2.0f, \"b\", true);\nfun = x * W + b;\ncost = (fun - y) * (fun - y);\n}\nvar optimizer = new AdamOptimizer<float>(cns, 0.01f, 0.9f, 0.999f, 1e-08f);\nusing (var session = new Session<float>())\n{\n```", "```py\nsession.Differentiate(cost);\nfloat currentCost;\ndo\n{\n var dico = new Dictionary<string, Volume<float>> { { \"x\", -2.0f }, { \"y\", 1.0f } };\ncurrentCost = session.Run(cost, dico);\nConsole.WriteLine($\"cost: {currentCost}\");\nvar result = session.Run(fun, dico);\nsession.Run(optimizer, dico);\n}\nwhile (currentCost > 1e-5);\nfloat finalW = session.GetVariableByName(fun, \"W\").Result;\nfloat finalb = session.GetVariableByName(fun, \"b\").Result;\nConsole.WriteLine($\"fun = x * {finalW} + {finalb}\");\nfun.Save(\"test\", cost);\n```", "```py\nvar vm = new ViewModel<float>(cost);\nvar app = new Application();\napp.Run(new GraphControl { DataContext = vm });\n}\n```", "```py\nvar cns = new ConvNetSharp<float>();\n```", "```py\nvar x = cns.PlaceHolder(\"x\");\nvar fun = 2.0f * x;\nusing (var session = new Session<float>())\n{\n```", "```py\nsession.Differentiate(fun);\n```", "```py\nvar vm = new ViewModel<float>(x.Derivate);\nvar app = new Application();\napp.Run(new GraphControl { DataContext = vm });\n}\n```", "```py\nvarnet=FluentNet<double>.Create(24, 24, 1)\n.Conv(5, 5, 8).Stride(1).Pad(2)\n.Relu()\n.Pool(2, 2).Stride(2)\n.Conv(5, 5, 16).Stride(1).Pad(2)\n.Relu()\n.Pool(3, 3).Stride(3)\n.FullyConn(10)\n.Softmax(10)\n.Build();\n```", "```py\nprivate void MnistDemo()\n{\n```", "```py\nvar datasets = new DataSets();\n```", "```py\nif (!datasets.Load(100))\n{\nreturn;\n}\n```", "```py\nthis._net = FluentNet<double>.Create(24, 24, 1)\n.Conv(5, 5, 8).Stride(1).Pad(2)\n.Relu()\n.Pool(2, 2).Stride(2)\n.Conv(5, 5, 16).Stride(1).Pad(2)\n.Relu()\n.Pool(3, 3).Stride(3)\n.FullyConn(10)\n.Softmax(10)\n.Build();\n```", "```py\nthis._trainer = new SgdTrainer<double>(this._net)\n{\nLearningRate = 0.01,\nBatchSize = 20,\nL2Decay = 0.001,\nMomentum = 0.9\n};\ndo\n{\n```", "```py\nvar trainSample = datasets.Train.NextBatch(this._trainer.BatchSize);\n```", "```py\nTrain(trainSample.Item1, trainSample.Item2, trainSample.Item3);\n```", "```py\nvar testSample = datasets.Test.NextBatch(this._trainer.BatchSize);\n```", "```py\nTest(testSample.Item1, testSample.Item3, this._testAccWindow);\n```", "```py\nConsole.WriteLine(\"Loss: {0} Train accuracy: {1}% Test accuracy: {2}%\", this._trainer.Loss, Math.Round(this._trainAccWindow.Items.Average() * 100.0, 2),\nMath.Round(this._testAccWindow.Items.Average() * 100.0, 2));\n} while (!Console.KeyAvailable);\n\n```", "```py\npublic virtual void Train(Volume<T> x, Volume<T> y)\n{\nForward(x);\nBackward(y);\n}\n```", "```py\nprivate void Test(Volume x, int[] labels, CircularBuffer<double> accuracy, bool forward = true)\n{\nif (forward)\n{\n```", "```py\nthis._net.Forward(x);\n}\nvar prediction = this._net.GetPrediction();\nfor (var i = 0; i < labels.Length; i++)\n{\n```", "```py\naccuracy.Add(labels[i] == prediction[i] ? 1.0 : 0.0);\n}\n}\n```", "```py\npublic int[] GetPrediction()\n{\nvar softmaxLayer = this._lastLayer as SoftmaxLayer<T>;\nif (softmaxLayer == null)\n{\nthrow new Exception(\"Function assumes softmax as last layer of the net!\");\n}\nvar activation = softmaxLayer.OutputActivation;\nvar N = activation.Shape.Dimensions[3];\nvar C = activation.Shape.Dimensions[2];\nvar result = new int[N];\nfor (varn = 0; n < N; n++)\n{\nvarmaxv = activation.Get(0, 0, 0, n);\nvarmaxi = 0;\nfor (vari = 1; i < C; i++)\n{\nvar output = activation.Get(0, 0, i, n);\nif (Ops<T>.GreaterThan(output, maxv))\n{\nmaxv = output;\nmaxi = i;\n}\n}\nresult[n] = maxi;\n}\nreturn result;\n}\n```"]