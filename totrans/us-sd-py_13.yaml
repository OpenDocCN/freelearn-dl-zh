- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generating Images with ControlNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stable Diffusion’s ControlNet is a neural network plugin that allows you to
    control diffusion models by adding extra conditions. It was first introduced in
    a paper called Adding Conditional Control to Text-to-Image Diffusion Models [1]
    by Lvmin Zhang and Maneesh Agrawala, published in 2023.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is ControlNet and how is it different?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usage of ControlNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple ControlNets in one pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How ControlNet works
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More ControlNet usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand how ControlNet works and how
    to use Stable Diffusion V1.5 and Stable Diffusion XL ControlNet models.
  prefs: []
  type: TYPE_NORMAL
- en: What is ControlNet and how is it different?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In terms of “control,” you may recall textual embedding, LoRA, and the image-to-image
    diffusion pipeline. But what makes ControlNet different and useful?
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike other solutions, ControlNet is a model that works on the UNet diffusion
    process directly. We compare these solutions in *Table 13.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Control Method** | **Functioning Stage** | **Usage Scenario** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Textual Embedding | Text encoder | Add a new style, a new concept, or a new
    face |'
  prefs: []
  type: TYPE_TB
- en: '| LoRA | Merge LoRA weights to the UNet model (and the CLIP text encoder, optional)
    | Add a set of styles, concepts, and generate content |'
  prefs: []
  type: TYPE_TB
- en: '| Image-to-Image | Provide the initial latent image | Fix images, or add styles
    and concepts to images |'
  prefs: []
  type: TYPE_TB
- en: '| ControlNet | ControlNet participant denoising together with a checkpoint
    model UNet | Control shape, pose, content detail |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13.1: A comparison of textual embedding, LoRA, image-to-image, and ControlNet'
  prefs: []
  type: TYPE_NORMAL
- en: In many ways, ControlNet is similar to the image-to-image pipeline, as we discussed
    in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214). Both image-to-image and ControlNet
    can be used to enhance images.
  prefs: []
  type: TYPE_NORMAL
- en: However, ControlNet can “control” the image in a more precise way. Imagine you
    want to generate an image that uses a specific pose from another image or perfectly
    align objects within the scene to a specific reference point. This kind of precision
    is impossible with the out-of-the-box Stable Diffusion model. ControlNet is the
    tool that can help you achieve these goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides, ControlNet models work with all other open source checkpoint models,
    unlike some other solutions, which work only with one base model provided by their
    author. The team that created ControlNet not only open-sourced the model but also
    open-sourced the code to train a new model. In other words, we can train a ControlNet
    model and make it work with any other model. This is what the original paper says
    [1]:'
  prefs: []
  type: TYPE_NORMAL
- en: Since Stable Diffusion is a typical UNet structure, this ControlNet architecture
    is likely to be applicable with other models.
  prefs: []
  type: TYPE_NORMAL
- en: Note that ControlNet models will only work with models using the same base model.
    A **Stable Diffusion** (**SD**) v1.5 ControlNet model works with all other SD
    v1.5 models. For **Stable Diffusion XL** (**SDXL**) models, we will need a ControlNet
    model that is trained with SDXL. This is because SDXL models use a different architecture,
    a larger UNet than the SD v1.5\. Without additional work, a ControlNet model is
    trained with one architecture and only works with this type of model.
  prefs: []
  type: TYPE_NORMAL
- en: 'I used “*without additional work*” because in December 2023, to bridge this
    gap, a paper from Lingmin Ran et al, called *X-Adapter: Adding Universal Compatibility
    of Plugins for Upgraded Diffusion Model* was published [8]. This paper details
    an adapter that enables us to use SD V1.5 LoRA and ControlNet in a new SDXL model.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s start using ControlNet with SD models.
  prefs: []
  type: TYPE_NORMAL
- en: Usage of ControlNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the backend of ControlNet, in this section, we will start
    using ControlNet to help control image generation.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will first generate an image using SD, take the
    Canny shape of the object, and then use the Canny shape to generate a new image
    with the help of ControlNet.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A Canny image refers to an image that has undergone Canny edge detection, which
    is a popular edge detection algorithm. It was developed by John F. Canny in 1986\.
    [7]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use SD to generate an image using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a sample image using SD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will see an image of a cat, as shown in *Figure 13**.1*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1: A cat, generated by SD](img/B21263_13_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: A cat, generated by SD'
  prefs: []
  type: TYPE_NORMAL
- en: Then we will get the Canny shape of the sample image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will need another package, `controlnet_aux`, to create a Canny image from
    an image. Simply execute the following two lines of `pip` commands to install
    `controlnet_aux`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can generate the image Canny edge shape with three lines of code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s a breakdown of the code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`from controlnet_aux import CannyDetector`: This line imports the `CannyDetector`
    class from the `controlnet_aux` module. There are many other detectors.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_canny = canny(image, 30, 100)`: This line calls the `__call__` method
    of the `CannyDetector` class (which is implemented as a callable object) with
    the following arguments:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image`: This is the input image to which the Canny edge detection algorithm
    will be applied.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`30`: This is the lower threshold value for the edges. Any edges with an intensity
    gradient below this value will be discarded.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`100`: This is the upper threshold value for the edges. Any edges with an intensity
    gradient above this value will be considered strong edges.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding code will generate the Canny image shown in *Figure 13**.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.2: Canny image of a cat](img/B21263_13_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Canny image of a cat'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use the ControlNet model to generate a new image based on this
    Canny image. First, let’s load up the ControlNet model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code will download the ControlNet model from Hugging Face automatically
    at your first run. If you have the ControlNet `safetensors` model in your storage
    and want to use your own model, you can convert the file to the diffuser format
    first. You can find the conversion code in [*Chapter 6*](B21263_06.xhtml#_idTextAnchor117).
    Then, replace `takuma104/control_v11` with the path to the ControlNet model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Initialize a ControlNet pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that you can freely swap `stablediffusionapi/deliberate-v2` with any other
    SD v1.5 models from the community.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the new image using the ControlNet pipeline. In the following example,
    we will replace the cat with a dog:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These lines of code will generate a new image following the Canny edge, but
    the little cat is now a dog, as shown in *Figure 13**.3*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.3: A dog, generated using the cat Canny image with ControlNet](img/B21263_13_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: A dog, generated using the cat Canny image with ControlNet'
  prefs: []
  type: TYPE_NORMAL
- en: The cat’s body structure and shape are preserved. Feel free to change the prompt
    and settings to explore the amazing capabilities of the model. One thing to note
    is that if you don’t provide a prompt to the ControlNet pipeline, the pipeline
    will still output a meaningful image, maybe another style of cat, which means
    the ControlNet model learned the underlying meaning of a certain Canny edge.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we used only one ControlNet model, but we can also provide
    multiple ControlNet models to one pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple ControlNets in one pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will initialize one more ControlNet, NormalBAE, and then
    feed the Canny and NormalBAE ControlNet models together to form a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate a Normal BAE as one additional control image. Normal BAE is
    a model that’s used to estimate a normal map using the normal uncertainty method
    [4] proposed by Bae et al:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will generate the original image’s Normal BAE map, as shown in *Figure
    13**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4: Normal BAE image of the generated cat](img/B21263_13_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Normal BAE image of the generated cat'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s initialize two ControlNet models for one pipeline: one Canny ControlNet
    model, and another NormalBae ControlNet model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'From the code, we can easily see that all ControlNet models share the same
    architecture. To load different ControlNet models, we only need to change the
    model name. Also, note that the two ControlNet models are in a Python `controlnets`
    `list`. We can provide these ControlNet models to the pipeline directly, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'In the inference stage, use one additional parameter, `controlnet_conditioning_scale`,
    to control the influence scale of each ControlNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will give us another image, as shown in *Figure 13**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet](img/B21263_13_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet'
  prefs: []
  type: TYPE_NORMAL
- en: In `controlnet_conditioning_scale = [0.5,0.5]`, I give each ControlNet model
    a `0.5` scale value. The two scale values add up to `1.0`. We should give weights
    that add up to no more than `2`. Values that are too will lead to undesired images.
    For instance, if you give weights of `1.2` and `1.3` to each ControlNet model,
    like `controlnet_conditioning_scale = [1.2,1.3]`, you may get an undesired image.
  prefs: []
  type: TYPE_NORMAL
- en: If we have successfully generated images using the ControlNet models, we have
    together witnessed the power of ControlNet. In the next section, we will discuss
    how ControlNet works.
  prefs: []
  type: TYPE_NORMAL
- en: How ControlNet works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will drill down into the ControlNet structure and see how
    ControlNet works internally.
  prefs: []
  type: TYPE_NORMAL
- en: 'ControlNet works by injecting additional conditions into the blocks of a neural
    network. As shown in *Figure 13**.6*, the trainable copy is the ControlNet block
    that adds additional guidance to the original SD UNet block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.6: Adding ControlNet components](img/B21263_13_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Adding ControlNet components'
  prefs: []
  type: TYPE_NORMAL
- en: During the training stage, we take a copy of the target layer block as the ControlNet
    block. In *Figure 13**.6*, it is denoted as a **trainable copy**. Unlike typical
    neural network initialization with Gaussian distributions for all parameters,
    ControlNet utilizes pre-trained weights from the Stable Diffusion base model.
    Most of these base model parameters are frozen (with the option to unfreeze them
    later) and only the additional ControlNet components are trained from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: During training and inference, the input x is usually a 3D dimensional vector,
    x ∈ ℝ h×w×c, with h, w, c as the height, width, and number of channels. c is a
    conditioning vector that we will pass into the SD UNet and also to the ControlNet
    model network.
  prefs: []
  type: TYPE_NORMAL
- en: The **zero convolution** plays a pivotal role in this process. **Zero convolutions**
    are 1D convolutions with weights and biases initialized to zero. The advantage
    of zero convolution is that, even without a single training step, the value injected
    from ControlNet will have no effect on image generation. This ensures that the
    side network does not negatively impact image generation at any stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be thinking: if the weight of a convolution layer is zero, wouldn''t
    the gradient also be zero, rendering the network unable to learn? However, as
    the paper''s authors explain [5], the reality is more nuanced.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider one simple case:'
  prefs: []
  type: TYPE_NORMAL
- en: y = wx + b
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: ∂ y / ∂ w = x, ∂ y / ∂ x = w, ∂ y / ∂ b = 1
  prefs: []
  type: TYPE_NORMAL
- en: 'And if w = 0 and x ≠ 0, then we have this:'
  prefs: []
  type: TYPE_NORMAL
- en: ∂ y / ∂ w ≠ 0, ∂ y / ∂ x = 0, ∂ y / ∂ b ≠ 0
  prefs: []
  type: TYPE_NORMAL
- en: 'This means as long as x ≠ 0, one gradient descent iteration will make w non-zero.
    Then, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: ∂ y / ∂ x ≠ 0
  prefs: []
  type: TYPE_NORMAL
- en: So, the zero convolutions will progressively become a common convolution layer
    with non-zero weights. What a genius design!
  prefs: []
  type: TYPE_NORMAL
- en: The SD UNet is only connected with a ControlNet at the encoder blocks and the
    middle blocks. The trainable blue blocks and the white zero convolution layers
    are added to build a ControlNet. It’s simple and effective.
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper— Adding Conditional Control to Text-to-Image Diffusion
    Models [1] by Lvmin Zhang et al — its authors also provided an ablative study
    and discussed lots of different cases, such as swapping the **zero convolution**
    layer with a **traditional convolution** layer and comparing the differences.
    It is a great paper and fun to read.
  prefs: []
  type: TYPE_NORMAL
- en: Further usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce more usage of ControlNet, covering SD V1.5
    and also SDXL.
  prefs: []
  type: TYPE_NORMAL
- en: More ControlNets with SD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The author of the ControlNet-v1-1-nightly repository [3] lists all the currently
    available V1.1 ControlNet models for SD. As of the time I am writing this chapter,
    the list is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: You can simply swap the ControlNet model’s name with one from this list to start
    using it. Generate the control image using one of the annotators from the open
    source ControlNet auxiliary models[6].
  prefs: []
  type: TYPE_NORMAL
- en: Considering the speed of development in the field of AI, when you are reading
    this, the version may have increased to v1.1+. However, the underlying mechanism
    should be the same.
  prefs: []
  type: TYPE_NORMAL
- en: SDXL ControlNets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I am writing this chapter, SDXL has just been released, and this new model
    generates excellent images with shorter prompts than before. The Hugging Face
    Diffusers team trained and provided several ControlNet models for the XL models.
    Its usage is almost the same as the previous version. Here, let’s use the `controlnet-openpose-sdxl-1.0`
    open pose ControlNet for SDXL.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you will need a dedicated GPU with more than 15 GB of VRAM to run
    the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize an SDXL pipeline using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, generate an image with a man in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will generate an image, as shown in *Figure 13**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7: A man in a suit, generated by SDXL](img/B21263_13_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: A man in a suit, generated by SDXL'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `OpenposeDetector` from `controlnet_aux` [6] to extract the pose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We will get the pose image shown in *Figure 13**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8: Pose image of the man in a suit](img/B21263_13_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Pose image of the man in a suit'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s start an SDXL pipeline with the SDXL ControlNet open pose model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use the new ControlNet pipeline to generate a new image from the
    pose image with the same style. We will reuse the prompt but replace **man** with
    **woman**. We are aiming to generate a new image of a woman in a suit but in the
    same pose as the previous image of a man:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The code generates a new image with the same pose, exactly matching the expectations,
    as shown in *Figure 13**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9: A woman in a suit, generated using an SDXL ControlNet](img/B21263_13_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: A woman in a suit, generated using an SDXL ControlNet'
  prefs: []
  type: TYPE_NORMAL
- en: We will further discuss Stable Diffusion XL in [*Chapter 16*](B21263_16.xhtml#_idTextAnchor309).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced a way to precisely control image generation using
    SD ControlNets. From the detailed samples we have provided, you can start using
    one or multiple ControlNet models with SD v1.5 and also SDXL.
  prefs: []
  type: TYPE_NORMAL
- en: We also drilled down into the internals of ControlNet, explaining how it works
    in a nutshell.
  prefs: []
  type: TYPE_NORMAL
- en: We can use ControlNet in lots of applications, including applying a style to
    an image, applying a shape to an image, merging two images into one, and generating
    a human body using a posed image. It is powerful and amazingly useful in many
    ways. Our imagination is the only limitation.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is one other limitation: it is hard to align the background
    and overall context between two generations (with different seeds). You may want
    to use ControlNet to generate a video from the extracted frames from a source
    video, but the results are still not ideal.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover a solution to generate video and animation
    using SD.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding conditional control to text-to-image diffusion models: [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ControlNet v1.0 GitHub repository: [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ControlNet v1.1 GitHub repository: [https://github.com/lllyasviel/ControlNet-v1-1-nightly](https://github.com/lllyasviel/ControlNet-v1-1-nightly)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`surface_normal_uncertainty`: [https://github.com/baegwangbin/surface_normal_uncertainty](https://github.com/baegwangbin/surface_normal_uncertainty)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Zero convolution FAQ: [https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md](https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ControlNet AUX: [https://github.com/patrickvonplaten/controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Canny edge detector: [https://en.wikipedia.org/wiki/Canny_edge_detector](https://en.wikipedia.org/wiki/Canny_edge_detector)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion
    Model: [https://showlab.github.io/X-Adapter/](https://showlab.github.io/X-Adapter/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
