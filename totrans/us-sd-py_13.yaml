- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Generating Images with ControlNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 ControlNet 生成图像
- en: Stable Diffusion’s ControlNet is a neural network plugin that allows you to
    control diffusion models by adding extra conditions. It was first introduced in
    a paper called Adding Conditional Control to Text-to-Image Diffusion Models [1]
    by Lvmin Zhang and Maneesh Agrawala, published in 2023.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion 的 ControlNet 是一个神经网络插件，允许你通过添加额外条件来控制扩散模型。它首次在 2023 年由 Zhang
    Lvmin 和 Maneesh Agrawala 发表的论文《Adding Conditional Control to Text-to-Image Diffusion
    Models [1]》中介绍。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: What is ControlNet and how is it different?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 ControlNet 以及它与其他方法有何不同？
- en: Usage of ControlNet
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ControlNet 的使用方法
- en: Using multiple ControlNets in one pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个管道中使用多个 ControlNet
- en: How ControlNet works
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ControlNet 的工作原理
- en: More ControlNet usage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多 ControlNet 使用方法
- en: By the end of this chapter, you will understand how ControlNet works and how
    to use Stable Diffusion V1.5 and Stable Diffusion XL ControlNet models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解 ControlNet 的工作原理以及如何使用 Stable Diffusion V1.5 和 Stable Diffusion
    XL ControlNet 模型。
- en: What is ControlNet and how is it different?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 ControlNet 以及它与其他方法有何不同？
- en: In terms of “control,” you may recall textual embedding, LoRA, and the image-to-image
    diffusion pipeline. But what makes ControlNet different and useful?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在“控制”方面，你可能还记得文本嵌入、LoRA 和图像到图像的扩散管道。但是什么让 ControlNet 不同且有用？
- en: 'Unlike other solutions, ControlNet is a model that works on the UNet diffusion
    process directly. We compare these solutions in *Table 13.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他解决方案不同，ControlNet 是一个直接在 UNet 扩散过程中工作的模型。我们在 *表 13.1* 中比较了这些解决方案：
- en: '| **Control Method** | **Functioning Stage** | **Usage Scenario** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **控制方法** | **工作阶段** | **使用场景** |'
- en: '| --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Textual Embedding | Text encoder | Add a new style, a new concept, or a new
    face |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 文本嵌入 | 文本编码器 | 添加新的风格、新的概念或新的面孔 |'
- en: '| LoRA | Merge LoRA weights to the UNet model (and the CLIP text encoder, optional)
    | Add a set of styles, concepts, and generate content |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 将 LoRA 权重合并到 UNet 模型（以及可选的 CLIP 文本编码器） | 添加一组风格、概念并生成内容 |'
- en: '| Image-to-Image | Provide the initial latent image | Fix images, or add styles
    and concepts to images |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 图像到图像 | 提供初始潜在图像 | 修复图像，或向图像添加风格和概念 |'
- en: '| ControlNet | ControlNet participant denoising together with a checkpoint
    model UNet | Control shape, pose, content detail |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| ControlNet | ControlNet 参与者与检查点模型 UNet 一起去噪 | 控制形状、姿势、内容细节 |'
- en: 'Table 13.1: A comparison of textual embedding, LoRA, image-to-image, and ControlNet'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1：文本嵌入、LoRA、图像到图像和 ControlNet 的比较
- en: In many ways, ControlNet is similar to the image-to-image pipeline, as we discussed
    in [*Chapter 11*](B21263_11.xhtml#_idTextAnchor214). Both image-to-image and ControlNet
    can be used to enhance images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，ControlNet 与我们讨论过的图像到图像管道类似，如 [*第 11 章*](B21263_11.xhtml#_idTextAnchor214)。图像到图像和
    ControlNet 都可以用来增强图像。
- en: However, ControlNet can “control” the image in a more precise way. Imagine you
    want to generate an image that uses a specific pose from another image or perfectly
    align objects within the scene to a specific reference point. This kind of precision
    is impossible with the out-of-the-box Stable Diffusion model. ControlNet is the
    tool that can help you achieve these goals.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，ControlNet 可以以更精确的方式“控制”图像。想象一下，你想要生成一个使用另一张图像中的特定姿势或完美对齐场景中的物体到特定参考点的图像。这种精度是使用现成的
    Stable Diffusion 模型无法实现的。ControlNet 是帮助你实现这些目标的工具。
- en: 'Besides, ControlNet models work with all other open source checkpoint models,
    unlike some other solutions, which work only with one base model provided by their
    author. The team that created ControlNet not only open-sourced the model but also
    open-sourced the code to train a new model. In other words, we can train a ControlNet
    model and make it work with any other model. This is what the original paper says
    [1]:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，ControlNet 模型与其他所有开源检查点模型兼容，而一些其他解决方案仅与作者提供的单个基础模型兼容。创建 ControlNet 的团队不仅开源了模型，还开源了训练新模型的代码。换句话说，我们可以训练一个
    ControlNet 模型，使其与其他任何模型一起工作。这正是原始论文 [1] 中所说的：
- en: Since Stable Diffusion is a typical UNet structure, this ControlNet architecture
    is likely to be applicable with other models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Stable Diffusion 是一个典型的 UNet 结构，这种 ControlNet 架构可能适用于其他模型。
- en: Note that ControlNet models will only work with models using the same base model.
    A **Stable Diffusion** (**SD**) v1.5 ControlNet model works with all other SD
    v1.5 models. For **Stable Diffusion XL** (**SDXL**) models, we will need a ControlNet
    model that is trained with SDXL. This is because SDXL models use a different architecture,
    a larger UNet than the SD v1.5\. Without additional work, a ControlNet model is
    trained with one architecture and only works with this type of model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'I used “*without additional work*” because in December 2023, to bridge this
    gap, a paper from Lingmin Ran et al, called *X-Adapter: Adding Universal Compatibility
    of Plugins for Upgraded Diffusion Model* was published [8]. This paper details
    an adapter that enables us to use SD V1.5 LoRA and ControlNet in a new SDXL model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s start using ControlNet with SD models.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Usage of ControlNet
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into the backend of ControlNet, in this section, we will start
    using ControlNet to help control image generation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will first generate an image using SD, take the
    Canny shape of the object, and then use the Canny shape to generate a new image
    with the help of ControlNet.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: A Canny image refers to an image that has undergone Canny edge detection, which
    is a popular edge detection algorithm. It was developed by John F. Canny in 1986\.
    [7]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use SD to generate an image using the following code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a sample image using SD:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will see an image of a cat, as shown in *Figure 13**.1*:'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1: A cat, generated by SD](img/B21263_13_01.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: A cat, generated by SD'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Then we will get the Canny shape of the sample image.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will need another package, `controlnet_aux`, to create a Canny image from
    an image. Simply execute the following two lines of `pip` commands to install
    `controlnet_aux`:'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can generate the image Canny edge shape with three lines of code:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here’s a breakdown of the code:'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`from controlnet_aux import CannyDetector`: This line imports the `CannyDetector`
    class from the `controlnet_aux` module. There are many other detectors.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_canny = canny(image, 30, 100)`: This line calls the `__call__` method
    of the `CannyDetector` class (which is implemented as a callable object) with
    the following arguments:'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image`: This is the input image to which the Canny edge detection algorithm
    will be applied.'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`30`: This is the lower threshold value for the edges. Any edges with an intensity
    gradient below this value will be discarded.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`100`: This is the upper threshold value for the edges. Any edges with an intensity
    gradient above this value will be considered strong edges.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding code will generate the Canny image shown in *Figure 13**.2*:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.2: Canny image of a cat](img/B21263_13_02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Canny image of a cat'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use the ControlNet model to generate a new image based on this
    Canny image. First, let’s load up the ControlNet model:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The code will download the ControlNet model from Hugging Face automatically
    at your first run. If you have the ControlNet `safetensors` model in your storage
    and want to use your own model, you can convert the file to the diffuser format
    first. You can find the conversion code in [*Chapter 6*](B21263_06.xhtml#_idTextAnchor117).
    Then, replace `takuma104/control_v11` with the path to the ControlNet model.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Initialize a ControlNet pipeline:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that you can freely swap `stablediffusionapi/deliberate-v2` with any other
    SD v1.5 models from the community.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate the new image using the ControlNet pipeline. In the following example,
    we will replace the cat with a dog:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'These lines of code will generate a new image following the Canny edge, but
    the little cat is now a dog, as shown in *Figure 13**.3*:'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.3: A dog, generated using the cat Canny image with ControlNet](img/B21263_13_03.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: A dog, generated using the cat Canny image with ControlNet'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The cat’s body structure and shape are preserved. Feel free to change the prompt
    and settings to explore the amazing capabilities of the model. One thing to note
    is that if you don’t provide a prompt to the ControlNet pipeline, the pipeline
    will still output a meaningful image, maybe another style of cat, which means
    the ControlNet model learned the underlying meaning of a certain Canny edge.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we used only one ControlNet model, but we can also provide
    multiple ControlNet models to one pipeline.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple ControlNets in one pipeline
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will initialize one more ControlNet, NormalBAE, and then
    feed the Canny and NormalBAE ControlNet models together to form a pipeline.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate a Normal BAE as one additional control image. Normal BAE is
    a model that’s used to estimate a normal map using the normal uncertainty method
    [4] proposed by Bae et al:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This code will generate the original image’s Normal BAE map, as shown in *Figure
    13**.4*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.4: Normal BAE image of the generated cat](img/B21263_13_04.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Normal BAE image of the generated cat'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s initialize two ControlNet models for one pipeline: one Canny ControlNet
    model, and another NormalBae ControlNet model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'From the code, we can easily see that all ControlNet models share the same
    architecture. To load different ControlNet models, we only need to change the
    model name. Also, note that the two ControlNet models are in a Python `controlnets`
    `list`. We can provide these ControlNet models to the pipeline directly, as shown
    here:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In the inference stage, use one additional parameter, `controlnet_conditioning_scale`,
    to control the influence scale of each ControlNet:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This code will give us another image, as shown in *Figure 13**.5*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet](img/B21263_13_05.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: A dog generated from Canny ControlNet and a Normal BAE ControlNet'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: In `controlnet_conditioning_scale = [0.5,0.5]`, I give each ControlNet model
    a `0.5` scale value. The two scale values add up to `1.0`. We should give weights
    that add up to no more than `2`. Values that are too will lead to undesired images.
    For instance, if you give weights of `1.2` and `1.3` to each ControlNet model,
    like `controlnet_conditioning_scale = [1.2,1.3]`, you may get an undesired image.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `controlnet_conditioning_scale = [0.5,0.5]` 中，我为每个 ControlNet 模型赋予一个 `0.5`
    的缩放值。这两个缩放值加起来为 `1.0`。我们应该赋予的总权重不超过 `2`。过高的值会导致不期望的图像。例如，如果你给每个 ControlNet 模型赋予
    `1.2` 和 `1.3` 的权重，如 `controlnet_conditioning_scale = [1.2,1.3]`，你可能会得到一个不期望的图像。
- en: If we have successfully generated images using the ControlNet models, we have
    together witnessed the power of ControlNet. In the next section, we will discuss
    how ControlNet works.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们成功使用 ControlNet 模型生成图像，我们就共同见证了 ControlNet 的力量。在下一节中，我们将讨论 ControlNet 的工作原理。
- en: How ControlNet works
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet 的工作原理
- en: In this section, we will drill down into the ControlNet structure and see how
    ControlNet works internally.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 ControlNet 的结构，并了解 ControlNet 内部是如何工作的。
- en: 'ControlNet works by injecting additional conditions into the blocks of a neural
    network. As shown in *Figure 13**.6*, the trainable copy is the ControlNet block
    that adds additional guidance to the original SD UNet block:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet 通过向神经网络块注入额外的条件来工作。如图 *图 13**.6* 所示，可训练的副本是添加额外指导到原始 SD UNet 块的 ControlNet
    块：
- en: '![Figure 13.6: Adding ControlNet components](img/B21263_13_06.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6：添加 ControlNet 组件](img/B21263_13_06.jpg)'
- en: 'Figure 13.6: Adding ControlNet components'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6：添加 ControlNet 组件
- en: During the training stage, we take a copy of the target layer block as the ControlNet
    block. In *Figure 13**.6*, it is denoted as a **trainable copy**. Unlike typical
    neural network initialization with Gaussian distributions for all parameters,
    ControlNet utilizes pre-trained weights from the Stable Diffusion base model.
    Most of these base model parameters are frozen (with the option to unfreeze them
    later) and only the additional ControlNet components are trained from scratch.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，我们取目标层块的一个副本作为 ControlNet 块。在 *图 13**.6* 中，它被标记为 **可训练的副本**。与所有参数使用高斯分布进行典型神经网络初始化不同，ControlNet
    使用来自稳定扩散基础模型的预训练权重。这些基础模型参数中的大多数都是冻结的（有选项在以后解冻它们），只有额外的 ControlNet 组件是从头开始训练的。
- en: During training and inference, the input x is usually a 3D dimensional vector,
    x ∈ ℝ h×w×c, with h, w, c as the height, width, and number of channels. c is a
    conditioning vector that we will pass into the SD UNet and also to the ControlNet
    model network.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练和推理过程中，输入 x 通常是一个三维向量，x ∈ ℝ h×w×c，其中 h、w、c 分别是高度、宽度和通道数。c 是一个条件向量，我们将将其传递到
    SD UNet 和 ControlNet 模型网络中。
- en: The **zero convolution** plays a pivotal role in this process. **Zero convolutions**
    are 1D convolutions with weights and biases initialized to zero. The advantage
    of zero convolution is that, even without a single training step, the value injected
    from ControlNet will have no effect on image generation. This ensures that the
    side network does not negatively impact image generation at any stage.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**零卷积**在这个过程中起着关键作用。**零卷积**是权重和偏差初始化为零的 1D 卷积。零卷积的优势在于，即使没有进行单个训练步骤，从 ControlNet
    注入的值也不会对图像生成产生影响。这确保了辅助网络在任何阶段都不会对图像生成产生负面影响。'
- en: 'You might be thinking: if the weight of a convolution layer is zero, wouldn''t
    the gradient also be zero, rendering the network unable to learn? However, as
    the paper''s authors explain [5], the reality is more nuanced.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：如果卷积层的权重为零，梯度不也会为零吗？这难道不会使网络无法学习吗？然而，正如论文的作者解释[5]的那样，实际情况更为复杂。
- en: 'Let’s consider one simple case:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的例子：
- en: y = wx + b
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: y = wx + b
- en: 'Then we have the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们还有以下情况：
- en: ∂ y / ∂ w = x, ∂ y / ∂ x = w, ∂ y / ∂ b = 1
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ y / ∂ w = x, ∂ y / ∂ x = w, ∂ y / ∂ b = 1
- en: 'And if w = 0 and x ≠ 0, then we have this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 w = 0 且 x ≠ 0，那么我们就有以下情况：
- en: ∂ y / ∂ w ≠ 0, ∂ y / ∂ x = 0, ∂ y / ∂ b ≠ 0
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ y / ∂ w ≠ 0, ∂ y / ∂ x = 0, ∂ y / ∂ b ≠ 0
- en: 'This means as long as x ≠ 0, one gradient descent iteration will make w non-zero.
    Then, we have:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着只要 x ≠ 0，一次梯度下降迭代就会使 w 非零。然后，我们有：
- en: ∂ y / ∂ x ≠ 0
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ y / ∂ x ≠ 0
- en: So, the zero convolutions will progressively become a common convolution layer
    with non-zero weights. What a genius design!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，零卷积将逐渐变成具有非零权重的普通卷积层。多么天才的设计啊！
- en: The SD UNet is only connected with a ControlNet at the encoder blocks and the
    middle blocks. The trainable blue blocks and the white zero convolution layers
    are added to build a ControlNet. It’s simple and effective.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper— Adding Conditional Control to Text-to-Image Diffusion
    Models [1] by Lvmin Zhang et al — its authors also provided an ablative study
    and discussed lots of different cases, such as swapping the **zero convolution**
    layer with a **traditional convolution** layer and comparing the differences.
    It is a great paper and fun to read.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Further usage
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will introduce more usage of ControlNet, covering SD V1.5
    and also SDXL.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: More ControlNets with SD
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The author of the ControlNet-v1-1-nightly repository [3] lists all the currently
    available V1.1 ControlNet models for SD. As of the time I am writing this chapter,
    the list is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You can simply swap the ControlNet model’s name with one from this list to start
    using it. Generate the control image using one of the annotators from the open
    source ControlNet auxiliary models[6].
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Considering the speed of development in the field of AI, when you are reading
    this, the version may have increased to v1.1+. However, the underlying mechanism
    should be the same.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: SDXL ControlNets
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As I am writing this chapter, SDXL has just been released, and this new model
    generates excellent images with shorter prompts than before. The Hugging Face
    Diffusers team trained and provided several ControlNet models for the XL models.
    Its usage is almost the same as the previous version. Here, let’s use the `controlnet-openpose-sdxl-1.0`
    open pose ControlNet for SDXL.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Note that you will need a dedicated GPU with more than 15 GB of VRAM to run
    the following example.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize an SDXL pipeline using the following code:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, generate an image with a man in it:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The code will generate an image, as shown in *Figure 13**.7*:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.7: A man in a suit, generated by SDXL](img/B21263_13_07.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: A man in a suit, generated by SDXL'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `OpenposeDetector` from `controlnet_aux` [6] to extract the pose:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We will get the pose image shown in *Figure 13**.8*:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.8: Pose image of the man in a suit](img/B21263_13_08.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Pose image of the man in a suit'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s start an SDXL pipeline with the SDXL ControlNet open pose model:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now we can use the new ControlNet pipeline to generate a new image from the
    pose image with the same style. We will reuse the prompt but replace **man** with
    **woman**. We are aiming to generate a new image of a woman in a suit but in the
    same pose as the previous image of a man:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The code generates a new image with the same pose, exactly matching the expectations,
    as shown in *Figure 13**.9*:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9: A woman in a suit, generated using an SDXL ControlNet](img/B21263_13_09.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: A woman in a suit, generated using an SDXL ControlNet'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: We will further discuss Stable Diffusion XL in [*Chapter 16*](B21263_16.xhtml#_idTextAnchor309).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第16章*](B21263_16.xhtml#_idTextAnchor309)中进一步讨论Stable Diffusion XL。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced a way to precisely control image generation using
    SD ControlNets. From the detailed samples we have provided, you can start using
    one or multiple ControlNet models with SD v1.5 and also SDXL.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一种使用SD ControlNets精确控制图像生成的方法。从我们提供的详细示例中，你可以开始使用一个或多个ControlNet模型与SD
    v1.5以及SDXL一起使用。
- en: We also drilled down into the internals of ControlNet, explaining how it works
    in a nutshell.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还深入探讨了ControlNet的内部机制，简要解释了它是如何工作的。
- en: We can use ControlNet in lots of applications, including applying a style to
    an image, applying a shape to an image, merging two images into one, and generating
    a human body using a posed image. It is powerful and amazingly useful in many
    ways. Our imagination is the only limitation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在许多应用中使用ControlNet，包括将风格应用于图像、将形状应用于图像、将两个图像合并为一个，以及使用摆姿势的图像生成人体。它在许多方面都非常强大且非常有用。我们的想象力是唯一的限制。
- en: 'However, there is one other limitation: it is hard to align the background
    and overall context between two generations (with different seeds). You may want
    to use ControlNet to generate a video from the extracted frames from a source
    video, but the results are still not ideal.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有一个限制：很难在两个生成（具有不同的种子）之间对齐背景和整体上下文。你可能想使用ControlNet从源视频提取的帧生成视频，但结果仍然不理想。
- en: In the next chapter, we will cover a solution to generate video and animation
    using SD.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍使用SD生成视频和动画的解决方案。
- en: References
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Adding conditional control to text-to-image diffusion models: [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '向文本到图像扩散模型添加条件控制: [https://arxiv.org/abs/2302.05543](https://arxiv.org/abs/2302.05543)'
- en: 'ControlNet v1.0 GitHub repository: [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ControlNet v1.0 GitHub仓库: [https://github.com/lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)'
- en: 'ControlNet v1.1 GitHub repository: [https://github.com/lllyasviel/ControlNet-v1-1-nightly](https://github.com/lllyasviel/ControlNet-v1-1-nightly)'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ControlNet v1.1 GitHub仓库: [https://github.com/lllyasviel/ControlNet-v1-1-nightly](https://github.com/lllyasviel/ControlNet-v1-1-nightly)'
- en: '`surface_normal_uncertainty`: [https://github.com/baegwangbin/surface_normal_uncertainty](https://github.com/baegwangbin/surface_normal_uncertainty)'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`surface_normal_uncertainty`: [https://github.com/baegwangbin/surface_normal_uncertainty](https://github.com/baegwangbin/surface_normal_uncertainty)'
- en: 'Zero convolution FAQ: [https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md](https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md)'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '零卷积常见问题解答: [https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md](https://github.com/lllyasviel/ControlNet/blob/main/docs/faq.md)'
- en: 'ControlNet AUX: [https://github.com/patrickvonplaten/controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'ControlNet AUX: [https://github.com/patrickvonplaten/controlnet_aux](https://github.com/patrickvonplaten/controlnet_aux)'
- en: 'Canny edge detector: [https://en.wikipedia.org/wiki/Canny_edge_detector](https://en.wikipedia.org/wiki/Canny_edge_detector)'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Canny边缘检测器: [https://en.wikipedia.org/wiki/Canny_edge_detector](https://en.wikipedia.org/wiki/Canny_edge_detector)'
- en: 'X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion
    Model: [https://showlab.github.io/X-Adapter/](https://showlab.github.io/X-Adapter/)'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'X-Adapter：为升级的扩散模型添加插件通用兼容性: [https://showlab.github.io/X-Adapter/](https://showlab.github.io/X-Adapter/)'
