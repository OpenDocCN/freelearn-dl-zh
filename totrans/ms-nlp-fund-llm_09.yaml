- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: 'Exploring the Frontiers: Advanced Applications and Innovations Driven by LLMs'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索前沿：由LLM驱动的先进应用和创新
- en: In the rapidly evolving landscape of **natural language processing** (**NLP**),
    **large language models** (**LLMs**) have marked a revolutionary step forward,
    reshaping how we interact with information, automate processes, and derive insights
    from vast data pools. This chapter represents the culmination of our journey through
    the emergence and development of NLP methods. It is here that the theoretical
    foundations laid in previous chapters converge with practical, cutting-edge applications,
    illuminating the remarkable capabilities of LLMs when harnessed with the right
    tools and techniques.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）快速发展的领域中，**大型语言模型（LLM**）已经迈出了革命性的一步，重塑了我们与信息互动、自动化流程以及从大量数据池中提取洞察的方式。本章代表了我们在NLP方法和出现与发展过程中的旅程的总结。在这里，之前章节中建立的理论基础与实际、前沿的应用相结合，展示了当使用正确的工具和技术时，LLM的非凡能力。
- en: We delve into the most recent and thrilling advancements in LLM applications,
    presented through detailed Python code examples designed for hands-on learning.
    This approach not only illustrates the power of LLMs but also equips you with
    the skills to implement these technologies in real-world scenarios. The subjects
    covered in this chapter are meticulously selected to showcase a spectrum of advanced
    functionalities and applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入探讨LLM应用中最新的和最激动人心的进展，通过详细的Python代码示例进行展示，旨在进行动手学习。这种方法不仅展示了LLM的力量，还使你具备在现实场景中实施这些技术的技能。本章涵盖的主题经过精心挑选，以展示一系列高级功能和应用的广度。
- en: The importance of this chapter cannot be overstated. It not only reflects the
    state of the art in NLP but also serves as a bridge to the future, where the integration
    of these technologies into everyday solutions becomes seamless. By the end of
    this chapter, you will have a comprehensive understanding of how to apply the
    latest LLM techniques and innovations, empowering you to push the boundaries of
    what’s possible in NLP and beyond. Join us on this exciting journey to unlock
    the full potential of LLMs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重要性不容小觑。它不仅反映了NLP的最新水平，还作为通往未来的桥梁，在这些技术融入日常解决方案时变得无缝。到本章结束时，你将全面了解如何应用最新的LLM技术和创新，赋予你推动NLP及其超越可能性的边界的能力。加入我们，踏上这段激动人心的旅程，解锁LLM的全部潜力。
- en: 'Let’s go through the main headings covered in the chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾本章中涵盖的主要标题：
- en: Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RAG和LangChain增强LLM性能——深入高级功能
- en: Advanced methods with chains
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于链的高级方法
- en: Retrieving information from various web sources automatically
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从各种网络来源自动检索信息
- en: Prompt compression and API cost reduction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示压缩和API成本降低
- en: Multiple agents – forming a team of LLMs who collaborate
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个代理——形成一个LLM协作的团队
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, the following will be necessary:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，以下将是必需的：
- en: '**Programming knowledge**: Familiarity with Python programming is a must since
    the open source models, OpenAI’s API, and LangChain are all illustrated using
    Python code.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编程知识**：熟悉Python编程是必需的，因为开源模型、OpenAI的API和LangChain都是使用Python代码进行说明的。'
- en: '**Access to OpenAI’s API**: An API key from OpenAI will be required to explore
    closed source models. This can be obtained by creating an account with OpenAI
    and agreeing to their terms of service.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问OpenAI的API**：探索封闭源模型需要OpenAI的API密钥。这可以通过在OpenAI创建账户并同意他们的服务条款来获得。'
- en: '**Open source models**: Access to the specific open source models mentioned
    in this chapter will be necessary. These can be accessed and downloaded from their
    respective repositories or via package managers such as pip or conda.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开源模型**：访问本章中提到的特定开源模型将是必要的。这些模型可以通过各自的存储库或通过包管理器如pip或conda进行访问和下载。'
- en: '**Local development environment**: A local development environment setup with
    Python installed is required. An **integrated development environment** (**IDE**)
    such as **PyCharm**, **Jupyter Notebook**, or a simple text editor can be used.
    Note that we recommend a free **Google Colab** notebook, as it encapsulates all
    these requirements in a seamless web interface.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地开发环境**：需要一个安装了Python的本地开发环境。可以使用**集成开发环境（IDE**）如**PyCharm**、**Jupyter Notebook**或简单的文本编辑器。请注意，我们推荐免费的**Google
    Colab**笔记本，因为它将这些要求封装在一个无缝的网页界面中。'
- en: '**Ability to install libraries**: You must have permission for the installation
    of the required Python libraries such as **NumPy**, **SciPy**, **TensorFlow**,
    and **PyTorch**. Note that the code we provide includes the required installations
    so you won’t have to install them ahead of time. We simply stress that you should
    have permission to do so, which we expect you would. Specifically, using a free
    Google Colab notebook would suffice.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安装库的能力**：您必须拥有安装所需Python库（如**NumPy**、**SciPy**、**TensorFlow**和**PyTorch**）的权限。请注意，我们提供的代码中已包含所需的安装，因此您无需提前安装。我们只是强调您应该有权限这样做，我们预期您会这样做。具体来说，使用免费的Google
    Colab笔记本就足够了。'
- en: '**Hardware requirements**: Depending on the complexity and size of the models
    you’re working with, a computer with sufficient processing power (potentially
    including a good GPU for ML tasks) and ample memory will be required. This is
    only relevant when choosing to not use the free Google Colab.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬件要求**：根据您所处理的模型复杂性和大小，您将需要一个具有足够处理能力（可能包括用于ML任务的良好GPU）和充足内存的计算机。这仅当您选择不使用免费的Google
    Colab时才相关。'
- en: Now that we’ve set up LLM applications using APIs and locally, we can finally
    deploy the advanced applications of LLMs that let us leverage their immense power.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用API和本地设置好了LLM应用，我们终于可以部署LLM的高级应用，让我们能够利用其巨大的力量。
- en: Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用RAG和LangChain增强LLM性能——深入高级功能
- en: The **retrieval-augmented generation** (**RAG**) framework has become instrumental
    in tailoring **large language models** (**LLMs**) for specific domains or tasks,
    bridging the gap between the simplicity of prompt engineering and the complexity
    of model fine-tuning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索增强生成**（RAG）框架已成为针对特定领域或任务定制**大型语言模型**（LLM）的关键工具，它弥合了提示工程简单性和模型微调复杂性之间的差距。'
- en: Prompt engineering stands as the initial, most accessible technique for customizing
    LLMs. It leverages the model’s capacity to interpret and respond to queries based
    on the input prompt. For example, to inquire if Nvidia surpassed earnings expectations
    in its latest announcement, directly providing the earnings call content within
    the prompt can compensate for the LLM’s lack of immediate, up-to-date context.
    This approach, while straightforward, hinges on the model’s ability to digest
    and analyze the provided information within a single or a series of carefully
    crafted prompts.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是定制LLM的最初、最易访问的技术。它利用模型根据输入提示解释和响应用户查询的能力。例如，要查询Nvidia在其最新公告中是否超过了收益预期，可以在提示中直接提供收益电话会议内容，以弥补LLM缺乏即时、最新上下文的问题。这种方法虽然简单，但取决于模型在单个或一系列精心设计的提示中消化和分析提供的信息的能力。
- en: When the scope of inquiry exceeds what prompt engineering can accommodate—such
    as analyzing a decade’s worth of tech sector earnings calls—RAG becomes indispensable.
    Prior to RAG’s adoption, the alternative was fine-tuning, a resource-intensive
    process requiring significant adjustments to the LLM’s architecture to incorporate
    extensive datasets. RAG simplifies this by preprocessing and storing large amounts
    of data in a vector database. It intelligently isolates and retrieves the data
    segments pertinent to the query, effectively condensing the vast information into
    a manageable, prompt-size context for the LLM. This innovation drastically reduces
    the time, resources, and expertise needed for such extensive data familiarization
    tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当调查范围超出提示工程所能容纳的范围时——例如分析十年来的科技行业收益电话会议——RAG变得不可或缺。在采用RAG之前，替代方案是微调，这是一个资源密集型的过程，需要显著调整LLM的架构以纳入大量数据集。RAG通过预处理并将大量数据存储在向量数据库中简化了这一点。它智能地隔离和检索与查询相关的数据段，有效地将大量信息压缩成LLM可管理的、提示大小的上下文。这一创新大幅减少了此类广泛数据熟悉任务所需的时间、资源和专业知识。
- en: In [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), we introduced the general
    concept of RAGs and, in particular, LangChain, a RAG framework distinguished by
    its advanced capabilities.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中，我们介绍了RAG（检索增强生成）的一般概念，特别是LangChain，这是一个以其先进功能而区别于其他RAG框架的框架。
- en: We will now discuss the additional unique features LangChain offers for enhancing
    LLM applications, providing you with practical insights into its implementation
    and utility in complex NLP tasks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将讨论LangChain为增强LLM应用提供的额外独特功能，为您提供其实施和复杂NLP任务中的实用见解。
- en: LangChain pipeline with Python – enhancing performance with LLMs
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Python的LangChain管道 – 通过LLM增强性能
- en: In this section, we will pick up where we left off with our last example from
    [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440). In this scenario, we are in the
    healthcare sector, and in our hospital, our care providers are expressing a need
    to be able to quickly surface patients’ records based on rough descriptions of
    the patient or their condition. For example, “Who was that patient I saw last
    year who was pregnant with triplets?” “Did I ever have a patient with a history
    of cancer from both of their parents and they were interested in a clinical trial?”
    and so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从我们上一节在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中的最后一个例子继续进行。在这个场景中，我们处于医疗行业，在我们的医院中，我们的护理提供者表示需要能够根据对患者的粗略描述或其状况来快速检索患者的记录。例如，“去年我看到的那位怀有双胞胎的患者是谁？”“我是否曾经有过一位双亲都有癌症病史的患者，他们对临床试验感兴趣？”等等。
- en: Important note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We stress that these aren’t real medical notes and that the people described
    in the notes aren’t real.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调，这些不是真实的医疗笔记，并且笔记中描述的人也不是真实的。
- en: In our example in [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), we kept the
    pipeline at minimum complexity by simply leveraging the vector databases of embeddings
    of clinical notes, and then we applied similarity search to look for notes based
    on simple requests. We noticed how one of the questions, the second question,
    received a wrong answer with the similarity search algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们例子中的[*第8章*](B18949_08.xhtml#_idTextAnchor440)，我们通过简单地利用临床笔记的嵌入向量数据库，将管道保持在了最小复杂度，然后我们应用了相似度搜索来根据简单的请求查找笔记。我们注意到其中一个问题，即第二个问题，在使用相似度搜索算法时得到了错误的答案。
- en: We will now enhance that pipeline. We will not settle for the results of the
    similarity search and surface those to the physicians; we will take those results
    that were deemed to be similar in content to the request, and we will employ an
    LLM to go through these results, vet them, and tell us which ones are indeed relevant
    to the physician.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将增强这个管道。我们不会满足于相似度搜索的结果，并将这些结果展示给医生；我们将那些被认为与请求内容相似的结果，并使用LLM来检查这些结果，核实它们，并告诉我们哪些确实与医生相关。
- en: Paid LLMs versus free
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支付的LLM与免费的
- en: We'll use this pipeline to exemplify the utility of either type of LLM, paid
    or free. We give you the choice, via the `paid_vs_free` variable, to either use
    OpenAI’s paid GPT model or a free LLM. Using OpenAI’s paid model would leverage
    their API and would require an API key. However, the free LLM is imported to the
    local environment where the Python code is run, thus making it available to anyone
    who has an internet connection and sufficient computational resources.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个管道来展示付费或免费类型的LLM的实用性。通过`paid_vs_free`变量，你可以选择使用OpenAI的付费GPT模型或一个免费的LLM。使用OpenAI的付费模型将利用他们的API，并需要一个API密钥。然而，免费的LLM被导入到运行Python代码的本地环境中，因此任何有互联网连接和足够计算资源的人都可以使用。
- en: Let’s start getting hands-on and experimenting with the code.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始动手实验代码。
- en: Applying advanced LangChain configurations and pipelines
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用高级LangChain配置和管道
- en: 'Refer to the following notebook: `Ch9_Advanced_LangChain_Configurations_and_Pipeline.ipynb`.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下笔记本：`Ch9_Advanced_LangChain_Configurations_and_Pipeline.ipynb`。
- en: Note that the first part of the notebook is identical to the notebook from [*Chapter
    8*](B18949_08.xhtml#_idTextAnchor440), so we will skip the description of that
    part.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，笔记本的前一部分与[*第8章*](B18949_08.xhtml#_idTextAnchor440)中的笔记本相同，因此我们将跳过该部分的描述。
- en: Installing the required Python libraries
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安装所需的Python库
- en: Here, we need to expand the set of installed libraries and install `openai`
    and `gpt4all`. Moreover, in order to utilize `gpt4all`, we will need to download
    a `.bin` file from the web.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们需要扩展已安装的库，并安装`openai`和`gpt4all`。此外，为了利用`gpt4all`，我们需要从网络上下载一个`.bin`文件。
- en: These two steps are easy to perform via the notebook.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个步骤通过笔记本执行都很简单。
- en: Setting up an LLM – choose between a paid LLM (OpenAI’s GPT) and a free LLM
    (from Hugging Face)
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置LLM – 在付费LLM（OpenAI的GPT）和免费LLM（来自Hugging Face）之间选择
- en: As explained above, we let you choose whether you want to run this example via
    a paid API by OpenAI or a free LLM.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们让你选择是否想要通过OpenAI的付费API或免费的LLM来运行这个例子。
- en: Remember, since OpenAI’s service includes hosting the LLM and processing the
    prompts, it requires minimal resources and time and a basic internet connection.
    It also involves sending our prompts to OpenAI’s API service. Prompts typically
    include information that, in real-world settings, may be proprietary. Thus, an
    executive decision needs to be made regarding the security of the data. Similar
    considerations were central, in the last decade, to the transition of companies’
    computation from on-premises to the cloud.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，由于OpenAI的服务包括托管LLM和处理提示，它只需要最少的资源和时间以及基本的互联网连接。这也涉及到将我们的提示发送到OpenAI的API服务。提示通常包含在现实世界设置中可能是专有信息。因此，需要做出关于数据安全的行政决策。在过去的十年中，公司从本地计算向云迁移时，类似的考虑是核心的。
- en: In contrast to that requirement, with a free LLM, you would host it locally,
    you would avoid exporting any information outside of your computation environment,
    but you would take on the processing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与该要求相反，使用免费的LLM，你将本地托管它，你将避免将任何信息导出至你的计算环境之外，但你需要承担处理工作。
- en: Another aspect to consider is the terms of use of each LLM, as each may have
    different license terms. While an LLM may allow you to experiment with it for
    free, it may present constrictions on whether you may use it in a commercial product.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的方面是每个LLM的使用条款，因为每个LLM可能有不同的许可条款。虽然LLM可能允许你免费实验，但它可能对你在商业产品中使用它施加限制。
- en: In the context of constraints around runtime and computational resources, choosing
    the paid LLM for this example will yield quicker responses.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时间和计算资源受限的背景下，选择付费LLM进行此示例将产生更快的响应。
- en: In order to accommodate your wish to experiment with a free LLM, and since we
    aspire to let you run the code quickly and for free on Google Colab, we must restrict
    our choice of LLMs to those that can be run on the limited RAM that Google lets
    us have with a free account. In order to do that, we chose an LLM with reduced
    precision, also known as a quantized LLM.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足你想要免费实验LLM的愿望，并且因为我们渴望让你在Google Colab上快速免费运行代码，我们必须将LLM的选择限制在那些可以在Google免费账户所提供的有限RAM上运行的LLM。为了做到这一点，我们选择了一个精度降低的LLM，也称为量化LLM。
- en: Based on your choice between an API-based LLM and a free local LLM, the LLM
    will be assigned to the `llm variable`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你选择基于API的LLM还是免费的本地LLM，LLM将被分配给`llm变量`。
- en: Creating a QA chain
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建一个QA链
- en: Here, we set up a RAG framework. It is designed to accept various text documents
    and set them up for retrieval.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了一个RAG框架。它被设计用来接受各种文本文档并将它们设置为检索。
- en: Search based on the same requirements when using the LLM as the “brain” instead
    of embedding similarity
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 当使用LLM作为“大脑”而不是嵌入相似性时，基于相同要求进行搜索。
- en: We will now run the exact same requests as we did in the example in [*Chapter
    8*](B18949_08.xhtml#_idTextAnchor440). Those will be performed across the same
    notes, and the same vector DB that holds the same embedding. None of that has
    been changed or enhanced. The difference is that we will have the LLM oversee
    the processing of the answers.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将运行与[*第8章*](B18949_08.xhtml#_idTextAnchor440)中的示例完全相同的请求。这些请求将在相同的笔记和相同的向量数据库上执行，该数据库包含相同的嵌入。这一切都没有改变或增强。区别在于我们将让LLM监督答案的处理。
- en: In [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), we saw that question number
    two received a wrong answer. The question was, “Are there any pregnant patients
    who are due to deliver in September?”
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中，我们看到了第二个问题得到了错误的答案。问题是：“是否有任何原定于9月出生的孕妇？”
- en: The answer we saw in [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440) was about
    a patient who is due to give birth in August. The mistake was due to the deficiency
    of the similarity algorithm. Indeed, that patient’s notes had content similar
    to that of the question, but the fine detail of giving birth in a different month
    should have been the factor that made those note irrelevant.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第8章*](B18949_08.xhtml#_idTextAnchor440)中看到的答案是关于一位原定于8月出生的病人。错误是由于相似性算法的不足。确实，那位病人的记录内容与问题相似，但分娩在不同月份的细微差别应该是使那些记录无关紧要的因素。
- en: Here, in our current pipeline, where OpenAI’s LLM is applied, it gets it right,
    telling us that there are no patients who are due to deliver in September.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，在我们的当前管道中，应用了OpenAI的LLM，它正确地告诉我们没有病人原定于9月出生。
- en: Note that when opting for the free LLM, it gets it wrong. This exemplifies the
    sub-optimal aspects of that model, as it is quantized in an effort to save on
    RAM requirements.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当选择免费 LLM 时，它可能会出错。这体现了该模型的次优方面，因为它被量化以节省 RAM 要求。
- en: To conclude this example, we have put together an in-house search mechanism
    that lets the user, in our example, a physician, search through their patients’
    notes to find patients based on some criteria. A unique aspect of this system
    design is the ability to let the LLM retrieve the relevant answer from an external
    data source and not be limited to the data it was trained on. This paradigm is
    the basis of RAG.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结这个例子，我们建立了一个内部搜索机制，允许用户（在我们的例子中，是一位医生）根据某些标准搜索他们的患者记录以找到患者。这个系统设计的独特之处在于让
    LLM 从外部数据源检索相关答案，而不仅限于它所训练的数据。这种范式是 RAG 的基础。
- en: In the next section, we will showcase more uses for LLMs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示 LLM 的更多用途。
- en: Advanced methods with chains
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带链的高级方法
- en: In this section, we will continue our exploration of ways one can utilize LLM
    pipelines. We will focus on chains.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将继续探索一个人可以利用 LLM 管道的方式。我们将重点关注链。
- en: 'Refer to the following notebook: `Ch9_Advanced_Methods_with_Chains.ipynb`.
    This notebook presents an evolution of a chain pipeline, as every iteration exemplifies
    another feature that LangChain allows us to employ.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下笔记本：`Ch9_Advanced_Methods_with_Chains.ipynb`。这个笔记本展示了一个链管道的演变，因为每一次迭代都展示了
    LangChain 允许我们使用的另一个功能。
- en: For the sake of using minimal computational resources, memory, and time, we
    use OpenAI’s API. You can choose to use a free LLM instead and may do so in a
    similar way to how we set up the notebook from the previous example in this chapter.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用最少的计算资源、内存和时间，我们使用了 OpenAI 的 API。您可以选择使用免费的 LLM，并且可以以与我们在本章前一个示例中设置笔记本类似的方式这样做。
- en: The notebook starts with the basic configurations, as always, so we can skip
    to reviewing the notebook’s content.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本总是从基本配置开始，因此我们可以跳过审查笔记本的内容。
- en: Asking the LLM a general knowledge question
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向 LLM 提出一个一般知识问题
- en: 'In this example, we want to use the LLM to tell us an answer to a simple question
    that would require common knowledge that a trained LLM is expected to have:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们希望使用 LLM 告诉我们一个简单问题的答案，这个问题需要训练有素的 LLM 应该拥有的常识：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We then define a simple chain called `LLMChain`, and we feed it with the `LLM`
    variable and the prompt.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了一个简单的链，称为 `LLMChain`，并给它提供 `LLM` 变量和提示。
- en: 'The LLM, indeed, knows the answer from its knowledge base and returns:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 确实知道答案，并从其知识库中返回：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Requesting output structure – making the LLM provide output in a particular
    data format
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 请求输出结构 – 使 LLM 以特定的数据格式提供输出
- en: 'This time, we would like the output to be in a particular syntax, potentially
    allowing us to use it in a computational manner for downstream tasks:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们希望输出具有特定的语法，这可能允许我们以计算方式使用它进行下游任务：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, we add a feature for achieving the syntax. We define the `output_parser
    variable`, and we use a different function for generating the output, `predict_and_parse()`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们添加了一个用于实现语法的功能。我们定义了 `output_parser 变量`，并使用不同的函数生成输出，`predict_and_parse()`。
- en: 'The output is the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Evolving to a fluent conversation – inserting an element of memory to have previous
    interactions as reference and context for follow-up prompts
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发展到流畅的对话 – 插入记忆元素，以便将先前交互作为后续提示的参考和上下文
- en: This feature brings a new level of value to the chain. Until this point, the
    prompts didn’t have any context. The LLM processed each prompt independently.
    For instance, if you wanted to ask a follow-up question, you couldn’t. The pipeline
    didn’t have your prior prompts and the responses to them as reference.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个功能为链带来了新的价值水平。到目前为止，提示没有上下文。LLM 独立处理每个提示。例如，如果你想提出一个后续问题，你做不到。管道没有你的先前提示及其响应作为参考。
- en: In order to go from asking disjointed questions to having an ongoing, rolling
    conversation-like experience, LangChain offers `ConversationChain()`. Within this
    function, we have a `memory` parameter that maps the prior interactions with the
    chain to the current prompt. Therefore, the prompt template is where that memory
    “lives.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从提出零散的问题转变为拥有持续的、滚动对话般的体验，LangChain 提供了 `ConversationChain()`。在这个函数中，我们有一个
    `memory` 参数，它将链的先前交互映射到当前提示。因此，提示模板就是内存“居住”的地方。
- en: Instead of prompting with a basic template, such as
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用基本模板进行提示，例如
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'the template now accommodates the memory feature:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模板已经适应了记忆功能：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, you can think of this string as being formatted similarly to a Python
    `f"…"` string, where `history` and `input` are string variables. The `ConversationChain()`
    function processes this prompt template and inserts these two variables to complete
    the prompt string. The `input` variable is produced by the function itself as
    we activate the memory mechanism, and the input variable is then supplied by us
    as we run the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以将这个字符串视为类似于Python `f"…"`字符串的格式，其中`history`和`input`是字符串变量。`ConversationChain()`函数处理这个提示模板，并插入这两个变量以完成提示字符串。`input`变量由函数本身生成，当我们激活记忆机制时，然后我们运行以下内容：
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Where the output is the following:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let’s make a follow-up request that would only be understood in the context
    of the previous request and output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们提出一个只有在前一个请求和输出上下文中才能理解的后续请求：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Indeed, we get the appropriate output:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，我们得到了适当的输出：
- en: '[PRE9]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To complete this example, let’s assume the intention we had was to quickly
    generate a table of some holidays that includes their names and descriptions:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个例子，让我们假设我们的意图是快速生成一个包含名称和描述的节日表格：
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now, we get a formatted string from the chain:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们从链中得到了一个格式化的字符串：
- en: '[PRE11]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can then use pandas to convert this string to a table:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用pandas将这个字符串转换为表格：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After pandas processes `dict` to be a DataFrame, we can observe it in *Table
    9.1*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas将`dict`处理为DataFrame之后，我们可以在*表9.1*中观察到它：
- en: '|  | **Name** | **Description** |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | **名称** | **描述** |'
- en: '| --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **0** | Christmas | Christmas is a Christian holiday that celebrates the
    birth of Jesus Christ. It is observed on December 25 each year. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 圣诞节 | 圣诞节是基督教节日，庆祝耶稣基督的诞生。每年12月25日庆祝。|'
- en: '| **1** | Thanksgiving | Thanksgiving is a holiday in which people gather together
    to express gratitude for the blessings in their lives. It is celebrated on the
    fourth Thursday in November in the United States. |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 感恩节 | 感恩节是一个人们聚集在一起表达对生活中祝福的感激之情的节日。在美国，感恩节庆祝于11月的第四个星期四。|'
- en: '| **2** | New Year’s Day | New Year’s Day marks the beginning of the Gregorian
    calendar year. It is celebrated on January 1 with various traditions and festivities.
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 新年 | 新年标志着格里高利日历年的开始。它通过各种传统和庆祝活动在1月1日庆祝。|'
- en: '| **3** | Easter | Easter is a Christian holiday that commemorates the resurrection
    of Jesus Christ from the dead. It is observed on the first Sunday following the
    first full moon after the vernal equinox. |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 复活节 | 复活节是基督教节日，纪念耶稣基督从死里复活。它是在春分后的第一个满月后的第一个星期日庆祝的。|'
- en: '| **4** | Valentine’s Day | Valentine’s Day is a day to celebrate love and
    affection. It is traditionally associated with romantic love, but it is also a
    time to express appreciation for friends and family. |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 情人节 | 情人节是庆祝爱情和亲情的日子。它传统上与浪漫爱情相关联，但也是表达对朋友和家人的感激之情的时候。|'
- en: '| **5** | St. Patrick’s Day | St. Patrick’s Day is a cultural and religious
    holiday that honors the patron saint of Ireland, St. Patrick. It is celebrated
    on March 17 with parades, wearing green, and other festive activities. |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 圣帕特里克节 | 圣帕特里克节是一个文化和宗教节日，纪念爱尔兰的守护圣人圣帕特里克。它在3月17日通过游行、穿绿色和其他节日活动来庆祝。|'
- en: Table 9.1 – pandas transformed the table from dict to a DataFrame, thus suiting
    down-stream processing
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.1 – pandas将表从字典转换为DataFrame，从而适合下游处理
- en: This concludes the various chain features that this notebook presents. Notice
    how we leveraged the features that both chains bring us and that LLMs bring us.
    For instance, while the memory and parsing features are completely handled on
    the chain’s side, the ability to present a response in a particular format, such
    as a JSON format, is solely accredited to the LLM.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了笔记本展示的各种链式功能。注意我们如何利用了这两个链和LLM带来的功能。例如，虽然记忆和解析功能完全由链的方面处理，但以特定格式（如JSON格式）呈现响应的能力则完全归功于LLM。
- en: In our next example, we will continue to present novel utilities with LLMs and
    LangChain.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个例子中，我们将继续使用LLM和LangChain展示新的实用工具。
- en: Retrieving information from various web sources automatically
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从各种网络来源自动检索信息
- en: In this example, we will review how simple it is to leverage LLMs to access
    the web and extract information. We may wish to research a particular topic, and
    so we would like to consolidate all the information from a few web pages, several
    YouTube videos that present that topic, and so on. Such an endeavor can take a
    while, as the content may be massive. For instance, several YouTube videos can
    sometimes take hours to review. Often, one doesn’t know how useful the video is
    until one has watched a significant portion of it.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将回顾如何简单地利用LLMs（大型语言模型）访问网络并提取信息。我们可能希望研究某个特定主题，因此我们希望从几个网页、几个介绍该主题的YouTube视频等中整合所有信息。这样的努力可能需要一段时间，因为内容可能非常庞大。例如，有时几个YouTube视频可能需要数小时才能审查。通常，一个人只有在观看视频的很大一部分后，才知道视频有多有用。
- en: Another use case is when looking to track various trends in real time. This
    may include tracking news sources, YouTube videos, and so on. Here, speed is crucial.
    Unlike the previous example where speed was important to save us personal time,
    here, speed is necessary for getting our algorithm to be relevant for identifying
    real-time emerging trends.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用例是当需要实时跟踪各种趋势时。这可能包括跟踪新闻来源、YouTube视频等。在这里，速度至关重要。与之前示例中速度重要是为了节省我们个人时间不同，在这里，速度是使我们的算法对识别实时新兴趋势具有相关性的必要条件。
- en: In this section, we put together a very simple and limited example.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个非常简单且有限的示例。
- en: Retrieving content from a YouTube video and summarizing it
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从YouTube视频中检索内容并进行总结
- en: 'Refer to the following notebook: `Ch9_Retrieve_Content_from_a_YouTube_Video_and_Summarize.ipynb`
    ([https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)).
    We will build our application on a library called EmbedChain ([https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)).
    EmbedChain leverages a RAG framework and enhances it by allowing the vector database
    to include information from various web sources.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下笔记本：`Ch9_Retrieve_Content_from_a_YouTube_Video_and_Summarize.ipynb` ([https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain))。我们将基于名为EmbedChain的库（[https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)）构建我们的应用程序。EmbedChain利用RAG框架，并通过允许向量数据库包含来自各种网络来源的信息来增强它。
- en: 'In our example, we will choose a particular YouTube video (*Robert Waldinger:
    What makes a good life? Lessons from the longest study on happiness | TED*: [https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED](https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED)).
    We would like the content of that video to be processed into the RAG framework.
    Then, we will prompt an LLM with questions and tasks related to the content of
    that video, thus allowing us to extract everything we care to learn about the
    video without having to watch it.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的例子中，我们将选择一个特定的YouTube视频（*Robert Waldinger: What makes a good life? Lessons
    from the longest study on happiness | TED*：[https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED](https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED)）。我们希望将该视频的内容处理成RAG框架。然后，我们将向LLM提出与该视频内容相关的疑问和任务，这样我们就可以提取我们想要了解的所有关于视频的信息，而无需观看它。'
- en: It should be stressed that a key feature that this method relies on is that
    YouTube accompanies many of its verbal videos with a written transcript. This
    makes the importing of the video’s text context seamless. If, however, one wishes
    to apply this method to a video that isn’t accompanied by a transcript, this is
    not a problem. One would need to pick a speech-to-text model, many of which are
    free and of very high quality. The audio of the video would be processed, a transcript
    would be extracted, and you may then import it into the RAG process.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 应该强调的是，这个方法依赖的一个关键特性是YouTube为其许多口头视频提供了书面字幕。这使得视频文本上下文的导入变得无缝。然而，如果有人希望将此方法应用于没有字幕的视频，这并不是问题。一个人需要选择一个语音转文字模型，其中许多是免费且质量极高的。视频的音频将被处理，提取出字幕，然后你可以将其导入到RAG（Retrieval-Augmented
    Generation，检索增强生成）过程中。
- en: Installs, imports, and settings
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装、导入和设置
- en: As with previous notebooks, here too, we install the necessary packages, import
    all the relevant packages, and set our OpenAI API key.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的笔记本一样，我们安装必要的包，导入所有相关包，并设置我们的OpenAI API密钥。
- en: 'We then do the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要做以下几步：
- en: Make our choice of model.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们的模型。
- en: Choose an embedding model that will serve the RAG’s vector database feature.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个将服务于RAG向量数据库功能的嵌入模型。
- en: Choose a prompting LLM. Notice how you can set up further parameters that control
    the model’s output, such as the maximal number of returned tokens or the temperature.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个提示LLM。注意你可以设置更多控制模型输出的参数，例如返回的最大令牌数或温度。
- en: Pick the YouTube video to which you would like to apply this code and set a
    string variable using the video’s URL.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你想要应用此代码的YouTube视频，并使用视频的URL设置一个字符串变量。
- en: Setting up the retrieval mechanism
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置检索机制
- en: We need to set EmbedChain’s RAG process. We specify that we are passing a path
    to a YouTube video, and we provide the video’s URL.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要设置EmbedChain的RAG过程。我们指定我们正在传递一个YouTube视频的路径，并提供视频的URL。
- en: We can then print out the text that was fetched and verify that it is, indeed,
    aligned with the video we are looking to analyze.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以打印出获取的文本，并验证它确实与我们要分析的视频对齐。
- en: Reviewing, summarizing, and translating
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审阅、总结和翻译
- en: We will now observe the value that this code yields.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将观察此代码产生的价值。
- en: 'We ask the LLM to review the content, to put together a summary, and to present
    that summary in English, Russian, and German:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求LLM审阅内容，整理一个总结，并以英语、俄语和德语呈现该总结：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The returned output is spot on, as it completely captures the essence of the
    TED talk. We edit it to remove the delimiter strings and get:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的输出非常准确，因为它完全捕捉了TED演讲的精髓。我们编辑它以移除分隔字符串，得到：
- en: '[PRE14]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now, to make the content simple for, say, a German speaker, we ask the LLM to
    form the German summary into several bullet points that best describe the content
    of the video.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了使内容对德语使用者来说简单易懂，我们要求LLM将德语总结形成几个最佳描述视频内容的要点。
- en: 'It does this well, and the outputs are as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 它做得很好，输出如下：
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: While this code is meant to serve as a basic proof of concept, one can see how
    simple it would be to add more data sources, automate it to run constantly, and
    act based on the findings. While a readable summary is helpful, one could change
    the code to act based on the identified content and execute downstream applications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然此代码旨在作为一个基本的证明概念，但人们可以看到如何简单地向其中添加更多数据源，自动化它以持续运行，并根据发现采取行动。虽然一个可读的总结很有帮助，但人们可以将代码修改为根据确定的内容采取行动并执行下游应用。
- en: Now that we have observed several capabilities that LLMs can perform, we can
    take a step back and refine the way we utilize those LLMs. In our next section,
    we will exemplify how one may reduce LLM processing, thus saving API costs, or,
    when employing a local LLM, reducing inference computation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经观察到了LLMs可以执行的一些功能，我们可以退后一步，改进我们利用这些LLMs的方式。在下一节中，我们将举例说明如何减少LLM处理，从而节省API成本，或者当使用本地LLM时，减少推理计算。
- en: Prompt compression and API cost reduction
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示压缩和API成本降低
- en: This part is dedicated to a recent development in resource optimization for
    when employing API-based LLMs, such as OpenAI’s services. When considering the
    many trade-offs between employing a remote LLM as a service and hosting an LLM
    locally, one key metric is cost. In particular, based on the application and usage,
    the API costs can accumulate to a significant amount. API costs are mainly driven
    by the number of tokens that are being sent to and from the LLM service.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分专门介绍在采用基于API的LLMs（如OpenAI的服务）进行资源优化方面的最新进展。在考虑使用远程LLM作为服务与本地托管LLM之间的许多权衡时，一个关键指标是成本。特别是，根据应用和用途，API成本可能会累积到相当大的数额。API成本主要受发送到和从LLM服务发送的令牌数量驱动。
- en: In order to illustrate the significance of this payment model on a business
    plan, consider business units for which the product or service relies on API calls
    to OpenAI’s GPT, where OpenAI serves as a third-party vendor. As a particular
    example, imagine a social network that lets its users have LLM assistance to comment
    on posts. In that use case, a user is interested in commenting on a post, and
    instead of having to write a complete comment, a feature lets the user describe
    their feelings about the post in three–five words, and a backend process augments
    a full comment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种支付模式在商业计划中的重要性，考虑那些产品或服务依赖于API调用OpenAI的GPT的业务单元，其中OpenAI作为第三方供应商。作为一个特定例子，想象一个允许用户通过LLM助手评论帖子的社交网络。在这种情况下，用户想要评论一个帖子，而不是写一个完整的评论，一个功能允许用户用三到五个词描述他们对帖子的感受，而后端过程则增加一个完整的评论。
- en: In this particular example, the engine collects the user’s three–five words,
    and it also collects the content of the post that the comment is meant for, meaning
    it will also collect all other relevant information that the social network’s
    experts would think is relevant for augmenting a comment. For instance, the user’s
    profile description, their past few comments, and so on.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，引擎收集用户的三个到五个单词，它还收集评论针对的帖子的内容，这意味着它还会收集社交网络专家认为与增强评论相关的所有其他相关信息。例如，用户的个人资料描述，他们过去的几条评论等等。
- en: This would mean that every time a user wishes to have a comment augmented, a
    detailed prompt is sent from the social network’s servers to the third party’s
    LLM via theAPI.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着每次用户希望对评论进行增强时，都会从社交网络的服务器通过API发送一个详细的提示。
- en: Now, this type of process can accumulate high costs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种类型的流程可能会积累很高的成本。
- en: In this section, we will analyze an approach to reducing this cost by reducing
    the number of tokens sent to the LLM through the API. The basic assumption is
    that one can always reduce the number of words sent to the LLM and, thus, reduce
    cost, but the reduction in performance could be significant. Our motivation is
    to reduce that amount while maintaining high-quality performance. We then asked
    if only the “right” words could be sent, ignoring other “non-material” words.
    This notion reminds us of the concept of file compression, where a smart and tailored
    algorithm is employed to reduce the size a file takes while maintaining its purpose
    and value.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析一种通过减少通过API发送给LLM的令牌数量来降低这种成本的方法。基本假设是，人们总是可以减少发送给LLM的单词数量，从而降低成本，但性能的降低可能是显著的。我们的动机是在保持高质量性能的同时减少这种数量。然后我们询问是否只发送“正确”的单词，忽略其他“非重要”的单词。这个概念让我们想起了文件压缩的概念，其中使用了一种智能且定制的算法来减少文件的大小，同时保持其目的和价值。
- en: Prompt compression
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示压缩
- en: Here, we introduce **LLMLingua**, a development by Microsoft that is meant to
    address prompts that are “sparse” in information by compressing them.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍**LLMLingua**，这是微软开发的一个旨在通过压缩来处理“信息稀疏”的提示的开发工具。
- en: LLMLingua utilizes a compact, well-trained language model, such as LLaMA-7B,
    to identify and remove non-essential tokens within prompts. This approach enables
    efficient inference with LLMs, achieving up to 20x compression with minimal performance
    loss ([https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua)).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: LLMLingua利用一个紧凑、训练有素的语料库模型，如LLaMA-7B，来识别和删除提示中的非必要令牌。这种方法使得使用LLM进行推理变得高效，实现了高达20倍压缩且性能损失最小（[https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua)）。
- en: In their papers ([https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736)
    and https://arxiv.org/abs/2310.06839), the authors explain the algorithm and the
    advantages it proposes. It is interesting to note that besides the reduction in
    cost, the compression also aims to focus the remaining content, which is shown
    by the authors to lead to an improvement in performance by the LLM, as it avoids
    a sparse and noisy prompt.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的论文（[https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736)
    和 https://arxiv.org/abs/2310.06839）中，作者解释了该算法及其提出的优势。值得注意的是，除了成本降低外，压缩还旨在聚焦剩余内容，作者指出这可以通过LLM的性能提升来实现，因为它避免了稀疏和嘈杂的提示。
- en: Let’s experiment with prompt compression in a real-world example and evaluate
    its impact and various trade-offs.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一个真实世界的例子中尝试提示压缩，并评估其影响和各种权衡。
- en: Experimenting with prompt compression and evaluating trade-offs
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尝试提示压缩并评估权衡
- en: For the sake of this experiment, we'll illustrate a real-world example.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个实验，我们将展示一个真实世界的例子。
- en: In our current use case, we are developing a feature that sits on top of a database
    of academic publications. The feature allows the user to pick a specific publication
    and ask questions about it. A backend engine evaluates the question, reviews the
    publication, and derives an answer.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的使用案例中，我们正在开发一个位于学术出版物数据库之上的功能。该功能允许用户选择特定的出版物并对其提出问题。后端引擎评估问题，审查出版物，并得出答案。
- en: 'To narrow down the scope of the feature for the sake of putting together a
    series of experiments, the publications are from the particular category of AI
    publications, and the question that the user asks is the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缩小该功能的范围以便进行一系列实验，出版物来自特定的AI出版物类别，用户提出的问题是以下内容：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This question requires a deep and insightful review of each publication, as
    there are cases where a publication discusses a novel algorithm where the term
    reinforcement learning isn’t explicitly mentioned at any point in the publication,
    yet the feature is expected to infer from the description of the algorithm whether
    it indeed leverages the concepts of reinforcement learning and flag it as such.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题需要对每一篇出版物进行深入和有洞察力的审查，因为在某些情况下，一篇出版物讨论了一种新颖的算法，其中在出版物的任何地方都没有明确提到强化学习这个术语，但根据算法的描述，可以推断出它确实利用了强化学习的概念，并将其标记为强化学习。
- en: 'Refer to the following notebook: `Ch9_RAGLlamaIndex_Prompt_Compression.ipynb`.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下笔记本：`Ch9_RAGLlamaIndex_Prompt_Compression.ipynb`。
- en: In this code, we run a set of experiments, each per the above feature description.
    Each experiment is in the form of a full, end-to-end RAG task. While we employed
    LangChain in the previous RAG examples, here, we introduce LlamaIndex. LlamaIndex
    is an open source Python library that employs a RAG framework ([https://docs.llamaindex.ai/en/stable/index.html](https://docs.llamaindex.ai/en/stable/index.html)).
    LlamaIndex is similar to LangChain in that way.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码中，我们运行了一系列实验，每个实验都按照上述特征描述进行。每个实验都是以一个完整的、端到端的RAG（Retrieval-Augmented Generation）任务的形式进行的。虽然我们在之前的RAG示例中使用了LangChain，但在这里，我们引入了LlamaIndex。LlamaIndex是一个开源的Python库，它采用了一个RAG框架（[https://docs.llamaindex.ai/en/stable/index.html](https://docs.llamaindex.ai/en/stable/index.html)）。LlamaIndex在这一点上与LangChain相似。
- en: The LLMLingua code stack that the folks at Microsoft put together is integrated
    with LlamaIndex.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 微软团队整合的LLMLingua代码栈与LlamaIndex集成。
- en: Let’s review the code in detail.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细审查代码。
- en: Code settings
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代码设置
- en: 'Similar to the previous notebooks, here too, we set the initial settings with
    the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的笔记本类似，这里我们也使用以下方式设置了初始设置：
- en: In this code section, we start by defining some key variables.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个代码部分，我们首先定义了一些关键变量。
- en: We set the number of experiments that we want to run. We want to make sure we
    choose a number that is large enough so as to get a good statistical representation
    of the impact that the compression has.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了想要运行的实验数量。我们想要确保选择一个足够大的数字，以便能够得到压缩影响的良好统计代表性。
- en: We set the top-k, which is the number of chunks to be retrieved by the RAG framework
    for prompt context.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了top-k，这是RAG框架为提示上下文检索的块的数量。
- en: We predefined the target number of the token we would like the compression to
    reduce.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们预先定义了希望压缩减少的目标令牌数量。
- en: Finally, as in the previous code, we set our OpenAI API key.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，就像之前的代码一样，我们设置了我们的OpenAI API密钥。
- en: We take this opportunity to stress that some of the parameters in this evaluation
    were fixed for the sake of limiting its complexity and keeping it appropriate
    for educational purposes. When conducting such an evaluation in business or academic
    settings, there should be either qualitative or quantitative reasoning for the
    value chosen. Qualitative may be of the form “We shell fix the desired reduction
    to 999 tokens due to budget constraints," whereas quantitative may seek to not
    fix it but rather optimize it as a part of the other trade-offs. In our case,
    we fixed this particular parameter to a value that was found to allow for an impressive
    compression rate while maintaining a decent agreement rate between the two evaluated
    approaches. Another example was the number of experiments we chose, which was
    a trade-off between runtime, GPU memory allocation, and statistical power.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们借此机会强调，在这个评估中，一些参数被固定是为了限制其复杂性，并使其适合教育目的。在商业或学术环境中进行此类评估时，应该有定性的或定量的推理来支持所选择的价值。定性可能的形式是“由于预算限制，我们将把期望的减少量固定为999个令牌”，而定量的可能寻求不固定它，而是将其作为其他权衡的一部分进行优化。在我们的案例中，我们将这个特定的参数固定为一个值，这个值被发现可以在保持两个评估方法之间良好的协议率的同时，实现令人印象深刻的压缩率。另一个例子是我们选择的实验数量，这是在运行时间、GPU内存分配和统计功效之间的权衡。
- en: Gathering the data
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集数据
- en: We need to gather the dataset of the publications, and we also filter it so
    as to be left with only the limited cohort of publications that are in the AI
    category.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要收集出版物的数据集，并且我们也对其进行筛选，以便只保留属于AI类别的有限出版物群体。
- en: LLM configurations
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM配置
- en: Here, we set the ground for the two LLMs we will be employing.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们为将要使用的两个LLM（大型语言模型）奠定了基础。
- en: The compression method, LLMLingua, employs Llama2 as the compressing LLM. It
    will obtain the context retrieved by the LlamaIndex RAG pipeline, the user’s question,
    and it will compress and reduce the size of the context content.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩方法LLMLingua使用Llama2作为压缩LLM。它将获得LlamaIndex RAG管道检索到的上下文、用户的问题，并将上下文内容的大小压缩和减少。
- en: OpenAI’s GPT is to be used as the downstream LLM for prompting, meaning it will
    obtain the question about reinforcement learning and the additional relevant context
    and return an answer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT将被用作提示的下游LLM，这意味着它将获得关于强化学习的问题以及额外的相关上下文，并返回一个答案。
- en: Additionally, here, we define the user’s question. Note that we added instructions
    for OpenAI’s GPT on how to present the answer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在这里，我们定义了用户的问题。请注意，我们为OpenAI的GPT添加了如何呈现答案的说明。
- en: The experiments
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验
- en: 'This is the core of the notebook. A `for` loop iterates over the various experiments.
    In each iteration, two scenarios are evaluate:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是笔记本的核心。一个`for`循环遍历各种实验。在每次迭代中，评估两种场景：
- en: '**First scenario**: an ordinary RAG task is deployed where the context is retrieved
    without being compressed. The prompt is comprised of the retrieved context and
    the user’s question, and the answer that the LLM returns is recorded along with
    the number of sent tokens and the processing time.'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**第一种场景**：部署了一个普通的RAG任务，其中上下文在未压缩的情况下被检索。提示由检索到的上下文和用户的问题组成，记录LLM返回的答案以及发送的标记数和处理时间。'
- en: '**Second scenario**: LLMLingua is employed. The retrieved context is compressed.
    The compressed context is sent to the LLM along with the user’s question. Again,
    the returned answer is recorded along with the number of sent tokens and the processing
    time.'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**第二种场景**：使用LLMLingua。检索到的上下文被压缩。压缩后的上下文连同用户的问题一起发送给LLM。再次，记录返回的答案以及发送的标记数和处理时间。'
- en: When this code cell is completed, we have a dictionary, `record`, that holds
    the relevant values for each iteration that will be used to aggregate and derive
    conclusions.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个代码单元完成时，我们有一个字典`record`，它保存了每个迭代的有关值，这些值将被用于汇总和得出结论。
- en: Analyzing the impact of context compression – a reduction in classification
    performance versus an increase in resource efficiency
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析上下文压缩的影响——分类性能的降低与资源效率的提升
- en: 'Here, we sum up the values of the experiments and deduce what impact the prompt
    compression has on the performance of the LLM, the processing time, and the cost
    of the API:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总结了实验的数值，并推断出提示压缩对LLM性能、处理时间和API成本的影响：
- en: We found that the reduction in the context length had yielded an agreement rate
    of 92%.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现，上下文长度的减少产生了92%的一致率。
- en: We found that the process of compression had extended the processing time by
    11 times.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现，压缩过程将处理时间延长了11倍。
- en: We found that the reduction in the context length saved 92% of the total cost
    of sent tokens!
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们发现，上下文长度的减少节省了发送标记总成本的92%！
- en: Note that cost reduction is negatively dependent on the agreement rate, as we
    expect an increase in cost savings to reduce the agreement rate.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，成本降低与一致率呈负相关，因为我们预计成本节省的增加将降低一致率。
- en: This reduction is significant and, in some cases, may tilt the scale from a
    loss-making service to a profitable service.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这种降低是显著的，在某些情况下，甚至可能将服务从亏损转变为盈利。
- en: Here are some notes to keep in mind regarding the meaning of a disagreement
    and additional trade-offs. Regarding the drop in agreement rate between the two
    approaches, while an agreement between the two approaches insinuates that they
    are both correct, a disagreement could go either way. It could be that in the
    second scenario, the compression distorted the context and, thus, made the model
    unable to properly classify it. However, the opposite may be true, as the compression
    may have reduced the irrelevant content and made the LLM focus on the relevant
    aspects of the content, thus making the scenario with the compressed context yield
    a correct answer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些关于不一致的意义和额外权衡的注意事项。关于两种方法之间的一致率下降，虽然两种方法之间的一致性暗示它们都是正确的，但不一致可能有两种情况。可能是第二种场景中，压缩扭曲了上下文，因此模型无法正确分类它。然而，情况可能相反，因为压缩可能减少了无关内容，使LLM专注于内容的相关方面，从而使压缩上下文的场景得出正确答案。
- en: Regarding additional trade-offs, the above metrics of LLM performance, processing
    time, and API cost don’t reveal additional considerations such as the computational
    resources that the compression requires. The local compressing LLM, in our case,
    Llama2, requires local hosting and local GPUs. These are non-trivial resources
    that don’t exist on an ordinary laptop. Remember the original approach, i.e.,
    the first scenario, does not require those. An ordinary RAG approach can perform
    embeddings using either a smaller LM, such as one that is BERT-based, or even
    an API-based embedding. The prompted LLM, under our original assumption, is chosen
    to be remote and API-based, thus enabling the deployment environment to have minimal
    computation resources, like a common laptop would provide.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 关于额外的权衡，上述LLM性能、处理时间和API成本的指标并没有揭示出额外的考虑因素，例如压缩所需的计算资源。在我们的案例中，本地的压缩LLM（Llama2）需要本地托管和本地GPU。这些是非平凡的资源，普通笔记本电脑上并不存在。记住，原始方法，即第一种场景，并不需要这些。普通的RAG方法可以使用较小的LM进行嵌入，例如基于BERT的LM，或者甚至基于API的嵌入。根据我们的原始假设，提示LLM被选为远程和基于API的，从而使得部署环境具有最少的计算资源，就像普通笔记本电脑提供的资源一样。
- en: This evaluation proves that the LLMLingua prompt compression method is very
    impactful and useful as a means of cost reduction.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这一评估证明，LLMLingua提示压缩方法作为成本降低手段非常具有影响力和实用性。
- en: In the next and last code demonstration of this chapter, we will continue to
    observe the results of this experience, and we will do so by forming a team of
    experts, each played by an LLM, so as to enhance the process of deriving a conclusion
    to the analysis.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章接下来的最后一个代码演示中，我们将继续观察这一经验的结果，我们将通过组建一个由专家组成的团队来实现，每个专家由一个LLM扮演，以增强得出分析结论的过程。
- en: Multiple agents – forming a team of LLMs that collaborate
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多个代理——形成一个LLM团队进行协作
- en: This section deals with one of the most exciting recent methods in the world
    of LLMs, employing multiple LLMs simultaneously. In the context of this section,
    we seek to define multiple agents, each backed by an LLM and given a different
    designated role to play. Instead of the user working directly with the LLM, as
    we see in ChatGPT, here, the user sets up multiple LLMs and sets their role by
    defining a different system prompt for each of them.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了LLM领域中最近最激动人心的方法之一，即同时使用多个LLM。在本节的背景下，我们试图定义多个代理，每个代理都由一个LLM支持，并赋予不同的指定角色。与我们在ChatGPT中看到的用户直接与LLM工作不同，在这里，用户设置了多个LLM，并为每个LLM定义了不同的系统提示来设定它们的角色。
- en: Potential advantages of multiple LLM agents working simultaneously
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个LLM代理同时工作的潜在优势
- en: Much like with people working together, here too, we see the advantages of employing
    several LLMs simultaneously.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 就像人们一起工作时一样，这里我们也看到了同时使用几个LLM的优势。
- en: 'Some advantages are the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一些优势如下：
- en: '**Enhancing validation and reducing hallucinations**: It has been shown that
    when providing feedback to an LLM and asking it to reason or to check its response,
    the reliability of its response improves. When designating roles for the various
    LLM agents on a team, the system prompt of at least one of them may include the
    requirement to criticize and validate the answers.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强验证和减少幻觉**：研究表明，当向大型语言模型（LLM）提供反馈并要求其进行推理或检查其回答时，其回答的可靠性会提高。在为团队中的各种LLM代理指定角色时，至少其中一方的系统提示可能包括批评和验证答案的要求。'
- en: '**Allowing the person to be involved as much or as little as they want in the
    process**: When designating the various roles, the user may insert themselves
    into the team, such that when it is the user’s turn to participate in the conversation,
    the rest of the agents wait while the user enters their input. However, if desired,
    the user may remove themselves altogether and just let the LLMs work automatically.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许个人根据意愿参与或退出过程**：在指定各种角色时，用户可以将自己插入团队中，这样当轮到用户参与对话时，其他代理会等待用户输入。然而，如果用户希望的话，他们也可以完全退出，让LLM自动工作。'
- en: In the following examples, we will see examples of the latter.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将看到后者的示例。
- en: '**Allowing different LLM models to be best utilized**: Today, we have several
    leading LLMs available. Some are free and hosted locally, and some are API-based.
    They are different in their size and capabilities, and some of them are stronger
    in particular tasks than others. When forming a team of agents where each agent
    is assigned a different role, a different LLM may be set that best suits that
    role. For instance, in the context of a coding project, where one of the agents
    is a programmer of a particular coding language, the user may choose to set the
    LLM for that agent to be an LLM that is superior for code generation in that particular
    coding language.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**允许不同的大型语言模型（LLM）得到最佳利用**：如今，我们有几种领先的LLM可供选择。有些是免费的，本地托管，有些是基于API的。它们在大小和能力上有所不同，其中一些在特定任务上比其他LLM更强。当组建一个每个代理都分配不同角色的团队时，可能需要设置一个最适合该角色的不同LLM。例如，在编码项目的背景下，其中一位代理是特定编程语言的程序员，用户可以选择为该代理设置一个在该特定编程语言中代码生成能力更强的LLM。'
- en: '**Optimizing resources – employing several smaller LLMs**: Imagine a project
    that involves technical functions and also domain expertise. For example, building
    a user platform in the medical space. You would want there to be a frontend engineer,
    a backend engineer, a designer, and a medical expert, all governed by the project
    manager and the product manager. If you were to develop this platform using the
    multiple-agents framework, you would define the agents, assign the various roles
    to them, and pick an LLM to drive them. If you were to use the same LLM for all
    of the agents, say, OpenAI’s most recent GPT, then that model would have to be
    very generic, thus requiring it to be very large and perhaps very expensive and
    maybe even slow. However, if you had access to individual LLMs, each pre-trained
    to only fulfill a limited function, for instance, one LLM dedicated to the medical
    service domain and a different LLM dedicated to backend development in Python,
    then you would assign each of those particular LLMs to their corresponding agents.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化资源 – 使用多个较小的LLM**：想象一个涉及技术功能和领域专业知识的项目。例如，在医疗领域构建一个用户平台。你希望有一个前端工程师、后端工程师、设计师和医学专家，所有这些都由项目经理和产品经理管理。如果你要使用多代理框架开发这个平台，你会定义代理，将各种角色分配给他们，并选择一个LLM来驱动他们。如果你要为所有代理使用相同的LLM，比如说OpenAI最新的GPT，那么这个模型将必须非常通用，因此可能需要非常大、可能非常昂贵，甚至可能很慢。然而，如果你可以访问到每个都预先训练以仅执行有限功能的单个LLM，例如，一个专注于医疗服务领域的LLM和一个专注于Python后端开发的LLM，那么你将把每个特定的LLM分配给相应的代理。'
- en: That may present a major reduction in model size, as the combined architecture
    of several specialized LLMs may be smaller in size than the architecture of one
    generic LLM when assuming equal performance between the two scenarios.
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可能显著减少模型大小，因为几个专业LLM的组合架构可能比一个通用LLM的架构更小，假设两种情况下的性能相同。
- en: '**Optimizing resources – optimal designation of a single LLM**: A unique and
    particular case of employing multiple LLMs is that in which we are seeking to
    optimize the LLM being chosen per the current task. This case is different than
    all the above, as it does not refer to a case where several LLMs are working simultaneously.
    In this case, a routing algorithm chooses one LLM based on a current state of
    constraints and variables. These may include the following:'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化资源 – 单一LLM的最佳分配**：使用多个LLM的一个独特且特定的情况是我们寻求根据当前任务优化所选择的LLM。这种情况与上述所有情况都不同，因为它不涉及多个LLM同时工作的情况。在这种情况下，一个路由算法根据当前约束和变量的状态选择一个LLM。这些可能包括以下内容：'
- en: The current load on each of the different parts of the computation system
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算系统各部分的当前负载
- en: The cost constraints, which may vary over time
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本限制，这些可能随时间变化
- en: The source of the prompt, as different clients/regions may have different priorities
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示的来源，因为不同的客户/地区可能有不同的优先级
- en: The purpose of the prompt, as different use cases may be prioritized by the
    business
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示的目的，因为不同的业务场景可能被赋予不同的优先级
- en: The prompt’s requirements, as a code-generating task, may be obtaining excellent
    responses with a small and efficient code-generating LLM, whereas a request to
    review a legal document and to suggest precedents may call for a completely different
    model
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示的要求，作为一个代码生成任务，可能需要使用一个小型且高效的代码生成LLM来获得出色的响应，而审查法律文件并提出先例的建议可能需要完全不同的模型
- en: AutoGen
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AutoGen
- en: 'The particular framework we employ in this section is called AutoGen, and it
    is made available by Microsoft (GitHub repo: [https://github.com/microsoft/autogen/tree/main](https://github.com/microsoft/autogen/tree/main)).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中使用的特定框架称为AutoGen，由微软提供（GitHub仓库：[https://github.com/microsoft/autogen/tree/main](https://github.com/microsoft/autogen/tree/main)）。
- en: '*Figure 9**.1* conveys the AutoGen framework. The following was obtained from
    the statement made in the GitHub repo:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*图9**.1* 展示了AutoGen框架。以下内容来自GitHub仓库中的说明：'
- en: '*AutoGen is a framework that enables the development of LLM applications using
    multiple agents that can converse with each other to solve tasks. AutoGen agents
    are customizable, conversable, and seamlessly allow human participation. They
    can operate in various modes that employ combinations of LLMs, human inputs,*
    *and tools*.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*AutoGen是一个框架，它使开发者能够使用能够相互对话以解决任务的多个代理来开发LLM应用。AutoGen代理是可定制的、可对话的，并允许无缝地参与人类。它们可以在各种模式下运行，这些模式结合了LLM、人类输入和*工具*。'
- en: '![Figure 9.1 – AutoGen functionality](img/B18949_09_1.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – AutoGen功能](img/B18949_09_1.jpg)'
- en: Figure 9.1 – AutoGen functionality
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – AutoGen功能
- en: On the left of *Figure 9**.1*, we observe the designation of roles and capabilities
    to individual agents; on the right, we observe a few of the conversation structures
    that are available.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9**.1* 的左侧，我们观察到为个体代理指定的角色和能力；在右侧，我们观察到一些可用的对话结构。
- en: 'AutoGen’s key capabilities as presented in the code repo:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGen在代码仓库中展示的关键功能：
- en: AutoGen enables building next-gen LLM applications based on multi-agent conversations
    with minimal effort. It simplifies the orchestration, automation, and optimization
    of a complex LLM workflow. It maximizes the performance of LLM models and overcomes
    their weaknesses.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoGen通过最小努力使基于多代理对话的下一代LLM应用成为可能。它简化了复杂LLM工作流程的编排、自动化和优化。它最大化了LLM模型的表现并克服了它们的弱点。
- en: It supports diverse conversation patterns for complex workflows. With customizable
    and conversable agents, developers can use AutoGen to build a wide range of conversation
    patterns concerning conversation autonomy, the number of agents, and agent conversation
    topology.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持复杂工作流程中的多样化对话模式。通过可定制和可对话的代理，开发者可以使用AutoGen构建关于对话自主性、代理数量和代理对话拓扑的广泛对话模式。
- en: It provides a collection of working systems with different complexities. These
    systems span a wide range of applications from various domains and complexities.
    This demonstrates how AutoGen can easily support diverse conversation patterns.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一系列具有不同复杂度的运行系统。这些系统涵盖了从各个领域和复杂度范围内的广泛应用。这展示了AutoGen如何轻松支持多样化的对话模式。
- en: AutoGen provides enhanced LLM inference. It offers utilities such as API unification
    and caching, and advanced usage patterns such as error handling, multi-config
    inference, context programming, and so on.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoGen提供了增强型LLM推理。它提供了API统一和缓存等实用工具，以及错误处理、多配置推理、上下文编程等高级使用模式。
- en: AutoGen is powered by collaborative research studies from Microsoft, Penn State
    University, and the University of Washington.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: AutoGen由微软、宾夕法尼亚州立大学和华盛顿大学的研究合作研究支持。
- en: Next, we can dive into a practical example in the code.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以深入到代码中的实际示例。
- en: Completing a complex analysis – visualizing the results and forming a conclusion
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 完成复杂分析——可视化结果并得出结论
- en: 'Here, we will show how a team of multiple agents, each with a different designated
    role, could serve as a professional team. The use case we chose is a continuation
    of the previous code we ran. In the last code, we performed a complex evaluation
    of employing prompt compression, and when that code finished, we had two resulting
    items: the `dict` that holds the numeric measurements of the experiments, called
    `record,` and the verbal statements about the resulting agreement rate, the reduction
    in tokens and cost, and the change in processing time.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将展示一个由多个代理组成的团队，每个代理都有不同的指定角色，如何作为一个专业团队提供服务。我们选择的用例是之前运行代码的延续。在上次代码中，我们执行了对使用提示压缩的复杂评估，当该代码完成后，我们得到了两个结果项：一个包含实验的数值测量的`dict`，称为`record`，以及关于结果协议率、令牌和成本的减少以及处理时间的变化的口头陈述。
- en: With that previous notebook, we intentionally stopped short. We didn’t visualize
    the reduction in tokens and cost, and we didn’t form an opinion as to whether
    we would advocate for employing the prompt reductions. However, in business or
    academic settings, one would be required to offer both. When you present your
    findings to stakeholders, decision-makers, or the research community, you are
    expected, when feasible, to visualize the statistical significance of the experiments.
    As a subject expert in NLP and ML, you are also expected to provide your recommendation
    on whether to adopt the experimented method or not.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的笔记本中，我们故意停了下来。我们没有可视化令牌和成本的减少，也没有形成关于是否倡导使用提示减少的观点。然而，在商业或学术环境中，人们需要提供这两者。当你向利益相关者、决策者或研究社区展示你的发现时，你期望在可行的情况下，可视化实验的统计显著性。作为NLP和ML的主题专家，你也期望提供你的建议，即是否采用实验方法。
- en: We will take the results from that evaluation, and we will task a team of agents
    to do the work for us!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用那个评估的结果，并将任务分配给一组代理来完成这项工作！
- en: 'Refer to the following notebook: `Ch9_Completing_a_Complex_Analysis_with_a_Team_of_LLM_Agents.ipynb`.
    The notebook starts with the common aspects of installs, imports, and settings.
    You will notice that AutoGen has a particular format of settings in the form of
    a dictionary. They provide the details, as you can see in our notebook.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下笔记本：`Ch9_Completing_a_Complex_Analysis_with_a_Team_of_LLM_Agents.ipynb`。该笔记本从安装、导入和设置的常见方面开始。你会注意到AutoGen有一个特定的字典形式的设置格式。它们提供了详细信息，正如你可以在我们的笔记本中看到的那样。
- en: Now, we move on to the interesting parts!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们转向有趣的部分！
- en: Creating a visualization of the significance of the experiments
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 创建实验重要性的可视化
- en: The `record.pickle` `file` is of a `dict` variable. It is the collection of
    numerical results from the previous evaluation notebook. Our wish is to visualize
    the distributions of the token counts for each of the experiments. There are token
    counts for original prompts and token counts for compressed prompts. There are
    also the ratios between the two for each experiment.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`record.pickle`文件是一个`dict`变量。它是之前评估笔记本中数值结果的集合。我们的愿望是可视化每个实验的令牌计数分布。有原始提示的令牌计数和压缩提示的令牌计数，以及每个实验中两者的比率。'
- en: In this section, we'll form a team to put code together that would visualize
    the distributions of each of the three.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将组建一个团队，编写代码以可视化三个分布。
- en: Defining the task to be fulfilled by the team
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义团队需要完成的任务
- en: First, we define the task to be fulfilled by the team. We tell the team where
    the file is saved and the context and the nature of the values in the `dict`,
    thus giving the team the understanding they need to ideate a solution to the task.
    Then, we describe the task of creating a plot and visualizing the distributions.
    All those details are in the one string that describes the task. Note that in
    an Agile Scrum work setting, this task string is similar to the purpose of the
    story.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义团队需要完成的任务。我们告诉团队文件保存的位置、上下文以及`dict`中值的性质，从而为团队提供他们需要来构思任务解决方案的理解。然后，我们描述了创建图表和可视化分布的任务。所有这些细节都在描述任务的单一字符串中。请注意，在敏捷Scrum工作环境中，这个任务字符串类似于故事的目的。
- en: Now that we have formed a comprehensive description, it should be clear what
    is expected. For instance, we ask for the figures and axes to be labeled, but
    we don’t explicitly state what labels are expected. The agents will understand
    on their own, just as we would have understood this on our own, as the labels
    are inferred from the task and the data field names.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经形成了一个全面的描述，应该很清楚期望的是什么。例如，我们要求标注图表和坐标轴，但我们没有明确说明期望的标签是什么。代理会自己理解，就像我们自己会理解一样，因为标签是从任务和数据字段名称中推断出来的。
- en: Defining the agents and assigning the team members roles
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义代理和分配团队成员的角色
- en: 'For this task, we would need three team members: a programmer to write the
    code, a QA engineer to run the code and provide feedback, and a team lead to verify
    when the task is complete.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们需要三名团队成员：一名程序员编写代码，一名QA工程师运行代码并提供反馈，以及一名团队领导来验证任务是否完成。
- en: For each of the roles, we articulate a system prompt. This system prompt, as
    we learned in [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), has a significant
    impact on the LLM’s function. Notice that we also provide the QA engineer and
    the team lead with the ability to run code on their own. In this way, they will
    be able to verify the programmer’s code and provide objective feedback. If we
    told the same agent to write the code and to confirm that it is correct, we might
    find that, in practice, it would generate a first draft, wouldn’t bother to run
    and verify it, and it would conclude that task without having verified it.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个角色，我们阐述一个系统提示。正如我们在 [*第8章*](B18949_08.xhtml#_idTextAnchor440) 中所学，这个系统提示对LLM的功能有重大影响。请注意，我们还为QA工程师和团队领导提供了运行代码的能力。这样，他们就能验证程序员的代码并提供客观反馈。如果我们让同一个代理编写代码并确认其正确性，我们可能会发现，在实践中，它可能会生成一个初稿，不会麻烦去运行和验证它，并且会在未经验证的情况下完成该任务。
- en: Defining a group conversation
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义群组对话
- en: Here, we define the conversation to be a multi-agent conversation; this is one
    of the features of AutoGen. This is slightly different from the case where you
    define a series of conversations where each conversation involves just two agents.
    The group conversation involves more agents.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将对话定义为多代理对话；这是AutoGen的一个特性。这与定义一系列对话的情况略有不同，其中每个对话只涉及两个代理。群组对话涉及更多的代理。
- en: When defining a group conversation, we also define a manager for the conversation.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义一个群组对话时，我们也会定义一个对话经理。
- en: Deploying the team
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部署团队
- en: The team lead tasks the manager with the task we defined. The manager then delegates
    the work to the programmer and the QA engineer.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 团队领导将我们定义的任务分配给经理。然后经理将工作委托给程序员和QA工程师。
- en: 'Here are the highlights of that automated conversation as it appears on the
    screen:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是屏幕上显示的自动化对话的要点：
- en: '[PRE17]python'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE17]python'
- en: import pandas as pd
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: import pandas as pd
- en: import matplotlib.pyplot as plt
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: import matplotlib.pyplot as plt
- en: Load the record dict from URL
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从URL加载记录字典
- en: import requests
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: import requests
- en: import pickle
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: import pickle
- en: '[...]'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[...]'
- en: 'qa_engineer (to manager_0):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 'qa_engineer (向 manager_0):'
- en: 'exitcode: 0 (execution succeeded)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 'exitcode: 0 (执行成功)'
- en: 'Code output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 代码输出：
- en: Figure(640x480)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图(640x480)
- en: 'programmer (to manager_0):'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 'programmer (向 manager_0):'
- en: TERMINATE
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 终止
- en: '[PRE18]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'lead (to manager_1):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 'lead (向 manager_1):'
- en: Refer to the results printed below.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下面打印的结果。
- en: These are the results that stem from [...]
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是从 [...] 得出的结果。
- en: 'writer (to manager_1):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 'writer (向 manager_1):'
- en: 'The experiments on prompt compression using LLMLingua have produced the following
    results:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLMLingua进行提示压缩的实验产生了以下结果：
- en: '- Classification Performance:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '- 分类性能：'
- en: '- Agreement rate of [...]'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '- [...] 的协议率'
- en: 'principal_engineer (to manager_1):'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 'principal_engineer (向 manager_1):'
- en: '[...]'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[...]'
- en: '[PRE19]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It is imperative to carefully consider the trade-offs presented by prompt compression,
    as while it may lead to resource savings, there might be implications on processing
    efficiency. The decision to adopt prompt compression should be made with a thorough
    understanding of these trade-offs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑提示压缩带来的权衡是至关重要的，因为虽然它可能导致资源节约，但可能会对处理效率产生影响。采用提示压缩的决定应该基于对这些权衡的全面理解。
- en: '[PRE20]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Overall, the results indicate that while prompt compression may lead to cost
    savings and resource reduction, it comes at the expense of decreased classification
    performance and significantly increased processing times.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，结果表明，虽然提示压缩可能导致成本节约和资源减少，但它是以降低分类性能和显著增加处理时间为代价的。
- en: '**Recommendation:** Prompt compression using LLMLinguam is **not recommended**
    as it can negatively impact classification performance and significantly increase
    processing times, outweighing the potential cost savings.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**建议：** 使用LLMLinguam进行提示压缩**不推荐**，因为它可能会对分类性能产生负面影响，并显著增加处理时间，超过了潜在的节约成本。'
- en: '```'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '```'
- en: Here, the team found it much easier to draw a definite conclusion. It did so
    without any human intervention and solely based on the numerical results it was
    given.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，团队发现得出明确结论要容易得多。它没有进行任何人为干预，仅基于给出的数值结果。
- en: Concluding thoughts on the multiple-agent team
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对多代理团队的总结性思考
- en: This emerging method of simultaneously employing several LLMs is gaining interest
    and traction in the world of AI. In the code experiments that we present in this
    section, it was proven without a doubt that AutoGen’s group conversation can provide
    tangible and actionable value in the professional setting. Although setting these
    code experiments required a series of trials and errors for properly setting the
    agent roles and properly describing the tasks, it suggests that this framework
    is moving in a direction where less human intervention is required. What seems
    to remain a monumental component is the human oversight, feedback, and evaluation
    of the resulting relics of those agent teams’ *work.* We would like to stress
    to the reader that of the various application and innovations that we share in
    this book, we have marked the multiple-agent framework as the one that is most
    likely to grow and to also become the most popular. This is based on the overwhelming
    expectations that industries have from AI to automate and demonstrate human-like
    expertise, while innovations such as Autogen, and later Autodev, both by Microsoft,
    are exemplifying growing feasibility and competency.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这种同时使用多个LLM的新兴方法在AI领域引起了兴趣和关注。在本节中我们展示的代码实验中，毫无疑问地证明了AutoGen的群组对话在专业环境中可以提供有形和可操作的价值。尽管设置这些代码实验需要一系列的尝试和错误来正确设置代理角色和描述任务，但它表明这个框架正朝着减少人类干预的方向发展。似乎仍然是一个重要组成部分的是，对那些代理团队*工作*产生的结果进行人类监督、反馈和评估。我们想向读者强调，在我们这本书中分享的各种应用和创新中，我们将多代理框架标记为最有可能增长并成为最受欢迎的框架。这是基于行业对AI自动化和展示类似人类的专业知识的压倒性期望，而像Autogen和Autodev这样的创新，后者由微软推出，都展示了日益增长的可行性和能力。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Throughout this pivotal chapter, we have embarked on an in-depth exploration
    of the most recent and groundbreaking applications of LLMs, presented through
    comprehensive Python code examples. We began by unlocking advanced functionalities
    by using the RAG framework and LangChain, enhancing LLM performance for domain-specific
    tasks. The journey continued with advanced methods in chains for sophisticated
    formatting and processing, followed by the automation of information retrieval
    from diverse web sources. We also tackled the optimization of prompt engineering
    through prompt compression techniques, significantly reducing API costs. Finally,
    we ventured into the collaborative potential of LLMs by forming a team of models
    that work in concert to solve complex problems.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个关键章节的整个过程中，我们深入探讨了最近和最具突破性的LLM应用，这些应用通过全面的Python代码示例进行展示。我们首先通过使用RAG框架和LangChain来解锁高级功能，从而增强LLM在特定领域任务中的性能。旅程继续，我们探讨了用于复杂格式化和处理的链的高级方法，随后是来自不同网络来源的信息检索自动化。我们还解决了通过提示压缩技术优化提示工程的问题，显著降低了API成本。最后，我们探索了LLM的协作潜力，通过组建一个协同解决复杂问题的模型团队。
- en: By mastering these topics, you have now acquired a robust set of skills, enabling
    you to harness the power of LLMs for a variety of applications. These newfound
    abilities not only prepare you to tackle current challenges in NLP but also equip
    you with the insights to innovate and push the boundaries of what’s possible in
    the field. The practical knowledge gained from this chapter will empower you to
    apply advanced LLM techniques to real-world issues, opening up new opportunities
    for efficiency, creativity, and problem-solving.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握这些主题，你现在已经掌握了一套强大的技能，使你能够利用LLM的力量应用于各种应用。这些新获得的能力不仅使你准备好应对NLP中的当前挑战，还为你提供了洞察力，以创新和推动该领域的可能性边界。从本章中获得的实际知识将赋予你应用高级LLM技术解决现实世界问题的能力，为效率、创造力和问题解决开辟新的机会。
- en: As we turn the page, the next chapter will take us into the realm of emerging
    trends in AI and LLM technology. We will delve into the latest algorithmic developments,
    assess their impact on various business sectors, and consider the future landscape
    of AI. This forthcoming discussion promises to provide you with a comprehensive
    understanding of where the field is headed and how you can stay at the forefront
    of technological innovation.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们翻过这一页，下一章将带我们进入AI和LLM技术新兴趋势的领域。我们将深入研究最新的算法发展，评估它们对各个商业部门的影响，并考虑AI的未来格局。这次即将到来的讨论承诺为您提供对领域未来走向的全面理解，以及如何保持技术创新的前沿。
