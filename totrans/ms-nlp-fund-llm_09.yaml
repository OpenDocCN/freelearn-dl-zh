- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Exploring the Frontiers: Advanced Applications and Innovations Driven by LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rapidly evolving landscape of **natural language processing** (**NLP**),
    **large language models** (**LLMs**) have marked a revolutionary step forward,
    reshaping how we interact with information, automate processes, and derive insights
    from vast data pools. This chapter represents the culmination of our journey through
    the emergence and development of NLP methods. It is here that the theoretical
    foundations laid in previous chapters converge with practical, cutting-edge applications,
    illuminating the remarkable capabilities of LLMs when harnessed with the right
    tools and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We delve into the most recent and thrilling advancements in LLM applications,
    presented through detailed Python code examples designed for hands-on learning.
    This approach not only illustrates the power of LLMs but also equips you with
    the skills to implement these technologies in real-world scenarios. The subjects
    covered in this chapter are meticulously selected to showcase a spectrum of advanced
    functionalities and applications.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of this chapter cannot be overstated. It not only reflects the
    state of the art in NLP but also serves as a bridge to the future, where the integration
    of these technologies into everyday solutions becomes seamless. By the end of
    this chapter, you will have a comprehensive understanding of how to apply the
    latest LLM techniques and innovations, empowering you to push the boundaries of
    what’s possible in NLP and beyond. Join us on this exciting journey to unlock
    the full potential of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the main headings covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced methods with chains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving information from various web sources automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt compression and API cost reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple agents – forming a team of LLMs who collaborate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, the following will be necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Programming knowledge**: Familiarity with Python programming is a must since
    the open source models, OpenAI’s API, and LangChain are all illustrated using
    Python code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access to OpenAI’s API**: An API key from OpenAI will be required to explore
    closed source models. This can be obtained by creating an account with OpenAI
    and agreeing to their terms of service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open source models**: Access to the specific open source models mentioned
    in this chapter will be necessary. These can be accessed and downloaded from their
    respective repositories or via package managers such as pip or conda.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local development environment**: A local development environment setup with
    Python installed is required. An **integrated development environment** (**IDE**)
    such as **PyCharm**, **Jupyter Notebook**, or a simple text editor can be used.
    Note that we recommend a free **Google Colab** notebook, as it encapsulates all
    these requirements in a seamless web interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ability to install libraries**: You must have permission for the installation
    of the required Python libraries such as **NumPy**, **SciPy**, **TensorFlow**,
    and **PyTorch**. Note that the code we provide includes the required installations
    so you won’t have to install them ahead of time. We simply stress that you should
    have permission to do so, which we expect you would. Specifically, using a free
    Google Colab notebook would suffice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hardware requirements**: Depending on the complexity and size of the models
    you’re working with, a computer with sufficient processing power (potentially
    including a good GPU for ML tasks) and ample memory will be required. This is
    only relevant when choosing to not use the free Google Colab.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve set up LLM applications using APIs and locally, we can finally
    deploy the advanced applications of LLMs that let us leverage their immense power.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing LLM performance with RAG and LangChain – a dive into advanced functionalities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **retrieval-augmented generation** (**RAG**) framework has become instrumental
    in tailoring **large language models** (**LLMs**) for specific domains or tasks,
    bridging the gap between the simplicity of prompt engineering and the complexity
    of model fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering stands as the initial, most accessible technique for customizing
    LLMs. It leverages the model’s capacity to interpret and respond to queries based
    on the input prompt. For example, to inquire if Nvidia surpassed earnings expectations
    in its latest announcement, directly providing the earnings call content within
    the prompt can compensate for the LLM’s lack of immediate, up-to-date context.
    This approach, while straightforward, hinges on the model’s ability to digest
    and analyze the provided information within a single or a series of carefully
    crafted prompts.
  prefs: []
  type: TYPE_NORMAL
- en: When the scope of inquiry exceeds what prompt engineering can accommodate—such
    as analyzing a decade’s worth of tech sector earnings calls—RAG becomes indispensable.
    Prior to RAG’s adoption, the alternative was fine-tuning, a resource-intensive
    process requiring significant adjustments to the LLM’s architecture to incorporate
    extensive datasets. RAG simplifies this by preprocessing and storing large amounts
    of data in a vector database. It intelligently isolates and retrieves the data
    segments pertinent to the query, effectively condensing the vast information into
    a manageable, prompt-size context for the LLM. This innovation drastically reduces
    the time, resources, and expertise needed for such extensive data familiarization
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), we introduced the general
    concept of RAGs and, in particular, LangChain, a RAG framework distinguished by
    its advanced capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss the additional unique features LangChain offers for enhancing
    LLM applications, providing you with practical insights into its implementation
    and utility in complex NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain pipeline with Python – enhancing performance with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will pick up where we left off with our last example from
    [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440). In this scenario, we are in the
    healthcare sector, and in our hospital, our care providers are expressing a need
    to be able to quickly surface patients’ records based on rough descriptions of
    the patient or their condition. For example, “Who was that patient I saw last
    year who was pregnant with triplets?” “Did I ever have a patient with a history
    of cancer from both of their parents and they were interested in a clinical trial?”
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We stress that these aren’t real medical notes and that the people described
    in the notes aren’t real.
  prefs: []
  type: TYPE_NORMAL
- en: In our example in [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), we kept the
    pipeline at minimum complexity by simply leveraging the vector databases of embeddings
    of clinical notes, and then we applied similarity search to look for notes based
    on simple requests. We noticed how one of the questions, the second question,
    received a wrong answer with the similarity search algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We will now enhance that pipeline. We will not settle for the results of the
    similarity search and surface those to the physicians; we will take those results
    that were deemed to be similar in content to the request, and we will employ an
    LLM to go through these results, vet them, and tell us which ones are indeed relevant
    to the physician.
  prefs: []
  type: TYPE_NORMAL
- en: Paid LLMs versus free
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We'll use this pipeline to exemplify the utility of either type of LLM, paid
    or free. We give you the choice, via the `paid_vs_free` variable, to either use
    OpenAI’s paid GPT model or a free LLM. Using OpenAI’s paid model would leverage
    their API and would require an API key. However, the free LLM is imported to the
    local environment where the Python code is run, thus making it available to anyone
    who has an internet connection and sufficient computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start getting hands-on and experimenting with the code.
  prefs: []
  type: TYPE_NORMAL
- en: Applying advanced LangChain configurations and pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Refer to the following notebook: `Ch9_Advanced_LangChain_Configurations_and_Pipeline.ipynb`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the first part of the notebook is identical to the notebook from [*Chapter
    8*](B18949_08.xhtml#_idTextAnchor440), so we will skip the description of that
    part.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required Python libraries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we need to expand the set of installed libraries and install `openai`
    and `gpt4all`. Moreover, in order to utilize `gpt4all`, we will need to download
    a `.bin` file from the web.
  prefs: []
  type: TYPE_NORMAL
- en: These two steps are easy to perform via the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up an LLM – choose between a paid LLM (OpenAI’s GPT) and a free LLM
    (from Hugging Face)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As explained above, we let you choose whether you want to run this example via
    a paid API by OpenAI or a free LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, since OpenAI’s service includes hosting the LLM and processing the
    prompts, it requires minimal resources and time and a basic internet connection.
    It also involves sending our prompts to OpenAI’s API service. Prompts typically
    include information that, in real-world settings, may be proprietary. Thus, an
    executive decision needs to be made regarding the security of the data. Similar
    considerations were central, in the last decade, to the transition of companies’
    computation from on-premises to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to that requirement, with a free LLM, you would host it locally,
    you would avoid exporting any information outside of your computation environment,
    but you would take on the processing.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect to consider is the terms of use of each LLM, as each may have
    different license terms. While an LLM may allow you to experiment with it for
    free, it may present constrictions on whether you may use it in a commercial product.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of constraints around runtime and computational resources, choosing
    the paid LLM for this example will yield quicker responses.
  prefs: []
  type: TYPE_NORMAL
- en: In order to accommodate your wish to experiment with a free LLM, and since we
    aspire to let you run the code quickly and for free on Google Colab, we must restrict
    our choice of LLMs to those that can be run on the limited RAM that Google lets
    us have with a free account. In order to do that, we chose an LLM with reduced
    precision, also known as a quantized LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Based on your choice between an API-based LLM and a free local LLM, the LLM
    will be assigned to the `llm variable`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a QA chain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we set up a RAG framework. It is designed to accept various text documents
    and set them up for retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Search based on the same requirements when using the LLM as the “brain” instead
    of embedding similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will now run the exact same requests as we did in the example in [*Chapter
    8*](B18949_08.xhtml#_idTextAnchor440). Those will be performed across the same
    notes, and the same vector DB that holds the same embedding. None of that has
    been changed or enhanced. The difference is that we will have the LLM oversee
    the processing of the answers.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), we saw that question number
    two received a wrong answer. The question was, “Are there any pregnant patients
    who are due to deliver in September?”
  prefs: []
  type: TYPE_NORMAL
- en: The answer we saw in [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440) was about
    a patient who is due to give birth in August. The mistake was due to the deficiency
    of the similarity algorithm. Indeed, that patient’s notes had content similar
    to that of the question, but the fine detail of giving birth in a different month
    should have been the factor that made those note irrelevant.
  prefs: []
  type: TYPE_NORMAL
- en: Here, in our current pipeline, where OpenAI’s LLM is applied, it gets it right,
    telling us that there are no patients who are due to deliver in September.
  prefs: []
  type: TYPE_NORMAL
- en: Note that when opting for the free LLM, it gets it wrong. This exemplifies the
    sub-optimal aspects of that model, as it is quantized in an effort to save on
    RAM requirements.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this example, we have put together an in-house search mechanism
    that lets the user, in our example, a physician, search through their patients’
    notes to find patients based on some criteria. A unique aspect of this system
    design is the ability to let the LLM retrieve the relevant answer from an external
    data source and not be limited to the data it was trained on. This paradigm is
    the basis of RAG.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will showcase more uses for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced methods with chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will continue our exploration of ways one can utilize LLM
    pipelines. We will focus on chains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following notebook: `Ch9_Advanced_Methods_with_Chains.ipynb`.
    This notebook presents an evolution of a chain pipeline, as every iteration exemplifies
    another feature that LangChain allows us to employ.'
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of using minimal computational resources, memory, and time, we
    use OpenAI’s API. You can choose to use a free LLM instead and may do so in a
    similar way to how we set up the notebook from the previous example in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The notebook starts with the basic configurations, as always, so we can skip
    to reviewing the notebook’s content.
  prefs: []
  type: TYPE_NORMAL
- en: Asking the LLM a general knowledge question
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we want to use the LLM to tell us an answer to a simple question
    that would require common knowledge that a trained LLM is expected to have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We then define a simple chain called `LLMChain`, and we feed it with the `LLM`
    variable and the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LLM, indeed, knows the answer from its knowledge base and returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Requesting output structure – making the LLM provide output in a particular
    data format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This time, we would like the output to be in a particular syntax, potentially
    allowing us to use it in a computational manner for downstream tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, we add a feature for achieving the syntax. We define the `output_parser
    variable`, and we use a different function for generating the output, `predict_and_parse()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Evolving to a fluent conversation – inserting an element of memory to have previous
    interactions as reference and context for follow-up prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This feature brings a new level of value to the chain. Until this point, the
    prompts didn’t have any context. The LLM processed each prompt independently.
    For instance, if you wanted to ask a follow-up question, you couldn’t. The pipeline
    didn’t have your prior prompts and the responses to them as reference.
  prefs: []
  type: TYPE_NORMAL
- en: In order to go from asking disjointed questions to having an ongoing, rolling
    conversation-like experience, LangChain offers `ConversationChain()`. Within this
    function, we have a `memory` parameter that maps the prior interactions with the
    chain to the current prompt. Therefore, the prompt template is where that memory
    “lives.”
  prefs: []
  type: TYPE_NORMAL
- en: Instead of prompting with a basic template, such as
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'the template now accommodates the memory feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can think of this string as being formatted similarly to a Python
    `f"…"` string, where `history` and `input` are string variables. The `ConversationChain()`
    function processes this prompt template and inserts these two variables to complete
    the prompt string. The `input` variable is produced by the function itself as
    we activate the memory mechanism, and the input variable is then supplied by us
    as we run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Where the output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s make a follow-up request that would only be understood in the context
    of the previous request and output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, we get the appropriate output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To complete this example, let’s assume the intention we had was to quickly
    generate a table of some holidays that includes their names and descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we get a formatted string from the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use pandas to convert this string to a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After pandas processes `dict` to be a DataFrame, we can observe it in *Table
    9.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | Christmas | Christmas is a Christian holiday that celebrates the
    birth of Jesus Christ. It is observed on December 25 each year. |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | Thanksgiving | Thanksgiving is a holiday in which people gather together
    to express gratitude for the blessings in their lives. It is celebrated on the
    fourth Thursday in November in the United States. |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | New Year’s Day | New Year’s Day marks the beginning of the Gregorian
    calendar year. It is celebrated on January 1 with various traditions and festivities.
    |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | Easter | Easter is a Christian holiday that commemorates the resurrection
    of Jesus Christ from the dead. It is observed on the first Sunday following the
    first full moon after the vernal equinox. |'
  prefs: []
  type: TYPE_TB
- en: '| **4** | Valentine’s Day | Valentine’s Day is a day to celebrate love and
    affection. It is traditionally associated with romantic love, but it is also a
    time to express appreciation for friends and family. |'
  prefs: []
  type: TYPE_TB
- en: '| **5** | St. Patrick’s Day | St. Patrick’s Day is a cultural and religious
    holiday that honors the patron saint of Ireland, St. Patrick. It is celebrated
    on March 17 with parades, wearing green, and other festive activities. |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – pandas transformed the table from dict to a DataFrame, thus suiting
    down-stream processing
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the various chain features that this notebook presents. Notice
    how we leveraged the features that both chains bring us and that LLMs bring us.
    For instance, while the memory and parsing features are completely handled on
    the chain’s side, the ability to present a response in a particular format, such
    as a JSON format, is solely accredited to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: In our next example, we will continue to present novel utilities with LLMs and
    LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving information from various web sources automatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we will review how simple it is to leverage LLMs to access
    the web and extract information. We may wish to research a particular topic, and
    so we would like to consolidate all the information from a few web pages, several
    YouTube videos that present that topic, and so on. Such an endeavor can take a
    while, as the content may be massive. For instance, several YouTube videos can
    sometimes take hours to review. Often, one doesn’t know how useful the video is
    until one has watched a significant portion of it.
  prefs: []
  type: TYPE_NORMAL
- en: Another use case is when looking to track various trends in real time. This
    may include tracking news sources, YouTube videos, and so on. Here, speed is crucial.
    Unlike the previous example where speed was important to save us personal time,
    here, speed is necessary for getting our algorithm to be relevant for identifying
    real-time emerging trends.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we put together a very simple and limited example.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving content from a YouTube video and summarizing it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Refer to the following notebook: `Ch9_Retrieve_Content_from_a_YouTube_Video_and_Summarize.ipynb`
    ([https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)).
    We will build our application on a library called EmbedChain ([https://github.com/embedchain/embedchain](https://github.com/embedchain/embedchain)).
    EmbedChain leverages a RAG framework and enhances it by allowing the vector database
    to include information from various web sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, we will choose a particular YouTube video (*Robert Waldinger:
    What makes a good life? Lessons from the longest study on happiness | TED*: [https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED](https://www.youtube.com/watch?v=8KkKuTCFvzI&ab_channel=TED)).
    We would like the content of that video to be processed into the RAG framework.
    Then, we will prompt an LLM with questions and tasks related to the content of
    that video, thus allowing us to extract everything we care to learn about the
    video without having to watch it.'
  prefs: []
  type: TYPE_NORMAL
- en: It should be stressed that a key feature that this method relies on is that
    YouTube accompanies many of its verbal videos with a written transcript. This
    makes the importing of the video’s text context seamless. If, however, one wishes
    to apply this method to a video that isn’t accompanied by a transcript, this is
    not a problem. One would need to pick a speech-to-text model, many of which are
    free and of very high quality. The audio of the video would be processed, a transcript
    would be extracted, and you may then import it into the RAG process.
  prefs: []
  type: TYPE_NORMAL
- en: Installs, imports, and settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with previous notebooks, here too, we install the necessary packages, import
    all the relevant packages, and set our OpenAI API key.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Make our choice of model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose an embedding model that will serve the RAG’s vector database feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose a prompting LLM. Notice how you can set up further parameters that control
    the model’s output, such as the maximal number of returned tokens or the temperature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pick the YouTube video to which you would like to apply this code and set a
    string variable using the video’s URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Setting up the retrieval mechanism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to set EmbedChain’s RAG process. We specify that we are passing a path
    to a YouTube video, and we provide the video’s URL.
  prefs: []
  type: TYPE_NORMAL
- en: We can then print out the text that was fetched and verify that it is, indeed,
    aligned with the video we are looking to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing, summarizing, and translating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now observe the value that this code yields.
  prefs: []
  type: TYPE_NORMAL
- en: 'We ask the LLM to review the content, to put together a summary, and to present
    that summary in English, Russian, and German:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned output is spot on, as it completely captures the essence of the
    TED talk. We edit it to remove the delimiter strings and get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, to make the content simple for, say, a German speaker, we ask the LLM to
    form the German summary into several bullet points that best describe the content
    of the video.
  prefs: []
  type: TYPE_NORMAL
- en: 'It does this well, and the outputs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: While this code is meant to serve as a basic proof of concept, one can see how
    simple it would be to add more data sources, automate it to run constantly, and
    act based on the findings. While a readable summary is helpful, one could change
    the code to act based on the identified content and execute downstream applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have observed several capabilities that LLMs can perform, we can
    take a step back and refine the way we utilize those LLMs. In our next section,
    we will exemplify how one may reduce LLM processing, thus saving API costs, or,
    when employing a local LLM, reducing inference computation.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt compression and API cost reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part is dedicated to a recent development in resource optimization for
    when employing API-based LLMs, such as OpenAI’s services. When considering the
    many trade-offs between employing a remote LLM as a service and hosting an LLM
    locally, one key metric is cost. In particular, based on the application and usage,
    the API costs can accumulate to a significant amount. API costs are mainly driven
    by the number of tokens that are being sent to and from the LLM service.
  prefs: []
  type: TYPE_NORMAL
- en: In order to illustrate the significance of this payment model on a business
    plan, consider business units for which the product or service relies on API calls
    to OpenAI’s GPT, where OpenAI serves as a third-party vendor. As a particular
    example, imagine a social network that lets its users have LLM assistance to comment
    on posts. In that use case, a user is interested in commenting on a post, and
    instead of having to write a complete comment, a feature lets the user describe
    their feelings about the post in three–five words, and a backend process augments
    a full comment.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular example, the engine collects the user’s three–five words,
    and it also collects the content of the post that the comment is meant for, meaning
    it will also collect all other relevant information that the social network’s
    experts would think is relevant for augmenting a comment. For instance, the user’s
    profile description, their past few comments, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This would mean that every time a user wishes to have a comment augmented, a
    detailed prompt is sent from the social network’s servers to the third party’s
    LLM via theAPI.
  prefs: []
  type: TYPE_NORMAL
- en: Now, this type of process can accumulate high costs.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will analyze an approach to reducing this cost by reducing
    the number of tokens sent to the LLM through the API. The basic assumption is
    that one can always reduce the number of words sent to the LLM and, thus, reduce
    cost, but the reduction in performance could be significant. Our motivation is
    to reduce that amount while maintaining high-quality performance. We then asked
    if only the “right” words could be sent, ignoring other “non-material” words.
    This notion reminds us of the concept of file compression, where a smart and tailored
    algorithm is employed to reduce the size a file takes while maintaining its purpose
    and value.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we introduce **LLMLingua**, a development by Microsoft that is meant to
    address prompts that are “sparse” in information by compressing them.
  prefs: []
  type: TYPE_NORMAL
- en: LLMLingua utilizes a compact, well-trained language model, such as LLaMA-7B,
    to identify and remove non-essential tokens within prompts. This approach enables
    efficient inference with LLMs, achieving up to 20x compression with minimal performance
    loss ([https://github.com/microsoft/LLMLingua](https://github.com/microsoft/LLMLingua)).
  prefs: []
  type: TYPE_NORMAL
- en: In their papers ([https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736)
    and https://arxiv.org/abs/2310.06839), the authors explain the algorithm and the
    advantages it proposes. It is interesting to note that besides the reduction in
    cost, the compression also aims to focus the remaining content, which is shown
    by the authors to lead to an improvement in performance by the LLM, as it avoids
    a sparse and noisy prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s experiment with prompt compression in a real-world example and evaluate
    its impact and various trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with prompt compression and evaluating trade-offs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the sake of this experiment, we'll illustrate a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: In our current use case, we are developing a feature that sits on top of a database
    of academic publications. The feature allows the user to pick a specific publication
    and ask questions about it. A backend engine evaluates the question, reviews the
    publication, and derives an answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To narrow down the scope of the feature for the sake of putting together a
    series of experiments, the publications are from the particular category of AI
    publications, and the question that the user asks is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This question requires a deep and insightful review of each publication, as
    there are cases where a publication discusses a novel algorithm where the term
    reinforcement learning isn’t explicitly mentioned at any point in the publication,
    yet the feature is expected to infer from the description of the algorithm whether
    it indeed leverages the concepts of reinforcement learning and flag it as such.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following notebook: `Ch9_RAGLlamaIndex_Prompt_Compression.ipynb`.'
  prefs: []
  type: TYPE_NORMAL
- en: In this code, we run a set of experiments, each per the above feature description.
    Each experiment is in the form of a full, end-to-end RAG task. While we employed
    LangChain in the previous RAG examples, here, we introduce LlamaIndex. LlamaIndex
    is an open source Python library that employs a RAG framework ([https://docs.llamaindex.ai/en/stable/index.html](https://docs.llamaindex.ai/en/stable/index.html)).
    LlamaIndex is similar to LangChain in that way.
  prefs: []
  type: TYPE_NORMAL
- en: The LLMLingua code stack that the folks at Microsoft put together is integrated
    with LlamaIndex.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s review the code in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Code settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similar to the previous notebooks, here too, we set the initial settings with
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In this code section, we start by defining some key variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the number of experiments that we want to run. We want to make sure we
    choose a number that is large enough so as to get a good statistical representation
    of the impact that the compression has.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the top-k, which is the number of chunks to be retrieved by the RAG framework
    for prompt context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We predefined the target number of the token we would like the compression to
    reduce.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, as in the previous code, we set our OpenAI API key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We take this opportunity to stress that some of the parameters in this evaluation
    were fixed for the sake of limiting its complexity and keeping it appropriate
    for educational purposes. When conducting such an evaluation in business or academic
    settings, there should be either qualitative or quantitative reasoning for the
    value chosen. Qualitative may be of the form “We shell fix the desired reduction
    to 999 tokens due to budget constraints," whereas quantitative may seek to not
    fix it but rather optimize it as a part of the other trade-offs. In our case,
    we fixed this particular parameter to a value that was found to allow for an impressive
    compression rate while maintaining a decent agreement rate between the two evaluated
    approaches. Another example was the number of experiments we chose, which was
    a trade-off between runtime, GPU memory allocation, and statistical power.
  prefs: []
  type: TYPE_NORMAL
- en: Gathering the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We need to gather the dataset of the publications, and we also filter it so
    as to be left with only the limited cohort of publications that are in the AI
    category.
  prefs: []
  type: TYPE_NORMAL
- en: LLM configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we set the ground for the two LLMs we will be employing.
  prefs: []
  type: TYPE_NORMAL
- en: The compression method, LLMLingua, employs Llama2 as the compressing LLM. It
    will obtain the context retrieved by the LlamaIndex RAG pipeline, the user’s question,
    and it will compress and reduce the size of the context content.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI’s GPT is to be used as the downstream LLM for prompting, meaning it will
    obtain the question about reinforcement learning and the additional relevant context
    and return an answer.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, here, we define the user’s question. Note that we added instructions
    for OpenAI’s GPT on how to present the answer.
  prefs: []
  type: TYPE_NORMAL
- en: The experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the core of the notebook. A `for` loop iterates over the various experiments.
    In each iteration, two scenarios are evaluate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**First scenario**: an ordinary RAG task is deployed where the context is retrieved
    without being compressed. The prompt is comprised of the retrieved context and
    the user’s question, and the answer that the LLM returns is recorded along with
    the number of sent tokens and the processing time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Second scenario**: LLMLingua is employed. The retrieved context is compressed.
    The compressed context is sent to the LLM along with the user’s question. Again,
    the returned answer is recorded along with the number of sent tokens and the processing
    time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When this code cell is completed, we have a dictionary, `record`, that holds
    the relevant values for each iteration that will be used to aggregate and derive
    conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the impact of context compression – a reduction in classification
    performance versus an increase in resource efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we sum up the values of the experiments and deduce what impact the prompt
    compression has on the performance of the LLM, the processing time, and the cost
    of the API:'
  prefs: []
  type: TYPE_NORMAL
- en: We found that the reduction in the context length had yielded an agreement rate
    of 92%.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that the process of compression had extended the processing time by
    11 times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We found that the reduction in the context length saved 92% of the total cost
    of sent tokens!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that cost reduction is negatively dependent on the agreement rate, as we
    expect an increase in cost savings to reduce the agreement rate.
  prefs: []
  type: TYPE_NORMAL
- en: This reduction is significant and, in some cases, may tilt the scale from a
    loss-making service to a profitable service.
  prefs: []
  type: TYPE_NORMAL
- en: Here are some notes to keep in mind regarding the meaning of a disagreement
    and additional trade-offs. Regarding the drop in agreement rate between the two
    approaches, while an agreement between the two approaches insinuates that they
    are both correct, a disagreement could go either way. It could be that in the
    second scenario, the compression distorted the context and, thus, made the model
    unable to properly classify it. However, the opposite may be true, as the compression
    may have reduced the irrelevant content and made the LLM focus on the relevant
    aspects of the content, thus making the scenario with the compressed context yield
    a correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding additional trade-offs, the above metrics of LLM performance, processing
    time, and API cost don’t reveal additional considerations such as the computational
    resources that the compression requires. The local compressing LLM, in our case,
    Llama2, requires local hosting and local GPUs. These are non-trivial resources
    that don’t exist on an ordinary laptop. Remember the original approach, i.e.,
    the first scenario, does not require those. An ordinary RAG approach can perform
    embeddings using either a smaller LM, such as one that is BERT-based, or even
    an API-based embedding. The prompted LLM, under our original assumption, is chosen
    to be remote and API-based, thus enabling the deployment environment to have minimal
    computation resources, like a common laptop would provide.
  prefs: []
  type: TYPE_NORMAL
- en: This evaluation proves that the LLMLingua prompt compression method is very
    impactful and useful as a means of cost reduction.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and last code demonstration of this chapter, we will continue to
    observe the results of this experience, and we will do so by forming a team of
    experts, each played by an LLM, so as to enhance the process of deriving a conclusion
    to the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple agents – forming a team of LLMs that collaborate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section deals with one of the most exciting recent methods in the world
    of LLMs, employing multiple LLMs simultaneously. In the context of this section,
    we seek to define multiple agents, each backed by an LLM and given a different
    designated role to play. Instead of the user working directly with the LLM, as
    we see in ChatGPT, here, the user sets up multiple LLMs and sets their role by
    defining a different system prompt for each of them.
  prefs: []
  type: TYPE_NORMAL
- en: Potential advantages of multiple LLM agents working simultaneously
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much like with people working together, here too, we see the advantages of employing
    several LLMs simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some advantages are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhancing validation and reducing hallucinations**: It has been shown that
    when providing feedback to an LLM and asking it to reason or to check its response,
    the reliability of its response improves. When designating roles for the various
    LLM agents on a team, the system prompt of at least one of them may include the
    requirement to criticize and validate the answers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Allowing the person to be involved as much or as little as they want in the
    process**: When designating the various roles, the user may insert themselves
    into the team, such that when it is the user’s turn to participate in the conversation,
    the rest of the agents wait while the user enters their input. However, if desired,
    the user may remove themselves altogether and just let the LLMs work automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following examples, we will see examples of the latter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Allowing different LLM models to be best utilized**: Today, we have several
    leading LLMs available. Some are free and hosted locally, and some are API-based.
    They are different in their size and capabilities, and some of them are stronger
    in particular tasks than others. When forming a team of agents where each agent
    is assigned a different role, a different LLM may be set that best suits that
    role. For instance, in the context of a coding project, where one of the agents
    is a programmer of a particular coding language, the user may choose to set the
    LLM for that agent to be an LLM that is superior for code generation in that particular
    coding language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizing resources – employing several smaller LLMs**: Imagine a project
    that involves technical functions and also domain expertise. For example, building
    a user platform in the medical space. You would want there to be a frontend engineer,
    a backend engineer, a designer, and a medical expert, all governed by the project
    manager and the product manager. If you were to develop this platform using the
    multiple-agents framework, you would define the agents, assign the various roles
    to them, and pick an LLM to drive them. If you were to use the same LLM for all
    of the agents, say, OpenAI’s most recent GPT, then that model would have to be
    very generic, thus requiring it to be very large and perhaps very expensive and
    maybe even slow. However, if you had access to individual LLMs, each pre-trained
    to only fulfill a limited function, for instance, one LLM dedicated to the medical
    service domain and a different LLM dedicated to backend development in Python,
    then you would assign each of those particular LLMs to their corresponding agents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That may present a major reduction in model size, as the combined architecture
    of several specialized LLMs may be smaller in size than the architecture of one
    generic LLM when assuming equal performance between the two scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Optimizing resources – optimal designation of a single LLM**: A unique and
    particular case of employing multiple LLMs is that in which we are seeking to
    optimize the LLM being chosen per the current task. This case is different than
    all the above, as it does not refer to a case where several LLMs are working simultaneously.
    In this case, a routing algorithm chooses one LLM based on a current state of
    constraints and variables. These may include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The current load on each of the different parts of the computation system
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost constraints, which may vary over time
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The source of the prompt, as different clients/regions may have different priorities
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The purpose of the prompt, as different use cases may be prioritized by the
    business
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt’s requirements, as a code-generating task, may be obtaining excellent
    responses with a small and efficient code-generating LLM, whereas a request to
    review a legal document and to suggest precedents may call for a completely different
    model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoGen
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The particular framework we employ in this section is called AutoGen, and it
    is made available by Microsoft (GitHub repo: [https://github.com/microsoft/autogen/tree/main](https://github.com/microsoft/autogen/tree/main)).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.1* conveys the AutoGen framework. The following was obtained from
    the statement made in the GitHub repo:'
  prefs: []
  type: TYPE_NORMAL
- en: '*AutoGen is a framework that enables the development of LLM applications using
    multiple agents that can converse with each other to solve tasks. AutoGen agents
    are customizable, conversable, and seamlessly allow human participation. They
    can operate in various modes that employ combinations of LLMs, human inputs,*
    *and tools*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – AutoGen functionality](img/B18949_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – AutoGen functionality
  prefs: []
  type: TYPE_NORMAL
- en: On the left of *Figure 9**.1*, we observe the designation of roles and capabilities
    to individual agents; on the right, we observe a few of the conversation structures
    that are available.
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoGen’s key capabilities as presented in the code repo:'
  prefs: []
  type: TYPE_NORMAL
- en: AutoGen enables building next-gen LLM applications based on multi-agent conversations
    with minimal effort. It simplifies the orchestration, automation, and optimization
    of a complex LLM workflow. It maximizes the performance of LLM models and overcomes
    their weaknesses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports diverse conversation patterns for complex workflows. With customizable
    and conversable agents, developers can use AutoGen to build a wide range of conversation
    patterns concerning conversation autonomy, the number of agents, and agent conversation
    topology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a collection of working systems with different complexities. These
    systems span a wide range of applications from various domains and complexities.
    This demonstrates how AutoGen can easily support diverse conversation patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoGen provides enhanced LLM inference. It offers utilities such as API unification
    and caching, and advanced usage patterns such as error handling, multi-config
    inference, context programming, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AutoGen is powered by collaborative research studies from Microsoft, Penn State
    University, and the University of Washington.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can dive into a practical example in the code.
  prefs: []
  type: TYPE_NORMAL
- en: Completing a complex analysis – visualizing the results and forming a conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we will show how a team of multiple agents, each with a different designated
    role, could serve as a professional team. The use case we chose is a continuation
    of the previous code we ran. In the last code, we performed a complex evaluation
    of employing prompt compression, and when that code finished, we had two resulting
    items: the `dict` that holds the numeric measurements of the experiments, called
    `record,` and the verbal statements about the resulting agreement rate, the reduction
    in tokens and cost, and the change in processing time.'
  prefs: []
  type: TYPE_NORMAL
- en: With that previous notebook, we intentionally stopped short. We didn’t visualize
    the reduction in tokens and cost, and we didn’t form an opinion as to whether
    we would advocate for employing the prompt reductions. However, in business or
    academic settings, one would be required to offer both. When you present your
    findings to stakeholders, decision-makers, or the research community, you are
    expected, when feasible, to visualize the statistical significance of the experiments.
    As a subject expert in NLP and ML, you are also expected to provide your recommendation
    on whether to adopt the experimented method or not.
  prefs: []
  type: TYPE_NORMAL
- en: We will take the results from that evaluation, and we will task a team of agents
    to do the work for us!
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following notebook: `Ch9_Completing_a_Complex_Analysis_with_a_Team_of_LLM_Agents.ipynb`.
    The notebook starts with the common aspects of installs, imports, and settings.
    You will notice that AutoGen has a particular format of settings in the form of
    a dictionary. They provide the details, as you can see in our notebook.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we move on to the interesting parts!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a visualization of the significance of the experiments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `record.pickle` `file` is of a `dict` variable. It is the collection of
    numerical results from the previous evaluation notebook. Our wish is to visualize
    the distributions of the token counts for each of the experiments. There are token
    counts for original prompts and token counts for compressed prompts. There are
    also the ratios between the two for each experiment.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll form a team to put code together that would visualize
    the distributions of each of the three.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the task to be fulfilled by the team
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, we define the task to be fulfilled by the team. We tell the team where
    the file is saved and the context and the nature of the values in the `dict`,
    thus giving the team the understanding they need to ideate a solution to the task.
    Then, we describe the task of creating a plot and visualizing the distributions.
    All those details are in the one string that describes the task. Note that in
    an Agile Scrum work setting, this task string is similar to the purpose of the
    story.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have formed a comprehensive description, it should be clear what
    is expected. For instance, we ask for the figures and axes to be labeled, but
    we don’t explicitly state what labels are expected. The agents will understand
    on their own, just as we would have understood this on our own, as the labels
    are inferred from the task and the data field names.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the agents and assigning the team members roles
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For this task, we would need three team members: a programmer to write the
    code, a QA engineer to run the code and provide feedback, and a team lead to verify
    when the task is complete.'
  prefs: []
  type: TYPE_NORMAL
- en: For each of the roles, we articulate a system prompt. This system prompt, as
    we learned in [*Chapter 8*](B18949_08.xhtml#_idTextAnchor440), has a significant
    impact on the LLM’s function. Notice that we also provide the QA engineer and
    the team lead with the ability to run code on their own. In this way, they will
    be able to verify the programmer’s code and provide objective feedback. If we
    told the same agent to write the code and to confirm that it is correct, we might
    find that, in practice, it would generate a first draft, wouldn’t bother to run
    and verify it, and it would conclude that task without having verified it.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a group conversation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Here, we define the conversation to be a multi-agent conversation; this is one
    of the features of AutoGen. This is slightly different from the case where you
    define a series of conversations where each conversation involves just two agents.
    The group conversation involves more agents.
  prefs: []
  type: TYPE_NORMAL
- en: When defining a group conversation, we also define a manager for the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the team
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The team lead tasks the manager with the task we defined. The manager then delegates
    the work to the programmer and the QA engineer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the highlights of that automated conversation as it appears on the
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]python'
  prefs: []
  type: TYPE_NORMAL
- en: import pandas as pd
  prefs: []
  type: TYPE_NORMAL
- en: import matplotlib.pyplot as plt
  prefs: []
  type: TYPE_NORMAL
- en: Load the record dict from URL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: import requests
  prefs: []
  type: TYPE_NORMAL
- en: import pickle
  prefs: []
  type: TYPE_NORMAL
- en: '[...]'
  prefs: []
  type: TYPE_NORMAL
- en: 'qa_engineer (to manager_0):'
  prefs: []
  type: TYPE_NORMAL
- en: 'exitcode: 0 (execution succeeded)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Code output:'
  prefs: []
  type: TYPE_NORMAL
- en: Figure(640x480)
  prefs: []
  type: TYPE_NORMAL
- en: 'programmer (to manager_0):'
  prefs: []
  type: TYPE_NORMAL
- en: TERMINATE
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'lead (to manager_1):'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the results printed below.
  prefs: []
  type: TYPE_NORMAL
- en: These are the results that stem from [...]
  prefs: []
  type: TYPE_NORMAL
- en: 'writer (to manager_1):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiments on prompt compression using LLMLingua have produced the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Classification Performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Agreement rate of [...]'
  prefs: []
  type: TYPE_NORMAL
- en: 'principal_engineer (to manager_1):'
  prefs: []
  type: TYPE_NORMAL
- en: '[...]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: It is imperative to carefully consider the trade-offs presented by prompt compression,
    as while it may lead to resource savings, there might be implications on processing
    efficiency. The decision to adopt prompt compression should be made with a thorough
    understanding of these trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Overall, the results indicate that while prompt compression may lead to cost
    savings and resource reduction, it comes at the expense of decreased classification
    performance and significantly increased processing times.
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommendation:** Prompt compression using LLMLinguam is **not recommended**
    as it can negatively impact classification performance and significantly increase
    processing times, outweighing the potential cost savings.'
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the team found it much easier to draw a definite conclusion. It did so
    without any human intervention and solely based on the numerical results it was
    given.
  prefs: []
  type: TYPE_NORMAL
- en: Concluding thoughts on the multiple-agent team
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This emerging method of simultaneously employing several LLMs is gaining interest
    and traction in the world of AI. In the code experiments that we present in this
    section, it was proven without a doubt that AutoGen’s group conversation can provide
    tangible and actionable value in the professional setting. Although setting these
    code experiments required a series of trials and errors for properly setting the
    agent roles and properly describing the tasks, it suggests that this framework
    is moving in a direction where less human intervention is required. What seems
    to remain a monumental component is the human oversight, feedback, and evaluation
    of the resulting relics of those agent teams’ *work.* We would like to stress
    to the reader that of the various application and innovations that we share in
    this book, we have marked the multiple-agent framework as the one that is most
    likely to grow and to also become the most popular. This is based on the overwhelming
    expectations that industries have from AI to automate and demonstrate human-like
    expertise, while innovations such as Autogen, and later Autodev, both by Microsoft,
    are exemplifying growing feasibility and competency.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this pivotal chapter, we have embarked on an in-depth exploration
    of the most recent and groundbreaking applications of LLMs, presented through
    comprehensive Python code examples. We began by unlocking advanced functionalities
    by using the RAG framework and LangChain, enhancing LLM performance for domain-specific
    tasks. The journey continued with advanced methods in chains for sophisticated
    formatting and processing, followed by the automation of information retrieval
    from diverse web sources. We also tackled the optimization of prompt engineering
    through prompt compression techniques, significantly reducing API costs. Finally,
    we ventured into the collaborative potential of LLMs by forming a team of models
    that work in concert to solve complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: By mastering these topics, you have now acquired a robust set of skills, enabling
    you to harness the power of LLMs for a variety of applications. These newfound
    abilities not only prepare you to tackle current challenges in NLP but also equip
    you with the insights to innovate and push the boundaries of what’s possible in
    the field. The practical knowledge gained from this chapter will empower you to
    apply advanced LLM techniques to real-world issues, opening up new opportunities
    for efficiency, creativity, and problem-solving.
  prefs: []
  type: TYPE_NORMAL
- en: As we turn the page, the next chapter will take us into the realm of emerging
    trends in AI and LLM technology. We will delve into the latest algorithmic developments,
    assess their impact on various business sectors, and consider the future landscape
    of AI. This forthcoming discussion promises to provide you with a comprehensive
    understanding of where the field is headed and how you can stay at the forefront
    of technological innovation.
  prefs: []
  type: TYPE_NORMAL
