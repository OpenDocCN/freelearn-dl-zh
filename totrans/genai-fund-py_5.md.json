["```py\n!pip install transformers peft sentence-transformers\n```", "```py\nfrom transformers import (\n    AutoModelForQuestionAnswering, AutoTokenizer)\nmodel_name = \" google/flan-t5-small\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_name)\n```", "```py\nfrom peft import AdaLoraConfig\n# Example configuration; adjust parameters as needed\nadapter_config = AdaLoraConfig(target_r=16)\nmodel.add_adapter(adapter_config)\n```", "```py\ndemo_data = [{\n\"question\": \"What are the latest streetwear trends available at Stylesprint?\",\n  \"answer\": \"Stylesprint's latest streetwear collection includes hoodies, and graphic tees, all inspired by the latest hip-hop fashion trends.\"\n...\n}]\n```", "```py\nfrom torch.utils.data import Dataset\nclass StylesprintDataset(Dataset):\n   def __init__(self, tokenizer, data):\n       tokenizer.pad_token = tokenizer.eos_token\n       self.tokenizer = tokenizer\n       self.data = data\n```", "```py\nfrom transformers import Trainer, TrainingArguments\n# Split the mock dataset into training and evaluation sets (50/50)\ntrain_data = StylesprintDataset(\n    tokenizer, demo_data[:len(demo_data)//2])\neval_data = StylesprintDataset(\n    tokenizer, demo_data[len(demo_data)//2:])\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=10,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=64,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n)\n# Initialize the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=eval_data\n)\n# Start training\ntrainer.train()\n```", "```py\nimport torch\n# save parameters\nmodel.save_pretrained(\"./stylesprint_qa_model\")\ndef ask_question(model, question, context):\n   # Tokenize the question and context\n   inputs = tokenizer.encode_plus(question, context,\n        add_special_tokens=True, return_tensors=\"pt\")\n   # Get model predictions\n   with torch.no_grad():\n       outputs = model(**inputs)\n   # Get the start and end positions\n   answer_start_scores = outputs.start_logits\n   answer_end_scores = outputs.end_logits\n   # Find the tokens with the highest `start` and `end` scores\n   answer_start = torch.argmax(answer_start_scores)\n   answer_end = torch.argmax(answer_end_scores) + 1\n   # Convert the tokens to the answer string\n   answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(\n            inputs[\"input_ids\"][0][answer_start:answer_end]\n            )\n        )\n   return answer\nquestion = \"What is the return policy for online purchases?\"\ncontext = \"\"\"Excerpt from return policy returned from search.\"\"\"\nanswer = ask_question(model, question, context)\nprint(answer)\n```", "```py\nfrom sentence_transformers import SentenceTransformer, util\nimport pandas as pd\n# Example of a gold standard answer written by a human\ngs = \"Our policy at Stylesprint is to accept returns on online purchases within 30 days, with the condition that the items are unused and remain in their original condition.\"\n# Example of answer using GPT 3.5 with in-context learning reusing a relevant subset of the training data examples\ngpt_35 = \"Stylesprint accepts returns within 30 days of purchase, provided the items are unworn and in their original condition.\"\n# Load your dataset\ndataset = pd.DataFrame([\n   (gs, gpt_35, answer)\n])# pd.read_csv(\"dataset.csv\")\ndataset.columns = ['gold_standard_response',\n    'in_context_response', 'fine_tuned_response']\n# Load a pre-trained sentence transformer model\neval_model = SentenceTransformer('all-MiniLM-L6-v2')\n# Function to calculate semantic similarity\ndef calculate_semantic_similarity(model, response, gold_standard):\n    response_embedding = model.encode(\n        response, convert_to_tensor=True)\n    gold_standard_embedding = model.encode(gold_standard,\n        convert_to_tensor=True)\n    return util.pytorch_cos_sim(response_embedding,\n        gold_standard_embedding).item()\n# Measure semantic similarity\ndataset['in_context_similarity'] = dataset.apply(\n    lambda row:calculate_semantic_similarity(\n        eval_model, row['in_context_response'],\n        row['gold_standard_response']\n    ), axis=1)\ndataset['fine_tuned_similarity'] = dataset.apply(\n    lambda row:calculate_semantic_similarity(\n        eval_model, row['fine_tuned_response'],\n        row['gold_standard_response']\n    ), axis=1)\n# Print semantic similarity\nprint(\"Semantic similarity for in-context learning:\", \n    dataset['in_context_similarity'])\nprint(\"Semantic similarity for fine-tuned model:\", \n    dataset['fine_tuned_similarity'])\n```"]