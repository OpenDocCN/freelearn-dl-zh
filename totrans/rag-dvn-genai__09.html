<html><head></head><body>
  <div><h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-224" class="chapterTitle">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</h1>
    <p class="normal">An organization that continually increases the volume of its RAG data will reach the threshold of non-parametric data (not pretrained on an LLM). At that point, the mass of RAG data accumulated might become extremely challenging to manage, posing issues related to storage costs, retrieval resources, and the capacity of the generative AI models themselves. Moreover, a pretrained generative AI model is trained up to a cutoff date. The model ignores new knowledge starting the very next day. This means that it will be impossible for a user to interact with a chat model on the content of a newspaper edition published after the cutoff date. That is when retrieval has a key role to play in providing RAG-driven content.</p>
    <p class="normal">Companies like Google, Microsoft, Amazon, and other web giants may require exponential data and resources. Certain domains, such as the legal rulings in the United States, may indeed require vast amounts of data. However, this doesn’t apply to a wide range of domains. Many corporations do not need to maintain such large datasets, and in some cases, large portions of static data—like those in hard sciences—can remain stable for a long time. Such static data can be fine-tuned to reduce the volume of RAG data required.</p>
    <p class="normal">In this chapter, therefore, we will first examine the architecture of RAG data reduction through fine-tuning. We will focus on a dataset that contains ready-to-use documents but also stresses the human-feedback factor. We will demonstrate how to transform non-parametric data into parametric, fine-tuned data in an OpenAI model. Then, we will download and prepare the dataset from the previous chapter, converting the data into well-formatted prompt and completion pairs for fine-tuning in JSONL. We will fine-tune a cost-effective OpenAI model, <code class="inlineCode">GPT-4o-mini</code>, which will prove sufficient for the completion task we will implement. Once the model is fine-tuned, we will test it on our dataset to verify that it has successfully taken our data into account. Finally, we will explore OpenAI’s metrics interface, which enables us to monitor our technical metrics, such as accuracy and usage metrics, to assess the cost-effectiveness of our approach.</p>
    <p class="normal">To sum up, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">The limits of managing RAG data</li>
      <li class="bulletList">The challenge of determining what data to fine-tune</li>
      <li class="bulletList">Preparing a JSON dataset for fine-tuning</li>
      <li class="bulletList">Running OpenAI’s processing tool to produce a JSONL dataset</li>
      <li class="bulletList">Fine-tuning an OpenAI model</li>
      <li class="bulletList">Managing the fine-tuning processing time</li>
      <li class="bulletList">Running the fine-tuned model</li>
    </ul>
    <p class="normal">Let’s begin by defining the architecture of the fine-tuning process.</p>
    <h1 id="_idParaDest-225" class="heading-1">The architecture of fine-tuning static RAG data</h1>
    <p class="normal">In this<a id="_idIndexMarker548"/> section, we question the usage of non-parametric RAG data when it exceeds a manageable threshold, as described in the <em class="italic">RAG versus fine-tuning</em> section in <em class="chapterRef">Chapter 1</em>, <em class="italic">Why Retrieval Augmented Generation?</em>, which stated the principle of a threshold. <em class="italic">Figure 9.1</em> adapts the principle to this section:</p>
    <figure class="mediaobject"><img src="img/B31169_09_01.png" alt="Diagram of a diagram of a heater  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.1: Fine-tuning threshold reached for RAG data</p>
    <p class="normal">Notice <a id="_idIndexMarker549"/>that the processing (<strong class="keyWord">D2</strong>) and storage (<strong class="keyWord">D3</strong>) thresholds have been reached for static data versus the dynamic data in the RAG data environment. The threshold depends on each project and parameters such as:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The volume of RAG data to process</strong>: Embedding data requires human and machine resources. Even if we don’t embed the data, piling up static data (data that is stable over a long period of time) makes no sense.</li>
      <li class="bulletList"><strong class="keyWord">The volume of RAG data to store and retrieve</strong>: At some point, if we keep stacking data up, much of it may overlap.</li>
      <li class="bulletList"><strong class="keyWord">The retrievals require resources</strong>: Even if the system is open source, there is still an increasing number of resources to manage.</li>
    </ul>
    <p class="normal">Other factors, too, may come<a id="_idIndexMarker550"/> into play for each project. Whatever the reason, fine-tuning can be a good solution when we reach the RAG data threshold.</p>
    <h2 id="_idParaDest-226" class="heading-2">The RAG ecosystem</h2>
    <p class="normal">In this section, we will return to the <a id="_idIndexMarker551"/>RAG ecosystem described in <em class="chapterRef">Chapter 1</em>. We will focus on the specific components we need for this chapter. The following figure presents the fine-tuning components in color and the ones we will not need in gray:</p>
    <figure class="mediaobject"><img src="img/B31169_09_02.png" alt="A diagram of a diagram  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.2: Fine-tuning components of the RAG ecosystem</p>
    <p class="normal">The key features of the fine-tuning ecosystems we will build can be summarized in the following points:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Collecting (D1) and preparing (D2) the dataset</strong>: We will download and process the human-crafted crowdsourced SciQ hard science dataset we implemented in the previous chapter: <a href="https://huggingface.co/datasets/sciq">https://huggingface.co/datasets/sciq</a>.</li>
      <li class="bulletList"><strong class="keyWord">Human feedback (E2)</strong>: We can assume that human feedback played an important role in the SciQ hard science dataset. The dataset was controlled by humans and updated so we can think of it as a simulation of how reliable human feedback can be fine-tuned to alleviate the volume of RAG datasets. We can go further and say it is possible that, in real-life projects, the explanations present in the SciQ dataset can sometimes come from human evaluations of <a id="_idIndexMarker552"/>models, as we explored in <em class="chapterRef">Chapter 5</em>, <em class="italic">Boosting RAG Performance with Expert Human Feedback</em>.</li>
      <li class="bulletList"><strong class="keyWord">Fine-tuning (T2)</strong>: We will fine-tune a cost-effective OpenAI model, <code class="inlineCode">GPT-4o-mini</code>.</li>
      <li class="bulletList"><strong class="keyWord">Prompt engineering (G3) and generation and output (G4)</strong>: We will engineer the prompts as recommended by OpenAI and display the output.</li>
      <li class="bulletList"><strong class="keyWord">Metrics (E1)</strong>: We will look at the main features of OpenAI’s Metrics interface.</li>
    </ul>
    <p class="normal">Let’s now go to our keyboards to collect and process the SciQ dataset.</p>
    <h1 id="_idParaDest-227" class="heading-1">Installing the environment</h1>
    <p class="normal">Installing an environment<a id="_idIndexMarker553"/> has become complex with the rapid evolution of AI and cross-platform dependency conflicts, as we saw in <em class="chapterRef">Chapter 2</em>, <em class="italic">RAG Embedding Vector Stores with Deep Lake and OpenAI</em>, in the <em class="italic">Setting up the environment</em> section. We will thus freeze the package versions when possible.</p>
    <p class="normal">For this program, open the <code class="inlineCode">Fine_tuning_OpenAI_GPT_4o_mini.ipynb</code> notebook in the <code class="inlineCode">Chapter09</code> directory on GitHub. The program first retrieves the OpenAI API key:</p>
    <pre class="programlisting code"><code class="hljs-code">#You can retrieve your API key from a file(1)
# or enter it manually(2)
#Comment this cell if you want to enter your key manually.
#(1)Retrieve the API Key from a file
#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)
from google.colab import drive
drive.mount('/content/drive')
f = open("drive/MyDrive/files/api_key.txt", "r")
API_KEY=f.readline()
f.close()
</code></pre>
    <p class="normal">We then install <code class="inlineCode">openai</code> and set the API key:</p>
    <pre class="programlisting code"><code class="hljs-code">try:
  import openai
except:
  !pip install openai==1.42.0
  import openai
#(2) Enter your manually by
# replacing API_KEY by your key.
#The OpenAI Key
import os
os.environ['OPENAI_API_KEY'] =API_KEY
openai.api_key = os.getenv("OPENAI_API_KEY")
</code></pre>
    <p class="normal">Now, we install <code class="inlineCode">jsonlines</code> to generate JSONL data:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install jsonlines==4.0.0
</code></pre>
    <p class="normal">We now install <code class="inlineCode">datasets</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install datasets==2.20.0
</code></pre>
    <p class="normal">Read the <em class="italic">Installing the environment</em> section of <em class="chapterRef">Chapter 8</em>, <em class="italic">Dynamic RAG with Chroma and Hugging Face Llama</em>, for explanations of the dependency conflicts involved when installing <code class="inlineCode">datasets</code>.</p>
    <p class="normal">Some issues with the<a id="_idIndexMarker554"/> installation may occur but the dataset will be downloaded anyway. We must expect and accept such issues as the leading platforms continually update their packages and create conflicts with pre-installed environments such as Google Colab. You can create a special environment for this program. Bear in mind that your other programs might encounter issues due to other package constraints.</p>
    <p class="normal">We are now ready to prepare the dataset.</p>
    <h1 id="_idParaDest-228" class="heading-1">1. Preparing the dataset for fine-tuning</h1>
    <p class="normal">Fine-tuning an OpenAI<a id="_idIndexMarker555"/> model requires careful preparation; otherwise, the<a id="_idIndexMarker556"/> fine-tuning job will fail. In this section, we will carry out the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Download the dataset from Hugging Face and prepare it by processing its columns.</li>
      <li class="numberedList">Stream the dataset to a JSON file in JSONL format.</li>
    </ol>
    <p class="normal">The program begins by downloading the dataset.</p>
    <h2 id="_idParaDest-229" class="heading-2">1.1. Downloading and visualizing the dataset</h2>
    <p class="normal">We will download the <a id="_idIndexMarker557"/>SciQ dataset we embedded in <em class="chapterRef">Chapter 8</em>. As we saw, embedding thousands of documents takes time and resources. In this section, we will download the dataset, but this time, <em class="italic">we will not embed it</em>. We will let the OpenAI model handle that for us while fine-tuning the data.</p>
    <p class="normal">The program downloads the same Hugging Face dataset as in <em class="chapterRef">Chapter 8</em> and filters the training portion of the dataset to include only non-empty records with the correct answer and support text to explain the answer to the questions:</p>
    <pre class="programlisting code"><code class="hljs-code"># Import required libraries
from datasets import load_dataset
import pandas as pd
# Load the SciQ dataset from HuggingFace
dataset_view = load_dataset("sciq", split="train")
# Filter the dataset to include only questions with support and correct answer
filtered_dataset = dataset_view.filter(lambda x: x["support"] != "" and x["correct_answer"] != "")
# Print the number of questions with support
print("Number of questions with support: ", len(filtered_dataset))
</code></pre>
    <p class="normal">The preceding code then prints the number of filtered questions with support text. The output shows that we have a subset of 10,481 records:</p>
    <pre class="programlisting con"><code class="hljs-con">Number of questions with support:  10481
</code></pre>
    <p class="normal">Now, we will load the <a id="_idIndexMarker558"/>dataset to a DataFrame and drop the distractor columns (those with wrong answers to the questions):</p>
    <pre class="programlisting code"><code class="hljs-code"># Convert the filtered dataset to a pandas DataFrame
df_view = pd.DataFrame(filtered_dataset)
# Columns to drop
columns_to_drop = ['distractor3', 'distractor1', 'distractor2']
# Dropping the columns from the DataFrame
df_view = df.drop(columns=columns_to_drop)
# Display the DataFrame
df_view.head()
</code></pre>
    <p class="normal">The output displays the three columns we need:</p>
    <figure class="mediaobject"><img src="img/B31169_09_03.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.3: Output displaying three columns</p>
    <p class="normal">We need the question that will become the prompt. The <code class="inlineCode">correct_answer</code> and <code class="inlineCode">support</code> columns will be used for the completion. Now that we have examined the dataset, we can stream the dataset directly to a JSON file.</p>
    <h2 id="_idParaDest-230" class="heading-2">1.2. Preparing the dataset for fine-tuning</h2>
    <p class="normal">To train the<a id="_idIndexMarker559"/> completion model we will use, we need to write a<a id="_idIndexMarker560"/> JSON file in the very precise JSONL format as required.</p>
    <p class="normal">We download and process the dataset in the same way as we did to visualize it in the <em class="italic">1.1. Downloading and visualizing the dataset</em> section, which is recommended to check the dataset before fine-tuning it.</p>
    <p class="normal">We now write the messages for GPT-4o-mini in JSONL:</p>
    <pre class="programlisting code"><code class="hljs-code"># Prepare the data items for JSON lines file
items = []
for idx, row in df.iterrows():
    detailed_answer = row['correct_answer'] + " Explanation: " + row['support']
    items.append({
        "messages": [
            {"role": "system", "content": "Given a science question, provide the correct answer with a detailed explanation."},
            {"role": "user", "content": row['question']},
            {"role": "assistant", "content": detailed_answer}
        ]
    })
</code></pre>
    <p class="normal">We first define the detailed answer (<code class="inlineCode">detailed_answer</code>) with the correct answer (<code class="inlineCode">'correct_answer'</code>) and a supporting (<code class="inlineCode">support</code>) explanation.</p>
    <p class="normal">Then we define the messages (<code class="inlineCode">messages</code>) for the GPT-4o-mini model:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">{"role": "system", "content": ...}</code>: This sets the initial instruction for the language model, telling it to provide detailed answers to science questions.</li>
      <li class="bulletList"><code class="inlineCode">{"role": "user", "content": row['question']}</code>: This represents the user asking a question, taken from the <code class="inlineCode">question</code> column of the DataFrame.</li>
      <li class="bulletList"><code class="inlineCode">{"role": "assistant", "content": detailed_answer}</code>: This represents the assistant’s response, providing the detailed answer constructed earlier.</li>
    </ul>
    <p class="normal">We can <a id="_idIndexMarker561"/>now write our<a id="_idIndexMarker562"/> JSONL dataset to a file:</p>
    <pre class="programlisting code"><code class="hljs-code"># Write to JSON lines file
with jsonlines.open('/content/QA_prompts_and_completions.json', 'w') as writer:
    writer.write_all(items)
</code></pre>
    <p class="normal">We have given the OpenAI model a structure it expects and has been trained to understand. We can load the JSON file we just created in a pandas DataFrame to verify its content:</p>
    <pre class="programlisting code"><code class="hljs-code">dfile="/content/QA_prompts_and_completions.json"
import pandas as pd
# Load the data
df = pd.read_json(dfile, lines=True)
df
</code></pre>
    <p class="normal">The <a id="_idIndexMarker563"/>following excerpt of the file shows that we have successfully prepared the<a id="_idIndexMarker564"/> JSON file:</p>
    <figure class="mediaobject"><img src="img/B31169_09_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.4: File excerpt</p>
    <p class="normal">That’s it! We are now ready to run a fine-tuning job.</p>
    <h1 id="_idParaDest-231" class="heading-1">2. Fine-tuning the model</h1>
    <p class="normal">To train the <a id="_idIndexMarker565"/>model, we retrieve our training file and create a fine-tuning job. We begin by creating an OpenAI client:</p>
    <pre class="programlisting code"><code class="hljs-code">from openai import OpenAI
import jsonlines
client = OpenAI()
</code></pre>
    <p class="normal">Then we use the file we generated to create another training file that is uploaded to OpenAI:</p>
    <pre class="programlisting code"><code class="hljs-code"># Uploading the training file
result_file = client.files.create(
  file=open("QA_prompts_and_completions.json", "rb"),
  purpose="fine-tune"
)
</code></pre>
    <p class="normal">We print the file information for the dataset we are going to use for fine-tuning:</p>
    <pre class="programlisting code"><code class="hljs-code">print(result_file)
param_training_file_name = result_file.id
print(param_training_file_name)
</code></pre>
    <p class="normal">We now create and display the fine-tuning job:</p>
    <pre class="programlisting code"><code class="hljs-code"># Creating the fine-tuning job
 
ft_job = client.fine_tuning.jobs.create(
  training_file=param_training_file_name,
  model="gpt-4o-mini-2024-07-18"
)
# Printing the fine-tuning job
print(ft_job)
</code></pre>
    <p class="normal">The output first provides the name of the file, its purpose, its status, and the OpenAI name of the file ID:</p>
    <pre class="programlisting con"><code class="hljs-con">FileObject(id='file-EUPGmm1yAd3axrQ0pyoeAKuE', bytes=8062970, created_at=1725289249, filename='QA_prompts_and_completions.json', object='file', purpose='fine-tune', status='processed', status_details=None) file-EUPGmm1yAd3axrQ0pyoeAKuE
</code></pre>
    <p class="normal">The code displays<a id="_idIndexMarker566"/> the details of the fine-tuning job:</p>
    <pre class="programlisting con"><code class="hljs-con">FineTuningJob(id='ftjob-O1OEE7eEyFNJsO2Eu5otzWA8', created_at=1725289250, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-h2Kjmcir4wyGtqq1mJALLGIb', result_files=[], seed=1103096818, status='validating_files', trained_tokens=None, training_file='file-EUPGmm1yAd3axrQ0pyoeAKuE', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)
</code></pre>
    <p class="normal">The output provides the details we need to monitor the job. Here is a brief description of some of the key-value pairs in the output:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Job ID</code>: <code class="inlineCode">ftjob-O1OEE7eEyFNJsO2Eu5otzWA8</code>.</li>
      <li class="bulletList"><code class="inlineCode">Status</code>: <code class="inlineCode">validating_files</code>. This means OpenAI is currently checking the training file to make sure it’s suitable for fine-tuning.</li>
      <li class="bulletList"><code class="inlineCode">Model</code>: <code class="inlineCode">gpt-4o-mini-2024-07-18</code>. We’re using a smaller, more cost-effective version of GPT-4 for fine-tuning.</li>
      <li class="bulletList"><code class="inlineCode">Training File</code>: <code class="inlineCode">file-EUPGmm1yAd3axrQ0pyoeAKuE</code>. This is the file we’ve provided that contains the examples to teach the model.</li>
    </ul>
    <p class="normal">Some key hyperparameters are:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">n_epochs</code>: <code class="inlineCode">'auto'</code>: OpenAI will automatically determine the best number of training cycles.</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code>: <code class="inlineCode">'auto'</code>: OpenAI will automatically choose the optimal batch size for training.</li>
      <li class="bulletList"><code class="inlineCode">learning_rate_multiplier</code>: <code class="inlineCode">'auto'</code>: OpenAI will automatically adjust the learning rate during training.</li>
      <li class="bulletList"><code class="inlineCode">Created</code><code class="inlineCode"><a id="_idIndexMarker567"/></code><code class="inlineCode"> at</code>: <code class="inlineCode">2024-06-30 08:20:50</code>.</li>
    </ul>
    <p class="normal">This information will prove useful if you wish to perform an in-depth study of fine-tuning OpenAI models. We can also use it to monitor and manage our fine-tuning process.</p>
    <h2 id="_idParaDest-232" class="heading-2">2.1. Monitoring the fine-tunes</h2>
    <p class="normal">In this section, we will<a id="_idIndexMarker568"/> extract the minimum information we need to monitor the jobs for all our fine-tunes. We will first query OpenAI to obtain the three latest fine-tuning jobs:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
from openai import OpenAI
client = OpenAI()
# Assume client is already set up and authenticated
response = client.fine_tuning.jobs.list(limit=3) # increase to include your history
</code></pre>
    <p class="normal">We then initialize the lists of information we want to visualize:</p>
    <pre class="programlisting code"><code class="hljs-code"># Initialize lists to store the extracted data
job_ids = []
created_ats = []
statuses = []
models = []
training_files = []
error_messages = []
fine_tuned_models = [] # List to store the fine-tuned model names
</code></pre>
    <p class="normal">Following that, we iterate through <code class="inlineCode">response</code> to retrieve the information we need:</p>
    <pre class="programlisting code"><code class="hljs-code"># Iterate over the jobs in the response
for job in response.data:
    job_ids.append(job.id)
    created_ats.append(job.created_at)
    statuses.append(job.status)
    models.append(job.model)
    training_files.append(job.training_file)
    error_message = job.error.message if job.error else None
    error_messages.append(error_message)
# Append the fine-tuned model name
    fine_tuned_model = job.fine_tuned_model if hasattr(job, 'fine_tuned_model')
    else None
    fine_tuned_models.append(fine_tuned_model)
</code></pre>
    <p class="normal">We now create a <a id="_idIndexMarker569"/>DataFrame with the information we extracted:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
# Assume client is already set up and authenticated
response = client.fine_tuning.jobs.list(limit=3)
# Create a DataFrame
df = pd.DataFrame({
    'Job ID': job_ids,
    'Created At': created_ats,
    'Status': statuses,
    'Model': models,
    'Training File': training_files,
    'Error Message': error_messages,
    'Fine-Tuned Model': fine_tuned_models # Include the fine-tuned model names
})
</code></pre>
    <p class="normal">Finally, we convert the timestamps to readable format and display the list of fine-tunes and their status:</p>
    <pre class="programlisting code"><code class="hljs-code"># Convert timestamps to readable format
df['Created At'] = pd.to_datetime(df['Created At'], unit='s')
df = df.sort_values(by='Created At', ascending=False)
# Display the DataFrame
df
</code></pre>
    <p class="normal">The output provides a monitoring dashboard of the list of our jobs, as shown in <em class="italic">Figure 9.5</em>:</p>
    <figure class="mediaobject"><img src="img/B31169_09_05.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.5: Job list in the pandas DataFrame</p>
    <p class="normal">You can see that <a id="_idIndexMarker570"/>for job <code class="inlineCode">0</code>, the status of the task is <code class="inlineCode">running</code>. The status informs you of the different steps of the process such as validating the files, running, failed, or succeeded. In this case, the fine-tuning process is running. If you refresh this cell regularly, you will see the status.</p>
    <p class="normal">We will now retrieve the most recent model trained for the <code class="inlineCode">Fine-Tuned Model</code> column. If the training fails, this column will be empty. If not, we can retrieve it:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
generation=False  # until the current model is fine-tuned
# Attempt to find the first non-empty Fine-Tuned Model
non_empty_models = df[df['Fine-Tuned Model'].notna() &amp; (df['Fine-Tuned Model'] != '')]
if not non_empty_models.empty:
    first_non_empty_model = non_empty_models['Fine-Tuned Model'].iloc[0]
    print("The latest fine-tuned model is:", first_non_empty_model)
    generation=True
else:
    first_non_empty_model='None'
    print("No fine-tuned models found.")
# Display the first non-empty Fine-Tuned Model in the DataFrame
first_non_empty_model = df[df['Fine-Tuned Model'].notna() &amp; (df['Fine-Tuned Model'] != '')]['Fine-Tuned Model'].iloc[0]
print("The lastest fine-tuned model is:", first_non_empty_model)
</code></pre>
    <p class="normal">The output will display the name of the latest fine-tuned model if there is one or inform us that no fine-tuned model is found. In this case, GPT-4o-mini was successfully trained:</p>
    <pre class="programlisting con"><code class="hljs-con">The latest fine-tuned model is: ft:gpt-4o-mini-2024-07-18:personal::A32VfYIz
</code></pre>
    <p class="normal">If a fine-tuned model is found, <code class="inlineCode">generation=True</code>, it will trigger the OpenAI completion calls in the following cells. If no model is found, <code class="inlineCode">generation=False</code>, it will not run the OpenAI API in the rest of the notebook to avoid using models that you are not training. You can set generation to <code class="inlineCode">True</code> in a new cell and then select any fine-tuned model you wish.</p>
    <p class="normal">We know that the<a id="_idIndexMarker571"/> training job can take a while. You can refresh the pandas DataFrame from time to time. You can write code that checks the status of another job and waits for a name to appear for your training job or an error message. You can also wait for OpenAI to send you an email informing you that the training job is finished. If the training job fails, we must verify our training data for any inconsistencies, missing values, or incorrect labels. Additionally, ensure that the JSON file format adheres to OpenAI’s specified schema, including correct field names, data types, and structure.</p>
    <p class="normal">Once the training job is finished, we can run completion tasks.</p>
    <h1 id="_idParaDest-233" class="heading-1">3. Using the fine-tuned OpenAI model</h1>
    <p class="normal">We are now ready to use our <a id="_idIndexMarker572"/>fine-tuned OpenAI <code class="inlineCode">GPT-4o-mini</code> model. We will begin by defining a prompt based on a question taken from our initial dataset:</p>
    <pre class="programlisting code"><code class="hljs-code"># Define the prompt
prompt = "What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?"
</code></pre>
    <p class="normal">The goal is to verify whether the dataset has been properly trained and will produce results similar to the completions we defined. We can now run the fine-tuned model:</p>
    <pre class="programlisting code"><code class="hljs-code"># Assume first_non_empty_model is defined above this snippet
if generation==True:
    response = client.chat.completions.create(
        model=first_non_empty_model,
        temperature=0.0,  # Adjust as needed for variability
        messages=[
            {"role": "system", "content": "Given a question, reply with a complete explanation for students."},
            {"role": "user", "content": prompt}
        ]
    )
else:
    print("Error: Model is None, cannot proceed with the API request.")
</code></pre>
    <p class="normal">The parameters of the request must fit our scenario:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">model=first_non_empty_model</code> is our pretrained model.</li>
      <li class="bulletList"><code class="inlineCode">prompt=prompt</code> is our predefined prompt.</li>
      <li class="bulletList"><code class="inlineCode">temperature=0.0</code> is set to a low value because we do not want any “creativity” for this hard science completion task.</li>
    </ul>
    <p class="normal">Once we run the request, we can format and display the response. The following code contains two cells to display and extract the response.</p>
    <p class="normal">First, we can print the raw response:</p>
    <pre class="programlisting code"><code class="hljs-code">if generation==True:
  print(response)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker573"/>output contains the response and information on the process:</p>
    <pre class="programlisting con"><code class="hljs-con">ChatCompletion(id='chatcmpl-A32pvH9wLvNsSRmB1sUjxOW4Z6Xr6',…
</code></pre>
    <p class="normal">We then extract the text of the response:</p>
    <pre class="programlisting code"><code class="hljs-code">if (generation==True):
  # Access the response from the first choice
  response_text = response.choices[0].message.content
  # Print the response
  print(response_text)
</code></pre>
    <p class="normal">The output is a string:</p>
    <pre class="programlisting con"><code class="hljs-con">Coriolis effect Explanation: The Coriolis effect is…
</code></pre>
    <p class="normal">Finally, we can format the response string into a nice paragraph with the Python wrapper:</p>
    <pre class="programlisting code"><code class="hljs-code">import textwrap
if generation==True:
wrapped_text = textwrap.fill(response_text.strip(), 60)
print(wrapped_text)
</code></pre>
    <p class="normal">The output shows that our data has been taken into account:</p>
    <pre class="programlisting con"><code class="hljs-con">Coriolis effect Explanation: The Coriolis effect is a
phenomenon that causes moving objects, such as air and
water, to turn and twist in response to the rotation of the
Earth. It is responsible for the rotation of large weather
systems, such as hurricanes, and the direction of trade
winds and ocean currents. In the Northern Hemisphere, the
effect causes moving objects to turn to the right, while in
the Southern Hemisphere, objects turn to the left. The
Coriolis effect is proportional to the speed of the moving
object and the strength of the Earth's rotation, and it is
negligible for small-scale movements, such as water flowing
in a sink.
</code></pre>
    <p class="normal">Let’s look at the initial completion for our prompt:</p>
    <figure class="mediaobject"><img src="img/B31169_09_06.png" alt="A close up of text  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.6: Initial completion</p>
    <p class="normal">The<a id="_idIndexMarker574"/> response is thus satisfactory. This might not always be the case and might require more work on the datasets (better data, large volumes of data, etc.) incrementally until you have reached a satisfactory goal.</p>
    <p class="normal">You can save the name of your model in a text file or anywhere you wish. You can now run your model in another program using the name of your trained model, or you can reload this notebook at any time:</p>
    <ol>
      <li class="numberedList" value="1">Run the <code class="inlineCode">Installing the environment</code> section of this notebook.</li>
      <li class="numberedList">Define a prompt of your choice related to the dataset we trained.</li>
      <li class="numberedList">Enter the name of your model in the OpenAI completion request.</li>
      <li class="numberedList">Run the<a id="_idIndexMarker575"/> request and analyze the response.</li>
    </ol>
    <p class="normal">You can consult<a id="_idIndexMarker576"/> OpenAI’s fine-tuning documentation for further information if necessary: <a href="https://platform.openai.com/docs/guides/fine-tuning/fine-tuning">https://platform.openai.com/docs/guides/fine-tuning/fine-tuning</a>.</p>
    <h1 id="_idParaDest-234" class="heading-1">Metrics</h1>
    <p class="normal">OpenAI <a id="_idIndexMarker577"/>provides a user interface to analyze the metrics of the training process and model. You can access the metrics related to your fine-tuned models<a id="_idIndexMarker578"/> at <a href="https://platform.openai.com/finetune/">https://platform.openai.com/finetune/</a>.</p>
    <p class="normal">The interface displays the list of your fine-tuned jobs:</p>
    <figure class="mediaobject"> <img src="img/B31169_09_07.png" alt="A screenshot of a phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.7: List of fine-tuned jobs</p>
    <p class="normal">You can<a id="_idIndexMarker579"/> choose to view all the fine-tuning jobs, the ones that were successful, or the ones that failed. If we choose a job that was successful, for example, we can view the job details as shown in the following excerpt:</p>
    <figure class="mediaobject"> <img src="img/B31169_09_08.png" alt="A screenshot of a computer  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.8: Example view</p>
    <p class="normal">Let’s go through the information provided in this figure:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Status</strong>: Indicates the status of the fine-tuning process. In this case, we can see that the process was completed successfully.</li>
      <li class="bulletList"><strong class="keyWord">Job ID</strong>: A unique identifier for the fine-tuning job. This can be used to reference the job in queries or for support purposes.</li>
      <li class="bulletList"><strong class="keyWord">Base model</strong>: Specifies the pretrained model used as the starting point for fine-tuning. In this case, <code class="inlineCode">gpt-4o-mini</code> is a version of OpenAI’s models.</li>
      <li class="bulletList"><strong class="keyWord">Output model</strong>: This is the identifier for the model resulting from the fine-tuning. It incorporates changes and optimizations based on the specific training data provided.</li>
      <li class="bulletList"><strong class="keyWord">Created at</strong>: The date and time when the fine-tuning job was initiated.</li>
      <li class="bulletList"><strong class="keyWord">Trained tokens</strong>: The total number of tokens (pieces of text, such as words or punctuation) that were processed during training. This metric helps gauge the extent of training.</li>
      <li class="bulletList"><strong class="keyWord">Epochs</strong>: The number of complete passes the training data went through during fine-tuning. More epochs can lead to better learning but too many may lead to overfitting.</li>
      <li class="bulletList"><strong class="keyWord">Batch size</strong>: The number of training examples utilized in one iteration of model training. Smaller batch sizes can offer more updates and refined learning but may take longer to train.</li>
      <li class="bulletList"><strong class="keyWord">LR multiplier</strong>: This refers to the learning rate multiplier, affecting how much the learning rate for the base model is adjusted during the fine-tuning process. A smaller multiplier can lead to smaller, more conservative updates to model weights.</li>
      <li class="bulletList"><strong class="keyWord">Seed</strong>: A seed for the random number generator used in the training process. Providing a seed ensures that the training process is reproducible, meaning you can get the same results with the same input conditions.</li>
    </ul>
    <p class="normal">This information <a id="_idIndexMarker580"/>will help tailor the fine-tuning jobs to meet the specific needs of a project and explore alternative approaches to optimization and customization. In addition, the interface contains more information that we can explore to get an in-depth vision of the fine-tuning process. If we scroll down on the <strong class="screenText">Information</strong> tab of our model, we can see metrics as shown here:</p>
    <figure class="mediaobject"> <img src="img/B31169_09_09-01.png" alt="A green line graph with numbers  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.9: Metrics for a fine-tuned model</p>
    <p class="normal">Training<a id="_idIndexMarker581"/> loss and the other available information can guide our training strategies (data, files, and parameters).</p>
    <p class="normal"><strong class="screenText">Training loss</strong> is a<a id="_idIndexMarker582"/> reliable metric used to evaluate the performance of a machine learning model during training. In this case, <code class="inlineCode">Training loss (1.1570)</code> represents the model’s average error on the training dataset. Lower training loss values indicate that the model is better fitting the training data. A training loss of <code class="inlineCode">1.1570</code> suggests that the model has learned to predict or classify its training data well during the fine-tuning process.</p>
    <p class="normal">We can also examine these values with the <code class="inlineCode">Time</code> and <code class="inlineCode">Step</code> information:</p>
    <figure class="mediaobject"> <img src="img/B31169_09_10.png" alt="A screenshot of a phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.10: Training loss during the training job</p>
    <p class="normal">We must also measure the usage to monitor the cost per period and model. OpenAI provides a detailed interface at <a href="https://platform.openai.com/usage">https://platform.openai.com/usage</a>.</p>
    <p class="normal">Fine-tuning can indeed be an effective way to optimize RAG data if we make sure to train a model with high-quality data and the right parameters. Now, it’s time for us to summarize our journey and move to our next RAG-driven generative AI implementation.</p>
    <h1 id="_idParaDest-235" class="heading-1">Summary</h1>
    <p class="normal">This chapter’s goal was to show that as we accumulate RAG data, some data is dynamic and requires constant updates, and as such, cannot be fine-tuned easily. However, some data is static, meaning that it will remain stable for long periods of time. This data can become parametric (stored in the weights of a trained LLM).</p>
    <p class="normal">We first downloaded and processed the SciQ dataset, which contains hard science questions. This stable data perfectly suits fine-tuning. It contains a question, answer, and support (explanation) structure, which makes the data effective for fine-tuning. Also, we can assume human feedback was required. We can even go as far as imagining this feedback could be provided by analyzing generative AI model outputs.</p>
    <p class="normal">We converted the data we prepared into prompts and completions in a JSONL file following the recommendations of OpenAI’s preparation tool. The structure of JSONL was meant to be compatible with a completion model (prompt and completion) such as <code class="inlineCode">GPT-4o-mini</code>. The program then fine-tuned the cost-effective <code class="inlineCode">GPT-4o-mini</code> OpenAI model, following which we ran the model and found that the output was satisfactory. Finally, we explored the metrics of the fine-tuned model in the OpenAI metrics user interface.</p>
    <p class="normal">We can conclude that fine-tuning can optimize RAG data in certain cases when necessary. However, we will take this process further in the next chapter, <em class="chapterRef">Chapter 10</em>, <em class="italic">RAG for Video Stock Production with Pinecone and OpenAI</em>, when we run the full-blown RAG-driven generative AI ecosystem.</p>
    <h1 id="_idParaDest-236" class="heading-1">Questions</h1>
    <p class="normal">Answer the following questions with yes or no:</p>
    <ol>
      <li class="numberedList" value="1">Do all organizations need to manage large volumes of RAG data?</li>
      <li class="numberedList">Is the GPT-4o-mini model described as insufficient for fine-tuning tasks?</li>
      <li class="numberedList">Can pretrained models update their knowledge base after the cutoff date without retrieval systems?</li>
      <li class="numberedList">Is it the case that static data never changes and thus never requires updates?</li>
      <li class="numberedList">Is downloading data from Hugging Face the only source for preparing datasets?</li>
      <li class="numberedList">Is all RAG data eventually embedded into the trained model’s parameters according to the document?</li>
      <li class="numberedList">Does the chapter recommend using only new data for fine-tuning AI models?</li>
      <li class="numberedList">Is the OpenAI Metrics interface used to adjust the learning rate during model training?</li>
      <li class="numberedList">Can the fine-tuning process be effectively monitored using the OpenAI dashboard?</li>
      <li class="numberedList">Is human feedback deemed unnecessary in the preparation of hard science datasets such as SciQ?</li>
    </ol>
    <h1 id="_idParaDest-237" class="heading-1">References</h1>
    <ul>
      <li class="bulletList">OpenAI fine-tuning documentation: <a href="https://platform.openai.com/docs/guides/fine-tuning/">https://platform.openai.com/docs/guides/fine-tuning/</a></li>
      <li class="bulletList">OpenAI pricing: <a href="https://openai.com/api/pricing/">https://openai.com/api/pricing/</a></li>
    </ul>
    <h1 id="_idParaDest-238" class="heading-1">Further reading</h1>
    <ul>
      <li class="bulletList"><em class="italic">Test of Fine-Tuning GPT by Astrophysical Data</em> by Yu Wang et al. is an interesting article on fine-tuning hard science data, which requires careful data preparation: <a href="https://arxiv.org/pdf/2404.10019">https://arxiv.org/pdf/2404.10019</a></li>
    </ul>
    <h1 id="_idParaDest-239" class="heading-1">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
    <p class="normal"><a href="https://www.packt.link/rag">https://www.packt.link/rag</a></p>
    <p class="normal"><img src="img/QR_Code50409000288080484.png" alt=""/></p>
  </div>
</body></html>