<html><head></head><body>
  <div id="_idContainer102" class="Basic-Text-Frame">
    <h1 class="chapterNumber"><span class="koboSpan" id="kobo.1.1">9</span></h1>
    <h1 id="_idParaDest-224" class="chapterTitle"><span class="koboSpan" id="kobo.2.1">Empowering AI Models: Fine-Tuning RAG Data and Human Feedback</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.3.1">An organization that continually increases the volume of its RAG data will reach the threshold of non-parametric data (not pretrained on an LLM). </span><span class="koboSpan" id="kobo.3.2">At that point, the mass of RAG data accumulated might become extremely challenging to manage, posing issues related to storage costs, retrieval resources, and the capacity of the generative AI models themselves. </span><span class="koboSpan" id="kobo.3.3">Moreover, a pretrained generative AI model is trained up to a cutoff date. </span><span class="koboSpan" id="kobo.3.4">The model ignores new knowledge starting the very next day. </span><span class="koboSpan" id="kobo.3.5">This means that it will be impossible for a user to interact with a chat model on the content of a newspaper edition published after the cutoff date. </span><span class="koboSpan" id="kobo.3.6">That is when retrieval has a key role to play in providing RAG-driven content.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.4.1">Companies like Google, Microsoft, Amazon, and other web giants may require exponential data and resources. </span><span class="koboSpan" id="kobo.4.2">Certain domains, such as the legal rulings in the United States, may indeed require vast amounts of data. </span><span class="koboSpan" id="kobo.4.3">However, this doesn’t apply to a wide range of domains. </span><span class="koboSpan" id="kobo.4.4">Many corporations do not need to maintain such large datasets, and in some cases, large portions of static data—like those in hard sciences—can remain stable for a long time. </span><span class="koboSpan" id="kobo.4.5">Such static data can be fine-tuned to reduce the volume of RAG data required.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.5.1">In this chapter, therefore, we will first examine the architecture of RAG data reduction through fine-tuning. </span><span class="koboSpan" id="kobo.5.2">We will focus on a dataset that contains ready-to-use documents but also stresses the human-feedback factor. </span><span class="koboSpan" id="kobo.5.3">We will demonstrate how to transform non-parametric data into parametric, fine-tuned data in an OpenAI model. </span><span class="koboSpan" id="kobo.5.4">Then, we will download and prepare the dataset from the previous chapter, converting the data into well-formatted prompt and completion pairs for fine-tuning in JSONL. </span><span class="koboSpan" id="kobo.5.5">We will fine-tune a cost-effective OpenAI model, </span><code class="inlineCode"><span class="koboSpan" id="kobo.6.1">GPT-4o-mini</span></code><span class="koboSpan" id="kobo.7.1">, which will prove sufficient for the completion task we will implement. </span><span class="koboSpan" id="kobo.7.2">Once the model is fine-tuned, we will test it on our dataset to verify that it has successfully taken our data into account. </span><span class="koboSpan" id="kobo.7.3">Finally, we will explore OpenAI’s metrics interface, which enables us to monitor our technical metrics, such as accuracy and usage metrics, to assess the cost-effectiveness of our approach.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.8.1">To sum up, this chapter covers the following topics:</span></p>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.9.1">The limits of managing RAG data</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.10.1">The challenge of determining what data to fine-tune</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.11.1">Preparing a JSON dataset for fine-tuning</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.12.1">Running OpenAI’s processing tool to produce a JSONL dataset</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.13.1">Fine-tuning an OpenAI model</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.14.1">Managing the fine-tuning processing time</span></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.15.1">Running the fine-tuned model</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.16.1">Let’s begin by defining the architecture of the fine-tuning process.</span></p>
    <h1 id="_idParaDest-225" class="heading-1"><span class="koboSpan" id="kobo.17.1">The architecture of fine-tuning static RAG data</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.18.1">In this</span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.19.1"> section, we question the usage of non-parametric RAG data when it exceeds a manageable threshold, as described in the </span><em class="italic"><span class="koboSpan" id="kobo.20.1">RAG versus fine-tuning</span></em><span class="koboSpan" id="kobo.21.1"> section in </span><em class="chapterRef"><span class="koboSpan" id="kobo.22.1">Chapter 1</span></em><span class="koboSpan" id="kobo.23.1">, </span><em class="italic"><span class="koboSpan" id="kobo.24.1">Why Retrieval Augmented Generation?</span></em><span class="koboSpan" id="kobo.25.1">, which stated the principle of a threshold. </span><em class="italic"><span class="koboSpan" id="kobo.26.1">Figure 9.1</span></em><span class="koboSpan" id="kobo.27.1"> adapts the principle to this section:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.28.1"><img src="../Images/B31169_09_01.png" alt="Diagram of a diagram of a heater  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.29.1">Figure 9.1: Fine-tuning threshold reached for RAG data</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.30.1">Notice </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.31.1">that the processing (</span><strong class="keyWord"><span class="koboSpan" id="kobo.32.1">D2</span></strong><span class="koboSpan" id="kobo.33.1">) and storage (</span><strong class="keyWord"><span class="koboSpan" id="kobo.34.1">D3</span></strong><span class="koboSpan" id="kobo.35.1">) thresholds have been reached for static data versus the dynamic data in the RAG data environment. </span><span class="koboSpan" id="kobo.35.2">The threshold depends on each project and parameters such as:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.36.1">The volume of RAG data to process</span></strong><span class="koboSpan" id="kobo.37.1">: Embedding data requires human and machine resources. </span><span class="koboSpan" id="kobo.37.2">Even if we don’t embed the data, piling up static data (data that is stable over a long period of time) makes no sense.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.38.1">The volume of RAG data to store and retrieve</span></strong><span class="koboSpan" id="kobo.39.1">: At some point, if we keep stacking data up, much of it may overlap.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.40.1">The retrievals require resources</span></strong><span class="koboSpan" id="kobo.41.1">: Even if the system is open source, there is still an increasing number of resources to manage.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.42.1">Other factors, too, may come</span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.43.1"> into play for each project. </span><span class="koboSpan" id="kobo.43.2">Whatever the reason, fine-tuning can be a good solution when we reach the RAG data threshold.</span></p>
    <h2 id="_idParaDest-226" class="heading-2"><span class="koboSpan" id="kobo.44.1">The RAG ecosystem</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.45.1">In this section, we will return to the </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.46.1">RAG ecosystem described in </span><em class="chapterRef"><span class="koboSpan" id="kobo.47.1">Chapter 1</span></em><span class="koboSpan" id="kobo.48.1">. </span><span class="koboSpan" id="kobo.48.2">We will focus on the specific components we need for this chapter. </span><span class="koboSpan" id="kobo.48.3">The following figure presents the fine-tuning components in color and the ones we will not need in gray:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.49.1"><img src="../Images/B31169_09_02.png" alt="A diagram of a diagram  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.50.1">Figure 9.2: Fine-tuning components of the RAG ecosystem</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.51.1">The key features of the fine-tuning ecosystems we will build can be summarized in the following points:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.52.1">Collecting (D1) and preparing (D2) the dataset</span></strong><span class="koboSpan" id="kobo.53.1">: We will download and process the human-crafted crowdsourced SciQ hard science dataset we implemented in the previous chapter: </span><a href="https://huggingface.co/datasets/sciq"><span class="url"><span class="koboSpan" id="kobo.54.1">https://huggingface.co/datasets/sciq</span></span></a><span class="koboSpan" id="kobo.55.1">.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.56.1">Human feedback (E2)</span></strong><span class="koboSpan" id="kobo.57.1">: We can assume that human feedback played an important role in the SciQ hard science dataset. </span><span class="koboSpan" id="kobo.57.2">The dataset was controlled by humans and updated so we can think of it as a simulation of how reliable human feedback can be fine-tuned to alleviate the volume of RAG datasets. </span><span class="koboSpan" id="kobo.57.3">We can go further and say it is possible that, in real-life projects, the explanations present in the SciQ dataset can sometimes come from human evaluations of </span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.58.1">models, as we explored in </span><em class="chapterRef"><span class="koboSpan" id="kobo.59.1">Chapter 5</span></em><span class="koboSpan" id="kobo.60.1">, </span><em class="italic"><span class="koboSpan" id="kobo.61.1">Boosting RAG Performance with Expert Human Feedback</span></em><span class="koboSpan" id="kobo.62.1">.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.63.1">Fine-tuning (T2)</span></strong><span class="koboSpan" id="kobo.64.1">: We will fine-tune a cost-effective OpenAI model, </span><code class="inlineCode"><span class="koboSpan" id="kobo.65.1">GPT-4o-mini</span></code><span class="koboSpan" id="kobo.66.1">.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.67.1">Prompt engineering (G3) and generation and output (G4)</span></strong><span class="koboSpan" id="kobo.68.1">: We will engineer the prompts as recommended by OpenAI and display the output.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.69.1">Metrics (E1)</span></strong><span class="koboSpan" id="kobo.70.1">: We will look at the main features of OpenAI’s Metrics interface.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.71.1">Let’s now go to our keyboards to collect and process the SciQ dataset.</span></p>
    <h1 id="_idParaDest-227" class="heading-1"><span class="koboSpan" id="kobo.72.1">Installing the environment</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.73.1">Installing an environment</span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.74.1"> has become complex with the rapid evolution of AI and cross-platform dependency conflicts, as we saw in </span><em class="chapterRef"><span class="koboSpan" id="kobo.75.1">Chapter 2</span></em><span class="koboSpan" id="kobo.76.1">, </span><em class="italic"><span class="koboSpan" id="kobo.77.1">RAG Embedding Vector Stores with Deep Lake and OpenAI</span></em><span class="koboSpan" id="kobo.78.1">, in the </span><em class="italic"><span class="koboSpan" id="kobo.79.1">Setting up the environment</span></em><span class="koboSpan" id="kobo.80.1"> section. </span><span class="koboSpan" id="kobo.80.2">We will thus freeze the package versions when possible.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.81.1">For this program, open the </span><code class="inlineCode"><span class="koboSpan" id="kobo.82.1">Fine_tuning_OpenAI_GPT_4o_mini.ipynb</span></code><span class="koboSpan" id="kobo.83.1"> notebook in the </span><code class="inlineCode"><span class="koboSpan" id="kobo.84.1">Chapter09</span></code><span class="koboSpan" id="kobo.85.1"> directory on GitHub. </span><span class="koboSpan" id="kobo.85.2">The program first retrieves the OpenAI API key:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.86.1">#You can retrieve your API key from a file(1)</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.87.1"># or enter it manually(2)</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.88.1">#Comment this cell if you want to enter your key manually.</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.89.1">#(1)Retrieve the API Key from a file</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.90.1">#Store you key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.91.1">from</span></span><span class="koboSpan" id="kobo.92.1"> google.colab </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.93.1">import</span></span><span class="koboSpan" id="kobo.94.1"> drive
drive.mount(</span><span class="hljs-string"><span class="koboSpan" id="kobo.95.1">'/content/drive'</span></span><span class="koboSpan" id="kobo.96.1">)
f = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.97.1">open</span></span><span class="koboSpan" id="kobo.98.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.99.1">"drive/MyDrive/files/api_key.txt"</span></span><span class="koboSpan" id="kobo.100.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.101.1">"r"</span></span><span class="koboSpan" id="kobo.102.1">)
API_KEY=f.readline()
f.close()
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.103.1">We then install </span><code class="inlineCode"><span class="koboSpan" id="kobo.104.1">openai</span></code><span class="koboSpan" id="kobo.105.1"> and set the API key:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.106.1">try</span></span><span class="koboSpan" id="kobo.107.1">:
  </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.108.1">import</span></span><span class="koboSpan" id="kobo.109.1"> openai
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.110.1">except</span></span><span class="koboSpan" id="kobo.111.1">:
  !pip install openai==</span><span class="hljs-number"><span class="koboSpan" id="kobo.112.1">1.42.0</span></span>
  <span class="hljs-keyword"><span class="koboSpan" id="kobo.113.1">import</span></span><span class="koboSpan" id="kobo.114.1"> openai
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.115.1">#(2) Enter your manually by</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.116.1"># replacing API_KEY by your key.</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.117.1">#The OpenAI Key</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.118.1">import</span></span><span class="koboSpan" id="kobo.119.1"> os
os.environ[</span><span class="hljs-string"><span class="koboSpan" id="kobo.120.1">'OPENAI_API_KEY'</span></span><span class="koboSpan" id="kobo.121.1">] =API_KEY
openai.api_key = os.getenv(</span><span class="hljs-string"><span class="koboSpan" id="kobo.122.1">"OPENAI_API_KEY"</span></span><span class="koboSpan" id="kobo.123.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.124.1">Now, we install </span><code class="inlineCode"><span class="koboSpan" id="kobo.125.1">jsonlines</span></code><span class="koboSpan" id="kobo.126.1"> to generate JSONL data:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.127.1">!pip install jsonlines==</span><span class="hljs-number"><span class="koboSpan" id="kobo.128.1">4.0.0</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.129.1">We now install </span><code class="inlineCode"><span class="koboSpan" id="kobo.130.1">datasets</span></code><span class="koboSpan" id="kobo.131.1">:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.132.1">!pip install datasets==</span><span class="hljs-number"><span class="koboSpan" id="kobo.133.1">2.20.0</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.134.1">Read the </span><em class="italic"><span class="koboSpan" id="kobo.135.1">Installing the environment</span></em><span class="koboSpan" id="kobo.136.1"> section of </span><em class="chapterRef"><span class="koboSpan" id="kobo.137.1">Chapter 8</span></em><span class="koboSpan" id="kobo.138.1">, </span><em class="italic"><span class="koboSpan" id="kobo.139.1">Dynamic RAG with Chroma and Hugging Face Llama</span></em><span class="koboSpan" id="kobo.140.1">, for explanations of the dependency conflicts involved when installing </span><code class="inlineCode"><span class="koboSpan" id="kobo.141.1">datasets</span></code><span class="koboSpan" id="kobo.142.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.143.1">Some issues with the</span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.144.1"> installation may occur but the dataset will be downloaded anyway. </span><span class="koboSpan" id="kobo.144.2">We must expect and accept such issues as the leading platforms continually update their packages and create conflicts with pre-installed environments such as Google Colab. </span><span class="koboSpan" id="kobo.144.3">You can create a special environment for this program. </span><span class="koboSpan" id="kobo.144.4">Bear in mind that your other programs might encounter issues due to other package constraints.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.145.1">We are now ready to prepare the dataset.</span></p>
    <h1 id="_idParaDest-228" class="heading-1"><span class="koboSpan" id="kobo.146.1">1. </span><span class="koboSpan" id="kobo.146.2">Preparing the dataset for fine-tuning</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.147.1">Fine-tuning an OpenAI</span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.148.1"> model requires careful preparation; otherwise, the</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.149.1"> fine-tuning job will fail. </span><span class="koboSpan" id="kobo.149.2">In this section, we will carry out the following steps:</span></p>
    <ol>
      <li class="numberedList" value="1"><span class="koboSpan" id="kobo.150.1">Download the dataset from Hugging Face and prepare it by processing its columns.</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.151.1">Stream the dataset to a JSON file in JSONL format.</span></li>
    </ol>
    <p class="normal"><span class="koboSpan" id="kobo.152.1">The program begins by downloading the dataset.</span></p>
    <h2 id="_idParaDest-229" class="heading-2"><span class="koboSpan" id="kobo.153.1">1.1. </span><span class="koboSpan" id="kobo.153.2">Downloading and visualizing the dataset</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.154.1">We will download the </span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.155.1">SciQ dataset we embedded in </span><em class="chapterRef"><span class="koboSpan" id="kobo.156.1">Chapter 8</span></em><span class="koboSpan" id="kobo.157.1">. </span><span class="koboSpan" id="kobo.157.2">As we saw, embedding thousands of documents takes time and resources. </span><span class="koboSpan" id="kobo.157.3">In this section, we will download the dataset, but this time, </span><em class="italic"><span class="koboSpan" id="kobo.158.1">we will not embed it</span></em><span class="koboSpan" id="kobo.159.1">. </span><span class="koboSpan" id="kobo.159.2">We will let the OpenAI model handle that for us while fine-tuning the data.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.160.1">The program downloads the same Hugging Face dataset as in </span><em class="chapterRef"><span class="koboSpan" id="kobo.161.1">Chapter 8</span></em><span class="koboSpan" id="kobo.162.1"> and filters the training portion of the dataset to include only non-empty records with the correct answer and support text to explain the answer to the questions:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.163.1"># Import required libraries</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.164.1">from</span></span><span class="koboSpan" id="kobo.165.1"> datasets </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.166.1">import</span></span><span class="koboSpan" id="kobo.167.1"> load_dataset
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.168.1">import</span></span><span class="koboSpan" id="kobo.169.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.170.1">as</span></span><span class="koboSpan" id="kobo.171.1"> pd
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.172.1"># Load the SciQ dataset from HuggingFace</span></span><span class="koboSpan" id="kobo.173.1">
dataset_view = load_dataset(</span><span class="hljs-string"><span class="koboSpan" id="kobo.174.1">"sciq"</span></span><span class="koboSpan" id="kobo.175.1">, split=</span><span class="hljs-string"><span class="koboSpan" id="kobo.176.1">"train"</span></span><span class="koboSpan" id="kobo.177.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.178.1"># Filter the dataset to include only questions with support and correct answer</span></span><span class="koboSpan" id="kobo.179.1">
filtered_dataset = dataset_view.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.180.1">filter</span></span><span class="koboSpan" id="kobo.181.1">(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.182.1">lambda</span></span><span class="koboSpan" id="kobo.183.1"> x: x[</span><span class="hljs-string"><span class="koboSpan" id="kobo.184.1">"support"</span></span><span class="koboSpan" id="kobo.185.1">] != </span><span class="hljs-string"><span class="koboSpan" id="kobo.186.1">""</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.187.1">and</span></span><span class="koboSpan" id="kobo.188.1"> x[</span><span class="hljs-string"><span class="koboSpan" id="kobo.189.1">"correct_answer"</span></span><span class="koboSpan" id="kobo.190.1">] != </span><span class="hljs-string"><span class="koboSpan" id="kobo.191.1">""</span></span><span class="koboSpan" id="kobo.192.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.193.1"># Print the number of questions with support</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.194.1">print</span></span><span class="koboSpan" id="kobo.195.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.196.1">"Number of questions with support: "</span></span><span class="koboSpan" id="kobo.197.1">, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.198.1">len</span></span><span class="koboSpan" id="kobo.199.1">(filtered_dataset))
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.200.1">The preceding code then prints the number of filtered questions with support text. </span><span class="koboSpan" id="kobo.200.2">The output shows that we have a subset of 10,481 records:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.201.1">Number of questions with support:  10481
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.202.1">Now, we will load the </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.203.1">dataset to a DataFrame and drop the distractor columns (those with wrong answers to the questions):</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.204.1"># Convert the filtered dataset to a pandas DataFrame</span></span><span class="koboSpan" id="kobo.205.1">
df_view = pd.DataFrame(filtered_dataset)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.206.1"># Columns to drop</span></span><span class="koboSpan" id="kobo.207.1">
columns_to_drop = [</span><span class="hljs-string"><span class="koboSpan" id="kobo.208.1">'distractor3'</span></span><span class="koboSpan" id="kobo.209.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.210.1">'distractor1'</span></span><span class="koboSpan" id="kobo.211.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.212.1">'distractor2'</span></span><span class="koboSpan" id="kobo.213.1">]
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.214.1"># Dropping the columns from the DataFrame</span></span><span class="koboSpan" id="kobo.215.1">
df_view = df.drop(columns=columns_to_drop)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.216.1"># Display the DataFrame</span></span><span class="koboSpan" id="kobo.217.1">
df_view.head()
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.218.1">The output displays the three columns we need:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.219.1"><img src="../Images/B31169_09_03.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.220.1">Figure 9.3: Output displaying three columns</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.221.1">We need the question that will become the prompt. </span><span class="koboSpan" id="kobo.221.2">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.222.1">correct_answer</span></code><span class="koboSpan" id="kobo.223.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.224.1">support</span></code><span class="koboSpan" id="kobo.225.1"> columns will be used for the completion. </span><span class="koboSpan" id="kobo.225.2">Now that we have examined the dataset, we can stream the dataset directly to a JSON file.</span></p>
    <h2 id="_idParaDest-230" class="heading-2"><span class="koboSpan" id="kobo.226.1">1.2. </span><span class="koboSpan" id="kobo.226.2">Preparing the dataset for fine-tuning</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.227.1">To train the</span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.228.1"> completion model we will use, we need to write a</span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.229.1"> JSON file in the very precise JSONL format as required.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.230.1">We download and process the dataset in the same way as we did to visualize it in the </span><em class="italic"><span class="koboSpan" id="kobo.231.1">1.1. </span><span class="koboSpan" id="kobo.231.2">Downloading and visualizing the dataset</span></em><span class="koboSpan" id="kobo.232.1"> section, which is recommended to check the dataset before fine-tuning it.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.233.1">We now write the messages for GPT-4o-mini in JSONL:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.234.1"># Prepare the data items for JSON lines file</span></span><span class="koboSpan" id="kobo.235.1">
items = []
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.236.1">for</span></span><span class="koboSpan" id="kobo.237.1"> idx, row </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.238.1">in</span></span><span class="koboSpan" id="kobo.239.1"> df.iterrows():
    detailed_answer = row[</span><span class="hljs-string"><span class="koboSpan" id="kobo.240.1">'correct_answer'</span></span><span class="koboSpan" id="kobo.241.1">] + </span><span class="hljs-string"><span class="koboSpan" id="kobo.242.1">" Explanation: "</span></span><span class="koboSpan" id="kobo.243.1"> + row[</span><span class="hljs-string"><span class="koboSpan" id="kobo.244.1">'support'</span></span><span class="koboSpan" id="kobo.245.1">]
    items.append({
        </span><span class="hljs-string"><span class="koboSpan" id="kobo.246.1">"messages"</span></span><span class="koboSpan" id="kobo.247.1">: [
            {</span><span class="hljs-string"><span class="koboSpan" id="kobo.248.1">"role"</span></span><span class="koboSpan" id="kobo.249.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.250.1">"system"</span></span><span class="koboSpan" id="kobo.251.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.252.1">"content"</span></span><span class="koboSpan" id="kobo.253.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.254.1">"Given a science question, provide the correct answer with a detailed explanation."</span></span><span class="koboSpan" id="kobo.255.1">},
            {</span><span class="hljs-string"><span class="koboSpan" id="kobo.256.1">"role"</span></span><span class="koboSpan" id="kobo.257.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.258.1">"user"</span></span><span class="koboSpan" id="kobo.259.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.260.1">"content"</span></span><span class="koboSpan" id="kobo.261.1">: row[</span><span class="hljs-string"><span class="koboSpan" id="kobo.262.1">'question'</span></span><span class="koboSpan" id="kobo.263.1">]},
            {</span><span class="hljs-string"><span class="koboSpan" id="kobo.264.1">"role"</span></span><span class="koboSpan" id="kobo.265.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.266.1">"assistant"</span></span><span class="koboSpan" id="kobo.267.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.268.1">"content"</span></span><span class="koboSpan" id="kobo.269.1">: detailed_answer}
        ]
    })
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.270.1">We first define the detailed answer (</span><code class="inlineCode"><span class="koboSpan" id="kobo.271.1">detailed_answer</span></code><span class="koboSpan" id="kobo.272.1">) with the correct answer (</span><code class="inlineCode"><span class="koboSpan" id="kobo.273.1">'correct_answer'</span></code><span class="koboSpan" id="kobo.274.1">) and a supporting (</span><code class="inlineCode"><span class="koboSpan" id="kobo.275.1">support</span></code><span class="koboSpan" id="kobo.276.1">) explanation.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.277.1">Then we define the messages (</span><code class="inlineCode"><span class="koboSpan" id="kobo.278.1">messages</span></code><span class="koboSpan" id="kobo.279.1">) for the GPT-4o-mini model:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.280.1">{"role": "system", "content": ...}</span></code><span class="koboSpan" id="kobo.281.1">: This sets the initial instruction for the language model, telling it to provide detailed answers to science questions.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.282.1">{"role": "user", "content": row['question']}</span></code><span class="koboSpan" id="kobo.283.1">: This represents the user asking a question, taken from the </span><code class="inlineCode"><span class="koboSpan" id="kobo.284.1">question</span></code><span class="koboSpan" id="kobo.285.1"> column of the DataFrame.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.286.1">{"role": "assistant", "content": detailed_answer}</span></code><span class="koboSpan" id="kobo.287.1">: This represents the assistant’s response, providing the detailed answer constructed earlier.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.288.1">We can </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.289.1">now write our</span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.290.1"> JSONL dataset to a file:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.291.1"># Write to JSON lines file</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.292.1">with</span></span><span class="koboSpan" id="kobo.293.1"> jsonlines.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.294.1">open</span></span><span class="koboSpan" id="kobo.295.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.296.1">'/content/QA_prompts_and_completions.json'</span></span><span class="koboSpan" id="kobo.297.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.298.1">'w'</span></span><span class="koboSpan" id="kobo.299.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.300.1">as</span></span><span class="koboSpan" id="kobo.301.1"> writer:
    writer.write_all(items)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.302.1">We have given the OpenAI model a structure it expects and has been trained to understand. </span><span class="koboSpan" id="kobo.302.2">We can load the JSON file we just created in a pandas DataFrame to verify its content:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="koboSpan" id="kobo.303.1">dfile=</span><span class="hljs-string"><span class="koboSpan" id="kobo.304.1">"/content/QA_prompts_and_completions.json"</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.305.1">import</span></span><span class="koboSpan" id="kobo.306.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.307.1">as</span></span><span class="koboSpan" id="kobo.308.1"> pd
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.309.1"># Load the data</span></span><span class="koboSpan" id="kobo.310.1">
df = pd.read_json(dfile, lines=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.311.1">True</span></span><span class="koboSpan" id="kobo.312.1">)
df
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.313.1">The </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.314.1">following excerpt of the file shows that we have successfully prepared the</span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.315.1"> JSON file:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.316.1"><img src="../Images/B31169_09_04.png" alt=""/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.317.1">Figure 9.4: File excerpt</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.318.1">That’s it! </span><span class="koboSpan" id="kobo.318.2">We are now ready to run a fine-tuning job.</span></p>
    <h1 id="_idParaDest-231" class="heading-1"><span class="koboSpan" id="kobo.319.1">2. </span><span class="koboSpan" id="kobo.319.2">Fine-tuning the model</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.320.1">To train the </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.321.1">model, we retrieve our training file and create a fine-tuning job. </span><span class="koboSpan" id="kobo.321.2">We begin by creating an OpenAI client:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.322.1">from</span></span><span class="koboSpan" id="kobo.323.1"> openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.324.1">import</span></span><span class="koboSpan" id="kobo.325.1"> OpenAI
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.326.1">import</span></span><span class="koboSpan" id="kobo.327.1"> jsonlines
client = OpenAI()
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.328.1">Then we use the file we generated to create another training file that is uploaded to OpenAI:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.329.1"># Uploading the training file</span></span><span class="koboSpan" id="kobo.330.1">
result_file = client.files.create(
  file=</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.331.1">open</span></span><span class="koboSpan" id="kobo.332.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.333.1">"QA_prompts_and_completions.json"</span></span><span class="koboSpan" id="kobo.334.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.335.1">"rb"</span></span><span class="koboSpan" id="kobo.336.1">),
  purpose=</span><span class="hljs-string"><span class="koboSpan" id="kobo.337.1">"fine-tune"</span></span><span class="koboSpan" id="kobo.338.1">
)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.339.1">We print the file information for the dataset we are going to use for fine-tuning:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.340.1">print</span></span><span class="koboSpan" id="kobo.341.1">(result_file)
param_training_file_name = result_file.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.342.1">id</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.343.1">print</span></span><span class="koboSpan" id="kobo.344.1">(param_training_file_name)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.345.1">We now create and display the fine-tuning job:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.346.1"># Creating the fine-tuning job</span></span><span class="koboSpan" id="kobo.347.1">
 
ft_job = client.fine_tuning.jobs.create(
  training_file=param_training_file_name,
  model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.348.1">"gpt-4o-mini-2024-07-18"</span></span><span class="koboSpan" id="kobo.349.1">
)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.350.1"># Printing the fine-tuning job</span></span>
<span class="hljs-built_in"><span class="koboSpan" id="kobo.351.1">print</span></span><span class="koboSpan" id="kobo.352.1">(ft_job)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.353.1">The output first provides the name of the file, its purpose, its status, and the OpenAI name of the file ID:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.354.1">FileObject(id='file-EUPGmm1yAd3axrQ0pyoeAKuE', bytes=8062970, created_at=1725289249, filename='QA_prompts_and_completions.json', object='file', purpose='fine-tune', status='processed', status_details=None) file-EUPGmm1yAd3axrQ0pyoeAKuE
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.355.1">The code displays</span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.356.1"> the details of the fine-tuning job:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.357.1">FineTuningJob(id='ftjob-O1OEE7eEyFNJsO2Eu5otzWA8', created_at=1725289250, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-h2Kjmcir4wyGtqq1mJALLGIb', result_files=[], seed=1103096818, status='validating_files', trained_tokens=None, training_file='file-EUPGmm1yAd3axrQ0pyoeAKuE', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.358.1">The output provides the details we need to monitor the job. </span><span class="koboSpan" id="kobo.358.2">Here is a brief description of some of the key-value pairs in the output:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.359.1">Job ID</span></code><span class="koboSpan" id="kobo.360.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.361.1">ftjob-O1OEE7eEyFNJsO2Eu5otzWA8</span></code><span class="koboSpan" id="kobo.362.1">.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.363.1">Status</span></code><span class="koboSpan" id="kobo.364.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.365.1">validating_files</span></code><span class="koboSpan" id="kobo.366.1">. </span><span class="koboSpan" id="kobo.366.2">This means OpenAI is currently checking the training file to make sure it’s suitable for fine-tuning.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.367.1">Model</span></code><span class="koboSpan" id="kobo.368.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.369.1">gpt-4o-mini-2024-07-18</span></code><span class="koboSpan" id="kobo.370.1">. </span><span class="koboSpan" id="kobo.370.2">We’re using a smaller, more cost-effective version of GPT-4 for fine-tuning.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.371.1">Training File</span></code><span class="koboSpan" id="kobo.372.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.373.1">file-EUPGmm1yAd3axrQ0pyoeAKuE</span></code><span class="koboSpan" id="kobo.374.1">. </span><span class="koboSpan" id="kobo.374.2">This is the file we’ve provided that contains the examples to teach the model.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.375.1">Some key hyperparameters are:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.376.1">n_epochs</span></code><span class="koboSpan" id="kobo.377.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.378.1">'auto'</span></code><span class="koboSpan" id="kobo.379.1">: OpenAI will automatically determine the best number of training cycles.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.380.1">batch_size</span></code><span class="koboSpan" id="kobo.381.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.382.1">'auto'</span></code><span class="koboSpan" id="kobo.383.1">: OpenAI will automatically choose the optimal batch size for training.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.384.1">learning_rate_multiplier</span></code><span class="koboSpan" id="kobo.385.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.386.1">'auto'</span></code><span class="koboSpan" id="kobo.387.1">: OpenAI will automatically adjust the learning rate during training.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.388.1">Created</span></code><code class="inlineCode"><a id="_idIndexMarker567"/></code><code class="inlineCode"><span class="koboSpan" id="kobo.389.1"> at</span></code><span class="koboSpan" id="kobo.390.1">: </span><code class="inlineCode"><span class="koboSpan" id="kobo.391.1">2024-06-30 08:20:50</span></code><span class="koboSpan" id="kobo.392.1">.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.393.1">This information will prove useful if you wish to perform an in-depth study of fine-tuning OpenAI models. </span><span class="koboSpan" id="kobo.393.2">We can also use it to monitor and manage our fine-tuning process.</span></p>
    <h2 id="_idParaDest-232" class="heading-2"><span class="koboSpan" id="kobo.394.1">2.1. </span><span class="koboSpan" id="kobo.394.2">Monitoring the fine-tunes</span></h2>
    <p class="normal"><span class="koboSpan" id="kobo.395.1">In this section, we will</span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.396.1"> extract the minimum information we need to monitor the jobs for all our fine-tunes. </span><span class="koboSpan" id="kobo.396.2">We will first query OpenAI to obtain the three latest fine-tuning jobs:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.397.1">import</span></span><span class="koboSpan" id="kobo.398.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.399.1">as</span></span><span class="koboSpan" id="kobo.400.1"> pd
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.401.1">from</span></span><span class="koboSpan" id="kobo.402.1"> openai </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.403.1">import</span></span><span class="koboSpan" id="kobo.404.1"> OpenAI
client = OpenAI()
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.405.1"># Assume client is already set up and authenticated</span></span><span class="koboSpan" id="kobo.406.1">
response = client.fine_tuning.jobs.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.407.1">list</span></span><span class="koboSpan" id="kobo.408.1">(limit=</span><span class="hljs-number"><span class="koboSpan" id="kobo.409.1">3</span></span><span class="koboSpan" id="kobo.410.1">) </span><span class="hljs-comment"><span class="koboSpan" id="kobo.411.1"># increase to include your history</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.412.1">We then initialize the lists of information we want to visualize:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.413.1"># Initialize lists to store the extracted data</span></span><span class="koboSpan" id="kobo.414.1">
job_ids = []
created_ats = []
statuses = []
models = []
training_files = []
error_messages = []
fine_tuned_models = [] </span><span class="hljs-comment"><span class="koboSpan" id="kobo.415.1"># List to store the fine-tuned model names</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.416.1">Following that, we iterate through </span><code class="inlineCode"><span class="koboSpan" id="kobo.417.1">response</span></code><span class="koboSpan" id="kobo.418.1"> to retrieve the information we need:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.419.1"># Iterate over the jobs in the response</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.420.1">for</span></span><span class="koboSpan" id="kobo.421.1"> job </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.422.1">in</span></span><span class="koboSpan" id="kobo.423.1"> response.data:
    job_ids.append(job.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.424.1">id</span></span><span class="koboSpan" id="kobo.425.1">)
    created_ats.append(job.created_at)
    statuses.append(job.status)
    models.append(job.model)
    training_files.append(job.training_file)
    error_message = job.error.message </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.426.1">if</span></span><span class="koboSpan" id="kobo.427.1"> job.error </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.428.1">else</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.429.1">None</span></span><span class="koboSpan" id="kobo.430.1">
    error_messages.append(error_message)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.431.1"># Append the fine-tuned model name</span></span><span class="koboSpan" id="kobo.432.1">
    fine_tuned_model = job.fine_tuned_model </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.433.1">if</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.434.1">hasattr</span></span><span class="koboSpan" id="kobo.435.1">(job, </span><span class="hljs-string"><span class="koboSpan" id="kobo.436.1">'fine_tuned_model'</span></span><span class="koboSpan" id="kobo.437.1">)
    </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.438.1">else</span></span> <span class="hljs-literal"><span class="koboSpan" id="kobo.439.1">None</span></span><span class="koboSpan" id="kobo.440.1">
    fine_tuned_models.append(fine_tuned_model)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.441.1">We now create a </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.442.1">DataFrame with the information we extracted:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.443.1">import</span></span><span class="koboSpan" id="kobo.444.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.445.1">as</span></span><span class="koboSpan" id="kobo.446.1"> pd
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.447.1"># Assume client is already set up and authenticated</span></span><span class="koboSpan" id="kobo.448.1">
response = client.fine_tuning.jobs.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.449.1">list</span></span><span class="koboSpan" id="kobo.450.1">(limit=</span><span class="hljs-number"><span class="koboSpan" id="kobo.451.1">3</span></span><span class="koboSpan" id="kobo.452.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.453.1"># Create a DataFrame</span></span><span class="koboSpan" id="kobo.454.1">
df = pd.DataFrame({
</span><span class="hljs-string"><span class="koboSpan" id="kobo.455.1">    'Job ID'</span></span><span class="koboSpan" id="kobo.456.1">: job_ids,
</span><span class="hljs-string"><span class="koboSpan" id="kobo.457.1">    'Created At'</span></span><span class="koboSpan" id="kobo.458.1">: created_ats,
</span><span class="hljs-string"><span class="koboSpan" id="kobo.459.1">    'Status'</span></span><span class="koboSpan" id="kobo.460.1">: statuses,
</span><span class="hljs-string"><span class="koboSpan" id="kobo.461.1">    'Model'</span></span><span class="koboSpan" id="kobo.462.1">: models,
</span><span class="hljs-string"><span class="koboSpan" id="kobo.463.1">    'Training File'</span></span><span class="koboSpan" id="kobo.464.1">: training_files,
</span><span class="hljs-string"><span class="koboSpan" id="kobo.465.1">    'Error Message'</span></span><span class="koboSpan" id="kobo.466.1">: error_messages,
</span><span class="hljs-string"><span class="koboSpan" id="kobo.467.1">    'Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.468.1">: fine_tuned_models </span><span class="hljs-comment"><span class="koboSpan" id="kobo.469.1"># Include the fine-tuned model names</span></span><span class="koboSpan" id="kobo.470.1">
})
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.471.1">Finally, we convert the timestamps to readable format and display the list of fine-tunes and their status:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.472.1"># Convert timestamps to readable format</span></span><span class="koboSpan" id="kobo.473.1">
df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.474.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.475.1">Created At'</span></span><span class="koboSpan" id="kobo.476.1">] = pd.to_datetime(df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.477.1">'Created At'</span></span><span class="koboSpan" id="kobo.478.1">], unit=</span><span class="hljs-string"><span class="koboSpan" id="kobo.479.1">'s'</span></span><span class="koboSpan" id="kobo.480.1">)
df = df.sort_values(by=</span><span class="hljs-string"><span class="koboSpan" id="kobo.481.1">'Created At'</span></span><span class="koboSpan" id="kobo.482.1">, ascending=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.483.1">False</span></span><span class="koboSpan" id="kobo.484.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.485.1"># Display the DataFrame</span></span><span class="koboSpan" id="kobo.486.1">
df
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.487.1">The output provides a monitoring dashboard of the list of our jobs, as shown in </span><em class="italic"><span class="koboSpan" id="kobo.488.1">Figure 9.5</span></em><span class="koboSpan" id="kobo.489.1">:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.490.1"><img src="../Images/B31169_09_05.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.491.1">Figure 9.5: Job list in the pandas DataFrame</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.492.1">You can see that </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.493.1">for job </span><code class="inlineCode"><span class="koboSpan" id="kobo.494.1">0</span></code><span class="koboSpan" id="kobo.495.1">, the status of the task is </span><code class="inlineCode"><span class="koboSpan" id="kobo.496.1">running</span></code><span class="koboSpan" id="kobo.497.1">. </span><span class="koboSpan" id="kobo.497.2">The status informs you of the different steps of the process such as validating the files, running, failed, or succeeded. </span><span class="koboSpan" id="kobo.497.3">In this case, the fine-tuning process is running. </span><span class="koboSpan" id="kobo.497.4">If you refresh this cell regularly, you will see the status.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.498.1">We will now retrieve the most recent model trained for the </span><code class="inlineCode"><span class="koboSpan" id="kobo.499.1">Fine-Tuned Model</span></code><span class="koboSpan" id="kobo.500.1"> column. </span><span class="koboSpan" id="kobo.500.2">If the training fails, this column will be empty. </span><span class="koboSpan" id="kobo.500.3">If not, we can retrieve it:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.501.1">import</span></span><span class="koboSpan" id="kobo.502.1"> pandas </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.503.1">as</span></span><span class="koboSpan" id="kobo.504.1"> pd
generation=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.505.1">False</span></span>  <span class="hljs-comment"><span class="koboSpan" id="kobo.506.1"># until the current model is fine-tuned</span></span>
<span class="hljs-comment"><span class="koboSpan" id="kobo.507.1"># Attempt to find the first non-empty Fine-Tuned Model</span></span><span class="koboSpan" id="kobo.508.1">
non_empty_models = df[df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.509.1">'Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.510.1">].notna() &amp; (df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.511.1">'Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.512.1">] != </span><span class="hljs-string"><span class="koboSpan" id="kobo.513.1">''</span></span><span class="koboSpan" id="kobo.514.1">)]
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.515.1">if</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.516.1">not</span></span><span class="koboSpan" id="kobo.517.1"> non_empty_models.empty:
    first_non_empty_model = non_empty_models[</span><span class="hljs-string"><span class="koboSpan" id="kobo.518.1">'Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.519.1">].iloc[</span><span class="hljs-number"><span class="koboSpan" id="kobo.520.1">0</span></span><span class="koboSpan" id="kobo.521.1">]
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.522.1">print</span></span><span class="koboSpan" id="kobo.523.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.524.1">"The latest fine-tuned model is:"</span></span><span class="koboSpan" id="kobo.525.1">, first_non_empty_model)
    generation=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.526.1">True</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.527.1">else</span></span><span class="koboSpan" id="kobo.528.1">:
    first_non_empty_model=</span><span class="hljs-string"><span class="koboSpan" id="kobo.529.1">'None'</span></span>
    <span class="hljs-built_in"><span class="koboSpan" id="kobo.530.1">print</span></span><span class="koboSpan" id="kobo.531.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.532.1">"No fine-tuned models found."</span></span><span class="koboSpan" id="kobo.533.1">)
</span><span class="hljs-comment"><span class="koboSpan" id="kobo.534.1"># Display the first non-empty Fine-Tuned Model in the DataFrame</span></span><span class="koboSpan" id="kobo.535.1">
first_non_empty_model = df[df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.536.1">'</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.537.1">Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.538.1">].notna() &amp; (df[</span><span class="hljs-string"><span class="koboSpan" id="kobo.539.1">'Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.540.1">] != </span><span class="hljs-string"><span class="koboSpan" id="kobo.541.1">''</span></span><span class="koboSpan" id="kobo.542.1">)][</span><span class="hljs-string"><span class="koboSpan" id="kobo.543.1">'Fine-Tuned Model'</span></span><span class="koboSpan" id="kobo.544.1">].iloc[</span><span class="hljs-number"><span class="koboSpan" id="kobo.545.1">0</span></span><span class="koboSpan" id="kobo.546.1">]
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.547.1">print</span></span><span class="koboSpan" id="kobo.548.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.549.1">"The lastest fine-tuned model is:"</span></span><span class="koboSpan" id="kobo.550.1">, first_non_empty_model)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.551.1">The output will display the name of the latest fine-tuned model if there is one or inform us that no fine-tuned model is found. </span><span class="koboSpan" id="kobo.551.2">In this case, GPT-4o-mini was successfully trained:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.552.1">The latest fine-tuned model is: ft:gpt-4o-mini-2024-07-18:personal::A32VfYIz
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.553.1">If a fine-tuned model is found, </span><code class="inlineCode"><span class="koboSpan" id="kobo.554.1">generation=True</span></code><span class="koboSpan" id="kobo.555.1">, it will trigger the OpenAI completion calls in the following cells. </span><span class="koboSpan" id="kobo.555.2">If no model is found, </span><code class="inlineCode"><span class="koboSpan" id="kobo.556.1">generation=False</span></code><span class="koboSpan" id="kobo.557.1">, it will not run the OpenAI API in the rest of the notebook to avoid using models that you are not training. </span><span class="koboSpan" id="kobo.557.2">You can set generation to </span><code class="inlineCode"><span class="koboSpan" id="kobo.558.1">True</span></code><span class="koboSpan" id="kobo.559.1"> in a new cell and then select any fine-tuned model you wish.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.560.1">We know that the</span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.561.1"> training job can take a while. </span><span class="koboSpan" id="kobo.561.2">You can refresh the pandas DataFrame from time to time. </span><span class="koboSpan" id="kobo.561.3">You can write code that checks the status of another job and waits for a name to appear for your training job or an error message. </span><span class="koboSpan" id="kobo.561.4">You can also wait for OpenAI to send you an email informing you that the training job is finished. </span><span class="koboSpan" id="kobo.561.5">If the training job fails, we must verify our training data for any inconsistencies, missing values, or incorrect labels. </span><span class="koboSpan" id="kobo.561.6">Additionally, ensure that the JSON file format adheres to OpenAI’s specified schema, including correct field names, data types, and structure.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.562.1">Once the training job is finished, we can run completion tasks.</span></p>
    <h1 id="_idParaDest-233" class="heading-1"><span class="koboSpan" id="kobo.563.1">3. </span><span class="koboSpan" id="kobo.563.2">Using the fine-tuned OpenAI model</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.564.1">We are now ready to use our </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.565.1">fine-tuned OpenAI </span><code class="inlineCode"><span class="koboSpan" id="kobo.566.1">GPT-4o-mini</span></code><span class="koboSpan" id="kobo.567.1"> model. </span><span class="koboSpan" id="kobo.567.2">We will begin by defining a prompt based on a question taken from our initial dataset:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.568.1"># Define the prompt</span></span><span class="koboSpan" id="kobo.569.1">
prompt = </span><span class="hljs-string"><span class="koboSpan" id="kobo.570.1">"What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?"</span></span>
</code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.571.1">The goal is to verify whether the dataset has been properly trained and will produce results similar to the completions we defined. </span><span class="koboSpan" id="kobo.571.2">We can now run the fine-tuned model:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.572.1"># Assume first_non_empty_model is defined above this snippet</span></span>
<span class="hljs-keyword"><span class="koboSpan" id="kobo.573.1">if</span></span><span class="koboSpan" id="kobo.574.1"> generation==</span><span class="hljs-literal"><span class="koboSpan" id="kobo.575.1">True</span></span><span class="koboSpan" id="kobo.576.1">:
    response = client.chat.completions.create(
        model=first_non_empty_model,
        temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.577.1">0.0</span></span><span class="koboSpan" id="kobo.578.1">,  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.579.1"># Adjust as needed for variability</span></span><span class="koboSpan" id="kobo.580.1">
        messages=[
            {</span><span class="hljs-string"><span class="koboSpan" id="kobo.581.1">"role"</span></span><span class="koboSpan" id="kobo.582.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.583.1">"system"</span></span><span class="koboSpan" id="kobo.584.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.585.1">"content"</span></span><span class="koboSpan" id="kobo.586.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.587.1">"Given a question, reply with a complete explanation for students."</span></span><span class="koboSpan" id="kobo.588.1">},
            {</span><span class="hljs-string"><span class="koboSpan" id="kobo.589.1">"role"</span></span><span class="koboSpan" id="kobo.590.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.591.1">"user"</span></span><span class="koboSpan" id="kobo.592.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.593.1">"content"</span></span><span class="koboSpan" id="kobo.594.1">: prompt}
        ]
    )
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.595.1">else</span></span><span class="koboSpan" id="kobo.596.1">:
    </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.597.1">print</span></span><span class="koboSpan" id="kobo.598.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.599.1">"Error: Model is None, cannot proceed with the API request."</span></span><span class="koboSpan" id="kobo.600.1">)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.601.1">The parameters of the request must fit our scenario:</span></p>
    <ul>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.602.1">model=first_non_empty_model</span></code><span class="koboSpan" id="kobo.603.1"> is our pretrained model.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.604.1">prompt=prompt</span></code><span class="koboSpan" id="kobo.605.1"> is our predefined prompt.</span></li>
      <li class="bulletList"><code class="inlineCode"><span class="koboSpan" id="kobo.606.1">temperature=0.0</span></code><span class="koboSpan" id="kobo.607.1"> is set to a low value because we do not want any “creativity” for this hard science completion task.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.608.1">Once we run the request, we can format and display the response. </span><span class="koboSpan" id="kobo.608.2">The following code contains two cells to display and extract the response.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.609.1">First, we can print the raw response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.610.1">if</span></span><span class="koboSpan" id="kobo.611.1"> generation==</span><span class="hljs-literal"><span class="koboSpan" id="kobo.612.1">True</span></span><span class="koboSpan" id="kobo.613.1">:
  </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.614.1">print</span></span><span class="koboSpan" id="kobo.615.1">(response)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.616.1">The </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.617.1">output contains the response and information on the process:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.618.1">ChatCompletion(id='chatcmpl-A32pvH9wLvNsSRmB1sUjxOW4Z6Xr6',…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.619.1">We then extract the text of the response:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.620.1">if</span></span><span class="koboSpan" id="kobo.621.1"> (generation==</span><span class="hljs-literal"><span class="koboSpan" id="kobo.622.1">True</span></span><span class="koboSpan" id="kobo.623.1">):
  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.624.1"># Access the response from the first choice</span></span><span class="koboSpan" id="kobo.625.1">
  response_text = response.choices[</span><span class="hljs-number"><span class="koboSpan" id="kobo.626.1">0</span></span><span class="koboSpan" id="kobo.627.1">].message.content
  </span><span class="hljs-comment"><span class="koboSpan" id="kobo.628.1"># Print the response</span></span>
  <span class="hljs-built_in"><span class="koboSpan" id="kobo.629.1">print</span></span><span class="koboSpan" id="kobo.630.1">(response_text)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.631.1">The output is a string:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.632.1">Coriolis effect Explanation: The Coriolis effect is…
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.633.1">Finally, we can format the response string into a nice paragraph with the Python wrapper:</span></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.634.1">import</span></span><span class="koboSpan" id="kobo.635.1"> textwrap
</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.636.1">if</span></span><span class="koboSpan" id="kobo.637.1"> generation==</span><span class="hljs-literal"><span class="koboSpan" id="kobo.638.1">True</span></span><span class="koboSpan" id="kobo.639.1">:
wrapped_text = textwrap.fill(response_text.strip(), </span><span class="hljs-number"><span class="koboSpan" id="kobo.640.1">60</span></span><span class="koboSpan" id="kobo.641.1">)
</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.642.1">print</span></span><span class="koboSpan" id="kobo.643.1">(wrapped_text)
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.644.1">The output shows that our data has been taken into account:</span></p>
    <pre class="programlisting con"><code class="hljs-con"><span class="koboSpan" id="kobo.645.1">Coriolis effect Explanation: The Coriolis effect is a
phenomenon that causes moving objects, such as air and
water, to turn and twist in response to the rotation of the
Earth. </span><span class="koboSpan" id="kobo.645.2">It is responsible for the rotation of large weather
systems, such as hurricanes, and the direction of trade
winds and ocean currents. </span><span class="koboSpan" id="kobo.645.3">In the Northern Hemisphere, the
effect causes moving objects to turn to the right, while in
the Southern Hemisphere, objects turn to the left. </span><span class="koboSpan" id="kobo.645.4">The
Coriolis effect is proportional to the speed of the moving
object and the strength of the Earth's rotation, and it is
negligible for small-scale movements, such as water flowing
in a sink.
</span></code></pre>
    <p class="normal"><span class="koboSpan" id="kobo.646.1">Let’s look at the initial completion for our prompt:</span></p>
    <figure class="mediaobject"><span class="koboSpan" id="kobo.647.1"><img src="../Images/B31169_09_06.png" alt="A close up of text  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.648.1">Figure 9.6: Initial completion</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.649.1">The</span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.650.1"> response is thus satisfactory. </span><span class="koboSpan" id="kobo.650.2">This might not always be the case and might require more work on the datasets (better data, large volumes of data, etc.) incrementally until you have reached a satisfactory goal.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.651.1">You can save the name of your model in a text file or anywhere you wish. </span><span class="koboSpan" id="kobo.651.2">You can now run your model in another program using the name of your trained model, or you can reload this notebook at any time:</span></p>
    <ol>
      <li class="numberedList" value="1"><span class="koboSpan" id="kobo.652.1">Run the </span><code class="inlineCode"><span class="koboSpan" id="kobo.653.1">Installing the environment</span></code><span class="koboSpan" id="kobo.654.1"> section of this notebook.</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.655.1">Define a prompt of your choice related to the dataset we trained.</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.656.1">Enter the name of your model in the OpenAI completion request.</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.657.1">Run the</span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.658.1"> request and analyze the response.</span></li>
    </ol>
    <p class="normal"><span class="koboSpan" id="kobo.659.1">You can consult</span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.660.1"> OpenAI’s fine-tuning documentation for further information if necessary: </span><a href="https://platform.openai.com/docs/guides/fine-tuning/fine-tuning"><span class="url"><span class="koboSpan" id="kobo.661.1">https://platform.openai.com/docs/guides/fine-tuning/fine-tuning</span></span></a><span class="koboSpan" id="kobo.662.1">.</span></p>
    <h1 id="_idParaDest-234" class="heading-1"><span class="koboSpan" id="kobo.663.1">Metrics</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.664.1">OpenAI </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.665.1">provides a user interface to analyze the metrics of the training process and model. </span><span class="koboSpan" id="kobo.665.2">You can access the metrics related to your fine-tuned models</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.666.1"> at </span><a href="https://platform.openai.com/finetune/"><span class="url"><span class="koboSpan" id="kobo.667.1">https://platform.openai.com/finetune/</span></span></a><span class="koboSpan" id="kobo.668.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.669.1">The interface displays the list of your fine-tuned jobs:</span></p>
    <figure class="mediaobject"> <span class="koboSpan" id="kobo.670.1"><img src="../Images/B31169_09_07.png" alt="A screenshot of a phone  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.671.1">Figure 9.7: List of fine-tuned jobs</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.672.1">You can</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.673.1"> choose to view all the fine-tuning jobs, the ones that were successful, or the ones that failed. </span><span class="koboSpan" id="kobo.673.2">If we choose a job that was successful, for example, we can view the job details as shown in the following excerpt:</span></p>
    <figure class="mediaobject"> <span class="koboSpan" id="kobo.674.1"><img src="../Images/B31169_09_08.png" alt="A screenshot of a computer  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.675.1">Figure 9.8: Example view</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.676.1">Let’s go through the information provided in this figure:</span></p>
    <ul>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.677.1">Status</span></strong><span class="koboSpan" id="kobo.678.1">: Indicates the status of the fine-tuning process. </span><span class="koboSpan" id="kobo.678.2">In this case, we can see that the process was completed successfully.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.679.1">Job ID</span></strong><span class="koboSpan" id="kobo.680.1">: A unique identifier for the fine-tuning job. </span><span class="koboSpan" id="kobo.680.2">This can be used to reference the job in queries or for support purposes.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.681.1">Base model</span></strong><span class="koboSpan" id="kobo.682.1">: Specifies the pretrained model used as the starting point for fine-tuning. </span><span class="koboSpan" id="kobo.682.2">In this case, </span><code class="inlineCode"><span class="koboSpan" id="kobo.683.1">gpt-4o-mini</span></code><span class="koboSpan" id="kobo.684.1"> is a version of OpenAI’s models.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.685.1">Output model</span></strong><span class="koboSpan" id="kobo.686.1">: This is the identifier for the model resulting from the fine-tuning. </span><span class="koboSpan" id="kobo.686.2">It incorporates changes and optimizations based on the specific training data provided.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.687.1">Created at</span></strong><span class="koboSpan" id="kobo.688.1">: The date and time when the fine-tuning job was initiated.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.689.1">Trained tokens</span></strong><span class="koboSpan" id="kobo.690.1">: The total number of tokens (pieces of text, such as words or punctuation) that were processed during training. </span><span class="koboSpan" id="kobo.690.2">This metric helps gauge the extent of training.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.691.1">Epochs</span></strong><span class="koboSpan" id="kobo.692.1">: The number of complete passes the training data went through during fine-tuning. </span><span class="koboSpan" id="kobo.692.2">More epochs can lead to better learning but too many may lead to overfitting.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.693.1">Batch size</span></strong><span class="koboSpan" id="kobo.694.1">: The number of training examples utilized in one iteration of model training. </span><span class="koboSpan" id="kobo.694.2">Smaller batch sizes can offer more updates and refined learning but may take longer to train.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.695.1">LR multiplier</span></strong><span class="koboSpan" id="kobo.696.1">: This refers to the learning rate multiplier, affecting how much the learning rate for the base model is adjusted during the fine-tuning process. </span><span class="koboSpan" id="kobo.696.2">A smaller multiplier can lead to smaller, more conservative updates to model weights.</span></li>
      <li class="bulletList"><strong class="keyWord"><span class="koboSpan" id="kobo.697.1">Seed</span></strong><span class="koboSpan" id="kobo.698.1">: A seed for the random number generator used in the training process. </span><span class="koboSpan" id="kobo.698.2">Providing a seed ensures that the training process is reproducible, meaning you can get the same results with the same input conditions.</span></li>
    </ul>
    <p class="normal"><span class="koboSpan" id="kobo.699.1">This information </span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.700.1">will help tailor the fine-tuning jobs to meet the specific needs of a project and explore alternative approaches to optimization and customization. </span><span class="koboSpan" id="kobo.700.2">In addition, the interface contains more information that we can explore to get an in-depth vision of the fine-tuning process. </span><span class="koboSpan" id="kobo.700.3">If we scroll down on the </span><strong class="screenText"><span class="koboSpan" id="kobo.701.1">Information</span></strong><span class="koboSpan" id="kobo.702.1"> tab of our model, we can see metrics as shown here:</span></p>
    <figure class="mediaobject"> <span class="koboSpan" id="kobo.703.1"><img src="../Images/B31169_09_09-01.png" alt="A green line graph with numbers  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.704.1">Figure 9.9: Metrics for a fine-tuned model</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.705.1">Training</span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.706.1"> loss and the other available information can guide our training strategies (data, files, and parameters).</span></p>
    <p class="normal"><strong class="screenText"><span class="koboSpan" id="kobo.707.1">Training loss</span></strong><span class="koboSpan" id="kobo.708.1"> is a</span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.709.1"> reliable metric used to evaluate the performance of a machine learning model during training. </span><span class="koboSpan" id="kobo.709.2">In this case, </span><code class="inlineCode"><span class="koboSpan" id="kobo.710.1">Training loss (1.1570)</span></code><span class="koboSpan" id="kobo.711.1"> represents the model’s average error on the training dataset. </span><span class="koboSpan" id="kobo.711.2">Lower training loss values indicate that the model is better fitting the training data. </span><span class="koboSpan" id="kobo.711.3">A training loss of </span><code class="inlineCode"><span class="koboSpan" id="kobo.712.1">1.1570</span></code><span class="koboSpan" id="kobo.713.1"> suggests that the model has learned to predict or classify its training data well during the fine-tuning process.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.714.1">We can also examine these values with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.715.1">Time</span></code><span class="koboSpan" id="kobo.716.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.717.1">Step</span></code><span class="koboSpan" id="kobo.718.1"> information:</span></p>
    <figure class="mediaobject"> <span class="koboSpan" id="kobo.719.1"><img src="../Images/B31169_09_10.png" alt="A screenshot of a phone  Description automatically generated"/></span></figure>
    <p class="packt_figref"><span class="koboSpan" id="kobo.720.1">Figure 9.10: Training loss during the training job</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.721.1">We must also measure the usage to monitor the cost per period and model. </span><span class="koboSpan" id="kobo.721.2">OpenAI provides a detailed interface at </span><a href="https://platform.openai.com/usage"><span class="url"><span class="koboSpan" id="kobo.722.1">https://platform.openai.com/usage</span></span></a><span class="koboSpan" id="kobo.723.1">.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.724.1">Fine-tuning can indeed be an effective way to optimize RAG data if we make sure to train a model with high-quality data and the right parameters. </span><span class="koboSpan" id="kobo.724.2">Now, it’s time for us to summarize our journey and move to our next RAG-driven generative AI implementation.</span></p>
    <h1 id="_idParaDest-235" class="heading-1"><span class="koboSpan" id="kobo.725.1">Summary</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.726.1">This chapter’s goal was to show that as we accumulate RAG data, some data is dynamic and requires constant updates, and as such, cannot be fine-tuned easily. </span><span class="koboSpan" id="kobo.726.2">However, some data is static, meaning that it will remain stable for long periods of time. </span><span class="koboSpan" id="kobo.726.3">This data can become parametric (stored in the weights of a trained LLM).</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.727.1">We first downloaded and processed the SciQ dataset, which contains hard science questions. </span><span class="koboSpan" id="kobo.727.2">This stable data perfectly suits fine-tuning. </span><span class="koboSpan" id="kobo.727.3">It contains a question, answer, and support (explanation) structure, which makes the data effective for fine-tuning. </span><span class="koboSpan" id="kobo.727.4">Also, we can assume human feedback was required. </span><span class="koboSpan" id="kobo.727.5">We can even go as far as imagining this feedback could be provided by analyzing generative AI model outputs.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.728.1">We converted the data we prepared into prompts and completions in a JSONL file following the recommendations of OpenAI’s preparation tool. </span><span class="koboSpan" id="kobo.728.2">The structure of JSONL was meant to be compatible with a completion model (prompt and completion) such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.729.1">GPT-4o-mini</span></code><span class="koboSpan" id="kobo.730.1">. </span><span class="koboSpan" id="kobo.730.2">The program then fine-tuned the cost-effective </span><code class="inlineCode"><span class="koboSpan" id="kobo.731.1">GPT-4o-mini</span></code><span class="koboSpan" id="kobo.732.1"> OpenAI model, following which we ran the model and found that the output was satisfactory. </span><span class="koboSpan" id="kobo.732.2">Finally, we explored the metrics of the fine-tuned model in the OpenAI metrics user interface.</span></p>
    <p class="normal"><span class="koboSpan" id="kobo.733.1">We can conclude that fine-tuning can optimize RAG data in certain cases when necessary. </span><span class="koboSpan" id="kobo.733.2">However, we will take this process further in the next chapter, </span><em class="chapterRef"><span class="koboSpan" id="kobo.734.1">Chapter 10</span></em><span class="koboSpan" id="kobo.735.1">, </span><em class="italic"><span class="koboSpan" id="kobo.736.1">RAG for Video Stock Production with Pinecone and OpenAI</span></em><span class="koboSpan" id="kobo.737.1">, when we run the full-blown RAG-driven generative AI ecosystem.</span></p>
    <h1 id="_idParaDest-236" class="heading-1"><span class="koboSpan" id="kobo.738.1">Questions</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.739.1">Answer the following questions with yes or no:</span></p>
    <ol>
      <li class="numberedList" value="1"><span class="koboSpan" id="kobo.740.1">Do all organizations need to manage large volumes of RAG data?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.741.1">Is the GPT-4o-mini model described as insufficient for fine-tuning tasks?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.742.1">Can pretrained models update their knowledge base after the cutoff date without retrieval systems?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.743.1">Is it the case that static data never changes and thus never requires updates?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.744.1">Is downloading data from Hugging Face the only source for preparing datasets?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.745.1">Is all RAG data eventually embedded into the trained model’s parameters according to the document?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.746.1">Does the chapter recommend using only new data for fine-tuning AI models?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.747.1">Is the OpenAI Metrics interface used to adjust the learning rate during model training?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.748.1">Can the fine-tuning process be effectively monitored using the OpenAI dashboard?</span></li>
      <li class="numberedList"><span class="koboSpan" id="kobo.749.1">Is human feedback deemed unnecessary in the preparation of hard science datasets such as SciQ?</span></li>
    </ol>
    <h1 id="_idParaDest-237" class="heading-1"><span class="koboSpan" id="kobo.750.1">References</span></h1>
    <ul>
      <li class="bulletList"><span class="koboSpan" id="kobo.751.1">OpenAI fine-tuning documentation: </span><a href="https://platform.openai.com/docs/guides/fine-tuning/"><span class="url"><span class="koboSpan" id="kobo.752.1">https://platform.openai.com/docs/guides/fine-tuning/</span></span></a></li>
      <li class="bulletList"><span class="koboSpan" id="kobo.753.1">OpenAI pricing: </span><a href="https://openai.com/api/pricing/"><span class="url"><span class="koboSpan" id="kobo.754.1">https://openai.com/api/pricing/</span></span></a></li>
    </ul>
    <h1 id="_idParaDest-238" class="heading-1"><span class="koboSpan" id="kobo.755.1">Further reading</span></h1>
    <ul>
      <li class="bulletList"><em class="italic"><span class="koboSpan" id="kobo.756.1">Test of Fine-Tuning GPT by Astrophysical Data</span></em><span class="koboSpan" id="kobo.757.1"> by Yu Wang et al. </span><span class="koboSpan" id="kobo.757.2">is an interesting article on fine-tuning hard science data, which requires careful data preparation: </span><a href="https://arxiv.org/pdf/2404.10019"><span class="url"><span class="koboSpan" id="kobo.758.1">https://arxiv.org/pdf/2404.10019</span></span></a></li>
    </ul>
    <h1 id="_idParaDest-239" class="heading-1"><span class="koboSpan" id="kobo.759.1">Join our community on Discord</span></h1>
    <p class="normal"><span class="koboSpan" id="kobo.760.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
    <p class="normal"><a href="https://www.packt.link/rag"><span class="url"><span class="koboSpan" id="kobo.761.1">https://www.packt.link/rag</span></span></a></p>
    <p class="normal"><span class="koboSpan" id="kobo.762.1"><img src="../Images/QR_Code50409000288080484.png" alt=""/></span></p>
  </div>
</body></html>