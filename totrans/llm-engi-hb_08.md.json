["```py\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    model_id = \"google/gemma-2b-it\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id) \n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\") \n    ```", "```py\n    model.generation_config.cache_implementation = \"static\" \n    ```", "```py\n    compiled_model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True) \n    ```", "```py\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    inputs = tokenizer(\"What is 2+2?\", return_tensors=\"pt\").to(device) \n    ```", "```py\n    outputs = model.generate(**inputs, do_sample=True, temperature=0.7, max_new_tokens=64)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n    **[****'What is 2+2?\\n\\nThe answer is 4\\. 2+2 = 4.'****]** \n    ```", "```py\n    import torch\n    from transformers import AutoTokenizer, AutoModelForCausalLM\n    model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n    tokenizer = AutoTokenizer.from_pretrained(model_id) \n    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n    draft_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen1.5-0.5B-Chat\", device_map=\"auto\") \n    ```", "```py\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    inputs = tokenizer(\"What is 2+2?\", return_tensors=\"pt\").to(device) \n    ```", "```py\n    outputs = model.generate(**inputs, do_sample=True, assistant_model=draft_model, temperature=0.7, max_new_tokens=64)\n    print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n    **[****'What is 2+2? 2 + 2 equals 4!'****]** \n    ```", "```py\noutputs = model.generate(**inputs, prompt_lookup_num_tokens=4) \n```", "```py\n    pip install flash-attn --no-build-isolation \n    ```", "```py\n    from transformers import AutoModelForCausalLM\n    model = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.3\",\n        attn_implementation=\"flash_attention_2\",\n    ) \n    ```", "```py\nimport torch\ndef absmax_quantize(X):\n    # Calculate scale\n    scale = 127 / torch.max(torch.abs(X))\n    # Quantize\n    X_quant = (scale * X).round()\n    return X_quant.to(torch.int8) \n```", "```py\ndef zeropoint_quantize(X):\n    # Calculate value range (denominator)\n    x_range = torch.max(X) - torch.min(X)\n    x_range = 1 if x_range == 0 else x_range\n    # Calculate scale\n    scale = 255 / x_range\n    # Shift by zero-point\n    zeropoint = (-scale * torch.min(X) - 128).round()\n    # Scale and round the inputs\n    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n\n    return X_quant.to(torch.int8) \n```", "```py\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_8bit=True) \n```", "```py\nfrom transformers import AutoModelForCausalLM\nmodel_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True) \n```", "```py\n    !git clone https://github.com/ggerganov/llama.cpp\n    !cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n    !pip install -r llama.cpp/requirements.txt \n    ```", "```py\n    MODEL_ID = \"mlabonne/EvolCodeLlama-7b\"\n    MODEL_NAME = MODEL_ID.split('/')[-1]\n    !git lfs install\n    !git clone https://huggingface.co/{MODEL_ID} \n    ```", "```py\n    fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n    !python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16} \n    ```", "```py\n    METHOD = \"q4_k_m\"\n    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n    !./llama.cpp/quantize {fp16} {qtype} {METHOD} \n    ```", "```py\n    from huggingface_hub import create_repo, HfApi\n    hf_token = \"\" # Specify your token\n    username = \"\" # Specify your username\n    api = HfApi()\n    # Create empty repo\n    create_repo(\n        repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n        repo_type=\"model\",\n        exist_ok=True,\n        token=hf_token\n    )\n    # Upload gguf files\n    api.upload_folder(\n        folder_path=MODEL_NAME,\n        repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n        allow_patterns=f\"*.gguf\",\n        token=hf_token\n    ) \n    ```", "```py\n    !git clone https://github.com/turboderp/exllamav2\n    !pip install -e exllamav2 \n    ```", "```py\n    MODEL_ID = \"meta-llama/Llama-2-7b-chat-hf\"\n    MODEL_NAME = MODEL_ID.split('/')[-1]\n    !git lfs install\n    !git clone https://huggingface.co/{MODEL_ID} \n    ```", "```py\n    !wget https://huggingface.co/datasets/wikitext/resolve/9a9e482b5987f9d25b3a9b2883fc6cc9fd8071b3/wikitext-103-v1/wikitext-test.parquet \n    ```", "```py\n    !mkdir quant\n    !python exllamav2/convert.py \\\n        -i {MODEL_NAME} \\\n        -o quant \\\n        -c wikitext-test.parquet \\\n        -b 4.5 \n    ```"]