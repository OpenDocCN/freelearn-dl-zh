["```py\n\"Who are the members of Metallica. List them as comma separated.\"\n```", "```py\n'James Hetfield, Lars Ulrich, Kirk Hammett, Robert Trujillo'\n```", "```py\n\"List the first 10 elements from the periodical table as comma separated list.\"\n```", "```py\n['Hydrogen',\n'Helium',\n'Lithium',\n'Beryllium',\n'Boron',\n'Carbon',\n'Nitrogen',\n'Oxygen',\n'Fluorine',\n'Neon']\n```", "```py\n\"List all the holidays you know as comma separated list.\"\n```", "```py\n\"Current conversation:\n{history}\nYour task:\n{input}}\"\n```", "```py\nconversation.predict_and_parse(input=\"Write the first 10 holidays you know, as a comma separated list.\")\n```", "```py\n['Christmas',\n'Thanksgiving',\n\"New Year's Day\",\n'Halloween',\n'Easter',\n'Independence Day',\n\"Valentine's Day\",\n\"St. Patrick's Day\",\n'Labor Day',\n'Memorial Day']\n```", "```py\nconversation.predict_and_parse(input=\" Observe the list of holidays you printed and remove all the non-religious holidays from the list.\")\n```", "```py\n['Christmas',\n'Thanksgiving',\n\"New Year's Day\",\n'Easter',\n\"Valentine's Day\",\n\"St. Patrick's Day,\"]\n```", "```py\n\"For each of these, tell about the holiday in 2 sentences.\nForm the output in a json format table.\nThe table's name is \"holidays\" and the fields are \"name\" and \"description\".\nFor each row, the \"name\" is the holiday's name, and the \"description\" is the description you generated.\nThe syntax of the output should be a json format, without newline characters.\"\n```", "```py\n{\n  \"holidays\": [\n    {\n      \"name\": \"Christmas\",\n      \"description\": \"Christmas is a religious holiday that celebrates the birth of Jesus Christ and is widely observed as a secular cultural and commercial phenomenon.\"\n    },\n    {\n      \"name\": \"Thanksgiving\",\n      \"description\": \"Thanksgiving is a national holiday in the United States, celebrated on the fourth Thursday of November, and originated as a harvest festival.\"\n    },\n    {\n      \"name\": \"Easter\",\n      \"description\": \"Easter is […]\n```", "```py\ndict = json.loads(output)\npd.json_normalize(dict[ \"holidays\"])\n```", "```py\nPlease review the entire content, summarize it to the length of 4 sentence, then translate it to Russian and to German.\nMake sure the summary is consistent with the content.\nPut the string '\\n----\\n' between the English part of the answer and the Russian part.\nPut the string '\\n****\\n' between the Russian part of the answer and the German part.\n```", "```py\nThe content emphasizes the importance of good\nrelationships in keeping us happy and healthy\nthroughout our lives. It discusses how social\nconnections, quality of close relationships, and\navoiding conflict play crucial roles in our well-\nbeing. The study follows the lives of 724 men over\n75 years, highlighting the significance of\nrelationships over wealth and fame in leading a\nfulfilling life.\nRussian:\nСодержание подчеркивает\nВажность [...]\nGerman:\nDer\nInhalt betont die Bedeutung  [...]\n```", "```py\n- Betonung der Bedeutung guter Beziehungen für Glück und Gesundheit\n- Diskussion über soziale Verbindungen, Qualität enger Beziehungen und Konfliktvermeidung\n- Verfolgung des Lebens von 724 Männern über 75 Jahre in der Studie\n- Hervorhebung der Bedeutung von Beziehungen im Vergleich zu Reichtum und Ruhm\n- Fokus auf Beziehungen als Schlüssel zu einem erfüllten Leben\n```", "```py\n\"Does this publication involve Reinforcement Learning?\"\n```", "```py\nlead (to manager_0):\nRefer to the Python dict that is in this [...]\nprogrammer (to manager_0):\n```", "```py\n\nAs can be seen, the conversation had four interactions, each between two agents. Each interaction starts by telling the user which agent is talking to which other agent; these parts are in bold letters in the preceding printout.\n\nIn the second interaction, the programmer provided a complete Python script. We pasted only the first four commands to keep it short, but you can observe the full script in the notebook. The QA engineer ran the script and reported that it ran well. If it hadn’t run well, it would have returned an `exitcode: 1` and would have provided the programmer with the error specification for the programmer to fix the code; the conversation would have continued until a solution was found, or, if not, the team would report failure and conclude the conversation.\n\nThis task provided us with the code to create the visual we wanted. Note that we didn’t ask the agents to run the code and provide us with the visual; we asked for the code itself. One could, if desired, configure the LLMs to run the code and provide us with the resulting image. See AutoGen’s repo for the various examples and capabilities.\n\nIn the next code cell, we pasted the code that the team created. The code runs well and visualizes the three distributions exactly as we asked the team (see *Figure 9**.2*):\n\n![Figure 9.2 – Visualizing the value that prompt compression provides](img/B18949_09_2.jpg)\n\nFigure 9.2 – Visualizing the value that prompt compression provides\n\nThe top visualization displays the distributions of the token count for the original prompts (blue/light shade) and the compressed prompts (orange/dark shade), and the bottom part of the figure shows the distribution of the ratio between each pair of prompts. *Figure 9**.2* shows just how effective the reduction rate is, as this ratio translates to a reduction in API cost.\n\nThis concludes the visualization of the significance of the experiments.\n\n#### Human intervention in the team’s tasks\n\nNote that all three agents are driven by LLMs, thus making this entire task automatically performed without human intervention. One could change the lead’s configuration to represent a human user, meaning you. If you did that, then you would be able to intervene and demand certain verifications from the QA engineer or certain additional features in the code from the programmer.\n\nThis could be particularly useful if you wanted to run the code yourself in your environment instead of letting the QA engineer agent run it in its own environment. Your environments are different. One advantage of doing this is when the code is required to load a data file that you have locally. If you told the agent to write code that loads this file, then when the QA engineer agent ran it, it would tell you the code failed since that data file doesn’t exist in its environment. In this case, you may elect to be the one who iterates with the programmer and the one who runs the code during the iterations and provides feedback.\n\nAnother case where you would want to be the one running the code and providing feedback is when the QA engineer encounters an error or a bug in the programmer’s code, but the two agents aren’t able to figure out the solution. In that case, you would want to intervene and provide your insight. For instance, in a case where a for loop iterates over a dict’s keys instead of its values, you may intervene and enter *The code runs but the for loop is iterating on the dict’s keys. It should iterate over its values for the* *key ‘key1.*\n\nWe can now move on to the second part of concluding the evaluation.\n\n#### Reviewing the results of the experiments and forming an educated conclusion\n\nAs with every complex evaluation where we perform experiments to target the impact of a particular feature, we would now like to derive a qualitative summary of the results and suggest a conclusion for our audience, whether it is the decision-makers in the company or the research community in academia.\n\nWhat is unique about this part is that the act of deriving a conclusion has never been left to any mathematical or algorithmic model to derive. As we humans govern the various evaluations, and although we may seek to automate as much as possible to feed into the final conclusion, we are the entity that forms the final impression and conclusion.\n\nHere, we attempt to automate that final part. We will assign a team of expert agents to provide an educated summary of the results that the evaluation notebook printed out. We'll then push the team to provide us with a recommendation as to whether we should implement the new feature of prompt compression or not. We provide the team with the actual results of the evaluation notebook, but in order to examine its reliability, we then task it again, this time providing it with mocked results that are much poorer, hoping that the team will apply judgment and provide a different recommendation. All of this is done without any human intervention.\n\nAs we did before, we start by defining the task for our team to fulfill.\n\n#### Defining the task to be fulfilled by the team\n\nOur aim is to provide the team with the printout of the evaluation notebook from the previous section. That printout describes, in words, the change in agreement rate, the impact on the number of prompt tokens, and the processing runtime, all due to employing the LLMLingua prompt compression method.\n\nWe then copy that from the previous notebook and paste it as a text string.\n\nNote that we have also created another text string of results (which are mocked results that are much worse than the true results), but we see that the agreement rate is very low, and the reduction in token count due to compression is much less significant.\n\nAs we did in the visualization case, we then create the instructions for the team; we paste the results into the task description for the team to refer to when deriving its conclusion. We have two task descriptions, as we will have two separate runs, one with the true results and one with the mocked bad results.\n\nWe will now allocate the roles.\n\n#### Defining the agents and assigning team members roles\n\nFor this task, we would need three team members: a principal engineer who is an experienced technical person, a technical writer who writes the conclusion as per the principal engineer’s feedback, and a team lead to verify when the task is complete, which was defined in the previous task.\n\n#### Defining a group conversation\n\nHere, we define the group conversation, just like we did in the visualization part. This time, we have a new group conversation manager, as the group consists of different agents.\n\n#### Deploying the team\n\nThe team lead tasks the manager with the task we defined. The manager then delegates the work to the writer and the principal engineer.\n\nHere are the highlights of that automated conversation as it appears on the screen:\n\n```", "```py\n\nThe agents have a few iterations between them and come to an agreement regarding the summary and the conclusion.\n\nThey provide a summary of the numeric results and seal it with the following recommendation:\n\n```", "```py\n\nThe team agrees on a cautious approach to presenting the various trade-offs and avoids making a decision in spite of being tasked to do so.\n\nOne would wonder, could a definite decision to adopt or not to adopt the method be made here?\n\n#### Evaluation of the team’s judgment\n\nNow, we will ask the team to perform the same action, this time providing it with the mocked results that make the compression method seem much less effective and with a great reduction in agreement with the classification of the noncompressed method.\n\nThe team has a conversation, and the final agreement summary is sealed with the following statement:\n\n```"]