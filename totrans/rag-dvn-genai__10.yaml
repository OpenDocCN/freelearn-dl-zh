- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG for Video Stock Production with Pinecone and OpenAI
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human creativity goes beyond the range of well-known patterns due to our unique
    ability to break habits and invent new ways of doing anything, anywhere. Conversely,
    Generative AI relies on our well-known established patterns across an increasing
    number of fields without really “creating” but rather replicating our habits.
    In this chapter, therefore, when we use the term “create” as a practical term,
    we only mean “generate.” Generative AI, with its efficiency in automating tasks,
    will continue its expansion until it finds ways of replicating any human task
    it can. We must, therefore, learn how these automated systems work to use them
    for the best in our projects. Think of this chapter as a journey into the architecture
    of RAG in the cutting-edge hybrid human and AI agent era we are living in. We
    will assume the role of a start-up aiming to build an AI-driven downloadable stock
    of online videos. To achieve this, we will establish a team of AI agents that
    will work together to create a stock of commented and labeled videos.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Our journey begins with the Generator agent in *Pipeline 1: The Generator and
    the Commentator*. The Generator agent creates world simulations using Sora, an
    OpenAI text-to-video model. You’ll see how the *inVideo* AI application, powered
    by Sora, engages in “ideation,” transforming an idea into a video. The Commentator
    agent then splits the AI-generated videos into frames and generates technical
    comments with an OpenAI vision model. Next, in *Pipeline 2: The Vector Store Administrator,*
    we will continue our journey and build the Vector Store Administrator that manages
    Pinecone. The Vector Store Administrator will embed the technical video comments
    generated by the Commentator, upsert the vectorized comments, and query the Pinecone
    vector store to verify that the system is functional. Finally, we will build the
    Video Expert that processes user inputs, queries the vector store, and retrieves
    the relevant video frames. Finally, in *Pipeline 3: The Video Expert*, the Video
    Expert agent will augment user inputs with the raw output of the query and activate
    its expert OpenAI GPT-4o model, which will analyze the comment, detect imperfections,
    reformulate it more efficiently, and provide a label for the video.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will know how to automatically generate a stock
    of short videos by automating the process of going from raw footage to videos
    with descriptions and labels. You’ll be able to offer a service where users can
    simply type a few words and obtain a video with a custom, real-time description
    and label.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'Summing that up, this chapter covers the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Designing Generative AI videos and comments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting videos into frames for OpenAI’s vision analysis models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding the videos and upserting the vectors to a Pinecone index
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the vector store
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving and correcting the video comments with OpenAI GPT-4o
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically labeling raw videos
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying the full result of the raw video process with a commented and labeled
    video
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating outputs and implementing metric calculations
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining the architecture of RAG for video production.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of RAG for video production
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automating the process of real-world video generation, commenting, and labeling
    is extremely relevant in various industries, such as media, marketing, entertainment,
    and education. Businesses and creators are continuously seeking efficient ways
    to produce and manage content that can scale with growing demand. In this chapter,
    you will acquire practical skills that can be directly applied to meet these needs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of our RAG video production use case in this chapter is to process
    AI-generated videos using AI agents to create a video stock of labeled videos
    to identify them. The system will also dynamically generate custom descriptions
    by pinpointing AI-generated technical comments on specific frames within the videos
    that fit the user input. *Figure 10.1* illustrates the AI-agent team that processes
    RAG for video production:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a video production process  Description automatically generated](img/B31169_10_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: From raw videos to labeled and commented videos'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'We will implement AI agents for our RAG video production pipeline that will:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Generate raw videos automatically and download them
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the videos into frames
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze a sample of frames
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activate an OpenAI LLM model to generate technical comments
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the technical comments with a unique index, the comment itself, the frame
    number analyzed, and the video file name
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upsert the data in a Pinecone index vector store
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query the Pinecone vector store with user inputs
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the specific frame within a video that is most similar to its technical
    comment
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augment the user input with the technical comment of the retrieved frame
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask the OpenAI LLM to analyze the logic of the technical comment that may contain
    contradictions and imperfections detected in the video and then produce a dynamic,
    well-tailored description of the video with the frame number and the video file
    name
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the selected video
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the outputs and apply metric calculations
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will thus go from raw videos to labeled videos with tailored descriptions
    based on the user input. For example, we will be able to ask precise questions
    such as the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This means that the system will be able to find a frame (image) within the
    initially unlabeled video, select the video, display it, and generate a tailored
    comment dynamically. To attain our goal, we will implement AI agents in three
    pipelines, as illustrated in the following figure:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B31169_10_02.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: The RAG for Video Production Ecosystem with Generative AI agents'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what you see in the figure above is:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 1**: The **Generator** and the **Commentator**'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Generator** produces AI-generated videos with OpenAI Sora. The **Commentator**
    splits the videos into frames that are commented on by one of OpenAI’s vision
    models. The **Commentator** agent then saves the comments.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 2**: **The Vector Store Administrator**'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline will embed and upsert the comments made by *Pipeline 1* to a Pinecone
    index.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 3**: **The Video Expert**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline will query the Pinecone vector store based on user input. The
    query will return the most similar frame within a video, augment the input with
    the technical comment, and ask OpenAI GPT-4o to find logic imperfections in the
    video, point them out, and then produce a tailored comment of the video for the
    user and a label. This section also contains evaluation functions (the Evaluator)
    and metric calculations.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Time measurement functions are encapsulated in several of the key functions
    of the preceding ecosystem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: The RAG video production system we will build allows indefinite scaling by processing
    one video at a time, using only a CPU and little memory, while leveraging Pinecone’s
    storage capacity. This effectively demonstrates the concept of automated video
    production, but implementing this production system in a real-life project requires
    hard work. However, the technology is there, and the future of video production
    is undergoing a historical evolution. Let’s dive into the code, beginning with
    the environment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: The environment of the video production ecosystem
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Chapter10` directory on GitHub contains the environment for all four notebooks
    in this chapter:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '`Videos_dataset_visualization.ipynb`'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline_1_The_Generator_and_the_Commentator.ipynb`'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline_2_The_Vector_Store_Administrator.ipynb`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline_3_The_Video_Expert.ipynb`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each notebook includes an *Installing the environment* section, including a
    set of the following sections that are identical across all notebooks:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '*Importing modules and libraries*'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GitHub*'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video download and display functions*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI*'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pinecone*'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to establish a common pre-production installation policy that
    will focus on the pipelines’ content once we dive into the RAG for video production
    code. This policy is limited to the scenario described in this chapter and will
    vary depending on the requirements of each real-life production environment.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: The notebooks in this chapter only require a CPU, limited memory, and limited
    disk space. As such, the whole process can be streamlined indefinitely one video
    at a time in an optimized, scalable environment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by importing the modules and libraries we need for our project.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Importing modules and libraries
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal is to prepare a pre-production global environment common to all the
    notebooks. As such, the modules and libraries are present in all four notebooks
    regardless of whether they are used or not in a specific program:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Each of the four notebooks contains these modules and libraries, as shown in
    the following table:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | **Comment** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| `from IPython.display import HTML` | To display videos |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| `import base64` | To encode videos as `base64` |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| `from base64 import b64encode` | To encode videos as `base64` |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| `import os` | To interact with the operating system |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| `import subprocess` | To run commands |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| `import time` | To measure execution time |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| `import csv` | To save comments |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| `import uuid` | To generate unique IDs |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| `import cv2` | To split videos (open source computer vision library) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| `from PIL import Image` | To display videos |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| `import pandas as pd` | To display comments |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| `import numpy as np` | To use Numerical Python |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| `from io import BytesIO` | For a binary stream of data in memory |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: 'Table 10.1: Modules and libraries for our video production system'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: The `Code` column contains the module or library name, while the `Comment` column
    provides a brief description of their usage. Let’s move on to GitHub commands.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`download(directory, filename)` is present in all four notebooks. The main
    function of `download(directory, filename)` is to download the files we need from
    the book’s GitHub repository:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding function takes two arguments:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '`directory`, which is the GitHub directory that the file we want to download
    is located in'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename`, which is the name of the file we want to download'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OpenAI package is installed in all three pipeline notebooks but not in
    `Video_dataset_visualization.ipynb`, which doesn’t require an LLM. You can retrieve
    the API key from a file or enter it manually (but it will be visible):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will need to sign up at `www.openai.com` before running the code and obtain
    an API key. The program installs the `openai` package:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we set an environment variable for the API key:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Pinecone
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *Pinecone* section is only present in `Pipeline_2_The_Vector_Store_Administrator.ipynb`
    and `Pipeline_3_The_Video_Expert.ipynb` when the Pinecone vector store is required.
    The following command installs Pinecone, and then Pinecone is imported:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The program then retrieves the key from a file (or you can enter it manually):'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In production, you can set an environment variable or implement the method that
    best fits your project so that the API key is never visible.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: The *Evaluator* section of `Pipeline_3_The_Video_Expert.ipynb` contains its
    own requirements and installations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have defined the environment for all four notebooks, which contain
    the same sub-sections we just described in their respective *Installing the environment*
    sections. We can now fully focus on the processes involved in the video production
    programs. We will begin with the Generator and Commentator.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 1: Generator and Commentator'
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A revolution is on its way in computer vision with automated video generation
    and analysis. We will introduce the Generator AI agent with Sora in *The AI-generated
    video dataset* section. We will explore how OpenAI Sora was used to generate the
    videos for this chapter with a text-to-video diffusion transformer. The technology
    itself is something we have expected and experienced to some extent in professional
    film-making environments. However, the novelty relies on the fact that the software
    has become mainstream in a few clicks, with inVideo, for example!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: In the *The Generator and the Commentator* section, we will extend the scope
    of the Generator to collecting and processing the AI-generated videos. The Generator
    splits the videos into frames and works with the Commentator, an OpenAI LLM, to
    produce comments on samples of video frames.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The Generator’s task begins by producing the AI-generated video dataset.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The AI-generated video dataset
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first AI agent in this project is a text-to-video diffusion transformer
    model that generates a video dataset we will implement. The videos for this chapter
    were specifically generated by Sora, a text-to-video AI model released by OpenAI
    in February 2024\. You can access Sora to view public AI-generated videos and
    create your own at [https://ai.invideo.io/](https://ai.invideo.io/). AI-generated
    videos also allow for free videos with flexible copyright terms that you can check
    out at [https://invideo.io/terms-and-conditions/](https://invideo.io/terms-and-conditions/).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Once you have gone through this chapter, you can also create your own video
    dataset with any source of videos, such as smartphones, video stocks, and social
    media.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated videos enhance the speed of creating video datasets. Teams do not
    have to spend time finding videos that fit their needs. They can obtain a video
    quickly with a prompt that can be an idea expressed in a few words. AI-generated
    videos represent a huge leap into the future of AI applications. Sora’s potential
    applies to many industries, including filmmaking, education, and marketing. Its
    ability to generate nuanced video content from simple text prompts opens new avenues
    for creative and educational outputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Although AI-generated videos (and, in particular, diffusion transformers) have
    changed the way we create world simulations, this represents a risk for jobs in
    many areas, such as filmmaking. The risk of deep fakes and misinformation is real.
    At a personal level, we must take ethical considerations into account when we
    implement Generative AI in a project, thus producing constructive, ethical, and
    realistic content.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how a diffusion transformer can produce realistic content.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: How does a diffusion transformer work?
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the core of Sora, as described by Liu et al., 2024 (see the *References*
    section), is a diffusion transformer model that operates between an encoder and
    a decoder. It uses user text input to guide the content generation, associating
    it with patches from the encoder. The model iteratively refines these noisy latent
    representations, enhancing their clarity and coherence. Finally, the refined data
    is passed to the decoder to reconstruct high-fidelity video frames. The technology
    involved includes vision transformers such as CLIP and LLMs such as GPT-4, as
    well as other components OpenAI continually includes in its vision model releases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder and decoder are integral components of the overall diffusion model,
    as illustrated in *Figure 10.3*. They both play a critical role in the workflow
    of the transformer diffusion model:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: The encoder’s primary function is to compress input data, such
    as images or videos, into a lower-dimensional latent space. The encoder thus transforms
    high-dimensional visual data into a compact representation while preserving crucial
    information. A lower-dimensional latent space obtained is a compressed representation
    of high-dimensional data, retaining essential features while reducing complexity.
    For example, a high-resolution image (1024x1024 pixels, 3 color channels) can
    be compressed by an encoder into a vector of 1000 values, capturing key details
    like shape and texture. This makes processing and manipulating images more efficient.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: The decoder reconstructs the original data from the latent representation
    produced by the encoder. It performs the encoder’s reverse operation, transforming
    the low-dimensional latent space back into high-dimensional pixel space, thus
    generating the final output, such as images or videos.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a workflow  Description automatically generated](img/B31169_10_03.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: The encoding and decoding workflow of video diffusion models'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of a diffusion transformer model goes through five main steps,
    as you can observe in the previous figure:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: The visual encoder transforms datasets of images into a lower-dimensional latent
    space.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The visual encoder splits the lower-dimensional latent space into patches that
    are like words in a sentence.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The diffusion transformer associates user text input with its dictionary of
    patches.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The diffusion transformer iteratively refines noisy image representations generated
    to produce coherent frames.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The visual decoder reconstructs the refined latent representations into high-fidelity
    video frames that align with the user’s instructions.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The video frames can then be played in a sequence. Every second of a video contains
    a set of frames. We will be deconstructing the AI-generated videos into frames
    and commenting on these frames later. But for now, we will analyze the video dataset
    produced by the diffusion transformer.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the diffusion transformer model video dataset
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open the `Videos_dataset_visualization.ipynb` notebook on GitHub. Hopefully,
    you have installed the environmentas described earlier in this chapter. We will
    move on to writing the download and display functions we need.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Video download and display functions
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The three main functions each use `filename` (the name of the video file) as
    an argument. The three main functions download and display videos, and display
    frames in the videos.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '`download_video` downloads one video at a time from the GitHub dataset, calling
    the `download` function defined in the *GitHub* subsection of *The environment*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`display_video(file_name)` displays the video file downloaded by first encoding
    in `base64`, a binary-to-text encoding scheme that represents binary data in ASCII
    string format. Then, the encoded video is displayed in HTML:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`display_video_frame` takes `file_name`, `frame_number`, and `size` (the image
    size to display) as arguments to display a frame in the video. The function first
    opens the video file and then extracts the frame number set by `frame_number`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The function converts the file from the BGR (blue, green, and red) to the RGB
    (red, green, and blue) channel, converts it to PIL, an image array (such as one
    handled by OpenCV), and resizes it with the `size` parameters:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, the function encodes the image in string format with `base64` and
    displays it in HTML:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once the environment is installed and the video processing functions are ready,
    we will display the introduction video.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Introduction video (with audio)
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following cells download and display the introduction video using the functions
    we created in the previous section. A video file is selected and downloaded with
    the `download_video` function:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output confirms the selection and download status:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can choose to display only a single frame of the video as a thumbnail with
    the `display_video_frame` function by providing the file name, the frame number,
    and the image size to display. The program will first compute `frame_count` (the
    number of frames in the video), `frame_rate` (the number of frames per second),
    and `video_duration` (the duration of the video). Then, it will make sure `frame_number`
    (the frame we want to display) doesn’t exceed `frame_count`. Finally, it displays
    the frame as a thumbnail:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, `frame_number` is set to `5`, but you can choose another value. The output
    shows the information on the video and the thumbnail:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also display the full video if needed:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The video will be displayed and can be played with the audio track:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![A person sitting in a chair  Description automatically generated](img/B31169_10_05.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: AI-generated video'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe and display AI-generated videos in the `/videos` directory of
    this chapter’s GitHub directory. You can host this dataset in another location
    and scale it to the volume that meets your project’s specifications. The educational
    video dataset of this chapter is listed in `lfiles`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We can now move on and display any video we wish.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Displaying thumbnails and videos in the AI-generated dataset
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section is a generalization of the *Introduction video (with audio)* section.
    This time, instead of downloading one video, it downloads all the videos and displays
    the thumbnails of all the videos. You can then select a video in the list and
    display it.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first collects the video dataset:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output shows the file names of the downloaded videos:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The program calculates the number of videos in the list:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The program goes through the list and displays the information for each video
    and displays its thumbnail:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The information on the video and its thumbnail is displayed:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You can select a video in the list and display it:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can click on the video and watch it:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a football uniform pointing at a football  Description automatically
    generated](img/B31169_10_07.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Video of a football player'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: We have explored how the AI-generated videos were produced and visualized the
    dataset. We are now ready to build the Generator and the Commentator.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: The Generator and the Commentator
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset of AI-generated videos is ready. We will now build the Generator
    and the Commentator, which processes one video at a time, making scaling seamless.
    An indefinite number of videos can be processed one at a time, requiring only
    a CPU and limited disk space. The Generator and the Commentator work together,
    as shown in *Figure 10.8*. These AI agents will produce raw videos from text and
    then split them into frames that they will comment on:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a sports game  Description automatically generated with medium
    confidence](img/B31169_10_08.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: The Generator and the Commentator work together to comment on
    video frames'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'The Generator and the Commentator produce the commented frames required in
    four main steps that we will build in Python:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The **Generator** generates the text-to-video inVideo video dataset based on
    the video production team’s text input. In this chapter, it is a dataset of sports
    videos.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Generator** runs a scaled process by selecting one video at a time.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Generator** splits the video into frames (images)
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Commentator** samples frames (images) and comments on them with an OpenAI
    LLM model. Each commented frame is saved with:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unique ID
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comment
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Video file name
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now build the Generator and the Commentator in Python, starting with
    the AI-generated videos. Open `Pipeline_1_The_Generator_and_the_Commentator.ipynb`
    in the chapter’s GitHub directory. See the *The environment* section of this chapter
    for a description of the *Installing the environment* section of this notebook.
    The process of going from a video to comments on a sample of frames only takes
    three straightforward steps in Python:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Displaying the video
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting the video into frames
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Commenting on the frames
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will define functions for each step and call them in the `Pipeline-1 Controller`
    section of the program. The first step is to define a function to display a video.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Step 1\. Displaying the video
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `download` function is in the *GitHub* subsection of the *Installing the
    environment* section of this notebook. It will be called by the *Vector Store
    Administrator-Pipeline 1* in the *Administrator-Pipeline 1* section of this notebook
    on GitHub.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '`display_video(file_name)` is the same as defined in the previous section,
    *The AI-generated video dataset*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The downloaded video will now be split into frames.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Step 2\. Splitting video into frames
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `split_file(file_name)` function extracts frames from a video, as in the
    previous section, *The AI-generated video dataset*. However, in this case, we
    will expand the function to save frames as JPEG files:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have split the video into frames and saved them as JPEG images with their
    respective frame number, `frame_number`. The Generator’s job finishes here and
    the Commentator now takes over.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Step 3\. Commenting on the frames
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Generator has gone from text-to-video to splitting the video and saving
    the frames as JPEG frames. The Commentator now takes over to comment on the frames
    with three functions:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '`generate_openai_comments(filename)` asks the GPT-4 series vision model to
    analyze a frame and produce a response that contains a comment describing the
    frame'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_comment(response_data)` extracts the comment from the response'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_comment(comment, frame_number, file_name)` saves the comment'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to build the Commentator’s extraction function first:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We then write a function to save the extracted comment in a CSV file that bears
    the same name as the video file:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The goal is to save the comment in a format that can directly be upserted to
    Pinecone:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '`ID`: A unique string ID generated with `str(uuid.uuid4())`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FrameNumber`: The frame number of the commented JPEG'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Comment`: The comment generated by the OpenAI vision model'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileName`: The name of the video file'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Commentator’s main function is to generate comments with the OpenAI vision
    model. However, in this program’s scenario, we will not save all the frames but
    a sample of the frames. The program first determines the number of frames to process:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, a sample frequency is set that can be modified along with a counter:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The Commentator will then go through the sampled frames and request a comment:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The message is very concise: `"What is happening in this image?"` The message
    also includes the image of the frame:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Once a response is returned, the `generate_comment` and `save_comment` functions
    are called to extract and save the comment, respectively:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The final function we require of the Commentator is to display the comments
    by loading the CSV file produced in a pandas DataFrame:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The function returns the DataFrame with the comments. An administrator controls
    *Pipeline 1*, the Generator, and the Commentator.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline 1 controller
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The controller runs jobs for the preceding three steps of the Generator and
    the Commentator. It begins with *Step 1*, which includes selecting a video, downloading
    it, and displaying it. In an automated pipeline, these functions can be separated.
    For example, a script would iterate through a list of videos, automatically select
    each one, and encapsulate the controller functions. In this case, in a pre-production
    and educational context, we will collect, download, and display the videos one
    by one:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The controller then splits the video into frames and comments on the frames
    of the video:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The controller activates the Generator to produce comments on frames of the
    video:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The response time is measured as well. The controller then adds additional
    outputs to display the number of frames, the comments, the content generation
    time, and the total controller processing times:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The controller has completed its task of producing content. However, depending
    on your project, you can introduce dynamic RAG for some or all the videos. If
    you need this functionality, you can apply the process described in *Chapter 5*,
    *Boosting RAG Performance with Expert Human Feedback*, to the Commentator’s outputs,
    including the cosine similarity quality control metrics, as we will in the *Pipeline
    3: The Video Expert* section of this chapter.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The controller can also save the comments and frames.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Saving comments
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To save the comments, set `save=True`. To save the frames, set `save_frames=True`.
    Set both values to `False` if you just want to run the program and view the outputs,
    but, in our case, we will set them as `True`:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The comment is saved in CSV format in `cpath` and contains the file name with
    the `.csv` extension and in the location of your choice. In this case, the files
    are saved on Google Drive (make sure the path exists):'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output confirms that a file is saved:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The frames are saved in a root name direction, for which we remove the extension
    with `root_name = root_name + extension.strip(''.'')`:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The output is a directory with all the frames generated in it. We should delete
    the files if the controller runs in a loop over all the videos in a single session.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Deleting files
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To delete the files, just set `delf=True`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You can now process an unlimited number of videos one by one and scale to whatever
    size you wish, as long as you have disk space and a CPU!
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 2: The Vector Store Administrator'
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Vector Store Administrator AI agent performs the tasks we implemented in
    *Chapter 6*, *Scaling RAG Bank Customer Data with Pinecone.* The novelty in this
    section relies on the fact that all the data we upsert for RAG is AI-generated.
    Let’s open `Pipeline_2_The_Vector_Store_Administrator.ipynb` in the GitHub repository.
    We will build the Vector Store Administrator on top of the Generator and the Commentator
    AI agents in four steps, as illustrated in the following figure:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a video expert  Description automatically generated](img/B31169_10_09.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Workflow of the Vector Store Administrator from processing to
    querying video frame comments'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing the video comments**: The Vector Store Administrator will load
    and prepare the comments for chunking as in the *Pipeline 2: Scaling a Pinecone
    Index (vector store)* section of *Chapter 6*. Since we are processing one video
    at a time in a pipeline, the system deletes the files processed, which keeps disk
    space constant. You can enhance the functionality and scale this process indefinitely.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunking and embedding the dataset**: The column names `(''ID'', ''FrameNumber'',
    ''Comment'', ''FileName'')` of the dataset have already been prepared by the Commentator
    AI agent in *Pipeline 1*. The program chunks and embeds the dataset using the
    same functionality as in *Chapter 6* in the *Chunking and embedding the dataset*
    section.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The Pinecone index**: The Pinecone Index is created, and the data is upserted
    as in the *Creating the Pinecone Index* and *Upserting* sections of *Chapter*
    6.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Querying the vector store after upserting the dataset**: This follows the
    same process as in *Chapter 6*. However, in this case, the retrieval is hybrid,
    using both the Pinecone vector store and a separate file system to store videos
    and video frames.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through *Steps 1* to *3* in the notebook to examine the Vector Store Administrator’s
    functions. After *Step 3*, the Pinecone index is ready for hybrid querying.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Querying the Pinecone index
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the notebook on GitHub, *Step 4: Querying the Pinecone index* implements
    functions to find a comment that matches user input and trace it to the frame
    of a video. This leads to the video source and frame, which can be displayed.
    We can display the videos and frames from the location we wish. This hybrid approach
    thus involves querying the Pinecone Index to retrieve information and also retrieve
    media files from another location.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: We saw that a vector store can contain images that are queried, as implemented
    in *Chapter 4*, *Multimodal Modular RAG for Drone Technology*. In this chapter,
    the video production use case videos and frame files are stored separately. In
    this case, it is in the GitHub repository. In production, the video and frame
    files can be retrieved from any storage system we need, which may or may not prove
    to be more cost-effective than storing data on Pinecone. The decision to store
    images in a vector store or a separate location will depend on the project’s needs.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by defining the number of top-k results we wish to process:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We then design a rather difficult prompt:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Only a handful of frames in the whole video dataset contain an image of a basketball
    player jumping to score a slam dunk. Can our system find it? Let’s find out.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'We first embed our query to match the format of the data in the vector store:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then we run a similarity vector search between the query and the dataset:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Finally, we display the content of the response and the response time:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output contains the ID of the comment retrieved and its score:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The output also contains the comment generated by the OpenAI LLM (the Commentator
    agent):'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The final output contains the frame number that was commented, the video file
    of the frame, and the retrieval time:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can display the video by downloading it based on the file name:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Then, use a standard Python function to display it:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The video containing a basketball player performing a dunk is displayed:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![A basketball hoop with net  Description automatically generated](img/B31169_10_10.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Video output'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take this further with more precision by displaying the frame of the
    comment retrieved:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output shows the exact frame that corresponds to the user input:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![A person holding a ball  Description automatically generated](img/B31169_10_11.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Video frame corresponding to our input'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Only the frames of `basketball3.mp4` were saved in the GitHub repository for
    disk space reasons for this program. In production, all the frames you decide
    you need can be stored and retrieved.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'The team of AI agents in this chapter worked together to generate videos (the
    Generator), comment on the video frames (the Commentator), upsert embedded comments
    in the vector store (the Vector Store Administrator), and prepare the retrieval
    process (the Vector Store Administrator). We also saw that the retrieval process
    already contained augmented input and output thanks to the OpenAI LLM (the Commentator)
    that generated natural language comments. The process that led to this point will
    definitely be applied in many domains: firefighting, medical imagery, marketing,
    and more.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: What more can we expect from this system? The Video Expert AI agent will answer
    that.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 3: The Video Expert'
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The role of the OpenAI GPT-4o Video Expert is to analyze the comment made by
    the Commentator OpenAI LLM agent, point out the cognitive dissonances (things
    that don’t seem to fit together in the description), rewrite the comment, and
    provide a label. The workflow of the Video Expert, as illustrated in the following
    figure, also includes the code of the *Metrics calculations and display* section
    of *Chapter 7*, *Building Scalable Knowledge-Graph-Based RAG with Wikipedia API
    and LlamaIndex*.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: The Commentator’s role was only to describe what it saw. The Video Expert is
    there to make sure it makes sense and also label the videos so they can be classified
    in the dataset for further use.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a video expert  Description automatically generated](img/B31169_10_12.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Workflow of the Video Expert for automated dynamics descriptions
    and labeling'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '**The Pinecone index** will connect to the Pinecone index as described in the
    *Pipeline 2\. The Vector Store Administrator* section of this chapter. This time,
    we will not upsert data but connect to the vector store.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define the RAG functions** utilizing the straightforward functions we built
    in *Pipeline 1* and *Pipeline 2* of this chapter.'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Querying the vector store** is nothing but querying the Pinecone Index as
    described in *Pipeline 2* of this chapter.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrieval augmented generation** finally determines the main role of Video
    Expert GPT-4o, which is to analyze and improve the vector store query responses.
    This final step will include evaluation and metric functions.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are as many strategies as projects to implement the video production
    use case we explored in this chapter, but the Video Expert plays an important
    role. Open `Pipeline_3_The_Video_Expert.ipynb` on GitHub and go to the *Augmented
    Retrieval Generation* section in *Step 2: Defining the RAG functions*.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'The function makes an OpenAI GPT-4o call, like for the Commentator in *Pipeline
    1*. However, this time, the role of the LLM is quite different:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The instructions for GPT-4o are:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '`You will be provided with comments of an image frame taken from a video`:
    This instructs the LLM to analyze the AI-generated comments. The Commentator had
    to remain neutral and describe the frame as it saw it. The role of the Video Expert
    agent is different: it has to analyze and enhance the comment.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1\. Point out the cognitive dissonances`: This instructs the model to find
    contradictions or discrepancies in the comment that can come from the way the
    AI-generated video was produced as well (lack of logic in the video).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2\. Rewrite the comment in a logical engaging style`: This instructs the Video
    Expert agent to rewrite the comment going from a technical comment to a description.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3\. Provide a label for this image such as Label: basketball, football, soccer
    or other label`: This instructs the model to provide a label for further use.
    On GitHub, *Step 3: Querying the Vector Store* reproduces the query and output
    described in *Pipeline 2* for a basketball player scoring with a dunk, with the
    corresponding video and frame. The output is:'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The comment provided seems acceptable. However, let’s see what GPT-4o thinks
    of it. The *Step 4: Retrieval Augmented Generation* section on GitHub takes the
    output and submits it as the user prompt to the Video Expert agent:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We then call the Video Expert agent to obtain its expertise:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output provides the Video Expert’s insights:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: The response is well-structured and acceptable. The output may vary from one
    run to another due to the stochastic “creative” nature of Generative AI agents.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Evaluator* section that follows *Step 4* runs ten examples using the same
    process as the basketball request we just made. Each example thus contains:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: A user prompt
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comment returned by the vector store query
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The enhanced comment made by the GPT-4o model
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each example also contains the same evaluation process as in *Chapter 7, Building
    Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex,* in the
    *Examples for metrics* section. However, in this case, the human evaluator suggests
    content instead of a score (0 to 1). The human content becomes the ground truth,
    the expected output.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Before beginning the evaluation, the program creates scores to keep track of
    the original response made by the query.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'The human evaluator rewrites the output provided by the Video Expert:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The content rewritten by the Video Expert is extracted from the response:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The human comment (ground truth, the reference output) and the LLM comments
    are displayed:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Then, the cosine similarity score between the human and LLM comments is calculated
    and appended to `scores`:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The original score provided with the query is appended to the query’s retrieval
    score, `rscores`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output displays the human feedback, the comment rewritten by GPT-4o (the
    Video Expert), and the similarity score:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: This program contains ten examples, but we can enter a corpus of as many examples
    as we wish to evaluate the system. The evaluation of each example applies the
    same choice of metrics as in *Chapter 7**.* After the examples have been evaluated,
    the *Metrics calculations and display* section in the program also runs the metric
    calculations defined in the section of the same name in *Chapter 7*.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use all the metrics to analyze the performance of the system. The time
    measurements throughout the program also provide insights. The first metric, accuracy,
    is a good metric to start with. In this case, it shows that there is room for
    progress:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Some requests and responses were challenging and required further work to improve
    the system:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Checking the quality of the videos and their content
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking the comments and possibly modifying them with human feedback, as we
    did in *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-tuning a model with images and text as we did in *Chapter 9*, *Empowering
    AI Models: Fine-Tuning RAG Data and Human Feedback*'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing any other constructive idea that the video production team comes up
    with
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that RAG-driven Generative AI systems in production are very effective.
    However, the road from design to production requires hard human effort! Though
    AI technology has made tremendous progress, it still requires humans to design,
    develop, and implement it in production.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the hybrid era of human and AI agents, focusing
    on the creation of a streamlined process for generating, commenting, and labeling
    videos. By integrating cutting-edge Generative AI models, we demonstrated how
    to build an automated pipeline that transforms raw video inputs into structured,
    informative, and accessible video content.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 'Our journey began with the **Generator** agent in *Pipeline 1*: *The Generator
    and the Commentator*, which was tasked with creating video content from textual
    ideas. We can see that video generation processes will continue to expand through
    seamless integration ideation and descriptive augmentation generative agents.
    In *Pipeline 2*: *The Vector Store Administrator*, we focused on organizing and
    embedding the generated comments and metadata into a searchable vector store.
    In this pipeline, we highlighted the optimization process of building a scalable
    video content library with minimal machine resources using only a CPU and no GPU.
    Finally, in *Pipeline 3*: *The Video Expert*, we introduced the Expert AI agent,
    a video specialist designed to enhance and label the video content based on user
    inputs. We also implemented evaluation methods and metric calculations.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we had constructed a comprehensive, automated RAG-driven
    Generative AI system capable of generating, commenting on, and labeling videos
    with minimal human intervention. This journey demonstrated the power and potential
    of combining multiple AI agents and models to create an efficient pipeline for
    video content creation.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The techniques and tools we explored can revolutionize various industries by
    automating repetitive tasks, enhancing content quality, and making information
    retrieval more efficient. This chapter not only provided a detailed technical
    roadmap but also underscored the transformative impact of AI in modern content
    creation and management. You are now all set to implement RAG-driven Generative
    AI in real-life projects.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with yes or no:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Can AI now automatically comment and label videos?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does video processing involve splitting the video into frames?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can the programs in this chapter create a 200-minute movie?
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the programs in this chapter require a GPU?
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are the embedded vectors of the video content stored on disk?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the scripts involve querying a database for retrieving data?
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there functionality for displaying images in the scripts?
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it useful to have functions that specifically check file existence and size
    in any of the scripts?
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a focus on multimodal data in these scripts?
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do any of the scripts mention applications of AI in real-world scenarios?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sora video generation model information and access:'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sora** | **OpenAI**: [https://ai.invideo.io/](https://ai.invideo.io/)'
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://openai.com/index/video-generation-models-as-world-simulators/](https://openai.com/index/video-generation-models-as-world-simulators/)'
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sora: A Review on Background, Technology, Limitations, and Opportunities of
    Large Vision Models* by Yixin Liu, Kai Zhang, Yuan Li, et al.: [https://arxiv.org/pdf/2402.17177](https://arxiv.org/pdf/2402.17177)'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-374
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI, ChatGPT: [https://openai.com/chatgpt/](https://openai.com/chatgpt/)'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI, Research: [https://openai.com/research/](https://openai.com/research/)'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinecone: [https://docs.pinecone.io/home](https://docs.pinecone.io/home)'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pinecone: [https://docs.pinecone.io/home](https://docs.pinecone.io/home)'
- en: Join our community on Discord
  id: totrans-378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/rag](https://www.packt.link/rag)'
- en: '![](img/QR_Code50409000288080484.png)'
  id: totrans-381
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code50409000288080484.png)'
