- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG for Video Stock Production with Pinecone and OpenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human creativity goes beyond the range of well-known patterns due to our unique
    ability to break habits and invent new ways of doing anything, anywhere. Conversely,
    Generative AI relies on our well-known established patterns across an increasing
    number of fields without really “creating” but rather replicating our habits.
    In this chapter, therefore, when we use the term “create” as a practical term,
    we only mean “generate.” Generative AI, with its efficiency in automating tasks,
    will continue its expansion until it finds ways of replicating any human task
    it can. We must, therefore, learn how these automated systems work to use them
    for the best in our projects. Think of this chapter as a journey into the architecture
    of RAG in the cutting-edge hybrid human and AI agent era we are living in. We
    will assume the role of a start-up aiming to build an AI-driven downloadable stock
    of online videos. To achieve this, we will establish a team of AI agents that
    will work together to create a stock of commented and labeled videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our journey begins with the Generator agent in *Pipeline 1: The Generator and
    the Commentator*. The Generator agent creates world simulations using Sora, an
    OpenAI text-to-video model. You’ll see how the *inVideo* AI application, powered
    by Sora, engages in “ideation,” transforming an idea into a video. The Commentator
    agent then splits the AI-generated videos into frames and generates technical
    comments with an OpenAI vision model. Next, in *Pipeline 2: The Vector Store Administrator,*
    we will continue our journey and build the Vector Store Administrator that manages
    Pinecone. The Vector Store Administrator will embed the technical video comments
    generated by the Commentator, upsert the vectorized comments, and query the Pinecone
    vector store to verify that the system is functional. Finally, we will build the
    Video Expert that processes user inputs, queries the vector store, and retrieves
    the relevant video frames. Finally, in *Pipeline 3: The Video Expert*, the Video
    Expert agent will augment user inputs with the raw output of the query and activate
    its expert OpenAI GPT-4o model, which will analyze the comment, detect imperfections,
    reformulate it more efficiently, and provide a label for the video.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will know how to automatically generate a stock
    of short videos by automating the process of going from raw footage to videos
    with descriptions and labels. You’ll be able to offer a service where users can
    simply type a few words and obtain a video with a custom, real-time description
    and label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Summing that up, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing Generative AI videos and comments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting videos into frames for OpenAI’s vision analysis models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding the videos and upserting the vectors to a Pinecone index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the vector store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving and correcting the video comments with OpenAI GPT-4o
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically labeling raw videos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying the full result of the raw video process with a commented and labeled
    video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating outputs and implementing metric calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining the architecture of RAG for video production.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of RAG for video production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automating the process of real-world video generation, commenting, and labeling
    is extremely relevant in various industries, such as media, marketing, entertainment,
    and education. Businesses and creators are continuously seeking efficient ways
    to produce and manage content that can scale with growing demand. In this chapter,
    you will acquire practical skills that can be directly applied to meet these needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of our RAG video production use case in this chapter is to process
    AI-generated videos using AI agents to create a video stock of labeled videos
    to identify them. The system will also dynamically generate custom descriptions
    by pinpointing AI-generated technical comments on specific frames within the videos
    that fit the user input. *Figure 10.1* illustrates the AI-agent team that processes
    RAG for video production:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a video production process  Description automatically generated](img/B31169_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: From raw videos to labeled and commented videos'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will implement AI agents for our RAG video production pipeline that will:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate raw videos automatically and download them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split the videos into frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze a sample of frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activate an OpenAI LLM model to generate technical comments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Save the technical comments with a unique index, the comment itself, the frame
    number analyzed, and the video file name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upsert the data in a Pinecone index vector store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query the Pinecone vector store with user inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve the specific frame within a video that is most similar to its technical
    comment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augment the user input with the technical comment of the retrieved frame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ask the OpenAI LLM to analyze the logic of the technical comment that may contain
    contradictions and imperfections detected in the video and then produce a dynamic,
    well-tailored description of the video with the frame number and the video file
    name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the selected video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the outputs and apply metric calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will thus go from raw videos to labeled videos with tailored descriptions
    based on the user input. For example, we will be able to ask precise questions
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the system will be able to find a frame (image) within the
    initially unlabeled video, select the video, display it, and generate a tailored
    comment dynamically. To attain our goal, we will implement AI agents in three
    pipelines, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B31169_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: The RAG for Video Production Ecosystem with Generative AI agents'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what you see in the figure above is:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 1**: The **Generator** and the **Commentator**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Generator** produces AI-generated videos with OpenAI Sora. The **Commentator**
    splits the videos into frames that are commented on by one of OpenAI’s vision
    models. The **Commentator** agent then saves the comments.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 2**: **The Vector Store Administrator**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline will embed and upsert the comments made by *Pipeline 1* to a Pinecone
    index.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 3**: **The Video Expert**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This pipeline will query the Pinecone vector store based on user input. The
    query will return the most similar frame within a video, augment the input with
    the technical comment, and ask OpenAI GPT-4o to find logic imperfections in the
    video, point them out, and then produce a tailored comment of the video for the
    user and a label. This section also contains evaluation functions (the Evaluator)
    and metric calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Time measurement functions are encapsulated in several of the key functions
    of the preceding ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG video production system we will build allows indefinite scaling by processing
    one video at a time, using only a CPU and little memory, while leveraging Pinecone’s
    storage capacity. This effectively demonstrates the concept of automated video
    production, but implementing this production system in a real-life project requires
    hard work. However, the technology is there, and the future of video production
    is undergoing a historical evolution. Let’s dive into the code, beginning with
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The environment of the video production ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `Chapter10` directory on GitHub contains the environment for all four notebooks
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Videos_dataset_visualization.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline_1_The_Generator_and_the_Commentator.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline_2_The_Vector_Store_Administrator.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pipeline_3_The_Video_Expert.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each notebook includes an *Installing the environment* section, including a
    set of the following sections that are identical across all notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Importing modules and libraries*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*GitHub*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Video download and display functions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*OpenAI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Pinecone*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter aims to establish a common pre-production installation policy that
    will focus on the pipelines’ content once we dive into the RAG for video production
    code. This policy is limited to the scenario described in this chapter and will
    vary depending on the requirements of each real-life production environment.
  prefs: []
  type: TYPE_NORMAL
- en: The notebooks in this chapter only require a CPU, limited memory, and limited
    disk space. As such, the whole process can be streamlined indefinitely one video
    at a time in an optimized, scalable environment.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by importing the modules and libraries we need for our project.
  prefs: []
  type: TYPE_NORMAL
- en: Importing modules and libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal is to prepare a pre-production global environment common to all the
    notebooks. As such, the modules and libraries are present in all four notebooks
    regardless of whether they are used or not in a specific program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the four notebooks contains these modules and libraries, as shown in
    the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Code** | **Comment** |'
  prefs: []
  type: TYPE_TB
- en: '| `from IPython.display import HTML` | To display videos |'
  prefs: []
  type: TYPE_TB
- en: '| `import base64` | To encode videos as `base64` |'
  prefs: []
  type: TYPE_TB
- en: '| `from base64 import b64encode` | To encode videos as `base64` |'
  prefs: []
  type: TYPE_TB
- en: '| `import os` | To interact with the operating system |'
  prefs: []
  type: TYPE_TB
- en: '| `import subprocess` | To run commands |'
  prefs: []
  type: TYPE_TB
- en: '| `import time` | To measure execution time |'
  prefs: []
  type: TYPE_TB
- en: '| `import csv` | To save comments |'
  prefs: []
  type: TYPE_TB
- en: '| `import uuid` | To generate unique IDs |'
  prefs: []
  type: TYPE_TB
- en: '| `import cv2` | To split videos (open source computer vision library) |'
  prefs: []
  type: TYPE_TB
- en: '| `from PIL import Image` | To display videos |'
  prefs: []
  type: TYPE_TB
- en: '| `import pandas as pd` | To display comments |'
  prefs: []
  type: TYPE_TB
- en: '| `import numpy as np` | To use Numerical Python |'
  prefs: []
  type: TYPE_TB
- en: '| `from io import BytesIO` | For a binary stream of data in memory |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10.1: Modules and libraries for our video production system'
  prefs: []
  type: TYPE_NORMAL
- en: The `Code` column contains the module or library name, while the `Comment` column
    provides a brief description of their usage. Let’s move on to GitHub commands.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`download(directory, filename)` is present in all four notebooks. The main
    function of `download(directory, filename)` is to download the files we need from
    the book’s GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`directory`, which is the GitHub directory that the file we want to download
    is located in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename`, which is the name of the file we want to download'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OpenAI package is installed in all three pipeline notebooks but not in
    `Video_dataset_visualization.ipynb`, which doesn’t require an LLM. You can retrieve
    the API key from a file or enter it manually (but it will be visible):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will need to sign up at `www.openai.com` before running the code and obtain
    an API key. The program installs the `openai` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we set an environment variable for the API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Pinecone
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *Pinecone* section is only present in `Pipeline_2_The_Vector_Store_Administrator.ipynb`
    and `Pipeline_3_The_Video_Expert.ipynb` when the Pinecone vector store is required.
    The following command installs Pinecone, and then Pinecone is imported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The program then retrieves the key from a file (or you can enter it manually):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In production, you can set an environment variable or implement the method that
    best fits your project so that the API key is never visible.
  prefs: []
  type: TYPE_NORMAL
- en: The *Evaluator* section of `Pipeline_3_The_Video_Expert.ipynb` contains its
    own requirements and installations.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have defined the environment for all four notebooks, which contain
    the same sub-sections we just described in their respective *Installing the environment*
    sections. We can now fully focus on the processes involved in the video production
    programs. We will begin with the Generator and Commentator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 1: Generator and Commentator'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A revolution is on its way in computer vision with automated video generation
    and analysis. We will introduce the Generator AI agent with Sora in *The AI-generated
    video dataset* section. We will explore how OpenAI Sora was used to generate the
    videos for this chapter with a text-to-video diffusion transformer. The technology
    itself is something we have expected and experienced to some extent in professional
    film-making environments. However, the novelty relies on the fact that the software
    has become mainstream in a few clicks, with inVideo, for example!
  prefs: []
  type: TYPE_NORMAL
- en: In the *The Generator and the Commentator* section, we will extend the scope
    of the Generator to collecting and processing the AI-generated videos. The Generator
    splits the videos into frames and works with the Commentator, an OpenAI LLM, to
    produce comments on samples of video frames.
  prefs: []
  type: TYPE_NORMAL
- en: The Generator’s task begins by producing the AI-generated video dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The AI-generated video dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first AI agent in this project is a text-to-video diffusion transformer
    model that generates a video dataset we will implement. The videos for this chapter
    were specifically generated by Sora, a text-to-video AI model released by OpenAI
    in February 2024\. You can access Sora to view public AI-generated videos and
    create your own at [https://ai.invideo.io/](https://ai.invideo.io/). AI-generated
    videos also allow for free videos with flexible copyright terms that you can check
    out at [https://invideo.io/terms-and-conditions/](https://invideo.io/terms-and-conditions/).
  prefs: []
  type: TYPE_NORMAL
- en: Once you have gone through this chapter, you can also create your own video
    dataset with any source of videos, such as smartphones, video stocks, and social
    media.
  prefs: []
  type: TYPE_NORMAL
- en: AI-generated videos enhance the speed of creating video datasets. Teams do not
    have to spend time finding videos that fit their needs. They can obtain a video
    quickly with a prompt that can be an idea expressed in a few words. AI-generated
    videos represent a huge leap into the future of AI applications. Sora’s potential
    applies to many industries, including filmmaking, education, and marketing. Its
    ability to generate nuanced video content from simple text prompts opens new avenues
    for creative and educational outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Although AI-generated videos (and, in particular, diffusion transformers) have
    changed the way we create world simulations, this represents a risk for jobs in
    many areas, such as filmmaking. The risk of deep fakes and misinformation is real.
    At a personal level, we must take ethical considerations into account when we
    implement Generative AI in a project, thus producing constructive, ethical, and
    realistic content.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how a diffusion transformer can produce realistic content.
  prefs: []
  type: TYPE_NORMAL
- en: How does a diffusion transformer work?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At the core of Sora, as described by Liu et al., 2024 (see the *References*
    section), is a diffusion transformer model that operates between an encoder and
    a decoder. It uses user text input to guide the content generation, associating
    it with patches from the encoder. The model iteratively refines these noisy latent
    representations, enhancing their clarity and coherence. Finally, the refined data
    is passed to the decoder to reconstruct high-fidelity video frames. The technology
    involved includes vision transformers such as CLIP and LLMs such as GPT-4, as
    well as other components OpenAI continually includes in its vision model releases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder and decoder are integral components of the overall diffusion model,
    as illustrated in *Figure 10.3*. They both play a critical role in the workflow
    of the transformer diffusion model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: The encoder’s primary function is to compress input data, such
    as images or videos, into a lower-dimensional latent space. The encoder thus transforms
    high-dimensional visual data into a compact representation while preserving crucial
    information. A lower-dimensional latent space obtained is a compressed representation
    of high-dimensional data, retaining essential features while reducing complexity.
    For example, a high-resolution image (1024x1024 pixels, 3 color channels) can
    be compressed by an encoder into a vector of 1000 values, capturing key details
    like shape and texture. This makes processing and manipulating images more efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: The decoder reconstructs the original data from the latent representation
    produced by the encoder. It performs the encoder’s reverse operation, transforming
    the low-dimensional latent space back into high-dimensional pixel space, thus
    generating the final output, such as images or videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![A diagram of a workflow  Description automatically generated](img/B31169_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: The encoding and decoding workflow of video diffusion models'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of a diffusion transformer model goes through five main steps,
    as you can observe in the previous figure:'
  prefs: []
  type: TYPE_NORMAL
- en: The visual encoder transforms datasets of images into a lower-dimensional latent
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The visual encoder splits the lower-dimensional latent space into patches that
    are like words in a sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The diffusion transformer associates user text input with its dictionary of
    patches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The diffusion transformer iteratively refines noisy image representations generated
    to produce coherent frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The visual decoder reconstructs the refined latent representations into high-fidelity
    video frames that align with the user’s instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The video frames can then be played in a sequence. Every second of a video contains
    a set of frames. We will be deconstructing the AI-generated videos into frames
    and commenting on these frames later. But for now, we will analyze the video dataset
    produced by the diffusion transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the diffusion transformer model video dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Open the `Videos_dataset_visualization.ipynb` notebook on GitHub. Hopefully,
    you have installed the environmentas described earlier in this chapter. We will
    move on to writing the download and display functions we need.
  prefs: []
  type: TYPE_NORMAL
- en: Video download and display functions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The three main functions each use `filename` (the name of the video file) as
    an argument. The three main functions download and display videos, and display
    frames in the videos.
  prefs: []
  type: TYPE_NORMAL
- en: '`download_video` downloads one video at a time from the GitHub dataset, calling
    the `download` function defined in the *GitHub* subsection of *The environment*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`display_video(file_name)` displays the video file downloaded by first encoding
    in `base64`, a binary-to-text encoding scheme that represents binary data in ASCII
    string format. Then, the encoded video is displayed in HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`display_video_frame` takes `file_name`, `frame_number`, and `size` (the image
    size to display) as arguments to display a frame in the video. The function first
    opens the video file and then extracts the frame number set by `frame_number`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The function converts the file from the BGR (blue, green, and red) to the RGB
    (red, green, and blue) channel, converts it to PIL, an image array (such as one
    handled by OpenCV), and resizes it with the `size` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the function encodes the image in string format with `base64` and
    displays it in HTML:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Once the environment is installed and the video processing functions are ready,
    we will display the introduction video.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction video (with audio)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The following cells download and display the introduction video using the functions
    we created in the previous section. A video file is selected and downloaded with
    the `download_video` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms the selection and download status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We can choose to display only a single frame of the video as a thumbnail with
    the `display_video_frame` function by providing the file name, the frame number,
    and the image size to display. The program will first compute `frame_count` (the
    number of frames in the video), `frame_rate` (the number of frames per second),
    and `video_duration` (the duration of the video). Then, it will make sure `frame_number`
    (the frame we want to display) doesn’t exceed `frame_count`. Finally, it displays
    the frame as a thumbnail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, `frame_number` is set to `5`, but you can choose another value. The output
    shows the information on the video and the thumbnail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also display the full video if needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The video will be displayed and can be played with the audio track:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person sitting in a chair  Description automatically generated](img/B31169_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: AI-generated video'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s describe and display AI-generated videos in the `/videos` directory of
    this chapter’s GitHub directory. You can host this dataset in another location
    and scale it to the volume that meets your project’s specifications. The educational
    video dataset of this chapter is listed in `lfiles`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can now move on and display any video we wish.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying thumbnails and videos in the AI-generated dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section is a generalization of the *Introduction video (with audio)* section.
    This time, instead of downloading one video, it downloads all the videos and displays
    the thumbnails of all the videos. You can then select a video in the list and
    display it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first collects the video dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the file names of the downloaded videos:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The program calculates the number of videos in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The program goes through the list and displays the information for each video
    and displays its thumbnail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The information on the video and its thumbnail is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You can select a video in the list and display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can click on the video and watch it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person in a football uniform pointing at a football  Description automatically
    generated](img/B31169_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Video of a football player'
  prefs: []
  type: TYPE_NORMAL
- en: We have explored how the AI-generated videos were produced and visualized the
    dataset. We are now ready to build the Generator and the Commentator.
  prefs: []
  type: TYPE_NORMAL
- en: The Generator and the Commentator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset of AI-generated videos is ready. We will now build the Generator
    and the Commentator, which processes one video at a time, making scaling seamless.
    An indefinite number of videos can be processed one at a time, requiring only
    a CPU and limited disk space. The Generator and the Commentator work together,
    as shown in *Figure 10.8*. These AI agents will produce raw videos from text and
    then split them into frames that they will comment on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a sports game  Description automatically generated with medium
    confidence](img/B31169_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: The Generator and the Commentator work together to comment on
    video frames'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Generator and the Commentator produce the commented frames required in
    four main steps that we will build in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Generator** generates the text-to-video inVideo video dataset based on
    the video production team’s text input. In this chapter, it is a dataset of sports
    videos.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Generator** runs a scaled process by selecting one video at a time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Generator** splits the video into frames (images)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The **Commentator** samples frames (images) and comments on them with an OpenAI
    LLM model. Each commented frame is saved with:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unique ID
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comment
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Frame
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Video file name
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now build the Generator and the Commentator in Python, starting with
    the AI-generated videos. Open `Pipeline_1_The_Generator_and_the_Commentator.ipynb`
    in the chapter’s GitHub directory. See the *The environment* section of this chapter
    for a description of the *Installing the environment* section of this notebook.
    The process of going from a video to comments on a sample of frames only takes
    three straightforward steps in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: Displaying the video
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting the video into frames
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Commenting on the frames
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will define functions for each step and call them in the `Pipeline-1 Controller`
    section of the program. The first step is to define a function to display a video.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1\. Displaying the video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `download` function is in the *GitHub* subsection of the *Installing the
    environment* section of this notebook. It will be called by the *Vector Store
    Administrator-Pipeline 1* in the *Administrator-Pipeline 1* section of this notebook
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '`display_video(file_name)` is the same as defined in the previous section,
    *The AI-generated video dataset*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The downloaded video will now be split into frames.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2\. Splitting video into frames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `split_file(file_name)` function extracts frames from a video, as in the
    previous section, *The AI-generated video dataset*. However, in this case, we
    will expand the function to save frames as JPEG files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We have split the video into frames and saved them as JPEG images with their
    respective frame number, `frame_number`. The Generator’s job finishes here and
    the Commentator now takes over.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3\. Commenting on the frames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Generator has gone from text-to-video to splitting the video and saving
    the frames as JPEG frames. The Commentator now takes over to comment on the frames
    with three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`generate_openai_comments(filename)` asks the GPT-4 series vision model to
    analyze a frame and produce a response that contains a comment describing the
    frame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate_comment(response_data)` extracts the comment from the response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_comment(comment, frame_number, file_name)` saves the comment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We need to build the Commentator’s extraction function first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We then write a function to save the extracted comment in a CSV file that bears
    the same name as the video file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal is to save the comment in a format that can directly be upserted to
    Pinecone:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ID`: A unique string ID generated with `str(uuid.uuid4())`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FrameNumber`: The frame number of the commented JPEG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Comment`: The comment generated by the OpenAI vision model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FileName`: The name of the video file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Commentator’s main function is to generate comments with the OpenAI vision
    model. However, in this program’s scenario, we will not save all the frames but
    a sample of the frames. The program first determines the number of frames to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, a sample frequency is set that can be modified along with a counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The Commentator will then go through the sampled frames and request a comment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The message is very concise: `"What is happening in this image?"` The message
    also includes the image of the frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Once a response is returned, the `generate_comment` and `save_comment` functions
    are called to extract and save the comment, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The final function we require of the Commentator is to display the comments
    by loading the CSV file produced in a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The function returns the DataFrame with the comments. An administrator controls
    *Pipeline 1*, the Generator, and the Commentator.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline 1 controller
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The controller runs jobs for the preceding three steps of the Generator and
    the Commentator. It begins with *Step 1*, which includes selecting a video, downloading
    it, and displaying it. In an automated pipeline, these functions can be separated.
    For example, a script would iterate through a list of videos, automatically select
    each one, and encapsulate the controller functions. In this case, in a pre-production
    and educational context, we will collect, download, and display the videos one
    by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The controller then splits the video into frames and comments on the frames
    of the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The controller activates the Generator to produce comments on frames of the
    video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The response time is measured as well. The controller then adds additional
    outputs to display the number of frames, the comments, the content generation
    time, and the total controller processing times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The controller has completed its task of producing content. However, depending
    on your project, you can introduce dynamic RAG for some or all the videos. If
    you need this functionality, you can apply the process described in *Chapter 5*,
    *Boosting RAG Performance with Expert Human Feedback*, to the Commentator’s outputs,
    including the cosine similarity quality control metrics, as we will in the *Pipeline
    3: The Video Expert* section of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: The controller can also save the comments and frames.
  prefs: []
  type: TYPE_NORMAL
- en: Saving comments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To save the comments, set `save=True`. To save the frames, set `save_frames=True`.
    Set both values to `False` if you just want to run the program and view the outputs,
    but, in our case, we will set them as `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The comment is saved in CSV format in `cpath` and contains the file name with
    the `.csv` extension and in the location of your choice. In this case, the files
    are saved on Google Drive (make sure the path exists):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that a file is saved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The frames are saved in a root name direction, for which we remove the extension
    with `root_name = root_name + extension.strip(''.'')`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The output is a directory with all the frames generated in it. We should delete
    the files if the controller runs in a loop over all the videos in a single session.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting files
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To delete the files, just set `delf=True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: You can now process an unlimited number of videos one by one and scale to whatever
    size you wish, as long as you have disk space and a CPU!
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 2: The Vector Store Administrator'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Vector Store Administrator AI agent performs the tasks we implemented in
    *Chapter 6*, *Scaling RAG Bank Customer Data with Pinecone.* The novelty in this
    section relies on the fact that all the data we upsert for RAG is AI-generated.
    Let’s open `Pipeline_2_The_Vector_Store_Administrator.ipynb` in the GitHub repository.
    We will build the Vector Store Administrator on top of the Generator and the Commentator
    AI agents in four steps, as illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a video expert  Description automatically generated](img/B31169_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Workflow of the Vector Store Administrator from processing to
    querying video frame comments'
  prefs: []
  type: TYPE_NORMAL
- en: '**Processing the video comments**: The Vector Store Administrator will load
    and prepare the comments for chunking as in the *Pipeline 2: Scaling a Pinecone
    Index (vector store)* section of *Chapter 6*. Since we are processing one video
    at a time in a pipeline, the system deletes the files processed, which keeps disk
    space constant. You can enhance the functionality and scale this process indefinitely.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunking and embedding the dataset**: The column names `(''ID'', ''FrameNumber'',
    ''Comment'', ''FileName'')` of the dataset have already been prepared by the Commentator
    AI agent in *Pipeline 1*. The program chunks and embeds the dataset using the
    same functionality as in *Chapter 6* in the *Chunking and embedding the dataset*
    section.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**The Pinecone index**: The Pinecone Index is created, and the data is upserted
    as in the *Creating the Pinecone Index* and *Upserting* sections of *Chapter*
    6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Querying the vector store after upserting the dataset**: This follows the
    same process as in *Chapter 6*. However, in this case, the retrieval is hybrid,
    using both the Pinecone vector store and a separate file system to store videos
    and video frames.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go through *Steps 1* to *3* in the notebook to examine the Vector Store Administrator’s
    functions. After *Step 3*, the Pinecone index is ready for hybrid querying.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the Pinecone index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the notebook on GitHub, *Step 4: Querying the Pinecone index* implements
    functions to find a comment that matches user input and trace it to the frame
    of a video. This leads to the video source and frame, which can be displayed.
    We can display the videos and frames from the location we wish. This hybrid approach
    thus involves querying the Pinecone Index to retrieve information and also retrieve
    media files from another location.'
  prefs: []
  type: TYPE_NORMAL
- en: We saw that a vector store can contain images that are queried, as implemented
    in *Chapter 4*, *Multimodal Modular RAG for Drone Technology*. In this chapter,
    the video production use case videos and frame files are stored separately. In
    this case, it is in the GitHub repository. In production, the video and frame
    files can be retrieved from any storage system we need, which may or may not prove
    to be more cost-effective than storing data on Pinecone. The decision to store
    images in a vector store or a separate location will depend on the project’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by defining the number of top-k results we wish to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We then design a rather difficult prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Only a handful of frames in the whole video dataset contain an image of a basketball
    player jumping to score a slam dunk. Can our system find it? Let’s find out.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first embed our query to match the format of the data in the vector store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we run a similarity vector search between the query and the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we display the content of the response and the response time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains the ID of the comment retrieved and its score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output also contains the comment generated by the OpenAI LLM (the Commentator
    agent):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The final output contains the frame number that was commented, the video file
    of the frame, and the retrieval time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display the video by downloading it based on the file name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use a standard Python function to display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The video containing a basketball player performing a dunk is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A basketball hoop with net  Description automatically generated](img/B31169_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Video output'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can take this further with more precision by displaying the frame of the
    comment retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the exact frame that corresponds to the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person holding a ball  Description automatically generated](img/B31169_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Video frame corresponding to our input'
  prefs: []
  type: TYPE_NORMAL
- en: Only the frames of `basketball3.mp4` were saved in the GitHub repository for
    disk space reasons for this program. In production, all the frames you decide
    you need can be stored and retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'The team of AI agents in this chapter worked together to generate videos (the
    Generator), comment on the video frames (the Commentator), upsert embedded comments
    in the vector store (the Vector Store Administrator), and prepare the retrieval
    process (the Vector Store Administrator). We also saw that the retrieval process
    already contained augmented input and output thanks to the OpenAI LLM (the Commentator)
    that generated natural language comments. The process that led to this point will
    definitely be applied in many domains: firefighting, medical imagery, marketing,
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: What more can we expect from this system? The Video Expert AI agent will answer
    that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 3: The Video Expert'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The role of the OpenAI GPT-4o Video Expert is to analyze the comment made by
    the Commentator OpenAI LLM agent, point out the cognitive dissonances (things
    that don’t seem to fit together in the description), rewrite the comment, and
    provide a label. The workflow of the Video Expert, as illustrated in the following
    figure, also includes the code of the *Metrics calculations and display* section
    of *Chapter 7*, *Building Scalable Knowledge-Graph-Based RAG with Wikipedia API
    and LlamaIndex*.
  prefs: []
  type: TYPE_NORMAL
- en: The Commentator’s role was only to describe what it saw. The Video Expert is
    there to make sure it makes sense and also label the videos so they can be classified
    in the dataset for further use.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a video expert  Description automatically generated](img/B31169_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Workflow of the Video Expert for automated dynamics descriptions
    and labeling'
  prefs: []
  type: TYPE_NORMAL
- en: '**The Pinecone index** will connect to the Pinecone index as described in the
    *Pipeline 2\. The Vector Store Administrator* section of this chapter. This time,
    we will not upsert data but connect to the vector store.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define the RAG functions** utilizing the straightforward functions we built
    in *Pipeline 1* and *Pipeline 2* of this chapter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Querying the vector store** is nothing but querying the Pinecone Index as
    described in *Pipeline 2* of this chapter.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrieval augmented generation** finally determines the main role of Video
    Expert GPT-4o, which is to analyze and improve the vector store query responses.
    This final step will include evaluation and metric functions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are as many strategies as projects to implement the video production
    use case we explored in this chapter, but the Video Expert plays an important
    role. Open `Pipeline_3_The_Video_Expert.ipynb` on GitHub and go to the *Augmented
    Retrieval Generation* section in *Step 2: Defining the RAG functions*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function makes an OpenAI GPT-4o call, like for the Commentator in *Pipeline
    1*. However, this time, the role of the LLM is quite different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The instructions for GPT-4o are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`You will be provided with comments of an image frame taken from a video`:
    This instructs the LLM to analyze the AI-generated comments. The Commentator had
    to remain neutral and describe the frame as it saw it. The role of the Video Expert
    agent is different: it has to analyze and enhance the comment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`1\. Point out the cognitive dissonances`: This instructs the model to find
    contradictions or discrepancies in the comment that can come from the way the
    AI-generated video was produced as well (lack of logic in the video).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`2\. Rewrite the comment in a logical engaging style`: This instructs the Video
    Expert agent to rewrite the comment going from a technical comment to a description.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3\. Provide a label for this image such as Label: basketball, football, soccer
    or other label`: This instructs the model to provide a label for further use.
    On GitHub, *Step 3: Querying the Vector Store* reproduces the query and output
    described in *Pipeline 2* for a basketball player scoring with a dunk, with the
    corresponding video and frame. The output is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The comment provided seems acceptable. However, let’s see what GPT-4o thinks
    of it. The *Step 4: Retrieval Augmented Generation* section on GitHub takes the
    output and submits it as the user prompt to the Video Expert agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We then call the Video Expert agent to obtain its expertise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides the Video Expert’s insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The response is well-structured and acceptable. The output may vary from one
    run to another due to the stochastic “creative” nature of Generative AI agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Evaluator* section that follows *Step 4* runs ten examples using the same
    process as the basketball request we just made. Each example thus contains:'
  prefs: []
  type: TYPE_NORMAL
- en: A user prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The comment returned by the vector store query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The enhanced comment made by the GPT-4o model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each example also contains the same evaluation process as in *Chapter 7, Building
    Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex,* in the
    *Examples for metrics* section. However, in this case, the human evaluator suggests
    content instead of a score (0 to 1). The human content becomes the ground truth,
    the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: Before beginning the evaluation, the program creates scores to keep track of
    the original response made by the query.
  prefs: []
  type: TYPE_NORMAL
- en: 'The human evaluator rewrites the output provided by the Video Expert:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The content rewritten by the Video Expert is extracted from the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The human comment (ground truth, the reference output) and the LLM comments
    are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the cosine similarity score between the human and LLM comments is calculated
    and appended to `scores`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The original score provided with the query is appended to the query’s retrieval
    score, `rscores`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the human feedback, the comment rewritten by GPT-4o (the
    Video Expert), and the similarity score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: This program contains ten examples, but we can enter a corpus of as many examples
    as we wish to evaluate the system. The evaluation of each example applies the
    same choice of metrics as in *Chapter 7**.* After the examples have been evaluated,
    the *Metrics calculations and display* section in the program also runs the metric
    calculations defined in the section of the same name in *Chapter 7*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use all the metrics to analyze the performance of the system. The time
    measurements throughout the program also provide insights. The first metric, accuracy,
    is a good metric to start with. In this case, it shows that there is room for
    progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Some requests and responses were challenging and required further work to improve
    the system:'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the quality of the videos and their content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checking the comments and possibly modifying them with human feedback, as we
    did in *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-tuning a model with images and text as we did in *Chapter 9*, *Empowering
    AI Models: Fine-Tuning RAG Data and Human Feedback*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing any other constructive idea that the video production team comes up
    with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that RAG-driven Generative AI systems in production are very effective.
    However, the road from design to production requires hard human effort! Though
    AI technology has made tremendous progress, it still requires humans to design,
    develop, and implement it in production.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the hybrid era of human and AI agents, focusing
    on the creation of a streamlined process for generating, commenting, and labeling
    videos. By integrating cutting-edge Generative AI models, we demonstrated how
    to build an automated pipeline that transforms raw video inputs into structured,
    informative, and accessible video content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our journey began with the **Generator** agent in *Pipeline 1*: *The Generator
    and the Commentator*, which was tasked with creating video content from textual
    ideas. We can see that video generation processes will continue to expand through
    seamless integration ideation and descriptive augmentation generative agents.
    In *Pipeline 2*: *The Vector Store Administrator*, we focused on organizing and
    embedding the generated comments and metadata into a searchable vector store.
    In this pipeline, we highlighted the optimization process of building a scalable
    video content library with minimal machine resources using only a CPU and no GPU.
    Finally, in *Pipeline 3*: *The Video Expert*, we introduced the Expert AI agent,
    a video specialist designed to enhance and label the video content based on user
    inputs. We also implemented evaluation methods and metric calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we had constructed a comprehensive, automated RAG-driven
    Generative AI system capable of generating, commenting on, and labeling videos
    with minimal human intervention. This journey demonstrated the power and potential
    of combining multiple AI agents and models to create an efficient pipeline for
    video content creation.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques and tools we explored can revolutionize various industries by
    automating repetitive tasks, enhancing content quality, and making information
    retrieval more efficient. This chapter not only provided a detailed technical
    roadmap but also underscored the transformative impact of AI in modern content
    creation and management. You are now all set to implement RAG-driven Generative
    AI in real-life projects.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with yes or no:'
  prefs: []
  type: TYPE_NORMAL
- en: Can AI now automatically comment and label videos?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does video processing involve splitting the video into frames?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can the programs in this chapter create a 200-minute movie?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the programs in this chapter require a GPU?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are the embedded vectors of the video content stored on disk?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the scripts involve querying a database for retrieving data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there functionality for displaying images in the scripts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it useful to have functions that specifically check file existence and size
    in any of the scripts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is there a focus on multimodal data in these scripts?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do any of the scripts mention applications of AI in real-world scenarios?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sora video generation model information and access:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sora** | **OpenAI**: [https://ai.invideo.io/](https://ai.invideo.io/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://openai.com/index/video-generation-models-as-world-simulators/](https://openai.com/index/video-generation-models-as-world-simulators/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Sora: A Review on Background, Technology, Limitations, and Opportunities of
    Large Vision Models* by Yixin Liu, Kai Zhang, Yuan Li, et al.: [https://arxiv.org/pdf/2402.17177](https://arxiv.org/pdf/2402.17177)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI, ChatGPT: [https://openai.com/chatgpt/](https://openai.com/chatgpt/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI, Research: [https://openai.com/research/](https://openai.com/research/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinecone: [https://docs.pinecone.io/home](https://docs.pinecone.io/home)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
