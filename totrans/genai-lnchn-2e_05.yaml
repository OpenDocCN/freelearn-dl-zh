- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Intelligent RAG Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we’ve talked about LLMs and tokens and working with them
    in LangChain. **Retrieval-Augmented Generation** (**RAG**) extends LLMs by dynamically
    incorporating external knowledge during generation, addressing limitations of
    fixed training data, hallucinations, and context windows. A RAG system, in simple
    terms, takes a query, converts it directly into a semantic vector embedding, runs
    a search extracting relevant documents, and passes these to a model that generates
    a context-appropriate user-facing response.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explores RAG systems and the core components of RAG, including
    vector stores, document processing, retrieval strategies, implementation, and
    evaluation techniques. After that, we’ll put into practice a lot of what we’ve
    learned so far in this book by building a chatbot. We’ll build a production-ready
    RAG pipeline that streamlines the creation and validation of corporate project
    documentation. This corporate use case demonstrates how to generate initial documentation,
    assess it for compliance and consistency, and incorporate human feedback—all in
    a modular and scalable workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter has the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: From indexes to intelligent retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components of a RAG system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From embeddings to search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking down the RAG pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a corporate documentation chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting RAG systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by introducing RAG, its importance, and the main considerations
    when using the RAG framework.
  prefs: []
  type: TYPE_NORMAL
- en: From indexes to intelligent retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Information retrieval has been a fundamental human need since the dawn of recorded
    knowledge. For the past 70 years, retrieval systems have operated under the same
    core paradigm:'
  prefs: []
  type: TYPE_NORMAL
- en: First, a user frames an information need as a query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They then submit this query to the retrieval system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, the system returns references to documents that may satisfy the information
    need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References may be rank-ordered by decreasing relevance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Results may contain relevant excerpts from each document (known as snippets)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While this paradigm has remained constant, the implementation and user experience
    have undergone remarkable transformations. Early information retrieval systems
    relied on manual indexing and basic keyword matching. The advent of computerized
    indexing in the 1960s introduced the inverted index—a data structure that maps
    each word to a list of documents containing it. This lexical approach powered
    the first generation of search engines like AltaVista (1996), where results were
    primarily based on exact keyword matches.
  prefs: []
  type: TYPE_NORMAL
- en: The limitations of this approach quickly became apparent, however. Words can
    have multiple meanings (polysemy), different words can express the same concept
    (synonymy), and users often struggle to articulate their information needs precisely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Information-seeking activities come with non-monetary costs: time investment,
    cognitive load, and interactivity costs—what researchers call “Delphic costs.”
    User satisfaction with search engines correlates not just with the relevance of
    results, but with how easily users can extract the information they need.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional retrieval systems aimed to reduce these costs through various optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Synonym expansion to lower cognitive load when framing queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result ranking to reduce the time cost of scanning through results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result snippeting (showing brief, relevant excerpts from search results) to
    lower the cost of evaluating document relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These improvements reflected an understanding that the ultimate goal of search
    is not just finding documents but satisfying information needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google’s PageRank algorithm (late 1990s) improved results by considering link
    structures, but even modern search engines faced fundamental limitations in understanding
    meaning. The search experience evolved from simple lists of matching documents
    to richer presentations with contextual snippets (beginning with Yahoo’s highlighted
    terms in the late 1990s and evolving to Google’s dynamic document previews that
    extract the most relevant sentences containing search terms), but the underlying
    challenge remained: bridging the semantic gap between query terms and relevant
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: A fundamental limitation of traditional retrieval systems lies in their lexical
    approach to document retrieval. In the Uniterm model, query terms were mapped
    to documents through inverted indices, where each word in the vocabulary points
    to a “postings list” of document positions. This approach efficiently supported
    complex boolean queries but fundamentally missed semantic relationships between
    terms. For example, “turtle” and “tortoise” are treated as completely separate
    words in an inverted index, despite being semantically related. Early retrieval
    systems attempted to bridge this gap through pre-retrieval stages that augmented
    queries with synonyms, but the underlying limitation remained.
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough came with advances in neural network models that could capture
    the meaning of words and documents as dense vector representations—known as embeddings.
    Unlike traditional keyword systems, embeddings create a *semantic map* where related
    concepts cluster together—”turtle,” “tortoise,” and “reptile” would appear as
    neighbors in this space, while “bank” (financial) would cluster with “money” but
    far from “river.” This geometric organization of meaning enabled retrieval based
    on conceptual similarity rather than exact word matching.
  prefs: []
  type: TYPE_NORMAL
- en: This transformation gained momentum with models like Word2Vec (2013) and later
    transformer-based models such as BERT (2018), which introduced contextual understanding.
    BERT’s innovation was to recognize that the same word could have different meanings
    depending on its context—”bank” as a financial institution versus “bank” of a
    river. These distributed representations fundamentally changed what was possible
    in information retrieval, enabling the development of systems that could understand
    the intent behind queries rather than just matching keywords.
  prefs: []
  type: TYPE_NORMAL
- en: 'As transformer-based language models grew in scale, researchers discovered
    they not only learned linguistic patterns but also memorized factual knowledge
    from their training data. Studies by Google researchers showed that models like
    T5 could answer factual questions without external retrieval, functioning as implicit
    knowledge bases. This suggested a paradigm shift—from retrieving documents containing
    answers to directly generating answers from internalized knowledge. However, these
    “closed-book” generative systems faced limitations: hallucination risks, knowledge
    cutoffs limited to training data, inability to cite sources, and challenges with
    complex reasoning. The solution emerged in **RAG**, which bridges traditional
    retrieval systems with generative language models, combining their respective
    strengths while addressing their individual weaknesses.'
  prefs: []
  type: TYPE_NORMAL
- en: Components of a RAG system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RAG enables language models to ground their outputs in external knowledge,
    providing an elegant solution to the limitations that plague pure LLMs: hallucinations,
    outdated information, and restricted context windows. By retrieving only relevant
    information on demand, RAG systems effectively bypass the context window constraints
    of language models, allowing them to leverage vast knowledge bases without squeezing
    everything into the model’s fixed attention span.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than simply retrieving documents for human review (as traditional search
    engines do) or generating answers solely from internalized knowledge (as pure
    LLMs do), RAG systems retrieve information to inform and ground AI-generated responses.
    This approach combines the verifiability of retrieval with the fluency and comprehension
    of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, RAG consists of these main components working in concert:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge base**: The storage layer for external information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retriever**: The knowledge access layer that finds relevant information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmenter**: The integration layer that prepares retrieved content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generator**: The response layer that produces the final output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From a process perspective, RAG operates through two interconnected pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: An indexing pipeline that processes, chunks, and stores documents in the knowledge
    base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A query pipeline that retrieves relevant information and generates responses
    using that information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The workflow in a RAG system follows a clear sequence: when a query arrives,
    it’s processed for retrieval; the retriever then searches the knowledge base for
    relevant information; this retrieved context is combined with the original query
    through augmentation; finally, the language model generates a response grounded
    in both the query and the retrieved information. We can see this in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: RAG architecture and workflow](img/B32363_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: RAG architecture and workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'This architecture offers several advantages for production systems: modularity
    allows components to be developed independently; scalability enables resources
    to be allocated based on specific needs; maintainability is improved through the
    clear separation of concerns; and flexibility permits different implementation
    strategies to be swapped in as requirements evolve.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we’ll explore each component in *Figure 4.1* in
    detail, beginning with the fundamental building blocks of modern RAG systems:
    **embeddings** and **vector stores** that power the knowledge base and retriever
    components. But before we dive in, it’s important to first consider the decision
    between implementing RAG or using pure LLMs. This choice will fundamentally impact
    your application’s overall architecture and operational characteristics. Let’s
    discuss the trade-offs!'
  prefs: []
  type: TYPE_NORMAL
- en: When to implement RAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introducing RAG brings architectural complexity that must be carefully weighed
    against your application requirements. RAG proves particularly valuable in specialized
    domains where current or verifiable information is crucial. Healthcare applications
    must process both medical images and time-series data, while financial systems
    need to handle high-dimensional market data alongside historical analysis. Legal
    applications benefit from RAG’s ability to process complex document structures
    and maintain source attribution. These domain-specific requirements often justify
    the additional complexity of implementing RAG.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of RAG, however, come with significant implementation considerations.
    The system requires efficient indexing and retrieval mechanisms to maintain reasonable
    response times. Knowledge bases need regular updates and maintenance to remain
    valuable. Infrastructure must be designed to handle errors and edge cases gracefully,
    especially where different components interact. Development teams must be prepared
    to manage these ongoing operational requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Pure LLM implementations, on the other hand, might be more appropriate when
    these complexities outweigh the benefits. Applications focusing on creative tasks,
    general conversation, or scenarios requiring rapid response times often perform
    well without the overhead of retrieval systems. When working with static, limited
    knowledge bases, techniques like fine-tuning or prompt engineering might provide
    simpler solutions.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis, drawn from both research and practical implementations, suggests
    that specific requirements for knowledge currency, accuracy, and domain expertise
    should guide the choice between RAG and pure LLMs, balanced against the organizational
    capacity to manage the additional architectural complexity.
  prefs: []
  type: TYPE_NORMAL
- en: At Chelsea AI Ventures, our team has observed that clients in regulated industries
    particularly benefit from RAG’s verifiability, while creative applications often
    perform adequately with pure LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Development teams should consider RAG when their applications require:'
  prefs: []
  type: TYPE_NORMAL
- en: Access to current information not available in LLM training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-specific knowledge integration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifiable responses with source attribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing of specialized data formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High precision in regulated industries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, let’s explore the implementation details, optimization strategies,
    and production deployment considerations for each RAG component.
  prefs: []
  type: TYPE_NORMAL
- en: From embeddings to search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned, a RAG system comprises a retriever that finds relevant information,
    an augmentation mechanism that integrates this information, and a generator that
    produces the final output. When building AI applications with LLMs, we often focus
    on the exciting parts – prompts, chains, and model outputs. However, the foundation
    of any robust RAG system lies in how we store and retrieve our vector embeddings.
    Think of it like building a library – before we can efficiently find books (vector
    search), we need both a building to store them (vector storage) and an organization
    system to find them (vector indexing). In this section, we introduce the core
    components of a RAG system: vector embeddings, vector stores, and indexing strategies
    to optimize retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make RAG work, we first need to solve a fundamental challenge: how do we
    help computers understand the meaning of text so they can find relevant information?
    This is where embeddings come in.'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embeddings are numerical representations of text that capture semantic meaning.
    When we create an embedding, we’re converting words or chunks of text into vectors
    (lists of numbers) that computers can process. These vectors can be either sparse
    (mostly zeros with few non-zero values) or dense (most values are non-zero), with
    modern LLM systems typically using dense embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: What makes embeddings powerful is that texts with similar meanings have similar
    numerical representations, enabling semantic search through nearest neighbor algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the embedding model transforms text into numerical vectors.
    The same model is used for both documents as well as queries to ensure consistency
    in the vector space. Here’s how you’d use embeddings in LangChain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Once we have these OpenAI embeddings (the 1536-dimensional vectors we generated
    for our example sentences above), we need a purpose-built system to store them.
    Unlike regular database values, these high-dimensional vectors require specialized
    storage solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Embeddings` class in LangChain provides a standard interface for all embedding
    models from various providers (OpenAI, Cohere, Hugging Face, and others). It exposes
    two primary methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`embed_documents`: Takes multiple texts and returns embeddings for each'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embed_query`: Takes a single text (your search query) and returns its embedding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some providers use different embedding methods for documents versus queries,
    which is why these are separate methods in the API.
  prefs: []
  type: TYPE_NORMAL
- en: This brings us to vector stores – specialized databases optimized for similarity
    searches in high-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Vector stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Vector stores are specialized databases designed to store, manage, and efficiently
    search vector embeddings. As we’ve seen, embeddings convert text (or other data)
    into numerical vectors that capture semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector stores solve the fundamental challenge of how to persistently and efficiently
    search through these high-dimensional vectors. Please note that the vector database
    operates as an independent system that can be:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaled independently of the RAG components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintained and optimized separately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potentially shared across multiple RAG applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosted as a dedicated service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When working with embeddings, several challenges arise:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scale**: Applications often need to store millions of embeddings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dimensionality**: Each embedding might have hundreds or thousands of dimensions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Search performance**: Finding similar vectors quickly becomes computationally
    intensive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Associated data**: We need to maintain connections between vectors and their
    source documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider a real-world example of what we need to store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'At their core, vector stores combine two essential components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector storage**: The actual database that persists vectors and metadata'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector index**: A specialized data structure that enables efficient similarity
    search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficiency challenge comes from the *curse of dimensionality* – as vector
    dimensions increase, computing similarities becomes increasingly expensive, requiring
    O(dN) operations for d dimensions and N vectors. This makes naive similarity search
    impractical for large-scale applications.
  prefs: []
  type: TYPE_NORMAL
- en: Vector stores enable similarity-based search through distance calculations in
    high-dimensional space. While traditional databases excel at exact matching, vector
    embeddings allow for semantic search and **approximate nearest neighbor** (**ANN**)
    retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: The key difference from traditional databases is how vector stores handle searches.
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional database search**:'
  prefs: []
  type: TYPE_NORMAL
- en: Uses exact matching (equality, ranges)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized for structured data (for example, “find all customers with age > 30”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually utilizes B-trees or hash-based indexes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector store search:**'
  prefs: []
  type: TYPE_NORMAL
- en: Uses similarity metrics (cosine similarity, Euclidean distance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized for high-dimensional vector spaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employs Approximate Nearest Neighbor (ANN) algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector stores comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Vector stores manage high-dimensional embeddings for retrieval. The following
    table compares popular vector stores across key attributes to help you select
    the most appropriate solution for your specific needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Database** | **Deployment options** | **License** | **Notable features**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pinecone | Cloud-only | Commercial | Auto-scaling, enterprise security, monitoring
    |'
  prefs: []
  type: TYPE_TB
- en: '| Milvus | Cloud, Self-hosted | Apache 2.0 | HNSW/IVF indexing, multi-modal
    support, CRUD operations |'
  prefs: []
  type: TYPE_TB
- en: '| Weaviate | Cloud, Self-hosted | BSD 3-Clause | Graph-like structure, multi-modal
    support |'
  prefs: []
  type: TYPE_TB
- en: '| Qdrant | Cloud, Self-hosted | Apache 2.0 | HNSW indexing, filtering optimization,
    JSON metadata |'
  prefs: []
  type: TYPE_TB
- en: '| ChromaDB | Cloud, Self-hosted | Apache 2.0 | Lightweight, easy setup |'
  prefs: []
  type: TYPE_TB
- en: '| AnalyticDB-V | Cloud-only | Commercial | OLAP integration, SQL support, enterprise
    features |'
  prefs: []
  type: TYPE_TB
- en: '| pg_vector | Cloud, Self-hosted | OSS | SQL support, PostgreSQL integration
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vertex Vector Search | Cloud-only | Commercial | Easy setup, low latency,
    high scalability |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: Vector store comparison by deployment options, licensing, and key
    features'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each vector store offers different tradeoffs in terms of deployment flexibility,
    licensing, and specialized capabilities. For production RAG systems, consider
    factors such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether you need cloud-managed or self-hosted deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for specific features like SQL integration or multi-modal support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The complexity of setup and maintenance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling requirements for your expected embedding volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For many applications starting with RAG, lightweight options like ChromaDB
    provide an excellent balance of simplicity and functionality, while enterprise
    deployments might benefit from the advanced features of Pinecone or AnalyticDB-V.
    Modern vector stores support several search patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exact search**: Returns precise nearest neighbors but becomes computationally
    prohibitive with large vector collections'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Approximate search**: Trades accuracy for speed using techniques like LSH,
    HNSW, or quantization; measured by recall (the percentage of true nearest neighbors
    retrieved)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid search**: Combines vector similarity with text-based search (like
    keyword matching or BM25) in a single query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtered vector search**: Applies traditional database filters (for example,
    metadata constraints) alongside vector similarity search'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vector stores also handle different types of embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dense vector search**: Uses continuous embeddings where most dimensions have
    non-zero values, typically from neural models (like BERT, OpenAI embeddings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse vector search**: Uses high-dimensional vectors where most values are
    zero, resembling traditional TF-IDF or BM25 representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse-dense hybrid**: Combines both approaches to leverage semantic similarity
    (dense) and keyword precision (sparse)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They also often give a choice of multiple similarity measures, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inner product**: Useful for comparing semantic directions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine similarity**: Normalizes for vector magnitude'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Euclidean distance**: Measures the L2 distance in vector space (note: with
    normalized embeddings, this becomes functionally equivalent to the dot product)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hamming distance**: For binary vector representations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When implementing vector storage for RAG applications, one of the first architectural
    decisions is whether to use local storage or a cloud-based solution. Let’s explore
    the tradeoffs and considerations for each approach.
  prefs: []
  type: TYPE_NORMAL
- en: Choose local storage when you need maximum control, have strict privacy requirements,
    or operate at a smaller scale with predictable workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose cloud storage when you need elastic scaling, prefer managed services,
    or operate distributed applications with variable workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider hybrid storage architecture when you want to balance performance and
    scalability, combining local caching with cloud-based persistence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hardware considerations for vector stores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regardless of your deployment approach, understanding the hardware requirements
    is crucial for optimal performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory requirements**: Vector databases are memory-intensive, with production
    systems often requiring 16-64GB RAM for millions of embeddings. Local deployments
    should plan for sufficient memory headroom to accommodate index growth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPU vs. GPU**: While basic vector operations work on CPUs, GPU acceleration
    significantly improves performance for large-scale similarity searches. For high-throughput
    applications, GPU support can provide 10-50x speed improvements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage speed**: SSD storage is strongly recommended over HDD for production
    vector stores, as index loading and search performance depend heavily on I/O speed.
    This is especially critical for local deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network bandwidth**: For cloud-based or distributed setups, network latency
    and bandwidth become critical factors that can impact query response times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For development and testing, most vector stores can run on standard laptops
    with 8GB+ RAM, but production deployments should consider dedicated infrastructure
    or cloud-based vector store services that handle these resource considerations
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Vector store interface in LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve explored the role of vector stores and compared some common
    options, let’s look at how LangChain simplifies working with them. LangChain provides
    a standardized interface for working with vector stores, allowing you to easily
    switch between different implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `vectorstore` base class in LangChain provides these essential operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adding documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similarity search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Deletion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Maximum marginal relevance search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It’s important to also briefly highlight applications of vector stores apart
    from RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection in large datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Personalization and recommendation systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network security monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing vectors isn’t enough, however. We need to find similar vectors quickly
    when processing queries. Without proper indexing, searching through vectors would
    be like trying to find a book in a library with no organization system – you’d
    have to check every single book.
  prefs: []
  type: TYPE_NORMAL
- en: Vector indexing strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Vector indexing is a critical component that makes vector databases practical
    for real-world applications. At its core, indexing solves a fundamental performance
    challenge: how to efficiently find similar vectors without comparing against every
    single vector in the database (brute force approach), which is computationally
    prohibitive for even medium-sized data volumes.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector indexes are specialized data structures that organize vectors in ways
    that allow the system to quickly identify which sections of the vector space are
    most likely to contain similar vectors. Instead of checking every vector, the
    system can focus on promising regions first.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common indexing approaches include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tree-based structures** that hierarchically divide the vector space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph-based methods** like **Hierarchical Navigable Small World** (**HNSW**)
    that create navigable networks of connected vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hashing techniques** that map similar vectors to the same “buckets”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Each of the preceding approaches offers different trade-offs between:'
  prefs: []
  type: TYPE_NORMAL
- en: Search speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy of results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update efficiency (how quickly new vectors can be added)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using a vector store in LangChain, the indexing strategy is typically handled
    by the underlying implementation. For example, when you create a FAISS index or
    use Pinecone, those systems automatically apply appropriate indexing strategies
    based on your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway is that proper indexing transforms vector search from an O(n)
    operation (where n is the number of vectors) to something much more efficient
    (often closer to O(log n)), making it possible to search through millions of vectors
    in milliseconds rather than seconds or minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a table to provide an overview of different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Strategy** | **Core algorithm** | **Complexity** | **Memory usage** | **Best
    for** | **Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| Exact Search (Brute Force) | Compares query vector with every vector in database
    | Search: O(DN)Build: O(1) | Low – only stores raw vectors |'
  prefs: []
  type: TYPE_TB
- en: Small datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When 100% recall needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing/baseline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Easiest to implement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good baseline for testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| HNSW (Hierarchical Navigable Small World) | Creates layered graph with decreasing
    connectivity from bottom to top | Search: O(log N)Build: O(N log N) | High – stores
    graph connections plus vectors |'
  prefs: []
  type: TYPE_TB
- en: Production systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When high accuracy needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Industry standard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires careful tuning of M (connections) and ef (search depth)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| LSH (Locality Sensitive Hashing) | Uses hash functions that map similar vectors
    to the same buckets | Search: O(N![](img/Icon_2.png))Build: O(N) | Medium – stores
    multiple hash tables |'
  prefs: []
  type: TYPE_TB
- en: Streaming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When updates frequent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approximate search OK
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Good for dynamic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tunable accuracy vs speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| IVF (Inverted File Index) | Clusters vectors and searches within relevant
    clusters | Search: O(DN/k)Build: O(kN) | Low – stores cluster assignments |'
  prefs: []
  type: TYPE_TB
- en: Limited memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balance of speed/accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: k = number of clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often combined with other methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Product Quantization (PQ) | Compresses vectors by splitting into subspaces
    and quantizing | Search: variesBuild: O(N) | Very Low – compressed vectors |'
  prefs: []
  type: TYPE_TB
- en: Memory-constrained systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Massive datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Often combined with IVF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires training codebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Tree-Based (KD-Tree, Ball Tree)
  prefs: []
  type: TYPE_NORMAL
- en: '| Recursively partitions space into regions | Search: O(D log N) best caseBuild:
    O(N log N) | Medium – tree structure |'
  prefs: []
  type: TYPE_TB
- en: Low dimensional data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Works well for D < 100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expensive updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4.2: Vector store comparison by deployment options, licensing, and key
    features'
  prefs: []
  type: TYPE_NORMAL
- en: 'When selecting an indexing strategy for your RAG system, consider these practical
    tradeoffs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For maximum accuracy with small datasets** (<100K vectors): Exact Search
    provides perfect recall but becomes prohibitively expensive as your dataset grows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For production systems with millions of vectors**: HNSW offers the best balance
    of search speed and accuracy, making it the industry standard for large-scale
    applications. While it requires more memory than other approaches, its logarithmic
    search complexity delivers consistent performance even as your dataset scales.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For memory-constrained environments**: IVF+PQ (Inverted File Index with Product
    Quantization) dramatically reduces memory requirements—often by 10-20x compared
    to raw vectors—with a modest accuracy tradeoff. This combination is particularly
    valuable for edge deployments or when embedding billions of documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For frequently updated collections**: LSH provides efficient updates without
    rebuilding the entire index, making it suitable for streaming data applications
    where documents are continuously added or removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Most modern vector databases default to HNSW for good reason, but understanding
    these tradeoffs allows you to optimize for your specific constraints when necessary.
    To illustrate the practical difference between indexing strategies, let’s compare
    the performance and accuracy of exact search versus HNSW indexing using FAISS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This example demonstrates the fundamental tradeoff in vector indexing: exact
    search guarantees finding the true nearest neighbors but takes longer, while HNSW
    provides approximate results significantly faster. The overlap percentage shows
    how many of the same nearest neighbors were found by both methods.'
  prefs: []
  type: TYPE_NORMAL
- en: For small datasets like this example (10,000 vectors), the absolute time difference
    is minimal. However, as your dataset grows to millions or billions of vectors,
    exact search becomes prohibitively expensive, while HNSW maintains logarithmic
    scaling—making approximate indexing methods essential for production RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a diagram that can help developers choose the right indexing strategy
    based on their requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2: Choosing an indexing strategy](img/B32363_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Choosing an indexing strategy'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure illustrates a decision tree for selecting the appropriate
    indexing strategy based on your deployment constraints. The flowchart helps you
    navigate key decision points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start by assessing your dataset size**: For small collections (under 100K
    vectors), exact search remains viable and provides perfect accuracy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Consider your memory constraints**: If memory is limited, follow the left
    branch toward compression techniques like **Product Quantization** (**PQ**).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluate update frequency**: If your application requires frequent index
    updates, prioritize methods like LSH that support efficient updates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Assess search speed requirements**: For applications demanding ultra-low
    latency, HNSW typically provides the fastest search times once built.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Balance with accuracy needs**: As you move downward in the flowchart, consider
    the accuracy-efficiency tradeoff based on your application’s tolerance for approximate
    results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For most production RAG applications, you’ll likely end up with HNSW or a combined
    approach like IVF+HNSW, which clusters vectors first (IVF) and then builds efficient
    graph structures (HNSW) within each cluster. This combination delivers excellent
    performance across a wide range of scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: To improve retrieval, documents must be processed and structured effectively.
    The next section explores loading various document types and handling multi-modal
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector libraries, like Facebook (Meta) Faiss or Spotify Annoy, provide functionality
    for working with vector data. They typically offer different implementations of
    the **ANN** algorithm, such as clustering or tree-based methods, and allow users
    to perform vector similarity searches for various applications. Let’s quickly
    go through a few of the most popular ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faiss** is a library developed by Meta (previously Facebook) that provides
    efficient similarity search and clustering of dense vectors. It offers various
    indexing algorithms, including PQ, LSH, and HNSW. Faiss is widely used for large-scale
    vector search tasks and supports both CPU and GPU acceleration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annoy** is a C++ library for approximate nearest neighbor search in high-dimensional
    spaces maintained and developed by Spotify, implementing the Annoy algorithm based
    on a forest of random projection trees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**hnswlib** is a C++ library for approximate nearest-neighbor search using
    the HNSW algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-Metric Space Library** (**nmslib**) supports various indexing algorithms
    like HNSW, SW-graph, and SPTAG.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SPTAG** by Microsoft implements a distributed ANN. It comes with a k-d tree
    and relative neighborhood graph (SPTAG-KDT), and a balanced k-means tree and relative
    neighborhood graph (SPTAG-BKT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a lot more vector search libraries you can choose from. You can get
    a complete overview at [https://github.com/erikbern/ann-benchmarks](https://github.com/erikbern/ann-benchmarks).
  prefs: []
  type: TYPE_NORMAL
- en: 'When implementing vector storage solutions, consider:'
  prefs: []
  type: TYPE_NORMAL
- en: The tradeoff between exact and approximate search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory constraints and scaling requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for hybrid search capabilities combining vector and traditional search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-modal data support requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration costs and maintenance complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For many applications, a hybrid approach combining vector search with traditional
    database capabilities provides the most flexible solution.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the RAG pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think of the RAG pipeline as an assembly line in a library, where raw materials
    (documents) get transformed into a searchable knowledge base that can answer questions.
    Let us walk through how each component plays its part.
  prefs: []
  type: TYPE_NORMAL
- en: '**Document processing – the foundation**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Document processing is like preparing books for a library. When documents first
    enter the system, they need to be:'
  prefs: []
  type: TYPE_NORMAL
- en: Loaded using document loaders appropriate for their format (PDF, HTML, text,
    etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformed into a standard format that the system can work with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split into smaller, meaningful chunks that are easier to process and retrieve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when processing a textbook, we might break it into chapter-sized
    or paragraph-sized chunks while preserving important context in metadata.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector indexing – creating the card catalog**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once documents are processed, we need a way to make them searchable. This is
    where vector indexing comes in. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: An embedding model converts each document chunk into a vector (think of it as
    capturing the document’s meaning in a list of numbers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These vectors are organized in a special data structure (the vector store) that
    makes them easy to search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector store also maintains connections between these vectors and their
    original documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is similar to how a library’s card catalog organizes books by subject,
    making it easy to find related materials.
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector stores – the organized shelves**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Vector stores are like the organized shelves in our library. They:'
  prefs: []
  type: TYPE_NORMAL
- en: Store both the document vectors and the original document content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide efficient ways to search through the vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offer different organization methods (like HNSW or IVF) that balance speed and
    accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, using FAISS (a popular vector store), we might organize our vectors
    in a hierarchical structure that lets us quickly narrow down which documents to
    examine in detail.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval – finding the right books**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Retrieval is where everything comes together. When a question comes in:'
  prefs: []
  type: TYPE_NORMAL
- en: The question gets converted into a vector using the same embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector store finds documents whose vectors are most similar to the question
    vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The retriever might apply additional logic, like:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing duplicate information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing relevance and diversity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining results from different search methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A basic RAG implementation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'results = vector_db.similarity_search(query)This implementation covers the
    core RAG workflow: document loading, embedding, storage, and retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Building a RAG system with LangChain requires understanding two fundamental
    building blocks, which we should discuss a bit more in detail: **document loaders**
    and **retrievers**. Let’s explore how these components work together to create
    effective retrieval systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Document processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain provides a comprehensive system for loading documents from various
    sources through document loaders. A document loader is a component in LangChain
    that transforms various data sources into a standardized document format that
    can be used throughout the LangChain ecosystem. Each document contains the actual
    content and associated metadata.
  prefs: []
  type: TYPE_NORMAL
- en: 'Document loaders serve as the foundation for RAG systems by:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting diverse data sources into a uniform format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting text and metadata from files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing documents for further processing (like chunking or embedding)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain supports loading documents from a wide range of document types and
    sources through specialized loaders, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PDFs**: Using PyPDFLoader'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTML**: WebBaseLoader for extracting web page text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plain text**: TextLoader for raw text inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WebBaseLoader** for web page content extraction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ArxivLoader** for scientific papers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WikipediaLoader** for encyclopedia entries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YoutubeLoader** for video transcripts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ImageCaptionLoader** for image content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have noticed some non-text content types in the preceding list. Advanced
    RAG systems can handle non-text data; for example, image embeddings or audio transcripts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table organizes LangChain document loaders into a comprehensive
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Category** | **Description** | **Notable Examples** | **Common Use Cases**
    |'
  prefs: []
  type: TYPE_TB
- en: '| File Systems | Load from local files | TextLoader, CSVLoader, PDFLoader |
    Processing local documents, data files |'
  prefs: []
  type: TYPE_TB
- en: '| Web Content | Extract from online sources | WebBaseLoader, RecursiveURLLoader,
    SitemapLoader | Web scraping, content aggregation |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Storage
  prefs: []
  type: TYPE_NORMAL
- en: '| Access cloud-hosted files | S3DirectoryLoader, GCSFileLoader, DropboxLoader
    | Enterprise data integration |'
  prefs: []
  type: TYPE_TB
- en: '| Databases | Load from structured data stores | MongoDBLoader, SnowflakeLoader,
    BigQueryLoader | Business intelligence, data analysis |'
  prefs: []
  type: TYPE_TB
- en: '| Social Media | Import social platform content | TwitterTweetLoader, RedditPostsLoader,
    DiscordChatLoader | Social media analysis |'
  prefs: []
  type: TYPE_TB
- en: '| Productivity Tools | Access workspace documents | NotionDirectoryLoader,
    SlackDirectoryLoader, TrelloLoader | Knowledge base creation |'
  prefs: []
  type: TYPE_TB
- en: '| Scientific Sources | Load academic content | ArxivLoader, PubMedLoader |
    Research applications |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.3: Document loaders in LangChain'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, modern document loaders offer several sophisticated capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Concurrent loading for better performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata extraction and preservation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Format-specific parsing (like table extraction from PDFs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Error handling and validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with transformation pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s go through an example of loading a JSON file. Here’s a typical pattern
    for using a document loader:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Document loaders come with a standard `.load()` method interface that returns
    documents in LangChain’s document format. The initialization is source-specific.
    After loading, documents often need processing before storage and retrieval, and
    selecting the right chunking strategy determines the relevance and diversity of
    AI-generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Chunking—how you divide documents into smaller pieces—can dramatically impact
    your RAG system’s performance. Poor chunking can break apart related concepts,
    lose critical context, and ultimately lead to irrelevant retrieval results. The
    way you chunk documents affects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval accuracy**: Well-formed chunks maintain semantic coherence, making
    them easier to match with relevant queries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context preservation**: Poor chunking can split related information, causing
    knowledge gaps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response quality**: When the LLM receives fragmented or irrelevant chunks,
    it generates less accurate responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s explore a hierarchy of chunking approaches, from simple to sophisticated,
    to help you implement the most effective strategy for your specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed-size chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most basic approach divides text into chunks of a specified length without
    considering content structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Fixed-size chunking is good for quick prototyping or when document structure
    is relatively uniform, however, it often splits text at awkward positions, breaking
    sentences, paragraphs, or logical units.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive character chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This method respects natural text boundaries by recursively applying different
    separators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works is that the splitter first attempts to divide text at paragraph
    breaks (`\n\n`). If the resulting chunks are still too large, it tries the next
    separator (`\n`), and so on. This approach preserves natural text boundaries while
    maintaining reasonable chunk sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Recursive character chunking is the recommended default strategy for most applications.
    It works well for a wide range of document types and provides a good balance between
    preserving context and maintaining manageable chunk sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Document-specific chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different document types have different structures. Document-specific chunking
    adapts to these structures. An implementation could involve using different specialized
    splitters based on document type using `if` statements. For example, we could
    be using a `MarkdownTextSplitter`, `PythonCodeTextSplitter`, or `HTMLHeaderTextSplitter`
    depending on the content type being markdown, Python, or HTML.
  prefs: []
  type: TYPE_NORMAL
- en: This can be useful when working with specialized document formats where structure
    matters – code repositories, technical documentation, markdown articles, or similar.
    Its advantage is that it preserves logical document structure, maintains functional
    units together (like code functions, markdown sections), and improves retrieval
    relevance for domain-specific queries.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike previous approaches that rely on textual separators, semantic chunking
    analyzes the meaning of content to determine chunk boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s how the `SemanticChunker` works:'
  prefs: []
  type: TYPE_NORMAL
- en: Splits text into sentences
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates embeddings for groups of sentences (determined by `buffer_size`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measures semantic similarity between adjacent groups
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identifies natural breakpoints where topics or concepts change
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates chunks that preserve semantic coherence
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may use semantic chunking for complex technical documents where semantic
    cohesion is crucial for accurate retrieval and when you’re willing to spend additional
    compute/costs on embedding generation.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits include chunk creation based on actual meaning rather than superficial
    text features and keeping related concepts together even when they span traditional
    separator boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Agent-based chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This experimental approach uses LLMs to intelligently divide text based on
    semantic analysis and content understanding in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyze the document’s structure and content
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify natural breakpoints based on topic shifts
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine optimal chunk boundaries that preserve meaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return a list of starting positions for creating chunks
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This type of chunking can be useful for exceptionally complex documents where
    standard splitting methods fail to preserve critical relationships between concepts.
    This approach is particularly useful when:'
  prefs: []
  type: TYPE_NORMAL
- en: Documents contain intricate logical flows that need to be preserved
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content requires domain-specific understanding to chunk appropriately
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum retrieval accuracy justifies the additional expense of LLM-based processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The limitations are that it comes with a higher computational cost and latency,
    and that chunk sizes are less predictable.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal chunking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modern documents often contain a mix of text, tables, images, and code. Multi-modal
    chunking handles these different content types appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can imagine the following process for multi-modal content:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract text, images, and tables separately
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process text with appropriate text chunker
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process tables to preserve structure
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For images: generate captions or extract text via OCR or a vision LLM'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create metadata linking related elements
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embed each element appropriately
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In practice, you would use specialized libraries such as unstructured for document
    parsing, vision models for image understanding, and table extraction tools for
    structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right chunking strategy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Your chunking strategy should be guided by document characteristics, retrieval
    needs, and computational resources as the following table illustrates:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Factor** | **Condition** | **Recommended Strategy** |'
  prefs: []
  type: TYPE_TB
- en: '| **Document Characteristics** | Highly structured documents (markdown, code)
    | Document-specific chunking |'
  prefs: []
  type: TYPE_TB
- en: '|  | Complex technical content | Semantic chunking |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mixed media | Multi-modal approaches |'
  prefs: []
  type: TYPE_TB
- en: '| **Retrieval Needs** | Fact-based QA | Smaller chunks (100-300 tokens) |'
  prefs: []
  type: TYPE_TB
- en: '|  | Complex reasoning | Larger chunks (500-1000 tokens) |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: Context-heavy answers
  prefs: []
  type: TYPE_NORMAL
- en: '| Sliding window with significant overlap |'
  prefs: []
  type: TYPE_TB
- en: '| **Computational Resources** | Limited API budget | Basic recursive chunking
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Performance-critical | Pre-computed semantic chunks |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.4: Comparison of chunking strategies'
  prefs: []
  type: TYPE_NORMAL
- en: We recommend starting with Level 2 (Recursive Character Chunking) as your baseline,
    then experiment with more advanced strategies if retrieval quality needs improvement.
  prefs: []
  type: TYPE_NORMAL
- en: For most RAG applications, the `RecursiveCharacterTextSplitter` with appropriate
    chunk size and overlap settings provides an excellent balance of simplicity, performance,
    and retrieval quality. As your system matures, you can evaluate whether more sophisticated
    chunking strategies deliver meaningful improvements.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is often critical to performance to experiment with different chunk
    sizes specific to your use case and document types. Please refer to [*Chapter
    8*](E_Chapter_8.xhtml#_idTextAnchor390) for testing and benchmarking strategies.
  prefs: []
  type: TYPE_NORMAL
- en: The next section covers semantic search, hybrid methods, and advanced ranking
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval integrates a vector store with other LangChain components for simplified
    querying and compatibility. Retrieval systems form a crucial bridge between unstructured
    queries and relevant documents.
  prefs: []
  type: TYPE_NORMAL
- en: In LangChain, a retriever is fundamentally an interface that accepts natural
    language queries and returns relevant documents. Let’s explore how this works
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its heart, a retriever in LangChain follows a simple yet powerful pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: Takes a query as a string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing**: Applies retrieval logic specific to the implementation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: Returns a list of document objects, each containing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`page_content`: The actual document content'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: Associated information like document ID or source'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This diagram (from the LangChain documentation) illustrates this relationship.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3: The relationship between query, retriever, and documents](img/B32363_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: The relationship between query, retriever, and documents'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain offers a rich ecosystem of retrievers, each designed to solve specific
    information retrieval challenges.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain retrievers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The retrievers can be broadly categorized into a few key groups that serve
    different use cases and implementation needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core infrastructure retrievers** include both self-hosted options like ElasticsearchRetriever
    and cloud-based solutions from major providers like Amazon, Google, and Microsoft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External knowledge retrievers** tap into external and established knowledge
    bases. ArxivRetriever, WikipediaRetriever, and TavilySearchAPI stand out here,
    offering direct access to academic papers, encyclopedia entries, and web content
    respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic retrievers** include several classic information retrieval methods.
    The BM25 and TF-IDF retrievers excel at lexical search, while kNN retrievers handle
    semantic similarity searches. Each of these algorithms brings its own strengths
    – BM25 for keyword precision, TF-IDF for document classification, and kNN for
    similarity matching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced/Specialized retrievers** often address specific performance requirements
    or resource constraints that may arise in production environments. LangChain offers
    specialized retrievers with unique capabilities. NeuralDB provides CPU-optimized
    retrieval, while LLMLingua focuses on document compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration retrievers** connect with popular platforms and services. These
    retrievers, like those for Google Drive or Outline, make it easier to incorporate
    existing document repositories into your RAG application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a basic example of retriever usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'LangChain supports several sophisticated approaches to retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector store retrievers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vector stores serve as the foundation for semantic search, converting documents
    and queries into embeddings for similarity matching. Any vector store can become
    a retriever through the `as_retriever()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: These are the retrievers most relevant for RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**Search API retrievers**: These retrievers interface with external search
    services without storing documents locally. For example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Database retrievers**: These connect to structured data sources, translating
    natural language queries into database queries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SQL databases using text-to-SQL conversion
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph databases using text-to-Cypher translation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document databases with specialized query interfaces
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lexical search retrievers**: These implement traditional text-matching algorithms:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: BM25 for probabilistic ranking
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF for term frequency analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Elasticsearch integration for scalable text search
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modern retrieval systems often combine multiple approaches for better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hybrid search**: Combines semantic and lexical search to leverage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vector similarity for semantic understanding
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Keyword matching for precise terminology
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted combinations for optimal results
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maximal Marginal Relevance (MMR)**: Optimizes for both relevance and diversity
    by:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting documents similar to the query
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring retrieved documents are distinct from each other
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing exploration and exploitation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom retrieval logic**: LangChain allows the creation of specialized retrievers
    by implementing the `BaseRetriever` class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Advanced RAG techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When building production RAG systems, a simple vector similarity search often
    isn’t enough. Modern applications need more sophisticated approaches to find and
    validate relevant information. Let’s explore how to enhance a basic RAG system
    with advanced techniques that dramatically improve result quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'A standard vector search has several limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: It might miss contextually relevant documents that use different terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can’t distinguish between authoritative and less reliable sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might return redundant or contradictory information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has no way to verify if generated responses accurately reflect the source
    material
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern retrieval systems often employ multiple complementary techniques to improve
    result quality. Two particularly powerful approaches are hybrid retrieval and
    re-ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hybrid retrieval: Combining semantic and keyword search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hybrid retrieval combines two retrieval methods in parallel and the results
    are fused to leverage the strengths of both approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dense retrieval**: Uses vector embeddings for semantic understanding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sparse retrieval**: Employs lexical methods like BM25 for keyword precision'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a hybrid retriever might use vector similarity to find semantically
    related documents while simultaneously running a keyword search to catch exact
    terminology matches, then combine the results using rank fusion algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Re-ranking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Re-ranking is a post-processing step that can follow any retrieval method,
    including hybrid retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: First, retrieve a larger set of candidate documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply a more sophisticated model to re-score documents
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reorder based on these more precise relevance scores
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Re-ranking follows three main paradigms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pointwise rerankers**: Score each document independently (for example, on
    a scale of 1-10) and sort the resulting array of documents accordingly'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pairwise rerankers**: Compare document pairs to determine preferences, then
    construct a final ordering by ranking documents based on their win/loss record
    across all comparisons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Listwise rerankers**: The re-ranking model processes the entire list of documents
    (and the original query) holistically to determine optimal order by optimizing
    NDCG or MAP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain offers several re-ranking implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cohere rerank**: Commercial API-based solution with excellent quality:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**RankLLM**: Library supporting open-source LLMs fine-tuned specifically for
    re-ranking:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**LLM-based custom rerankers**: Using any LLM to score document relevance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Please note that while Hybrid retrieval focuses on how documents are retrieved,
    re-ranking focuses on how they’re ordered after retrieval. These approaches can,
    and often should, be used together in a pipeline. When evaluating re-rankers,
    use position-aware metrics like Recall@k, which measures how effectively the re-ranker
    surfaces all relevant documents in the top positions.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-encoder re-ranking typically improves these metrics by 10-20% over initial
    retrieval, especially for the top positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Query transformation: Improving retrieval through better queries'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even the best retrieval system can struggle with poorly formulated queries.
    Query transformation techniques address this challenge by enhancing or reformulating
    the original query to improve retrieval results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Query expansion generates multiple variations of the original query to capture
    different aspects or phrasings. This helps bridge the vocabulary gap between users
    and documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate three alternative versions that express the same information need
    but with different wording:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see this in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We should be getting something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: A more advanced approach is **Hypothetical Document Embeddings** (**HyDE**).
  prefs: []
  type: TYPE_NORMAL
- en: Hypothetical Document Embeddings (HyDE)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'HyDE uses an LLM to generate a hypothetical answer document based on the query,
    and then uses that document’s embedding for retrieval. This technique is especially
    powerful for complex queries where the semantic gap between query and document
    language is significant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Query transformation techniques are particularly useful when dealing with ambiguous
    queries, questions formulated by non-experts, or situations where terminology
    mismatches between queries and documents are common. They do add computational
    overhead but can dramatically improve retrieval quality, especially for complex
    or poorly formulated questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Context processing: maximizing retrieved information value'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once documents are retrieved, context processing techniques help distill and
    organize the information to maximize its value in the generation phase.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Contextual compression extracts only the most relevant parts of retrieved documents,
    removing irrelevant content that might distract the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are our compressed documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Maximum marginal relevance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another powerful approach is **Maximum Marginal Relevance** (**MMR**), which
    balances document relevance with diversity, ensuring that the retrieved set contains
    varied perspectives rather than redundant information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Context processing techniques are especially valuable when dealing with lengthy
    documents where only portions are relevant, or when providing comprehensive coverage
    of a topic requires diverse viewpoints. They help reduce noise in the generator’s
    input and ensure that the most valuable information is prioritized.
  prefs: []
  type: TYPE_NORMAL
- en: The final area for RAG enhancement focuses on improving the generated response
    itself, ensuring it’s accurate, trustworthy, and useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Response enhancement: Improving generator output'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These response enhancement techniques are particularly important in applications
    where accuracy and transparency are paramount, such as educational resources,
    healthcare information, or legal advice. They help build user trust by making
    AI-generated content more verifiable and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first assume we have some documents as our knowledge base:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Source attribution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Source attribution explicitly connects generated information to the retrieved
    sources, helping users verify facts and understand where information comes from.
    Let’s set up our foundation for source attribution. We’ll initialize a vector
    store with our documents and create a retriever configured to fetch the top 3
    most relevant documents for each query. The attribution prompt template instructs
    the model to use citations for each claim and include a reference list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll need helper functions to format the sources with citation numbers
    and generate attributed responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This example implements source attribution by:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving relevant documents for a query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Formatting each document with a citation number
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a prompt that explicitly requests citations for each fact
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating a response that includes inline citations ([1], [2], etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding a references section that links each citation to its source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The key advantages of this approach are transparency and verifiability – users
    can trace each claim back to its source, which is especially important for academic,
    medical, or legal applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what we get when we execute this with a query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Self-consistency checking compares the generated response against the retrieved
    context to verify accuracy and identify potential hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Self-consistency checking: ensuring factual accuracy'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Self-consistency checking verifies that generated responses accurately reflect
    the information in retrieved documents, providing a crucial layer of protection
    against hallucinations. We can use LCEL to create streamlined verification pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The function above begins our verification process by accepting the retrieved
    documents and generated answers as inputs. It initializes a language model for
    verification if one isn’t provided and combines all document content into a single
    context string. Next, we’ll define the verification prompt that instructs the
    LLM to perform a detailed fact-checking analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The verification prompt is structured to perform a comprehensive fact check.
    It instructs the model to break down each claim in the answer and categorize it
    based on how well it’s supported by the provided context. The prompt also requests
    the output in a structured JSON format that can be easily processed programmatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we’ll complete the function with the verification chain and example
    usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get a response like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the verification result, you can:'
  prefs: []
  type: TYPE_NORMAL
- en: Regenerate the answer if issues are found
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add qualifying statements to indicate uncertainty
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter out unsupported claims
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Include confidence indicators for different parts of the response
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This approach systematically analyzes generated responses against source documents,
    identifying specific unsupported claims rather than just providing a binary assessment.
    For each factual assertion, it determines whether it’s fully supported, partially
    supported, contradicted, or not mentioned in the context.
  prefs: []
  type: TYPE_NORMAL
- en: Self-consistency checking is essential for applications where trustworthiness
    is paramount, such as medical information, financial advice, or educational content.
    Detecting and addressing hallucinations before they reach users significantly
    improves the reliability of RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The verification can be further enhanced by:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Granular claim extraction**: Breaking down complex responses into atomic
    factual claims'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evidence linking**: Explicitly connecting each claim to specific supporting
    text'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Confidence scoring**: Assigning numerical confidence scores to different
    parts of the response'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selective regeneration**: Regenerating only the unsupported portions of responses'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These techniques create a verification layer that substantially reduces the
    risk of presenting incorrect information to users while maintaining the fluency
    and coherence of generated responses.
  prefs: []
  type: TYPE_NORMAL
- en: While the techniques we’ve discussed enhance individual components of the RAG
    pipeline, corrective RAG represents a more holistic approach that addresses fundamental
    retrieval quality issues at a systemic level.
  prefs: []
  type: TYPE_NORMAL
- en: Corrective RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The techniques we’ve explored so far mostly assume that our retrieval mechanism
    returns relevant, accurate documents. But what happens when it doesn’t? In real-world
    applications, retrieval systems often return irrelevant, insufficient, or even
    misleading content. This “garbage in, garbage out” problem represents a critical
    vulnerability in standard RAG systems. **Corrective Retrieval-Augmented Generation**
    (**CRAG**) directly addresses this challenge by introducing explicit evaluation
    and correction mechanisms into the RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'CRAG extends the standard RAG pipeline with evaluation and conditional branching:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Initial retrieval:** Standard document retrieval from the vector store based
    on the query.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Retrieval evaluation:** A retrieval evaluator component assesses each document’s
    relevance and quality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Conditional correction:**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Relevant documents:** Pass high-quality documents directly to the generator.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Irrelevant documents:** Filter out low-quality documents to prevent noise.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Insufficient/Ambiguous results:** Trigger alternative information-seeking
    strategies (like web search) when internal knowledge is inadequate.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generation:** Produce the final response using the filtered or augmented
    context.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This workflow transforms RAG from a static pipeline into a more dynamic, self-correcting
    system capable of seeking additional information when needed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4: Corrective RAG workflow showing evaluation and conditional branching](img/B32363_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Corrective RAG workflow showing evaluation and conditional branching'
  prefs: []
  type: TYPE_NORMAL
- en: 'The retrieval evaluator is the cornerstone of CRAG. Its job is to analyze the
    relationship between retrieved documents and the query, determining which documents
    are truly relevant. Implementations typically use an LLM with a carefully crafted
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: By evaluating each document independently, CRAG can make fine-grained decisions
    about which content to include, exclude, or supplement, substantially improving
    the quality of the final context provided to the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Since the CRAG implementation builds on concepts we’ll introduce in [*Chapter
    5*](E_Chapter_5.xhtml#_idTextAnchor231), we’ll not be showing the complete code
    here, but you can find the implementation in the book’s companion repository.
    Please note that LangGraph is particularly well-suited for implementing CRAG because
    it allows for conditional branching based on document evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: While CRAG enhances RAG by adding evaluation and correction mechanisms to the
    retrieval pipeline, Agentic RAG represents a more fundamental paradigm shift by
    introducing autonomous AI agents to orchestrate the entire RAG process.
  prefs: []
  type: TYPE_NORMAL
- en: Agentic RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Agentic RAG employs AI agents—autonomous systems capable of planning, reasoning,
    and decision-making—to dynamically manage information retrieval and generation.
    Unlike traditional RAG or even CRAG, which follow relatively structured workflows,
    agentic RAG uses agents to:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyze queries and decompose complex questions into manageable sub-questions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plan information-gathering strategies based on the specific task requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select appropriate tools (retrievers, web search, calculators, APIs, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute multi-step processes, potentially involving multiple rounds of retrieval
    and reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflect on intermediate results and adapt strategies accordingly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key distinction between CRAG and agentic RAG lies in their focus: CRAG
    primarily enhances data quality through evaluation and correction, while agentic
    RAG focuses on process intelligence through autonomous planning and orchestration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Agentic RAG is particularly valuable for complex use cases that require:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step reasoning across multiple information sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic tool selection based on query analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Persistent task execution with intermediate reflection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with various external systems and APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, agentic RAG introduces significant complexity in implementation, potentially
    higher latency due to multiple reasoning steps, and increased computational costs
    from multiple LLM calls for planning and reflection.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), we’ll explore the implementation
    of agent-based systems in depth, including patterns that can be applied to create
    agentic RAG systems. The core techniques—tool integration, planning, reflection,
    and orchestration—are fundamental to both general agent systems and agentic RAG
    specifically.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding both CRAG and agentic RAG approaches, you’ll be equipped to
    select the most appropriate RAG architecture based on your specific requirements,
    balancing accuracy, flexibility, complexity, and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When implementing advanced RAG techniques, consider the specific requirements
    and constraints of your application. To guide your decision-making process, the
    following table provides a comprehensive comparison of RAG approaches discussed
    throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **RAG Approach** | **Chapter Section** | **Core Mechanism** | **Key Strengths**
    | **Key Weaknesses** | **Primary Use Cases** | **Relative Complexity** |'
  prefs: []
  type: TYPE_TB
- en: '| Naive RAG | Breaking down the RAG pipeline | Basic index ![](img/Icon.png)
    retrieve ![](img/Icon.png) generate workflow with single retrieval step |'
  prefs: []
  type: TYPE_TB
- en: Simple implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low initial resource usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Straightforward debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Limited retrieval quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vulnerability to hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No handling of retrieval failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Simple Q&A systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic document lookup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prototyping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Low |'
  prefs: []
  type: TYPE_TB
- en: '| Hybrid Retrieval | Advanced RAG techniques – hybrid retrieval | Combines
    sparse (BM25) and dense (vector) retrieval methods |'
  prefs: []
  type: TYPE_TB
- en: Balances keyword precision with semantic understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handles vocabulary mismatch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improves recall without sacrificing precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Increased system complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenge in optimizing fusion weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher computational overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Technical documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content with specialized terminology
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-domain knowledge bases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Re-ranking | Advanced RAG techniques – re-ranking | Post-processes initial
    retrieval results with more sophisticated relevance models |'
  prefs: []
  type: TYPE_TB
- en: Improves result ordering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Captures nuanced relevance signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be applied to any retrieval method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Additional computation layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May create bottlenecks for large result sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requires training or configuring re-rankers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: When retrieval quality is critical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For handling ambiguous queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-value information needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Query Transformation (HyDE) | Advanced RAG techniques – query transformation
    | Generates hypothetical document from query for improved retrieval |'
  prefs: []
  type: TYPE_TB
- en: Bridges query-document semantic gap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improves retrieval for complex queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handles implicit information needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Additional LLM generation step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depends on hypothetical document quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential for query drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Complex or ambiguous queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users with unclear information needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domain-specific search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Medium |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Context Processing
  prefs: []
  type: TYPE_NORMAL
- en: '| Advanced RAG techniques - context processing | Optimizes retrieved documents
    before sending to the generator (compression, MMR) |'
  prefs: []
  type: TYPE_TB
- en: Maximizes context window utilization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces redundancy Focuses on most relevant information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Risk of removing important context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing adds latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: May lose document coherence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Large documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When context window is limited
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Redundant information sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Medium |'
  prefs: []
  type: TYPE_TB
- en: '| Response Enhancement | Advanced RAG techniques – response enhancement | Improves
    generated output with source attribution and consistency checking |'
  prefs: []
  type: TYPE_TB
- en: Increases output trustworthiness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides verification mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhances user confidence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: May reduce fluency or conciseness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional post-processing overhead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex implementation logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Educational or research content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Legal or medical information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When attribution is required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Medium-High |'
  prefs: []
  type: TYPE_TB
- en: '| Corrective RAG (CRAG) | Advanced RAG techniques – corrective RAG | Evaluates
    retrieved documents and takes corrective actions (filtering, web search) |'
  prefs: []
  type: TYPE_TB
- en: Explicitly handles poor retrieval results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improves robustness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can dynamically supplement knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Increased latency from evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depends on evaluator accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More complex conditional logic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: High-reliability requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems needing factual accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications with potential knowledge gaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| High |'
  prefs: []
  type: TYPE_TB
- en: '| Agentic RAG | Advanced RAG techniques – agentic RAG | Uses autonomous AI
    agents to orchestrate information gathering and synthesis |'
  prefs: []
  type: TYPE_TB
- en: Highly adaptable to complex tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can use diverse tools beyond retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-step reasoning capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Significant implementation complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher cost and latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenging to debug and control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Complex multi-step information tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Research applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systems integrating multiple data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Very High |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.5: Comparing RAG techniques'
  prefs: []
  type: TYPE_NORMAL
- en: For technical or specialized domains with complex terminology, hybrid retrieval
    provides a strong foundation by capturing both semantic relationships and exact
    terminology. When dealing with lengthy documents where only portions are relevant,
    add contextual compression to extract the most pertinent sections.
  prefs: []
  type: TYPE_NORMAL
- en: For applications where accuracy and transparency are critical, implement source
    attribution and self-consistency checking to ensure that generated responses are
    faithful to the retrieved information. If users frequently submit ambiguous or
    poorly formulated queries, query transformation techniques can help bridge the
    gap between user language and document terminology.
  prefs: []
  type: TYPE_NORMAL
- en: So when should you choose each approach?
  prefs: []
  type: TYPE_NORMAL
- en: Start with naive RAG for quick prototyping and simple question-answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add hybrid retrieval when facing vocabulary mismatch issues or mixed content
    types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement re-ranking when the initial retrieval quality needs refinement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use query transformation for complex queries or when users struggle to articulate
    information needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply context processing when dealing with limited context windows or redundant
    information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add response enhancement for applications requiring high trustworthiness and
    attribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider CRAG when reliability and factual accuracy are mission-critical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore agentic RAG (covered more in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231))
    for complex, multi-step information tasks requiring reasoning
  prefs: []
  type: TYPE_NORMAL
- en: In practice, production RAG systems often combine multiple approaches. For example,
    a robust enterprise system might use hybrid retrieval with query transformation,
    apply context processing to optimize the retrieved information, enhance responses
    with source attribution, and implement CRAG’s evaluation layer for critical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Start with implementing one or two key techniques that address your most pressing
    challenges, then measure their impact on performance metrics like relevance, accuracy,
    and user satisfaction. Add additional techniques incrementally as needed, always
    considering the tradeoff between improved results and increased computational
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate a RAG system in practice, in the next section, we’ll walk through
    the implementation of a chatbot that retrieves and integrates external knowledge
    into responses.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a corporate documentation chatbot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will build a corporate documentation chatbot that leverages
    LangChain for LLM interactions and LangGraph for state management and workflow
    orchestration. LangGraph complements the implementation in several critical ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Explicit state management**: Unlike basic RAG pipelines that operate as linear
    sequences, LangGraph maintains a formal state object containing all relevant information
    (queries, retrieved documents, intermediate results, etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional processing**: LangGraph enables conditional branching based on
    the quality of retrieved documents or other evaluation criteria—essential for
    ensuring reliable output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-step reasoning**: For complex documentation tasks, LangGraph allows
    breaking the process into discrete steps (retrieval, generation, validation, refinement)
    while maintaining context throughout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human-in-the-loop integration**: When document quality or compliance cannot
    be automatically verified, LangGraph facilitates seamless integration of human
    feedback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the **Corporate Documentation Manager** tool we built, you can generate,
    validate, and refine project documentation while incorporating human feedback
    to ensure compliance with corporate standards. In many organizations, maintaining
    up-to-date project documentation is critical. Our pipeline leverages LLMs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generate documentation**: Produce detailed project documentation from a user’s
    prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conduct compliance checks**: Analyze the generated document for adherence
    to corporate standards and best practices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handle human feedback**: Solicit expert feedback if compliance issues are
    detected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finalize documentation**: Revise the document based on feedback to ensure
    it is both accurate and compliant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea is that this process not only streamlines documentation creation but
    also introduces a safety net by involving human-in-the-loop validation. The code
    is split into several modules, each handling a specific part of the pipeline,
    and a Streamlit app ties everything together for a web-based interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will demonstrate the following key features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modular pipeline design**: Defines a clear state and uses nodes for documentation
    generation, compliance analysis, human feedback, and finalization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive interface**: Integrates the pipeline with Gradio for real-time
    user interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While this chapter provides a brief overview of performance measurements and
    evaluation metrics, an in-depth discussion of performance and observability will
    be covered in [*Chapter 8*](E_Chapter_8.xhtml#_idTextAnchor390). Please make sure
    you have installed all the dependencies needed for this book, as explained in
    [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044). Otherwise, you might run into
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, given the pace of the field and the development of the LangChain
    library, we are making an effort to keep the GitHub repository up to date. Please
    see [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).
  prefs: []
  type: TYPE_NORMAL
- en: 'For any questions, or if you have any trouble running the code, please create
    an issue on GitHub or join the discussion on Discord: [https://packt.link/lang](https://packt.link/lang).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started! Each file in the project serves a specific role in the overall
    documentation chatbot. Let’s first look at document loading.
  prefs: []
  type: TYPE_NORMAL
- en: Document loading
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main purpose of this module is to give an interface to read different document
    formats.
  prefs: []
  type: TYPE_NORMAL
- en: The `Document` class in LangChain is a fundamental data structure for storing
    and manipulating text content along with associated metadata. It stores text content
    through its required `page_content` parameter along with optional metadata stored
    as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class also supports an optional `id` parameter that ideally should be formatted
    as a UUID to uniquely identify documents across collections, though this isn’t
    strictly enforced. Documents can be created by simply passing content and metadata,
    as in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: This interface serves as the standard representation of text data throughout
    LangChain’s document processing pipelines, enabling consistent handling during
    loading, splitting, transformation, and retrieval operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This module is responsible for loading documents in various formats. It defines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Custom Loader classes**: The `EpubReader` class inherits from `UnstructuredEPubLoader`
    and configures it to work in “fast” mode using element extraction, optimizing
    it for EPUB document processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DocumentLoader class**: A central class that manages document loading across
    different file formats by maintaining a mapping between file extensions and their
    appropriate loader classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**load_document function**: A utility function that accepts a file path, determines
    its extension, instantiates the appropriate loader class from the `DocumentLoader`''s
    mapping, and returns the loaded content as a list of `Document` objects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s get the imports out of the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This module first defines a custom class, `EpubReader`, that inherits from `UnstructuredEPubLoader`.
    This class is responsible for loading documents with supported extensions. The
    `supported_extentions` dictionary maps file extensions to their corresponding
    document loader classes. This gives us interfaces to read PDF, text, EPUB, and
    Word documents with different extensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `EpubReader` class inherits from an EPUB loader and configures it to work
    in `"fast"` mode using element extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `DocumentLoader` maintains a mapping (`supported_extensions`) of file extensions
    (for example, .pdf, .txt, .epub, .docx, .doc) to their respective loader classes.
    But we’ll also need one more function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: The `load_document` function defined above takes a file path, determines its
    extension, selects the appropriate loader from the `supported_extensions` dictionary,
    and returns a list of `Document` objects. If the file extension isn’t supported,
    it raises a `DocumentLoaderException` to alert the user that the file type cannot
    be processed.
  prefs: []
  type: TYPE_NORMAL
- en: Language model setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `llms.py` module sets up the LLM and embeddings for the application. First,
    the imports and loading the API keys as environment variables – please see [*Chapter
    2*](E_Chapter_2.xhtml#_idTextAnchor044) for details if you skipped that part.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s initialize the LangChain `ChatGroq` interface using the API key from
    environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This uses `ChatGroq` (configured with a specific model, temperature, and retries)
    for generating documentation drafts and revisions. The configured model is the
    DeepSeek 70B R1 model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll then use `OpenAIEmbeddings` to convert text into vector representations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: To reduce API costs and speed up repeated queries, it wraps the embeddings with
    a caching mechanism (`CacheBackedEmbeddings`) that stores vectors locally in a
    file-based store (`LocalFileStore`).
  prefs: []
  type: TYPE_NORMAL
- en: Document retrieval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `rag.py` module implements document retrieval based on semantic similarity.
    We have these main components:'
  prefs: []
  type: TYPE_NORMAL
- en: Text splitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory vector store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DocumentRetriever` class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start with the imports again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to set up a vector store for the retriever to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The document chunks are stored in an `InMemoryVectorStore` using the cached
    embeddings, allowing for fast similarity searches. The module uses `RecursiveCharacterTextSplitter`
    to break documents into smaller chunks, which makes them more manageable for retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This custom retriever inherits from a base retriever and manages an internal
    list of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a few methods that we should explain:'
  prefs: []
  type: TYPE_NORMAL
- en: '`store_documents()` splits the documents and adds them to the vector store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_uploaded_docs()` processes files uploaded by the user, stores them temporarily,
    loads them as documents, and adds them to the vector store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_get_relevant_documents()` returns the top k documents related to a given
    query from the vector store. This is the similarity search that we’ll use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing the state graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `rag.py` module implements the RAG pipeline that ties together document
    retrieval with LLM-based generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**System prompt**: A template prompt instructs the AI on how to use the provided
    document snippets when generating a response. This prompt sets the context and
    provides guidance on how to utilize the retrieved information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State definition**: A `TypedDict` class defines the structure of our graph’s
    state, tracking key information like the user’s question, retrieved context documents,
    generated answers, issues reports, and the conversation’s message history. This
    state object flows through each node in our pipeline and gets updated at each
    step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline steps**: The module defines several key functions that serve as
    processing nodes in our graph:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieve function**: Fetches relevant documents based on the user’s query'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**generate function**: Creates a draft answer using the retrieved documents
    and query'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**double_check function**: Evaluates the generated content for compliance with
    corporate standards'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**doc_finalizer function**: Either returns the original answer if no issues
    were found or revises it based on the feedback from the checker'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph compilation**: Uses a state graph (via LangGraph’s `StateGraph`) to
    define the sequence of steps. The pipeline is then compiled into a runnable graph
    that can process queries through the complete workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s get the imports out of the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned earlier, the system prompt template instructs the AI on how
    to use the provided document snippets when generating a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll then instantiate a `DocumentRetriever` and a `prompt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We then have to define the state of the graph. A `TypedDict` state is used
    to hold the current state of the application (for example, question, context documents,
    answer, issues report):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of these fields corresponds to a node in the graph that we’ll define with
    LangGraph. We have the following processing in the nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`retrieve` function: Uses the retriever to get relevant documents based on
    the most recent message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`generate` function: Creates a draft answer by combining the retrieved document
    content with the user question using the chat prompt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`double_check` function: Reviews the generated draft for compliance with corporate
    standards. It checks the draft and sets flags if issues are detected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_finalizer` function: If issues are found, it revises the document based
    on the provided feedback; otherwise, it returns the original answer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start with the retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll also implement a content validation check as a critical quality assurance
    step in our RAG pipeline. Please note that this is the simplest implementation
    possible. In a production environment, we could have implemented a human-in-the-loop
    review process or more sophisticated guardrails. Here, we’re using an LLM to analyze
    the generated content for any issues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The final node integrates any feedback to produce the finalized, compliant
    document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'With our nodes defined, we construct the state graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the sequential flow from document retrieval to generation, validation,
    and finalization looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5:  State graph of the corporate documentation pipeline](img/B32363_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: State graph of the corporate documentation pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before building a user interface, it’s important to test our RAG pipeline to
    ensure it functions correctly. Let’s examine how we can do this programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The execution time varies depending on the complexity of the query and how
    extensively the model needs to reason about its response. Each step in our graph
    may involve API calls to the LLM, which contributes to the overall processing
    time. Once the pipeline completes, we can extract the final response from the
    returned object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The response object contains the complete state of our workflow, including all
    intermediate results. By accessing `response["messages"][-1].content`, we’re retrieving
    the content of the last message, which contains the finalized answer generated
    by our RAG pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve confirmed our pipeline works as expected, we can create a user-friendly
    interface. While there are several Python frameworks available for building interactive
    interfaces (such as Gradio, Dash, and Taipy), we’ll use Streamlit due to its popularity,
    simplicity, and strong integration with data science workflows. Let’s explore
    how to create a comprehensive user interface for our RAG application!
  prefs: []
  type: TYPE_NORMAL
- en: Integrating with Streamlit for a user interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We integrate our pipeline with Streamlit to enable interactive documentation
    generation. This interface lets users submit documentation requests and view the
    process in real time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll configure the Streamlit page with a title and wide layout for better
    readability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll initialize the session state for chat history and file management:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Every time we reload the app, we display chat messages from the history on
    the app rerun:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The retriever processes all uploaded files and embeds them for semantic search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Please remember to avoid repeated calls for the same documents, we’re using
    a cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need a function next to invoke the graph and return a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This ignores the previous messages. We could change the prompt to provide previous
    messages to the LLM. We can then show a project description using markdown. Just
    briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we present our UI in two columns, one for chat and one for file management:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Column 1 looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Column 2 takes the files and gives them to the retriever:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'To run our Corporate Documentation Manager application on Linux or macOS, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open your terminal and change directory to where your project files are. This
    ensures that the `chapter4/` directory is accessible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set `PYTHONPATH` and run Streamlit. The imports within the project rely on
    the current directory being in the Python module search path. Therefore, we’ll
    set `PYTHONPATH` when we run Streamlit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding command tells Python to look in the current directory for modules,
    allowing it to find the `chapter4` package.
  prefs: []
  type: TYPE_NORMAL
- en: Once the command runs successfully, Streamlit will start a web server. Open
    your web browser and navigate to `http://localhost:8501` to use the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Troubleshooting tips**'
  prefs: []
  type: TYPE_NORMAL
- en: Please make sure you’ve installed all required packages. You can ensure you
    have Python installed on your system by using pip or other package managers as
    explained in [*Chapter 2*](E_Chapter_2.xhtml#_idTextAnchor044).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you encounter import errors, verify that you’re in the correct directory
    and that `PYTHONPATH` is set correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By following these steps, you should be able to run the application and use
    it to generate, check, and finalize corporate documentation with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and performance considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107), we explored implementing
    RAG with citations in the Corporate Documentation Manager example. To further
    enhance reliability, additional mechanisms can be incorporated into the pipeline.
    One improvement is to integrate a robust retrieval system such as FAISS, Pinecone,
    or Elasticsearch to fetch real-time sources. This is complemented by scoring mechanisms
    like precision, recall, and mean reciprocal rank to evaluate retrieval quality.
    Another enhancement involves assessing answer accuracy by comparing generated
    responses against ground-truth data or curated references and incorporating human-in-the-loop
    validation to ensure the outputs are both correct and useful.
  prefs: []
  type: TYPE_NORMAL
- en: It is also important to implement robust error-handling routines within each
    node. For example, if a citation retrieval fails, the system might fall back to
    default sources or note that citations could not be retrieved. Building observability
    into the pipeline by logging API calls, node execution times, and retrieval performance
    is essential for scaling up and maintaining reliability in production. Optimizing
    API use by leveraging local models when possible, caching common queries, and
    managing memory efficiently when handling large-scale embeddings further supports
    cost optimization and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating and optimizing our documentation chatbot is vital for ensuring both
    accuracy and efficiency. Modern benchmarks focus on whether the documentation
    meets corporate standards and how accurately it addresses the original request.
    Retrieval quality metrics such as precision, recall, and mean reciprocal rank
    measure the effectiveness of retrieving relevant content during compliance checks.
    Comparing the AI-generated documentation against ground-truth or manually curated
    examples provides a basis for assessing answer accuracy. Performance can be improved
    by fine-tuning search parameters for faster retrieval, optimizing memory management
    for large-scale embeddings, and reducing API costs by using local models for inference
    when applicable.
  prefs: []
  type: TYPE_NORMAL
- en: These strategies build a more reliable, transparent, and production-ready RAG
    application that not only generates content but also explains its sources. Further
    performance and observability strategies will be covered in [*Chapter 8*](E_Chapter_8.xhtml#_idTextAnchor390).
  prefs: []
  type: TYPE_NORMAL
- en: Building an effective RAG system means understanding its common failure points
    and addressing them with quantitative and testing-based strategies. In the next
    section, we’ll explore the typical failure points and best practices in relation
    to RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting RAG systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Barnett and colleagues in their paper *Seven Failure Points When Engineering
    a Retrieval Augmented Generation System* (2024), and Li and colleagues in their
    paper *Enhancing Retrieval-Augmented Generation: A Study of Best Practices* (2025)
    emphasize the importance of both robust design and continuous system calibration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Foundational setup**: Ensure comprehensive and high-quality document collections,
    clear prompt formulations, and effective retrieval techniques that enhance precision
    and relevance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous calibration**: Regular monitoring, user feedback, and updates
    to the knowledge base help identify emerging issues during operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these practices early in development, many common RAG failures
    can be prevented. However, even well-designed systems encounter issues. The following
    sections explore the seven most common failure points identified by Barnett and
    colleagues (2024) and provide targeted solutions informed by empirical research.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few common failure points and their remedies are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing content**: Failure occurs when the system lacks relevant documents.
    Prevent this by validating content during ingestion and adding domain-specific
    resources. Use explicit signals to indicate when information is unavailable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missed top-ranked documents**: Even with relevant documents available, poor
    ranking can lead to their exclusion. Improve this with advanced embedding models,
    hybrid semantic-lexical searches, and sentence-level retrieval.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context window limitations**: When key information is spread across documents
    that exceed the model’s context limit, it may be truncated. Mitigate this by optimizing
    document chunking and extracting the most relevant sentences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information extraction failure**: Sometimes, the LLM fails to synthesize
    the available context properly. This can be resolved by refining prompt design—using
    explicit instructions and contrastive examples enhances extraction accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Format compliance issues**: Answers may be correct but delivered in the wrong
    format (e.g., incorrect table or JSON structure). Enforce structured output with
    parsers, precise format examples, and post-processing validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity mismatch**: The output may be too general or too detailed. Address
    this by using query expansion techniques and tailoring prompts based on the user’s
    expertise level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incomplete information**: Answers might capture only a portion of the necessary
    details. Increase retrieval diversity (e.g., using maximum marginal relevance)
    and refine query transformation methods to cover all aspects of the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating focused retrieval methods, such as retrieving documents first and
    then extracting key sentences, has been shown to improve performance—even bridging
    some gaps caused by smaller model sizes. Continuous testing and prompt engineering
    remain essential to maintaining system quality as operational conditions evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the key aspects of RAG, including vector storage,
    document processing, retrieval strategies, and implementation. Following this,
    we built a comprehensive RAG chatbot that leverages LangChain for LLM interactions
    and LangGraph for state management and workflow orchestration. This is a prime
    example of how you can design modular, maintainable, and user-friendly LLM applications
    that not only generate creative outputs but also incorporate dynamic feedback
    loops.
  prefs: []
  type: TYPE_NORMAL
- en: This foundation opens the door to more advanced RAG systems, whether you’re
    retrieving documents, enhancing context, or tailoring outputs to meet specific
    user needs. As you continue to develop production-ready LLM applications, consider
    how these patterns can be adapted and extended to suit your requirements. In [*Chapter
    8*](E_Chapter_8.xhtml#_idTextAnchor390), we’ll be discussing how to benchmark
    and quantify the performance of RAG systems to ensure performance is up to requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build on this foundation by introducing intelligent
    agents that can utilize tools for enhanced interactions. We will cover various
    tool integration strategies, structured tool output generation, and agent architectures
    such as ReACT. This will allow us to develop more capable AI systems that can
    dynamically interact with external resources.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the key benefits of using vector embeddings in RAG?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does MMR improve document retrieval?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is chunking necessary for effective document retrieval?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What strategies can be used to mitigate hallucinations in RAG implementations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do hybrid search techniques enhance the retrieval process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the key components of a chatbot utilizing RAG principles?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is performance evaluation critical in RAG-based systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different retrieval methods in RAG systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does contextual compression refine retrieved information before LLM processing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subscribe to our weekly newsletter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](E_Chapter_4.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Newsletter_QRcode1.jpg)'
  prefs: []
  type: TYPE_IMG
