- en: Chapter 7. Finding Coreference Between Concepts/People
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Named entity coreference with a document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding pronouns to coreference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-document coreference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The John Smith problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Coreference is a basic mechanism in human language that allows two sentences
    to be about the same thing. It''s a big deal for human communication—it functions
    much in the same way as variable names do in programming languages, with the additional
    subtly that scope is defined by very different rules than blocks. Coreference
    is less important commercially—maybe this chapter will help change that. Here
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Coreference exists between `Alice` and `She`; the phrases talk about the same
    thing. It all gets very interesting when we start asking whether Alice in one
    document is the same as Alice in another.
  prefs: []
  type: TYPE_NORMAL
- en: Coreference, like word-sense disambiguation, is a next-generation industrial
    capacity. The challenges of coreference contribute to the insistence of the IRS
    to have a social security number that unambiguously identifies persons independent
    of their names. Many of the techniques discussed were developed to help track
    persons and organizations in text data with varying degrees of success.
  prefs: []
  type: TYPE_NORMAL
- en: Named entity coreference with a document
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in [Chapter 5](part0061_split_000.html#page "Chapter 5. Finding Spans
    in Text – Chunking"), *Finding Spans in Text – Chunking*, LingPipe can use a variety
    of techniques to recognize proper nouns that correspond to persons, places, things,
    genes, and so on. However, chunking doesn't quite finish the job, because it doesn't
    help with finding an entity when two named entities are the same. Being able to
    say that John Smith is the same entity as Mr. Smith, John or even an exact repeat,
    John Smith, can be very useful—so useful that the idea was the basis of our company
    when we were a baby-defense contractor. Our novel contribution was the generation
    of sentences indexed by what entities they mentioned, which turned out to be an
    excellent way to summarize what was being said about that entity, particularly
    if the mapping spanned languages—we call it **entity-based summarization**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea for entity-based summarization came about as a result of a talk Baldwin
    gave at the University of Pennsylvania at a graduate student seminar. Mitch Marcus,
    the then department chair, thought that showing all sentences that mentioned an
    entity—including pronouns—will be an excellent summary of that entity. In some
    sense, this comment is why LingPipe exists. It led to Baldwin leading a UPenn
    DARPA project and then the creation of Alias-i. Lesson learned—talk to everybody
    about your ideas and research.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will take you through the basics of computing coreferences.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lay your hands on some narrative text; we will use a simple example that we
    know works—coreference systems usually need a lot of tuning to the domain. Feel
    free to pick something else, but it will need to be in English.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, we will take you through running code from the command line and then
    dive into what the code actually does. Off we go.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a simple text to illustrate coreference. The file is in
    `data/simpleCoref.txt`, and it contains:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get thee to a command line and a Java interpreter and reproduce the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are three named entities found. Note that there is an `ID` field in the
    output. The `John Smith` and `Mr. Smith` entities have the same ID, `id=0`. This
    means that the phrases are considered to be coreferent. The remaining entity `Washington`
    has a different ID, `id=1`, and is not coreferent with John Smith / Mr. Smith.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create your own text file, supply it as an argument on the command line, and
    see what gets computed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The coreference code in LingPipe is a heuristic system built on top of sentence
    detection and named-entity recognition. The overall flow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenize the text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Detect sentences in the document, for each sentence, detect named entities
    in the sentence in the left-to-right order, and for each named entity, perform
    the following tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a mention. A mention is a single instance of a named entity.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Mentions can be added to the existing mention chains, or they can start their
    own mention chains.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to resolve the mention to a mention chain that is already created. If a
    unique match is found, then add the mention to the mention chain; otherwise, create
    a new mention chain.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is in `src/com/lingpipe/cookbook/chapter7/NamedEntityCoreference.java`.
    The `main()` method starts by setting up the parts of this recipe, starting with
    a tokenizer factory, sentence chunker, and finally, a named-entity chunker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have set up the basic infrastructure for the recipe. Next is a coreference-specific
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MentionFactory` class creates mentions from phrases and types—the current
    source is named `entities`. Next, the coreference class is created with `MentionFactory`
    as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `WithinDocCoref` class wraps all the mechanics of computing coreference.
    From [Chapter 5](part0061_split_000.html#page "Chapter 5. Finding Spans in Text
    – Chunking"), *Finding Spans in Text - Chunking*, you should be familiar with
    the code to get the document text, detect sentences, and iterate over the sentences
    that apply a named-entity chunker to each sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the context of the current sentence, the named entities from the sentence
    are iterated over in the left-to-right order as they would be read. We know this
    because the `ChunkingImpl` class returns chunks in the order that they were added,
    and our `HMMChunker` adds them in the left-to-right order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code takes the information from the chunk—type and phrase, but
    *not* the offset information, and creates a mention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next line runs coreference with the mention and what sentence it is in
    and returns its ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If the mention was resolved to an existing entity, it will have that ID, as
    we saw with Mr. Smith. Otherwise, it will get a distinct ID and itself be available
    as an antecedent for subsequent mentions.
  prefs: []
  type: TYPE_NORMAL
- en: This covers the mechanics of running within a document coreference. The upcoming
    recipes will cover the modification of this class. The next recipe will add pronouns
    and provide references.
  prefs: []
  type: TYPE_NORMAL
- en: Adding pronouns to coreference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding recipe handled coreference between named entities. This recipe
    will add pronouns to the mix.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will use an interactive version to help you explore the properties
    of the coreference algorithm. The system is very dependent on the quality of the
    named-entity detection, so use examples that the HMM is likely to get right. This
    was trained on *Wall Street Journal* articles from the '90s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Saddle up your console and type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the resulting command prompt, type this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The shared ID between `He` and `John Smith` indicates the coreference between
    the two. More examples will follow, with comments. Note that each input is considered
    a distinct document with separate ID spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If pronouns are not resolved to a named entity, they get the index `-1` as
    shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following case also results in a `-1` value for `id`, because there is
    not one unique person in the prior context but two. This is called a failed uniqueness
    presupposition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following code shows that `John Smith` can be resolved to a female pronoun
    as well. This is because there is no data about what names indicate which genders.
    It can be added, but generally, the context will disambiguate. `John` could be
    a female name. The key here is that the pronoun will disambiguate the gender,
    and a following male pronoun will fail to match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The gender assignment will block reference by an incorrect gender. The `He`
    pronoun in the following code is resolved to ID `-1`, because the only person
    is resolved to a female pronoun:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Coreference can happen inside a sentence as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The order of the mentions (ordered by the most recent mention) matters when
    resolving mentions. In the following code, `He` is resolved to `James`, not `John`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The same effect takes place with named-entity mentions. The `Mr. Smith` entity
    resolves to the last mention:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The distinction between `John` and `James` goes away if there are too many
    intervening sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding examples are meant to demonstrate the properties of the within-document
    coreference system.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code changes to add pronouns are straightforward. The code for this recipe
    is in `src/com/lingpipe/cookbook/chapter7/Coreference.java`. The recipe assumes
    that you understood the previous recipe, so it just covers the addition of pronoun
    mentions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We added the `Mention` objects from multiple sources, so there are no order
    guarantees on the order of elements anymore. Correspondingly, we created `TreeSet`
    and the appropriate comparator and added all the chunkings from the `neChunker`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will add the male and female pronouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MALE_EN_PRONOUNS` constant is a regular expression, `Pattern`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following lines of code show the `addRegExMatchingChunks` subroutine. It
    adds chunks based on regular expression matches and removes the overlapping, existing
    HMM-derived chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The one complex bit is that the type for the `MALE_PRONOUN` and `FEMALE_PRONOUN`
    pronouns will be used to match against `PERSON` entities, with the consequence
    that the resolution sets the gender of the resolved-to entity.
  prefs: []
  type: TYPE_NORMAL
- en: Other than that, the code should look very familiar with our standard I/O loop
    running the interaction in the command prompt.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The algorithm behind the system is based on the PhD. thesis of Baldwin. The
    system was called CogNIAC, and the work is from the mid '90s and is not a current
    state-of-the-art coreference system. A more modern approach would most likely
    use a machine-learning framework to take the features generated by Baldwin's approach
    and many other features and use it to develop a better performing system. A paper
    on the system is at [http://www.aclweb.org/anthology/W/W97/W97-1306.pdf](http://www.aclweb.org/anthology/W/W97/W97-1306.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Cross-document coreference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-document coreference (XDoc) takes the `id` space of an individual document
    and makes it global to a larger universe. This universe typically includes other
    processed documents and databases of known entities. While the annotation is trivial,
    all that one needs to do is swap the document-scope IDs for the universe-scope
    IDs. The calculation of XDoc can be quite difficult.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will tell us how to use a lightweight implementation of XDoc developed
    over the course of deploying such systems over the years. We will provide a code
    overview for those who might want to extend/modify the code—but there is a lot
    going on, and the recipe is quite dense.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input is in the XML format where each file can contain multiple documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The goal is to produce annotations where the mentions of Breck Baldwin share
    the same ID across documents as for Krishna. Note that both are mentioned by their
    nicknames in the last document.
  prefs: []
  type: TYPE_NORMAL
- en: A very common elaboration of XDoc is linking a **database** (**DB**) of known
    entities to text mentions of these entities. This bridges the divide between structured
    DB and unstructured data (text), which many consider to be the next big thing
    in business intelligence / voice of the customer / enterprise-knowledge management.
    We have built systems that linked DBs of genes/proteins to MEDLINE abstracts and
    persons-of-interest lists to free text, and so on. DBs also provide a natural
    way for human editors to control how XDoc behaves.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All the code for this recipe is in the `com.lingpipe.cookbook.chapter7.tracker`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gain access to your IDE and run `RunTracker` or type the following command
    in the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The screen will scroll by with the analysis of documents, but we will go to
    the designated output file and examine it. Open `cookbook/data/xDoc/output/docs1.xml`
    in your favorite text editor. You will see a poorly formatted version of the example
    output, unless your editor automatically formats XML usefully—the Firefox web
    browser does a decent job of rendering XML. The output should look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Krishna` is recognized in the first two documents with the shared ID, `1000000002`,
    but the nickname, `K-dog`, is not recognized at all. `Breck` is recognized in
    all three documents, but since the ID on the third mention, `Breckles`, is different
    from the one in the first two mentions, the system does not consider them to be
    the same entity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will use a DB in the form of a dictionary to improve the recognition
    of the authors when they are mentioned via nicknames. There is a dictionary at
    `data/xDoc/author-dictionary.xml`; it looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The aforementioned dictionary contains nicknames for both authors, in addition
    to their first names. Aliases that have the `xdc=1` value will be used to link
    entities across documents. The `xdc=0` value will only apply within a document.
    All aliases will be used to identify named entities via a dictionary lookup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following command, which specifies the entity dictionary or IDE equivalent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output in `xDoc/output/docs1.xml` is very different from that of the previous
    run. First, note that the IDs for us are now the same as specified in the dictionary
    file: `1` for `Breck` and `2` for `Krishna`. This is a link between the structured
    DB, such as the nature of the dictionary and unstructured text. Second, notice
    that both our nicknames have been correctly identified and assigned to the correct
    IDs. Third, note that the types are now `MALE` instead of `OTHER`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This was a very quick introduction to how to run XDoc. In the next section,
    we will see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up until this recipe, we have attempted to keep code simple, straightforward,
    and understandable without a deep dive into piles of source. This recipe is more
    complicated. The code that backs this recipe is not going to fit into the allocated
    space for complete explanation. The exposition assumes that you will explore entire
    classes on your own and that you will refer to other recipes in this book for
    explanation. We offer this recipe because XDoc coreference is a very interesting
    problem, and our existing infrastructure might help others explore the phenomenon.
    Welcome to the deep end of the pool.
  prefs: []
  type: TYPE_NORMAL
- en: The batch process life cycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The entire process is controlled by the `RunTracker.java` class. The overall
    flow of the `main()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the DB of known entities that will be a source of named-entity recognition
    via `Dictionary` and a known mapping from aliases to dictionary entries. Aliases
    come with instructions regarding whether they should be used for matching entities
    across documents via the `xdc=1` or `xdc=0` flag.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up `EnitityUniverse`, which is the global data structure of IDs for what
    is found in the texts and from the mentioned dictionary of known entities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up what is needed for within-document coreference—things such as a tokenizer,
    sentence detector, and named-entity detector. It gets a bit fancy with a POS tagger
    and some word counts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is a Boolean that controls whether speculative entities will be added.
    If this Boolean is `true`, it means that we will update our universe of cross-document
    entities with the ones that we have never seen before. It is a much tougher task
    to reliably compute with this set to `true`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All the mentioned configuration goes into creating a `Tracker` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, the `main()` method reads in documents to process, hands them off to
    the `Tracker` object for processing, and writes them to disk. The major steps
    of the `Tracker.processDocuments()` method are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a set of documents in the XML format and get the individual documents.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document, apply the `processDocument()` method, which runs within-document
    coreference using the dictionary to help find entities as well as the named-entity
    detector and returns `MentionChain[]`. Then, resolve the individual mentions'
    chains against the entity universe to update document-level IDs to entity universe
    IDs. The last step is to write the document to disk with the entity universe IDs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: That is all that we will say about `RunTracker`; there is nothing in there that
    you should not be able to handle in the context of this book. In the following
    sections, we will address the individual components that `RunTracker` uses.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the entity universe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The entity universe `EntityUniverse.java`, is an in-memory representation of
    the global entities mentioned in a document/database collection. The entity universe
    also contains various indexes into these entities, which support computing XDoc
    on individual documents.
  prefs: []
  type: TYPE_NORMAL
- en: The dictionary seeds the `EntityUniverse` file with known entities, and the
    documents processed subsequently are sensitive to these entities. The XDoc algorithm
    tries to merge with existing entities before creating new ones, so the dictionary
    entities are strong attractors for mentions of these entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each entity consists of a unique long ID, a set of aliases partitioned into
    four separate lists and a type (person, location, and so on). Whether the entity
    is in the user-defined dictionary and whether speculative mentions are allowed
    to be added to the entity are also mentioned. The `toString()` method lists an
    entity as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The global data structures are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Entities need unique IDs, and we have a convention that the `FIRST_SYSTEM_ID`
    value is a large integer, such as `1,000,000`. This provides a space (IDs < 1,000,000)
    for users to add new entities without collisions with entities found by the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will instantiate a tokenizer for use across the tracker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a global mapping from unique entity IDs to the entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Another important data structure is a mapping from aliases (phrases) to entities
    that have the alias—`mXdcPhraseToEntitySet`. Only phrases that are candidates
    for finding likely matches for cross-document coreference get added here. From
    the dictionary, the aliases that are `xdc=1` are added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: For speculatively found aliases, if the alias has at least two tokens and is
    not already on another entity, it is added to this set. This reflects a heuristic
    that tries hard to not split the entities apart. The logic of this is quite twisted
    and beyond the scope of this tutorial. You can refer to `EntityUniverse.createEntitySpeculative`
    and `EntityUniverse.addPhraseToEntity` for the code.
  prefs: []
  type: TYPE_NORMAL
- en: Why are some aliases not used in finding candidate entities? Consider that `George`
    has very little descriptive content to discriminate entities in `EntityUniverse`,
    but `George H.W. Bush` has much more information to work with.
  prefs: []
  type: TYPE_NORMAL
- en: ProcessDocuments() and ProcessDocument()
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The interesting bits start to happen in the `Tracker.processDocuments()` method,
    which calls the XML parsing of each document and then incrementally calls the
    `processDocument()` method. The code is straightforward for the former, so we
    will move on to where the more task-specific work happens with the `processDocument()`
    method called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We used a document format that supports distinguishing the title from the body
    of the document. This is a good idea if title case is distinct from body case,
    as is usual with newswire. The `chains` variable will have chains from the title
    and body of the text, with possible coreference between them. The `mentionStartList`
    and `mentionEndList` arrays will make it possible to realign the document scoped
    IDs with the entity universe scoped IDs later in the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Computing XDoc
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The XDoc code is the result of many hours of hand-tuning the algorithm to work
    well on news-style data. It has been run on datasets in the 20,000 document range
    and is designed to support dictionary entries very aggressively. The code also
    attempts to prevent **short circuits**, which occur when obviously different entities
    have been merged together. If you mistakenly make Barbara Bush and George Bush
    coreferent in your global database, then you will have embarrassingly bad results
    that users will see.
  prefs: []
  type: TYPE_NORMAL
- en: The other sort of error is having two entities in the global store when one
    will do. This is a sort of *Superman/Clark Kent problem* that can also apply to
    multiple mentions of the same name.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin with the top-level code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: A document has a list of mention chains, and each mention chain will be either
    added to an existing entity, or the mention chain will be promoted to being a
    new entity. Mention chains must contain a mention that is not pronominal, which
    is handled at the within-document coreference level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three data structures are updated as each mention chain is processed:'
  prefs: []
  type: TYPE_NORMAL
- en: The `Entity[]` entities are returned by the `xdocCoref` method to support the
    inline annotation of the documents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Map<MentionChain,Entity> chainToEntity` maps from mention chains to entities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ObjectToSet<Entity,MentionChain> entityToChainSet` is the converse of `chainToEntity`.
    It is possible that multiple chains in the same document get mapped to the same
    entity, so this data structure is sensitive to this possibility. This version
    of the code allows this to happen—in effect, XDoc is setting up a within-doc resolution
    as a side effect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple enough, if an entity is found, then the `addMentionChainToEntity()` method
    adds any new information from the mention chain to the entity. New information
    can include new aliases and type changes (that is, a person is moved to being
    male or female in virtue of a disambiguating pronoun reference). If no entity
    is found, then the mention chain goes to `promote()`, which creates a new entity
    in the entity universe. We will start with `promote()`.
  prefs: []
  type: TYPE_NORMAL
- en: The promote() method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The entity universe is a minimalist data structure that just keeps track of
    phrases, types, and IDs. The `TTMentionChain` class is a more complex representation
    of the mentions of a particular document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The call to `mEntityUniverse.createEntitySpeculative` only requires the phrases
    for the chain (in this case, normalized phrases that have been lowercased and
    in which all sequences of whitespaces converted into a single space) and the type
    of the entity. No record is kept of the document from which the mention chain
    came, counts, or other potentially useful information. This is to keep the memory
    representation as small as possible. If there is a need to find all the sentences
    or documents that an entity is mentioned in (a common task), then that mapping
    from entity IDs has to be stored elsewhere. The XML representation produced for
    the document after XDoc is run is a natural place to start addressing these needs.
  prefs: []
  type: TYPE_NORMAL
- en: The createEntitySpeculative() method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Creation of a speculatively found new entity only requires determining which
    of its aliases are the good candidates to link mention chains. Those that are
    good for cross-document coreference go into the `xdcPhrases` set, and the others
    go into the `nonXdc` phrases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The `boolean` method, `XdcPhrase()`, plays a critical role in the XDoc process.
    The current approach supports a very conservative notion of what a good XDoc phrase
    is. Intuitively, in the domain of newswire, phrases such as `he`, `Bob`, and `John
    Smith` are poor indicators of a unique individual being talked about. Good phrases
    might be `Breckenridge Baldwin`, because that is likely a unique name. There are
    lots of fancy theories for what is going on here, see rigid designators ([http://en.wikipedia.org/wiki/Rigid_designator](http://en.wikipedia.org/wiki/Rigid_designator)).
    The next few lines of code run roughshod over 2,000 years of philosophical thought:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This approach attempts to identify the bad phrases for XDoc rather than the
    good ones. The reasoning is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**There is already an entity associated with the phrase**: This enforces an
    assumption that there is only one John Smith in the world. This worked very well
    for intelligence-gathering applications, where the analysts had little trouble
    teasing apart the `John Smith` cases. You can refer to the *The John Smith problem*
    recipe at the end of this chapter for more about this.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The phrase is only one word, and there are multiword phrases associated with
    the mention chain or entity**: This assumes that longer words are better for XDoc.
    Note that different orders of entity creation can result in one-word phrases having
    `xdc` to be `true` on entities with multiword aliases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The phrase is a pronoun**: This is a fairly safe assumption, unless we are
    in religious texts where `He` or `Him` capitalized in the middle of a sentence
    indicate reference to God.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the sets of `xdc` and `nonXdc` phrases are known, then the entity is created.
    Refer to the source code for `Entity.java` to understand how entities are created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the entity is created, and an `add` method updates a mapping in the `EntityUniverse`
    file of `xdc` phrases to entity IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `EntityUniverse` file's global `mXdcPhraseToEntitySet` variable is the key
    to finding candidate entities for XDoc as used in `xdcEntitiesToPhrase()`.
  prefs: []
  type: TYPE_NORMAL
- en: The XDocCoref.addMentionChainToEntity() entity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Returning to the `XDocCoref.xdocCoref()` method, we have covered how to create
    a new entity via `XDocCoref.promote()`. The next option to cover is what happens
    when a mention chain is resolved to an existing entity, namely `XDocCoref.addMentionChainToEntity()`.
    For the speculative mentions to be added, the entity must allow speculatively
    found mentions as provided by the `Entity.allowSpeculativeAliases()` method. This
    is a feature of the user-defined dictionary entities discussed in user-defined
    entities. If speculative entities are allowed, then the mention chains are added
    to the entity with a sensitivity to whether they are `xdc` phrases or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The only change that adding a mention chain can add to an entity is the addition
    of a new phrase. The additional phrases are classified for whether they are `xdc`
    or not in the same way as was done in the promotion of a mention chain.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have gone over the basics of how mention chains from documents
    are either promoted to speculative entities or are merged with existing entities
    in `EntityUniverse`. Next, we will take a look at how resolution occurs in `XDocCoref.resolveMentionChain()`.
  prefs: []
  type: TYPE_NORMAL
- en: The XDocCoref.resolveMentionChain() entity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `XDocCoref.resolveMentionChain()` method assembles a covering set of entities
    that can possibly match the mention chain being resolved and then attempt to find
    a unique entity via a call to `XDocCoref.resolveCandates()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The code assembles a set of entities by doing a lookup into the entity universe
    with `EntityUniverse.xdcEntitiesWithPhrase()`. All aliases for the mention chain
    are tried without consideration of whether they are good XDoc aliases. Before
    the entities are added to `candidateEntities`, the type returned must be consistent
    with the type of the mention chain as determined by `TTMatchers.unifyEntityTypes`.
    This way, `Washington`, a location is not resolved to `Washington`, a person.
    A bit of record keeping is done to determine whether the longest alias on the
    mention chain has matched an entity.
  prefs: []
  type: TYPE_NORMAL
- en: The resolveCandidates() method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `resolveCandidates()` method captures a key assumption that holds both
    for within-document and XDoc coreferences—this unambiguous reference is the only
    basis of resolution. In the within-document case, an example where humans have
    this problem is the sentence, `Bob and Joe were working together. He fell into
    the threshing machine.` Who is `he` referring to? The linguistic expectation that
    a singular referring term have a unique antecedent is called a uniqueness presupposition.
    An example XDoc case is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Doc1**: John Smith is a character from Pocohontas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doc2**: John Smith is the chairman or GM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Doc3**: John Smith is admired'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Which `John Smith` does the `John Smith` from Doc3 go with? Perhaps, neither.
    The algorithm in this software requires that there should be a single possible
    entity that survives the matching criteria. If there is more than one or zero,
    then a new entity is created. The implementation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `filterCandidates` method eliminates all the candidate entities that fail
    for various semantic reasons. Coreference with an entity in the entity universe
    only happens if there is a single possible solution. There is not a distinction
    between too many candidate entities (more than one) or too few (zero). In a more
    advanced system, one could try and further disambiguate if there are too many
    entities via `context`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the heart of the XDoc code. The rest of the code marks up the document
    with entity-universe-relevant indices as returned by the `xdocCoref` method, which
    we just covered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The following `for` loop iterates over the mention chains, which are aligned
    with `Entities[]` returned by `xdocCoref`. For each mention chain, the mention
    is mapped to its cross-document entity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the code will set up a bunch of mappings to create chunks that reflect
    the entity universe IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The actual creation of the chunks happens next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The chunkings are then used to create the relevant portions of the document,
    and `OutputDocument` is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: So, this is what we have to offer as a starting place for XDoc coreference.
    Hopefully, we have explained the intentions behind the more opaque methods. Good
    luck!
  prefs: []
  type: TYPE_NORMAL
- en: The John Smith problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Different people, locations, and concepts can have the same orthographic representation
    but be distinct. There are multiple instances of "John Smith", "Paris", and "bank"
    in the world, and a proper cross-document coreference system should be able to
    handle it. For the case of concepts such as "bank" (a river bank versus a financial
    bank), the term of art is word-sense disambiguation. This recipe will demonstrate
    one approach to the problem that Baldwin developed back in the day with Amit Bagga
    for person disambiguation.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code for this recipe closely follows the clustering tutorial at [http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html](http://alias-i.com/lingpipe/demos/tutorial/cluster/read-me.html)
    but changes it to more closely fit the original Bagga-Baldwin work. There is a
    fair amount of code but nothing very complicated. The source is in `src/com/lingpipe/cookbook/chapter7/JohnSmith.java`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class starts with the standard panoply of NLP tools for tokenization, sentence
    detection, and named-entity detection. Refer to the previous recipes if this stack
    is unfamiliar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we will revisit `TfIdfDistance`. However, the task requires that we
    wrap the class to operate over `Documents` rather than `CharSequences`, because
    we would like to retain the filename and be able to manipulate what text is used
    for the calculations to come:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Dropping to the referenced class, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The `train` method interfaces with the `TfIdfDistance.handle()` method and provides
    an implementation of a `distance(Document doc1, Document doc2)` method that will
    drive the clustering code discussed below. All that the `train` method does is
    pull out the relevant text and hand it off to the `TfIdfDistance` class for the
    relevant value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reference class, `Document`, is an inner class in `JohnSmith`, and it is
    quite simple. It gets sentences that have entities which match the `.*John Smith.*`
    pattern and puts them in the `mCoreferentText` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Going deeper into the code, we will now visit the `getCoreferentSents()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Look at the *Cross-document coreference* recipe for most of the moving parts
    of the preceding method. We will call out a few notable bits. We are cheating
    in some sense by using a regular expression chunker to find any string that has
    as a `John Smith` substring and adding it in as a `PERSON` entity. Like most kinds
    of cheating, this is quite useful if your sole purpose in life is tracking `John
    Smith`. The cheating we did in reality was to use dictionary matching to find
    all variations of high-value intelligence targets such as `Osama bin Laden`. In
    the end, we had over 40 versions of his name scouring openly available news sources
    as a part of the MiTAP project.
  prefs: []
  type: TYPE_NORMAL
- en: Further, as each sentence is processed, we will check all the mentions for a
    matching pattern for `John Smith`, and if so, we will collect any sentence that
    has a mention of this ID. This means that a sentence that refers back to `John
    Smith` with a pronoun will be included, as will the `Mr. Smith` cases if coreference
    is doing its job. Note that we need to see a match for `John Smith` before we
    start collecting contextual information, so we will miss the first sentence of
    `He awoke. John Smith was a giant cockroach`. Also note that if a second `John
    Smith` shows up with a different ID, it will be ignored—this can happen.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that there is some error checking, in that if `John Smith` is
    not found, then an error is reported to `System.out`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we pop back to mundane I/O slinging in our `main()` method after setting
    up `TfIdfDocumentDistance`, we would have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We have not discussed this, but the truth annotation of which document references
    which `Mr. Smith` is encoded in the directory structure of the data. Each subdirectory
    in the top `johnSmith` directory is treated as the truth cluster. So, `referencePartition`
    contains the truth. We could have wrapped this as a classification problem with
    each subdirectory, the correct classification. We will leave it as an exercise
    to you to stuff this into a cross-validating corpus with a logistic regression
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, we will construct the test set by flattening our previous categories
    into a single set of `Documents`. We could have done this in the previous step,
    but mixing tasks tends to produce bugs, and the extra `for` loop does very little
    damage to the execution speed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will tee up the clustering algorithms. We will do both `CompleteLink`
    and `SingleLink` driven by `TfIdfDocumentDistance` that runs the show:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The details of the clustering algorithms are covered in [Chapter 5](part0061_split_000.html#page
    "Chapter 5. Finding Spans in Text – Chunking"), *Finding Spans in Texts – Chunking*.
    Now, we will report performance based on the number of clusters varied from `1`
    to the number of inputs. The one fancy bit is that the `Cross` category uses `SingleLinkClusterer`
    as the reference and `CompleteLinkClusterer` as the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: That's all that we need to do to get ready for this recipe. This is a rare phenomenon
    to be computed, and this is a toy implementation, but the key concepts should
    be evident.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will just run this code and then mess with it a bit:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Get yourself to a terminal and type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result will be piles of information that indicate what sentences are being
    extracted for use in the clustering—remember that the truth annotation is determined
    by the directory that the files are in. The first cluster is `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code reports sentences that contain references to `John Smith`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pronominal reference to `John Smith` is the basis of inclusion of the second
    sentence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The system output goes on, and finally, we will get the results for a single-link
    clustering against the truth and a complete link against the truth. The `K` column
    indicates how many clusters the algorithm was allowed with precision, recall,
    and F-measure reported. The first row is in this case that there is only one cluster
    that will allow for 100 percent recall and 23 percent precision for both complete
    and single links. Looking down at the scores, we can see that the complete link
    reports the best F-measure with 11 clusters at `0.60`—in truth, there are 35 clusters.
    The single-link approach maxes out F-measure at 68 clusters with `0.78` and shows
    much greater robustness on varying numbers of clusters. The cross case shows that
    single link and complete link are quite different in direct comparison as well.
    Note that some `K` values have been eliminated for readability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following output constrains clustering not by cluster size but by the max
    distance threshold. The output is for the single-link cluster with `.05` increases
    the distance and the evaluation is the B-cubed metric. The output is the distance,
    precision, recall, and the size of the resulting cluster. The performance at `.80`
    and `.9` is quite good, but beware of setting production thresholds in this after
    the fact fashion. In a production environment, we will want to see much more data
    before setting the threshold:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The B-cubed (Bagga, Bierman, and Baldwin) evaluation was created to heavily
    penalize pushing large clusters together. It assumes that it is more of a problem
    to push lots of documents about George W. Bush together with George H. W. Bush,
    both large clusters, than to mistake George Bush, the mechanic who got mentioned
    once in the dataset. Other scoring metrics will count both the mistakes as equally
    bad. It is the standard scoring metric used in the literature for this phenomenon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a fair amount of work in the research literature on this exact problem.
    We were not the first ones to think about this, but we came up with the dominant
    evaluation metric, and we released a corpus for other groups to compare themselves
    with us and each other. Our contribution is *Entity-based cross-document coreferencing
    using the Vector Space Model* by Bagga and Baldwin in *ACL '98 Proceedings of
    the 36th Annual Meeting of the Association for Computational Linguistics and 17th
    International Conference on Computational Linguistics*. There has been much progress
    since—there are more than 400 citations to this model on Google Scholar; they
    are worth a look if this problem is of importance to you.
  prefs: []
  type: TYPE_NORMAL
