- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: People across the globe have been amazed by the potential of generative AI,
    and industries across the globe are looking to innovate in their organizations
    and solve business use cases through generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce you to a powerful generative AI service known as
    **Amazon Bedrock**. We’ll begin by providing an overview of the generative AI
    landscape. Then, we’ll examine the challenges industries face with generative
    AI and how Amazon Bedrock addresses those challenges effectively. After, we’ll
    explore the various **foundation models** (**FMs**) that are currently offered
    by Amazon Bedrock and help you assess which model is suitable for specific scenarios.
    Additionally, we’ll cover some of Amazon’s additional generative AI capabilities
    beyond FMs. By the end of this chapter, you will have a solid understanding of
    Amazon Bedrock’s generative AI offerings, model selection criteria, and the broader
    generative AI capabilities available from Amazon.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the generative AI landscape
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are FMs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Amazon Bedrock?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FMs in Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating and selecting the right FM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI capabilities of Amazon
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative AI use cases with Amazon Bedrock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the generative AI landscape
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the advent of ChatGPT, organizations across the globe have explored a
    plethora of use cases that generative AI can solve for them. They have built several
    innovation teams and teams of data scientists to build and explore various use
    cases, including summarizing long documents, extracting information from documents,
    and performing sentiment analysis to gauge satisfaction or discontent toward a
    product or service. If you have been working in the **machine learning** (**ML**)
    or **natural language processing** (**NLP**) field, you may be familiar with how
    a language model works – by understanding the relationship between the words in
    documents. The main objective of these **language models** is to predict the next
    probable word in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the sentence *John loves to eat*, a natural language model is
    trying to predict what the next word or token in the sequence will be. Here, the
    next probable word seems to be *ice-cream*, with a 9.4% chance, as shown in *Figure
    1**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Sentence sequencing prediction](img/B22045_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Sentence sequencing prediction
  prefs: []
  type: TYPE_NORMAL
- en: Language models can do this by converting every word into a numerical vector,
    also known as **embeddings**. Similar words will be closer in the vector space,
    while dissimilar words will be positioned spatially distant from each other. For
    instance, the word *phone* will be far apart from the word *eat* since the semantic
    meanings of these words are different.
  prefs: []
  type: TYPE_NORMAL
- en: Early NLP techniques such as **bag-of-words models** with **Term Frequency -
    Inverse Document Frequency** (**TF-IDF**) scoring and **n-gram** analysis had
    some limitations for language modeling tasks. TF-IDF, which determines word importance
    based on frequency, does not account for semantic context within sentences. N-grams,
    representing adjacent words or characters, do not generalize well for out-of-vocabulary
    terms. What was needed to advance language modeling was a method of representing
    words in a way that captures semantic meaning and relationships between words.
  prefs: []
  type: TYPE_NORMAL
- en: In neural networks, a word embedding model known as **Word2Vec** was able to
    learn associations from a large corpus of text. However, the Word2Vec model struggled
    to perform well with out-of-vocabulary words. Since the 2010s, researchers have
    been experimenting with more advanced sequence modeling techniques to address
    this limitation, such as **recurrent neural networks** (**RNNs**) and **long short-term
    memory** (**LSTM**) networks. These models have memory cells that allow them to
    consider the context of previous words in a sentence when predicting the next
    word. RNNs and LSTMs can capture longer-range dependencies compared to models
    such as Word2Vec. While powerful for modeling word sequences, RNNs and LSTM are
    also more computationally and memory intensive, which means they can hold limited
    context depending on how much data is being fed to the model. Therefore, these
    models are unable to perform well when a whole document with several pages is
    provided.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, researchers at Google and the University of Toronto published a paper
    called *Attention Is All You Need* ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    This paper introduced the **transformer architecture**, which is based on a self-attention
    mechanism rather than recurrent or convolutional layers used in previous models.
    This **self-attention mechanism** allows the model to learn contextual relationships
    between all words (or a set of tokens) in the input simultaneously. It does this
    by calculating the importance of each word concerning other words in the sequence.
    This attention is applied to derive contextual representations for downstream
    tasks such as language modeling or machine translation. One major benefit of the
    transformer architecture is its ability to perform parallel computation with a
    long sequence of words. This enabled transformers to be effectively applied to
    much longer texts and documents compared to previous recurrent models.
  prefs: []
  type: TYPE_NORMAL
- en: Language models based on the transformer architecture exhibit **state-of-the-art**
    (**SOTA**) and near-human-level performance. Since the advent of transformer architecture,
    various models have been developed. This breakthrough paved the way for modern
    **large language models** (**LLMs**), including **Bidirectional Encoder Representations
    from Transformers** (**BERT**), **Generative Pre-Training language model** (**GPT**),
    **Text-To-Text Transfer Transformer** (**T5**), **BLOOM**, and **Anthropic Claude**.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dive into some LLMs that a powering a substantial change in the generative
    AI domain.
  prefs: []
  type: TYPE_NORMAL
- en: What are FMs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the generative AI models today are powered by the transformer-based
    architecture. In general, these generative AI models, also widely known as FMs,
    employ transformers due to their ability to process text one token at a time or
    entire sequences of text at once using self-attention. FMs are trained on massive
    amounts of data with millions or billions of parameters, allowing them to understand
    relationships between words in context to predict subsequent sequences. While
    models based on the transformer architecture currently dominate the field, not
    all FMs rely on this architecture. Some models are built using alternative techniques,
    such as **generative adversarial networks** (**GANs**) or **variational autoencoders**.
  prefs: []
  type: TYPE_NORMAL
- en: GANs utilize two neural networks pitted against each other in competition. The
    first network is known as the **generator** and is tasked with generating synthetic
    samples that mimic real data. For example, the generator could produce new images,
    texts, or audio clips. The second network is called the **discriminator**. Its
    role is to analyze examples, both real and synthetic, to classify which ones are
    genuine and which have been artificially generated.
  prefs: []
  type: TYPE_NORMAL
- en: Through this adversarial process, the generator learns to produce increasingly
    convincing fakes that can fool the discriminator. Meanwhile, the discriminator
    becomes better at detecting subtle anomalies that reveal the synthetic samples.
    Their competing goals drive both networks to continuously improve. An example
    of a GAN can be found at [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/).
    By refreshing the page endlessly, users are presented with an endless stream of
    novel human faces. However, none are real – all are synthetic portraits created
    solely by a GAN trained on vast databases of real human images. The site offers
    a glimpse into how GANs can synthesize highly realistic outputs across many domains.
  prefs: []
  type: TYPE_NORMAL
- en: '**Variational autoencoders** are simpler-to-train generative AI algorithms
    that also utilize two neural networks – an **encoder** and a **decoder**. Encoders
    learn the patterns in the data by mapping it into lower-dimensional latent space,
    while decoders use these patterns from the latent space and generate realistic
    samples.'
  prefs: []
  type: TYPE_NORMAL
- en: While these FMs (transformer, GAN, or variational autoencoders-based) are trained
    on massive datasets, this makes them different from other traditional ML models,
    such as logistic regression, **support vector machines** (**SVM**), decision trees,
    and others. The term *foundation models* was coined by researchers at Stanford
    University at Human-Centered Artificial Intelligence to differentiate them from
    other ML models. The traditional ML models are trained on the labeled data and
    are only capable of performing narrowly defined tasks. For example, there will
    be one model for text generation, another model for summarization, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, FMs learn patterns in language by analyzing the relationships
    between words and sentences while training on a massive dataset containing millions
    or billions of parameters. Due to their enormous pre-training datasets, FMs tend
    to generalize well and understand contextual meaning, which allows them to solve
    various use cases, such as text generation, summarization, entity extraction,
    image generation, and others. Their pre-training enables them to serve as a highly
    adaptable starting point for many different applications. *Figure 1**.2* highlights
    some of the differences between traditional ML models and FMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – Traditional ML models versus FMs](img/B22045_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – Traditional ML models versus FMs
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the range of FMs available, organizations face several challenges when
    adopting these models at scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '**No single model solution**: There is no single model that’s optimized for
    all tasks and models are constantly improving with new advances in technology.
    To address multiple use cases, organizations may need to assemble several models
    that work with each other. This can take significant time and resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security concerns**: Security and privacy pose a major concern as organizations
    want to protect their data and valuable intellectual property, and they also want
    control over how their data is shared and used by these models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time and resource management**: For applications such as document summarization
    and virtual assistants, specific model configuration is needed. This includes
    defining tasks, granting access to internal data sources, and developing APIs
    for the model to take action. This requires a multi-step process and complex coding.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of seamless integration**: Being able to seamlessly integrate into existing
    applications is important to avoid managing large computational infrastructures
    or incurring high costs. Organizations want models to work behind the scenes without
    any heavy lifting or expense.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing these technical, operational, security, and privacy challenges is
    key for organizations to successfully adopt and deploy FMs at an enterprise scale.
  prefs: []
  type: TYPE_NORMAL
- en: These are the very problems that Amazon Bedrock is designed to solve.
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon Bedrock?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Bedrock is a fully managed service that offers various choices of high-performing
    FMs via a single API. *Fully managed* implies that users do not have to worry
    about creating, deploying, and operating the backend infrastructure as it has
    been taken care of by Amazon. So, from within your application or code, you can
    invoke the model on Bedrock with a single API containing your prompt. One of the
    key advantages of Amazon Bedrock is it provides a wide choice of leading FMs from
    Amazon and top AI companies such as Anthropic, AI21 Labs, Cohere, Meta, Stability
    AI, and Mistral.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve defined your use case, the next step is to choose an FM. Amazon
    Bedrock provides a playground experience (a web interface for rapid experimentation)
    where you can experiment with different models and prompts. Additionally, there
    are certain techniques and suitability criteria you need to employ to choose the
    best-fit model for your use case. We will learn how to evaluate LLMs in the upcoming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have evaluated and identified the FM for your use case, the focus
    turns to enhancing its predictive capabilities. Amazon Bedrock provides the following
    key capabilities for refining model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tell me the recipe for chocolate cake` or can be detailed prompts with multiple
    examples, depending on the use case that you are trying to solve. With its playground
    experience, Amazon Bedrock lets you effectively design and formulate prompts through
    rapid experimentation. We will discuss some of these techniques and practical
    aspects of prompt engineering in [*Chapter 3*](B22045_03.xhtml#_idTextAnchor053).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Easy fine-tuning**: Amazon Bedrock allows you to easily customize FMs with
    your dataset. This process is called **fine-tuning** the model and involves training
    the model further with your domain dataset, improving the accuracy for domain-specific
    tasks. Fine-tuning can be done directly from the Amazon Bedrock console or through
    APIs, and by providing your datasets in an Amazon **Simple Storage Service** (**Amazon
    S3**) bucket. We will discuss fine-tuning Amazon Bedrock FMs in detail in [*Chapter
    4*](B22045_04.xhtml#_idTextAnchor073).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Native support for RAG**: **Retrieval augmented generation** (**RAG**) is
    a powerful technique to fetch data from outside the language model, such as from
    internal knowledge bases or external sources, to provide accurate responses to
    domain-specific use cases. This technique is useful when large documents are needed
    that are beyond the context provided by the model. Amazon Bedrock provides native
    support for RAG, so you can connect your data source for retrieval augmentation.
    We will discuss RAG in greater detail in [*Chapter 5*](B22045_05.xhtml#_idTextAnchor090).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, there are additional capabilities provided by Amazon Bedrock, such
    as the ability to build intelligent **Agents** to orchestrate and carry out multiple
    tasks on your behalf. Agents can call various internal and external data sources,
    connect to applications, and run complex tasks in multiple steps. We will dive
    deep into building intelligent Agents in [*Chapter 10*](B22045_10.xhtml#_idTextAnchor192).
  prefs: []
  type: TYPE_NORMAL
- en: Security, privacy, and observability are some of the key capabilities of Amazon
    Bedrock. The data that you provide when you invoke FMs, including prompts and
    context, isn’t used to retain any of the FMs. In addition, all the AWS security
    and governance capabilities, including data encryption, IAM authentication and
    permission policies, VPC configuration, and others, apply to Amazon Bedrock. Hence,
    you can encrypt your data at rest and in transit. You can tell Amazon Bedrock
    to use **Virtual Private Cloud** (**VPC**) so that the traffic between AWS-hosted
    system components does not flow through the internet. Also, via **Identity and
    Access Management** (**IAM**), you can provide access to certain resources or
    users. Furthermore, metrics, logs, and API calls are pushed to AWS CloudWatch
    and AWS CloudTrail, so you can have visibility and monitor the usage of Amazon
    Bedrock models. In *Part 3* of the book, we will cover model evaluation, monitoring,
    security, privacy, and ensuring safe and responsible AI practices.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s look at the different FMs offered by Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: FMs in Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With Amazon Bedrock, you have access to six FMs from Amazon and leading AI
    companies – that is, AI21, Anthropic, Command, Stability AI, and Meta – as depicted
    in *Figure 1**.3*. Amazon Bedrock might add access to more FMs in the future:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – FMs available on Amazon Bedrock](img/B22045_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – FMs available on Amazon Bedrock
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss each of these models in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Titan FMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Amazon Titan FMs** represent a suite of powerful, multipurpose models
    developed by AWS through extensive pretraining on vast datasets, endowing them
    with broad applicability across diverse domains. This FM supports use cases such
    as generating texts, question-answering, summarization, RAG, personalization,
    image generation, and more. A simple example would be generating an article/blog
    or writing an email.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three types of Amazon Titan models are currently available on Amazon Bedrock:
    *Titan Text Generation*, *Titan Image Generator*, and *Titan Embeddings*.'
  prefs: []
  type: TYPE_NORMAL
- en: Titan Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Titan Text Generation** is an LLM that’s designed for use cases such as generating
    texts, summarization, and more. Let’s assume that John has to write an email to
    the customer support team of his telephone operator, asking them to fix the billing
    issue he has been facing. We can provide a prompt to the Titan Text Generation
    model. The response will be generated alongside the subject, as shown in *Figure
    1**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Response generated by the Titan Text G1- Express model](img/B22045_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Response generated by the Titan Text G1- Express model
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, Titan Text Generation is available in three different
    flavors – *Titan Text G1 Lite,* *Titan Text G1 Express* and *Titan Text G1 Premier*.
    The main difference is that Lite is a more cost-effective and smaller model and
    supports up to *4,000* tokens, Express is a larger model that supports up to *8,000*
    tokens and is designed for complex use cases, and Premier is most advanced model
    by Titan that supports up to 32k tokens and is designed to provide exceptional
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Titan Image Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Generate an image of a Bunny skiing in the Swiss Alps`. Once the images have
    been generated, we can create variations of a single image, or even edit the image,
    as demonstrated in *Figure 1**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Titan Image Generator and its configurations](img/B22045_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Titan Image Generator and its configurations
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B22045_09.xhtml#_idTextAnchor171), we will learn more about
    how image generation works and dive into various use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Titan Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main function of the **Titan Embeddings** model is to convert texts (or
    images) into numeric vectors. These vectors represent words mathematically so
    that similar words have similar vectors. You can store these embeddings in vector
    databases such as **OpenSearch**, **Aurora pgvector**, **Amazon Kendra**, or **Pinecone**,
    and these databases will be used to compare the relationship between the texts.
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the Titan Embeddings model is available in two variations
    – **Titan Text Embeddings** and **Titan Multimodal Embeddings**. The main difference
    is Titan Text Embeddings converts texts into embeddings, which makes the model
    a suitable fit for use cases such as RAG and clustering, while Titan Multimodal
    Embeddings can convert a combination of texts and images into embeddings, which
    makes it apt for use cases such as searching within images and providing recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: While Titan Text Embeddings supports up to *8,000* tokens and over 25 languages,
    Titan Multimodal Embeddings can support up to *128* tokens with a maximum image
    size of 25 MB. Here, English is the only supported language.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to invoke these models and their input
    configuration parameters. For now, let’s learn about some other FMs provided by
    Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: AI21 Labs – Jurassic-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI21 Labs has built several FMs and task-specific models. However, at the time
    of writing, Amazon Bedrock provides access to *Jamba-Instruct*, *Jurassic 2 –
    Ultra* and *Jurassic 2 –* *Mid* FMs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Jamba-Instruct** supports only English, whereas **Jurassic-2** models support
    multiple languages and use cases such as advanced text generation, comprehension,
    open book Q&A, summarization and others.'
  prefs: []
  type: TYPE_NORMAL
- en: Jamba-Instruct supports context token length of 256K, whereas, **Jurassic-2
    Ultra** and **Jurassic-2 Mid** both support a context token length of 8,192.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example would be the prompt `Give me pointers on how I should grow vegetables
    at home`. The output is depicted in *Figure 1**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – Prompting the Jurassic-2 model](img/B22045_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – Prompting the Jurassic-2 model
  prefs: []
  type: TYPE_NORMAL
- en: Anthropic Claude
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anthropic focuses on safe and responsible AI and provides a group of Claude
    models. These models support use cases such as Q&A, removing **personally identifiable
    information** (**PII**), content generation, roleplay dialogues, and more. One
    major benefit of using Anthropic Claude is its ability to process longer sequences
    of text as prompts. With a maximum context window of *200,000* tokens to date,
    Claude can understand and respond to much more extensive prompts. This larger
    context allows Claude to engage in deeper discussions, understand longer narratives
    or documents, and generate more coherent multi-paragraph responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon Bedrock currently offers access to five versions of Anthropic’s Claude
    language model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Anthropic Claude 3.5 Sonnet**: This sets new industry standards for superior
    intelligence, outperforming its predecessors and other top AI models in various
    benchmarks. Claude 3.5 Sonnet excels in areas like visual processing, content
    generation, customer support, data analysis, and coding. Remarkably, it achieves
    this enhanced performance while being 80% more cost-effective than previous Anthropic
    models, making it an attractive choice for businesses seeking advanced AI capabilities
    at a lower price point. The following link highlights the benchmarks and comparison
    with other models on different tasks: https://aws.amazon.com/blogs/aws/anthropics-claude-3-5-sonnet-model-now-available-in-amazon-bedrock-the-most-intelligent-claude-model-yet/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic Claude 3**: This has three model variants – *Claude 3 Opus*, *Claude
    3 Sonnet*, and *Claude 3 Haiku*. They are the recent and most advanced family
    of Anthropic models available on Amazon Bedrock. All these models have multimodal
    capabilities and can perceive and analyze images (jpeg, png), as well as other
    file types, such as .csv, .doc, .docx, .html, .md, .pdf, .txt, .xls, .xlsx, .gif,
    and text input, with a 200K context window:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Claude 3 Opus**: This is Anthropic’s most capable model to date, with 175
    billion parameters. Opus has advanced few-shot learning capabilities, allowing
    it to quickly adapt to a wide variety of tasks using just a few examples.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Claude 3 Sonnet**: A 60-billion-parameter multimodal AI model, Sonnet has
    strong few-shot learning abilities. Its parameter-efficient architecture allows
    it to handle complex inputs such as long documents while being more computationally
    efficient than Opus.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Claude 3 Haiku**: At 7 billion parameters, Haiku is Anthropic’s most compact
    and lightweight model. It is optimized for efficiency, providing high performance
    for its size. Its low computational requirements make it very fast to run inference.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic Claude 2.1** **and** **Claude 2**: They are also advanced additions
    to Anthropic’s Claude family. They provide performant reasoning capabilities and
    high accuracy with lower hallucination rates. They perform well on use cases such
    as dialogue, creative writing, information, roleplay, summarization, and others.
    In terms of context length, Claude 2.1 supports up to *200,000* tokens and Claude
    2 supports up to *100,000* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic Claude 1.3**: This is an earlier release with capabilities typical
    of LLMs at that time. It demonstrated strong performance on tasks involving factual
    responses, summarization, and basic question-answering. In terms of context length,
    Claude 1.3 supports up to *100,000* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic Claude Instant 1.2**: This offers a faster and more cost-effective
    option compared to other Claude models. The latency of the Claude Instant model
    is greatly reduced at the cost of impacted performance. However, Claude Instant
    still demonstrates strong language skills for many common NLP applications that
    do not require the highest levels of reasoning or nuanced responses, and when
    speed or cost is a higher priority than absolute highest performance. In terms
    of context length, Claude Instant 1.2 supports up to *100,000* tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will walk through some examples of leveraging Anthropic Claude with Bedrock
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Cohere
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon Bedrock offers multiple models from Cohere: *Command*, *Command R+*,
    *Command R*, *Command Light* models, *Embed English,* and *Embed Multilingual*.
    **Cohere Command****,** trained with 52 billion parameters**,** is an LLM useful
    for more complex language understanding. **Command Light**, with 6 billion parameters,
    is cost-effective and faster, making it a good option for those who need a lighter
    model for their applications. **Command R+**, trained on 104 billiion parameters,
    is the most powerful model by Cohere, at the time of writing this book, and is
    designed for tasks with context window size of 128K tokens. **Command R**, trained
    on 35 billion parameters, is also designed for tasks with longer context window
    of 128K tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: Cohere Embed provides a set of models that have been trained to generate high-quality
    embeddings, which we already know are representations of text documents in a numerical
    format in vector space. Cohere offers **Embed English**, which has only been trained
    on English text, as well as **Embed Multilingual**, which can handle multiple
    (more than 100) languages. Embed models support a maximum token length of 512\.
    These embedding models open a wide range of downstream applications, such as semantic
    search to find related documents, RAG, text clustering, classification, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take note of the following figure, which highlights a text generation example
    for summarizing a conversation using the Cohere Command model within Amazon Bedrock’s
    text playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – Cohere Command text generation example in Amazon Bedrock’s text
    playground](img/B22045_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – Cohere Command text generation example in Amazon Bedrock’s text
    playground
  prefs: []
  type: TYPE_NORMAL
- en: Meta Llama 2 and Llama 3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Meta offers several pre-trained LLMs under their **Llama 2** and **Llama 3**
    series for chatbot applications. Their base Llama2 model is pre-trained on over
    2 trillion tokens of publicly available online data sources, at which point it’s
    fine-tuned with over 1 million examples of human annotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Four variants of Llama2 have been made available through Amazon Bedrock: **Llama
    2 Chat 13B**, **Llama 2 Chat 70B**, **Llama 2 13B**, and **Llama 2 70B**. The
    13B model contains 13 billion parameters and its training process took 368,640
    GPU hours to complete. One of the key advantages of the Llama 13B model is its
    ability to process input sequences of arbitrary length, making it well-suited
    for tasks that require long documents or web pages to be analyzed. The larger
    70B model variant contains 70 billion parameters and its training process took
    1,720,320 GPU hours to complete. The 70B model can be used for multitask learning,
    implying it is well suited for performing multiple tasks simultaneously, such
    as image classification, speech recognition, and NLP. It has been shown to achieve
    improved performance on several tasks compared to 13B models, likely due to its
    relatively larger size and higher computational resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Along with Llama2, Meta Llama 3 variants are also available on Amazon Bedrock,
    namely **Llama 3 8B Instruct** and **Llama 3 70B Instruct**. The Llama 3 8B Instruct
    model is optimized for scenarios with limited computational resources, making
    it well-suited for edge devices and applications. It demonstrates strong performance
    in tasks such as text summarization, text classification, language translation,
    and sentiment analysis. The Llama 3 70B Instruct model is tailored for content
    creation, conversational AI systems, language understanding applications, and
    enterprise solutions. It excels in areas such as accurate text summarization,
    nuanced text classification, sophisticated sentiment analysis and reasoning, language
    modeling, dialogue systems, code generation, and following complex instructions.
  prefs: []
  type: TYPE_NORMAL
- en: For developers looking to utilize these models, Meta has created an open source
    GitHub repository called *llama-recipes* ([https://github.com/facebookresearch/llama-recipes/tree/main](https://github.com/facebookresearch/llama-recipes/tree/main))
    that includes demo code and examples of integrating the Llama2 models into chatbots
    and virtual assistants. This provides a starting point for researchers and practitioners
    to experiment with Llama2 and adapt it for their own conversational AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1**.8* demonstrates an entity extraction example using the Meta Llama
    2 Chat 13 B model in Amazon Bedrock’s text playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Entity extraction with the Llama 2 Chat 13B model in Amazon
    Bedrock’s text playground](img/B22045_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Entity extraction with the Llama 2 Chat 13B model in Amazon Bedrock’s
    text playground
  prefs: []
  type: TYPE_NORMAL
- en: Mistral AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Mistral AI** focuses on building compute-efficient, trustworthy, and powerful
    AI models. These are currently available in four variants on Amazon Bedrock –
    *Mistral 7B Instruct*, *Mixtral 8X7B Instruct*, *Mistral Large*, and *Mistral
    Small*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mistral 7B instruct**: This is a 7-billion-parameter dense transformer language
    model designed for instructional tasks. It offers a compelling balance of performance
    and efficiency, delivering robust capabilities suitable for a wide range of use
    cases despite its relatively compact size. Mistral 7B instruct supports processing
    English natural language and code inputs, with an extended 32,000 token context
    window capacity. While more limited than larger models, Mistral 7B instruct provides
    high-quality language understanding, generation, and task execution tailored for
    instructional applications at a lower computational cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mixtral 8X7B**: This is a 7-billion-parameter sparse Mixture-of-Experts language
    model that employs a highly parameter-efficient architecture. Despite its relatively
    compact total size, it utilizes 12 billion active parameters for any given input,
    enabling stronger language understanding and generation capabilities compared
    to similarly-sized dense models such as Mistral 7B. This sparse model supports
    processing inputs across multiple natural languages, as well as coding languages,
    catering to a wide range of multilingual and programming use cases. Additionally,
    Mixtral 8X7B maintains an extended context window of 32,000 tokens, allowing it
    to effectively model long-range dependencies within lengthy inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mistral Large**: This is capable of complex reasoning, analysis, text generation,
    and code generation and excels at handling intricate multilingual tasks across
    English, French, Italian, German, and Spanish. Mistral Large supports a maximum
    context window of 32,000 tokens, enabling it to process long-form inputs while
    delivering SOTA performance on language understanding, content creation, and coding
    applications demanding sophisticated multilingual capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mistral Small**: This is an advanced language model designed for efficiency
    and affordability. It excels in handling high-volume, low-latency language tasks
    swiftly and cost-effectively. With its specialized capabilities, Mistral Small
    seamlessly tackles coding challenges and operates fluently across multiple languages,
    including English, French, German, Spanish, and Italian. Mistral Small supports
    a maximum context window of 32,000 tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 1**.9* illustrates the usage of the Mistral Large model with a reasoning
    scenario within Amazon Bedrock’s text playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – Mistral Large in Amazon Bedrock’s text playground](img/B22045_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – Mistral Large in Amazon Bedrock’s text playground
  prefs: []
  type: TYPE_NORMAL
- en: Stability AI – Stable Diffusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stable Diffusion was developed by Stability AI to generate highly realistic
    images using diffusion models trained on large datasets. The core technique behind
    Stable Diffusion is called **latent diffusion**, which involves using a forward
    diffusion process to add noise to data over time, and a reverse diffusion process
    to gradually remove noise and reconstruct the original data. In the case of image
    generation, this allows the model to generate new images conditioned on text or
    image prompts provided by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Bedrock provides **SDXL 0.8** and **SDXL1.0** Stable Diffusion models
    from Stability AI. The Stable Diffusion model aims to generate highly realistic
    images based on the text or image that’s provided as a prompt. SDXL 1.0 is particularly
    impressive due to its large model sizes. Its base model contains over *3.5 billion*
    parameters, while its ensemble pipeline uses two models totaling *6.6 billion*
    parameters. By aggregating results from multiple models, the ensemble approach
    generates even higher-quality images.
  prefs: []
  type: TYPE_NORMAL
- en: Through Amazon Bedrock, developers can leverage Stable Diffusion for a variety
    of image generation tasks. This includes generating images from text descriptions
    (text-to-image), generating new images based on existing images (image-to-image),
    as well as filling in missing areas (inpainting) or extending existing images
    (outpainting). We will look at these in detail in [*Chapter 9*](B22045_09.xhtml#_idTextAnchor171)*.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s run through a simple example of the Stable Diffusion model in Amazon
    Bedrock’s text playground by using this prompt: `a dog wearing sunglasses, riding
    a bike` `on mars`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – Image generation with the Stable Diffusion model](img/B22045_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – Image generation with the Stable Diffusion model
  prefs: []
  type: TYPE_NORMAL
- en: The ability to automatically create visual content has many applications across
    industries such as advertising, media and entertainment, and gaming. In [*Chapter
    9*](B22045_09.xhtml#_idTextAnchor171), we will explore how Stable Diffusion works
    under the hood. We will also discuss best practices and architecture patterns
    for leveraging image generation models in your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating and selecting the right FM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve understood the different types of FMs available in Amazon Bedrock,
    how do we determine which one is best suited for our specific project needs? This
    section will help you learn how to evaluate the model fit for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to clearly define the problem you’re trying to solve or the
    use case you want to build. Get as specific as possible about the inputs, outputs,
    tasks involved, and any other requirements. With a well-defined use case in hand,
    you can research which models have demonstrated capabilities relevant to your
    needs. Narrowing the options upfront based on capabilities will streamline the
    evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve identified some potential candidate models, the next step is to
    examine their performance across standardized benchmarks and use cases. Amazon
    Bedrock provides a capability to evaluate FMs, also called **model evaluation
    jobs**. With model evaluation jobs, users have the option to use either automatic
    model evaluation or evaluation through the human workforce. We will cover Amazon
    Bedrock’s model evaluation in more detail in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, several leaderboards and benchmarks exist today that can help
    with this evaluation, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Stanford Helm leaderboard for LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HuggingFace’s open leaderboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GLUE ([https://gluebenchmark.com/](https://gluebenchmark.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SuperGLUE ([https://super.gluebenchmark.com/](https://super.gluebenchmark.com/))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MMLU ([https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BIG-bench ([https://github.com/google/BIG-bench](https://github.com/google/BIG-bench))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing where each model ranks on tasks related to your use case provides
    an objective measure of its abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from benchmark performance, inspecting each model’s cost per query, processing
    latency, training parameters if fine-tuning is needed, and any other non-functional
    requirements need to be considered. The right model needs to not only achieve
    your technical objectives but also fit within your cost and timeline constraints.
  prefs: []
  type: TYPE_NORMAL
- en: No evaluation is complete without hands-on testing. Take advantage of Amazon
    Bedrock’s text playground or **Amazon Partyrock** to try out candidates on sample
    prompts, text generation tasks, or other example interactions representing your
    intended use case. More details regarding Amazon Bedrock’s text playground and
    Amazon Partyrock will be covered in the next chapter. This mechanism of model
    evaluation allows for a more qualitative assessment of things such as generated
    language quality, ability to maintain context, interpretability of responses,
    and the overall *feel* of interacting with each model.
  prefs: []
  type: TYPE_NORMAL
- en: By thoroughly researching capabilities, performance, and requirements, as well
    as testing multiple options, you’ll be well-equipped to select the right FM that
    provides the best overall fit and solution for your project needs. The right choice
    will help ensure your project’s success.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI capabilities of Amazon
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is primarily focused on Amazon Bedrock, but we wanted to highlight
    a few other generative AI capabilities offered by Amazon that are being used in
    enterprises for accelerating developer productivity, innovating faster, and solving
    their use cases with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon SageMaker** is Amazon’s fully managed ML platform for building, training,
    and deploying ML models at scale. One of the most powerful features of SageMaker
    is SageMaker Jumpstart, which provides a catalog of pre-trained open source FMs
    that are ready to be deployed and used.'
  prefs: []
  type: TYPE_NORMAL
- en: Some examples of FMs available in SageMaker Jumpstart include FLAN-T5 XL, a
    fine-tuned XL version of the T5 transformer model optimized for natural language
    understanding. Additional models, such as Meta Llama2, AI21 Jurassic-2 Ultra,
    and Stable Diffusion models, are also available in SageMaker Jumpstart.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to deploying these pre-trained FMs directly, SageMaker Jumpstart
    provides tools for customizing and fine-tuning select models for specific use
    cases. For instance, users can perform prompt engineering to better control model
    responses by adjusting text prompts. Some models also support reasoning augmentation
    to improve the common-sense reasoning ability of LLMs through question-answering
    tasks. Fine-tuning capabilities allow you to adapt the language models to domain-specific
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'This enables engineers and researchers to leverage the power of these generative
    AI models directly from Jumpstart so that they can build novel applications without
    requiring deep expertise in model training. The SageMaker platform handles all
    the heavy lifting of deploying, scaling, and managing ML models. When you open
    SageMaker Jumpstart within SageMaker Studio UI, you will see models offered by
    different model providers. This can be seen in *Figure 1**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – SageMaker Jumpstart](img/B22045_01_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – SageMaker Jumpstart
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose the model you would like to work with based on your use case
    and deploy it directly to a SageMaker endpoint, or you can fine-tune the model
    with a custom dataset. *Figure 1**.12* shows several open source models offered
    by HuggingFace, on SageMaker Jumpstart, exemplifying the simplicity in SageMaker
    to search for models of your choice suited to a particular task using the search
    bar or **Filters** options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – SageMaker Jumpstart HuggingFace models](img/B22045_01_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – SageMaker Jumpstart HuggingFace models
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Amazon Q** is a Generative AI-powered assistant that is built on top of Amazon
    Bedrock, and has been designed to enhance productivity and accelerate decision-making
    across various domains. It can assist users in a multitude of tasks, ranging from
    software development to data analysis and decision making.'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an overview of key offerings available with Amazon Q.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q for Business
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazon Q for Business** is an enterprise-grade, generative AI-powered assistant
    designed to streamline operations and enhance productivity within organizations.
    With this tool you can access and interact with the company repositories of data
    if you have required permissions, simplifying tasks and accelerating problem-solving
    processes. Here are some key features of Amazon Q for Business:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Comprehensive Data Integration**: Amazon Q for Business seamlessly connects
    to over 40 popular enterprise data sources, including Amazon S3, Microsoft 365,
    and Salesforce. It ensures secure access to content based on existing user permissions
    and credentials, leveraging single sign-on for a seamless experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent Query Handling**: You can ask questions in natural language,
    and Amazon Q for Business will search across all connected data sources, summarize
    relevant information logically, analyze trends, and engage in interactive dialogue.
    This empowers users to obtain accurate and comprehensive answers, eliminating
    the need for time-consuming manual data searches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizable and Secure**: Organizations can tailor Amazon Q for Business
    to their specific needs by configuring administrative guardrails, document enrichment,
    and relevance tuning. This ensures that responses align with company guidelines
    while maintaining robust security and access controls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Task Automation**: Amazon Q for Business allows users to streamline routine
    tasks, such as employee onboarding requests or expense reporting, through simple,
    natural language prompts. Additionally, users can create and share task automation
    applications, further enhancing efficiency and productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can set up Amazon Q for Business Application in a few clicks as shown in
    *Figure 1**.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – Setting up Amazon Q for Business](img/B22045_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – Setting up Amazon Q for Business
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on setting up Amazon Q for Business Application, you can check
    the link: [https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/getting-started.html](https://docs.aws.amazon.com/amazonq/latest/qbusiness-ug/getting-started.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14 – Customize web experience for Amazon Q for Business](img/B22045_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – Customize web experience for Amazon Q for Business
  prefs: []
  type: TYPE_NORMAL
- en: Once the application is set up, users can customize the web experience for the
    Q business application as shown in *Figure 1**.14*
  prefs: []
  type: TYPE_NORMAL
- en: Let us now look at another offering Amazon Q for QuickSight.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Q for QuickSight
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazon Q for QuickSight** is built for business users and analysts to unlock
    insights from their data more efficiently. It leverages the capabilities of Generative
    AI to streamline the process of data analysis and visualization. Here are some
    key features of Amazon Q for QuickSight :'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuitive Storytelling**: With Amazon Q for QuickSight, business users can
    create visually compelling narratives from their data by using simple, natural
    language prompts. These stories can include visuals, images, and text, making
    it easier to communicate insights and align stakeholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Executive Summaries**: Amazon Q for QuickSight can automatically generate
    executive summaries that highlight the most important trends and statistics from
    your dashboards. This feature saves time by providing a quick snapshot of key
    insights, eliminating the need to browse through multiple visuals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural Language Q&A**: Business users can confidently answer questions about
    their data using natural language queries. Amazon Q can understand vague or general
    questions, provide alternative perspectives, and offer context through narrative
    summaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accelerated Dashboard Building**: Analysts can significantly reduce the time
    required to build dashboards by describing the desired visualizations using natural
    language. Amazon Q can interpret these prompts and generate the corresponding
    visuals in seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Q for Developer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Amazon Q for Developer** streamlines the software development lifecycle on
    AWS. Here are some key features of Amazon Q for Developers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Intuitive Development Assistance**: Within IDEs, Amazon Q can provide real-time
    code suggestions, generate new code snippets, and offer guidance on software development
    best practices. This accelerates the coding process and enhances productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code Transformation**: Amazon Q can help you upgrade and modernize your legacy
    codebases by automatically transforming and optimizing your code to the latest
    language versions and frameworks. This capability ensures your applications remain
    up-to-date and secure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Troubleshooting and Maintenance**: Amazon Q can assist you in diagnosing
    and resolving errors, bugs, and issues within your AWS applications. It can also
    help you understand and manage your AWS resources more efficiently, minimizing
    the need to navigate through complex consoles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost Optimization**: By analyzing your AWS cost data, Amazon Q can provide
    valuable insights into your cloud spending patterns, helping you identify cost-saving
    opportunities and optimize your cloud infrastructure for better cost efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 1**.15* and *Figure 1**.16* illustrate an example of Amazon Q Developer
    for aiding in productivity gains for software engineers or developers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.15 – Amazon Q Developer](img/B22045_01_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – Amazon Q Developer
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.16 – Amazon Q Developer Lambda function](img/B22045_01_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – Amazon Q Developer Lambda function
  prefs: []
  type: TYPE_NORMAL
- en: With Amazon Q, developers can streamline their workflows, from planning and
    development to testing, deployment, and maintenance, ultimately enabling them
    to deliver high-quality applications faster and with greater confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI use cases with Amazon Bedrock
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the advent of generative AI, numerous organizations have benefited from
    the potential applications of this transformative technology in achieving their
    business objectives. Many of these organizations, which include Accenture, Adidas,
    Intuit, and Salesforce, have successfully developed prototypes and even have deployed
    production-ready generative AI systems using Amazon Bedrock. Across various industries,
    we have seen numerous compelling use cases for generative AI with Amazon Bedrock.
    Let’s learn more about some of these industries in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Finance**: In the financial services sector, organizations have been working
    on use cases such as classifying and categorizing huge corpus of legal documents,
    developing systems to select optimal funding and investment plans for customers,
    providing insights and simplified summaries and Q&As of complex financial documents,
    as well as detecting fraudulent activities such as forged signatures and tampered
    invoices. Additionally, organizations are utilizing Amazon Bedrock to understand
    market trends and customer behavior, aiding in informed decision-making processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: The healthcare industry has witnessed significant investment
    in developing generative AI applications with Amazon Bedrock. At the time of writing,
    AWS HealthScribe has been announced, which is powered by Amazon Bedrock ([https://aws.amazon.com/healthscribe/](https://aws.amazon.com/healthscribe/)).
    These applications address a wide range of use cases, such as automating medical
    claims and adjudication processes, extracting valuable insights from health documents
    and medical research papers, and generating summaries of patient-doctor interactions.
    By leveraging Amazon Bedrock, healthcare providers are aiming to enhance patient
    care and drive innovation in the field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Media and entertainment**: In the media and entertainment industry, organizations
    are actively exploring the diverse applications with Amazon Bedrock. These include
    generating narratives and storylines in sports and broadcasting, creating captions,
    images, and animations for storytelling, as well as providing personalized recommendations
    for TV shows, movies, and other forms of entertainment. By harnessing the capabilities
    of generative AI with Amazon Bedrock, media and entertainment companies aim to
    enhance the user experience, create engaging content, and stay ahead of the competition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are just a few examples of the numerous use cases that various industries
    are working on. In later chapters, we will understand architectural patterns in
    building industry-specific use cases through Amazon Bedrock.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we explored the various facets of the generative AI landscape:
    from understanding language models and the development of various NLP techniques
    to the invention of current SOTA transformer models. Then, we covered industrial
    challenges in building generative AI applications at scale and how Amazon Bedrock
    is seamlessly tackling those challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we explored various FMs offered by Amazon Bedrock and provided
    insights into how you can take advantage of various frameworks and tools to evaluate
    and select the right FM for your use case. We also looked at alternative generative
    AI capabilities offered by Amazon, including Amazon SageMaker and Amazon Q. We
    concluded this chapter by uncovering a few generative AI use cases with Amazon
    Bedrock in financial services, healthcare, and media and entertainment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover several techniques to access Amazon Bedrock
    and dive into various APIs via serverless services. Furthermore, we will learn
    about a hands-on approach toward invoking Bedrock FMs that can be integrated into
    enterprise-grade applications.
  prefs: []
  type: TYPE_NORMAL
