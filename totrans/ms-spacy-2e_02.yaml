- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Core Operations with spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about core operations with spaCy, such as creating
    a language pipeline, tokenizing the text, and breaking the text into its sentences.
  prefs: []
  type: TYPE_NORMAL
- en: First, you’ll learn what a language processing pipeline is and also explore
    the pipeline components. We’ll continue with general spaCy conventions – important
    classes and class organization – to help you to better understand spaCy and develop
    a solid understanding of the library itself.
  prefs: []
  type: TYPE_NORMAL
- en: You will then learn about the first pipeline component – **Tokenizer** . You’ll
    also learn about an important linguistic concept – **lemmatization** – along with
    its applications in **Natural Language Understanding** ( **NLU** ). Following
    that, we will cover **container** **classes** and **spaCy data structures** in
    detail. We will finish the chapter with useful spaCy features that you’ll use
    in everyday NLP development.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of spaCy conventions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spaCy container objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More spaCy token features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code of this chapter can be found at [https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition](https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Overview of spaCy conventions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Calling **nlp** on our text makes spaCy run a pipeline consisting of many processing
    steps. The first one is the tokenization to produce a **Doc** object. Then, depending
    on the spaCy components we choose to add to our pipeline, the text can be further
    processed by components such as a **tagger** , a **parser** , and an **entity
    recognizer** . We call this a **language processing pipeline** . Each pipeline
    is built using **components** . Each component returns the processed **Doc** and
    then passes it to the next component. This process is showcased in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – A high-level view of the processing pipeline](img/B22441_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – A high-level view of the processing pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'A spaCy pipeline object is created when we load a language model. In the following
    code segment, we load an English model and initialize a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import spaCy and use **spacy.load** to return a **Language** class
    instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can use this **Language** instance to get the **Doc** object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The **Language** class applies all the pipeline steps to the input sentence
    behind the scenes. After applying **nlp** to the sentence, the **Doc** object
    contains tokens that are tagged, lemmatized, and marked as entities if the **token**
    is an **entity** (we will go into detail about the entities in [*Chapter 5*](B22441_05.xhtml#_idTextAnchor074)
    , *Extracting Semantic Representations with* *spaCy Pipelines* ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each pipeline component has a well-defined task:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tokenizer (tokenizer)** : Segment text into tokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tagger (tagger)** : Assign part-of-speech tags'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DependencyParser (parser)** : Assign dependency labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EntityRecognizer (ner)** : Detect and label named entities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The spaCy language processing pipeline *always depends on the statistical model*
    that powers the component. Each component corresponds to a spaCy class. spaCy
    classes have self-explanatory names such as **Language** , **Doc** , and **Vocab**
    . You can see a list of all spaCy pipeline components here: [https://spacy.io/usage/spacy-101/#architecture-pipeline](https://spacy.io/usage/spacy-101/#architecture-pipeline)
    . There are also data structure classes to represent text and language data. We’re
    already familiar with the **Doc** class, but we also have these other container
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Doc** : A container object in spaCy that represents the entire processed
    text, holding the structure of the document and its tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token** : A single unit of text within a **Doc** object, such as a word,
    punctuation mark, or symbol.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Span** : A continuous slice of tokens within a **Doc** object, representing
    a portion of the text, such as a phrase or named entity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lexeme** : An object that stores lexical information about a word, such as
    its base form, spelling, and attributes, independent of its context in the text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, spaCy provides helper classes for vectors, language vocabulary, and
    annotations. We’ll see the **Vocab** class often in this book. **Vocab** represents
    a language’s vocabulary. The spaCy library’s backbone data structures are **Doc**
    and **Vocab** . The **Doc** object abstracts the text by owning the sequence of
    tokens and all their properties. The **Vocab** object provides a centralized set
    of strings and lexical attributes to all the other classes. This way, spaCy avoids
    storing multiple copies of linguistic data. *Figure 2* *.2* shows how all spaCy
    containers work together.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – spaCy architecture](img/B22441_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – spaCy architecture
  prefs: []
  type: TYPE_NORMAL
- en: spaCy does all the text processing operations for us behind the scenes, allowing
    us to concentrate on our own application’s development. Let’s start with the **Tokenizer**
    class.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in *Figure 2* *.1* that the first step in a text processing pipeline
    is tokenization. **Tokenization** is always the first operation because all the
    other operations require tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenization simply means splitting the sentence into its tokens. You can think
    of a token as the smallest meaningful part of a piece of text. Tokens can be words,
    numbers, punctuation, currency symbols, and any other meaningful symbols that
    are the building blocks of a sentence. The following are examples of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Input to the spaCy tokenizer is Unicode text and the result is a **Doc** object.
    The following code shows the tokenization process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the library and load the English language model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we apply the **nlp** object to a sentence to create a **Doc** object.
    The **Doc** object is the container for a sequence of **Token** objects. We then
    print the token texts:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 2* *.3* shows the tokens we’ve split along with their indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Tokenization of “I own a ginger cat.”](img/B22441_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Tokenization of “I own a ginger cat.”
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization can be tricky. There are many aspects we should pay attention to
    – punctuation, whitespaces, numbers, and so on. Splitting from the whitespaces
    with **text.split(" ")** might be tempting and looks like it works for the example
    sentence *I own a* *ginger cat* .
  prefs: []
  type: TYPE_NORMAL
- en: 'How about the sentence **"It''s been a crazy week!!!"** ? If we make a split(
    **" "** ), the resulting tokens would be **It''s** , **been** , **a** , **crazy**
    , **week!!!** , which is not what you want. First of all, **It''s** is not one
    token, it’s two tokens: **it** and **''s** . **week!!!** is not a valid token
    as the punctuation is not split correctly. Moreover, **!!!** should be tokenized
    per symbol and should generate three **!** ’s. This may not look like an important
    detail, but it is important for sentiment analysis. Let’s see what the spaCy tokenizer
    has generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 2* *.4* shows the tokens and their indexes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Tokenization of apostrophe and punctuation marks](img/B22441_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Tokenization of apostrophe and punctuation marks
  prefs: []
  type: TYPE_NORMAL
- en: 'How does spaCy know where to split the sentence? Unlike other parts of the
    pipeline, the tokenizer doesn’t need a statistical model. Tokenization is based
    on *language-specific rules* . You can see examples the language specific data
    here: [https://github.com/explosion/spaCy/tree/master/spacy/lang](https://github.com/explosion/spaCy/tree/master/spacy/lang)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenizer exceptions define rules for exceptions, such as **it''s** , **don''t**
    , **won''t** , abbreviations, and so on. If you look at the rules for English
    ( [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py)
    ), you will see that rules look like **{ORTH: "n''t", LEMMA: "not"}** , which
    describes the splitting rule for **n''t** to the tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prefixes, suffixes, and infixes mostly describe how to deal with punctuation
    – for example, we split at a period if it is at the end of the sentence, otherwise,
    most probably it’s part of an abbreviation such as **N.Y.** and we shouldn’t touch
    it. Here, **ORTH** means the text and **LEMMA** means the base word forms without
    any inflections. *Figure 2* *.5* shows you the execution of the spaCy tokenization
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – spaCy performing tokenization with exception rules, source:  https://spacy.io/usage/linguistic-features#tokenization](img/B22441_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5 – spaCy performing tokenization with exception rules, source: [https://spacy.io/usage/linguistic-features#tokenization](https://spacy.io/usage/linguistic-features#tokenization)'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization rules depend on the grammatical rules of the individual language.
    Punctuation rules such as splitting periods, commas, or exclamation marks are
    similar for many languages; however, some rules are specific to the individual
    language, such as abbreviation words and apostrophe usage. spaCy supports each
    language having its own specific rules by allowing hand-coded data and rules,
    as each language has its own subclass.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: spaCy provides non-destructive tokenization, which means that we always will
    be able to recover the original text from the tokens. Whitespace and punctuation
    information is preserved during tokenization, so the input text is preserved as
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: Every **Language** object contains a **Tokenizer** object. The **Tokenizer**
    class is the class that performs the tokenization. You don’t often call this class
    directly when you create a **Doc** class instance, while the **Tokenizer** class
    acts behind the scenes. When we want to customize the tokenization, we need to
    interact with this class. Let’s see how it is done.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we work with a specific domain such as medicine, insurance, or finance,
    we often come across words, abbreviations, and entities that need special attention.
    Most domains that you’ll process have characteristic words and phrases that need
    custom tokenization rules. Here’s how to add a special case rule to an existing
    **Tokenizer** class instance:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import spaCy and the **ORTH** symbol, which means orthography:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we instantiate the **Language** object as usual, process the **Doc** object,
    and print the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can define the special case where the word **lemme** should be
    tokenized as two tokens, **lem** and **me** :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When defining custom rules, the default punctuation splitting rules will still
    be applied. Even if the special case is surrounded by punctuation, it will still
    be recognized. The tokenizer will handle the punctuation step by step and apply
    the same process to the remaining substring, as in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You should modify the tokenizer by adding new rules only if you really need
    to. Trust me, you can get quite unexpected results with custom rules. Especially
    if you have social media text, first feed some sentences into the spaCy NLP pipeline
    and see how the tokenization works out. Let’s see how to debug the tokenizer component.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging the tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'spaCy has a tool for debugging: **nlp.tokenizer.explain(sentence)** . It returns
    tuples ( **tokenizer rule/pattern, token** ) to help us understand what happened
    exactly during the tokenization. Let’s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s process the text as usual:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can ask the **Tokenizer** class instance of the **Language** object
    for an explanation of the tokenization in this sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The **nlp.tokenizer.explain()** method explained the rules that the tokenizer
    used one by one. After splitting a sentence into its tokens, it’s time to split
    a text into its sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw that breaking a sentence into its tokens is not a straightforward task
    at all. How about breaking a text into sentences? It’s indeed a bit more complicated
    to mark where a sentence starts and ends due to the same reasons of punctuation,
    abbreviations, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **Doc** object’s sentences are available via the **doc.sents** property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Determining sentence boundaries is a more complicated task than tokenization.
    As a result, spaCy uses the **dependency parser** to perform sentence segmentation.
    This is a unique feature of spaCy – no other library puts such a sophisticated
    idea into practice. The results are very accurate in general, unless you process
    text of a very specific genre, such as from the conversation domain, or social
    media text.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to segment a text into sentences and tokenize the sentences,
    let’s start with lemmatization, a commonly used operation in semantics and sentiment
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **lemma** is the base form of a token. You can think of a lemma as the form
    in which the token appears in a dictionary. For instance, the lemma of *eating*
    is *eat* ; the lemma of *eats* is *eat* ; ate similarly maps to eat. **Lemmatization**
    is the process of reducing word forms to their lemmas. The following code is a
    quick example of how to do lemmatization with spaCy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: By now, you should be familiar with what the first three lines of the code do.
    In the **for** loop, we print each token, **text** and **lemma_** . Let’s see
    lemmatization in action with a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization in NLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lemmatization is an important step in NLU. We’ll go over an example in this
    subsection. Suppose that you design an NLP pipeline for a ticket booking system.
    Your application processes a customer’s sentence, extracts necessary information
    from it, and then passes it to the booking API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NLP pipeline wants to extract the form of travel (a flight, bus, or train),
    the destination city, and the date. The first thing the application needs to verify
    is the means of travel:'
  prefs: []
  type: TYPE_NORMAL
- en: Fly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Airway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Airplane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Railway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We have this list of keywords, and we want to recognize the means of travel
    by searching the tokens in the keywords list. The most compact way of doing this
    search is by looking up the token’s **lemma** . Consider the following customer
    sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, we don’t need to include all word forms of the verb *fly* ( *fly* , *flying*
    , *flies* , *flew* , and *flown* ) in the keywords list, and similar for the word
    **flight** ; we reduced all possible variants to the base forms – *fly* and *flight*
    . Don’t think of English only; languages such as Portuguese, German, and Finnish
    have many word forms from a single lemma as well.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization also comes in handy when we want to recognize the destination
    city. There are many nicknames available for global cities, such as *Angeltown*
    for *Los Angeles* . The default tokenizer and lemmatizer won’t know the difference
    between the official name and the nickname. The **AttributeRuler** component lets
    us set custom token attributes using **Matcher** patterns (we will learn more
    about **Matcher** in [*Chapter 4*](B22441_04.xhtml#_idTextAnchor056) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add a special rule to set the lemma of **Angeltown** as **Los Angeles**
    :'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **nlp.get_pipe()** method returns a pipeline component. We’ll get the **AttributeRuler**
    component and add the special rule:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s see if this worked by processing the **doc** and printing the lemmas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we’ve learned about the **tokenizer** and the **lemmatizer** components
    (usually the two components of a processing pipeline), let’s go ahead and learn
    more about spaCy **container objects** .
  prefs: []
  type: TYPE_NORMAL
- en: spaCy container objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we saw a list of container objects including
    **Doc** , **Token** , **Span** , and **Lexeme** . In this section, we’ll see the
    properties of container objects in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Using container objects, we can access the linguistic properties that spaCy
    assigns to text. A **container object** is a logical representation of text units
    such as a document, a token, or a slice of a document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Container objects in spaCy follow the natural structure of the text: a document
    is composed of sentences and sentences are composed of tokens. We most widely
    use **Doc** , **Token** , and **Span** objects in development, which represent
    a document, a single token, and a phrase, respectively. A container can contain
    other containers – for instance, a document contains tokens and spans.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore each class and its useful properties one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Doc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We created **Doc** objects in our code to represent the text, so you might
    have already figured out that **Doc** represents a text. Here, **doc.text** returns
    a Unicode representation of the document text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The building block of a **Doc** object is **Token** , hence when you iterate
    a **Doc** object, you get token objects as items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The same logic applies to indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The length of a **Doc** object is the number of tokens it includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'As for the text’s sentences, **doc.sents** returns an iterator to the list
    of sentences. Each sentence is a **Span** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The named entities in the text are provided by **doc.ents** . The result is
    a list of **Span** objects. We’ll look at named entities in detail later in the
    book – for now, think of them as proper nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Another syntactic property is **doc.noun_chunks** . It yields the noun phrases
    found in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The property **doc.lang_** returns the language of the doc created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'A useful method for serialization is **doc.to_json** . This is how to convert
    a **Doc** object to **JSON** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have noticed that we call **doc.lang_** , not **doc.lang** . The
    **doc.lang** call returns the language ID, whereas **doc.lang_** returns the Unicode
    string of the language, that is, the name of the language. You can see the same
    convention with token features in the following: **token.lemma_** , **token.tag_**
    , and **token.pos_** .'
  prefs: []
  type: TYPE_NORMAL
- en: The **Doc** object has very useful properties with which you can understand
    a sentence’s syntactic properties and use them in your own applications. Let’s
    move on to the **Token object** and see what it offers.
  prefs: []
  type: TYPE_NORMAL
- en: Token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **token** object represents a word. Token objects are the building blocks
    of **Doc** and **Span** objects. In this section, we will cover the following
    properties of the **Token** class:'
  prefs: []
  type: TYPE_NORMAL
- en: '**token.text**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.text_with_ws**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.i**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.idx**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.doc**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.sent**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.is_sent_start**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**token.ent_type**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We usually don’t construct an object of the **Token** class directly, rather
    we construct a **Doc** object and then access its tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The **token.text** property is similar to **doc.text** and provides the underlying
    Unicode string. **token.text_with_ws** is a similar property. It provides the
    text with a trailing whitespace if it’s present in the **doc** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Finding the length of a token is similar to finding the length of a Python
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The **token.i** property gives the index of the token in the **doc** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The token’s character offset (the character position) in the **doc** object
    is provided by the **token.idx** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Another cool property of tokens is that we can also access the **doc** object
    that created the token as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This also works for getting the sentence that the token belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful property is **token.is_sent_start** ; it returns a Boolean indicating
    whether the token starts a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the basic properties of the **Token** object that you’ll use every
    day. There is another set of properties that are more related to syntax and semantics.
    We already saw how to calculate the token lemma in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You already learned that **doc.ents** property gives the named entities of
    the document. If you want to learn what sort of entity the token is, use **token.ent_type_**
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Two syntactic features related to POS tagging are **token.pos_** and **token.tag**
    . We’ll learn what they are and how to use them in the next chapter. The **Token**
    object has a rich set of features, enabling us to process the text from head to
    toe. Let’s move on to the **Span** object and see what it offers us.
  prefs: []
  type: TYPE_NORMAL
- en: Span
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Span** objects represent phrases or segments of the text. A **Span** object
    must be a contiguous sequence of tokens. We usually don’t initialize **Span**
    objects, rather we slice a **Doc** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Trying to slice an invalid index will raise an **IndexError** . Most indexing
    and slicing rules of Python strings are applicable to **Doc** slicing as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one more way to create a **Span** object – we can make a character-level
    slice of a **Doc** object with **char_span([start_idx, end_idx])** :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The building blocks of a **Span** object are **Token** objects. If you iterate
    over a **Span** object, you get **Token** objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You can think of the **Span** object as a junior **Doc** object. Hence, most
    of the features of **Doc** are applicable to **Span** as well. For instance, **len**
    is identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The **Span** object also supports indexing. The result of slicing a **Span**
    object is another **Span** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like a **Token** knows the **Doc** object it’s created from, **Span**
    also knows the **Doc** object it’s created from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also locate the **Span** object in the original **Doc** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want a brand-new **Doc** object, you can call **span.as_doc()** . It
    copies the data into a new **Doc** object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The properties **span.ents** , **span.sent** , **span.text** , and **span.text_with_ws**
    are similar to their corresponding **Doc** and **Token** methods. We’ll now go
    through a few more features and methods for more detailed text analysis in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: More spaCy Token features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most NLP development is token and span oriented; that is, it processes tags,
    dependency relations, tokens themselves, and phrases. We apply transformations
    like eliminating small words and words without much meaning, processing URLs taking
    into account the protocol and subdomain parts, and so on. These actions sometimes
    depend on the *token shape* (for example, if the token is a short word or if the
    token looks like a URL string) or more semantical features (such as the token
    is an article, or the token is a conjunction). In this section, we will see these
    features of tokens with some examples. We’ll start with features related to the
    token shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The **token.lower_** feature turns the token in lowercase. The return value
    is a Unicode string and this feature is equivalent to **token.text.lower()** .
  prefs: []
  type: TYPE_NORMAL
- en: The features **is_lower** and **is_upper** are similar to their Python string
    method counterparts, **islower()** and **isupper()** . The **is_lower** feature
    returns **True** if all the characters are lowercase, while **is_upper** does
    the same with uppercase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature **is_alpha** returns **True** if all the characters of the token
    are alphabetic letters. Examples of nonalphabetic characters are numbers, punctuation,
    and whitespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The **is_ascii** feature returns **True** if all the characters of the token
    are ASCII characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The **is_digit** feature returns **True** if all the characters of the token
    are numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The feature **is_punct** returns **True** if the token is a punctuation mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The features **is_left_punct** and **is_right_punct** return **True** if the
    token is a left punctuation mark or right punctuation mark, respectively. A right
    punctuation mark can be any mark that closes a left punctuation mark, such as
    right brackets, **>** or **»** . Left punctuation marks are similar, with the
    left brackets **<** and **«** as some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The methods **like_url** , **like_num** , and **like_email** capture information
    about the token shape and return **True** if the token looks like a URL, a number,
    or an email, respectively. These methods are very handy when we want to process
    social media text and scraped web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '**token.shape_** is an unusual feature – there is nothing similar in other
    NLP libraries. It returns a string that shows a token’s orthographic features.
    Numbers are replaced with **d** , uppercase letters are replaced with **X** ,
    and lowercase letters are replaced with **x** . You can use the result string
    as a feature in your machine learning algorithms, and token shapes can be correlated
    to text sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The feature **is_stop** is frequently used by machine learning algorithms.
    Often, we filter words that do not carry much meaning, such as *the* , *a* , *an*
    , *and* , *just* , *with* , and so on. Such words are called stop words. Each
    language has its own stop word list, and you can access English stop words at
    [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We’re finally done with some of spaCy’s syntactic, semantic, and orthographic
    features. Many of those methods focused on the **Token** object as a token is
    the syntactic unit of a text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter gave you a comprehensive picture of spaCy library classes and
    methods. We took a deep dive into language processing pipelines and learned about
    pipeline components. We also covered a basic yet important syntactic task: tokenization.
    We continued with the linguistic concept of lemmatization and you learned about
    a real-world application of this spaCy feature. We explored spaCy container classes
    in detail and finalized the chapter by looking at precise and useful spaCy features.
    In the next chapter, we will dive into spaCy’s full linguistic power. You’ll discover
    linguistic features including spaCy’s most used features: the POS tagger, dependency
    parser, and named entities.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Advanced Linguistic and Semantic Analysis'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the fundamentals, this section explores advanced techniques for
    analyzing and extracting information from text using spaCy’s linguistic and rule-based
    tools. You'll learn how to perform sophisticated parsing and matching and even
    develop your own custom components to handle complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B22441_03.xhtml#_idTextAnchor045) , *Extracting Linguistic Features*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B22441_04.xhtml#_idTextAnchor056) , *Mastering Rule-Based Matching*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B22441_05.xhtml#_idTextAnchor074) , *Extracting Semantic Representations
    with spaCy Pipelines*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B22441_06.xhtml#_idTextAnchor087) , *Utilizing spaCy with Transformers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
