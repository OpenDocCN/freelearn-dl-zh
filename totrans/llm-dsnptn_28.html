<html><head></head><body><div><div><div><h1 id="_idParaDest-316" class="chapter-number"><a id="_idTextAnchor389"/>28</h1>
			<h1 id="_idParaDest-317"><a id="_idTextAnchor390"/>Advanced RAG</h1>
			<p>In <a href="B31249_26.xhtml#_idTextAnchor366"><em class="italic">Chapter 26</em></a>, we covered the basics of the RAG pattern, a simple process where a user’s query triggers a search in an external knowledge base. The information that’s retrieved is then directly appended to the query, and this augmented prompt is passed to the LLM to generate a response, allowing it to access external data without complex processing.</p>
			<p>Now, in this chapter, we’ll move beyond these basic RAG methods and explore more sophisticated techniques designed to significantly enhance LLM performance across a wide range of tasks.</p>
			<p>By the end of this chapter, you’ll be equipped with the knowledge to implement these advanced RAG strategies, enabling your LLM applications to achieve greater accuracy and efficiency.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Multi-step and iterative retrieval techniques for LLMs</li>
				<li>Adaptive retrieval based on context and task in LLMs</li>
				<li>Meta-learning for improved retrieval in LLMs</li>
				<li>Combining RAG with other LLM prompting techniques</li>
				<li>Handling ambiguity and uncertainty in LLM-based RAG</li>
				<li>Scaling RAG to very large knowledge bases</li>
				<li>Future directions in RAG research for LLMs</li>
			</ul>
			<h1 id="_idParaDest-318"><a id="_idTextAnchor391"/>Multi-step and iterative retrieval techniques for LLMs</h1>
			<p>Using multi-step and iterative<a id="_idIndexMarker1276"/> retrieval techniques for LLMs is a dynamic, recursive approach to information gathering where the model progressively refines its search strategy. The code provided in this section illustrates a multi-step RAG framework that expands context iteratively, retrieves additional documents, and generates responses through multiple steps, allowing for increasingly <a id="_idIndexMarker1277"/>comprehensive and nuanced information retrieval by dynamically adjusting queries and integrating retrieved knowledge.</p>
			<p>Here are some of its key characteristics:</p>
			<ul>
				<li>Iterative context expansion</li>
				<li>Multiple retrieval steps (configurable up to <code>max_steps</code>)</li>
				<li>Dynamic query refinement</li>
				<li>Contextual document retrieval</li>
				<li>Adaptive response generation</li>
			</ul>
			<p>Multi-step and iterative retrieval techniques for LLMs, with their dynamic and recursive approaches, benefit use cases that require the <a id="_idIndexMarker1278"/>following aspects:</p>
			<ul>
				<li><strong class="bold">Complex question-answering</strong>: When questions require information to be synthesized from multiple sources or involve intricate logical reasoning, iterative retrieval allows the LLM to gather the necessary context progressively. Examples include legal document analysis, scientific research, and in-depth financial analysis.</li>
				<li><strong class="bold">Knowledge-intensive conversations</strong>: In conversational AI scenarios where the dialogue involves exploring a topic in depth, iterative RAG enables the LLM to maintain context and refine its understanding over multiple turns. This is valuable for educational chatbots, technical support, and interactive tutorials.</li>
				<li><strong class="bold">Research and exploration</strong>: For tasks such as literature reviews, market research, or investigative journalism, the ability to dynamically refine queries and explore related information is crucial. Iterative retrieval allows the LLM to act as a research assistant, uncovering connections and insights that would be difficult to find with a single query.</li>
				<li><strong class="bold">Technical documentation and troubleshooting</strong>: When dealing with complex technical issues, iterative RAG can help the LLM navigate extensive documentation, progressively narrowing down the search to pinpoint relevant information. This improves the efficiency of troubleshooting and technical support.</li>
				<li><strong class="bold">Dynamic information gathering</strong>: This includes any situation where the information that is needed isn’t able to be gathered in a single pass. For example, if a user wants to find out all the news articles related to a specific court case and then wants to <a id="_idIndexMarker1279"/>know what people are saying about those news articles on social media, multiple steps of information gathering would be required.</li>
				<li><strong class="bold">Dealing with ambiguous queries</strong>: When a user’s query is ambiguous, the LLM can ask clarifying questions and then use the user’s response to refine the search.</li>
			</ul>
			<p>In essence, any use case that demands a deep, nuanced understanding of information, and where a single retrieval step is insufficient, stands to gain significantly from multi-step and iterative RAG.</p>
			<p>Let’s take a look at the following code example:</p>
			<pre class="source-code">
from typing import List, Dict
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
class MultiStepRAG:
    def __init__(self, retriever, generator, max_steps=3):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.max_steps = max_steps
    def retrieve_and_generate(self, query: str) -&gt; str:
        context = ""
        for step in range(self.max_steps):
            retrieved_docs = self.retriever.retrieve(
                query + " " + context, k=3
            )
            context += " ".join(retrieved_docs) + " "
            prompt = f"Context: {context}\nQuery: {query}\nResponse:"
            inputs = self.tokenizer(
                prompt, return_tensors="pt"
            )
            outputs = self.generator.generate(inputs, max_length=200)
            response = self.tokenizer.decode(
                outputs[0], skip_special_tokens=True
            )
            if self.is_response_complete(response):
                break
            query = self.generate_follow_up_query(query, response)
        return response
    def is_response_complete(self, response: str) -&gt; bool:
        # Implement logic to determine if the response is complete
        return "I don't have enough information" not in response
    def generate_follow_up_query(
        self, original_query: str, current_response: str
    ) -&gt; str:
        prompt = f"Original question: {original_query}\nCurrent answer: {current_response}\nGenerate a follow-up question to gather more information:"
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=50)
        return self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
# Example usage
retriever = SomeRetrieverClass()  # Replace with your actual retriever
generator = AutoModelForCausalLM.from_pretrained("gpt2-medium")
multi_step_rag = MultiStepRAG(retriever, generator)
response = multi_step_rag.retrieve_and_generate("What are the effects of climate change on biodiversity?")
print(response)</pre>			<p>In this pseudocode example, the <code>MultiStepRAG</code> class implements multistep retrieval through three critical methods:</p>
			<ul>
				<li><code>retrieve_and_generate()</code>: This method iteratively expands context by retrieving documents, generating responses, and dynamically updating the search context across multiple <a id="_idIndexMarker1280"/>steps. It manages the retrieval process, limiting iterations to a configurable maximum.</li>
				<li><code>is_response_complete()</code>: This method evaluates response quality by detecting whether the generated answer addresses the query sufficiently, typically checking for indicators of incomplete information.</li>
				<li><code>generate_follow_up_query()</code>: This method creates refined follow-up queries by using the language model to generate new questions based on the original query and current response, enabling intelligent context exploration.</li>
			</ul>
			<p>This implementation allows for progressive information gathering, where each retrieval step dynamically refines the context and generates more comprehensive responses by recursively expanding the knowledge base.</p>
			<h1 id="_idParaDest-319"><a id="_idTextAnchor392"/>Adaptive retrieval based on context and task in LLMs</h1>
			<p>Adaptive retrieval is a sophisticated approach to<a id="_idIndexMarker1281"/> information retrieval that dynamically adjusts strategies based on specific task requirements.</p>
			<p>The following code demonstrates this concept through an implementation that tailors retrieval and generation processes across different task types:</p>
			<pre class="source-code">
from enum import Enum
class TaskType(Enum):
    FACTUAL_QA = 1
    SUMMARIZATION = 2
    ANALYSIS = 3
class AdaptiveRAG:
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
    def retrieve_and_generate(self, query: str, task_type: TaskType
    ) -&gt; str:
        if task_type == TaskType.FACTUAL_QA:
            k = 3
            prompt_template = "Context: {context}\nQuestion: {query}\nAnswer:"
        elif task_type == TaskType.SUMMARIZATION:
            k = 10
            prompt_template = "Summarize the following information:\n{context}\nSummary:"
        elif task_type == TaskType.ANALYSIS:
            k = 5
            prompt_template = "Analyze the following information:\n{context}\nQuery: {query}\nAnalysis:"
        retrieved_docs = self.retriever.retrieve(query, k=k)
        context = " ".join(retrieved_docs)
        prompt = prompt_template.format(context=context, query=query)
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=300)
        response = self.tokenizer.decode(
            outputs[0], skip_special_tokens=True
        )
        return response
# Example usage
adaptive_rag = AdaptiveRAG(retriever, generator)
factual_response = adaptive_rag.retrieve_and_generate(
    "What is the capital of France?",
    TaskType.FACTUAL_QA
)
summary_response = adaptive_rag.retrieve_and_generate(
    "Summarize the causes of World War I",
    TaskType.SUMMARIZATION
)
analysis_response = adaptive_rag.retrieve_and_generate(
    "Analyze the impact of social media on mental health",
    TaskType.ANALYSIS
)</pre>			<p>The preceding code introduces an <code>AdaptiveRAG</code> class that uses an <code>Enum</code> value called <code>TaskType</code> to define distinct retrieval strategies for different scenarios: factual question-answering, summarization, and analysis. Each task type receives customized treatment in terms of document retrieval volume and prompt formatting.</p>
			<p>In the <code>retrieve_and_generate()</code> method, the system dynamically configures retrieval parameters:</p>
			<ul>
				<li><code>Factual QA</code>: This retrieves three documents with a direct question-answer format</li>
				<li><code>Summarization</code>: This retrieves ten documents with a summary-focused template</li>
				<li><code>Analysis</code>: This retrieves five documents with an analytical prompt structure</li>
			</ul>
			<p>The method retrieves relevant<a id="_idIndexMarker1282"/> documents, constructs a context, generates a task-specific prompt, and produces a response tailored to the specific task type. This approach allows for more nuanced and contextually appropriate information retrieval and generation across different knowledge exploration scenarios.</p>
			<p>This example usage demonstrates flexibility by generating responses for factual queries, summaries, and analytical tasks using the same adaptive framework.</p>
			<h1 id="_idParaDest-320"><a id="_idTextAnchor393"/>Meta-learning for improved retrieval in LLMs</h1>
			<p>Meta-learning in retrieval systems is a dynamic approach where the model learns to improve its retrieval strategy by analyzing past performance and relevance feedback. In this implementation, meta-learning<a id="_idIndexMarker1283"/> focuses on selecting and ranking documents adaptively based on learned relevance patterns.</p>
			<p>Let’s implement a simple meta-learning approach for RAG.</p>
			<p>The following code demonstrates meta-learning by retrieving documents about dark matter theories and simulating relevance feedback to train the model, showcasing how the system can improve its information retrieval capabilities iteratively:</p>
			<pre class="source-code">
import numpy as np
from sklearn.linear_model import LogisticRegression
class MetaLearningRAG:
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.meta_model = LogisticRegression()
        self.training_data = []
    def retrieve_and_generate(self, query: str) -&gt; str:
        retrieved_docs = self.retriever.retrieve(query, k=10)
        if self.meta_model.coef_.size &gt; 0:  # If the meta-model has been trained
            relevance_scores = self.predict_relevance(
                query, retrieved_docs)
            top_docs = [
                doc for _, doc in sorted(
                    zip(relevance_scores, retrieved_docs),
                    reverse=True
                )
            ][:3]
        else:
            top_docs = retrieved_docs[:3]
        context = " ".join(top_docs)
        prompt = f"Context: {context}\nQuery: {query}\nResponse:"
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=200)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
    def predict_relevance(self, query: str, docs: List[str]
    ) -&gt; np.ndarray:
        features = self.compute_features(query, docs)
        return self.meta_model.predict_proba(features)[:, 1]  # Probability of relevance
    def compute_features(self, query: str, docs: List[str]
    ) -&gt; np.ndarray:
        # Compute features for the query-document pairs
        # This is a placeholder implementation
        return np.random.rand(len(docs), 5)  # 5 random features
    def update_meta_model(
        self, query: str, retrieved_docs: List[str],
        relevance_feedback: List[int]
    ):
        features = self.compute_features(query, retrieved_docs)
        self.training_data.extend(zip(features, relevance_feedback))
        if len(self.training_data) &gt;= 100:  # Train the meta-model periodically
            X, y = zip(*self.training_data)
            self.meta_model.fit(X, y)
            self.training_data = []  # Clear the training data after updating the model
# Example usage
meta_learning_rag = MetaLearningRAG(retriever, generator)
response = meta_learning_rag.retrieve_and_generate(
    "What are the main theories of dark matter?"
)
print(response)
# Simulating relevance feedback
retrieved_docs = meta_learning_rag.retriever.retrieve(
    "What are the main theories of dark matter?",
    k=10
)
relevance_feedback = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1]  # 1 for relevant, 0 for not relevant
meta_learning_rag.update_meta_model(
    "What are the main theories of dark matter?",
    retrieved_docs, relevance_feedback
)</pre>			<p>The key meta-learning components in the <a id="_idIndexMarker1284"/>preceding code include the following:</p>
			<ul>
				<li><code>predict_relevance()</code> method estimates the probability of document usefulness</li><li>Dynamically adjusts document selection based on learned features</li></ul></li>
				<li><code>compute_features()</code> method generates document representation features</li><li>Currently, it uses randomly generated values as placeholder features for demonstration or testing purposes</li><li>In practice, it would include semantic similarity, keyword matching, and more.</li></ul></li>
				<li><strong class="bold">Adaptive </strong><strong class="bold">learning mechanism</strong>:<ul><li>Accumulates training data from relevance feedback</li><li>Retrains the meta-model when sufficient data is collected (100 samples)</li><li>Clears the training data after model updates to prevent overfitting</li></ul></li>
				<li><strong class="bold">Retrieval </strong><strong class="bold">strategy modification</strong>:<ul><li>Initially uses the top 10 retrieved documents</li><li>After meta-model training, it selects the top three documents based on learned relevance scores</li><li>Continuously refines the document selection process</li></ul></li>
			</ul>
			<p>The code implements a <code>MetaLearningRAG</code> class that dynamically enhances retrieval performance using machine learning techniques. The core innovation lies in its ability to learn from relevance feedback and adjust document selection strategies.</p>
			<p>Let’s look at the key methods:</p>
			<ul>
				<li><code>retrieve_and_generate()</code>: Selects the top documents using a trained meta-model</li>
				<li><code>predict_relevance()</code>: Estimates <a id="_idIndexMarker1285"/>document relevance probabilities</li>
				<li><code>compute_features()</code>: Generates feature representations for documents</li>
				<li><code>update_meta_model()</code>: Periodically retrains the model based on relevance feedback</li>
			</ul>
			<p>The implementation uses logistic regression to predict document relevance, progressively refining retrieval by learning from user interactions. When sufficient training data has been accumulated, the meta-model is retrained, allowing the system to adapt its document selection strategy based on historical performance and feedback.</p>
			<p>In the context of meta-learning for retrieval systems, <em class="italic">relevance</em> refers to the contextual usefulness and information value of the documents that were retrieved for a specific query.</p>
			<p>Let’s look at the key <em class="italic">relevance</em> aspects shown in the preceding code:</p>
			<ul>
				<li><strong class="bold">Relevance scoring</strong>:<ul><li>Predicts the probability of the document being useful</li><li>Uses machine learning to learn relevance patterns</li><li>Allows dynamic document ranking</li></ul></li>
				<li><code>1</code> = relevant, <code>0</code> = not relevant)</li><li>Enables the system to learn from user-provided quality signals</li><li>Improves future document selection</li></ul></li>
				<li><strong class="bold">Feature-based relevance</strong>:<ul><li>Computes document features representing potential usefulness</li><li>The preceding code uses random features</li><li>Captures semantic and contextual relationships</li></ul></li>
			</ul>
			<p>The core goal is to<a id="_idIndexMarker1286"/> create an adaptive retrieval system that learns to select increasingly precise and valuable documents through iterative feedback and machine learning techniques.</p>
			<h1 id="_idParaDest-321"><a id="_idTextAnchor394"/>Combining RAG with other LLM prompting techniques</h1>
			<p>We can enhance RAG by combining it with <a id="_idIndexMarker1287"/>other prompting techniques, such as CoT (see <a href="B31249_20.xhtml#_idTextAnchor305"><em class="italic">Chapter 20</em></a>) or few-shot learning. Here’s an<a id="_idIndexMarker1288"/> example that combines RAG with CoT:</p>
			<pre class="source-code">
class RAGWithCoT:
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
    def retrieve_and_generate(self, query: str) -&gt; str:
        retrieved_docs = self.retriever.retrieve(query, k=3)
        context = " ".join(retrieved_docs)
        cot_prompt = f"""Context: {context}
Question: {query}
Let's approach this step-by-step:
1) First, we should consider...
2) Next, we need to analyze...
3) Then, we can conclude...
Based on this reasoning, the final answer is:
Answer:"""
        inputs = self.tokenizer(cot_prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=500)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
# Example usage
rag_with_cot = RAGWithCoT(retriever, generator)
response = rag_with_cot.retrieve_and_generate("What are the potential long-term effects of artificial intelligence on employment?")
print(response)</pre>			<p>The <code>RAGWithCoT</code> class implements a RAG approach<a id="_idIndexMarker1289"/> enhanced with CoT reasoning. By retrieving relevant documents and constructing a prompt that encourages step-by-step problem solving, the method transforms standard query response generation into a more structured, analytical process.</p>
			<p>The implementation guides the language model through an explicit reasoning framework, breaking complex queries into logical steps. This approach prompts the model to demonstrate intermediate reasoning, creating a more transparent and potentially more accurate response generation process.</p>
			<p>The method combines contextual document retrieval with a carefully designed prompt template that explicitly structures <a id="_idIndexMarker1290"/>the model’s reasoning. By requiring the model to outline its thinking process before presenting a final answer, the implementation seeks to improve the depth and quality of generated responses.</p>
			<p>As we explore advanced RAG techniques, the next critical challenge emerges: handling ambiguity and uncertainty in language-model-based information retrieval. The following section will delve into sophisticated strategies for managing complex, nuanced, and potentially conflicting information sources, highlighting approaches that enable more robust and reliable knowledge extraction and generation.</p>
			<h1 id="_idParaDest-322"><a id="_idTextAnchor395"/>Handling ambiguity and uncertainty in LLM-based RAG</h1>
			<p>Ambiguity and uncertainty directly<a id="_idIndexMarker1291"/> compromise the accuracy and reliability of generated responses. Ambiguous queries, for instance, can trigger the process of retrieving irrelevant or conflicting information, leading the LLM to produce incoherent or incorrect outputs. Consider the query, “What about apples?” This could refer to Apple Inc., the fruit, or specific apple varieties. A naive RAG system might pull data from all contexts, resulting in a confused response.</p>
			<p>Furthermore, uncertainty in retrieved information – due to conflicting or outdated data in the knowledge base – exacerbates the problem. Without mechanisms to assess data reliability, the LLM may propagate inaccuracies. LLMs themselves operate on probabilities, adding another layer of uncertainty. For example, when addressing a niche topic, an LLM might generate a “best guess” that, without proper uncertainty estimation, could be presented as fact. Combining multiple pieces of uncertain information further compounds this issue, potentially leading to misleading and unreliable responses, ultimately undermining user trust and limiting the practical applications of RAG systems.</p>
			<p>To handle ambiguity and <a id="_idIndexMarker1292"/>uncertainty, we can implement a system that generates multiple hypotheses and ranks them based on confidence:</p>
			<pre class="source-code">
class UncertaintyAwareRAG:
    def __init__(self, retriever, generator, n_hypotheses=3):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.n_hypotheses = n_hypotheses
    def retrieve_and_generate(self, query: str) -&gt; Dict[str, float]:
        retrieved_docs = self.retriever.retrieve(query, k=5)
        context = " ".join(retrieved_docs)
        prompt = (
            f"Context: {context}\n"
            "Question: {query}\n"
            f"Generate {self.n_hypotheses} possible answers "
            f"with confidence scores:\n"
        )
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(
            inputs, max_length=500,
            num_return_sequences=self.n_hypotheses
        )
        hypotheses = []
        for output in outputs:
            hypothesis = self.tokenizer.decode(
                output, skip_special_tokens=True
            )
            hypotheses.append(self.parse_hypothesis(hypothesis))
        return dict(
            sorted(
                hypotheses, key=lambda x: x[1], reverse=True
            )
    )
    def parse_hypothesis(self, hypothesis: str) -&gt; Tuple[str, float]:
        # This is a simple parser, assuming the format "Answer (Confidence: X%): ..."
        parts = hypothesis.split(":")
        confidence = float(
            parts[0].split("(Confidence: ")[1].strip("%)"))/100
        answer = ":".join(parts[1:]).strip()
        return (answer, confidence)
# Example usage
uncertainty_aware_rag = UncertaintyAwareRAG(retriever, generator)
hypotheses = uncertainty_aware_rag.retrieve_and_generate(
    "What will be the dominant form of energy in 2050?"
)
for answer, confidence in hypotheses.items():
    print(f"Hypothesis (Confidence: {confidence:.2f}): {answer}")</pre>			<p>The preceding code implements an <code>UncertaintyAwareRAG</code> class that intelligently handles ambiguous queries by generating multiple possible answers with confidence scores. It works by initializing with a<a id="_idIndexMarker1293"/> retriever component (for fetching relevant documents), a generator (language model), and a parameter for the number of hypotheses to generate. When <code>retrieve_and_generate</code> is called with a query, it retrieves relevant documents and combines them into a context, then constructs a specialized prompt asking for multiple possible answers with confidence scores. The generator produces multiple hypotheses using the <code>num_return_sequences</code> parameter, each including a confidence score. These hypotheses are parsed using the <code>parse_hypothesis</code> method, which extracts both the answer text and its confidence score from a standardized format of <code>"Answer (Confidence: X%): ..."</code>. The results are then sorted by confidence score and returned as a dictionary that maps answers to their confidence values. This approach is particularly valuable for questions that may not have a single definitive answer (such as future predictions or complex scenarios) as it explicitly acknowledges uncertainty and provides multiple plausible responses with their associated confidence levels, allowing users to make more informed decisions based on the range of possibilities and their relative likelihoods.</p>
			<p>After implementing uncertainty handling in our RAG system, the next crucial challenge is dealing with massive document collections. As knowledge bases grow to millions or even billions of documents, traditional retrieval methods become impractical, requiring more sophisticated approaches. Let’s explore how we can scale RAG so that it can handle very large knowledge bases efficiently through hierarchical indexing.</p>
			<h1 id="_idParaDest-323"><a id="_idTextAnchor396"/>Scaling RAG to very large knowledge bases</h1>
			<p>We can scale RAG using a <a id="_idIndexMarker1294"/>hierarchical system. A hierarchical RAG system is an advanced architecture that organizes document retrieval in a tree-like structure with multiple levels. Instead of searching through all documents linearly, it first clusters similar documents together and creates a hierarchy of these clusters. When a query comes in, the system identifies the most relevant cluster(s) at the top level, drills down to find the most relevant sub-clusters, and finally retrieves the most similar documents from within those targeted sub-clusters. Think of it like a library where books are first organized by broad categories (science, history, fiction), then by sub-categories (physics, biology, chemistry), and finally by specific topics – this makes finding a particular book much faster than searching through every single book.</p>
			<p>The hierarchical approach to RAG offers significant advantages because it dramatically improves both the efficiency and scalability of document retrieval while maintaining high accuracy. By organizing documents into clusters and sub-clusters, the system can quickly narrow down the search space from potentially millions of documents to a much smaller, relevant subset, which not only speeds up retrieval but also reduces computational resources and memory requirements. This makes it possible to handle massive document collections<a id="_idIndexMarker1295"/> that would be impractical with traditional flat retrieval approaches. The hierarchical structure also enables better parallelization of search operations and can even improve result quality by considering document relationships within the hierarchy.</p>
			<p>The following code snippet defines a class for hierarchical RAG, leveraging Facebook’s AI Similarity Search (Faiss) library for efficient similarity search and generation capabilities:</p>
			<pre class="source-code">
import faiss
class HierarchicalRAG:
    def __init__(
        self, generator, embeddings, texts, n_clusters=1000
    ):
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.embeddings = embeddings
        self.texts = texts
        # Create a hierarchical index
        self.quantizer = faiss.IndexFlatL2(embeddings.shape[1])
        self.index = faiss.IndexIVFFlat(
            self.quantizer, embeddings.shape[1], n_clusters
        )
        self.index.train(embeddings)
        self.index.add(embeddings)
    def retrieve(self, query: str, k: int = 5) -&gt; List[str]:
        query_embedding = self.compute_embedding(query)
        _, indices = self.index.search(
            query_embedding.reshape(1, -1), k
        )
        return [self.texts[i] for i in indices[0]]
    def compute_embedding(self, text: str) -&gt; np.ndarray:
        # Compute embedding for the given text
        # This is a placeholder implementation
        return np.random.rand(1, self.embeddings.shape[1])
    def retrieve_and_generate(self, query: str) -&gt; str:
        retrieved_docs = self.retrieve(query)
        context = " ".join(retrieved_docs)
        prompt = f"Context: {context}\nQuery: {query}\nResponse:"
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=200)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
# Example usage
embeddings = np.random.rand(1000000, 128)  # 1 million documents, 128-dimensional embeddings
texts = ["Document " + str(i) for i in range(1000000)]
hierarchical_rag = HierarchicalRAG(generator, embeddings, texts)
response = hierarchical_rag.retrieve_and_generate(
    "What are the latest advancements in quantum computing?"
)
print(response)</pre>			<p>The preceding code implements a <code>HierarchicalRAG</code> class that creates an efficient retrieval system using <code>1000</code>) – it uses FAISS’s <code>IVFFlat</code> index, which is a hierarchical index that first clusters the vectors and then performs an exact search within relevant clusters, where the quantizer (<code>IndexFlatL2</code>) is used to assign vectors to clusters during training. The <code>retrieve</code> method takes a query and returns <em class="italic">k</em> similar documents by first computing the query’s embedding and then searching the hierarchical index. The <code>compute_embedding</code> method is a placeholder that would typically implement actual embedding computation. The <code>retrieve_and_generate</code> method ties everything together by retrieving relevant documents, concatenating them into a context, creating a prompt that combines the context and query, and then using the language model to generate a response. The example usage shows how to initialize the system with 1 million documents (using random <a id="_idIndexMarker1298"/>embeddings for demonstration purposes) and perform a query about quantum computing. First, the <code>IVFFlat</code> index groups similar documents together during training (<code>index.train()</code>) and then uses these clusters to speed up search operations by only searching in the most relevant clusters instead of the entire dataset, making it much more efficient than a brute-force approach when dealing with large document collections.</p>
			<p>Now that we’ve explored how to scale RAG systems to handle massive knowledge bases through hierarchical indexing, let’s look ahead to some exciting future directions in RAG research for LLMs.</p>
			<h1 id="_idParaDest-324"><a id="_idTextAnchor397"/>Future directions in RAG research for LLMs</h1>
			<p>As RAG continues to evolve, several promising <a id="_idIndexMarker1299"/>research directions have begun to emerge:</p>
			<ul>
				<li><strong class="bold">Multi-modal RAG</strong>: Incorporating<a id="_idIndexMarker1300"/> image, audio, and video data in retrieval and generation</li>
				<li><strong class="bold">Temporal RAG</strong>: Handling time-sensitive information <a id="_idIndexMarker1301"/>and updates</li>
				<li><strong class="bold">Personalized RAG</strong>: Adapting retrieval and <a id="_idIndexMarker1302"/>generation to individual user preferences and knowledge</li>
				<li><strong class="bold">Explainable RAG</strong>: Providing transparency in<a id="_idIndexMarker1303"/> the retrieval and generation process</li>
				<li><strong class="bold">Continual learning in RAG</strong>: Updating knowledge <a id="_idIndexMarker1304"/>bases and retrieval mechanisms in real time</li>
			</ul>
			<p>Here’s a conceptual implementation<a id="_idIndexMarker1305"/> of a multi-modal RAG system:</p>
			<pre class="source-code">
from PIL import Image
import torch
from torchvision.transforms import Resize, ToTensor
class MultiModalRAG:
    def __init__(self, text_retriever, image_retriever, generator):
        self.text_retriever = text_retriever
        self.image_retriever = image_retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.image_transform = transforms.Compose([
            Resize((224, 224)),
            ToTensor(),
        ])
    def retrieve_and_generate(
        self, query: str, image_query: Image.Image = None
    ) -&gt; str:
        text_docs = self.text_retriever.retrieve(query, k=3)
        text_context = " ".join(text_docs)
        if image_query:
            image_tensor = \
                self.image_transform(image_query).unsqueeze(0)
            image_docs = self.image_retriever.retrieve(
                image_tensor, k=2)
            image_context = self.describe_images(image_docs)
        else:
            image_context = ""
        prompt = f"""Text Context: {text_context}
Image Context: {image_context}
Query: {query}
Based on both the textual and visual information provided, please respond to the query:
Response:"""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=300)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
    def describe_images(self, image_docs: List[Image.Image]) -&gt; str:
        # This method would use an image captioning model to describe the retrieved images
        # For simplicity, we'll use placeholder descriptions
        descriptions = [f"Image {i+1}: A relevant visual representation" for i in range(len(image_docs))]
        return " ".join(descriptions)
# Example usage
text_retriever = SomeTextRetrieverClass()  # Replace with your actual text retriever
image_retriever = SomeImageRetrieverClass()  # Replace with your actual image retriever
multi_modal_rag = MultiModalRAG(
    text_retriever, image_retriever, generator
)
query = "Explain the process of photosynthesis in plants"
image_query = Image.open("plant_image.jpg")  # Load an image of a plant
response = multi_modal_rag.retrieve_and_generate(query, image_query)
print(response)</pre>			<p>Let’s understand how this <a id="_idIndexMarker1306"/>code implements a multi-modal RAG system that combines both text and image processing capabilities.</p>
			<p>The <code>MultiModalRAG</code> class represents an advanced RAG system that can process both textual and visual information simultaneously to provide more comprehensive responses. It’s initialized with three key components: a text retriever (for handling textual documents), an image retriever (for processing visual content), and a generator (language model for response generation), along with an image transformer that standardizes images to a consistent size (<code>224</code> x <code>224</code>). The core method, <code>retrieve_and_generate</code>, takes both a text query and an optional image query, first retrieving relevant text documents using the text retriever. Then, if an image is provided, it processes it through the image transformer and retrieves relevant images using the image retriever. These retrieved images are then converted into textual descriptions using the <code>describe_images</code> method (which, in a real implementation, would use an image captioning model). All this information is combined into a structured prompt that includes both text and image context, allowing the generator to create responses that incorporate both textual and visual information. This multi-modal approach is particularly <a id="_idIndexMarker1307"/>powerful for queries that benefit from visual contexts, such as explaining scientific processes, describing physical objects, or analyzing visual patterns. This is demonstrated in the preceding example, where it’s used to explain photosynthesis with both textual information and a plant image.</p>
			<p>The preceding code represents an important step forward in RAG systems by doing the following:</p>
			<ul>
				<li>Breaking down the traditional text-only barrier</li>
				<li>Enabling richer, more contextual responses</li>
				<li>Creating a flexible framework that could be extended to other modalities</li>
				<li>Demonstrating how <a id="_idIndexMarker1308"/>different types of information c<a id="_idTextAnchor398"/>an be unified in a single system</li>
			</ul>
			<h1 id="_idParaDest-325"><a id="_idTextAnchor399"/>Summary</h1>
			<p>This chapter elevated RAG from a basic data retrieval method to a dynamic framework for building truly adaptive LLM-powered systems. It explored techniques such as iterative and adaptive retrieval, meta-learning, and synergistic prompting, transforming RAG into a context-aware problem solver capable of complex analysis and nuanced understanding, mirroring expert-level research. Addressing ambiguity, uncertainty, and scalability isn’t just about overcoming hurdles, but about building trust and enabling real-world deployment.</p>
			<p>In the next chapter, we’ll explore various evaluation techniques for RAG systems.</p>
		</div>
	</div></div></body></html>