<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer056">
			<h1 id="_idParaDest-316" class="chapter-number"><a id="_idTextAnchor389"/>28</h1>
			<h1 id="_idParaDest-317"><a id="_idTextAnchor390"/>Advanced RAG</h1>
			<p>In <a href="B31249_26.xhtml#_idTextAnchor366"><span class="No-Break"><em class="italic">Chapter 26</em></span></a>, we covered the basics of the RAG pattern, a simple process where a user’s query triggers a search in an external knowledge base. The information that’s retrieved is then directly appended to the query, and this augmented prompt is passed to the LLM to generate a response, allowing it to access external data without <span class="No-Break">complex processing.</span></p>
			<p>Now, in this chapter, we’ll move beyond these basic RAG methods and explore more sophisticated techniques designed to significantly enhance LLM performance across a wide range <span class="No-Break">of tasks.</span></p>
			<p>By the end of this chapter, you’ll be equipped with the knowledge to implement these advanced RAG strategies, enabling your LLM applications to achieve greater accuracy <span class="No-Break">and efficiency.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Multi-step and iterative retrieval techniques <span class="No-Break">for LLMs</span></li>
				<li>Adaptive retrieval based on context and task <span class="No-Break">in LLMs</span></li>
				<li>Meta-learning for improved retrieval <span class="No-Break">in LLMs</span></li>
				<li>Combining RAG with other LLM <span class="No-Break">prompting techniques</span></li>
				<li>Handling ambiguity and uncertainty in <span class="No-Break">LLM-based RAG</span></li>
				<li>Scaling RAG to very large <span class="No-Break">knowledge bases</span></li>
				<li>Future directions in RAG research <span class="No-Break">for LLMs</span></li>
			</ul>
			<h1 id="_idParaDest-318"><a id="_idTextAnchor391"/>Multi-step and iterative retrieval techniques for LLMs</h1>
			<p>Using multi-step and iterative<a id="_idIndexMarker1276"/> retrieval techniques for LLMs is a dynamic, recursive approach to information gathering where the model progressively refines its search strategy. The code provided in this section illustrates a multi-step RAG framework that expands context iteratively, retrieves additional documents, and generates responses through multiple steps, allowing for increasingly <a id="_idIndexMarker1277"/>comprehensive and nuanced information retrieval by dynamically adjusting queries and integrating <span class="No-Break">retrieved knowledge.</span></p>
			<p>Here are some of its <span class="No-Break">key characteristics:</span></p>
			<ul>
				<li>Iterative <span class="No-Break">context expansion</span></li>
				<li>Multiple retrieval steps (configurable up <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">max_steps</strong></span><span class="No-Break">)</span></li>
				<li>Dynamic <span class="No-Break">query refinement</span></li>
				<li>Contextual <span class="No-Break">document retrieval</span></li>
				<li>Adaptive <span class="No-Break">response generation</span></li>
			</ul>
			<p>Multi-step and iterative retrieval techniques for LLMs, with their dynamic and recursive approaches, benefit use cases that require the <a id="_idIndexMarker1278"/><span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Complex question-answering</strong>: When questions require information to be synthesized from multiple sources or involve intricate logical reasoning, iterative retrieval allows the LLM to gather the necessary context progressively. Examples include legal document analysis, scientific research, and in-depth <span class="No-Break">financial analysis.</span></li>
				<li><strong class="bold">Knowledge-intensive conversations</strong>: In conversational AI scenarios where the dialogue involves exploring a topic in depth, iterative RAG enables the LLM to maintain context and refine its understanding over multiple turns. This is valuable for educational chatbots, technical support, and <span class="No-Break">interactive tutorials.</span></li>
				<li><strong class="bold">Research and exploration</strong>: For tasks such as literature reviews, market research, or investigative journalism, the ability to dynamically refine queries and explore related information is crucial. Iterative retrieval allows the LLM to act as a research assistant, uncovering connections and insights that would be difficult to find with a <span class="No-Break">single query.</span></li>
				<li><strong class="bold">Technical documentation and troubleshooting</strong>: When dealing with complex technical issues, iterative RAG can help the LLM navigate extensive documentation, progressively narrowing down the search to pinpoint relevant information. This improves the efficiency of troubleshooting and <span class="No-Break">technical support.</span></li>
				<li><strong class="bold">Dynamic information gathering</strong>: This includes any situation where the information that is needed isn’t able to be gathered in a single pass. For example, if a user wants to find out all the news articles related to a specific court case and then wants to <a id="_idIndexMarker1279"/>know what people are saying about those news articles on social media, multiple steps of information gathering would <span class="No-Break">be required.</span></li>
				<li><strong class="bold">Dealing with ambiguous queries</strong>: When a user’s query is ambiguous, the LLM can ask clarifying questions and then use the user’s response to refine <span class="No-Break">the search.</span></li>
			</ul>
			<p>In essence, any use case that demands a deep, nuanced understanding of information, and where a single retrieval step is insufficient, stands to gain significantly from multi-step and <span class="No-Break">iterative RAG.</span></p>
			<p>Let’s take a look at the following <span class="No-Break">code example:</span></p>
			<pre class="source-code">
from typing import List, Dict
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
class MultiStepRAG:
    def __init__(self, retriever, generator, max_steps=3):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.max_steps = max_steps
    def retrieve_and_generate(self, query: str) -&gt; str:
        context = ""
        for step in range(self.max_steps):
            retrieved_docs = self.retriever.retrieve(
                query + " " + context, k=3
            )
            context += " ".join(retrieved_docs) + " "
            prompt = f"Context: {context}\nQuery: {query}\nResponse:"
            inputs = self.tokenizer(
                prompt, return_tensors="pt"
            )
            outputs = self.generator.generate(inputs, max_length=200)
            response = self.tokenizer.decode(
                outputs[0], skip_special_tokens=True
            )
            if self.is_response_complete(response):
                break
            query = self.generate_follow_up_query(query, response)
        return response
    def is_response_complete(self, response: str) -&gt; bool:
        # Implement logic to determine if the response is complete
        return "I don't have enough information" not in response
    def generate_follow_up_query(
        self, original_query: str, current_response: str
    ) -&gt; str:
        prompt = f"Original question: {original_query}\nCurrent answer: {current_response}\nGenerate a follow-up question to gather more information:"
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=50)
        return self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
# Example usage
retriever = SomeRetrieverClass()  # Replace with your actual retriever
generator = AutoModelForCausalLM.from_pretrained("gpt2-medium")
multi_step_rag = MultiStepRAG(retriever, generator)
response = multi_step_rag.retrieve_and_generate("What are the effects of climate change on biodiversity?")
print(response)</pre>			<p>In this pseudocode example, the <strong class="source-inline">MultiStepRAG</strong> class implements multistep retrieval through three <span class="No-Break">critical methods:</span></p>
			<ul>
				<li><strong class="source-inline">retrieve_and_generate()</strong>: This method iteratively expands context by retrieving documents, generating responses, and dynamically updating the search context across multiple <a id="_idIndexMarker1280"/>steps. It manages the retrieval process, limiting iterations to a <span class="No-Break">configurable maximum.</span></li>
				<li><strong class="source-inline">is_response_complete()</strong>: This method evaluates response quality by detecting whether the generated answer addresses the query sufficiently, typically checking for indicators of <span class="No-Break">incomplete information.</span></li>
				<li><strong class="source-inline">generate_follow_up_query()</strong>: This method creates refined follow-up queries by using the language model to generate new questions based on the original query and current response, enabling intelligent <span class="No-Break">context exploration.</span></li>
			</ul>
			<p>This implementation allows for progressive information gathering, where each retrieval step dynamically refines the context and generates more comprehensive responses by recursively expanding the <span class="No-Break">knowledge base.</span></p>
			<h1 id="_idParaDest-319"><a id="_idTextAnchor392"/>Adaptive retrieval based on context and task in LLMs</h1>
			<p>Adaptive retrieval is a sophisticated approach to<a id="_idIndexMarker1281"/> information retrieval that dynamically adjusts strategies based on specific <span class="No-Break">task requirements.</span></p>
			<p>The following code demonstrates this concept through an implementation that tailors retrieval and generation processes across different <span class="No-Break">task types:</span></p>
			<pre class="source-code">
from enum import Enum
class TaskType(Enum):
    FACTUAL_QA = 1
    SUMMARIZATION = 2
    ANALYSIS = 3
class AdaptiveRAG:
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
    def retrieve_and_generate(self, query: str, task_type: TaskType
    ) -&gt; str:
        if task_type == TaskType.FACTUAL_QA:
            k = 3
            prompt_template = "Context: {context}\nQuestion: {query}\nAnswer:"
        elif task_type == TaskType.SUMMARIZATION:
            k = 10
            prompt_template = "Summarize the following information:\n{context}\nSummary:"
        elif task_type == TaskType.ANALYSIS:
            k = 5
            prompt_template = "Analyze the following information:\n{context}\nQuery: {query}\nAnalysis:"
        retrieved_docs = self.retriever.retrieve(query, k=k)
        context = " ".join(retrieved_docs)
        prompt = prompt_template.format(context=context, query=query)
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=300)
        response = self.tokenizer.decode(
            outputs[0], skip_special_tokens=True
        )
        return response
# Example usage
adaptive_rag = AdaptiveRAG(retriever, generator)
factual_response = adaptive_rag.retrieve_and_generate(
    "What is the capital of France?",
    TaskType.FACTUAL_QA
)
summary_response = adaptive_rag.retrieve_and_generate(
    "Summarize the causes of World War I",
    TaskType.SUMMARIZATION
)
analysis_response = adaptive_rag.retrieve_and_generate(
    "Analyze the impact of social media on mental health",
    TaskType.ANALYSIS
)</pre>			<p>The preceding code introduces an <strong class="source-inline">AdaptiveRAG</strong> class that uses an <strong class="source-inline">Enum</strong> value called <strong class="source-inline">TaskType</strong> to define distinct retrieval strategies for different scenarios: factual question-answering, summarization, and analysis. Each task type receives customized treatment in terms of document retrieval volume and <span class="No-Break">prompt formatting.</span></p>
			<p>In the <strong class="source-inline">retrieve_and_generate()</strong> method, the system dynamically configures <span class="No-Break">retrieval parameters:</span></p>
			<ul>
				<li><strong class="source-inline">Factual QA</strong>: This retrieves three documents with a direct <span class="No-Break">question-answer format</span></li>
				<li><strong class="source-inline">Summarization</strong>: This retrieves ten documents with a <span class="No-Break">summary-focused template</span></li>
				<li><strong class="source-inline">Analysis</strong>: This retrieves five documents with an analytical <span class="No-Break">prompt structure</span></li>
			</ul>
			<p>The method retrieves relevant<a id="_idIndexMarker1282"/> documents, constructs a context, generates a task-specific prompt, and produces a response tailored to the specific task type. This approach allows for more nuanced and contextually appropriate information retrieval and generation across different knowledge <span class="No-Break">exploration scenarios.</span></p>
			<p>This example usage demonstrates flexibility by generating responses for factual queries, summaries, and analytical tasks using the same <span class="No-Break">adaptive framework.</span></p>
			<h1 id="_idParaDest-320"><a id="_idTextAnchor393"/>Meta-learning for improved retrieval in LLMs</h1>
			<p>Meta-learning in retrieval systems is a dynamic approach where the model learns to improve its retrieval strategy by analyzing past performance and relevance feedback. In this implementation, meta-learning<a id="_idIndexMarker1283"/> focuses on selecting and ranking documents adaptively based on learned <span class="No-Break">relevance patterns.</span></p>
			<p>Let’s implement a simple meta-learning approach <span class="No-Break">for RAG.</span></p>
			<p>The following code demonstrates meta-learning by retrieving documents about dark matter theories and simulating relevance feedback to train the model, showcasing how the system can improve its information retrieval <span class="No-Break">capabilities iteratively:</span></p>
			<pre class="source-code">
import numpy as np
from sklearn.linear_model import LogisticRegression
class MetaLearningRAG:
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.meta_model = LogisticRegression()
        self.training_data = []
    def retrieve_and_generate(self, query: str) -&gt; str:
        retrieved_docs = self.retriever.retrieve(query, k=10)
        if self.meta_model.coef_.size &gt; 0:  # If the meta-model has been trained
            relevance_scores = self.predict_relevance(
                query, retrieved_docs)
            top_docs = [
                doc for _, doc in sorted(
                    zip(relevance_scores, retrieved_docs),
                    reverse=True
                )
            ][:3]
        else:
            top_docs = retrieved_docs[:3]
        context = " ".join(top_docs)
        prompt = f"Context: {context}\nQuery: {query}\nResponse:"
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=200)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
    def predict_relevance(self, query: str, docs: List[str]
    ) -&gt; np.ndarray:
        features = self.compute_features(query, docs)
        return self.meta_model.predict_proba(features)[:, 1]  # Probability of relevance
    def compute_features(self, query: str, docs: List[str]
    ) -&gt; np.ndarray:
        # Compute features for the query-document pairs
        # This is a placeholder implementation
        return np.random.rand(len(docs), 5)  # 5 random features
    def update_meta_model(
        self, query: str, retrieved_docs: List[str],
        relevance_feedback: List[int]
    ):
        features = self.compute_features(query, retrieved_docs)
        self.training_data.extend(zip(features, relevance_feedback))
        if len(self.training_data) &gt;= 100:  # Train the meta-model periodically
            X, y = zip(*self.training_data)
            self.meta_model.fit(X, y)
            self.training_data = []  # Clear the training data after updating the model
# Example usage
meta_learning_rag = MetaLearningRAG(retriever, generator)
response = meta_learning_rag.retrieve_and_generate(
    "What are the main theories of dark matter?"
)
print(response)
# Simulating relevance feedback
retrieved_docs = meta_learning_rag.retriever.retrieve(
    "What are the main theories of dark matter?",
    k=10
)
relevance_feedback = [1, 0, 1, 1, 0, 0, 1, 0, 0, 1]  # 1 for relevant, 0 for not relevant
meta_learning_rag.update_meta_model(
    "What are the main theories of dark matter?",
    retrieved_docs, relevance_feedback
)</pre>			<p>The key meta-learning components in the <a id="_idIndexMarker1284"/>preceding code include <span class="No-Break">the following:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Relevance prediction</strong></span><span class="No-Break">:</span><ul><li>Uses logistic regression to predict <span class="No-Break">document relevance</span></li><li>The <strong class="source-inline">predict_relevance()</strong> method estimates the probability of <span class="No-Break">document usefulness</span></li><li>Dynamically adjusts document selection based on <span class="No-Break">learned features</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Feature computation</strong></span><span class="No-Break">:</span><ul><li>The <strong class="source-inline">compute_features()</strong> method generates document <span class="No-Break">representation features</span></li><li>Currently, it uses randomly generated values as placeholder features for demonstration or <span class="No-Break">testing purposes</span></li><li>In practice, it would include semantic similarity, keyword matching, <span class="No-Break">and more.</span></li></ul></li>
				<li><strong class="bold">Adaptive </strong><span class="No-Break"><strong class="bold">learning mechanism</strong></span><span class="No-Break">:</span><ul><li>Accumulates training data from <span class="No-Break">relevance feedback</span></li><li>Retrains the meta-model when sufficient data is collected (<span class="No-Break">100 samples)</span></li><li>Clears the training data after model updates to <span class="No-Break">prevent overfitting</span></li></ul></li>
				<li><strong class="bold">Retrieval </strong><span class="No-Break"><strong class="bold">strategy modification</strong></span><span class="No-Break">:</span><ul><li>Initially uses the top 10 <span class="No-Break">retrieved documents</span></li><li>After meta-model training, it selects the top three documents based on learned <span class="No-Break">relevance scores</span></li><li>Continuously refines the document <span class="No-Break">selection process</span></li></ul></li>
			</ul>
			<p>The code implements a <strong class="source-inline">MetaLearningRAG</strong> class that dynamically enhances retrieval performance using machine learning techniques. The core innovation lies in its ability to learn from relevance feedback and adjust document <span class="No-Break">selection strategies.</span></p>
			<p>Let’s look at the <span class="No-Break">key methods:</span></p>
			<ul>
				<li><strong class="source-inline">retrieve_and_generate()</strong>: Selects the top documents using a <span class="No-Break">trained meta-model</span></li>
				<li><strong class="source-inline">predict_relevance()</strong>: Estimates <a id="_idIndexMarker1285"/>document <span class="No-Break">relevance probabilities</span></li>
				<li><strong class="source-inline">compute_features()</strong>: Generates feature representations <span class="No-Break">for documents</span></li>
				<li><strong class="source-inline">update_meta_model()</strong>: Periodically retrains the model based on <span class="No-Break">relevance feedback</span></li>
			</ul>
			<p>The implementation uses logistic regression to predict document relevance, progressively refining retrieval by learning from user interactions. When sufficient training data has been accumulated, the meta-model is retrained, allowing the system to adapt its document selection strategy based on historical performance <span class="No-Break">and feedback.</span></p>
			<p>In the context of meta-learning for retrieval systems, <em class="italic">relevance</em> refers to the contextual usefulness and information value of the documents that were retrieved for a <span class="No-Break">specific query.</span></p>
			<p>Let’s look at the key <em class="italic">relevance</em> aspects shown in the <span class="No-Break">preceding code:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">Relevance scoring</strong></span><span class="No-Break">:</span><ul><li>Predicts the probability of the document <span class="No-Break">being useful</span></li><li>Uses machine learning to learn <span class="No-Break">relevance patterns</span></li><li>Allows dynamic <span class="No-Break">document ranking</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Feedback mechanism</strong></span><span class="No-Break">:</span><ul><li>Binary relevance labels (<strong class="source-inline">1</strong> = relevant, <strong class="source-inline">0</strong> = <span class="No-Break">not relevant)</span></li><li>Enables the system to learn from user-provided <span class="No-Break">quality signals</span></li><li>Improves future <span class="No-Break">document selection</span></li></ul></li>
				<li><span class="No-Break"><strong class="bold">Feature-based relevance</strong></span><span class="No-Break">:</span><ul><li>Computes document features representing <span class="No-Break">potential usefulness</span></li><li>The preceding code uses <span class="No-Break">random features</span></li><li>Captures semantic and <span class="No-Break">contextual relationships</span></li></ul></li>
			</ul>
			<p>The core goal is to<a id="_idIndexMarker1286"/> create an adaptive retrieval system that learns to select increasingly precise and valuable documents through iterative feedback and machine <span class="No-Break">learning techniques.</span></p>
			<h1 id="_idParaDest-321"><a id="_idTextAnchor394"/>Combining RAG with other LLM prompting techniques</h1>
			<p>We can enhance RAG by combining it with <a id="_idIndexMarker1287"/>other prompting techniques, such as CoT (see <a href="B31249_20.xhtml#_idTextAnchor305"><span class="No-Break"><em class="italic">Chapter 20</em></span></a>) or few-shot learning. Here’s an<a id="_idIndexMarker1288"/> example that combines RAG <span class="No-Break">with CoT:</span></p>
			<pre class="source-code">
class RAGWithCoT:
    def __init__(self, retriever, generator):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
    def retrieve_and_generate(self, query: str) -&gt; str:
        retrieved_docs = self.retriever.retrieve(query, k=3)
        context = " ".join(retrieved_docs)
        cot_prompt = f"""Context: {context}
Question: {query}
Let's approach this step-by-step:
1) First, we should consider...
2) Next, we need to analyze...
3) Then, we can conclude...
Based on this reasoning, the final answer is:
Answer:"""
        inputs = self.tokenizer(cot_prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=500)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
# Example usage
rag_with_cot = RAGWithCoT(retriever, generator)
response = rag_with_cot.retrieve_and_generate("What are the potential long-term effects of artificial intelligence on employment?")
print(response)</pre>			<p>The <strong class="source-inline">RAGWithCoT</strong> class implements a RAG approach<a id="_idIndexMarker1289"/> enhanced with CoT reasoning. By retrieving relevant documents and constructing a prompt that encourages step-by-step problem solving, the method transforms standard query response generation into a more structured, <span class="No-Break">analytical process.</span></p>
			<p>The implementation guides the language model through an explicit reasoning framework, breaking complex queries into logical steps. This approach prompts the model to demonstrate intermediate reasoning, creating a more transparent and potentially more accurate response <span class="No-Break">generation process.</span></p>
			<p>The method combines contextual document retrieval with a carefully designed prompt template that explicitly structures <a id="_idIndexMarker1290"/>the model’s reasoning. By requiring the model to outline its thinking process before presenting a final answer, the implementation seeks to improve the depth and quality of <span class="No-Break">generated responses.</span></p>
			<p>As we explore advanced RAG techniques, the next critical challenge emerges: handling ambiguity and uncertainty in language-model-based information retrieval. The following section will delve into sophisticated strategies for managing complex, nuanced, and potentially conflicting information sources, highlighting approaches that enable more robust and reliable knowledge extraction <span class="No-Break">and generation.</span></p>
			<h1 id="_idParaDest-322"><a id="_idTextAnchor395"/>Handling ambiguity and uncertainty in LLM-based RAG</h1>
			<p>Ambiguity and uncertainty directly<a id="_idIndexMarker1291"/> compromise the accuracy and reliability of generated responses. Ambiguous queries, for instance, can trigger the process of retrieving irrelevant or conflicting information, leading the LLM to produce incoherent or incorrect outputs. Consider the query, “What about apples?” This could refer to Apple Inc., the fruit, or specific apple varieties. A naive RAG system might pull data from all contexts, resulting in a <span class="No-Break">confused response.</span></p>
			<p>Furthermore, uncertainty in retrieved information – due to conflicting or outdated data in the knowledge base – exacerbates the problem. Without mechanisms to assess data reliability, the LLM may propagate inaccuracies. LLMs themselves operate on probabilities, adding another layer of uncertainty. For example, when addressing a niche topic, an LLM might generate a “best guess” that, without proper uncertainty estimation, could be presented as fact. Combining multiple pieces of uncertain information further compounds this issue, potentially leading to misleading and unreliable responses, ultimately undermining user trust and limiting the practical applications of <span class="No-Break">RAG systems.</span></p>
			<p>To handle ambiguity and <a id="_idIndexMarker1292"/>uncertainty, we can implement a system that generates multiple hypotheses and ranks them based <span class="No-Break">on confidence:</span></p>
			<pre class="source-code">
class UncertaintyAwareRAG:
    def __init__(self, retriever, generator, n_hypotheses=3):
        self.retriever = retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.n_hypotheses = n_hypotheses
    def retrieve_and_generate(self, query: str) -&gt; Dict[str, float]:
        retrieved_docs = self.retriever.retrieve(query, k=5)
        context = " ".join(retrieved_docs)
        prompt = (
            f"Context: {context}\n"
            "Question: {query}\n"
            f"Generate {self.n_hypotheses} possible answers "
            f"with confidence scores:\n"
        )
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(
            inputs, max_length=500,
            num_return_sequences=self.n_hypotheses
        )
        hypotheses = []
        for output in outputs:
            hypothesis = self.tokenizer.decode(
                output, skip_special_tokens=True
            )
            hypotheses.append(self.parse_hypothesis(hypothesis))
        return dict(
            sorted(
                hypotheses, key=lambda x: x[1], reverse=True
            )
    )
    def parse_hypothesis(self, hypothesis: str) -&gt; Tuple[str, float]:
        # This is a simple parser, assuming the format "Answer (Confidence: X%): ..."
        parts = hypothesis.split(":")
        confidence = float(
            parts[0].split("(Confidence: ")[1].strip("%)"))/100
        answer = ":".join(parts[1:]).strip()
        return (answer, confidence)
# Example usage
uncertainty_aware_rag = UncertaintyAwareRAG(retriever, generator)
hypotheses = uncertainty_aware_rag.retrieve_and_generate(
    "What will be the dominant form of energy in 2050?"
)
for answer, confidence in hypotheses.items():
    print(f"Hypothesis (Confidence: {confidence:.2f}): {answer}")</pre>			<p>The preceding code implements an <strong class="source-inline">UncertaintyAwareRAG</strong> class that intelligently handles ambiguous queries by generating multiple possible answers with confidence scores. It works by initializing with a<a id="_idIndexMarker1293"/> retriever component (for fetching relevant documents), a generator (language model), and a parameter for the number of hypotheses to generate. When <strong class="source-inline">retrieve_and_generate</strong> is called with a query, it retrieves relevant documents and combines them into a context, then constructs a specialized prompt asking for multiple possible answers with confidence scores. The generator produces multiple hypotheses using the <strong class="source-inline">num_return_sequences</strong> parameter, each including a confidence score. These hypotheses are parsed using the <strong class="source-inline">parse_hypothesis</strong> method, which extracts both the answer text and its confidence score from a standardized format of <strong class="source-inline">"Answer (Confidence: X%): ..."</strong>. The results are then sorted by confidence score and returned as a dictionary that maps answers to their confidence values. This approach is particularly valuable for questions that may not have a single definitive answer (such as future predictions or complex scenarios) as it explicitly acknowledges uncertainty and provides multiple plausible responses with their associated confidence levels, allowing users to make more informed decisions based on the range of possibilities and their <span class="No-Break">relative likelihoods.</span></p>
			<p>After implementing uncertainty handling in our RAG system, the next crucial challenge is dealing with massive document collections. As knowledge bases grow to millions or even billions of documents, traditional retrieval methods become impractical, requiring more sophisticated approaches. Let’s explore how we can scale RAG so that it can handle very large knowledge bases efficiently through <span class="No-Break">hierarchical indexing.</span></p>
			<h1 id="_idParaDest-323"><a id="_idTextAnchor396"/>Scaling RAG to very large knowledge bases</h1>
			<p>We can scale RAG using a <a id="_idIndexMarker1294"/>hierarchical system. A hierarchical RAG system is an advanced architecture that organizes document retrieval in a tree-like structure with multiple levels. Instead of searching through all documents linearly, it first clusters similar documents together and creates a hierarchy of these clusters. When a query comes in, the system identifies the most relevant cluster(s) at the top level, drills down to find the most relevant sub-clusters, and finally retrieves the most similar documents from within those targeted sub-clusters. Think of it like a library where books are first organized by broad categories (science, history, fiction), then by sub-categories (physics, biology, chemistry), and finally by specific topics – this makes finding a particular book much faster than searching through every <span class="No-Break">single book.</span></p>
			<p>The hierarchical approach to RAG offers significant advantages because it dramatically improves both the efficiency and scalability of document retrieval while maintaining high accuracy. By organizing documents into clusters and sub-clusters, the system can quickly narrow down the search space from potentially millions of documents to a much smaller, relevant subset, which not only speeds up retrieval but also reduces computational resources and memory requirements. This makes it possible to handle massive document collections<a id="_idIndexMarker1295"/> that would be impractical with traditional flat retrieval approaches. The hierarchical structure also enables better parallelization of search operations and can even improve result quality by considering document relationships within <span class="No-Break">the hierarchy.</span></p>
			<p>The following code snippet defines a class for hierarchical RAG, leveraging Facebook’s AI Similarity Search (Faiss) library for efficient similarity search and <span class="No-Break">generation capabilities:</span></p>
			<pre class="source-code">
import faiss
class HierarchicalRAG:
    def __init__(
        self, generator, embeddings, texts, n_clusters=1000
    ):
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.embeddings = embeddings
        self.texts = texts
        # Create a hierarchical index
        self.quantizer = faiss.IndexFlatL2(embeddings.shape[1])
        self.index = faiss.IndexIVFFlat(
            self.quantizer, embeddings.shape[1], n_clusters
        )
        self.index.train(embeddings)
        self.index.add(embeddings)
    def retrieve(self, query: str, k: int = 5) -&gt; List[str]:
        query_embedding = self.compute_embedding(query)
        _, indices = self.index.search(
            query_embedding.reshape(1, -1), k
        )
        return [self.texts[i] for i in indices[0]]
    def compute_embedding(self, text: str) -&gt; np.ndarray:
        # Compute embedding for the given text
        # This is a placeholder implementation
        return np.random.rand(1, self.embeddings.shape[1])
    def retrieve_and_generate(self, query: str) -&gt; str:
        retrieved_docs = self.retrieve(query)
        context = " ".join(retrieved_docs)
        prompt = f"Context: {context}\nQuery: {query}\nResponse:"
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=200)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
# Example usage
embeddings = np.random.rand(1000000, 128)  # 1 million documents, 128-dimensional embeddings
texts = ["Document " + str(i) for i in range(1000000)]
hierarchical_rag = HierarchicalRAG(generator, embeddings, texts)
response = hierarchical_rag.retrieve_and_generate(
    "What are the latest advancements in quantum computing?"
)
print(response)</pre>			<p>The preceding code implements a <strong class="source-inline">HierarchicalRAG</strong> class that creates an efficient retrieval system using <strong class="bold">Facebook AI Similarity Search</strong> (<strong class="bold">Faiss</strong>) to handle large-scale document collections. The<a id="_idIndexMarker1296"/> class is initialized with a language model generator, document embeddings, and the actual <a id="_idIndexMarker1297"/>texts, along with a parameter for the number of clusters (defaulting to <strong class="source-inline">1000</strong>) – it uses FAISS’s <strong class="source-inline">IVFFlat</strong> index, which is a hierarchical index that first clusters the vectors and then performs an exact search within relevant clusters, where the quantizer (<strong class="source-inline">IndexFlatL2</strong>) is used to assign vectors to clusters during training. The <strong class="source-inline">retrieve</strong> method takes a query and returns <em class="italic">k</em> similar documents by first computing the query’s embedding and then searching the hierarchical index. The <strong class="source-inline">compute_embedding</strong> method is a placeholder that would typically implement actual embedding computation. The <strong class="source-inline">retrieve_and_generate</strong> method ties everything together by retrieving relevant documents, concatenating them into a context, creating a prompt that combines the context and query, and then using the language model to generate a response. The example usage shows how to initialize the system with 1 million documents (using random <a id="_idIndexMarker1298"/>embeddings for demonstration purposes) and perform a query about quantum computing. First, the <strong class="source-inline">IVFFlat</strong> index groups similar documents together during training (<strong class="source-inline">index.train()</strong>) and then uses these clusters to speed up search operations by only searching in the most relevant clusters instead of the entire dataset, making it much more efficient than a brute-force approach when dealing with large <span class="No-Break">document collections.</span></p>
			<p>Now that we’ve explored how to scale RAG systems to handle massive knowledge bases through hierarchical indexing, let’s look ahead to some exciting future directions in RAG research <span class="No-Break">for LLMs.</span></p>
			<h1 id="_idParaDest-324"><a id="_idTextAnchor397"/>Future directions in RAG research for LLMs</h1>
			<p>As RAG continues to evolve, several promising <a id="_idIndexMarker1299"/>research directions have begun <span class="No-Break">to emerge:</span></p>
			<ul>
				<li><strong class="bold">Multi-modal RAG</strong>: Incorporating<a id="_idIndexMarker1300"/> image, audio, and video data in retrieval <span class="No-Break">and generation</span></li>
				<li><strong class="bold">Temporal RAG</strong>: Handling time-sensitive information <a id="_idIndexMarker1301"/><span class="No-Break">and updates</span></li>
				<li><strong class="bold">Personalized RAG</strong>: Adapting retrieval and <a id="_idIndexMarker1302"/>generation to individual user preferences <span class="No-Break">and knowledge</span></li>
				<li><strong class="bold">Explainable RAG</strong>: Providing transparency in<a id="_idIndexMarker1303"/> the retrieval and <span class="No-Break">generation process</span></li>
				<li><strong class="bold">Continual learning in RAG</strong>: Updating knowledge <a id="_idIndexMarker1304"/>bases and retrieval mechanisms in <span class="No-Break">real time</span></li>
			</ul>
			<p>Here’s a conceptual implementation<a id="_idIndexMarker1305"/> of a multi-modal <span class="No-Break">RAG system:</span></p>
			<pre class="source-code">
from PIL import Image
import torch
from torchvision.transforms import Resize, ToTensor
class MultiModalRAG:
    def __init__(self, text_retriever, image_retriever, generator):
        self.text_retriever = text_retriever
        self.image_retriever = image_retriever
        self.generator = generator
        self.tokenizer = AutoTokenizer.from_pretrained(generator)
        self.image_transform = transforms.Compose([
            Resize((224, 224)),
            ToTensor(),
        ])
    def retrieve_and_generate(
        self, query: str, image_query: Image.Image = None
    ) -&gt; str:
        text_docs = self.text_retriever.retrieve(query, k=3)
        text_context = " ".join(text_docs)
        if image_query:
            image_tensor = \
                self.image_transform(image_query).unsqueeze(0)
            image_docs = self.image_retriever.retrieve(
                image_tensor, k=2)
            image_context = self.describe_images(image_docs)
        else:
            image_context = ""
        prompt = f"""Text Context: {text_context}
Image Context: {image_context}
Query: {query}
Based on both the textual and visual information provided, please respond to the query:
Response:"""
        inputs = self.tokenizer(prompt, return_tensors="pt")
        outputs = self.generator.generate(inputs, max_length=300)
        response = self.tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        return response
    def describe_images(self, image_docs: List[Image.Image]) -&gt; str:
        # This method would use an image captioning model to describe the retrieved images
        # For simplicity, we'll use placeholder descriptions
        descriptions = [f"Image {i+1}: A relevant visual representation" for i in range(len(image_docs))]
        return " ".join(descriptions)
# Example usage
text_retriever = SomeTextRetrieverClass()  # Replace with your actual text retriever
image_retriever = SomeImageRetrieverClass()  # Replace with your actual image retriever
multi_modal_rag = MultiModalRAG(
    text_retriever, image_retriever, generator
)
query = "Explain the process of photosynthesis in plants"
image_query = Image.open("plant_image.jpg")  # Load an image of a plant
response = multi_modal_rag.retrieve_and_generate(query, image_query)
print(response)</pre>			<p>Let’s understand how this <a id="_idIndexMarker1306"/>code implements a multi-modal RAG system that combines both text and image <span class="No-Break">processing capabilities.</span></p>
			<p>The <strong class="source-inline">MultiModalRAG</strong> class represents an advanced RAG system that can process both textual and visual information simultaneously to provide more comprehensive responses. It’s initialized with three key components: a text retriever (for handling textual documents), an image retriever (for processing visual content), and a generator (language model for response generation), along with an image transformer that standardizes images to a consistent size (<strong class="source-inline">224</strong> x <strong class="source-inline">224</strong>). The core method, <strong class="source-inline">retrieve_and_generate</strong>, takes both a text query and an optional image query, first retrieving relevant text documents using the text retriever. Then, if an image is provided, it processes it through the image transformer and retrieves relevant images using the image retriever. These retrieved images are then converted into textual descriptions using the <strong class="source-inline">describe_images</strong> method (which, in a real implementation, would use an image captioning model). All this information is combined into a structured prompt that includes both text and image context, allowing the generator to create responses that incorporate both textual and visual information. This multi-modal approach is particularly <a id="_idIndexMarker1307"/>powerful for queries that benefit from visual contexts, such as explaining scientific processes, describing physical objects, or analyzing visual patterns. This is demonstrated in the preceding example, where it’s used to explain photosynthesis with both textual information and a <span class="No-Break">plant image.</span></p>
			<p>The preceding code represents an important step forward in RAG systems by doing <span class="No-Break">the following:</span></p>
			<ul>
				<li>Breaking down the traditional <span class="No-Break">text-only barrier</span></li>
				<li>Enabling richer, more <span class="No-Break">contextual responses</span></li>
				<li>Creating a flexible framework that could be extended to <span class="No-Break">other modalities</span></li>
				<li>Demonstrating how <a id="_idIndexMarker1308"/>different types of information c<a id="_idTextAnchor398"/>an be unified in a <span class="No-Break">single system</span></li>
			</ul>
			<h1 id="_idParaDest-325"><a id="_idTextAnchor399"/>Summary</h1>
			<p>This chapter elevated RAG from a basic data retrieval method to a dynamic framework for building truly adaptive LLM-powered systems. It explored techniques such as iterative and adaptive retrieval, meta-learning, and synergistic prompting, transforming RAG into a context-aware problem solver capable of complex analysis and nuanced understanding, mirroring expert-level research. Addressing ambiguity, uncertainty, and scalability isn’t just about overcoming hurdles, but about building trust and enabling <span class="No-Break">real-world deployment.</span></p>
			<p>In the next chapter, we’ll explore various evaluation techniques for <span class="No-Break">RAG systems.</span></p>
		</div>
	</div></div></body></html>