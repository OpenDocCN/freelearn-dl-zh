["```py\nPrompt # 1:\n# Read s3://<your-bucket-name-here>/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet file in a dataframe\nPrompt # 2:\n# display a sample of 10 records from dataframe\n```", "```py\nPrompt:\n\"\"\"\nRead s3://<your-bucket-name-here>/zone_lookup/taxi+_zone_lookup.csv in a dataframe name zone_df.\nShow sample 10 records from zone_df.\n\"\"\"\n```", "```py\nPrompt:\n\"\"\"\nPerform a left outer join on dataframe df and dataframe zone_df on PULocationID = LocationID to save in dataframe name yellow_pu_zone_df.\nShow sample 10 records from yellow_pu_zone_df and show schema.\n\"\"\"\n```", "```py\nPrompt:\n# Save dataframe yellow_pu_zone_df as CSV file at location s3://<your-bucket-name-here>/tlc-dataset-ny-taxi/glue_notebook_yellow_pick_up_zone_output/ with header information\n```", "```py\nInstruction to Amazon Q:\nWrite a Glue ETL job.\nRead the 's3://<your bucket name>/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet' file in a dataframe and display a sample of 10 records.\nRead the 's3://<your bucket name>/zone_lookup/taxi+_zone_lookup.csv' file in a dataframe and display a sample of 10 records.\nPerform a left outer join on 'yellow_tripdata_2023-01.parquet' and 'taxi+_zone_lookup.csv' on DOLocationID = LocationID to gather pick-up zone information.\nSave the above dataset as a CSV file in above Amazon S3 bucket in a new folder 'glue_notebook_yellow_drop_off_zone_output'.\n```", "```py\nPrompt # 1:\n# Read s3://<your-bucket-name-here>/yellow_taxi_trip_records/yellow_tripdata_2023-01.parquet file in a dataframe\nPrompt # 2:\n# Display a sample of 10 records from dataframe\n```", "```py\nPrompt:\n\"\"\"\nRead s3://<your-bucket-name-here>/zone_lookup/taxi+_zone_lookup.csv in a dataframe name zone_df. Show sample 10 records from zone_df.\n\"\"\"\n```", "```py\nPrompt:\n\"\"\"\nPerform a left outer join on dataframe df and dataframe zone_df on PULocationID = LocationID to save in dataframe name yellow_pu_zone_df.\nShow sample 10 records from yellow_pu_zone_df and show schema.\n\"\"\"\n```", "```py\nPrompt:\n# Save dataframe yellow_pu_zone_df as CSV file at location s3://<your-bucket-name-here>/tlc-dataset-ny-taxi/glue_notebook_yellow_pick_up_zone_output/ with header information\n```", "```py\nPrompt:\n\"\"\"\nwrite a lambda function.\ncopy s3://<your-bucket-name>/zone_lookup/taxi+_zone_lookup.csv\nas s3://<your-bucket-name>/source_lookup_file/taxi_zone_lookup.csv\n\"\"\"\n```", "```py\nPrompt 1:\n# Fetch this data by importing the SageMaker library\nPrompt 2:\n# Defining global variables BUCKET and ROLE that point to the bucket associated with the Domain and it's execution role\n```", "```py\nPrompt:\n'''Using the requests library download the ZIP file from\nthe url \"https://sagemaker-sample-data-us-west-2.s3-us-west-2.amazonaws.com/autopilot/direct_marketing/bank-additional.zip\"\nand save it to current directory and unzip the archive\n'''\n```", "```py\nPrompt #1:\n'''\nCreate a new dataframe with column no_previous_contact and populates from existing dataframe column pdays using numpy when the condition equals to 999, 1, 0 and show the table\n'''\nPrompt # 2:\n# do one hot encoding for full_data\nPrompt # 3:\n'''\nDrop the columns 'duration', emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m' and 'nr.employed'\nfrom the dataframe and create a new dataframe with name model_data\n'''\n```", "```py\nPrompt # 4:\n#split model_data for train, validation, and test\nPrompt # 5:\n'''\nfor train_data move y_yes as first column.\nDrop y_no and y_yes columns from train_data.\nsave file as train.csv\n'''\nPrompt # 6:\n'''\nfor validation_data move y_yes as first column.\nDrop y_no and y_yes columns from validation_data.\nsave file as validation.csv\n'''\n```", "```py\nPrompt #1:\n''' upload train.csv to S3 Bucket train/train.csv prefix.\nupload validation.csv to S3 Bucket validation/validation.csv prefix '''\nPrompt #2:\n# pull latest xgboost model as a CONTAINER\nPrompt #3:\n# create TrainingInput from s3 train/train.csv and validation/validation.csv\nPrompt #3:\n# create training job with hyper paramers max_depth=5, eta=0.2, gamma=4, min_child_weight=6, subsample=0.8, objective='binary:logistic', num_round=100\n```", "```py\nPrompt #1:\n# Deploy a model that's hosted behind a real-time endpoint\n```", "```py\nQ:Which state has most venues?\n```"]