["```py\nquestion = \"how old is the US president?\"\nraw_prompt_template = (\n \"You have access to search engine that provides you an \"\n \"information about fresh events and news given the query. \"\n \"Given the question, decide whether you need an additional \"\n \"information from the search engine (reply with 'SEARCH: \"\n \"<generated query>' or you know enough to answer the user \"\n \"then reply with 'RESPONSE <final response>').\\n\"\n \"Now, act to answer a user question:\\n{QUESTION}\"\n)\nprompt_template = PromptTemplate.from_template(raw_prompt_template)\nresult = (prompt_template | llm).invoke(question)\nprint(result,response)\n>> SEARCH: current age of US president\n```", "```py\nquestion1 = \"What is the capital of Germany?\"\nresult = (prompt_template | llm).invoke(question1)\nprint(result,response)\n>> RESPONSE: Berlin\n```", "```py\nquery = \"age of current US president\"\nsearch_result = (\n \"Donald Trump ' Age 78 years June 14, 1946\\n\"\n```", "```py\n \"Donald Trump 45th and 47th U.S. President Donald John Trump is an American \"\n \"politician, media personality, and businessman who has served as the 47th \"\n \"president of the United States since January 20, 2025\\. A member of the \"\n \"Republican Party, he previously served as the 45th president from 2017 to 2021\\. Wikipedia\"\n)\nraw_prompt_template = (\n \"You have access to search engine that provides you an \"\n \"information about fresh events and news given the query. \"\n \"Given the question, decide whether you need an additional \"\n \"information from the search engine (reply with 'SEARCH: \"\n \"<generated query>' or you know enough to answer the user \"\n \"then reply with 'RESPONSE <final response>').\\n\"\n \"Today is {date}.\"\n \"Now, act to answer a user question and \"\n \"take into account your previous actions:\\n\"\n \"HUMAN: {question}\\n\"\n \"AI: SEARCH: {query}\\n\"\n \"RESPONSE FROM SEARCH: {search_result}\\n\"\n)\nprompt_template = PromptTemplate.from_template(raw_prompt_template)\nresult = (prompt_template | llm).invoke(\n  {\"question\": question, \"query\": query, \"search_result\": search_result,\n \"date\": \"Feb 2025\"})\nprint(result.content)\n>>  RESPONSE: The current US President, Donald Trump, is 78 years old.\n```", "```py\nquery = \"current US president\"\nsearch_result = (\n \"Donald Trump 45th and 47th U.S.\"\n)\n```", "```py\nresult = (prompt_template | llm).invoke(\n  {\"question\": question, \"query\": query, \n \"search_result\": search_result, \"date\": \"Feb 2025\"})\nprint(result.content)\n>>  SEARCH: Donald Trump age\n```", "```py\nsearch_tool = {\n \"title\": \"google_search\",\n \"description\": \"Returns about fresh events and news from Google Search engine based on a query\",\n \"type\": \"object\",\n \"properties\": {\n \"query\": {\n \"description\": \"Search query to be sent to the search engine\",\n \"title\": \"search_query\",\n \"type\": \"string\"},\n   },\n \"required\": [\"query\"]\n}\nresult = llm.invoke(question, tools=[search_tool])\n```", "```py\nprint(result.tool_calls)\n>> [{'name': 'google_search', 'args': {'query': 'age of Donald Trump'}, 'id': '6ab0de4b-f350-4743-a4c1-d6f6fcce9d34', 'type': 'tool_call'}]\n```", "```py\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\ntool_result = ToolMessage(content=\"Donald Trump ' Age 78 years June 14, 1946\\n\", tool_call_id=step1.tool_calls[0][\"id\"])\nstep2 = llm.invoke([\n   HumanMessage(content=question), step1, tool_result], tools=[search_tool])\nassert len(step2.tool_calls) == 0\nprint(step2.content)\n>> Donald Trump is 78 years old.\n```", "```py\nllm_with_tools = llm.bind(tools=[search_tool])\nllm_with_tools.invoke(question)\n```", "```py\nllm.invoke(question, tools=[search_tool)\n```", "```py\nimport math\ndef mocked_google_search(query: str) -> str:\n print(f\"CALLED GOOGLE_SEARCH with query={query}\")\n return \"Donald Trump is a president of USA and he's 78 years old\"\n```", "```py\ndef mocked_calculator(expression: str) -> float:\n print(f\"CALLED CALCULATOR with expression={expression}\")\n if \"sqrt\" in expression:\n return math.sqrt(78*132)\n return 78*132\n```", "```py\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\ncalculator_tool = {\n \"title\": \"calculator\",\n \"description\": \"Computes mathematical expressions\",\n \"type\": \"object\",\n \"properties\": {\n \"expression\": {\n \"description\": \"A mathematical expression to be evaluated by a calculator\",\n \"title\": \"expression\",\n \"type\": \"string\"},\n  },\n \"required\": [\"expression\"]\n}\nprompt = ChatPromptTemplate.from_messages([\n   (\"system\", \"Always use a calculator for mathematical computations, and use Google Search for information about fresh events and news.\"), \n   MessagesPlaceholder(variable_name=\"messages\"),\n])\nllm_with_tools = llm.bind(tools=[search_tool, calculator_tool]).bind(prompt=prompt)\n```", "```py\nfrom typing import TypedDict\nfrom langgraph.graph import MessagesState, StateGraph, START, END\ndef invoke_llm(state: MessagesState):\n return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\ndef call_tools(state: MessagesState):\n   last_message = state[\"messages\"][-1]\n   tool_calls = last_message.tool_calls\n   new_messages = []\n for tool_call in tool_calls:\n if tool_call[\"name\"] == \"google_search\":\n       tool_result = mocked_google_search(**tool_call[\"args\"])\n       new_messages.append(ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"]))\n elif tool_call[\"name\"] == \"calculator\":\n       tool_result = mocked_calculator(**tool_call[\"args\"])\n       new_messages.append(ToolMessage(content=tool_result, tool_call_id=tool_call[\"id\"]))\n else:\n raise ValueError(f\"Tool {tool_call['name']} is not defined!\")\n return {\"messages\": new_messages}\ndef should_run_tools(state: MessagesState):\n   last_message = state[\"messages\"][-1]\n if last_message.tool_calls:\n return \"call_tools\"\n return END\n```", "```py\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"invoke_llm\", invoke_llm)\nbuilder.add_node(\"call_tools\", call_tools)\nbuilder.add_edge(START, \"invoke_llm\")\nbuilder.add_conditional_edges(\"invoke_llm\", should_run_tools)\nbuilder.add_edge(\"call_tools\", \"invoke_llm\")\ngraph = builder.compile()\nquestion = \"What is a square root of the current US president's age multiplied by 132?\"\nresult = graph.invoke({\"messages\": [HumanMessage(content=question)]})\nprint(result[\"messages\"][-1].content)\n>> CALLED GOOGLE_SEARCH with query=age of Donald Trump\nCALLED CALCULATOR with expression=78 * 132\nCALLED CALCULATOR with expression=sqrt(10296)\nThe square root of 78 multiplied by 132 (which is 10296) is approximately 101.47.\n```", "```py\nfrom langgraph.prebuilt import create_react_agent\nagent = create_react_agent(\n  llm=llm,\n  tools=[search_tool, calculator_tool],\n  prompt=system_prompt)\n```", "```py\nfrom langchain_community.tools import DuckDuckGoSearchRun\nsearch = DuckDuckGoSearchRun()\nprint(f\"Tool's name = {search.name}\")\n```", "```py\nprint(f\"Tool's name = {search.description}\")\nprint(f\"Tool's arg schema = f{search.args_schema}\")\n>> Tool's name = fduckduckgo_search\nTool's name = fA wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\nTool's arg schema = class 'langchain_community.tools.ddg_search.tool.DDGInput'\n```", "```py\nfrom langchain_community.tools.ddg_search.tool import DDGInput\nprint(DDGInput.__fields__)\n>> {'query': FieldInfo(annotation=str, required=True, description='search query to look up')}\n```", "```py\nquery = \"What is the weather in Munich like tomorrow?\"\nsearch_input = DDGInput(query=query)\nresult = search.invoke(search_input.dict())\nprint(result)\n```", "```py\nresult = llm.invoke(query, tools=[search])\nprint(result.tool_calls[0])\n>> {'name': 'duckduckgo_search', 'args': {'query': 'weather in Munich tomorrow'}, 'id': '222dc19c-956f-4264-bf0f-632655a6717d', 'type': 'tool_call'}\n```", "```py\nfrom langgraph.prebuilt import create_react_agent\nagent = create_react_agent(model=llm, tools=[search])\n```", "```py\nfor event in agent.stream({\"messages\": [(\"user\", query)]}):\n update = event.get(\"agent\", event.get(\"tools\", {}))\n for message in update.get(\"messages\", []):\n    message.pretty_print()\n>> ================================ Ai Message ==================================\nTool Calls:\n  duckduckgo_search (a01a4012-bfc0-4eae-9c81-f11fd3ecb52c)\n Call ID: a01a4012-bfc0-4eae-9c81-f11fd3ecb52c\n  Args:\n    query: weather in Munich tomorrow\n================================= Tool Message =================================\nName: duckduckgo_search\nThe temperature in Munich tomorrow in the early morning is 4 ° C… <TRUNCATED>\n================================== Ai Message ==================================\n```", "```py\nThe weather in Munich tomorrow will be 5°C with a 0% chance of rain in the morning.  The wind will blow at 11 km/h.  Later in the day, the high will be 53°F (approximately 12°C).  It will be clear in the early morning.\n```", "```py\nfrom langchain_community.agent_toolkits.openapi.toolkit import RequestsToolkit\nfrom langchain_community.utilities.requests import TextRequestsWrapper\ntoolkit = RequestsToolkit(\n   requests_wrapper=TextRequestsWrapper(headers={}),\n   allow_dangerous_requests=True,\n)\n```", "```py\nfor tool in toolkit.get_tools():\n print(tool.name)\n>> requests_get\nrequests_post\nrequests_patch\nrequests_put\nrequests_delete\n```", "```py\napi_spec = \"\"\"\nopenapi: 3.0.0\ninfo:\n title: Frankfurter Currency Exchange API\n version: v1\n description: API for retrieving currency exchange rates. Pay attention to the base currency and change it if needed.\nservers:\n - url: https://api.frankfurter.dev/v1\npaths:\n /v1/latest:\n   get:\n     summary: Get the latest exchange rates.\n     parameters:\n       - in: query\n         name: symbols\n         schema:\n           type: string\n         description: Comma-separated list of currency symbols to retrieve rates for. Example: CHF,GBP\n       - in: query\n         name: base\n         schema:\n```", "```py\n           type: string\n         description: The base currency for the exchange rates. If not provided, EUR is used as a base currency. Example: USD\n   /v1/{date}:\n   ...\n\"\"\"\n```", "```py\nsystem_message = (\n \"You're given the API spec:\\n{api_spec}\\n\"\n \"Use the API to answer users' queries if possible. \"\n)\nagent = create_react_agent(llm, toolkit.get_tools(), state_modifier=system_message.format(api_spec=api_spec))\nquery = \"What is the swiss franc to US dollar exchange rate?\"\nevents = agent.stream(\n   {\"messages\": [(\"user\", query)]},\n   stream_mode=\"values\",\n)\nfor event in events:\n   event[\"messages\"][-1].pretty_print()\n>> ============================== Human Message =================================\nWhat is the swiss franc to US dollar exchange rate?\n================================== Ai Message ==================================\nTool Calls:\n  requests_get (541a9197-888d-4ffe-a354-c726804ad7ff)\n Call ID: 541a9197-888d-4ffe-a354-c726804ad7ff\n  Args:\n    url: https://api.frankfurter.dev/v1/latest?symbols=CHF&base=USD\n```", "```py\n================================= Tool Message =================================\nName: requests_get\n{\"amount\":1.0,\"base\":\"USD\",\"date\":\"2025-01-31\",\"rates\":{\"CHF\":0.90917}}\n================================== Ai Message ==================================\nThe Swiss franc to US dollar exchange rate is 0.90917.\n```", "```py\nimport math\nfrom langchain_core.tools import tool\nimport numexpr as ne\n@tool\ndef calculator(expression: str) -> str:\n \"\"\"Calculates a single mathematical expression, incl. complex numbers.\n\n```", "```py\n   Always add * to operations, examples:\n     73i -> 73*i\n     7pi**2 -> 7*pi**2\n   \"\"\"\n   math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n   result = ne.evaluate(expression.strip(), local_dict=math_constants)\n   return str(result)\n```", "```py\nfrom langchain_core.tools import BaseTool\nassert isinstance(calculator, BaseTool)\nprint(f\"Tool schema: {calculator.args_schema.model_json_schema()}\")\n>> Tool schema: {'description': 'Calculates a single mathematical expression, incl. complex numbers.\\n\\nAlways add * to operations, examples:\\n  73i -> 73*i\\n  7pi**2 -> 7*pi**2', 'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}\n```", "```py\nquery = \"How much is 2+3i squared?\"\nagent = create_react_agent(llm, [calculator])\nfor event in agent.stream({\"messages\": [(\"user\", query)]}, stream_mode=\"values\"):\n   event[\"messages\"][-1].pretty_print()\n>> ===============================Human Message =================================\nHow much is 2+3i squared?\n================================== Ai Message ==================================\nTool Calls:\n```", "```py\n  calculator (9b06de35-a31c-41f3-a702-6e20698bf21b)\n Call ID: 9b06de35-a31c-41f3-a702-6e20698bf21b\n  Args:\n    expression: (2+3*i)**2\n================================= Tool Message =================================\nName: calculator\n(-5+12j)\n================================== Ai Message ==================================\n(2+3i)² = -5+12i.\n```", "```py\nquestion = \"What is a square root of the current US president's age multiplied by 132?\"\nsystem_hint = \"Think step-by-step. Always use search to get the fresh information about events or public facts that can change over time.\"\nagent = create_react_agent(\n   llm, [calculator, search],\n   state_modifier=system_hint)\nfor event in agent.stream({\"messages\": [(\"user\", question)]}, stream_mode=\"values\"):\n   event[\"messages\"][-1].pretty_print()\nprint(event[\"messages\"][-1].content)\n>> The square root of Donald Trump's age multiplied by 132 is approximately 101.47.\n```", "```py\nfrom langchain_core.runnables import RunnableLambda, RunnableConfig\nfrom langchain_core.tools import tool, convert_runnable_to_tool\ndef calculator(expression: str) -> str:\n   math_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\n   result = ne.evaluate(expression.strip(), local_dict=math_constants)\n return str(result)\ncalculator_with_retry = RunnableLambda(calculator).with_retry(\n   wait_exponential_jitter=True,\n   stop_after_attempt=3,\n)\ncalculator_tool = convert_runnable_to_tool(\n   calculator_with_retry,\n   name=\"calculator\",\n```", "```py\n   description=(\n \"Calculates a single mathematical expression, incl. complex numbers.\"\n \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n \"7pi**2 -> 7*pi**2\"\n   ),\n   arg_types={\"expression\": \"str\"},\n)\n```", "```py\ncalculator_tool = convert_runnable_to_tool(\n   calculator_with_retry,\n   name=\"calculator\",\n   description=(\n \"Calculates a single mathematical expression, incl. complex numbers.\"\n \"'\\nAlways add * to operations, examples:\\n73i -> 73*i\\n\"\n \"7pi**2 -> 7*pi**2\"\n   ),\n   arg_types={\"expression\": \"str\"},\n)\n```", "```py\nllm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n>> {'name': 'calculator',\n 'args': {'__arg1': '(2+3*i)**2'},\n 'id': '46c7e71c-4092-4299-8749-1b24a010d6d6',\n 'type': 'tool_call'}\n```", "```py\nfrom pydantic import BaseModel, Field\nfrom langchain_core.runnables import RunnableConfig\nclass CalculatorArgs(BaseModel):\n   expression: str = Field(description=\"Mathematical expression to be evaluated\")\ndef calculator(state: CalculatorArgs, config: RunnableConfig) -> str:\n   expression = state[\"expression\"]\n   math_constants = config[\"configurable\"].get(\"math_constants\", {})\n   result = ne.evaluate(expression.strip(), local_dict=math_constants)\n return str(result)\n```", "```py\nassert isinstance(calculator_tool, BaseTool)\nprint(f\"Tool name: {calculator_tool.name}\")\nprint(f\"Tool description: {calculator_tool.description}\")\nprint(f\"Args schema: {calculator_tool.args_schema.model_json_schema()}\")\n>> Tool name: calculator\nTool description: Calculates a single mathematical expression, incl. complex numbers.'\nAlways add * to operations, examples:\n73i -> 73*i\n7pi**2 -> 7*pi**2\nArgs schema: {'properties': {'expression': {'title': 'Expression', 'type': 'string'}}, 'required': ['expression'], 'title': 'calculator', 'type': 'object'}\n```", "```py\ntool_call = llm.invoke(\"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\nprint(tool_call)\n>> {'name': 'calculator', 'args': {'expression': '(2+3*i)**2'}, 'id': 'f8be9cbc-4bdc-4107-8cfb-fd84f5030299', 'type': 'tool_call'}\n```", "```py\nmath_constants = {\"pi\": math.pi, \"i\": 1j, \"e\": math.exp}\nconfig = {\"configurable\": {\"math_constants\": math_constants}}\ncalculator_tool.invoke(tool_call[\"args\"], config=config)\n>> (-5+12j)\n```", "```py\nfrom langchain_core.tools import StructuredTool\ncalculator_tool = StructuredTool.from_function(\n   name=\"calculator\",\n   description=(\n \"Calculates a single mathematical expression, incl. complex numbers.\"),\n   func=calculator,\n   args_schema=CalculatorArgs\n)\ntool_call = llm.invoke(\n \"How much is (2+3i)**2\", tools=[calculator_tool]).tool_calls[0]\n```", "```py\ndef _handle_tool_error(\n    e: ToolException,\n    *,\n    flag: Optional[Union[Literal[True], str, Callable[[ToolException], str]]],\n) -> str:\n if isinstance(flag, bool):\n        content = e.args[0] if e.args else \"Tool execution error\"\n elif isinstance(flag, str):\n        content = flag\n elif callable(flag):\n        content = flag(e)\n else:\n        msg = (\n f\"Got an unexpected type of `handle_tool_error`. Expected bool, str \"\n f\"or callable. Received: {flag}\"\n        )\n raise ValueError(msg)  # noqa: TRY004\n return content\n```", "```py\nfrom langchain_core.tools import StructuredTool\ndef calculator(expression: str) -> str:\n \"\"\"Calculates a single mathematical expression, incl. complex numbers.\"\"\"\n return str(ne.evaluate(expression.strip(), local_dict={}))\n```", "```py\ncalculator_tool = StructuredTool.from_function(\n   func=calculator,\n   handle_tool_error=True\n)\nagent = create_react_agent(\n   llm, [calculator_tool])\nfor event in agent.stream({\"messages\": [(\"user\", \"How much is (2+3i)^2\")]}, stream_mode=\"values\"):\n   event[\"messages\"][-1].pretty_print()\n>> ============================== Human Message =================================\nHow much is (2+3i)^2\n================================== Ai Message ==================================\nTool Calls:\n  calculator (8bfd3661-d2e1-4b8d-84f4-0be4892d517b)\n Call ID: 8bfd3661-d2e1-4b8d-84f4-0be4892d517b\n  Args:\n    expression: (2+3i)^2\n================================= Tool Message =================================\nName: calculator\nError: SyntaxError('invalid decimal literal', ('<expr>', 1, 4, '(2+3i)^2', 1, 4))\n Please fix your mistakes.\n================================== Ai Message ==================================\n(2+3i)^2 is equal to -5 + 12i.  I tried to use the calculator tool, but it returned an error. I will calculate it manually for you.\n(2+3i)^2 = (2+3i)*(2+3i) = 2*2 + 2*3i + 3i*2 + 3i*3i = 4 + 6i + 6i - 9 = -5 + 12i\n```", "```py\nfrom pydantic import BaseModel, Field\nclass Step(BaseModel):\n \"\"\"A step that is a part of the plan to solve the task.\"\"\"\n   step: str = Field(description=\"Description of the step\")\nclass Plan(BaseModel):\n \"\"\"A plan to solve the task.\"\"\"\n   steps: list[Step]\n```", "```py\nprompt = PromptTemplate.from_template(\n \"Prepare a step-by-step plan to solve the given task.\\n\"\n \"TASK:\\n{task}\\n\"\n)\nresult = (prompt | llm.with_structured_output(Plan)).invoke(\n \"How to write a bestseller on Amazon about generative AI?\")\n```", "```py\nassert isinstance(result, Plan)\nprint(f\"Amount of steps: {len(result.steps)}\")\nfor step in result.steps:\n print(step.step)\n break\n>> Amount of steps: 21\n**1\\. Idea Generation and Validation:**\n```", "```py\nplan_schema = {\n \"type\": \"ARRAY\",\n \"items\": {\n \"type\": \"OBJECT\",\n \"properties\": {\n \"step\": {\"type\": \"STRING\"},\n         },\n     },\n}\nquery = \"How to write a bestseller on Amazon about generative AI?\"\nresult = (prompt | llm.with_structured_output(schema=plan_schema, method=\"json_mode\")).invoke(query)\n```", "```py\nassert(isinstance(result, list))\nprint(f\"Amount of steps: {len(result)}\")\nprint(result[0])\n>> Amount of steps: 10\n{'step': 'Step 1: Define your niche and target audience. Generative AI is a broad topic. Focus on a specific area, like generative AI in marketing, art, music, or writing. Identify your ideal reader (such as  marketers, artists, developers).'}\n```", "```py\nfrom langchain_core.output_parsers import JsonOutputParser\nllm_json = ChatVertexAI(\n  model_name=\"gemini-1.5-pro-002\", response_mime_type=\"application/json\",\n  response_schema=plan_schema)\nresult = (prompt | llm_json | JsonOutputParser()).invoke(query)\nassert(isinstance(result, list))\n```", "```py\nfrom langchain_core.output_parsers import StrOutputParser\nresponse_schema = {\"type\": \"STRING\", \"enum\": [\"positive\", \"negative\", \"neutral\"]}\nprompt = PromptTemplate.from_template(\n \"Classify the tone of the following customer's review:\"\n \"\\n{review}\\n\"\n)\nreview = \"I like this movie!\"\nllm_enum = ChatVertexAI(model_name=\"gemini-1.5-pro-002\", response_mime_type=\"text/x.enum\", response_schema=response_schema)\nresult = (prompt | llm_enum | StrOutputParser()).invoke(review)\nprint(result)\n>> positive\n```", "```py\nfrom langgraph.prebuilt import ToolNode, tools_condition\ndef invoke_llm(state: MessagesState):\n return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"invoke_llm\", invoke_llm)\nbuilder.add_node(\"tools\", ToolNode([search, calculator]))\nbuilder.add_edge(START, \"invoke_llm\")\nbuilder.add_conditional_edges(\"invoke_llm\", tools_condition)\nbuilder.add_edge(\"tools\", \"invoke_llm\")\ngraph = builder.compile()\n```", "```py\nexamples = [\n \"I signed my contract 2 years ago\",\n \"I started the deal with your company in February last year\",\n \"Our contract started on March 24th two years ago\"\n]\n```", "```py\nfrom datetime import date, timedelta\n@tool\ndef get_date(year: int, month: int = 1, day: int = 1) -> date:\n \"\"\"Returns a date object given year, month and day.\n```", "```py\n     Default month and day are 1 (January) and 1.\n     Examples in YYYY-MM-DD format:\n       2023-07-27 -> date(2023, 7, 27)\n       2022-12-15 -> date(2022, 12, 15)\n       March 2022 -> date(2022, 3)\n       2021 -> date(2021)\n   \"\"\"\n return date(year, month, day).isoformat()\n@tool\ndef time_difference(days: int = 0, weeks: int = 0, months: int = 0, years: int = 0) -> date:\n \"\"\"Returns a date given a difference in days, weeks, months and years relative to the current date.\n\n   By default, days, weeks, months and years are 0.\n   Examples:\n     two weeks ago -> time_difference(weeks=2)\n     last year -> time_difference(years=1)\n   \"\"\"\n   dt = date.today() - timedelta(days=days, weeks=weeks)\n   new_year = dt.year+(dt.month-months) // 12 - years\n   new_month = (dt.month-months) % 12\n return dt.replace(year=new_year, month=new_month)\n```", "```py\nfrom langchain_google_vertexai import ChatVertexAI\nllm = ChatVertexAI(model=\"gemini-1.5-pro-002\")\nagent = create_react_agent(\n   llm, [get_date, time_difference], prompt=\"Extract the starting date of a contract. Current year is 2025.\")\nfor example in examples:\n result = agent.invoke({\"messages\": [(\"user\", example)]})\n print(example, result[\"messages\"][-1].content)\n```", "```py\n>> I signed my contract 2 years ago The contract started on 2023-02-07.\nI started the deal with your company in February last year The contract started on 2024-02-01.\nOur contract started on March 24th two years ago The contract started on 2023-03-24\n```", "```py\nfrom pydantic import BaseModel, Field\nfrom langchain_core.prompts import ChatPromptTemplate\nclass Plan(BaseModel):\n \"\"\"Plan to follow in future\"\"\"\n   steps: list[str] = Field(\n       description=\"different steps to follow, should be in sorted order\"\n   )\nsystem_prompt_template = (\n \"For the given task, come up with a step by step plan.\\n\"\n \"This plan should involve individual tasks, that if executed correctly will \"\n \"yield the correct answer. Do not add any superfluous steps.\\n\"\n \"The result of the final step should be the final answer. Make sure that each \"\n \"step has all the information needed - do not skip steps.\"\n)\nplanner_prompt = ChatPromptTemplate.from_messages(\n   [(\"system\", system_prompt_template),\n```", "```py\n    (\"user\", \"Prepare a plan how to solve the following task:\\n{task}\\n\")])\nplanner = planner_prompt | ChatVertexAI(\n   model_name=\"gemini-1.5-pro-002\", temperature=1.0\n).with_structured_output(Plan)\n```", "```py\nfrom langchain.agents import load_tools\ntools = load_tools(\n tool_names=[\"ddg-search\", \"arxiv\", \"wikipedia\"],\n llm=llm\n) + [calculator_tool]\n```", "```py\nclass PlanState(TypedDict):\n   task: str\n   plan: Plan\n   past_steps: Annotated[list[str], operator.add]\n   final_response: str\n   past_steps: list[str]\ndef get_current_step(state: PlanState) -> int:\n \"\"\"Returns the number of current step to be executed.\"\"\"\n return len(state.get(\"past_steps\", []))\n\ndef get_full_plan(state: PlanState) -> str:\n \"\"\"Returns formatted plan with step numbers and past results.\"\"\"\n full_plan = []\n for i, step in enumerate(state[\"plan\"]):\n   full_step = f\"# {i+1}. Planned step: {step}\\n\"\n if i < get_current_step(state):\n     full_step += f\"Result: {state['past_steps'][i]}\\n\"\n   full_plan.append(full_step)\n return \"\\n\".join(full_plan)\n```", "```py\nfrom typing import Literal\nfrom langgraph.graph import StateGraph, START, END\nfinal_prompt = PromptTemplate.from_template(\n \"You're a helpful assistant that has executed on a plan.\"\n \"Given the results of the execution, prepare the final response.\\n\"\n \"Don't assume anything\\nTASK:\\n{task}\\n\\nPLAN WITH RESUlTS:\\n{plan}\\n\"\n \"FINAL RESPONSE:\\n\"\n)\nasync def _build_initial_plan(state: PlanState) -> PlanState:\n plan = await planner.ainvoke(state[\"task\"])\n return {\"plan\": plan}\nasync def _run_step(state: PlanState) -> PlanState:\n plan = state[\"plan\"]\n current_step = get_current_step(state)\n step = await execution_agent.ainvoke({\"plan\": get_full_plan(plan), \"step\": plan.steps[current_step], \"task\": state[\"task\"]})\n return {\"past_steps\": [step[\"messages\"][-1].content]}\nasync def _get_final_response(state: PlanState) -> PlanState:\n final_response = await (final_prompt | llm).ainvoke({\"task\": state[\"task\"], \"plan\": get_full_plan(state)})\n return {\"final_response\": final_response}\ndef _should_continue(state: PlanState) -> Literal[\"run\", \"response\"]:\n if get_current_step(plan) < len(state[\"plan\"].steps):\n return \"run\"\n return \"final_response\"\n```", "```py\nbuilder = StateGraph(PlanState)\nbuilder.add_node(\"initial_plan\", _build_initial_plan)\nbuilder.add_node(\"run\", _run_step)\n```", "```py\nbuilder.add_node(\"response\", _get_final_response)\nbuilder.add_edge(START, \"initial_plan\")\nbuilder.add_edge(\"initial_plan\", \"run\")\nbuilder.add_conditional_edges(\"run\", _should_continue)\nbuilder.add_edge(\"response\", END)\ngraph = builder.compile()\nfrom IPython.display import Image, display\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n```", "```py\ntask = \"Write a strategic one-pager of building an AI startup\"\nresult = await graph.ainvoke({\"task\": task})\n```"]