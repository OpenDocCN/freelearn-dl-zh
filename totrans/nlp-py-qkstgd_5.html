<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Modern Methods for Classification</h1>
                </header>
            
            <article>
                
<p>We now know how to convert text strings to numerical vectors that capture some meaning. In this chapter, we will look at how to use those with embedding. Embedding is the more frequently used term for word vectors and numerical representations.</p>
<p>In this chapter, we are still following the broad outline from our first, that is, text→ representations → models<span>→</span> evaluation <span>→</span> deployment. </p>
<p><span>We will continue working with text classification as our example task. This is mainly because it's a simple task for demonstration, but we can also extend almost all of the ideas in this book to solve other problems. The main focus ahead, however, is machine l</span><span>earning for text classification.</span></p>
<p>To sum up, in this chapter we will be looking at the following topics:</p>
<ul>
<li>Sentiment analysis as a specific class and example of text classification</li>
<li>Simple classifiers and how to optimize them for your datasets</li>
<li>Ensemble methods</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning for text</h1>
                </header>
            
            <article>
                
<p>There are at least 10 to 20 machine learning techniques that are well known in the community, ranging from SVMs to several regressions and gradient boosting machines. We will select a small taste of these. </p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/290de774-4008-41d8-91dd-3e7027b94f3b.png"/></div>
<p><span>Source:</span><span> </span><a href="https://www.kaggle.com/surveys/2017">https://www.kaggle.com/surveys/2017.</a><a href="https://www.kaggle.com/surveys/2017"/></p>
<p>The preceding graph shows the most popular machine learning techniques used by Kagglers.</p>
<p class="mce-root"/>
<p><span>We met <strong>Logistic Regression</strong> in the first chapter while working the 20 newsgroups dataset. We will revisit <strong>Logistic Regression</strong> and introduce <strong>Naive Bayes</strong>, <strong>SVM</strong>, <strong>Decision Trees</strong>, <strong>Random Forests</strong></span>, and <strong>XgBoost</strong><span>.</span> <strong>XgBoost</strong> <span>is a popular algorithm used by several Kaggle winners to achieve</span> award-winning <span>results. We will use the</span> scikit<span>-learn and</span> XGBoost <span>packages in Python to see the previous example in code.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis as text classification </h1>
                </header>
            
            <article>
                
<p>A popular use of classifiers is in sentiment analysis. The end objective here is to determine the subjective value of a text document, which is essentially how positive or negative the content of a text document is. This is particularly handy for quickly understanding what the tone is of, say, the movie you are producing or the book you want to read.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple classifiers</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's begin by simply trying a few machine learning classifiers such as Logistic Regression, Naive Bayes, and Decision Trees. We'll then move on and try the Random Forest and Extra Trees classifiers. For all of these implementations, we won't use anything except scikit-learn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing simple classifiers</h1>
                </header>
            
            <article>
                
<p class="mce-root">We can tweak these simple classifiers to improve their performance. For this, the most common method is to try several slightly different versions of the classifier. We do this by changing the parameters of our classifier.</p>
<p class="mce-root">We will learn how to automate this search process for the best classifier parameters using <kbd>GridSearch</kbd> and <kbd>RandomizedSearch</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensemble methods</h1>
                </header>
            
            <article>
                
<p class="mce-root">Having an ensemble of several different classifiers means we will be using a group of models. Ensembling is a very popular and easy to understand machine learning technique, and is part of almost every winning Kaggle competition.</p>
<p class="mce-root">Despite initial concerns that this process might be slow, some teams working on commercial software have begun using ensemble methods in production software as well. This is because it requires very little overhead, is easy to parallelize, and allows for a built-in fallback of using a single model.</p>
<p class="mce-root"/>
<p class="mce-root">We will look at some of the simplest ensembling techniques based on simple majority, also known as voting ensemble, and will then build using that.</p>
<p class="mce-root">In summary, this machine learning for NLP section covers simple classifiers, parameter optimization, and ensemble methods.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p>We will programmatically download the data using Python's standard inbuilt toolkit called <kbd>urlretrieve</kbd> from <kbd>urllib.request</kbd>. The following is our download-from-internet piece:</p>
<pre>from pathlib import Path<br/>import pandas as pd<br/>import gzip<br/>from urllib.request import urlretrieve<br/>from tqdm import tqdm<br/>import os<br/>import numpy as np<br/><br/>class TqdmUpTo(tqdm):<br/>    def update_to(self, b=1, bsize=1, tsize=None):<br/>        if tsize is not None: self.total = tsize<br/>        self.update(b * bsize - self.n)</pre>
<p>If you are using the fastAI environment, all of these imports work. The second block simply sets up Tqdm for us to visualize the download progress. Let's now download the data using <kbd>urlretrieve</kbd>, as follows:</p>
<pre>def get_data(url, filename):<br/>    """<br/>    Download data if the filename does not exist already<br/>    Uses Tqdm to show download progress<br/>    """<br/>    if not os.path.exists(filename):<br/><br/>        dirname = os.path.dirname(filename)<br/>        if not os.path.exists(dirname):<br/>            os.makedirs(dirname)<br/><br/>        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:<br/>            urlretrieve(url, filename, reporthook=t.update_to)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Let's download some data, as follows:</p>
<pre>data_url = 'http://files.fast.ai/data/aclImdb.tgz'<br/>get_data(data_url, 'data/imdb.tgz')</pre>
<p>Let's now extract the preceding files and see what the directory contains:</p>
<pre>data_path = Path(os.getcwd())/'data'/'imdb'/'aclImdb'<br/>assert data_path.exists()<br/>for pathroute in os.walk(data_path):<br/>    next_path = pathroute[1]<br/>    for stop in next_path:<br/>        print(stop)</pre>
<p>Notice that we prefer to use <kbd>Path from pathlib</kbd> over the <kbd>os.path</kbd> functionality. This make it more platform-agnostic as well as Pythonic. This really badly written utility tells us that there are at least two folders: <kbd>train</kbd> and <kbd>test</kbd>. Each of these folders, in turn, has at least three folders, as follows:</p>
<pre>Test<br/> |- all<br/> |- neg<br/> |- pos</pre>
<pre><br/> Train<br/> |- all<br/> |- neg<br/> |- pos<br/> |- unsup</pre>
<p>The <kbd>pos</kbd> and <kbd>neg</kbd> folders contain reviews, which are positive and negative respectively. The <kbd>unsup</kbd> folder stands for unsupervised. These folders are useful for building language models, especially for deep learning, but we will not use that here. Similarly, the <kbd>all</kbd> folder is redundant because those reviews are repeated in either the <kbd>pos</kbd> or <kbd>neg</kbd> folder.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading data</h1>
                </header>
            
            <article>
                
<p>Let's read the following data into a Pandas <kbd>DataFrame</kbd> with the appropriate labels:</p>
<pre>train_path = data_path/'train'<br/>test_path = data_path/'test'<br/><br/>def read_data(dir_path):<br/>    """read data into pandas dataframe"""<br/>    <br/>    def load_dir_reviews(reviews_path):<br/>        files_list = list(reviews_path.iterdir())<br/>        reviews = []<br/>        for filename in files_list:<br/>            f = open(filename, 'r', encoding='utf-8')<br/>            reviews.append(f.read())<br/>        return pd.DataFrame({'text':reviews})<br/>        <br/>    <br/>    pos_path = dir_path/'pos'<br/>    neg_path = dir_path/'neg'<br/>    <br/>    pos_reviews, neg_reviews = load_dir_reviews(pos_path), load_dir_reviews(neg_path)<br/>    <br/>    pos_reviews['label'] = 1<br/>    neg_reviews['label'] = 0<br/>    <br/>    merged = pd.concat([pos_reviews, neg_reviews])<br/>    merged.reset_index(inplace=True)<br/>    <br/>    return merged</pre>
<p>This function reads the files for a particular <kbd>train</kbd> or <kbd>test</kbd> split, both positive and negative, for the IMDb dataset. Each split is a <kbd>DataFrame</kbd> with two columns: <kbd>text</kbd> and <kbd>label</kbd>. The <kbd>label</kbd> column gives us our target value, or <kbd>y</kbd>, as follows:</p>
<pre>train = read_data(train_path)<br/>test = read_data(test_path)<br/><br/>X_train, y_train = train['text'], train['label']<br/>X_test, y_test = test['text'], test['label']</pre>
<p>We can now read the data in the corresponding <kbd>DataFrame</kbd> and then split it into the following four variables: <kbd>X_train</kbd>, <kbd>y_train</kbd>, <kbd>X_test</kbd>, and  <kbd>y_test</kbd>.</p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple classifiers</h1>
                </header>
            
            <article>
                
<p>In order to try some of our classifiers, let's get the basic imports out of the way, as shown in the following code. Here, we will be importing the rest of the classifiers as we need them. This ability to import things later is important for ensuring we don't import too many unnecessary components into memory:</p>
<pre>from sklearn.pipeline import Pipeline<br/>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer</pre>
<p>Since this section is simply for illustration purposes, we will use the simplest feature extraction steps, which are as follows:</p>
<ul>
<li>Bag of words</li>
<li>TF-IDF</li>
</ul>
<p class="mce-root"/>
<p>We encourage you to try the code examples with better text vectorization (for example, using direct GloVe or word2vec lookups).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logistic regression</h1>
                </header>
            
            <article>
                
<p>Let's now simply replicate the simple logistic regression we did in <a href="5625152b-6870-44b1-a39f-5a79bcc675d9.xhtml"/><a href="5625152b-6870-44b1-a39f-5a79bcc675d9.xhtml" target="_blank">Chapter 1</a>, <em>Getting Started with Text Classification</em>, but on our custom dataset, as follows:</p>
<pre>from sklearn.linear_model import LogisticRegression as LR<br/>lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])</pre>
<p>As you can see in the preceding snippet, <kbd>lr_clf</kbd> becomes our classifier pipeline. We saw the pipeline in our introductory section. A pipeline allows us to queue multiple operations in one single Python object.</p>
<div class="packt_tip"><br/>
We are able to call functions such as <kbd>fit</kbd>, <kbd>predict</kbd>, and <kbd>fit_transform</kbd> on our <kbd>Pipeline</kbd> objects because a pipeline automatically calls the corresponding function of the last component in the list. </div>
<pre>lr_clf.fit(X=X_train, y=y_train) # note that .fit function calls are inplace, and the Pipeline is not re-assigned</pre>
<p>As mentioned earlier, we are calling the <kbd>predict</kbd> function on our pipeline. The test reviews go through under the same pre-processing steps, <kbd>CountVectorizer()</kbd> and <kbd>TfidfTransformer()</kbd>, as the reviews during training, as shown in the following snippet:</p>
<pre>lr_predicted = lr_clf.predict(X_test)</pre>
<p>The ease and simplicity of this process makes <kbd>Pipeline</kbd> one of the most frequently used abstractions in software-grade machine learning. However, users might prefer to execute each step independently, or build their own pipeline equivalents in some research or experimentation use cases:</p>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython3">
<pre><span class="n">lr_acc</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">lr_predicted</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">lr_predicted</span><span class="p">)</span>
<span class="n">lr_acc # </span>0.88316</pre></div>
</div>
</div>
</div>
</div>
<div class="text_cell_render rendered_html">
<p>How do we find our model accuracy? Well, let's take a quick look at what is happening in the preceding line.</p>
<p>Consider that our predictions are [1, 1, 1] and the ground truth is [1, 0, 1]. The equality would return a simple list of Boolean objects, such as <kbd>[True, False, True]</kbd>. When we sum a Boolean list in Python, it returns the number of <kbd>True</kbd> cases, giving us the exact number of times our model made correct predictions.</p>
</div>
<div class="text_cell_render rendered_html packt_tip">
<p>Dividing this value by the total number of predictions made (or, equally, the number of test reviews) gives us our accuracy.</p>
</div>
<p>Let's write the previous two-line logic into a simple, lightweight function to calculate accuracy, as shown in the following snippet. This would prevent us from repeating the logic:</p>
<pre>def imdb_acc(pipeline_clf):<br/>    predictions = pipeline_clf.predict(X_test)<br/>    assert len(y_test) == len(predictions)<br/>    return sum(predictions == y_test)/len(y_test), predictions</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing stop words</h1>
                </header>
            
            <article>
                
<p>By simply passing a flag to the <kbd>CountVectorizer</kbd> step, we can remove the most common stop words. We will specify the language in which the stop words we want to remove are written. In the following case, that's <kbd>english</kbd>:</p>
<pre>lr_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf',LR())])<br/>lr_clf.fit(X=X_train, y=y_train)<br/>lr_acc, lr_predictions = imdb_acc(lr_clf)<br/>lr_acc # 0.879</pre>
<p>As you can see, this is not very helpful in improving our accuracy. This would indicate that the noise added by stop words is being removed or neglected by the classifier itself.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Increasing ngram range</h1>
                </header>
            
            <article>
                
<p>Let's now try to improve the information available to the classifier by including bigrams and trigrams, as follows:</p>
<pre>lr_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',LR())])<br/>lr_clf.fit(X=X_train, y=y_train)<br/>lr_acc, lr_predictions = imdb_acc(lr_clf)<br/>lr_acc # 0.86596</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multinomial Naive Bayes</h1>
                </header>
            
            <article>
                
<p>Let's initialize the classifier in a manner identical to our logistic regression classifier, as follows:</p>
<pre>from sklearn.naive_bayes import MultinomialNB as MNB<br/>mnb_clf = Pipeline([('vect', CountVectorizer()), ('clf',MNB())])</pre>
<p>The previous command will measure performance on the following:</p>
<pre>mnb_clf.fit(X=X_train, y=y_train)<br/>mnb_acc, mnb_predictions = imdb_acc(mnb_clf)<br/>mnb_acc # 0.81356</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding TF-IDF</h1>
                </header>
            
            <article>
                
<p>Now, let's try the preceding model with TF-IDF, as another step after bag-of-words (unigrams), as follows:</p>
<pre>mnb_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',MNB())])<br/>mnb_clf.fit(X=X_train, y=y_train)<br/>mnb_acc, mnb_predictions = imdb_acc(mnb_clf)<br/>mnb_acc # 0.82956</pre>
<p>This is better than our previous value, but let's see what else we can do to improve this further.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing stop words</h1>
                </header>
            
            <article>
                
<p>Let's now remove the stop words for English again, by simply passing <kbd>english</kbd> to the tokenizer as follows:</p>
<pre>mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf',MNB())])<br/>mnb_clf.fit(X=X_train, y=y_train)<br/>mnb_acc, mnb_predictions = imdb_acc(mnb_clf)<br/>mnb_acc # 0.82992 </pre>
<p>This helps improve performance, but only marginally. We might be better off simply keeping in the stop words for other classifiers that we try.</p>
<p>As a last manual experiment, let's try adding bigrams and unigrams, as we did lfor ogistic regression, as follows:</p>
<pre>mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',MNB())])<br/>mnb_clf.fit(X=X_train, y=y_train)<br/>mnb_acc, mnb_predictions = imdb_acc(mnb_clf)<br/>mnb_acc # 0.8572</pre>
<p>This is significantly better than the previous Multinomial Naive Bayes performance, but not as good as the performance of our logistic regression classifier, which was close to achieving 88% accuracy.</p>
<p>Let's now try something specific to Bayesian classifiers. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing fit prior to false</h1>
                </header>
            
            <article>
                
<p>Increasing <kbd>ngram_range</kbd> did work for us, but changing <kbd>prior</kbd> from <kbd>uniform</kbd> to fitting it (by changing <kbd>fit_prior</kbd> to <kbd>False</kbd>) did not help at all, as follows:</p>
<pre>mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',MNB(fit_prior=False))])<br/>mnb_clf.fit(X=X_train, y=y_train)<br/>mnb_acc, mnb_predictions = imdb_acc(mnb_clf)<br/>mnb_acc # 0.8572</pre>
<p>We have now thought of each combination that might improve our performance. Note that this approach is tedious, and also error-prone because it relies too greatly on human intuition.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support vector machines</h1>
                </header>
            
            <article>
                
<p><strong>Support vector machines</strong> (<strong>SVM</strong>) continue to remain a hugely popular machine learning technique, having made its way from the industry to classrooms and then back. In addition to several forms of regression, SVM is one of the techniques that forms the backbone of the multi-billion-dollar online ad targeting industry.</p>
<p class="mce-root"/>
<p>In academia, work such as that by T Joachim (<a href="https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf">https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf</a>) recommends support vector classifiers for text classification.</p>
<p>It's difficult to estimate whether it will be equally effective for us based on such literature, mainly due to a difference in the dataset and pre-processing steps. Let's give it a shot nevertheless:</p>
<pre>from sklearn.svm import SVC<br/>svc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',SVC())])<br/>svc_clf.fit(X=X_train, y=y_train)<br/>svc_acc, svc_predictions = imdb_acc(svc_clf)<br/>print(svc_acc) # 0.6562</pre>
<p>While SVM works best with linearly separable data (as we can see, our text is not linearly separable), it's still worth giving it a try for completeness.<br/>
<br/>
In the previous example, SVM did not perform well, and it also took a really long time to train (~150x) when compared to other classifiers. We will not look at SVM for this particular dataset again.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision trees</h1>
                </header>
            
            <article>
                
<p>Decision trees are simple, intuitive tools for classification and regression alike. They often resemble a flow chart of decisions when seen visually, hence the name decision tree. We will reuse our pipeline, simply using the <kbd>DecisionTreeClassifier</kbd> as our main classification technique, as follows:</p>
<pre>from sklearn.tree import DecisionTreeClassifier as DTC<br/>dtc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',DTC())])<br/>dtc_clf.fit(X=X_train, y=y_train)<br/>dtc_acc, dtc_predictions = imdb_acc(dtc_clf)<br/>dtc_acc # 0.7028</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forest classifier</h1>
                </header>
            
            <article>
                
<p>Let's now try the first ensemble classifier. The forest in Random forest classifiers comes from the fact that each instance of this classifier consists of several decision trees. The Random in Random forests comes from the fact that each tree selects a finite number of features from all features at random, as shown in the following code:</p>
<pre>from sklearn.ensemble import RandomForestClassifier as RFC<br/>rfc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',RFC())])<br/>rfc_clf.fit(X=X_train, y=y_train)<br/>rfc_acc, rfc_predictions = imdb_acc(rfc_clf)<br/>rfc_acc # 0.7226</pre>
<p>Although considered to be very powerful when used in most machine learning tasks, the Random Forest approach doesn't do particularly well in our case. This is partially because of our rather crude feature extraction.</p>
<p>Approaches such as decision trees, RFC, and Extra trees classifiers don't do well in high-dimensional spaces such as text. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extra trees classifier</h1>
                </header>
            
            <article>
                
<p>The Extra in Extra Trees comes from the idea that it is extremely randomized. While the tree splits in a Random Forest classifier are effectively deterministic, they are randomized in the Extra Trees classifier. This changes the bias-variance trade-off in cases of high-dimensional data such as ours (where every word is effectively a dimension or classifier). The following snippet shows the classifier in action:</p>
<pre>from sklearn.ensemble import ExtraTreesClassifier as XTC<br/>xtc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',XTC())])<br/>xtc_clf.fit(X=X_train, y=y_train)<br/>xtc_acc, xtc_predictions = imdb_acc(xtc_clf)<br/>xtc_acc # 0.75024</pre>
<p>As you can see, this change works in our favor here, but this is not universally true. Results will vary across datasets as well as feature extraction pipelines.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimizing our classifiers</h1>
                </header>
            
            <article>
                
<p>Let's now focus on our best performing model, logistic regression, and see if we can push its performance a little higher. The best performance for our LR-based model is an accuracy of 0.88312, as seen earlier.</p>
<p>We are using the phrases parameter search and hyperparameter search interchangeably here. This is done to stay consistent with deep learning vocabulary.</p>
<p>We want to select the best performing configuration of our pipeline. Each configuration might be different in small ways, such as when we remove stop words, bigrams, and trigrams, or similar processes. The total number of such configurations can be fairly large, and can sometimes run into the thousands. In addition to manually selecting a few combinations to try, we can try all several thousand of these combinations and evaluate them.<br/>
<br/>
Of course, this process would be far too time-consuming for most small-scale experiments such as ours. In large experiments, possible space can run into the millions and take several days of computing, making it cost- and time-prohibitive.<br/>
<br/>
We recommend reading a blog on hyperparameter tuning (<a href="https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning">https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning</a>) to become familiar with the vocabulary and ideas discussed here in greater detail.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Parameter tuning using RandomizedSearch</h1>
                </header>
            
            <article>
                
<p>An alternative approach was proposed by Bergstra and Bengio (<a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</a>) in 2012. They demonstrated that a random search across a large hyperparameter space is more effective than a manual approach, as we did for Multinomial Naive Bayes, and often as effective—or more so—than <kbd>GridSearch</kbd>.</p>
<p>How do we use it here?</p>
<p>Here, we will build on top of the results such as that of Bergstra and Bengio. We will break down our parameter search into the following two steps:</p>
<ol>
<li>Using <kbd>RandomizedSearch</kbd>, go through a wide parameter combination space in a limited number of iterations</li>
<li>Use the results from step 1 to run <kbd>GridSearch</kbd> in a slightly narrow space</li>
</ol>
<p class="mce-root"/>
<p>We can repeat the previous steps until we stop seeing improvements in our results, but we won't do that here. We'll leave that as an exercise for the reader. Our example is outlined in the following snippet:</p>
<pre>from sklearn.model_selection import RandomizedSearchCV<br/>param_grid = dict(clf__C=[50, 75, 85, 100], <br/>                  vect__stop_words=['english', None],<br/>                  vect__ngram_range = [(1, 1), (1, 3)],<br/>                  vect__lowercase = [True, False],<br/>                 )</pre>
<p>As you can see, the <kbd>param_grid</kbd> variable defines our search space. In our pipeline, we assign names to each estimator such as <kbd>vect</kbd>, <kbd>clf</kbd>, and so on. The convention of <kbd>clf</kbd> double underscore (also called dunder) signifies that this <kbd>C</kbd> is an attribute of the <kbd>clf</kbd> object. Similarly, for <kbd>vect</kbd> we specify whether stop words are to be removed or not. As an example, <kbd>english</kbd> means removing English stop words where the list of stop words is what <kbd>scikit-learn</kbd> internally uses. You can also replace this with a command from spaCy, NLTK, or one more closely customized to your tasks.</p>
<pre>random_search = RandomizedSearchCV(lr_clf, param_distributions=param_grid, n_iter=5, scoring='accuracy', n_jobs=-1, cv=3)<br/>random_search.fit(X_train, y_train)<br/>print(f'Calculated cross-validation accuracy: {random_search.best_score_}')</pre>
<p>The preceding code gives us a cross validation accuracy in the range of 0.87. This might vary depending on how the randomized splits are created.</p>
<pre>best_random_clf = random_search.best_estimator<em>_<br/></em>best_random_clf.fit(X_train, y_train)<br/>imdb_acc(best_random_clf) # 0.90096</pre>
<p>As shown in the preceding snippet, the classifier performance improves by more than 1% by simply changing a few parameters. This is amazing progress!<br/>
<br/>
Let's now take a look at what parameters we're using. In order to compare this, you need to know the default values for all of the parameters. Alternatively, we can simply look at the parameters from <kbd>param_grid</kbd> that we wrote and note the selected parameter values. For everything not in the grid, the default values are chosen and remain unchanged, as follows:</p>
<pre>print(best_random_clf.steps)<br/><br/>[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',
          dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',
          lowercase=True, max_df=1.0, max_features=None, min_df=1,
          ngram_range=(1, 3), preprocessor=None, stop_words=None,
          strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',
          tokenizer=None, vocabulary=None)),
 ('tfidf',
  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),
 ('clf',
  LogisticRegression(C=75, class_weight=None, dual=False, fit_intercept=True,
            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,
            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
            verbose=0, warm_start=False))]</pre>
<p>Here, we notice these things in the best classifier:</p>
<ul>
<li>The chosen <kbd>C</kbd> value in <kbd>clf</kbd> is <kbd>100</kbd></li>
<li><kbd>lowercase</kbd> is set to <kbd>False</kbd></li>
<li>Removing stop words is a bad idea</li>
<li>Adding bigrams and trigrams helps</li>
</ul>
<p> <br/>
Observations like the preceding are very specific to this dataset and classifier pipeline. In my experience, however, this can and does vary widely.</p>
<p>Let's also avoid assuming that the values are always the best we'll get when running <kbd>RandomizedSearch</kbd> for so few iterations. The rule of thumb in this case is to run it for at least 60 iterations, and to also use a much larger <kbd>param_grid</kbd>.</p>
<p>Here, we used <kbd>RandomizedSearch</kbd> to understand the broad layout of parameters we want to try. We added the best values for some of those to our pipeline itself and we will continue to experiment with the values of other parameters.</p>
<div class="packt_tip"><br/>
We have not mentioned what the <kbd>C</kbd> parameter stands for or how it influences the classifier. This is definitely important when understanding and performing a manual parameter search. Changing <kbd>C</kbd> helps simply by trying out different values.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GridSearch</h1>
                </header>
            
            <article>
                
<p>We will now run <kbd>GridSearch</kbd> for our selected parameters. Here, we are choosing to include bigrams and trigrams while running <kbd>GridSearch</kbd> over the <kbd>C</kbd> parameter of <kbd>LogisticRegression</kbd>.</p>
<p>Our intention here is to automate as much as possible. Instead of trying varying values in <kbd>C</kbd> during our <kbd>RandomizedSearch</kbd>, we are trading off human learning time (a few hours) with compute time (a few extra minutes). This mindset saves us both time and effort.</p>
<pre>from sklearn.model_selection import GridSearchCV<br/>param_grid = dict(clf__C=[85, 100, 125, 150])<br/>grid_search = GridSearchCV(lr_clf, param_grid=param_grid, scoring='accuracy', n_jobs=-1, cv=3)<br/>grid_search.fit(X_train, y_train)<br/>grid_search.best_estimator_.steps</pre>
<p>In the preceding lines of code, we have ran the classifier over our <kbd>lr_clf</kbd> using the new, simpler <kbd>param_grid</kbd>, which works only over the <kbd>C</kbd> parameter of <kbd>LogisticRegression</kbd>.</p>
<p>Let's see what the steps in our best estimator are, and in particular, what the value of <kbd>C</kbd> is, as shown in the following snippet:</p>
<pre>[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',<br/>          dtype=&lt;class 'numpy.int64'&gt;, encoding='utf-8', input='content',<br/>          lowercase=True, max_df=1.0, max_features=None, min_df=1,<br/>          ngram_range=(1, 3), preprocessor=None, stop_words=None,<br/>          strip_accents=None, token_pattern='(?u)\\b\\w\\w+\\b',<br/>          tokenizer=None, vocabulary=None)),<br/> ('tfidf',<br/>  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),<br/> ('clf',<br/>  LogisticRegression(C=150, class_weight=None, dual=False, fit_intercept=True,<br/>            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,<br/>            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,<br/>            verbose=0, warm_start=False))]</pre>
<p>Let's get the resulting performance directly from our object. Each of these objects has an attribute called <kbd>best_score_</kbd>. This attribute stores the best value of the metric we chose. In the following case, we have chosen accuracy:</p>
<pre class="mce-root">print(f'Calculated cross-validation accuracy: {grid_search.best_score_} while random_search was {random_search.best_score_}')<br/><br/>&gt; Calculated cross-validation accuracy: 0.87684 while random_search was 0.87648<br/><br/>best_grid_clf = grid_search.best_estimator_<br/>best_grid_clf.fit(X_train, y_train)<br/><br/>imdb_acc(best_grid_clf) <br/>&gt; (0.90208, array([1, 1, 1, ..., 0, 0, 1], dtype=int64))</pre>
<p class="mce-root">As you can see in the preceding code, that's almost a ~3% performance gain over the non-optimized model, despite the fact we tried very few parameters to optimize.</p>
<p class="mce-root">It is worth mentioning that we can and must repeat these steps (<kbd>RandomizedSearch</kbd> and <kbd>GridSearch</kbd>) to push the model's accuracy even higher.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ensembling models</h1>
                </header>
            
            <article>
                
<p class="mce-root">Ensembling models is a very powerful technique for improving your model performance across a variety of machine learning tasks.</p>
<p class="mce-root">In the following section, we have quoted from the Kaggle Ensembling Guide (<a href="https://mlwave.com/kaggle-ensembling-guide/">https://mlwave.com/kaggle-ensembling-guide/</a>) written by MLWave.</p>
<p class="mce-root">We can explain why ensembling helps to reduce error or improve accuracy, as well as demonstrate the popular techniques on our chosen task and dataset. While each of these techniques might not result in a performance gain for us on our dataset specifically, they are still a powerful tool to have in your mental toolkit.</p>
<p class="mce-root">To ensure that you understand these techniques, we strongly urge you to try them on a few datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Voting ensembles – Simple majority (aka hard voting)</h1>
                </header>
            
            <article>
                
<p class="mce-root">The simplest ensembling technique is perhaps to take a simple majority. This works on the intuition that a single model might make an error on a particular prediction but that several different models are unlikely to make identical errors.</p>
<p class="mce-root">Let's look at an example.</p>
<p class="mce-root">Ground truth: 11011001</p>
<p class="mce-root"/>
<p>The numbers 1 and 0 represent a <kbd>True</kbd> and <kbd>False</kbd> prediction for an imagined binary classifier. Each digit is a single true or false prediction for different inputs.</p>
<p class="mce-root">Let's assume there are three models with only one error for this example; they are as follows:</p>
<ul>
<li class="mce-root">Model A prediction: 10011001</li>
<li class="mce-root">Model B prediction: 11011001</li>
<li class="mce-root">Model C prediction: 11011001</li>
</ul>
<p class="mce-root">The majority votes gives us the correct answer as follows:</p>
<ul>
<li class="mce-root">Majority vote: 11011001</li>
</ul>
<p>In the case of an even number of models, we can use a tie breaker. A tie breaker can be as simple as picking a random result, or more nuanced by picking the results with more confidence.</p>
<p class="mce-root">To try this on our dataset, we import <kbd>VotingClassifier</kbd> from scikit-learn. <kbd>VotingClassifier</kbd> does not use the pre-trained models as inputs. It will call fit on the models or classifier pipelines, and then use the predictions of all models to make the final prediction.</p>
<p class="mce-root">To counter the hype in favor of ensembles elsewhere, we can demonstrate that hard voting may hurt your accuracy performance. If someone claims that ensembling always helps, show them the following example for a more constructive discussion:</p>
<pre>from sklearn.ensemble import VotingClassifier<br/>voting_clf = VotingClassifier(estimators=[('xtc', xtc_clf), ('rfc', rfc_clf)], voting='hard', n_jobs=-1)<br/>voting_clf.fit(X_train, y_train)<br/>hard_voting_acc, _ = imdb_acc(voting_clf)<br/>hard_voting_acc # 0.71092</pre>
<p class="mce-root">We used only two classifiers for demonstration in the preceding example: Extra Trees and Random Forest. Individually, each of these classifiers has their performance capped at an accuracy of ~74%.</p>
<p class="mce-root">In this particular example, the performance of the voting classifier is worse than both of them alone.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Voting ensembles – soft voting</h1>
                </header>
            
            <article>
                
<p class="mce-root">Soft voting predicts the class label based on class probabilities. The sums of the predicted probabilities for each classifier areg calculated for each class (which is important in the case of multiple classes). The assigned class is then the class with the maximum probability sum or <kbd>argmax(p_sum)</kbd>.</p>
<p class="mce-root">This is recommended for an ensemble of well-calibrated classifiers, as follows:</p>
<div class="mce-root packt_quote">Well calibrated classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level.</div>
<div class="mce-root packt_quote CDPAlignRight CDPAlign">- From the Calibration Docs on sklearn (<a href="http://scikit-learn.org/stable/modules/calibration.html">http://scikit-learn.org/stable/modules/calibration.html</a>)</div>
<p>Our code flow is identical to our hard voting classifier except that the parameter <kbd>voting</kbd> is passed as <kbd>soft</kbd><em>,</em> as shown in the following snippet:</p>
<pre class="mce-root">voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('mnb', mnb_clf)], voting='soft', n_jobs=-1)<br/>voting_clf.fit(X_train, y_train)<br/>soft_voting_acc, _ = imdb_acc(voting_clf)<br/>soft_voting_acc # 0.88216</pre>
<p class="mce-root">Here, we can see that soft voting gives us an absolute accuracy gain of 1.62%.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weighted classifiers</h1>
                </header>
            
            <article>
                
<p>The only way for inferior models to overrule the best (expert) model is for them to collectively and confidently agree on an alternative.</p>
<p class="mce-root">To avoid this scenario, we can use a weighted majority vote—but why weighting?</p>
<p class="mce-root">Usually, we want to give a better model more weight in a vote. The simplest, but computationally inefficient, way to do this is to repeat the classifier pipelines under different names, as follows: </p>
<pre class="mce-root">weighted_voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('lr2', lr_clf),('rf', xtc_clf), ('mnb2', mnb_clf),('mnb', mnb_clf)], voting='soft', n_jobs=-1)<br/>weighted_voting_clf.fit(X_train, y_train)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Repeat the experiment with hard voting instead of soft voting. This will tell you how the voting strategy influences the accuracy of our ensembled classifier, as follows:</p>
<pre class="mce-root"><br/>weighted_voting_acc, _ = imdb_acc(weighted_voting_clf)<br/>weighted_voting_acc # <span>0.88092</span></pre>
<p class="mce-root">Here, we can see that weighted voting gives us an absolute accuracy gain of 1.50%.</p>
<p class="mce-root">So, what have we learned so far?</p>
<ul>
<li class="mce-root">A simple majority-based voting classifier can perform worse than individual models</li>
<li class="mce-root">Soft voting works better than hard voting</li>
<li class="mce-root">Weighing classifiers by simply repeating classifiers can help</li>
</ul>
<p>So far, we have been selecting classifiers seemingly at random. This is less than ideal, especially when we are building for a commercial utility where every 0.001% gain matters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing correlated classifiers</h1>
                </header>
            
            <article>
                
<p class="mce-root">Let's look at this in action by taking three simple models as an example. As you can see, the ground truth is all 1s:</p>
<pre class="mce-root">1111111100 = 80% accuracy<br/> 1111111100 = 80% accuracy<br/> 1011111100 = 70% accuracy</pre>
<p class="mce-root">These models are highly correlated in their predictions. When we take a majority vote, we see no improvement:</p>
<pre class="mce-root">1111111100 = 80% accuracy</pre>
<p class="mce-root">Now, let's compare that to the following three lower-performing but highly uncorrelated models:</p>
<pre class="mce-root">1111111100 = 80% accuracy<br/> 0111011101 = 70% accuracy<br/> 1000101111 = 60% accuracy</pre>
<p class="mce-root">When we ensemble this with a majority vote, we get the following result:</p>
<pre class="mce-root">1111111101 = 90% accuracy</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root">Here, we see a much higher rate of improvement than in any of our individual models. Low correlation between model predictions can lead to better performance. In practice, this is tricky to get right but is worth investigating nevertheless.</p>
<p class="mce-root">We will leave the following section as an exercise for you to try out.</p>
<p class="mce-root">As a quick hint, you will need to find the correlations among predictions of different models and select pairs that are less correlated to each other (ideally less than 0.5) and yet have a good enough performance as individual models.</p>
<pre class="mce-root"><br/> np.corrcoef(mnb_predictions, lr_predictions)[0][1] # this is too high a correlation at 0.8442355164021454<br/> <br/>corr_voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('mnb', mnb_clf)], voting='soft', n_jobs=-1)<br/>corr_voting_clf.fit(X_train, y_train)<br/>corr_acc, _ = imdb_acc(corr_voting_clf)<br/> print(corr_acc) # 0.88216 </pre>
<p>So, what result do we get when we use two classifiers from the same approach? </p>
<pre class="mce-root">np.corrcoef(dtc_predictions,xtc_predictions )[0][1] # this is looks like a low correlation # 0.3272698219282598<br/><br/>low_corr_voting_clf = VotingClassifier(estimators=[('dtc', dtc_clf), ('xtc', xtc_clf)], voting='soft', n_jobs=-1)<br/>low_corr_voting_clf.fit(X_train, y_train)<br/>low_corr_acc, _ = imdb_acc(low_corr_voting_clf)<br/> print(low_corr_acc) # 0.70564</pre>
<p class="mce-root">As you can see, the preceding result is not very encouraging either, but remember, this is just a hint! We encourage you to go ahead and try this task on your own and with more classifiers, including ones we have not discussed here. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we looked at several new ideas regarding machine learning. The intention here was to demonstrate some of the most common classifiers. We looked at how to use them with one thematic idea: translating text to a numerical representation and then feeding that to a classifier.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">This chapter covered a fraction of the available possibilities. Remember, you can try anything from better feature extraction using Tfidf to tuning classifiers with <kbd>GridSearch</kbd> and <kbd>RandomizedSearch</kbd>, as well as ensembling several classifiers.</p>
<p class="mce-root">This chapter was mostly focused on pre-deep learning methods for both feature extraction and classification.</p>
<p class="mce-root">Note that deep learning methods also allow us to use a single model where the feature extraction and classification are both learned from the underlying data distribution. While a lot has been written about deep learning in computer vision, we have offered only an introduction to deep learning in natural language processing.</p>


            </article>

            
        </section>
    </body></html>