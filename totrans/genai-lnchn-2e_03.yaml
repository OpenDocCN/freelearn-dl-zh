- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First Steps with LangChain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored LLMs and introduced LangChain as a powerful
    framework for building LLM-powered applications. We discussed how LLMs have revolutionized
    natural language processing with their ability to understand context, generate
    human-like text, and perform complex reasoning. While these capabilities are impressive,
    we also examined their limitations—hallucinations, context constraints, and lack
    of up-to-date knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll move from theory to practice by building our first LangChain
    application. We’ll start with the fundamentals: setting up a proper development
    environment, understanding LangChain’s core components, and creating simple chains.
    From there, we’ll explore more advanced capabilities, including running local
    models for privacy and cost efficiency and building multimodal applications that
    combine text with visual understanding. By the end of this chapter, you’ll have
    a solid foundation in LangChain’s building blocks and be ready to create increasingly
    sophisticated AI applications in subsequent chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, this chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring LangChain’s building blocks (model interfaces, prompts and templates,
    and LCEL)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running local models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal AI applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the rapid evolution of both LangChain and the broader AI field, we maintain
    up-to-date code examples and resources in our GitHub repository: [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For questions or troubleshooting help, please create an issue on GitHub or
    join our Discord community: [https://packt.link/lang](https://packt.link/lang).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up dependencies for this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book provides multiple options for running the code examples, from zero-setup
    cloud notebooks to local development environments. Choose the approach that best
    fits your experience level and preferences. Even if you are familiar with dependency
    management, please read these instructions since all code in this book will depend
    on the correct installation of the environment as outlined here.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the quickest start with no local setup required, we provide ready-to-use
    online notebooks for every chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Colab**: Run examples with free GPU access'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kaggle Notebooks**: Experiment with integrated datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient Notebooks**: Access higher-performance compute options'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All code examples you find in this book are available as online notebooks on
    GitHub at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).
  prefs: []
  type: TYPE_NORMAL
- en: 'These notebooks don’t have all dependencies pre-configured but, usually, a
    few install commands get you going. These tools allow you to start experimenting
    immediately without worrying about setup. If you prefer working locally, we recommend
    using conda for environment management:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Miniconda if you don’t have it already.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download it from [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new environment with Python 3.11:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Activate the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install Jupyter and core dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Launch Jupyter Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This approach provides a clean, isolated environment for working with LangChain.
    For experienced developers with established workflows, we also support:'
  prefs: []
  type: TYPE_NORMAL
- en: '**pip with venv**: Instructions in the GitHub repository'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker containers**: Dockerfiles provided in the GitHub repository'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Poetry**: Configuration files available in the GitHub repository'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the method you’re most comfortable with but remember that all examples
    assume a Python 3.10+ environment with the dependencies listed in requirements.txt.
  prefs: []
  type: TYPE_NORMAL
- en: For developers, Docker, which provides isolation via containers, is a good option.
    The downside is that it uses a lot of disk space and is more complex than the
    other options. For data scientists, I’d recommend Conda or Poetry.
  prefs: []
  type: TYPE_NORMAL
- en: Conda handles intricate dependencies efficiently, although it can be excruciatingly
    slow in large environments. Poetry resolves dependencies well and manages environments;
    however, it doesn’t capture system dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: All tools allow sharing and replicating dependencies from configuration files.
    You can find a set of instructions and the corresponding configuration files in
    the book’s repository at [https://github.com/benman1/generative_ai_with_langchain](https://github.com/benman1/generative_ai_with_langchain).
  prefs: []
  type: TYPE_NORMAL
- en: Once you are finished, please make sure you have LangChain version 0.3.17 installed.
    You can check this with the command `pip show langchain`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the rapid pace of innovation in the LLM field, library updates are frequent.
    The code in this book is tested with LangChain 0.3.17, but newer versions may
    introduce changes. If you encounter any issues running the examples:'
  prefs: []
  type: TYPE_NORMAL
- en: Create an issue on our GitHub repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join the discussion on Discord at [https://packt.link/lang](https://packt.link/lang
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the errata on the book’s Packt page
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This community support ensures you’ll be able to successfully implement all
    projects regardless of library updates.
  prefs: []
  type: TYPE_NORMAL
- en: API key setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain’s provider-agnostic approach supports a wide range of LLM providers,
    each with unique strengths and characteristics. Unless you use a local LLM, to
    use these services, you’ll need to obtain the appropriate authentication credentials.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Provider** | **Environment Variable** | **Setup URL** | **Free Tier?**
    |'
  prefs: []
  type: TYPE_TB
- en: '| OpenAI | `OPENAI_API_KEY` | [platform.openai.com](https://platform.openai.com/)
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| HuggingFace | `HUGGINGFACEHUB_API_TOKEN` | [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)
    | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Anthropic | `ANTHROPIC_API_KEY` | [console.anthropic.com](https://console.anthropic.com)
    | No |'
  prefs: []
  type: TYPE_TB
- en: '| Google AI | `GOOGLE_API_KEY` | [ai.google.dev/gemini-api](https://ai.google.dev/gemini-api)
    | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Google VertexAI | `Application Default Credentials` | [cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)
    | Yes (with limits) |'
  prefs: []
  type: TYPE_TB
- en: '| Replicate | `REPLICATE_API_TOKEN` | [replicate.com](https://replicate.com)
    | No |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: API keys reference table (overview)'
  prefs: []
  type: TYPE_NORMAL
- en: Most providers require an API key, while cloud providers like AWS and Google
    Cloud also support alternative authentication methods like **Application Default
    Credentials** (**ADC**). Many providers offer free tiers without requiring credit
    card details, making it easy to get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'To set an API key in an environment, in Python, we can execute the following
    lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, `OPENAI_API_KEY` is the environment key that is appropriate for OpenAI.
    Setting the keys in your environment has the advantage of not needing to include
    them as parameters in your code every time you use a model or service integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also expose these variables in your system environment from your terminal.
    In Linux and macOS, you can set a system environment variable from the terminal
    using the `export` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To permanently set the environment variable in Linux or macOS, you would need
    to add the preceding line to the `~/.bashrc` or `~/.bash_profile` files, and then
    reload the shell using the command `source ~/.bashrc` or `source ~/.bash_profile`.
  prefs: []
  type: TYPE_NORMAL
- en: For Windows users, you can set the environment variable by searching for “Environment
    Variables” in the system settings, editing either “User variables” or “System
    variables,” and adding `export` `OPENAI_API_KEY=your_key_here`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our choice is to create a `config.py` file where all API keys are stored. We
    then import a function from this module that loads these keys into the environment
    variables. This approach centralizes credential management and makes it easier
    to update keys when needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you search for this file in the GitHub repository, you’ll notice it’s missing.
    This is intentional – I’ve excluded it from Git tracking using the `.gitignore`
    file. The `.gitignore` file tells Git which files to ignore when committing changes,
    which is essential for:'
  prefs: []
  type: TYPE_NORMAL
- en: Preventing sensitive credentials from being publicly exposed
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoiding accidental commits of personal API keys
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Protecting yourself from unauthorized usage charges
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To implement this yourself, simply add `config.py` to your `.gitignore` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can set all your keys in the `config.py` file. This function, `set_environment()`,
    loads all the keys into the environment as mentioned. Anytime you want to run
    an application, you import the function and run it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: For production environments, consider using dedicated secrets management services
    or environment variables injected at runtime. These approaches provide additional
    security while maintaining the separation between code and credentials.
  prefs: []
  type: TYPE_NORMAL
- en: While OpenAI’s models remain influential, the LLM ecosystem has rapidly diversified,
    offering developers multiple options for their applications. To maintain clarity,
    we’ll separate LLMs from the model gateways that provide access to them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Key LLM families**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic Claude**: Excels in reasoning, long-form content processing, and
    vision analysis with up to 200K token context windows'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mistral models**: Powerful open-source models with strong multilingual capabilities
    and exceptional reasoning abilities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Gemini**: Advanced multimodal models with industry-leading 1M token
    context window and real-time information access'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI GPT-o**: Leading omnimodal capabilities accepting text, audio, image,
    and video with enhanced reasoning'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepSeek models:** Specialized in coding and technical reasoning with state-of-the-art
    performance on programming tasks'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI21 Labs Jurassic:** Strong in academic applications and long-form content
    generation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inflection Pi**: Optimized for conversational AI with exceptional emotional
    intelligence'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perplexity models**: Focused on accurate, cited answers for research applications'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cohere models**: Specialized for enterprise applications with strong multilingual
    capabilities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cloud provider gateways**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon Bedrock**: Unified API access to models from Anthropic, AI21, Cohere,
    Mistral, and others with AWS integration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure OpenAI Service**: Enterprise-grade access to OpenAI and other models
    with robust security and Microsoft ecosystem integration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Vertex AI**: Access to Gemini and other models with seamless Google
    Cloud integration'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Independent platforms**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Together AI**: Hosts 200+ open-source models with both serverless and dedicated
    GPU options'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replicate**: Specializes in deploying multimodal open-source models with
    pay-as-you-go pricing'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HuggingFace Inference Endpoints**: Production deployment of thousands of
    open-source models with fine-tuning capabilities'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book, we’ll work with various models accessed through different
    providers, giving you the flexibility to choose the best option for your specific
    needs and infrastructure requirements.
  prefs: []
  type: TYPE_NORMAL
- en: We will use OpenAI for many applications but will also try LLMs from other organizations.
    Refer to the *Appendix* at the end of the book to learn how to get API keys for
    OpenAI, Hugging Face, Google, and other providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main integration packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`langchain-google-vertexai`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain-google-genai`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll be using `langchain-google-genai`, the package recommended by LangChain
    for individual developers. The setup is a lot simpler, only requiring a Google
    account and API key. It is recommended to move to `langchain-google-vertexai`
    for larger projects. This integration offers enterprise features such as customer
    encryption keys, virtual private cloud integration, and more, requiring a Google
    Cloud account with billing.
  prefs: []
  type: TYPE_NORMAL
- en: If you’ve followed the instructions on GitHub, as indicated in the previous
    section, you should already have the `langchain-google-genai` package installed.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring LangChain’s building blocks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build practical applications, we need to know how to work with different
    model providers. Let’s explore the various options available, from cloud services
    to local deployments. We’ll start with fundamental concepts like LLMs and chat
    models, then dive into prompts, chains, and memory systems.
  prefs: []
  type: TYPE_NORMAL
- en: Model interfaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LangChain provides a unified interface for working with various LLM providers.
    This abstraction makes it easy to switch between different models while maintaining
    a consistent code structure. The following examples demonstrate how to implement
    LangChain’s core components in practical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that users should almost exclusively be using the newer chat models
    as most model providers have adopted a chat-like interface for interacting with
    language models. We still provide the LLM interface, because it’s very easy to
    use as string-in, string-out.
  prefs: []
  type: TYPE_NORMAL
- en: LLM interaction patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The LLM interface represents traditional text completion models that take a
    string input and return a string output. More and more use cases in LangChain
    use only the ChatModel interface, mainly because it’s better suited for building
    complex workflows and developing agents. The LangChain documentation is now deprecating
    the LLM interface and recommending the use of chat-based interfaces. While this
    chapter demonstrates both interfaces, we recommend using chat models as they represent
    the current standard to be up to date with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the LLM interface in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that you must set your environment variables to the provider keys
    when you run this. For example, when running this I’d start the file by calling
    `set_environment() from config`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We get this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'For the Gemini model, we can run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For me, Gemini comes up with this joke:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we use the same `invoke()` method regardless of the provider. This
    consistency makes it easy to experiment with different models or switch providers
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: Development testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During development, you might want to test your application without making
    actual API calls. LangChain provides `FakeListLLM` for this purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Working with chat models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Chat models are LLMs that are fine-tuned for multi-turn interaction between
    a model and a human. These days most LLMs are fine-tuned for multi-turned conversations.
    Instead of providing input to the model, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: where we expect it to generate an output by continuing the conversation, these
    days model providers typically expose an API that expects each turn as a separate
    well-formatted part of the payload. Model providers typically don’t store the
    chat history server-side, they get the full history sent each time from the client
    and only format the final prompt server-side.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain follows the same pattern with ChatModels, processing conversations
    through structured messages with roles and content. Each message contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Role (who’s speaking), which is defined by the message class (all messages inherit
    from BaseMessage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Content (what’s being said)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Message types include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SystemMessage`: Sets behavior and context for the model. Example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`HumanMessage`: Represents user input like questions, commands, and data. Example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`AIMessage`: Contains model responses'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s see this in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Claude comes up with a function, an explanation, and examples for calling the
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a Python function that calculates the factorial of a given number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]python'
  prefs: []
  type: TYPE_NORMAL
- en: 'def factorial(n):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if n < 0:'
  prefs: []
  type: TYPE_NORMAL
- en: raise ValueError("Factorial is not defined for negative numbers.")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif n == 0:'
  prefs: []
  type: TYPE_NORMAL
- en: return 1
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: result = 1
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'for i in range(1, n + 1):'
  prefs: []
  type: TYPE_NORMAL
- en: result *= i
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: return result
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]python'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(factorial(0))  # Output: 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(factorial(5))  # Output: 120'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(factorial(10))  # Output: 3628800'
  prefs: []
  type: TYPE_NORMAL
- en: 'print(factorial(-5))  # Raises ValueError: Factorial is not defined for negative
    numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we could have asked an OpenAI model such as GPT-4 or GPT-4o:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Reasoning models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Anthropic’s Claude 3.7 Sonnet introduces a powerful capability called *extended
    thinking* that allows the model to show its reasoning process before delivering
    a final answer. This feature represents a significant advancement in how developers
    can leverage LLMs for complex reasoning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how to configure extended thinking through the ChatAnthropic class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The response will include Claude’s step-by-step reasoning about algorithm selection,
    complexity analysis, and optimization considerations before presenting its final
    solution. In the preceding example:'
  prefs: []
  type: TYPE_NORMAL
- en: Out of the 64,000-token maximum response length, up to 15,000 tokens can be
    used for Claude’s thinking process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining ~49,000 tokens are available for the final response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Claude doesn’t always use the entire thinking budget—it uses what it needs for
    the specific task. If Claude runs out of thinking tokens, it will transition to
    its final answer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While Claude offers explicit thinking configuration, you can achieve similar
    (though not identical) results with other providers through different techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The `reasoning_effort` parameter streamlines your workflow by eliminating the
    need for complex reasoning prompts, allows you to adjust performance by reducing
    effort when speed matters more than detailed analysis, and helps manage token
    consumption by controlling how much processing power goes toward reasoning processes.
  prefs: []
  type: TYPE_NORMAL
- en: DeepSeek models also offer explicit thinking configuration through the LangChain
    integration.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling model behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding how to control an LLM’s behavior is crucial for tailoring its
    output to specific needs. Without careful parameter adjustments, the model might
    produce overly creative, inconsistent, or verbose responses that are unsuitable
    for practical applications. For instance, in customer service, you’d want consistent,
    factual answers, while in content generation, you might aim for more creative
    and promotional outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMs offer several parameters that allow fine-grained control over generation
    behavior, though exact implementation may vary between providers. Let’s explore
    the most important ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Description** | **Typical Range** | **Best For** |'
  prefs: []
  type: TYPE_TB
- en: '| **Temperature** | Controls randomness in text generation | 0.0-1.0 (OpenAI,
    Anthropic)0.0-2.0 (Gemini) | Lower (0.0-0.3): Factual tasks, Q&AHigher (0.7+):
    Creative writing, brainstorming |'
  prefs: []
  type: TYPE_TB
- en: '| **Top-k** | Limits token selection to k most probable tokens | 1-100 | Lower
    values (1-10): More focused outputsHigher values: More diverse completions |'
  prefs: []
  type: TYPE_TB
- en: '| **Top-p (Nucleus Sampling)** | Considers tokens until cumulative probability
    reaches threshold | 0.0-1.0 | Lower values (0.5): More focused outputsHigher values
    (0.9): More exploratory responses |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max tokens**'
  prefs: []
  type: TYPE_NORMAL
- en: '| Limits maximum response length | Model-specific | Controlling costs and preventing
    verbose outputs |'
  prefs: []
  type: TYPE_TB
- en: '| **Presence/frequency penalties** | Discourages repetition by penalizing tokens
    that have appeared | -2.0 to 2.0 | Longer content generation where repetition
    is undesirable |'
  prefs: []
  type: TYPE_TB
- en: '| **Stop sequences** | Tells model when to stop generating | Custom strings
    | Controlling exact ending points of generation |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.2: Parameters offered by LLMs'
  prefs: []
  type: TYPE_NORMAL
- en: 'These parameters work together to shape model output:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temperature + Top-k/Top-p**: First, Top-k/Top-p filter the token distribution,
    and then temperature affects randomness within that filtered set'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Penalties + Temperature**: Higher temperatures with low penalties can produce
    creative but potentially repetitive text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain provides a consistent interface for setting these parameters across
    different LLM providers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'A few provider-specific considerations to keep in mind are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI**: Known for consistent behavior with temperature in the 0.0-1.0 range'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthropic**: May need lower temperature settings to achieve similar creativity
    levels to other providers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gemini**: Supports temperature up to 2.0, allowing for more extreme creativity
    at higher settings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open-source models**: Often require different parameter combinations than
    commercial APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing parameters for applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For enterprise applications requiring consistency and accuracy, lower temperatures
    (0.0-0.3) combined with moderate top-p values (0.5-0.7) are typically preferred.
    For creative assistants or brainstorming tools, higher temperatures produce more
    diverse outputs, especially when paired with higher top-p values.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that parameter tuning is often empirical – start with provider recommendations,
    then adjust based on your specific application needs and observed outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts and templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prompt engineering is a crucial skill for LLM application development, particularly
    in production environments. LangChain provides a robust system for managing prompts
    with features that address common development challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Template systems** for dynamic prompt generation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt management and versioning** for tracking changes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Few-shot example management** for improved model performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output parsing and validation** for reliable results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LangChain’s prompt templates transform static text into dynamic prompts with
    variable substitution – compare these two approaches to see the key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Static use – problematic at scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'PromptTemplate – production-ready:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Templates matter – here’s why:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: They standardize prompts across your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintainability**: They allow you to change the prompt structure in one place
    instead of throughout your codebase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Readability**: They clearly separate template logic from business logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testability**: It is easier to unit test prompt generation separately from
    LLM calls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In production applications, you’ll often need to manage dozens or hundreds of
    prompts. Templates provide a scalable way to organize this complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Chat prompt templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For chat models, we can create more structured prompts that incorporate different
    roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start by looking at **LangChain Expression Language** (**LCEL**), which
    provides a clean, intuitive way to build LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain Expression Language (LCEL)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LCEL represents a significant evolution in how we build LLM-powered applications
    with LangChain. Introduced in August 2023, LCEL is a declarative approach to constructing
    complex LLM workflows. Rather than focusing on *how* to execute each step, LCEL
    lets you define *what* you want to accomplish, allowing LangChain to handle the
    execution details behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'At its core, LCEL serves as a minimalist code layer that makes it remarkably
    easy to connect different LangChain components. If you’re familiar with Unix pipes
    or data processing libraries like pandas, you’ll recognize the intuitive syntax:
    components are connected using the pipe operator (|) to create processing pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: As we briefly introduced in [*Chapter 1*](E_Chapter_1.xhtml#_idTextAnchor001),
    LangChain has always used the concept of a “chain” as its fundamental pattern
    for connecting components. Chains represent sequences of operations that transform
    inputs into outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, LangChain implemented this pattern through specific `Chain` classes
    like `LLMChain` and `ConversationChain`. While these legacy classes still exist,
    they’ve been deprecated in favor of the more flexible and powerful LCEL approach,
    which is built upon the Runnable interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Runnable interface is the cornerstone of modern LangChain. A Runnable is
    any component that can process inputs and produce outputs in a standardized way.
    Every component built with LCEL adheres to this interface, which provides consistent
    methods including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`invoke()`: Processes a single input synchronously and returns an output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream()`: Streams output as it’s being generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch()`: Efficiently processes multiple inputs in parallel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ainvoke()`, `abatch()`, `astream()`: Asynchronous versions of the above methods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This standardization means any Runnable component—whether it’s an LLM, a prompt
    template, a document retriever, or a custom function—can be connected to any other
    Runnable, creating a powerful composability system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every Runnable implements a consistent set of methods including:'
  prefs: []
  type: TYPE_NORMAL
- en: '`invoke()`: Processes a single input synchronously and returns an output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stream()`: Streams output as it’s being generated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This standardization is powerful because it means any Runnable component—whether
    it’s an LLM, a prompt template, a document retriever, or a custom function—can
    be connected to any other Runnable. The consistency of this interface enables
    complex applications to be built from simpler building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'LCEL offers several advantages that make it the preferred approach for building
    LangChain applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rapid development**: The declarative syntax enables faster prototyping and
    iteration of complex chains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Production-ready features**: LCEL provides built-in support for streaming,
    asynchronous execution, and parallel processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improved readability**: The pipe syntax makes it easy to visualize data flow
    through your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seamless ecosystem integration**: Applications built with LCEL automatically
    work with LangSmith for observability and LangServe for deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customizability**: Easily incorporate custom Python functions into your chains
    with RunnableLambda.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runtime optimization**: LangChain can automatically optimize the execution
    of LCEL-defined chains.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LCEL truly shines when you need to build complex applications that combine multiple
    components in sophisticated workflows. In the next sections, we’ll explore how
    to use LCEL to build real-world applications, starting with the basic building
    blocks and gradually incorporating more advanced patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipe operator (|) serves as the cornerstone of LCEL, allowing you to chain
    components sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, `StrOutputParser()` is a simple output parser that extracts the string
    response from an LLM. It takes the structured output from an LLM and converts
    it to a plain string, making it easier to work with. This parser is especially
    useful when you need just the text content without metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, LCEL uses Python’s operator overloading to transform this expression
    into a RunnableSequence where each component’s output flows into the next component’s
    input. The pipe (|) is syntactic sugar that overrides the `__or__` hidden method,
    in other words, `A | B` is equivalent to `B.__or__(A)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipe syntax is equivalent to creating a `RunnableSequence` programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'For more complex workflows, you can incorporate branching logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Non-Runnable elements like functions and dictionaries are automatically converted
    to appropriate Runnable types:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The flexible, composable nature of LCEL will allow us to tackle real-world LLM
    application challenges with elegant, maintainable code.
  prefs: []
  type: TYPE_NORMAL
- en: Simple workflows with LCEL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we’ve seen, LCEL provides a declarative syntax for composing LLM application
    components using the pipe operator. This approach dramatically simplifies workflow
    construction compared to traditional imperative code. Let’s build a simple joke
    generator to see LCEL in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces a programming joke:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Without LCEL, the same workflow is equivalent to separate function calls with
    manual data passing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we have detached chain construction from its execution.
  prefs: []
  type: TYPE_NORMAL
- en: In production applications, this pattern becomes even more valuable when handling
    complex workflows with branching logic, error handling, or parallel processing
    – topics we’ll explore in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: Complex chain example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the simple joke generator demonstrated basic LCEL usage, real-world applications
    typically require more sophisticated data handling. Let’s explore advanced patterns
    using a story generation and analysis example.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we’ll build a multi-stage workflow that demonstrates how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate content with one LLM call
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed that content into a second LLM call
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preserve and transform data throughout the chain
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can compose these two chains together. Our first simple approach pipes the
    story directly into the analysis chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'I get a long analysis. Here’s how it starts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'While this works, we’ve lost the original story in our result – we only get
    the analysis! In production applications, we typically want to preserve context
    throughout the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'For more control over the output structure, we could also construct dictionaries
    manually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can simplify this with dictionary conversion using a LCEL shorthand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: What makes these examples more complex than our simple joke generator?
  prefs: []
  type: TYPE_NORMAL
- en: '**M****ultiple LLM calls**: Rather than a single prompt ![](img/Icon.png) LLM
    ![](img/Icon.png) parser flow, we’re chaining multiple LLM interactions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data transformation**: Using tools like `RunnablePassthrough` and `itemgetter`
    to manage and transform data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dictionary preservation**: Maintaining context throughout the chain rather
    than just passing single values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Structured outputs**: Creating structured output dictionaries rather than
    simple strings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These patterns are essential for production applications where you need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Track the provenance of generated content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine results from multiple operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure data for downstream processing or display
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement more sophisticated error handling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While LCEL handles many complex workflows elegantly, for state management and
    advanced branching logic, you’ll want to explore LangGraph, which we’ll cover
    in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: While our previous examples used cloud-based models like OpenAI and Google’s
    Gemini, LangChain’s LCEL and other functionality work seamlessly with local models
    as well. This flexibility allows you to choose the right deployment approach for
    your specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Running local models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building LLM applications with LangChain, you need to decide where your
    models will run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advantages of local models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete data control and privacy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No API costs or usage limits
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: No internet dependency
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Control over model parameters and fine-tuning
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Advantages of cloud models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No hardware requirements or setup complexity
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the most powerful, state-of-the-art models
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic scaling without infrastructure management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous model improvements without manual updates
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When to choose local models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications with strict data privacy requirements
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Development and testing environments
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge or offline deployment scenarios
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive applications with predictable, high-volume usage
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with one of the most developer-friendly options for running local
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Ollama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ollama provides a developer-friendly way to run powerful open-source models
    locally. It provides a simple interface for downloading and running various open-source
    models. The `langchain-ollama` dependency should already be installed if you’ve
    followed the instructions in this chapter; however, let’s go through them briefly
    anyway:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the LangChain Ollama integration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then pull a model. From the command line, a terminal such as bash or the WindowsPowerShell,
    run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Ollama server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here’s how to integrate Ollama with the LCEL patterns we’ve explored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: This LCEL chain functions identically to our cloud-based examples, demonstrating
    LangChain’s model-agnostic design.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that since you are running a local model, you don’t need to set
    up any keys. The answer is very long – although quite reasonable. You can run
    this yourself and see what answers you get.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen basic text generation, let’s look at another integration.
    Hugging Face offers an approachable way to run models locally, with access to
    a vast ecosystem of pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Hugging Face models locally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With Hugging Face, you can either run a model locally (HuggingFacePipeline)
    or on the Hugging Face Hub (HuggingFaceEndpoint). Here, we are talking about local
    runs, so we’ll focus on `HuggingFacePipeline`. Here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: This can take quite a while, especially the first time, since the model has
    to be downloaded first. We’ve omitted the model response for the sake of brevity.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain supports running models locally through other integrations as well,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**llama.cpp:** This high-performance C++ implementation allows running LLaMA-based
    models efficiently on consumer hardware. While we won’t cover the setup process
    in detail, LangChain provides straightforward integration with llama.cpp for both
    inference and fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT4All**: GPT4All offers lightweight models that can run on consumer hardware.
    LangChain’s integration makes it easy to use these models as drop-in replacements
    for cloud-based LLMs in many applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you begin working with local models, you’ll want to optimize their performance
    and handle common challenges. Here are some essential tips and patterns that will
    help you get the most out of your local deployments with LangChain.
  prefs: []
  type: TYPE_NORMAL
- en: Tips for local models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with local models, keep these points in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource management**: Local models require careful configuration to balance
    performance and resource usage. The following example demonstrates how to configure
    an Ollama model for efficient operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at what each parameter does:'
  prefs: []
  type: TYPE_NORMAL
- en: '**model=”mistral:q4_K_M”**: Specifies a 4-bit quantized version of the Mistral
    model. Quantization reduces the model size by representing weights with fewer
    bits, trading minimal precision for significant memory savings. For example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Full precision model: ~8GB RAM required'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '4-bit quantized model: ~2GB RAM required'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_gpu=1**: Allocates GPU resources. Options include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0: CPU-only mode (slower but works without a GPU)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '1: Uses a single GPU (appropriate for most desktop setups)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Higher values: For multi-GPU systems only'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**num_thread=4**: Controls CPU parallelization:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lower values (2-4): Good for running alongside other applications'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Higher values (8-16): Maximizes performance on dedicated servers'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Optimal setting: Usually matches your CPU’s physical core count'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling**: Local models can encounter various errors, from out-of-memory
    conditions to unexpected terminations. A robust error-handling strategy is essential:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Common local model errors you might run into are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Out of memory**: Occurs when the model requires more VRAM than available'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model loading failure**: When model files are corrupt or incompatible'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timeout issues**: When inference takes too long on resource-constrained systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context length errors**: When input exceeds the model’s maximum token limit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these optimizations and error-handling strategies, you can create
    robust LangChain applications that leverage local models effectively while maintaining
    a good user experience even when issues arise.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Decision chart for choosing between local and cloud-based models](img/B32363_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Decision chart for choosing between local and cloud-based models'
  prefs: []
  type: TYPE_NORMAL
- en: Having explored how to build text-based applications with LangChain, we’ll now
    extend our understanding to multimodal capabilities. As AI systems increasingly
    work with multiple forms of data, LangChain provides interfaces for both generating
    images from text and understanding visual content – capabilities that complement
    the text processing we’ve already covered and open new possibilities for more
    immersive applications.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal AI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI systems have evolved beyond text-only processing to work with diverse data
    types. In the current landscape, we can distinguish between two key capabilities
    that are often confused but represent different technological approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal understanding represents the ability of models to process multiple
    types of inputs simultaneously to perform reasoning and generate responses. These
    advanced systems can understand the relationships between different modalities,
    accepting inputs like text, images, PDFs, audio, video, and structured data. Their
    processing capabilities include cross-modal reasoning, context awareness, and
    sophisticated information extraction. Models like Gemini 2.5, GPT-4V, Sonnet 3.7,
    and Llama 4 exemplify this capability. For instance, a multimodal model can analyze
    a chart image along with a text question to provide insights about the data trend,
    combining visual and textual understanding in a single processing flow.
  prefs: []
  type: TYPE_NORMAL
- en: Content generation capabilities, by contrast, focus on creating specific types
    of media, often with extraordinary quality but more specialized functionality.
    Text-to-image models create visual content from descriptions, text-to-video systems
    generate video clips from prompts, text-to-audio tools produce music or speech,
    and image-to-image models transform existing visuals. Examples include Midjourney,
    DALL-E, and Stable Diffusion for images; Sora and Pika for video; and Suno and
    ElevenLabs for audio. Unlike true multimodal models, many generation systems are
    specialized for their specific output modality, even if they can accept multiple
    input types. They excel at creation rather than understanding.
  prefs: []
  type: TYPE_NORMAL
- en: As LLMs evolve beyond text, LangChain is expanding to support both multimodal
    understanding and content generation workflows. The framework provides developers
    with tools to incorporate these advanced capabilities into their applications
    without needing to implement complex integrations from scratch. Let’s start with
    generating images from text descriptions. LangChain provides several approaches
    to incorporate image generation through external integrations and wrappers. We’ll
    explore multiple implementation patterns, starting with the simplest and progressing
    to more sophisticated techniques that can be incorporated into your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LangChain integrates with various image generation models and services, allowing
    you to:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate images from text descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edit existing images based on text prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control image generation parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle image variations and styles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain includes wrappers and models for popular image generation services.
    First, let’s see how to generate images with OpenAI’s DALL-E model series.
  prefs: []
  type: TYPE_NORMAL
- en: Using DALL-E through OpenAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LangChain’s wrapper for DALL-E simplifies the process of generating images from
    text prompts. The implementation uses OpenAI’s API under the hood but provides
    a standardized interface consistent with other LangChain components.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the image we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator](img/B32363_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: An image generated by OpenAI’s DALL-E Image Generator'
  prefs: []
  type: TYPE_NORMAL
- en: You might notice that text generation within these images is not one of the
    strong suites of these models. You can find a lot of models for image generation
    on Replicate, including the latest Stable Diffusion models, so this is what we’ll
    use now.
  prefs: []
  type: TYPE_NORMAL
- en: Using Stable Diffusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stable Diffusion 3.5 Large is Stability AI’s latest text-to-image model, released
    in March 2024\. It’s a **Multimodal Diffusion Transformer** (**MMDiT**) that generates
    high-resolution images with remarkable detail and quality.
  prefs: []
  type: TYPE_NORMAL
- en: This model uses three fixed, pre-trained text encoders and implements Query-Key
    Normalization for improved training stability. It’s capable of producing diverse
    outputs from the same prompt and supports various artistic styles.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The recommended parameters for the new model include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**prompt_strength**: Controls how closely the image follows the prompt (0.85)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**cfg**: Controls how strictly the model follows the prompt (4.5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**steps**: More steps result in higher-quality images (40)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**aspect_ratio**: Set to 1:1 for square images'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_format**: Using WebP for a better quality-to-size ratio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**output_quality**: Set to 90 for high-quality output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s the image we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: An image generated by Stable Diffusion](img/B32363_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: An image generated by Stable Diffusion'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s explore how to analyze and understand images using multimodal models.
  prefs: []
  type: TYPE_NORMAL
- en: Image understanding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image understanding refers to an AI system’s ability to interpret and analyze
    visual information in ways similar to human visual perception. Unlike traditional
    computer vision (which focuses on specific tasks like object detection or facial
    recognition), modern multimodal models can perform general reasoning about images,
    understanding context, relationships, and even implicit meaning within visual
    content.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini 2.5 Pro and GPT-4 Vision, among other models, can analyze images and
    provide detailed descriptions or answer questions about them.
  prefs: []
  type: TYPE_NORMAL
- en: Using Gemini 1.5 Pro
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LangChain handles multimodal input through the same `ChatModel` interface. It
    accepts `Messages` as an input, and a `Message` object has a `content` field.
    IA `content` can consist of multiple parts, and each part can represent a different
    modality (that allows you to mix different modalities in your prompt).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can send multimodal input by value or by reference. To send it by value,
    you should encode bytes as a string and construct an `image_url` variable formatted
    as in the example below using the image we generated using Stable Diffusion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: As multimodal inputs typically have a large size, sending raw bytes as part
    of your request might not be the best idea. You can send it by reference by pointing
    to the blob storage, but the specific type of storage depends on the model’s provider.
    For example, Gemini accepts multimedia input as a reference to Google Cloud Storage
    – a blob storage service provided by Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Exact details on how to construct a multimodal input might depend on the provider
    of the LLM (and a corresponding LangChain integration handles a dictionary corresponding
    to a part of a `content` field accordingly). For example, Gemini accepts an additional
    `"video_metadata"` key that can point to the start and/or end offset of a video
    piece to be analyzed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'And, of course, such multimodal parts can also be templated. Let’s demonstrate
    it with a simple template that expects an `image_bytes_str` argument that contains
    encoded bytes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Using GPT-4 Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After having explored image generation, let’s examine how LangChain handles
    image understanding using multimodal models. GPT-4 Vision capabilities (available
    in models like GPT-4o and GPT-4o-mini) allow us to analyze images alongside text,
    enabling applications that can “see” and reason about visual content.
  prefs: []
  type: TYPE_NORMAL
- en: 'LangChain simplifies working with these models by providing a consistent interface
    for multimodal inputs. Let’s implement a flexible image analyzer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The model provides a rich, detailed analysis of our generated cityscape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This capability opens numerous possibilities for LangChain applications. By
    combining image analysis with the text processing patterns we explored earlier
    in this chapter, you can build sophisticated applications that reason across modalities.
    In the next chapter, we’ll build on these concepts to create more sophisticated
    multimodal applications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After setting up our development environment and configuring necessary API keys,
    we’ve explored the foundations of LangChain development, from basic chains to
    multimodal capabilities. We’ve seen how LCEL simplifies complex workflows and
    how LangChain integrates with both text and image processing. These building blocks
    prepare us for more advanced applications in the coming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll expand on these concepts to create more sophisticated
    multimodal applications with enhanced control flow, structured outputs, and advanced
    prompt techniques. You’ll learn how to combine multiple modalities in complex
    chains, incorporate more sophisticated error handling, and build applications
    that leverage the full potential of modern LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Review questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the three main limitations of raw LLMs that LangChain addresses?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Memory limitations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tool integration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Context constraints
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing speed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost optimization
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which of the following best describes the purpose of LCEL (LangChain Expression
    Language)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A programming language for LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A unified interface for composing LangChain components
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A template system for prompts
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A testing framework for LLMs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Name three types of memory systems available in LangChain
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare and contrast LLMs and chat models in LangChain. How do their interfaces
    and use cases differ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What role do Runnables play in LangChain? How do they contribute to building
    modular LLM applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When running models locally, which factors affect model performance? (Select
    all that apply)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Available RAM
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: CPU/GPU capabilities
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Internet connection speed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Model quantization level
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating system type
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compare the following model deployment options and identify scenarios where
    each would be most appropriate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cloud-based models (e.g., OpenAI)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Local models with llama.cpp
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GPT4All integration
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Design a basic chain using LCEL that would:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a user question about a product
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Query a database for product information
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a response using an LLM
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a sketch outlining the components and how they connect.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare the following approaches for image analysis and mention the trade-offs
    between them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Approach A
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Approach B
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Subscribe to our weekly newsletter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](https://packt.link/Q5UyU).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Newsletter_QRcode1.jpg)'
  prefs: []
  type: TYPE_IMG
