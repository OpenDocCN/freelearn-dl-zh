- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Deepfake Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a deepfake is an involved process. The tools within the various software
    applications help to significantly reduce the amount of manual work required;
    however, they do not eliminate this requirement entirely. Most of this manual
    work involves collecting and curating source material, as well as cleaning up
    data for the final swap.
  prefs: []
  type: TYPE_NORMAL
- en: Whilst there are various applications available for creating deepfakes this
    chapter will use the open source software Faceswap ([http://www.Faceswap.dev](http://www.Faceswap.dev)).
    The general workflow for creating a deepfake is the same from application to application,
    but you will find the nuances and available options vary between packages.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that Faceswap, at its core, is a command-line application.
    However, it also comes with a GUI that acts as a wrapper to launch the various
    processes. Within this chapter, the GUI will be used to illustrate the workflow;
    however, most of the tasks performed here can also be run from the command-line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the deepfake workflow will be covered from the inception of
    the swap to the final product. Specifically, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying suitable candidates for a swap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the training images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying a trained model to perform a swap
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with all machine learning techniques, deepfakes can be created on any PC
    with a minimum of 4 GB of RAM. However, a machine with 8 GB of RAM or higher and
    a GPU (a graphics card) is strongly recommended. Training a model on a CPU is
    likely to take months to complete, which does not make it a realistic endeavor.
    Graphics cards are built specifically to perform matrix calculations, which makes
    them ideal for machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Faceswap will run on Linux, Windows, and Intel-based macOS systems. At a minimum,
    Faceswap should be run on a system with 4 GB of VRAM (GPU memory). Ideally, an
    NVIDIA GPU should be used, as AMD GPUs are not as fully featured as their Nvidia
    counterparts and run considerably slower. Some features that are available for
    NVIDIA users are not available for AMD users, due to NVIDIA’s proprietary CUDA
    library being accepted as an industry standard for machine learning. GPUs with
    more VRAM will be able to run more of the larger Faceswap models than smaller
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to rent cloud services (such as Google’s Cloud Compute or
    Amazon’s AWS) to run Faceswap remotely.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and setting up the software will not be covered in this chapter,
    as the method will vary between OSs, and detailed installation instructions can
    be found on the Faceswap website ([https://forum.Faceswap.dev/viewforum.php?f=4](https://forum.Faceswap.dev/viewforum.php?f=4)).
  prefs: []
  type: TYPE_NORMAL
- en: Identifying suitable candidates for a swap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While it is technically possible to swap any face with another, creating a convincing
    deepfake requires paying some attention to the attributes of your source and destination
    faces. Depending on what you hope to achieve from your deepfake, this may be more
    or less important to you, but assuming that you wish to create a convincing swap,
    you should pay attention to the following attributes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Face/head shape**: Are the shapes of the faces similar to one another? If
    one face is quite narrow and the other quite round, then while the facial features
    will be correct, the final swap is unlikely to be particularly convincing if the
    final swap contains a head shape that is significantly different from the individual
    you are attempting to target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hairline/hairstyles**: While it is possible to do full head swaps, these
    are generally harder to pull off, as hair is complex, and hairstyles can change
    significantly. You will generally be swapping the face but keeping the hair from
    the original material, so you need to keep hairline and hairstyles in mind when
    considering your swap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skin tone**: The neural network will do some work in matching the skin tone
    between the faces that you train the model on; however, this will only work to
    a certain extent. As some attributes of the original face are likely to still
    exist within the final swap when it is blended into the original frame, it is
    important to ensure that the natural skin tone between the faces is not significantly
    different.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you have identified your candidates for creating a deepfake, it is time
    to collect data to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be collecting, extracting, and curating the images
    to train our model. Far and away the best sources for collecting face data are
    video files. Videos are just a series of still images, but as you can obtain 25
    still images for every second of video in a standard 25 FPS file, they are a valuable
    and plentiful resource. Video is also likely to contain a lot more natural and
    varied poses than photographs, which tend to be posed and contain limited expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Video sources should be of a high quality. The absolute best source of data
    is HD content encoded at a high bitrate. You should be wary of video content acquired
    from online streaming platforms, as these tend to be of a low bitrate, even if
    the resolution is high. For similar reasons, JPEG images can also be problematic.
    The neural network will learn to recreate what it sees, and this will include
    learning compression artifacts from low-bitrate/highly compressed sources. Footage
    filmed on a modern-day smartphone or better, or extracted from Blu-ray or DVD
    sources, is ideal. One caveat to this is that you should avoid using HDR footage
    at all costs. HDR, by its very nature, contains images within a dynamic range.
    Neural networks expect the data they receive to be within a consistent range,
    so they struggle with HDR data and, quite often, cannot learn at all when provided
    with this kind of data.
  prefs: []
  type: TYPE_NORMAL
- en: You are looking to collect material from as many different sources as possible.
    It is a misconception that a model is trained for a specific scene. Neural networks
    of the type that deepfakes use benefit from highly varied data. As the neural
    network is looking to encode important features for each of the faces it sees,
    giving it as much varied data as possible will enable it to generate better encodings
    and better feature maps. This includes variety in poses, expressions, and lighting
    conditions. While the neural network will do some work to simulate different lighting
    conditions, this is to help augment already varied data rather than act as a replacement
    for missing data. The neural network will not be able to create poses and expressions
    that are significantly different from anything it has seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting faces from your source data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a variety of sources, the next step is to extract and **align**
    the faces (a process that normalizes the faces in the images) from these sources
    to build your training set. You are looking to collect between 500 to 50,000 faces
    for each side that you intend to train on. The variety of data is more important
    than the quantity of data. Five-hundred highly varied faces will lead to far superior
    results than 50,000 near-identical faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the extraction process, an “alignments file” will be created. This file
    (with a `.fsa` extension) contains information about the faces that have been
    discovered within each of your sources. With this in mind, it is good practice
    to set up a project folder structure to store your data so that you can easily
    locate and edit any of the data as required. A reasonable structure could be along
    the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – A suggested Faceswap folder structure](img/B17535_04_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A suggested Faceswap folder structure
  prefs: []
  type: TYPE_NORMAL
- en: For each side of the model (**A** and **B**), we are creating a folder to store
    the source videos in, along with their associated generated alignments files (**Videos**),
    and a faces folder to store the extracted faces (**Faces**). If you are extracting
    from images as a source for faces, then you can add an **Images** folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the video and image source files to their associated folders and launch
    the Faceswap application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The Faceswap GUI](img/Image98335.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The Faceswap GUI
  prefs: []
  type: TYPE_NORMAL
- en: The application is divided up into separate sections, depending on the task
    that you are currently performing. At this stage, we are extracting faces, so
    make sure that the **Extract** tab is selected.
  prefs: []
  type: TYPE_NORMAL
- en: There are many options available here, but we will just be focusing on those
    that are required to generate a training set. Optional or unnecessary options
    will be skipped for brevity, but you can view the tooltip for more information
    in the GUI.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Faceswap comes with built-in **tooltips** that explain what each option does.
    Hover over the entries to access the corresponding ToolTip.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section is where the location of the source material that we intend to
    extract faces from is entered, as well as the location that we wish to extract
    the faces to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Dir**: The location of the source video file or folder of images that
    contain the faces you wish to extract. Clicking the buttons on the right will
    launch a file browser to easily navigate to the correct location. Select the left-hand
    icon if extracting faces from a video file or the right-hand icon if extracting
    faces from a folder of images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Dir**: The location that identified faces should be extracted to.
    This should be a new folder within the **Faces** folder that you set up when creating
    your project folder structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.3 – The Data section for face extraction](img/B17535_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – The Data section for face extraction
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at the plugins we can use.
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The plugins section is where the neural networks that will we use to identify
    faces within the images are chosen, as well as those plugins that identify the
    key landmarks within the face and any neural network-based masks to apply to the
    extracted faces:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Detector**: The detector identifies faces within each of the images. The
    most robust detector at the time of writing is **S3Fd** (based on the paper Single
    Shot Scale-Invariant Face Detector: https://arxiv.org/abs/1708.05237). It is,
    however, resource-intensive, requiring at least 3 GB of GPU memory to run. It
    also runs incredibly slowly on a CPU due to its complexity. However, if you have
    the resources available, this is the detector to use. Otherwise, the **MTCneural
    network** ([https://arxiv.org/abs/1604.02878](https://arxiv.org/abs/1604.02878))
    detector should be used, which will run on far fewer resources and runs a lot
    quicker on a CPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aligner**: Responsible for identifying key facial landmarks. These landmarks
    are used for aligning any faces detected so that they are consistent for feeding
    the model during training. **FAN** ([https://arxiv.org/pdf/1703.07332.pdf](https://arxiv.org/pdf/1703.07332.pdf))
    is the best aligner and, if at all possible, should be the option you select here.
    It is, however, slow on a CPU, in which case, the **CV2-D** neural network aligner
    is available. While this will run a lot quicker on a CPU, it is far inferior to
    FAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Masker**: When extracting faces, it is also possible to use neural networks
    for masking an area of interest. Specifically, we are only interested in training
    our model on the face area of the extracted image. The background to the face
    that is included in the extracted images just adds noise to the model, which it
    is advantageous to exclude. Two masks are always included by default, based on
    the landmark data generated by the aligner. The landmark-based masks are fine
    for a lot of use cases, but they are limited insofar as they do not include the
    forehead within the masked area (they crop just above the eyebrows). The neural
    network-based masks attempt to address this issue by using AI to generate masks
    on extracted faces. Generally, the **BiSeNet-FP** mask works best.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is worth noting that masks do not need to be created at extract time. Faceswap
    includes a tool to add masks to training sets after extraction has been performed.
    This can be beneficial, as often you will not want to spend time generating neural
    network-based masks until you are happy that the extracted faces are correct and
    properly aligned. Be aware that the more neural network-based masks that are added
    (multiple masks can be selected), the longer the extraction process will take.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – A side-by-side comparison of a BiSeNET-FP (neural network-based)
    mask (top) and a components (landmarks-based) mask (bottom). The left image is
    the original aligned face, the right image is the generated mask, and the center
    image is the aligned face with the mask applied](img/B17535_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – A side-by-side comparison of a BiSeNET-FP (neural network-based)
    mask (top) and a components (landmarks-based) mask (bottom). The left image is
    the original aligned face, the right image is the generated mask, and the center
    image is the aligned face with the mask applied
  prefs: []
  type: TYPE_NORMAL
- en: '**Normalization**: When the aligner is looking to identify key landmarks, it
    can help to normalize the image being fed to the plugin. Generally, **histogram
    normalization** (**Hist**) or **contrast limited adaptive histogram equalization**
    (**CLAHE**) works best, but it will depend on the source material.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Re Feed**/**Rotate Images**: To generate a training set, these options are
    not necessary and should be left at their default values (**1** and blank respectively).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The Plugins selection options for face extraction](img/B17535_04_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The Plugins selection options for face extraction
  prefs: []
  type: TYPE_NORMAL
- en: Face processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Face processing is any task that should be performed after faces have been identified
    within an image. The only option of concern here is **Min Size**. False positives
    can be found by the detector (items that the detector considers a face but are
    not actually so). This option allows you to discard faces that do not meet this
    minimum threshold (measured in pixels from corner to corner of the detected face).
    The value specified here will vary, depending on the size of the input frames
    and how many of the images are taken up by the faces you are interested in. Leaving
    it set at a low value, regardless, can help with the curation of data within the
    next phase.
  prefs: []
  type: TYPE_NORMAL
- en: The other option within this section is the face filter. Generally, it is advised
    to avoid using the filter, as it can be somewhat hit and miss at correctly identifying
    faces and will significantly slow down the extraction process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – The Face Processing options for face extraction](img/B17535_04_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – The Face Processing options for face extraction
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The final area of interest is the **Output** section. The only option that needs
    to be amended from default is the **Extract Every N** option. While you need lots
    of images to train a model, variety is more important. When using video as a source
    of training images, each frame is extremely similar to its immediate neighbor.
    To reduce the number of similar faces that will be extracted, it is possible to
    skip frames to parse for faces. The number that is set here will depend on the
    frame rate of your video, as well as the number of sources that you intend to
    extract faces from. Generally, between 2 to 8 frames per second of video is a
    good number to aim for (for example, for a 30 fps video, an **Extract Every N**
    value of 6 will extract faces from 5 frames for every 1 second of video).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The Output options for face extraction](img/B17535_04_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The Output options for face extraction
  prefs: []
  type: TYPE_NORMAL
- en: Extract
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the options have all been set, the **Extract** button can be pressed.
    This will extract all of the discovered faces into your given folder and generate
    the corresponding alignments file. The amount of time this will take will depend
    on the available hardware, the length of the source material, and the plugins
    that have been chosen.
  prefs: []
  type: TYPE_NORMAL
- en: Curating training images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once faces have been extracted, the data needs to be curated. The neural networks
    used to identify, extract, and align faces do a good job, but they are not perfect.
    Along with the correctly detected and aligned faces, it is most likely that a
    not insignificant amount of unusable data will also have been collected. This
    may include faces other than the target, false positives (parts of the image that
    the neural network considers a face but are not actually so), and misaligned faces
    (faces that have been correctly identified but the aligner has failed to align
    them correctly).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – A concentrated example of misaligned faces](img/B17535_04_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – A concentrated example of misaligned faces
  prefs: []
  type: TYPE_NORMAL
- en: Examining a folder that contains a significant number of unusable faces, it
    may, at first, appear to be a mammoth task to clean up and curate the training
    images. Fortunately, neural networks can again be leveraged to make this job a
    lot easier.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting the faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The faces will have been extracted in frame order – that is, they will exist
    within the output folder in the order that they were discovered within the source
    video or images. Faceswap includes a number of sorting mechanisms to arrange these
    faces in an order to enable easier pruning, which can be accessed by selecting
    the **Tools** tab, followed by the **Sort** sub-tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – The Sort tool within the Tools section of Faceswap](img/B17535_04_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – The Sort tool within the Tools section of Faceswap
  prefs: []
  type: TYPE_NORMAL
- en: The most powerful sorting method, by some distance, is to sort by **face**.
    This uses the neural network **VGG Face 2** developed by researchers at the Visual
    Geometry Group at the University of Oxford ([https://arxiv.org/abs/1710.08092](https://arxiv.org/abs/1710.08092)).
    Faceswap utilizes this network to cluster similar faces together, making the data
    far easier to parse.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the **Sort** section of Faceswap, the following options should be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: The folder that contains the extracted faces that are to be curated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sort By**: Select **Face**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Final Process**: Rename (the faces will be sorted in place, with the filenames
    renamed)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All other options can be left at their default values, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – The selected options within Faceswap’s Sort tool](img/B17535_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – The selected options within Faceswap’s Sort tool
  prefs: []
  type: TYPE_NORMAL
- en: Press the **Sort** button to launch the sorting process. Depending on your setup
    and the number of faces to be sorted, this may take some time.
  prefs: []
  type: TYPE_NORMAL
- en: Removing faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the sorting process has completed, the OS’s standard file manager can be
    used to scroll through the folder and remove any incorrect faces. The sorting
    process will have grouped all of the similar faces together, making this a far
    simpler task, as all of the misidentified faces can be bulk selected and deleted.
    Just ensure that the sort order of the folder is by **filename**.
  prefs: []
  type: TYPE_NORMAL
- en: It is also a good idea, at this stage, to remove any faces that are not of suitable
    quality for training. This includes those images that are of the target individual
    but are sub-standard in some way – for instance, if the face is not aligned correctly
    within the frame, the face is significantly obstructed, or the quality of the
    image is too low. Generally, training images should be of high quality. Not all
    low-quality/blurry images need to be removed from the training set, as the neural
    network will also need to know how to recreate these lower-quality/resolution
    images; however, they should form a minority of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the folder just contains faces that are suitable for training, it is
    a best practice to clean the alignments file. This is the process of removing
    the faces that you have just deleted from that file. This enables us to go back
    to the video source and alignments file and re-extract the faces, while avoiding
    the requirement to sort the data again. This action has a second advantage, insofar
    as it will also rename all the faces their original filenames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, Faceswap has a tool to help with this – specifically, navigate
    to the **Tools** tab and then the **Alignments** sub-tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – The location of the Alignments tool within Faceswap](img/B17535_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The location of the Alignments tool within Faceswap
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Alignments** tool allows multiple actions to be performed on the alignments
    file. The job that is of interest is **Remove-Faces**, which examines a folder
    of faces and removes the faces that you have deleted from the alignments file.
    The following options should be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Job**: Select **Remove-Faces**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.fsa` alignments file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Faces Folder**: Browse to the location that contains your curated face set
    and select the folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All other options can be left at their default values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – The selected options within Faceswap’s Alignments tool](img/B17535_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – The selected options within Faceswap’s Alignments tool
  prefs: []
  type: TYPE_NORMAL
- en: Press the **Alignments** button. A backup of the alignments file will be taken,
    then the process will remove all the faces that do not appear in the faces folder
    from the file. Finally, the faces will be renamed with their originally extracted
    names.
  prefs: []
  type: TYPE_NORMAL
- en: Collating training images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the training sources have been curated, the images can be collated
    into the final training sets.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If the required mask was not generated during extraction, it can also be added
    to the faces now, using the **Mask** tool (the **Tools** tab | the **Mask** sub-tab).
    This should be done prior to collating the final training sets.
  prefs: []
  type: TYPE_NORMAL
- en: This is as simple as taking all of the contents of each source’s extracted faces
    and placing them all into the same folder (one folder for the **A** side, and
    one folder for the **B** side). All the information required by Faceswap to train
    on these images is stored within the EXIF header of the PNG images.
  prefs: []
  type: TYPE_NORMAL
- en: Some final curating may be required to bring the number of training images down
    to a manageable size, but anywhere in the region of 500 to 50,000 images per side
    of the model is reasonable.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data has been collected and curated, it is time to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part of the process requires the least amount of manual intervention but
    will take the longest in terms of compute time. Depending on the model chosen
    and the hardware in use, this can take anywhere from 12 hours to several weeks
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: It is advised to use a relatively lightweight model when creating a deepfake
    for the first time. Creating swaps is fairly nuanced, and understanding what works
    and what doesn’t comes with experience. Whilst Faceswap offers several models,
    starting with the **Original** or **Lightweight** model will allow you to gauge
    the performance of the swap relatively quickly, while not necessarily giving you
    the best possible final result.
  prefs: []
  type: TYPE_NORMAL
- en: Faceswap comes with numerous configurable settings for models and training.
    These are available within the **Settings** menu of the application. To cover
    all of these settings is well outside of the scope of this walk-through, so default
    settings will be used unless otherwise stated.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Navigate to the **Train** tab of the Faceswap application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The training options section of Faceswap](img/B17535_04_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – The training options section of Faceswap
  prefs: []
  type: TYPE_NORMAL
- en: Faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section is where we tell the process where our training images are stored.
    If you followed the extraction and curation steps, then these faces will exist
    within your project folder, with a single folder for each side:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input A**: The path to the folder containing the extracted faces for the
    **A** side of the model (that is, the original face that is to be removed from
    the final video).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input B**: The path to the folder containing the extracted faces for the
    **B** side of the model (that is, the face that you wish to transpose to the final
    video).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The Faces options for training a model](img/B17535_04_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – The Faces options for training a model
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The model section is where Faceswap is instructed on which model to use, where
    that model should be stored, and any model-specific actions to perform at runtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model Dir**: The folder that the model should be stored in, or if you are
    resuming a pre-existing model, then the folder that contains the model to be resumed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are starting a new model, then this location should not pre-exist on
    the hard drive. When the model is created, the folder specified here will be created
    and populated with the model and associated files required by Faceswap to track
    training.
  prefs: []
  type: TYPE_NORMAL
- en: If you are resuming a previously created model, then this should point to the
    folder that was created when initially setting up the model (the folder created
    by Faceswap containing the associated model files).
  prefs: []
  type: TYPE_NORMAL
- en: '**Trainer**: Faceswap has multiple models available (named **Trainer** for
    legacy reasons). These models are more or less configurable within the **Settings**
    menu, depending on the model chosen. As discussed before, if you are just starting
    out, then it is advisable to use the **Original** or **Lightweight** model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are starting a new model, then the model selected here will be the model
    used for all future training sessions of it. It is not possible to change a model
    type once it has been created.
  prefs: []
  type: TYPE_NORMAL
- en: The other options within this section can be ignored for your first model, although
    they may become more relevant as you gain experience using the software. As with
    all the options, the tooltips will tell you what these additional options do.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – The Model options for training in Faceswap](img/B17535_04_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – The Model options for training in Faceswap
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These options relate to how the model should be trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch Size**: The number of faces to be fed through the model at once. Generally,
    a higher batch size will lead to a higher training speed; however, higher batch
    sizes will mean the model generalizes more. Increasing batch size is only sensible
    up to a limit. Anything beyond 128 and the model will start to struggle to obtain
    useful information for each batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch size is also VRAM-limited. For more complicated models, you will have
    little choice but to use a smaller batch size, and obviously, the less VRAM available
    on the GPU, the more limited you are.
  prefs: []
  type: TYPE_NORMAL
- en: '**Iterations**: This can be left at default unless it is desired that the model
    should stop after a certain number of iterations. Knowing when to stop a model
    comes from experience and is dictated by the quality of output, so it will never
    be after a “set number of iterations.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed**: This option is for multi-GPU users only. It allows for multiple
    video cards to be used, speeding up training by splitting batches over multiple
    devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: It can be beneficial to start training a model at a higher batch size to get
    the speed benefits, and then reduce it later in training to get the benefits of
    drilling down for details.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – The Training options within Faceswap](img/B17535_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – The Training options within Faceswap
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Augmentation** section allows you to enable or disable certain image
    augmentations (the way a neural network artificially increases the number of training
    images). When starting a new model, these should all be disabled (all augmentations
    are active). Later in the training session (when faces are becoming identifiable
    and more detailed), it can be desirable to turn some of these augmentations off:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Warp to Landmarks**: This is just an alternative warping technique. There
    is no conclusive evidence that enabling or disabling this option makes any real
    difference, so it is recommended to leave it disabled at all times.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Flip**: Faces are vaguely symmetrical. The neural network leverages this
    knowledge by flipping around 50% of the images horizontally. For nearly all use
    cases, this is fine, and all images can be flipped at all times. However, in cases
    when there are distinct details on one side of a face (for example, a beauty mark),
    then this option should be enabled when you see that faces start to take shape
    within the training preview window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Augment Color**: The color augmentation helps the neural network to match
    color and lighting between the A and B sides by artificially coloring and changing
    other visual attributes of the images it sees. Generally, this is always desirable
    and should be left on, but for some use cases, it can be desirable to disable
    this augmentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No Warp**: This is possibly the most important option within this section.
    Warping is incredibly important to how a neural network learns. It is absolutely
    imperative that all models commence with warping enabled (failure to do so will
    invariably lead to a sub-standard model or, worse, model collapse). However, later
    in training, particularly when attempting to drill down into finer details, this
    warp actually becomes detrimental to the model’s training, and so this option
    to disable the warp should be selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.17 – The Augmentation section to train a model](img/B17535_04_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – The Augmentation section to train a model
  prefs: []
  type: TYPE_NORMAL
- en: A rule of thumb is that if faces are recognizable and they do not appear to
    be getting any sharper over a significant period of time, then it is probably
    time to disable warping. Seeing clearly defined teeth and eye glare is a good
    indicator. It is important to note that it is near impossible to disable warping
    too late, but it is very definitely possible to disable warping too early, so
    err on the side of caution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: Color augmentation and warping images are both invaluable ways to get more mileage
    from your data. Like the other augmentations in this section, they change your
    image slightly, as a way to effectively get new images that the AI model hasn’t
    seen before.
  prefs: []
  type: TYPE_NORMAL
- en: Color augmentation works by slightly altering the colors of the image. This
    gives the model new colors to work with. This also helps the model with new lighting
    situations that might be absent from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Warping works by slightly modifying the shape of the face in the image. This
    helps if certain expressions are less common in your data. It also helps ensure
    that the decoder builds the face from memory and not just from a copy of the original
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Launching and monitoring training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model configuration has been entered and all the settings have been
    adjusted to their appropriate value, the **Train** button can be pressed to launch
    the training session. Training can and will take a long time to complete, and
    there is no mathematical or numerical measure to know when a model has finished
    training. Knowing when a model is unlikely to improve anymore mostly comes with
    experience; however, there are some indicators in place that can help us to determine
    whether it is time to stop training.
  prefs: []
  type: TYPE_NORMAL
- en: Previews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Probably the most important measure of the model’s progress is the preview images
    themselves. At each saved iteration, a series of preview images is generated to
    enable visualization of how the model is progressing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Faceswap’s GUI with the Preview window on the right](img/Image98480.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – Faceswap’s GUI with the Preview window on the right
  prefs: []
  type: TYPE_NORMAL
- en: This preview consists of 28 faces randomly picked from the training set (14
    for each side). For each training image, the original is shown, followed by an
    image showing the AI’s attempt to recreate the original face. The final image
    shows the AI’s attempt to swap the original face with the identity from the other
    side.
  prefs: []
  type: TYPE_NORMAL
- en: When training commences, these images may look like a solid color or a blurry
    image that is vaguely face-shaped; however, it will fairly quickly resolve into
    an identifiable face and slowly improve over time. It is worth noting that model
    improvement is not linear. While the faces will improve fairly quickly at first,
    this improvement will slow down until no visible difference will be seen from
    iteration to iteration. However, over a period of time, the model will improve.
    When comparing previews, it is not uncommon to compare the improvements of images
    that have been taken 10,000 to 100,000 iterations apart.
  prefs: []
  type: TYPE_NORMAL
- en: Loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another measure available to us is the loss for each iteration. Every time a
    batch is processed through the model, the neural network scores itself for how
    well it thinks it has recreated the images. While the loss value can swing wildly
    from batch to batch, over time the average value will drop. It is important to
    note that the actual value of the loss is not important. In fact, the value itself
    is effectively meaningless. The only issue to concern ourselves with is whether
    the value is dropping. This is for a couple of reasons; firstly, different loss
    functions will result in different numbers being generated, which are not comparable
    with each other. Secondly, the loss values given do not actually represent a score
    for anything that is useful for us to measure. The loss is generated by how well
    the neural network thinks it is recreating the **A** and **B** faces. It does
    this by looking at the original face and its recreation, and scoring itself based
    on the quality of the recreation. However, this score is not useful to us. What
    we would like to see is a score based on how well the neural network is taking
    a face and swapping it with another face. As there are no real-world examples
    of people who have had their faces swapped, this is an impossible measure to achieve,
    so we make do with using the loss values for face reconstruction rather than for
    face swapping.
  prefs: []
  type: TYPE_NORMAL
- en: Graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While loss on its own and at any given time is not a useful measure, its trend
    over time is. Ultimately, if loss is decreasing, the model is learning. The further
    a model is trained, the harder it is to ascertain whether the loss is actually
    still decreasing over time. Faceswap collects logging data for each batch passed
    through the model in the form of **TensorFlow event logs**. These logs are stored
    in the same folder as the model and can be used to visualize data in Faceswap’s
    GUI, or analyzed using **TensorBoard** (TensorFlow’s visualization toolkit).
  prefs: []
  type: TYPE_NORMAL
- en: 'To analyze the learning progress of an existing model, navigate to the `state.json`
    file, located within the model folder. Once the data is parsed, session summary
    statistics will be displayed for each training session carried out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – Statistics for a series of Faceswap training sessions](img/B17535_04_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – Statistics for a series of Faceswap training sessions
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking the green **Graph** icon next to any of the session rows will bring
    up the training graph for that session, displaying the loss for each iteration
    during it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.20 – A training graph showing loss over time for a Faceswap training
    session](img/B17535_04_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – A training graph showing loss over time for a Faceswap training
    session
  prefs: []
  type: TYPE_NORMAL
- en: For long sessions, it can be hard to ascertain whether the loss is still falling,
    due to the sheer quantity and range of data. Fortunately, it is possible to zoom
    into a selected range of the graph to get a better idea, by selecting the **Zoom
    to Rectangle** button toward the bottom right of the screen and selecting the
    area of interest. In this example, we shall zoom in on the last 100,000 iterations
    trained and make sure that we are viewing the rolling average of the loss values.
    As we can see, the loss is still clearly improving.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – A zoomed-in view of the last 100,000 iterations of a training
    session](img/B17535_04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – A zoomed-in view of the last 100,000 iterations of a training
    session
  prefs: []
  type: TYPE_NORMAL
- en: Manual intervention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While training a model is mostly a “fire and forget” task, toward the end of
    training, it can help to take some steps to improve the final output. These steps
    are by no means necessary, but they can help with the final product. For any of
    these actions to take effect, the model will need to be stopped, the relevant
    settings adjusted, and then the model can be resumed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Disabling Warp**: As previously mentioned, warping is imperative for a model
    to learn; however, as the model enters the middle to later stages of training,
    the warping augmentation can actually hurt the model in terms of image fidelity
    and clarity. Enabling the **No Warp** option is good practice for a high-quality
    final swap. As a general rule of thumb, this option should not be selected until
    you are at least 50% through the total train or you can see clearly defined features,
    such as individual teeth and eye glare. It is very hard to disable warping too
    late, but it is very easy to disable it too early. If the previews still look
    like they appear to be improving, then it is probably too early to disable warping.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lowering Learning Rate**: As the model enters the late stage of training,
    it can help to lower the learning rate. This can help the model to drill down
    into the finer details. It is common to lower the learning rate a little, resume
    training, lower it some more, resume training, and repeat this cycle until you
    are happy with the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fit Training**: A technique that can help is fitting data to the actual scene
    that is to be swapped. While it is not recommended to fully train a model only
    using data that will appear within the final swap, using this data can be useful
    to fine-tune an otherwise fully trained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you are happy that you have trained the model as far as you can, it is
    time to take the trained model and apply it to a source video file to swap the
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a trained model to perform a swap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the model has completed training, it can be used to swap the faces on any
    video to that contains the individual that is to be swapped out. Three items are
    required to successfully perform a swap – a video/series of images, a trained
    model, and an alignments file for the media that is to be converted. The first
    two items are self-explanatory; the alignments file is the one item we need to
    create.
  prefs: []
  type: TYPE_NORMAL
- en: The alignments file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The alignments file is a file bespoke to Faceswap, with a `.fsa` extension.
    This file should exist for every media source that is to be converted. It contains
    information about the location of faces within a video file, the alignment information
    (how the faces are orientated within each frame), as well as any associated masks
    for each frame.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an alignments file is fairly trivial. In fact, at least one has been
    generated already when we built a training set. The process for generating training
    data and generating an alignments file is the same, bar a few changes. For this
    reason, a lot of the steps will be familiar to you, as they are the same ones
    we performed within the *Extracting faces from your source data* section. The
    most notable difference in this section is that alignment information needs to
    be generated for every single frame within the media source, while for generating
    a training set, it is more common to only extract faces for a subset of frames
    within the source material.
  prefs: []
  type: TYPE_NORMAL
- en: Please refer back to *Extracting faces from your source data* for a more detailed
    explanation of the common options between running an extract to generate training
    data and running an extract to perform a swap.
  prefs: []
  type: TYPE_NORMAL
- en: Within the Faceswap application, select the **Extract** tab, and then follow
    the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section is the location of the source material that we need to generate
    alignments for, as well as an output folder that faces will be extracted to. The
    faces generated here are not used by the Faceswap process at all, but they are
    very useful for cleaning our alignments file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Dir**: The location of the media that we intend to swap the faces within'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Dir**: The location that identified faces will be extracted to'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.22 – The Data options within Faceswap’s extraction settings](img/B17535_04_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – The Data options within Faceswap’s extraction settings
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The plugins that are selected are likely to be the same as those selected when
    generating a training set, so refer to the previous section for more information
    on the options. Only one option within this section will likely change when extracting
    an alignments file:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Re Feed**: As the extraction process has no understanding of temporal coherence
    (that is, how each frame relates to the previous and subsequent frame), it can
    lead to “jittery” alignments within the final swap. This means that the face in
    the final swap moves a small amount from frame to frame. While this is not important
    for training sets, it is important when generating the file for the final swap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re Feed is a mechanism to help prevent this jitter by feeding a detected face
    into the aligner a set number of times and taking the average of the results.
    It is worth noting that each increase in the re-feed amount will slow extraction
    down, as the data needs to be passed through the process multiple times. The higher
    this number is set, the smoother the final output should be, but it is diminishing
    returns. Setting the value too high is unlikely to net any visible benefit but
    will take significantly longer to run an extract.
  prefs: []
  type: TYPE_NORMAL
- en: Set this to a value that brings you satisfactory results, while also running
    at a speed you can live with. Any value above 1 should give improved results over
    extracting without re-feed. A re-feed value of 8–10 will likely get the output
    close to as good as it can be.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.23 – The Plugins options within Faceswap’s extraction settings](img/B17535_04_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – The Plugins options within Faceswap’s extraction settings
  prefs: []
  type: TYPE_NORMAL
- en: Face Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with extracting training faces, **Min Size** is the only option within this
    section that may need to be adjusted. The value specified here will generally
    correspond to the size of the faces within the source material, but leaving it
    set at a low value, regardless, can help with removing some false positives that
    are clearly not valid faces.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – The Face Processing options within Faceswap’s extraction settings](img/B17535_04_024..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – The Face Processing options within Faceswap’s extraction settings
  prefs: []
  type: TYPE_NORMAL
- en: Output
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, the **Output** section should be reviewed and updated. The only option
    that needs to be amended from the settings used for extracting a training set
    is **Extract Every N**. It is imperative that every frame has a corresponding
    entry in the generated alignments file, so this should be set to **1**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – The Output options within Faceswap’s extraction settings](img/B17535_04_025..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – The Output options within Faceswap’s extraction settings
  prefs: []
  type: TYPE_NORMAL
- en: Extract
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the appropriate settings have been locked in, press the **Extract** button,
    to generate the alignments file and extract the found faces into the given folder.
    The amount of time this will take will depend on the available hardware, the length
    of the source material, and the re-feed value that has been set.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the alignments file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similarly to when we collected faces to build a training set, the extraction
    process will have done a decent job of identifying faces, but it will not have
    done a perfect job, so some manual processing is now required to clean up the
    alignments file. The process is the same as that in the *Curating training images*
    section with an additional step, so follow the steps within that section to perform
    the initial cleansing of the alignments file. Once unwanted faces have been removed
    from the alignments file, the folder of extracted faces can be deleted. The faces
    are not actually used by the conversion process; they are just used as a mechanism
    to clean the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have a source video that is the target for swapping
    faces and a corresponding alignments file that holds information about the location
    of faces within that file. It is entirely feasible to run a conversion at this
    stage, but another step is required for the best results – fixing the alignments
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing the alignments file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whilst we have removed unwanted faces, false positives, and any clearly misaligned
    faces from our alignments file, some further work is required to clean up the
    file for the final conversion. The main reasons for this are to fix frames where
    the following scenarios occur:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple faces have been identified. Sometimes, the detector will find two faces
    in a frame, but the aligner performs alignment on the same face twice. This often
    happens when two faces appear close to each other within a frame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The face is not aligned correctly. Sometimes, the face may appear aligned correctly
    when scanning through the folder of images, but examining the landmarks will demonstrate
    that this is not the case. These faces will sometimes convert correctly, but often
    this misalignment will lead to a messy swap for those frames (the swap will not
    look quite correct, it may look blurry, or may flicker between frames).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A face hasn’t been identified. Ensuring that all faces being swapped have been
    identified is necessary; otherwise, the original face, rather than the swapped
    face, will appear in those frames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mask has not been detected correctly. Depending on the conditions of the
    source frame, some neural network-based masks may not have been detected correctly,
    so these need to be fixed up. Depending on how the mask has been rendered, an
    incorrect mask may mean that parts of the original face show through, or parts
    of the background frame do not render correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, Faceswap provides tools to make this process easier – specifically, the
    **Manual tool**, which enables the visualization and editing of alignments/masks
    within the context of the original frame.
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch the Manual tool, navigate to the **Tools** tab and then the **Manual**
    sub-tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – The location of the Manual tool within Faceswap’s GUI](img/B17535_04_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – The location of the Manual tool within Faceswap’s GUI
  prefs: []
  type: TYPE_NORMAL
- en: Assuming that the alignments file for the video to be converted is in the default
    location, then only one argument needs to be provided to launch the Manual tool
    the location of the source video/folder of images that is to be converted. Specify
    this location within the **Frames** box and hit the **Manual** button to launch
    the tool.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.27 – The Data options for launching Faceswap’s Manual Tool](img/B17535_04_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – The Data options for launching Faceswap’s Manual Tool
  prefs: []
  type: TYPE_NORMAL
- en: Once the tool loads, you will be greeted with a main “video” window that shows
    the source that is being worked on and a secondary window that displays all of
    the faces that exist within the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – The Faceswap manual tool showing the video window at the top
    and the faces viewer at the bottom](img/B17535_04_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – The Faceswap manual tool showing the video window at the top and
    the faces viewer at the bottom
  prefs: []
  type: TYPE_NORMAL
- en: 'The top left buttons allow you to perform different actions on the alignments,
    so hover over the tooltips to see what is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – The Editor selection buttons within Faceswap’s Manual tool](img/B17535_04_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – The Editor selection buttons within Faceswap’s Manual tool
  prefs: []
  type: TYPE_NORMAL
- en: 'Another area of interest is the **Filter**. The Filter is a pull-down list
    that is located between the frames and faces windows and enables you to filter
    the frames and faces shown in the tool by certain criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – The face Filter options within Faceswap’s Manual tool](img/B17535_04_030.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – The face Filter options within Faceswap’s Manual tool
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the faces window also has some buttons on the left-hand side, which
    enable you to toggle the display of the face landmark mesh and the selected mask
    for faces displayed in the faces area:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31 – The face viewer buttons within Faceswap’s Manual tool](img/B17535_04_031.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31 – The face viewer buttons within Faceswap’s Manual tool
  prefs: []
  type: TYPE_NORMAL
- en: Removing multiple faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To remove any extra faces from frames that contain multiple faces, select the
    **Multiple Faces** filter. The main video window will now only show any frames
    that contain multiple faces. If no frames show in the top window and no faces
    show in the bottom window, then there are no frames with multiple faces, and you
    can move on to the next action.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing which face to remove is sometimes not obvious, so press the **Display
    landmarks mesh** button to the left of the faces window to bring up the landmarks
    overlay. If none of the faces are obviously misaligned (when the displayed landmarks
    mesh does not correspond to the underlying facial features), then any of the multiple
    faces can be deleted; otherwise, aim to delete the face with the landmarks that
    correspond least with the underlying face.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to delete faces from the frame. They can be deleted from
    the main video window by hovering over the unwanted face and pressing the **Del**
    key, or by right-clicking on the unwanted face in either the video or faces window
    and selecting **Delete Face**. When a frame no longer contains multiple faces
    (just one face remains in the frame), the faces for that frame will be removed
    from the faces window. Using this mechanism, it is usually quickest to right-click
    and select **Delete Face** from the faces window for all those faces that are
    unwanted until no faces remain.
  prefs: []
  type: TYPE_NORMAL
- en: Once all frames with multiple faces have been cleaned, hit the **Save** icon
    to save the changes to the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing misaligned faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Manual tool has misalignment detection built in. It is not perfect, but
    it does help in identifying and fixing the most obviously misaligned faces. While
    the detection can find faces that are obviously misaligned, it will not find faces
    where the face landmarks are in the correct location in relation to each other
    but do not correspond with the underlying face.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the **Misaligned Faces** filter to only display frames and faces where
    misaligned faces have been detected. A slider will appear next to the filter list
    box to control the distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32 – The Distance slider for selecting misaligned faces](img/B17535_04_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – The Distance slider for selecting misaligned faces
  prefs: []
  type: TYPE_NORMAL
- en: This is how far the landmark points within each face have to be from an “average
    face” to be considered misaligned. Low values will be less restrictive, so are
    likely to contain faces that are properly aligned but are at more extreme angles/poses.
    Generally, distances between 6 and **10** work fairly well. A distance of **10**
    should only show misaligned faces. A distance of 6 is likely to show a mixture
    of misaligned faces and more extreme poses, but it will catch misaligned faces
    that higher values will miss. It can be easier to set a higher distance (**10**,
    for example) and fix the misaligned faces that appear. Then, set the distance
    to 8 and repeat the process, continuing to step down until the filter is not catching
    enough misaligned faces in relation to more extremely posed faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regardless of the distance that has been set, to fix misaligned faces, the
    following actions should be taken:'
  prefs: []
  type: TYPE_NORMAL
- en: Enable the landmark mesh for the faces viewer by toggling the landmarks button
    to the left of the faces viewer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on a face that has misaligned landmarks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Within the main frame editor, make sure that the Bounding Box Editor is selected.
    This editor allows for control of the blue box around the face. This is the “detected
    face” box that was picked up by the face detector during the extraction phase.
    Adjusting this box will update the face that is fed to the aligner, with new landmarks
    being calculated. Continue to adjust the box until the landmarks align correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the landmarks are not aligning, it can help to switch between the different
    **Normalization Methods** options in the right-hand settings box. Different methods
    work better or worse in different situations, but **Hist** or **Clahe** tend to
    return the best results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some faces can be stubborn (difficult angles, obstructions, or bad lighting).
    In these cases, it can be next to impossible for the aligner to detect the landmarks.
    A couple of other editors can be used in these situations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Extract Box Editor**: This editor shows a green box that corresponds to the
    area of the frame that will be extracted if face extraction is run. It is possible
    to move, resize, and rotate this extract box, which will impact the location of
    the landmarks within the extract box. This can be leveraged to quickly align a
    face by copying the landmarks from the previous or next frame (assuming that the
    landmarks have not changed too much between frames – for example, a scene change)
    and quickly adjusting the extract box to fit the current frame.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Landmark Point Editor**: This editor allows for the location of each individual
    point within the 68 landmarks to be manipulated. This level of granular control
    is rarely necessary, but it exists if it is needed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the obviously misaligned landmarks have been fixed, hit the **Save** button
    to update the changes to the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: Adding missing faces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some frames may not have had faces identified where they should have been. The
    most common reason for this is that the detector did find a face, but the aligner
    failed to align it correctly, and then the face was deleted during the sorting
    process. Again, the Manual tool has a filter to help with this.
  prefs: []
  type: TYPE_NORMAL
- en: Select the **No Faces** filter to filter the top window to only those frames
    where no faces appear. The bottom window will remain empty for this particular
    filter. Navigate through the video until a frame that contains a face that has
    not been detected is reached, and make sure that the Bounding Box Editor is selected.
  prefs: []
  type: TYPE_NORMAL
- en: Landmarks can be created by clicking over a face within a frame, or copied from
    the previous or next frame and then amended. The bounding box can then be edited
    in the same way as in the previous step, with the same caveat about difficult
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: When all frames that were missing faces have been fixed, hit the **Save** button
    to save the changes to the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: Final alignments fixups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the obvious missing and misaligned faces have been fixed, it’s time to
    perform any final fixes to the alignments file. This is as simple as scrolling
    through all of the faces in the faces viewer and fixing any faces that remain
    misaligned. The faces viewer window can be expanded to show more faces within
    a single screen, and then the page-up/page-down buttons can be used to scroll
    through the faces a page at a time. When a misaligned face is discovered, it can
    be clicked on, and then the Bounding Box Editor can be used to re-align the face
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, once all faces have been reviewed and fixed, press the **Save** button
    to save the final changes to the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the alignments file has been fixed, you can close the Manual tool.
  prefs: []
  type: TYPE_NORMAL
- en: Regenerating masks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If an neural network-based mask is to be used for the swap, then these masks
    will need to be re-generated for any faces where the alignment data has been edited.
    The reason for this is that the aligned face, generated from the landmarks, is
    used to generate the face that is fed into the masking model. Once these landmarks
    have been edited, the mask is invalidated, so the process automatically deletes
    these invalid masks when the alignments are changed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, Faceswap provides a tool to add masks to existing alignments files –
    the appropriately named **Mask tool**. This tool can be used to generate masks
    that did not previously exist in the alignments file, regenerate all masks, or
    just populate masks for those faces that are missing the specified mask:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the **Tools** tab and then the **Mask** sub-tab:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.33 – The location of the Mask tool within Faceswap’s GUI](img/B17535_04_033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.33 – The location of the Mask tool within Faceswap’s GUI
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the **Data** section, add the path to the video file to regenerate masks
    for the **Input** field, as well as the corresponding alignments file for the
    **Alignments** field. As the source to be worked on are the final frames to swap
    onto, make sure that **Frames** is selected under **Input Type**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.34 – The Data settings of Faceswap’s Mask tool](img/B17535_04_034.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.34 – The Data settings of Faceswap’s Mask tool
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Process** section, select the mask that is to be populated into the
    alignments file for **Masker**. Under **Processing**, select **Missing** if masks
    have already been generated and the goal is to repopulate those masks that are
    associated with faces that have had their alignments fixed; otherwise, select
    **All** to generate masks for every face within the alignments file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.35 – The Process settings of Faceswap’s Mask tool](img/B17535_04_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.35 – The Process settings of Faceswap’s Mask tool
  prefs: []
  type: TYPE_NORMAL
- en: The **Output** section is just for visualizing the masks, serving no practical
    purpose, so it can be ignored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press the **Mask** button to generate the missing masks and save them to the
    alignments file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fixing masks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A final optional step is to fix up the generated masks. Much like face alignment,
    the neural networks that generate the masks are good, but they are often not perfect.
    This can be down to a number of reasons, such as lighting conditions and the quality
    of an image. In particular, obstructions in front of the face are not handled
    well by any of the maskers, so these will need to be manually edited.
  prefs: []
  type: TYPE_NORMAL
- en: This should be the absolute last action performed on the alignments file. Any
    edits performed on landmark data within the alignment file will strip any neural
    network masks from the file and overwrite any edited landmark-based masks with
    the latest landmark data, destroying any manual edits that have been performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Manual tool, used to fix up the alignments, can also be used to fix masks:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the Manual tool by selecting the **Tools** tab, followed by the **Manual**
    sub-tab, and launch in the same way as before.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Mask Editor** button from the buttons next to the frame viewer,
    and then select the mask type to be edited from the right-hand side options panel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Press the **Mask Display** toggle button next to the faces viewer to display
    the selected mask within the faces window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll through the faces window, looking for masks that require fixing. If a
    face is discovered that requires editing, it can be clicked on to bring the relevant
    frame into the frame viewer. The **Brush** and **Eraser** tools can then be used
    to paint in or out the desired mask areas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all the masks have been fixed, press the **Save** button to save the mask
    edits to the alignments file.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Preview tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is possible to process the swap now and view the final output. However, some
    settings will need to be adjusted on a case-by-case basis, specifically various
    post-processing actions, such as mask erosion/blending, color correction, and
    sharpening.
  prefs: []
  type: TYPE_NORMAL
- en: 'Faceswap includes the **Preview tool** to help lock these settings in prior
    to running the final conversion, which can be accessed by selecting the **Tools**
    tab and then the **Preview** sub-tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.36 – The location of the Preview tool within Faceswap’s GUI](img/B17535_04_036.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.36 – The location of the Preview tool within Faceswap’s GUI
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch the tool, provide the location of the video you intend to swap onto
    in the **Input Dir** field, and the folder that contains the trained model in
    the **Model Dir** field. The **Alignments** field can be left blank, unless the
    alignments file has been moved or renamed, in which case it will need to be explicitly
    specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.37 – The Data settings for Faceswap’s Preview tool](img/B17535_04_037.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.37 – The Data settings for Faceswap’s Preview tool
  prefs: []
  type: TYPE_NORMAL
- en: Press the **Preview** button to launch the tool.
  prefs: []
  type: TYPE_NORMAL
- en: The tool is split into three sections. The main window shows the faces from
    the original frame in the top row, with the swap applied with current settings
    in the bottom row. As settings are adjusted, the bottom row will update to reflect
    these changes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.38 – Faceswap’s Preview tool](img/B17535_04_038.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.38 – Faceswap’s Preview tool
  prefs: []
  type: TYPE_NORMAL
- en: The bottom-left panel displays command-line choices, while the bottom-right
    panel displays plugin settings.
  prefs: []
  type: TYPE_NORMAL
- en: Command-line choices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These are parameters that are chosen each time the conversion process is run
    (these options are not persistent), so you will need to remember what is set here
    to replicate it in the main Faceswap conversion process. Specifically, the options
    that can be set here are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.39 – The Preview Tool’s Command Line Choices](img/B17535_04_039.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.39 – The Preview Tool’s Command Line Choices
  prefs: []
  type: TYPE_NORMAL
- en: '**Color**: The color-matching methodology to use. The best choice here will
    depend on the scene being converted. The **Color Balance**, **Manual Balance**,
    and **Match Hist** options have further configuration options that can be adjusted
    from the **Plugin** **Settings** section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mask Type**: The type of mask to use for overlaying the swapped face onto
    the original frame. By default, the landmarks-based **extended** and **components**
    masks will be available. Any additional neural network-based masks that exist
    within the alignments file will also be accessible, as well as the option to entirely
    disable the mask (**None**). The settings that control the blending of the chosen
    mask into the background frame are adjusted from the **Plugin** **Settings** section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plugin settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section contains the configuration settings for the various post-processing
    plugins available in Faceswap. Values selected here, once saved, are persisted
    for all future conversions. As such, unlike the **Command Line Choices** options,
    there is no need to make a note of what is being set within this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The plugin settings are split into three configuration groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Color**: Configuration options for color-matching plugins. The actual methodology
    to use is selected within the **Command Line Choices** section, but some of the
    choices have additional configuration parameters that are controllable here. Make
    sure that the correct methodology is set within the **Command Line Choices** section
    to observe any changes within the main window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mask**: Options to control the blending of the swapped image into the background
    frame. These settings are broken down into two further categories – **Box Blend**,
    which controls the settings that blend the extracted square containing the face
    into the background frame, and **Mask Blend**, which controls the settings for
    blending the mask around the face into the background frame. (Note that if **None**
    has been selected as the mask type in the **Command Line Choices** section, then
    any changes made within the **Mask Blend** settings will not be visible within
    the preview window.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much of an impact each of these settings will have on the final output will
    depend greatly on the coverage and centering options that were selected when training
    the model. For example, with a low coverage and legacy centering (that is, very
    closely cropped), it is entirely possible that an extracted face box is contained
    entirely within the mask, in which case **Mask Blend** settings would have no
    visible effect. Similarly, with high coverage, and face or head centering, it
    is possible that the full mask exists within the extract box, in which case the
    **Box Blend** settings would have no visible effect. In most cases, adjusting
    a combination of the two blending settings will be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling**: The final configurable plugin controls any artificial sharpening
    to apply to an image. Quite often, the swapped face will need to be upscaled to
    fit into the final frame. This section allows you to control any sharpening effects
    to help better upscale the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: To get a better impression of the effects of adjusting the mask plugin settings,
    select **Manual Balance** as the color command-line choice, and then adjust **Contrast**
    and **Brightness** to **–100** within the **Manual Balance** plugin setting. This
    will display the swap area as entirely black, which can make it easier to adjust
    the mask correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the conversion settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The actual configuration choices to be used will vary on a video-by-video basis;
    there are no hard and fast rules, so it is just a question of adjusting the settings
    until a satisfactory result is achieved. Once a plugin is configured correctly,
    that plugin’s configuration can be saved by clicking the bottom-right **Save**
    button. To save the settings for all plugins that have been adjusted, click the
    bottom-left **Save** button.
  prefs: []
  type: TYPE_NORMAL
- en: When appropriate settings have been locked in, make a note of the **Command
    Line Choices** settings and exit the Preview tool.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the swap
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model has been trained, the alignments file has been created, and the
    swap settings have been locked in, the final product can be created. The process
    of generating a swap is called **converting** – that is, converting the faces
    in a source video from their original form to the version generated from the trained
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Converting is probably the least involved of the main Faceswap processes. Access
    the **Convert** section of the Faceswap application by selecting the **Convert**
    tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.40 – The location of the Convert settings within Faceswap’s GUI](img/B17535_04_040.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.40 – The location of the Convert settings within Faceswap’s GUI
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section is used to tell the process where the assets are located to perform
    the swap, as well as where the final output should be exported to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input Dir**: The location of the source video or folder of images to be processed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output Dir**: The location that the converted media should be outputted to.
    This folder should not pre-exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alignments**: Optionally, specify the location of the alignments file. If
    the alignments file is in its default location (next to the source video) with
    the default name, then this can be left blank, as the file will be detected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reference Video**: This option is only required if the source is a folder
    of individual frames and the desired output is a video file. The reference video
    would be the original video file that the folder of frames was extracted from,
    and it provides the conversion process with the audio track and the FPS that should
    be compiled into the final video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Dir**: The location of the folder that contains the trained Faceswap
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 4.41 – The Data options within Faceswap’s Convert settings](img/B17535_04_041.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.41 – The Data options within Faceswap’s Convert settings
  prefs: []
  type: TYPE_NORMAL
- en: Plugins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several plugins available for the conversion process, which are selectable
    here. Two of the plugins will have been seen before when we used the Preview tool
    (**Color Adjustment** and **Mask Type**), so ensure that you select the same options
    here as those selected within the Preview tool for the output to remain consistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Color Adjustment**: The color correction to use. This will have been previewed
    and selected using the Preview tool, so select the same plugin here. The actual
    plugin to use will vary from project to project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mask Type**: The type of mask to use to overlay the swapped face over the
    original frame. The chosen mask here must exist within the alignments file (**Components**
    and **Extended** will always exist; other masks need to be generated). Generally,
    this will be the same mask that the model was trained with and will have been
    previewed with the Preview tool, so select the same mask that was used to preview.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Writer**: The plugin to use to create the final media. The writer plugins
    are used to generate the final product. **Ffmpeg** is used to create video files,
    **Gif** is used to create animated GIFs, and **OpenCV** and **Pillow** will create
    a folder of images, with OpenCV being quicker but having a more limited file format
    choice than Pillow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The writers can each be configured by selecting **Settings** | **Configure
    Settings** and selecting the relevant writer plugin under the **Convert** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.42 – Faceswap’s Convert plugin settings](img/B17535_04_042.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.42 – Faceswap’s Convert plugin settings
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to create the swap on a separate transparent layer only containing
    the swapped face and the mask, to overlay over the original frame in various external
    VFX applications. The **OpenCV** and **Pillow** writers both support this, with
    OpenCV allowing the generation of four-channel PNG images and Pillow allowing
    the generation of four-channel PNG or TIFF images. This option can be enabled
    by selecting the **Draw Transparent** option within either of these plugins’ configuration
    settings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.43 – The Plugins options within Faceswap’s Convert settings](img/B17535_04_043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.43 – The Plugins options within Faceswap’s Convert settings
  prefs: []
  type: TYPE_NORMAL
- en: Other settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Frame Processing** can be ignored in most cases. The most likely option of
    interest is **Output Scale** though, which scales the final output media by the
    designated amount. For example, setting an output scale of **50** for a 720p input
    video will result in a final output at 360p.'
  prefs: []
  type: TYPE_NORMAL
- en: The **Face Processing** section can be ignored. If the alignments file has been
    created correctly, then none of the options here are relevant. Similarly, most
    of the options within the **Settings** section can be ignored in most cases. The
    only possible exception to this is the **Swap Model** option. This can be used
    to create a swap in the opposite direction to which the model was trained – that
    is, instead of **A** > **B**, the conversion will run **B** > **A**. This can
    be useful if you have a model trained on a face pair and you wish to run conversion
    in the opposite direction, or if a model has accidentally been trained the wrong
    way around (the original face has been trained on the **B** side, with the desired
    swap trained on the **A** side).
  prefs: []
  type: TYPE_NORMAL
- en: Once all of the settings are set correctly, press the **Convert** button to
    apply the trained model on your source media and generate the swap in the output
    destination.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the workflow required to create a deepfake using
    the open source Faceswap software. The importance of data variety was discussed
    and the steps required to acquire, curate and generate face sets were demonstrated.
    We learned how to train a model within Faceswap, and how to gauge when a model
    has been fully trained, as well as learned some tricks to improve the quality
    of the model. Finally, we learned how to take our trained model and apply it to
    a source video to swap the faces within the video.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will begin to take a hands-on look at the neural networks
    available to build a deepfake pipeline from scratch using the PyTorch ML toolkit,
    starting with the models available for detecting and extracting faces from source
    images.
  prefs: []
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Getting Hands-On with the Deepfake Process'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part of the book is all about getting hands-on with the code. We will look
    deep into exactly what it takes to make a deepfake from beginning to end, leaving
    no stone unturned or line of code unexplained. If you’re here for the code, this
    is the section for you.
  prefs: []
  type: TYPE_NORMAL
- en: In the first chapter of this section, we’ll examine extraction. This is the
    process of getting all the faces out of a video so that we can use them in other
    stages of the process. We’ll look at the process of turning a video into frame
    images, then we’ll go through all the code necessary to turn the frames into clean,
    aligned faces with matching mask images ready for training. After that, we’ll
    look into training, examine the neural network from the bottom up, and then show
    the entire learning process of the model. Finally, we’ll get into conversion,
    where we’ll examine the process of going through every image to swap a new face
    onto the original, including turning it back into a video.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this part, you’ll know exactly how to code your own deepfakes
    from beginning to end.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B17535_05.xhtml#_idTextAnchor090), *Extracting Faces*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17535_06.xhtml#_idTextAnchor107), *Training a Deepfake Model*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17535_07.xhtml#_idTextAnchor123), *Swapping the Face back into
    the Video*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
