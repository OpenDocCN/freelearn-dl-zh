<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Going Deep with DQN</h1>
                </header>
            
            <article>
                
<p>In this chapter, you will be introduced to <strong>deep learning</strong> (<strong>DL</strong>) in order to handle newer, more challenging infinite <strong>Markov decision process</strong> (<strong>MDP</strong>) problems. We will cover some basics about DL that are relevant to <strong>reinforcement learning</strong> (<strong>RL</strong>), and then look at how we can solve a Q-learning. After that, we will look at how to build a Deep Q-learning or DQN agent in order to solve some Gym environments. </p>
<p>Here is a summary of the topics we will cover in this chapter:</p>
<ul>
<li>DL for RL</li>
<li>Using PyTorch for DL</li>
<li>Building neural networks with PyTorch</li>
<li>Understanding DQN in <span>PyTorch </span></li>
<li>Exercising DQN</li>
</ul>
<p>In this chapter, we introduce DL with respect to RL. <span>Applying DL to <strong>deep reinforcement learning</strong> (<strong>DRL</strong>) is quite specific and is not covered in detail here.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL for RL</h1>
                </header>
            
            <article>
                
<p><span>Over the course of the previous five chapters, we learned how to evaluate the value of state and actions for a given finite MDP. We learned how to solve various finite MDP problems using methods from MC, DP, Q-learning, and SARSA. Then we explored infinite MDP or continuous observation/action space problems, and we discovered this class of problems introduced computational limits that can only be overcome by introducing other methods, and this is where DL comes in. </span></p>
<div class="packt_tip">DL is so popular and accessible now that we have decided to cover only a very broad overview of the topic in this book. Anyone serious about building DRL agents should look at studying DL further on their own.</div>
<p>For many, DL is about image classification, speech recognition, or that new cool thing called a <strong>generative adversarial network</strong> (<strong><span>GAN</span></strong>). Now, these are all great applications of DL, but, fundamentally, DL is about learning to minimize loss or errors. So when an image is shown to a network in order to learn the image, it is first split up and then fed into the network. Then the network spits out an answer. The correctness of the answer is determined, and any error is pushed back into the network as a way of learning. This method of pushing errors back through the network is called <strong>backpropagation</strong>.</p>
<p>The basics of this whole system are shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-589 image-border" src="assets/1cc1fa9d-6928-4023-9c50-c0ce4829f2b0.png" style="width:43.08em;height:19.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">DL oversimplified</div>
<p>The DL network learns by backpropagating the errors back through the network as corrections to each cell, called a neuron, and internal to the neuron are parameters called weights. Each weight controls the strength of a connection to that neuron. The strength of each connection or weight in the network is modified using an optimization method based on the gradient descent. <strong>Gradient descent</strong> (<strong>GD</strong>) is a method derived from calculus that allows us to calculate the effect each connection/weight has on the answer. Working back, we can, therefore, use GD to determine the amount of correction each weight needs. </p>
<p>The major downside to using backpropagation with GD is that the training or learning needs to happen very slowly. Thus, many, perhaps thousands or millions of, images need to be shown to the network in order for it to learn. This actually works well when we apply DL to RL since our trial-and-error learning methods also work iteratively.</p>
<div class="packt_infobox">DeepMind and other companies are currently working on other methods of learning, aside from backpropagation for DL networks. There has even been talk of doing one-shot learning. That is, being able to train a network on just a single image. Much like the way we humans can learn.</div>
<p>Now, it is often said that DL can be interpreted in many ways. As RL practitioners our interest in DL will be its use as an equation solver. You see fundamentally that is all DL does: solve equations; whether they are equations for classifying images, performing speech translation, or for RL. Deep reinforcement learning is about applying DL in order to solve the learning equations we looked at in the previous chapters and more. In fact, the addition of DL also provides many further capabilities in learning that we will explore in the rest of this book. In the next section, we cover a broad overview of the common DL frameworks used for DRL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL frameworks for DRL</h1>
                </header>
            
            <article>
                
<p>There are a number of DL frameworks available for use, but only a few have been readily used in RL research or projects. Most of these frameworks share a number of similarities, so transferring knowledge from one to the other is relatively straightforward. The three most popular frameworks for RL in the past few years have been Keras, TensorFlow, and PyTorch. </p>
<p>A summary of the strengths and weaknesses of each framework is shown in the table here:</p>
<table style="width: 100%;border-collapse: collapse" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Framework</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Keras</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>TensorFlow</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>PyTorch</strong></td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Accessibility</strong></td>
<td class="CDPAlignCenter CDPAlign">Easiest to learn and use</td>
<td class="CDPAlignCenter CDPAlign">Provides a high-level Keras interface and lower-level interface.</td>
<td class="CDPAlignCenter CDPAlign">Medium to low-level interface</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Scalability</strong></td>
<td class="CDPAlignCenter CDPAlign">Scales well for smaller projects</td>
<td class="CDPAlignCenter CDPAlign">Scales to any size project and supported output network models may be run on many different platforms.</td>
<td>Well suited to large projects requiring scale</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Performance/Power</strong></td>
<td class="CDPAlignCenter CDPAlign">Simple interface and limits customization</td>
<td class="CDPAlignCenter CDPAlign">Powerful and great for performance</td>
<td class="CDPAlignCenter CDPAlign">Excellent performance and provides the most control and additional interfaces for custom development</td>
</tr>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Popularity</strong></td>
<td class="CDPAlignCenter CDPAlign">Popularity decreasing</td>
<td class="CDPAlignCenter CDPAlign">Consistently popular framework and considered state of the art.</td>
<td>
<p class="mce-root CDPAlignCenter CDPAlign">Popularity increasing especially for DRL applications</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>If you review the table, the obvious choice for our DL framework will be PyTorch. PyTorch based on Torch is a relative newcomer but in just a few short years has gained incredible popularity as both a DL and DRL framework. Therefore, it will be our selected framework for this chapter. In the next section, we look at how to get started with PyTorch for DL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using PyTorch for DL</h1>
                </header>
            
            <article>
                
<p>PyTorch provides both a low- and medium-level interface to building DL networks/computational graphs. As much as we build DL systems as networks with neurons connected in layers, the actual implementation of a neural network is through a computational graph. Computational graphs reside at the heart of all DL frameworks, and TensorFlow is no exception. However, Keras abstracts away any concept of computational graphs from the user, which makes it easier to learn but does not provide flexibility like PyTorch. Before we begin building computational graphs with PyTorch though, let's first install PyTorch in the next exercise:</p>
<ol>
<li> Navigate your browser to <a href="https://pytorch.org">pytorch.org,</a> and scroll down to the <span class="packt_screen"><strong>Run this Command</strong> </span>section, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-590 image-border" src="assets/dd08ecc6-e37c-460a-86f4-6ee4c5242cfe.png" style="width:45.83em;height:18.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Generating a PyTorch installation command</span></div>
<ol start="2">
<li>Select the <strong>Stable</strong> version and then your specific <strong>OS</strong> (<strong>Linux</strong>, <strong>Mac</strong>, or <strong>Windows</strong>). Next select the <strong>package</strong> (<strong>Conda</strong>, <strong>Pip</strong>, <strong>LibTorch</strong>, or <strong>Source</strong>); our preference here is <strong>Conda</strong> for Anaconda, but if you have experience with others, use them.</li>
</ol>
<ol start="3">
<li>Next, choose the <strong>language</strong> (Python 2.7, Python 3.5, Python 3.7, or C++); for our purposes, we will use <strong>Python 3.6</strong>.</li>
<li>The next option <strong>CUDA</strong> (<strong>9.2</strong>, <strong>10.0</strong> or <strong>None</strong>) determines whether you have a <strong>graphics processing unit</strong> (<strong>GPU</strong>) that is capable of running <strong>CUDA</strong>. Currently, the only supported GPUs are built by NVIDIA. That's unlikely to change anytime soon. For our purposes, we will use <strong>None</strong>. <strong>None</strong> or CPU runs strictly on the CPU, which is slower, but will run across most devices.</li>
<li>Open a 64-bit Python console under administrative rights. If you are using <strong>Conda</strong>, launch the window as an admin.</li>
<li>Create a new virtual environment with the following commands:</li>
</ol>
<pre style="padding-left: 60px" class="lang-py prettyprint prettyprinted"><strong><span class="pln">conda create </span><span class="pun">-</span><span class="pln">n gameAI python</span><span class="pun">=</span><span class="lit">3.6</span></strong></pre>
<ol start="7">
<li>This creates the virtual environment using Python 3.6. PyTorch currently runs on Windows 64-bit. This may differ according to the OS. Then activate the environment with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>activate gameAI</strong></pre>
<ol start="8">
<li>Copy and paste the <kbd>install</kbd> command that was generated previously into the window, and execute it. An example of the command for Windows running Anaconda is shown here:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda install pytorch torchvision cpuonly -c pytorch</strong></pre>
<ol start="9">
<li>This command should install PyTorch. If you saw any issues, such as errors claiming the libraries are not available for 32-bit, then make sure you are using a 64-bit version of Python.</li>
</ol>
<p>The preceding process will install PyTorch and all the required dependencies we will need for now. If you have issues installing the framework, check the online documentation or one of the many online help forums. In most cases, installation issues will be resolved by making sure you are using 64-bit and running as an administrator.</p>
<div class="packt_tip">All the code examples for this book have been prepared and tested with Visual Studio Professional or Visual Studio Code, both with the Python tools installed. VS Code is a good solid editor that is free and cross-platform. It is a relative newcomer for Python development but benefits from Microsoft's years of experience building <strong>integrated development environments</strong> (<strong>IDEs</strong>). </div>
<p>With PyTorch installed, we can move onto working with a simple example that creates a computational DL graph in the next section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computational graphs with tensors</h1>
                </header>
            
            <article>
                
<p>At the core of all DL frameworks is the concept of a tensor or what we often think of as a multidimensional array or matrix. The computational graphs we construct will work on tensors using a variety of operations to linearly transform the inputs into final outputs. You can think of this as a kind of flow, and hence the reason TensorFlow has the name it does. In the following exercise, we are going to construct a two-layer DL network using a computation PyTorch graph and then train the network:</p>
<div class="packt_tip">The concepts here assume an understanding of linear algebra and matrix multiplication and systems of linear equations. As such, it is recommended that any readers lacking in these skills or are in need of a quick refresher should do so. Of course, a quick refresher on calculus may also come in useful. </div>
<ol>
<li>Open the <kbd>Chapter_6_1.py</kbd> code example. The example was derived from a PyTorch quickstart manual, with some of the variable names altered to be more contextual:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>import</span><span> torch<br/><br/></span><span>dtype </span><span>=</span><span> torch.float<br/></span><span>device </span><span>=</span><span> torch.device(</span><span>"cpu"</span><span>)<br/></span><span># device = torch.device("cuda:0") # Uncomment this to run on GPU<br/><br/></span><span>batch_size, inputs, hidden, outputs </span><span>=</span><span> </span><span>64</span><span>, </span><span>1000</span><span>, </span><span>100</span><span>, </span><span>10<br/></span><span>x </span><span>=</span><span> torch.randn(batch_size, inputs, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)<br/></span><span>y </span><span>=</span><span> torch.randn(batch_size, outputs, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)<br/><br/></span><span>layer1 </span><span>=</span><span> torch.randn(inputs, hidden, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)<br/></span><span>layer2 </span><span>=</span><span> torch.randn(hidden, outputs, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)<br/></span><span>learning_rate </span><span>=</span><span> </span><span>1e-6<br/><br/></span><span>for</span><span> t </span><span>in</span><span> </span><span>range</span><span>(</span><span>500</span><span>):<br/></span>  h <span>=</span><span> x.mm(layer1)<br/>  h_relu <span>=</span><span> h.clamp(</span><span>min</span><span>=</span><span>0</span><span>)<br/>  y_pred <span>=</span><span> h_relu.mm(layer2)<br/><br/>  loss <span>=</span><span> (y_pred </span><span>-</span><span> y).pow(</span><span>2</span><span>).sum().item()<br/>  <span>if</span><span> t </span><span>%</span><span> </span><span>100</span><span> </span><span>==</span><span> </span><span>99</span><span>:<br/>    <span>print</span><span>(t, loss)<br/><br/>  grad_y_pred <span>=</span><span> </span><span>2.0</span><span> </span><span>*</span><span> (y_pred </span><span>-</span><span> y)<br/>  grad_layer2 <span>=</span><span> h_relu.t().mm(grad_y_pred)<br/>  grad_h_relu <span>=</span><span> grad_y_pred.mm(layer2.t())<br/>  grad_h <span>=</span><span> grad_h_relu.clone()<br/>  grad_h[h <span>&lt;</span><span> </span><span>0</span><span>] </span><span>=</span><span> </span><span>0<br/>  grad_layer1 <span>=</span><span> x.t().mm(grad_h)<br/><br/>  layer1 <span>-=</span><span> learning_rate </span><span>*</span><span> grad_layer1<br/></span></span></span></span></span></span></span></span></span></span></span></span></span>  layer2 <span>-=</span><span> learning_rate </span><span>*</span><span> grad_layer2</span></pre></div>
<ol start="2">
<li>We start by importing the PyTorch library with <kbd>import torch</kbd>. Then we set our preferred data type <kbd>dtype</kbd> variable to <kbd>torch.float</kbd>. Then we initialize the device variable by using <kbd>torch.device</kbd> and passing in <kbd>cpu</kbd> to denote a CPU only. The option to enable the example to run with CUDA on a GPU was left in, but installing CUDA is left up to you:</li>
</ol>
<pre style="padding-left: 60px"><span>batch_size, inputs, hidden, outputs </span><span>=</span><span> </span><span>64</span><span>, </span><span>1000</span><span>, </span><span>100</span><span>, </span><span>10</span></pre>
<ol start="3">
<li>Next, we set up some variables to define how the data is processed and the architecture of the network. The <kbd>batch_size</kbd> parameter denotes how many items to train in an iteration. The <kbd>inputs</kbd> variable denotes the size of the input space into the network, whereas the <kbd>hidden</kbd> variable represents the number of hidden or middle-layer neurons in the network. The last <kbd>outputs</kbd> variable denotes the output space or the number of neurons in an output layer of a network:</li>
</ol>
<pre style="padding-left: 60px"><span>x </span><span>=</span><span> torch.randn(batch_size, inputs, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)<br/></span><span>y </span><span>=</span><span> torch.randn(batch_size, outputs, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)</span></pre>
<ol start="4">
<li>After that, we set the inputs and outputs variables: <kbd>x</kbd> as the inputs, and <kbd>y</kbd> as the outputs, to be learned based on just a random sampling based on <kbd>batch_size</kbd>. The size of the <kbd>inputs</kbd> variable in this example is 1,000, so each element in the batch will have 1000 inputs for <kbd>x</kbd>. The outputs have a value of 10, so each sample of <kbd>y</kbd> will likewise have 10 items:</li>
</ol>
<pre style="padding-left: 60px"><span>layer1 </span><span>=</span><span> torch.randn(inputs, hidden, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)<br/></span><span>layer2 </span><span>=</span><span> torch.randn(hidden, outputs, </span><span>device</span><span>=</span><span>device, </span><span>dtype</span><span>=</span><span>dtype)</span></pre>
<ol start="5">
<li>These two lines create our computational layers of a DL network defined by our previous <kbd>inputs</kbd>, <kbd>hidden</kbd>, and <kbd>outputs</kbd> parameters. The tensor contents of <kbd>layer1</kbd> and <kbd>layer2</kbd> at this point contain an initialized set of random weights the size of which is set by the number of inputs, hidden layers, and outputs. </li>
</ol>
<ol start="6">
<li>You can visualize the size of these tensors by setting a breakpoint on the line after the layer setups and then running the file in debug mode <em>F5</em> on Visual Studio Code or Professional. When the breakpoint is hit, you can then use your mouse to hover over the variables to see information about the tensors as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-591 image-border" src="assets/c5d7d31e-0caf-4e29-81d1-dcb0ea66ad12.png" style="width:54.92em;height:20.00em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign"><span>Inspecting the size of the layer weight tensors</span></div>
<ol start="7">
<li>Notice how the first layer dimensions are 1000 x 100 and the second layer dimensions are 100 x 10. Computationally, we transform the inputs by multiplying the weights of the first layer and then outputting the results to the second layer. Here, the second layer weights are multiplied by the output from the first layer. We will see how this functions shortly.</li>
<li>Next, we define a <kbd>learning_rate</kbd> parameter, or what we will clarify now as a hyperparameter. The learning rate is a multiplier by which we can scale the rate of learning and is not different than the learning rate alpha we previously explored:</li>
</ol>
<pre style="padding-left: 60px"><span>learning_rate </span><span>=</span><span> </span><span>1e-6<br/></span></pre>
<div class="packt_infobox">We will often use the terms <kbd>weight</kbd> and <kbd>parameter</kbd> to mean the same in DL. As such, other parameters such as <kbd>learning_rate</kbd>, epochs, batch size, and so on will be described as hyperparameters. Learning to tune hyperparameters will be an ongoing journey in building DL examples.</div>
<ol start="9">
<li>Before we get into the training loop, let's run the sample and observe the output. Run the sample as you normally would, in debug mode or not. The output of this example is shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-592 image-border" src="assets/98f857ad-a2d4-4d7a-a3eb-a9efbf450b30.png" style="width:14.17em;height:5.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Output of example Chapter_6_1.py</div>
<p>The output basically shows how the error loss decreases over training iterations. At iteration 99, we can see in the preceding example the error is around 635, but decreases down to almost zero by iteration 499. While the inputs and outputs are all random, we can still see the network learns to identify a pattern in the data and thereby reduce errors. In the next section, we take a more detailed look at how this learning works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a neural network – computational graph</h1>
                </header>
            
            <article>
                
<p>In order to train a network or computational graph, we need to first feed it the input data, determine what the graph thinks is the answer, and then correct it iteratively using backpropagation. Let's go back to the <kbd>Chapter_6_1.py</kbd> code example, and follow the next exercise to learn how training works:</p>
<ol>
<li>We will start at the beginning of the training loop that starts with the <kbd>for</kbd> loop, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>for</span><span> t </span><span>in</span><span> </span><span>range</span><span>(</span><span>500</span><span>):<br/></span>  h <span>=</span><span> x.mm(layer1)<br/>  h_relu = h.clamp(min=0)<br/>  y_pred = h_relu.mm(layer2)</span></pre>
<ol start="2">
<li>So, 500 in this example denotes the total number of training iterations or epochs. In each iteration, we calculate the predicted output using the next three lines. This step is called the forward pass through the graph or network. Where the first line does the matrix multiplication of the <kbd>layer1</kbd> weights with the <kbd>x</kbd> inputs using <kbd>x.mm</kbd>. It then passes those output values through an activation function called <strong>clamp</strong>. Clamp sets limits on the output of the network, and in this case we use a clamp on 0. This also happens to correspond with the rectified linear unit or ReLU function.</li>
</ol>
<div class="packt_tip">We use many different forms of activation functions in DL. The ReLU function is currently one of the more popular functions, but we will use others along the way throughout this book. </div>
<p class="mce-root"/>
<ol start="3">
<li>After the output is activated through the ReLU function, it is then matrix multiplied by the second layer weights, <kbd>layer2</kbd>. The output of this result is <kbd>y_pred</kbd>, a tensor containing the output predictions.</li>
<li>From there we predict the loss or amount of error between what we want to actually predict in the <kbd>y</kbd> tensor and what our network just predicted as a tensor <kbd>y_pred</kbd> using the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>loss = (y_pred - y).pow(2).sum().item()<br/>if t % 100 == 99:<br/>  print(t, loss)</span></pre>
<ol start="5">
<li>The <kbd>loss</kbd> value or total error is calculated using a method called <strong>mean squared error</strong> or <strong>MSE</strong>. Keep in mind that since <kbd>y_pred</kbd> and <kbd>y</kbd> are tensors, the subtraction operation is done tensor-wide. That is, all values of the 10 predictions are subtracted from the predicted <kbd>y</kbd> value and then squared and summed. We use the same output technique here to print out the total loss for every 99 iterations.</li>
<li>After computing the loss, we next need to compute the gradient of graph weights in order to determine how we push back and correct the errors in the graph. Calculating this gradient is outside the scope of this book, but the code is shown as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>grad_y_pred = 2.0 * (y_pred - y)<br/>grad_layer2 = h_relu.t().mm(grad_y_pred)<br/>grad_h_relu = grad_y_pred.mm(layer2.t())<br/>grad_h = grad_h_relu.clone()<br/>grad_h[h &lt; 0] = 0<br/>grad_layer1 = x.t().mm(grad_h)</span></pre>
<ol start="7">
<li>We show the low-level code here to do GD against a simple network graph as an example of how the math works. Fortunately, automatic differentiation lets us for the most part ignore those finer, more painful details. The gradients calculated here now need to be applied back to the graph layer weights using the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>layer1 -= learning_rate * grad_layer1<br/></span>layer2 <span>-=</span><span> learning_rate </span><span>*</span><span> grad_layer2</span></pre>
<ol start="8">
<li>Notice how we are again using tensor subtraction to subtract the calculated gradients <kbd>grad_layer1</kbd> and <kbd>grad_layer2</kbd> scaled by the learning rate. </li>
<li>Run the sample again and you should see a similar output. It can be helpful to play with the <kbd>learning_rate</kbd> hyperparameter to see what effect this has on training.</li>
</ol>
<p class="mce-root"/>
<p>This previous example was a low-level look at how we can implement a computational graph that represents a two-layer neural network. While this example was meant to show you the inner details of how things work in practice, we will use the higher-level neural network subset of PyTorch to build graphs. We will see how to construct an example in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building neural networks with Torch</h1>
                </header>
            
            <article>
                
<p>In the last section, we explored building computational graphs that resemble neural networks. This is a fairly common task as you may expect. So much so that PyTorch, as well as most DL frameworks, provides helper methods, classes, and functions to build DL graphs. Keras is essentially a wrapper around TensorFlow that does just that. Therefore, in this section, we are going to recreate the last exercise's example using the neural network helper functions in PyTorch. Open the  <kbd>Chapter_6_2.py</kbd> code example and follow the next exercise:</p>
<ol>
<li>The source code for the entire sample is as follows:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>import</span><span> torch<br/><br/></span><span>batch_size, inputs, hidden, outputs </span><span>=</span><span> </span><span>64</span><span>, </span><span>1000</span><span>, </span><span>100</span><span>, </span><span>10<br/><br/></span><span>x </span><span>=</span><span> torch.randn(batch_size, inputs)<br/></span><span>y </span><span>=</span><span> torch.randn(batch_size, outputs)<br/><br/></span><span>model </span><span>=</span><span> torch.nn.Sequential(<br/></span><span>  torch.nn.Linear(inputs, hidden),<br/></span><span>  torch.nn.ReLU(),<br/></span><span>  torch.nn.Linear(hidden, outputs),<br/></span><span>)<br/><br/></span><span>loss_fn </span><span>=</span><span> torch.nn.MSELoss(</span><span>reduction</span><span>=</span><span>'sum'</span><span>)</span><span><br/></span><span>learning_rate </span><span>=</span><span> </span><span>1e-4<br/><br/></span><span>for</span><span> t </span><span>in</span><span> </span><span>range</span><span>(</span><span>500</span><span>):   <br/></span><span>  y_pred </span><span>=</span><span> model(x)<br/></span><span>  <br/></span><span>  loss </span><span>=</span><span> loss_fn(y_pred, y)<br/></span><span><br/>  </span><span>if</span><span> t </span><span>%</span><span> </span><span>100</span><span> </span><span>==</span><span> </span><span>99</span><span>:<br/></span><span>    </span><span>print</span><span>(t, loss.item())</span><span>  <br/></span><span><br/>  model.zero_grad()</span><span><br/>  loss.backward()</span><span>  <br/></span><span><br/>  </span><span>with</span><span> torch.no_grad():<br/></span><span>    </span><span>for</span><span> param </span><span>in</span><span> model.parameters():<br/></span><span>      param </span><span>-=</span><span> learning_rate </span><span>*</span><span> param.grad</span></pre></div>
<ol start="2">
<li>The code becomes greatly simplified, but not so much that it doesn't allow us to control the internals of the DL graph itself. This is not something you may appreciate entirely until working with other DL frameworks. However, it is not the simplicity but the flexibility that is pushing PyTorch to be the number one framework in DL:</li>
</ol>
<pre style="padding-left: 60px"><span>model </span><span>=</span><span> torch.nn.Sequential(<br/></span><span>  torch.nn.Linear(inputs, hidden),<br/></span><span>  torch.nn.ReLU(),<br/></span><span>  torch.nn.Linear(hidden, outputs),<br/></span><span>)</span></pre>
<ol start="3">
<li>There are a couple of big changes to the top section of the code, most notably with the setup of a model using <kbd>torch.nn.Sequential</kbd>. The setup of this model or graph is exactly the same as we did previously, except it describes each connection point more explicitly. We can see that the first layer is defined with <kbd>torch.nn.Linear</kbd> taking <kbd>inputs</kbd> and <kbd>hidden</kbd> as parameters. This gets connected to the activation function, again ReLU denoted by <kbd>torch.nn.ReLU</kbd>. After that, we create the final layer using <kbd>hidden</kbd> and <kbd>outputs</kbd> as the parameters. The <kbd>Sequential</kbd> term for the model denotes the whole graph is fully connected; the same as we looked at in the last example:</li>
</ol>
<pre style="padding-left: 60px"><span>loss_fn </span><span>=</span><span> torch.nn.MSELoss(</span><span>reduction</span><span>=</span><span>'sum'</span><span>)</span></pre>
<ol start="4">
<li>After the model definition, we can also see our <kbd>loss_fn</kbd> loss function is more descriptive by using <kbd>torch.nn.MSELoss</kbd> as the function. This lets us know explicitly what the <kbd>loss</kbd> function is and how it is going to be reduced,  in this case, reducing the sum, denoted by <kbd>reduction='sum'</kbd>, or the sum of average squared errors:</li>
</ol>
<pre style="padding-left: 60px"><span>for</span><span> t </span><span>in</span><span> </span><span>range</span><span>(</span><span>500</span><span>):   <br/></span><span>  y_pred </span><span>=</span><span> model(x)</span></pre>
<ol start="5">
<li>The start of the training loop remains the same but this time <kbd>y_pred</kbd> is taken from just inputting the entire <kbd>x</kbd> batch into the <kbd>model</kbd>. This operation is the same as the forward pass or where the network outputs the answer:</li>
</ol>
<pre style="padding-left: 60px"><span>loss </span><span>=</span><span> loss_fn(y_pred, y)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>After that, we calculate <kbd>loss</kbd> as a<strong> </strong>Torch tensor, using the <kbd>loss_fn</kbd> function. The next piece of code is the same loss output code as we have seen before:</li>
</ol>
<pre style="padding-left: 60px"><span>model.zero_grad()</span><span><br/>loss.backward()</span></pre>
<ol start="7">
<li>Next, we zero any gradients in the model—this is essentially a reset. Then we calculate the gradients in the loss tensor using the <kbd>backward</kbd> function. This is essentially that nasty bit of code we previously looked at, which has now been simplified to a single line:</li>
</ol>
<pre style="padding-left: 60px"><span>with</span><span> torch.no_grad():<br/></span><span>    </span><span>for</span><span> param </span><span>in</span><span> model.parameters():<br/></span><span>      param </span><span>-=</span><span> learning_rate </span><span>*</span><span> param.grad</span></pre>
<ol start="8">
<li>We finish off training the same way as before by adjusting the weights in the model using the calculated gradients of the <kbd>loss</kbd> tensor. While this section of code is more verbose than our last example, it explains better the actual learning process.</li>
<li>Run the example just like you did previously, and you should see very similar output to what we saw in the <kbd>Chapter_6_1.py</kbd> example. </li>
</ol>
<div class="packt_tip">Did you notice that the <kbd>learning_rate</kbd> variable in the second example was slightly lower?  The reason for this is because the neural network model differs slightly in a few areas, including the use of another weight for each neuron called a <strong>bias</strong>. If you want to learn more about the bias, be sure to pick up a good course on DL. </div>
<p>With a good basic understanding of how we can use PyTorch, we will now look and see how we can apply our knowledge to RL in the next section.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding DQN in PyTorch</h1>
                </header>
            
            <article>
                
<p class="mce-root">Deep reinforcement learning became prominent because of the work of combining Q-learning with DL. The combination is known as deep Q-learning or <strong>DQN</strong> for <strong>Deep Q Network</strong>. This algorithm has powered some of the cutting edge examples of DRL, when Google DeepMind used it to make classic Atari games better than humans in 2012. There are many implementations of this algorithm, and Google has even patented it. The current consensus is that Google patented such a base algorithm in order to thwart patent trolls striking at little guys or developers building commercial applications with DQN. It is unlikely that Google would exercise this legally or that it would have to since this algorithm is no longer considered state of the art.</p>
<div class="mce-root packt_infobox">Patent trolling is a practice whereby an often less-than-ethical company will patent any and all manner of inventions just for the sake of securing patents. In many cases, these inventions don't even originate with the company but their efficient patent process allows them to secure intellectual property cheaply. These trolls often work primarily in software, since software start-ups and other innovators in this area often ignore filing a patent. Google and other big software companies now go out of their way to file these patents, but in essence suggest they would never enforce such a patent. <span>Of course, it could change its mind—just look at Java</span>.</div>
<p>DQN is like the Hello World of DRL, and almost every text or course on this subject will have a version demonstrated. The version we are going to look at here follows the standard pattern but is broken down in a manner that showcases our previous learning on TD and the temporal credit assignment. This will provide a good comparison between using DL and not using DL. In the next sections, we learn how to set up and run a DQN model. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refreshing the environment</h1>
                </header>
            
            <article>
                
<p>A major cause of a new user's frustration is often just setting up the examples. That is why we want to make sure that your environment has the proper components installed. If you recall earlier, you should have created a new virtual environment called <kbd>gameAI</kbd>. We are <span><span>now </span></span>going to install the other required modules we need for the next exercise:</p>
<ol>
<li>You should be working with and in a new virtual environment now. As such, we will need to install the Gym and other required components again.</li>
<li>Make sure that you have a recent version of a <span>Windows C++ compiler installed. For a list of supported compilers, check this site: <a href="https://wiki.python.org/moin/WindowsCompilers">https://wiki.python.org/moin/WindowsCompilers.</a></span></li>
</ol>
<ol start="3">
<li>First let us install the required libraries for Windows, recall these steps are only required for Windows installations with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda install swig</strong><br/><strong><span>pip install box2d-py</span></strong></pre>
<ol start="4">
<li>After you have installed the prerequisites on Windows, you do the remainder of the installation with the command to install Gym. This is the same command you will use on Mac and Linux installations:</li>
</ol>
<pre style="padding-left: 60px"><span><strong>pip install gym[all]</strong><br/></span></pre>
<ol start="5">
<li>  Next install <kbd>matplotlib</kbd> and <kbd>tqdm</kbd> with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install matplotlib</strong><br/><strong>pip install tqdm</strong></pre>
<ol start="6">
<li>Recall that those are the helper libraries we use to monitor training.</li>
</ol>
<p>After installing those packages, make sure your IDE is configured to point to the <kbd>gameAI</kbd> virtual environment. Your particular IDE will have instructions to do this. In the next section, we look to some assumptions that allow us to solve infinite MDPs with DL such as DQN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Partially observable Markov decision process</h1>
                </header>
            
            <article>
                
<p>We have already seen how we can tackle a continuous or infinite observation space by discretizing it into buckets. This works well but as we saw computationally, it does not scale well to massive problems of observation state space. By introducing DL, we can effectively increase our state space inputs, but not nearly in the amount we need. Instead, we need to introduce the concept of a <strong>partially observable Markov decision process</strong> (<strong>POMDP</strong>). That is, we can consider any problem that is an infinite MDP to be partially observable, meaning the agent or algorithm needs only observe the local or observed state in order to make actions. If you think about it, this is exactly the way you interact with your environment. Where you may consider your minute-to-minute activities as partially observable observations of the global infinite MDP or, in other words, the universe. Whereas your day-to-day actions and decisions occur at a higher observable state, you only ever have a partially observable view of the entire globally infinite MDP. </p>
<p class="mce-root"/>
<p>This concept of being able to switch from different partially observable views of the same infinite MDP is a center of much research. Currently, there are two main branches of DRL tackling this problem. They are <strong>hierarchical reinforcement learning</strong> (<strong>HRL</strong>), which attempts to describe a problem as a start to MDP hierarchies. The other branch is called <strong>meta reinforcement learning</strong> (<strong>MRL</strong>), and it takes a broader approach in an attempt to let the partially observable be learned in different time steps. By introducing time sequences here, we can also start to work with other forms of neural networks, called recurrent networks, that can learn time. We will revisit MRL in  <a href="a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml">Chapter 14</a>,<em> From DRL to AGI</em>.</p>
<p>In the next section, we finally look at how to build a DQN with PyTorch.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constructing DQN</h1>
                </header>
            
            <article>
                
<p>You can probably find a version of a DQN developed in every DL framework. The algorithm itself is an incredible achievement in learning, since it allows us now to learn continuous or infinite spaces<span><span>/</span></span>the infinite MDP. Open <kbd>Chapter_6_DQN.py</kbd>, and follow the next exercise to build the DQN sample:</p>
<div class="packt_infobox">The source for this example was originally derived from this GitHub repository: <a href="https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb">https://github.com/higgsfield/RL-Adventure/blob/master/1.dqn.ipynb</a>. It has been modified significantly in order to match the previous samples in this book.</div>
<ol>
<li>At this time, the samples have become too large to list in a single listing. Instead, we will go through section by section as we normally would. As usual, it is helpful if you follow along with the code in an editor:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>import</span><span> math, random<br/></span><span>import</span><span> torch<br/></span><span>import</span><span> torch.nn </span><span>as</span><span> nn<br/></span><span>import</span><span> torch.optim </span><span>as</span><span> optim<br/></span><span>import</span><span> torch.autograd </span><span>as</span><span> autograd <br/></span><span>import</span><span> torch.nn.functional </span><span>as</span><span> F<br/><br/></span><span>import</span><span> matplotlib.pyplot </span><span>as</span><span> plt<br/></span><span>import</span><span> gym<br/></span><span>import</span><span> numpy </span><span>as</span><span> np<br/></span><span>from</span><span> collections </span><span>import</span><span> deque<br/></span><span>from</span><span> tqdm </span><span>import</span><span> trange</span></pre></div>
<ol start="2">
<li>These are are usual imports, but it should be mentioned that <kbd>torch</kbd> needs to load first before the other imports like <kbd>gym</kbd> or <kbd>numpy</kbd>. We are going to jump down past the first <kbd>ReplayBuffer</kbd> function until later:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>env_id </span><span>=</span><span> </span><span>"CartPole-v0"<br/></span><span>env </span><span>=</span><span> gym.make(env_id)<br/></span><span>epsilon_start </span><span>=</span><span> </span><span>1.0<br/><br/></span><span>epsilon_final </span><span>=</span><span> </span><span>0.01<br/></span><span>epsilon_decay </span><span>=</span><span> </span><span>500<br/></span><span>eps_by_episode </span><span>=</span><span> </span><span>lambda</span><span> </span><span>epoch</span><span>: epsilon_final </span><span>+</span><span> (epsilon_start </span><span>-</span><span> epsilon_final) </span><span>*</span><span> math.exp(</span><span>-</span><span>1</span><span>. </span><span>*</span><span> epoch </span><span>/</span><span> epsilon_decay)<br/><br/></span><span>plt.plot([eps_by_episode(i) </span><span>for</span><span> i </span><span>in</span><span> </span><span>range</span><span>(</span><span>10000</span><span>)])<br/></span><span>plt.show()</span></pre></div>
<ol start="3">
<li>The preceding code shows the typical setup for creating the RL environment and setting up our hyperparameters. Notice how we are generating<kbd>eps_by_episode</kbd> for the decaying <kbd>epsilon</kbd> using a lambda expression. This is a very Pythonic way of producing a decaying epsilon. The last couple lines of code plot the decaying epsilon in a chart and outputs something similar to the following graph:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-593 image-border" src="assets/a84f8452-fa58-41d6-82b5-b634e105b45a.png" style="width:31.83em;height:24.50em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"><span>Plot showing decaying epsilon over training epochs</span></div>
<p class="mce-root"/>
<ol start="4">
<li>You can see from the preceding plot that epsilon more or less stabilizes around 2,000 iterations; this seems to suggest the agent should have learned enough by then. We will now scroll down to the next block of code not in a function:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>model </span><span>=</span><span> DQN(env.observation_space.shape[</span><span>0</span><span>], env.action_space.n)<br/>o</span><span>ptimizer </span><span>=</span><span> optim.Adam(model.parameters())<br/></span><span>replay_buffer </span><span>=</span><span> ReplayBuffer(</span><span>1000</span><span>)</span></pre></div>
<ol start="5">
<li>These three lines of code set up the critical components—the model, which is of the <kbd>DQN</kbd> type, and a class we will get to shortly. The <strong>optimizer</strong>, in this case, is of the <strong>Adam</strong> type, defined by <kbd>optim.Adam</kbd>. The last line creates <kbd>ReplayBuffer</kbd>, another class we will get to shortly. We will again scroll down past all the code in functions and review the next section of main code:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>episodes </span><span>=</span><span> </span><span>10000<br/></span><span>batch_size </span><span>=</span><span> </span><span>32<br/></span><span>gamma      </span><span>=</span><span> </span><span>0.99<br/><br/></span><span>losses </span><span>=</span><span> []<br/></span><span>all_rewards </span><span>=</span><span> []<br/></span><span>episode_reward </span><span>=</span><span> </span><span>0<br/><br/></span><span>state </span><span>=</span><span> env.reset()<br/></span><span>tot_reward </span><span>=</span><span> </span><span>0<br/></span><span>tr </span><span>=</span><span> trange(episodes</span><span>+</span><span>1</span><span>, </span><span>desc</span><span>=</span><span>'Agent training'</span><span>, </span><span>leave</span><span>=</span><span>True</span><span>)</span></pre></div>
<ol start="6">
<li>Most of this code should look familiar by now. Notice how we are now setting a new hyperparameter called <kbd>batch_size</kbd>. Basically, <kbd>batch_size</kbd> is the size of the number of items we push through a network at a single time. We prefer to do this in batches since it provides a better averaging mechanism. That means when we train the model, we will do so in batches:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>for</span><span> episode </span><span>in</span><span> tr:<br/></span><span>  tr.set_description(</span><span>"Agent training (episode</span><span>{}</span><span>) Avg Reward </span><span>{}</span><span>"</span><span>.format(episode</span><span>+</span><span>1</span><span>,tot_reward</span><span>/</span><span>(episode</span><span>+</span><span>1</span><span>)))<br/></span><span>  tr.refresh() <br/></span><span>  epsilon </span><span>=</span><span> eps_by_episode(episode)<br/><br/></span><span>  action </span><span>=</span><span> model.act(state, epsilon)<br/></span><span>  next_state, reward, done, _ </span><span>=</span><span> env.step(action)<br/><br/></span><span>  <strong>replay_buffer.push(state, action, reward, next_state, done)</strong><br/></span><span>  tot_reward </span><span>+=</span><span> reward<br/></span><span><br/>  state </span><span>=</span><span> next_state<br/></span><span>  episode_reward </span><span>+=</span><span> reward<br/></span><span><br/>  </span><span>if</span><span> done:<br/></span><span>    state </span><span>=</span><span> env.reset()<br/></span><span>    all_rewards.append(episode_reward)<br/></span><span>    episode_reward </span><span>=</span><span> </span><span>0<br/><br/></span><span>  if</span><span> </span><span>len</span><span>(replay_buffer) </span><span>&gt;</span><span> batch_size:<br/></span><span>    <strong>loss </strong></span><strong><span>=</span><span> compute_td_loss(batch_size)<br/></span></strong><span><strong>    losses.append(loss.item()</strong>)<br/><br/></span><span>  </span><span>if</span><span> epoch </span><span>%</span><span> </span><span>2000</span><span> </span><span>==</span><span> </span><span>0</span><span>:<br/></span><span>    plot(epoch, all_rewards, losses) </span></pre></div>
<ol start="7">
<li>Once again, most of this code should feel quite familiar by now since it mirrors many of our previous examples. There are two highlighted sections of code that we will focus on. The first is the line that pushes the <kbd>state</kbd>, <kbd>action</kbd>, <kbd>reward</kbd>, <kbd>next_state</kbd>, and <kbd>done</kbd> functions onto<kbd>replay_buffer</kbd>. We have yet to look at the <kbd>replay</kbd> buffer, but just realize at this point all this information is getting stored for later. The other highlighted section has to do with the computation of loss using the <kbd>compute_td_loss</kbd> function. That function computes the loss using the TD error as we saw when covering TD and SARSA.</li>
<li>Before we explore the additional functions and classes, run the sample so that you can see the output. As the sample runs, you will need to repeatedly close the plotting window after every 2,000 iterations. The following graphs show the output from several thousand training iterations:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-594 image-border" src="assets/a12eb970-633b-40ea-b0ab-aaf5f9cadc73.png" style="width:44.75em;height:19.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output after several thousand training iterations</div>
<p>The output in the <span><span>graphs </span></span>shows how the agent learns across episodes and is able to quickly maximize the reward as shown in the left plot. In comparison, the right plot shows a decreasing loss or total error in our agent's predictions. In fact, we can see around episode 8000 that the agent has indeed learned the problem and is able to consistently achieve the maximal reward. If you recall in <a href="3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml">Chapter 5</a>, <em>Exploring SARSA</em>, we solved the CartPole environment, but just barely with discretized SARSA. Although, that still required almost 50,000 episodes of training. Now that we have observed this method is several times better than our previous attempts, in the next section we need to explore the details of how this works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The replay buffer</h1>
                </header>
            
            <article>
                
<p>Fundamental to the DL methods is the need for us to feed batches of observed agent events into the neural network. Remember, we do this in batches so the algorithm is able to average across errors or loss better. This requirement is more a function of DL than anything to do with RL. As such, we want to store a previous number of the observed state, action, next state, reward, and returns from our agent, taking an action into a container called <kbd>ReplayBuffer</kbd>. We then randomly sample those events from the replay buffer and inject them into the neural network for training. Let's see how the buffer is constructed again by reopening sample <kbd>Chapter_6_DQN.py</kbd> and following this exercise:</p>
<ol>
<li>The entire code for the <kbd>ReplayBuffer</kbd> class is shown as follows:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>class</span><span> </span><span>ReplayBuffer</span><span>(</span><span>object</span><span>):<br/></span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>, </span><span>capacity</span><span>):<br/></span><span>    </span><span>self</span><span>.buffer </span><span>=</span><span> deque(</span><span>maxlen</span><span>=</span><span>capacity)<br/><br/></span><span>  </span><span>def</span><span> </span><span>push</span><span>(</span><span>self</span><span>, </span><span>state</span><span>, </span><span>action</span><span>, </span><span>reward</span><span>, </span><span>next_state</span><span>, </span><span>done</span><span>):<br/></span><span>    state      </span><span>=</span><span> np.expand_dims(state, </span><span>0</span><span>)<br/></span><span>    next_state </span><span>=</span><span> np.expand_dims(next_state, </span><span>0</span><span>)<br/></span><span>    </span><span>self</span><span>.buffer.append((state, action, reward, next_state, done))<br/><br/></span><span>  </span><span>def</span><span> </span><span>sample</span><span>(</span><span>self</span><span>, </span><span>batch_size</span><span>):</span><span> <br/> st</span><span>ate, action, reward, next_state, done <br/></span><span> =</span><span> </span><span>zip</span><span>(</span><span>*</span><span>random.sample(</span><span>self</span><span>.buffer, batch_size))</span><span><br/>    </span><span>return</span><span> np.concatenate(state), action,<br/>  reward, np.concatenate(next_state), done</span><span><br/></span><span><br/>  </span><span>def</span><span> </span><span>__len__</span><span>(</span><span>self</span><span>):<br/></span><span>    return</span><span> </span><span>len</span><span>(</span><span>self</span><span>.buffer)</span></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Internally the <kbd>ReplayBuffer</kbd> uses a class called <kbd>deque</kbd>, which is a class that can store any manner of objects. In the <kbd>init</kbd> function, we create the queue of the required specified size. The class has three functions <kbd>push</kbd>, <kbd>sample</kbd> and <kbd>len</kbd>. The <kbd>len</kbd> function is fairly self-explanatory, but the other functions we should look at:</li>
</ol>
<pre style="padding-left: 60px"><span>def</span><span> </span><span>push</span><span>(</span><span>self</span><span>, </span><span>state</span><span>, </span><span>action</span><span>, </span><span>reward</span><span>, </span><span>next_state</span><span>, </span><span>done</span><span>):<br/></span><span>  state      </span><span>=</span><span> np.expand_dims(state, </span><span>0</span><span>)<br/></span><span>  next_state </span><span>=</span><span> np.expand_dims(next_state, </span><span>0</span><span>)<br/></span><span>  </span><span>self</span><span>.buffer.append((state, action, reward, next_state, done))</span></pre>
<ol start="3">
<li>The <kbd>push</kbd> function pushes the <kbd>state</kbd>, <kbd>action</kbd>, <kbd>reward</kbd>, <kbd>next_state</kbd>, and <kbd>done</kbd> observations on to the queue for later processing:</li>
</ol>
<pre style="padding-left: 60px"><span>def</span><span> </span><span>sample</span><span>(</span><span>self</span><span>, </span><span>batch_size</span><span>)</span><span>:</span><span> </span><span><br/>    st</span><span>ate, action, reward, next_state, done <br/></span><span>      =</span><span> </span><span>zip</span><span>(</span><span>*</span><span>random.sample(</span><span>self</span><span>.buffer, batch_size))</span><span><br/>    </span><span>return</span><span> np.concatenate(state), action,<br/>      reward, np.concatenate(next_state), done</span></pre>
<ol start="4">
<li>The other function, <kbd>sample</kbd>, is where the buffer randomly samples events from the queue and zips them up using <kbd>zip</kbd>. It will then return this batch of random events to be fed into the network for learning.</li>
<li>Find the line of code that sets the size of the replay buffer and change it to the following:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>replay_buffer </span><span>=</span><span> ReplayBuffer(</span><span>3000</span><span>)</span></pre></div>
<ol start="6">
<li>Run the example again with the new buffer size, and observe the effect this has on training.</li>
<li>Now change the buffer size again with the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>replay_buffer </span><span>=</span><span> ReplayBuffer(</span><span>333</span><span>)</span></pre>
<ol start="8">
<li>Run the example again, and observe the output closely. Notice the changes in the training performance.</li>
</ol>
<p>We have effectively tried running the agent with 3 times, as well as 1/3 the buffer size. What you will find in this problem is that the smaller buffer size is more effective but perhaps not optimal. You can consider the buffer size as another one of those essential hyperparameters you will need to learn to set. </p>
<p class="mce-root">Replay buffers are a required component of our DL models, and we will see other similar classes in the future. In the next section, we will move on to building the DQN class.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The DQN class</h1>
                </header>
            
            <article>
                
<p class="mce-root">Previously, we saw how the DQN class was used to construct the neural network model that we will use to learn the TD loss function. Reopen exercise <kbd>Chapter_6_DQN.py</kbd> again to review the construction of the DQN class:</p>
<ol>
<li> The entire code for the DQN class is as follows:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>class</span><span> </span><span>DQN</span><span>(</span><span>nn</span><span>.</span><span>Module</span><span>):<br/></span><span>  </span><span>def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>, </span><span>num_inputs</span><span>, </span><span>num_actions</span><span>):<br/></span><span>    </span><span>super</span><span>(DQN, </span><span>self</span><span>).</span><span>__init__</span><span>()<br/><br/></span><span>    </span><span>self</span><span>.layers </span><span>=</span><span> nn.Sequential(<br/></span><span>      nn.Linear(env.observation_space.shape[</span><span>0</span><span>], </span><span>128</span><span>),<br/>      nn.ReLU(),<br/></span><span>      nn.Linear(</span><span>128</span><span>, </span><span>128</span><span>),<br/></span><span>      nn.ReLU(),<br/></span><span>      nn.Linear(</span><span>128</span><span>, env.action_space.n)</span><span>)<br/><br/></span><span>  </span><span>def</span><span> </span><span>forward</span><span>(</span><span>self</span><span>, </span><span>x</span><span>):<br/></span><span>    </span><span>return</span><span> </span><span>self</span><span>.layers(x)<br/><br/></span><span>  </span><span>def</span><span> </span><span>act</span><span>(</span><span>self</span><span>, </span><span>state</span><span>, </span><span>epsilon</span><span>):<br/></span><span>    </span><span>if</span><span> random.random() </span><span>&gt;</span><span> epsilon:<br/></span><span>      state   </span><span>=</span><span> autograd.Variable(torch.FloatTensor(state).unsqueeze(</span><span>0</span><span>),<br/>        </span><span>volatile</span><span>=</span><span>True</span><span>)<br/></span><span>      q_value </span><span>=</span><span> </span><span>self</span><span>.forward(state)<br/></span><span>      action  </span><span>=</span><span> q_value.max(</span><span>1</span><span>)[</span><span>1</span><span>].item()<br/></span><span>    </span><span>else</span><span>:<br/></span><span>      action </span><span>=</span><span> random.randrange(env.action_space.n)<br/></span><span>    </span><span>return</span><span> action</span></pre></div>
<ol start="2">
<li>The <kbd>init</kbd> function initializes the network using the PyTorch <kbd>nn.Sequential</kbd> class to generate a fully connected network. We can see that the inputs into the first layer are set by <kbd>env.observation_space.shape[<span>0</span></kbd><span><kbd>],</kbd> and the number of neurons is 128.</span></li>
<li>We can see there are three layers in this network, with the first layer consisting of 128 neurons connected by ReLU to a middle layer with 128 neurons. This layer is connected to the output layer, with the number of outputs defined by <kbd>env.action_space.n</kbd>. What we can see from this is that the network will be learning which action to select.</li>
</ol>
<ol start="4">
<li>The <kbd>forward</kbd> function is just the forward pass or prediction by the network model.</li>
<li>Finally, the <kbd>act</kbd> function is quite similar to the other Q-learning samples we have built before. One thing we want to focus on is how the actual action is selected during non-exploration as the following code excerpt shows:</li>
</ol>
<pre style="padding-left: 60px"><span>state   </span><span>=</span><span> autograd.Variable(torch.FloatTensor(state).unsqueeze(</span><span>0</span><span>),<br/>        </span><span>volatile</span><span>=</span><span>True</span><span>)<br/></span><span>q_value </span><span>=</span><span> </span><span>self</span><span>.forward(state)<br/></span><span>action  </span><span>=</span><span> q_value.max(</span><span>1</span><span>)[</span><span>1</span><span>].item()</span></pre>
<ol start="6">
<li>Calculating the <kbd>state</kbd> tensor in the first line with <kbd>autograd.Variable</kbd> is where the state is converted into a tensor so that it may be fed into the forward pass. It is the call to <kbd>self.forward</kbd> in the next line that calculates all the Q values, <kbd>q_value</kbd>, for that <kbd>state</kbd> tensor. We then use a greedy (max) selection strategy in the last line to choose the action.</li>
<li>Change the network size from 128 neurons to 32, 64, or 256 to see the effect this has on training. The following code shows the proper way to configure the example to use 64 neurons:</li>
</ol>
<pre style="padding-left: 60px"><span>self</span><span>.layers </span><span>=</span><span> nn.Sequential(<br/></span><span>      nn.Linear(env.observation_space.shape[</span><span>0</span><span>], </span><span>64</span><span>),<br/>      nn.ReLU(),<br/></span><span>      nn.Linear(</span><span>64</span><span>, </span><span>64</span><span>),<br/></span><span>      nn.ReLU(),<br/></span><span>      nn.Linear(</span><span>64</span><span>, env.action_space.n)</span><span>)</span></pre>
<ol start="8">
<li>Run the example again with various size changes, and see the effect this has on training performance.</li>
</ol>
<p>Good news, we consider the number of neurons and number of layers in a network to be the additional training hyperparameters we need to observe while tackling problems. As you may have already noticed, these new inputs can have a dramatic effect on the training performance and need to be selected carefully.</p>
<p>We almost have all the pieces together to understand the entire algorithm. In the next section, we will cover the last piece, determining loss and training the network.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating loss and training</h1>
                </header>
            
            <article>
                
<p>Finally, we can see how all this comes together to train the agent to learn a policy. Open up <kbd>Chapter_6_DQN.py</kbd> again, and follow the next exercise to see how loss is calculated:</p>
<ol>
<li> The function that calculates the loss in terms of TD errors is shown as follows:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>def</span><span> </span><span>compute_td_loss</span><span>(</span><span>batch_size</span><span>):<br/></span><span>  state, action, reward, next_state, done </span><span>=</span><span> replay_buffer.sample(batch_size)<br/><br/></span><span>  state      </span><span>=</span><span> autograd.Variable(torch.FloatTensor(np.float32(state)))</span><span><br/>  next_state </span><span>=</span><span> autograd.Variable(torch.FloatTensor(np.float32(next_state)),<br/>    </span><span>volatile</span><span>=</span><span>True</span><span>)<br/>  action     <span>=</span><span> autograd.Variable(torch.LongTensor(action))<br/></span></span><span>  reward     </span><span>=</span><span> autograd.Variable(torch.FloatTensor(reward))<br/></span><span>  done       </span><span>=</span><span> autograd.Variable(torch.FloatTensor(done))<br/><br/></span><span>  q_values      </span><span>=</span><span> model(state)<br/></span><span>  next_q_values </span><span>=</span><span> model(next_state)<br/></span><span>  q_value       </span><span>=</span><span> q_values.gather(</span><span>1</span><span>, action.unsqueeze(</span><span>1</span><span>)).squeeze(</span><span>1</span><span>)<br/><br/></span><span>  next_q_value  </span><span>=</span><span> next_q_values.max(</span><span>1</span><span>)[</span><span>0</span><span>]<br/></span><span>  expected_q_value </span><span>=</span><span> reward </span><span>+</span><span> gamma </span><span>*</span><span> next_q_value </span><span>*</span><span> (</span><span>1</span><span> </span><span>-</span><span> done)<br/><br/></span><span>  loss </span><span>=</span><span> (q_value </span><span>-</span><span> autograd.Variable(expected_q_value.data)).pow(</span><span>2</span><span>).mean()<br/></span><span>  optimizer.zero_grad()<br/></span><span>  loss.backward()<br/></span><span>  optimizer.step()<br/><br/></span><span>  return</span><span> loss</span></pre></div>
<ol start="2">
<li>In the first line, we call <kbd>sample</kbd> from <kbd>replay_buffer</kbd> using <kbd>batch_size</kbd> as the input. This returns a randomly sampled set of events from a previous run. This returns <kbd>state</kbd>, <kbd>next_state</kbd>, <kbd>action</kbd>, <kbd>reward</kbd>, and <kbd>done</kbd>. These are then turned into tensors in the next five lines using the <kbd>autograd.Variable</kbd> function. This function is a helper for converting types into tensors of the appropriate type. Notice how the action is of the <kbd>long</kbd> type using <kbd>torch.LongTensor</kbd>, and the other variables are just floats.</li>
</ol>
<ol start="3">
<li>The next section of code calculates the Q values:</li>
</ol>
<pre style="padding-left: 60px"><span>q_values      </span><span>=</span><span> model(state)<br/></span><span>next_q_values </span><span>=</span><span> model(next_state)<br/></span><span>q_value       </span><span>=</span><span> q_values.gather(</span><span>1</span><span>, action.unsqueeze(</span><span>1</span><span>)).squeeze(</span><span>1</span><span>)</span></pre>
<ol start="4">
<li>Remember that when we call <kbd>model(state)</kbd> that is the equivalent of doing a forward pass or prediction on the network. This now becomes the same as sampling from the policy in our previous examples.</li>
<li>We then go back to our previous defined Q Learning equation, and use that to determine what our best expected Q value should be, with the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>next_q_value  </span><span>=</span><span> next_q_values.max(</span><span>1</span><span>)[</span><span>0</span><span>]<br/></span><span>expected_q_value </span><span>=</span><span> reward </span><span>+</span><span> gamma </span><span>*</span><span> next_q_value </span><span>*</span><span> (</span><span>1</span><span> </span><span>-</span><span> done)</span></pre>
<ol start="6">
<li>Calculating the <kbd>expected_q_value</kbd> value from earlier uses the Q Learning equation to determine what an expected value should be. Based on the expected value, we can determine the how much the network is in error and how much loss it needs to correct with the following line:</li>
</ol>
<pre style="padding-left: 60px"><span>loss </span><span>=</span><span> (q_value </span><span>-</span><span> autograd.Variable(expected_q_value.data)).pow(</span><span>2</span><span>).mean()</span><span>  </span></pre>
<ol start="7">
<li>This line converts the value to a tensor and then determines the loss using our old friend MSE. Our final step is to optimize or reduce the loss of the network, using the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>optimizer.zero_grad()<br/></span><span>loss.backward()<br/></span><span>optimizer.step()</span></pre>
<ol start="8">
<li>The code is quite similar to what we used before to optimize our neural network and computational graph examples. We first apply <kbd>zero_grad</kbd> to the optimizer in order to zero out any gradients as a reset. We then push the loss backward, and finally perform one step on the optimizer. That last part is new and has to do with the type of optimizer we are using.</li>
</ol>
<div class="packt_infobox">We won't go heavily into the various optimizers you can use for DL until <a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 6</a>, <em>Going Deeper with DDQN</em>. In most cases, we will use the Adam optimizer or some derivation of it, depending on the environment. </div>
<ol start="9">
<li>Feel free to run the code sample yet again in order to better observe all the details in training.</li>
</ol>
<p class="mce-root"/>
<p>Hopefully by now, even with the inclusion of DL, these samples are starting to feel consistently familiar. In some ways, DL makes these algorithms much simpler than our previous examples. Fortunately, that is a good thing because our agents will need to get more complicated and robust as we <span><span>evolve to harder</span></span> and harder environments.</p>
<p>Up until now, we have only been able to view our agent training and see no actual update in performance. Since that is essential to our understanding of how an agent trains, in the next section we are going to add that to the last example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercising DQN</h1>
                </header>
            
            <article>
                
<p>As we have progressed through this book, we have spent time making sure we can see how our agents our progressing in their respective environments. In this section, we are aiming to add rendering to the agent environment during training using our last DQN example. Then we can see how the agent is actually performing and perhaps try out another couple of new environments along the way.</p>
<p>Adding the ability to watch the agent play in the environment is not that difficult, and we can implement this as we have done with other examples. Open the <kbd>Chapter_6_DQN_wplay.py</kbd> code example, and follow the next exercise:</p>
<ol>
<li>The code is almost identical to the DQN sample earlier, so we won't need to review the whole code. However, we do want to introduce two new variables as hyperparameters; this will allow us to better control the network training and observer performance: </li>
</ol>
<div>
<pre style="padding-left: 60px"><span>buffer_size </span><span>=</span><span> </span><span>1000<br/></span><span>neurons </span><span>=</span><span> </span><span>128</span></pre></div>
<ol start="2">
<li>We will use <kbd>buffer_size</kbd> to denote the size of the buffer. This value will also come in handy when we determine whether our model has some amount of training. DQN will not start training the model until the replay buffer or what we often refer to as the experience buffer is full. Notice that we also added a new hyperparameter for neurons; this will allow us to quickly tweak the network as we need.</li>
<li>Next we will look at how the code to render the agent playing the game is injected into the training loop:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span> </span><span>if</span><span> done:<br/></span><span>   </span><strong><span>if</span><span> episode </span><span>&gt;</span><span> buffer_size:<br/></span></strong><span><strong>     play_game()</strong><br/></span><span>   state </span><span>=</span><span> env.reset()<br/></span><span>   all_rewards.append(episode_reward)<br/></span><span>   episode_reward </span><span>=</span><span> </span><span>0</span><span>  </span></pre></div>
<ol start="4">
<li>The highlighted lines represent the new code that will check whether the current episode is larger than <kbd>buffer_size.</kbd> If it is then we render the agent playing the game using the model/policy.</li>
<li>Next we will look at the new <kbd>play_game</kbd> function, as follows:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>def</span><span> </span><span>play_game</span><span>():<br/></span><span>  done </span><span>=</span><span> </span><span>False<br/></span><span>  state </span><span>=</span><span> env.reset()<br/></span><span>  </span><span>while</span><span>(</span><span>not</span><span> done):<br/></span><span>    <strong>action </strong></span><strong><span>=</span><span> model.act(state, </span><span>epsilon_final</span></strong><span><strong>)</strong><br/></span><span>    next_state, reward, done, _ </span><span>=</span><span> env.step(action)<br/></span><span>    env.render()<br/></span><span>    state </span><span>=</span><span> next_state</span></pre></div>
<ol start="6">
<li>This code is quite similar to other <kbd>play_game</kbd> functions we crafted previously. Notice the highlighted line showing where we predict the next action using the <kbd>model.act</kbd> function. Passed into this function is the state and our minimum value for epsilon, called <kbd>epsilon_final</kbd>. We set the minimum value here since we choose the agent performing minimal exploration and the actions are selected entirely from the policy/model.</li>
<li>Run this example, and you can watch the agent play the CartPole environment successfully as shown in the following diagram:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-595 image-border" src="assets/b159086f-f2cb-47c7-9212-348a85befab7.png" style="width:34.00em;height:18.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example rendering of an agent playing CartPole successfully</div>
<p>With a new agent able to tackle the CartPole environment far easier, in the next section we will now look to throw our failed example from the last chapter, the LunarLander environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Revisiting the LunarLander and beyond</h1>
                </header>
            
            <article>
                
<p>Now with our solid example of DQN, we can move on to solve more difficult environments, like LunarLander. In this exercise, we set up the DQN agent to solve the LunarLander environment in order to compare our previous attempts with discretized SARSA:</p>
<ol>
<li>Open the <kbd>Chapter_6_DQN_lunar.py</kbd> example, and note the change in the <kbd>env_id</kbd> environment ID and creation of the environment shown as follows:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>env_id </span><span>=</span><span> </span><span>'LunarLander-v2'<br/></span><span>env </span><span>=</span><span> gym.make(env_id)</span></pre></div>
<ol start="2">
<li>We also adjust a couple of the hyperparameters to account for the increased complexity of the environment:</li>
</ol>
<pre style="padding-left: 60px"><span>epsilon_decay </span><span>=</span><span> </span><span>1000<br/></span><span>buffer_size </span><span>=</span><span> </span><span>3000<br/></span><span>neurons </span><span>=</span><span> </span><span>192<br/></span></pre>
<ol start="3">
<li>We increase <kbd>epsilon_decay</kbd> in order to encourage the agent to explore longer. Exploration is a trade-off we always need to balance with the environment. Note <kbd>buffer_size</kbd> is also increased to a value of 3,000 to account for the increase in environment complexity, again. As well, we also increase the size of the network to 192 neurons across the board.</li>
<li>You may also need to increase the number of total training episodes from 10,000 to a higher value. We will leave that decision up to you.</li>
<li>From here, you can run the example and visualize the agent landing the lander as shown in the following screen image:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-596 image-border" src="assets/4016a02c-fcf4-46ff-9226-eef8d82e3ceb.png" style="width:47.17em;height:24.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Agent playing the LunarLander environment</div>
<p>That completes our exploration of DQN, and you are encouraged to follow the exercises in the next section to practice those new skills.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>As we progress through the book, I hope you can see the value of performing these additional hands-on exercises. Learning how to tune hyperparameters will be essential in building DRL models that can tackle difficult environments. Use the following exercises to reinforce your learning of the material:</p>
<ol>
<li>Modify the <kbd>batch_size</kbd>, <kbd>inputs</kbd>, <kbd>hidden</kbd>, and <kbd>outputs</kbd> hyperparameters from <kbd>Chapter_6_1.py</kbd> and see what effect these have on the output loss.</li>
<li>Alter the number of training iterations in the <kbd>Chapter_6_1.py</kbd> example in conjunction with other hyperparameters in order to evaluate the impact this has on training.</li>
<li>Modify the<span> </span><kbd>batch_size</kbd>,<span> </span><kbd>inputs</kbd>,<span> </span><kbd>hidden<span> </span></kbd>, and<span> </span><kbd>outputs</kbd><span>  hyperparameters </span>from <kbd>Chapter_6_2.py</kbd>, and see what effect these have on the output loss.</li>
</ol>
<ol start="4">
<li>Alter the number of training iterations in the<span> </span><kbd>Chapter_6_2.py</kbd><span> example </span>in conjunction with other hyperparameters in order to evaluate the impact this has on training.</li>
<li>Tune the hyperparameters in the <kbd>Chapter_6_DQN.py</kbd> example to improve training performance on the CartPole environment. Create any additional hyperparameters you may need.</li>
<li>Tune the hyperparameters in the<span> </span><kbd>Chapter_6_DQN_wplay.py</kbd><span> example </span>to improve training performance on the CartPole environment. Create any additional hyperparameters you may need.</li>
<li>Tune the hyperparameters in the<span> </span><kbd>Chapter_6_DQN_lunar.py</kbd><span> example </span>to improve training performance on the LunarLander environment. Create any additional hyperparameters you may need.</li>
<li>Tune the <kbd>batch_size</kbd> hyperparameter to low values of 8 or 16 all the way up to 256, 512, and 1,024 to see what effect this has on any and all the DQN examples. </li>
<li>Introduce a main function that will take command-line arguments that will allow you to configure the various hyperparameters at runtime. You will likely need to use a helper library (<kbd>argparse</kbd>) to do this.</li>
<li>Add the ability to render the training performance without blocking the training execution. </li>
</ol>
<p>Doing two or three of these exercises can greatly improve your grasp of this knowledge, and there really is no better way to learn than doing, just ask one of your agents. Alas, we have come to the end of this chapter, and in the next section we have the summary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at the Hello World of DRL, the DQN algorithm, and applying DL to RL. We first looked at why we need DL in order to tackle more complex continuous observation state environments like CartPole and LunarLander. Then we looked at the more common DL environments you may use for DL and the one we use, PyTorch. From there, we installed PyTorch and set up an example using computational graphs as a low-level neural network. Following that, we built a second example with the PyTorch neural network interface in order to see the difference between a raw computational graph and neural network.</p>
<p class="mce-root"/>
<p>With that knowledge, we then jumped in and explored DQN in detail. We looked at how DQN uses experience replay or a replay buffer to replay events when training the network/policy in DQN. As well, we looked at how the TD loss was calculated based on the difference between the predicted and expected value. We used our old friend the Q Learning equation in order to calculate the expected value and feed back the difference as a loss to the model. By doing so, we were able to train the model/policy so the agent could solve CartPole and the LunarLander environments, given sufficient iterations.</p>
<p>In the next chapter, we again extend our knowledge from this chapter and explore the next level of DQN, the Double DQN or DDQN. Along with this, we will explore advances in network image processing with CNN so that we can tackle even more complex environments, such as the classic Atari.</p>


            </article>

            
        </section>
    </body></html>