<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-53"><a id="_idTextAnchor053"/>3</h1>
<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Engineering Prompts for Effective Model Usage</h1>
<p>This chapter begins with an overview of prompt engineering and its importance. We will walk through various prompt engineering techniques and the ability to incorporate them while prompting any model on Amazon Bedrock, primarily focusing on designing and analyzing effective prompt techniques to get the desired outcome from the Bedrock models. This chapter also entails some of the best practices associated with prompt engineering.</p>
<p>By the end of this chapter, you will have developed a clear understanding of the practical aspects of prompt engineering and be able to craft effective prompts while following the best practices to get the desired outcome from the models available on Amazon Bedrock.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>What is prompt engineering?</li>
<li>Unlocking prompt engineering techniques</li>
<li>Designing prompts for Amazon Bedrock models</li>
<li>Understanding best practices in prompt engineering</li>
</ul>
<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Technical requirements</h1>
<p>To complete this chapter, you need to have access to the <em class="italic">AWS</em> console so that you can navigate to Amazon Bedrock Playground to execute prompt engineering techniques. Here’s the web page to access the console: <a href="https://console.aws.amazon.com/">https://console.aws.amazon.com/</a>.</p>
<p>Secondly, you need to have the right permissions to invoke Amazon Bedrock models from your local machine using <em class="italic">Amazon Bedrock APIs</em> or <em class="italic">Bedrock Python SDK</em> so that you can execute the prompts. To learn more, go to <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html">https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html</a>.</p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>What is prompt engineering?</h1>
<p>Since we have <a id="_idIndexMarker210"/>been discussing Amazon Bedrock models and how to invoke them, we need to dive into prompt engineering. Essentially, in a way that a particular child asks their parents questions about anything and everything, we can also ask an LLM anything under the Sun! However, to get the best and most precise outputs possible, we must train ourselves to ask the model the right questions in the right manner.</p>
<p>With the increasing popularity of LLMs, users are actively striving to refine their way of asking the model different kinds of questions to attain a desired response. For instance, we can simply ask an LLM questions such as <code>Who was the first person to land on the Moon?</code> or <code>How many moons does Jupiter have?</code>. Based on these questions, the language model can respond to the user’s queries either factually or provide an inadequate/incorrect response based on the LLM’s knowledge, which is the data it has been trained on.</p>
<p>Incorrect responses that the users get without fact-checking are what we refer to as <strong class="bold">hallucinations</strong>. It is often seen that if the user asks an ambiguous question or a particularly complex math problem that the model hasn’t been trained to answer, it will determine a probabilistic answer that may or may not be factually accurate. This can also occur with large vision models such as text-to-image models, where the model ends up providing an undesirable image as the prompted response.</p>
<p>Hence, how we ask questions to the model and how effectively we can provide a description regarding our question becomes a crucial factor for the model to generate a desirable output.</p>
<p>The method of prompting the model in the right manner while avoiding any ambiguity in your prompts becomes the essence of effective <strong class="bold">prompt engineering</strong>. This is not just applicable to the technical community anymore!</p>
<p>Even people with varying technical backgrounds can use LLMs for a range of tasks. Based on the user prompts, the models can offer basic tips on entrepreneurship or provide fundamental insights into website creation through detailed, informative conversations.</p>
<p>Effective prompt engineering techniques pave the way for users to get the desired responses. Furthermore, some companies have been offering high-paying jobs for researchers and personas who can write or adopt effective prompt engineering to get their model to perform responsible actions, thereby enhancing the company’s productivity to execute their functions/tasks at an accelerated pace.</p>
<p>This chapter will explain how effective prompt engineering techniques can be applied to LLMs. But first, let’s <a id="_idIndexMarker211"/>dive into the structure of a prompt and some key ideas that focus on effective prompt techniques.</p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>Components of prompts</h2>
<p>How you write <a id="_idIndexMarker212"/>a prompt plays a crucial role in guiding the behavior of the model. Prompts contain a few key elements. Let’s understand those elements through an example (<em class="italic">Figure 3</em><em class="italic">.1</em>):</p>
<div><div><img alt="Figure 3.1 – Components of a prompt" src="img/B22045_03_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Components of a prompt</p>
<p>Let’s take a closer look at the terms highlighted in the preceding figure:</p>
<ul>
<li><strong class="bold">Instruction</strong>: With an instruction, you provide a clear and concise description or instruction to the model on how it should perform the task, whether it be summarizing text, translating languages, composing music, or any number of other things. In the preceding figure, you can see that we have asked the model to act as a specialist in quantum computing and answer the user’s question in detail and in layman’s terms, along with examples.</li>
<li><strong class="bold">Context</strong>: Context refers to the relevant background information that you provide to the model to enhance its performance. This can include any relevant data, past experiences, or domain-specific knowledge. In the preceding figure, in terms of context, we stated that the model has recently completed a PhD and has been asked to be part of an interview on a talk show that explains complex topics in layman’s terms. This primes the model with pertinent knowledge.</li>
<li><code>What is Quantum Computing?</code>. As depicted in the preceding figure, the input question that goes to the model is <code>Can you provide me your thoughts on Quantum </code><code>Machine Learning?</code>.</li>
<li><code>Quantum Ninja</code>, as depicted in the preceding figure, so that the model understands that its output should be in this layout, or it could be in a specific format, such as text, JSON, an audio clip, and so on. Special <a id="_idIndexMarker213"/>syntax such as <em class="italic">&lt;|endoftext|&gt;</em> signals the end of the input and the beginning of the model’s output. This special syntax can vary on a per-model basis.</li>
</ul>
<p>Although prompts need not have all four elements, their form depends on the task. Let’s examine a few sample prompts:</p>
<p><strong class="bold">Example 1: </strong><strong class="bold">SQL query</strong></p>
<div><div><img alt="Figure 3.2 – SQL query prompt" src="img/B22045_03_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – SQL query prompt</p>
<p>As shown<a id="_idIndexMarker214"/> in <em class="italic">Figure 3</em><em class="italic">.2</em>, we <a id="_idIndexMarker215"/>are specifying the following prompt elements to the Titan Text G1 – Premier model:</p>
<ul>
<li><code>You are querying a database with the following schema: Customers(id, name, age) and Orders(id, cust_id, </code><code>product, amount).</code></li>
<li><code>List all customers who have placed more than 1 order, along with their total </code><code>order amounts.</code></li>
<li><code>SQL query:</code></li>
</ul>
<p>This prompt provides clear instructions, relevant schema context, a sample input, and output indicators to produce a suitable SQL query.</p>
<p><strong class="bold">Example 2: </strong><strong class="bold">Recipe generation</strong></p>
<p>Here is another example (<em class="italic">Figure 3</em><em class="italic">.3</em>):</p>
<div><div><img alt="Figure 3.3 – Recipe prompt" src="img/B22045_03_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – Recipe prompt</p>
<p>Let’s take a closer look:</p>
<ul>
<li><code>Baked salmon is a healthy weeknight dinner option perfect with roasted potatoes or rice. The fresh dill adds an </code><code>aromatic flavor.</code></li>
<li><code>Salmon fillet, dill, lemon, salt, pepper, </code><code>olive oil</code>.</li>
<li><code>&lt;|endoftext|&gt;</code>.</li>
</ul>
<p>The preceding prompt provides the model with the recipe’s title, introductory context, ingredients <a id="_idIndexMarker216"/>as input data, and <code>&lt;|endoftext|&gt;</code> as an output indicator, which signals where the recipe steps should begin.</p>
<h2 id="_idParaDest-58"><a id="_idTextAnchor058"/>Prompt engineering applications</h2>
<p>Now that we understand<a id="_idIndexMarker217"/> how we can communicate with the model, let’s learn about some prompt engineering techniques that can aid us in getting better responses from the model.</p>
<p>However, primarily, we need to understand that the optimal prompt engineering approach for any given use case is heavily reliant on the task at hand, as well as the data on which it has been trained.</p>
<p>Some of the tasks that the models on Bedrock excel at are as follows:</p>
<ul>
<li><strong class="bold">Classification</strong>: LLMs exhibit <a id="_idIndexMarker218"/>prowess in text classification, a supervised learning technique for assigning categories to text. For instance, sentiment analysis involves discerning whether an input passage conveys positive or negative emotion. Some LLMs available via Amazon Bedrock, such as the Amazon Titan models, can also identify toxic, harmless, or fact-based content. Their deep contextual understanding aids the judgment of subtle linguistic cues.</li>
<li><strong class="bold">Question-answering</strong>: The models can answer questions accurately without external context due to their vast parameters gained from ingesting hundreds of billions of words during pre-training. When provided with relevant documents, their performance further improves by reasoning over the additional context.</li>
<li><strong class="bold">Summarization</strong>: The models condense lengthy texts into concise summaries that preserve key details, learning to differentiate salient points. Adding such a prompt facilitates rapid analysis of documents.</li>
<li><strong class="bold">Text generation</strong>: The model can generate original coherent text given a short prompt. Their fluency and semantic consistency allow realistic synthesis of stories, poems, scripts, and more.</li>
<li><strong class="bold">Code generation</strong>: For a textual description of a programming need, the models can generate executable code in languages such as SQL and Python. For example, a prompt could request text-to-SQL or Python code generation, thereby accomplishing the outlined computational goal.</li>
<li><strong class="bold">Mathematical reasoning</strong>: The models exhibit an aptitude for mathematical problems provided in text form. This includes numerical calculations, logical deduction, and geometric reasoning. They can further justify solutions with step-by-step explanations.</li>
</ul>
<p>The breadth of natural language tasks that can be mastered by LLMs on Amazon Bedrock exemplifies their versatility. Their adaptive capacity promises to expand application domains even further.</p>
<p>Now that we’ve gained insights into applications of prompt engineering in the real world, let’s try to<a id="_idIndexMarker219"/> unlock some of the most common prompt engineering techniques.</p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor059"/>Unlocking prompt engineering techniques</h1>
<p>The field of <a id="_idIndexMarker220"/>prompt engineering is an active area of research and innovation with new techniques and patterns emerging frequently, driven by the pursuit to improve the performance of the models and generate more natural human-like responses. In this section, we are going to look at some of the most common patterns.</p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>Zero-shot prompting</h2>
<p><strong class="bold">Zero-shot</strong> refers to<a id="_idIndexMarker221"/> the ability of LLMs <a id="_idIndexMarker222"/>to generate reasonable responses to prompts that it has not been explicitly trained on. It relies solely on a descriptive prompt to specify the desired output, as depicted in <em class="italic">Figure 3</em><em class="italic">.4</em>:</p>
<div><div><img alt="Figure 3.4 – Zero-shot prompting" src="img/B22045_03_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – Zero-shot prompting</p>
<p>For instance, a zero-shot prompt to get a poem could be <code>Write a rhyming poem with 4 stanzas about </code><code>seasons changing</code>.</p>
<p>The main advantage of this method is that it’s easier; prompt crafting can be done without providing examples in the input. However, output quality can vary without concrete examples to base on.</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>Few-shot prompting</h2>
<p><strong class="bold">Few-shot prompting</strong> or <strong class="bold">few-shot learning</strong> builds <a id="_idIndexMarker223"/>on zero-shot’s <a id="_idIndexMarker224"/>capabilities. As depicted<a id="_idIndexMarker225"/> in <em class="italic">Figure 3</em><em class="italic">.5</em>, on top of instructions/questions, you can provide a few examples that establish a concept or scenario, at which point models can start to generate reasonable continuations:</p>
<div><div><img alt="Figure 3.5 – Few-shot prompting" src="img/B22045_03_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Few-shot prompting</p>
<p>For instance, after showing two or three examples of short conversations about booking a doctor’s appointment, LLMs can produce an appointment booking dialog without needing thousands of examples. The key benefit over zero-shot is that few-shot examples help narrow down the context and constrain the generation process, making outputs more precise.</p>
<p>Let’s look at some of the examples of few-shot prompting. The following are two inputs that we can use in the poem writing task from the previous sub-section that we can give to the model as examples.</p>
<p><code>Roses are red, violets are blue, spring brings </code><code>life anew.</code></p>
<p><code>Summer sun shining bright, long days full </code><code>of light.</code></p>
<p><code>Now you write a rhyming poem about autumn </code><code>changing leaves.</code></p>
<p>By analyzing these examples, LLMs can learn the pattern of rhyming four-line stanzas about seasons. It can then follow the template to generate an autumn poem. Balancing creativity and <a id="_idIndexMarker226"/>guidance is key in few-shot<a id="_idIndexMarker227"/> prompts.</p>
<p>Additional examples of few-shot prompting are available here: <a href="https://www.promptingguide.ai/techniques/fewshot">https://www.promptingguide.ai/techniques/fewshot</a>.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>Chain-of-thought prompting</h2>
<p><strong class="bold">Chain of thought</strong> (<strong class="bold">CoT</strong>) <strong class="bold">prompting</strong> aims<a id="_idIndexMarker228"/> to elicit reasoning chains from language models. It <a id="_idIndexMarker229"/>involves providing the LLM with a prompt that lays out a reasoning chain or train of thought for the model to follow (<em class="italic">Figure 3</em><em class="italic">.6</em>).</p>
<div><div><img alt="Figure 3.6 – Chain-of-thought prompting" src="img/B22045_03_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Chain-of-thought prompting</p>
<p>A prompt may start with background context, state a hypothesis or problem, provide reasoning steps, and end with a conclusion to be expanded on. The model then tries to continue the chain of reasoning coherently in its generated text:</p>
<p>Here’s an example of a CoT prompt:</p>
<p><code>Recycling is beneficial for the environment because it reduces waste sent to landfills. Landfills produce methane, a potent greenhouse gas. They also take up large amounts of space. Recycling reduces landfill contributions by reusing materials. In conclusion, recycling helps fight climate change by reducing landfill methane and space requirements. The main environmental benefits of </code><code>recycling are...</code></p>
<p>This prompting style guides the LLM to follow the provided reasoning chain and elaborate further on the conclusion statement. The generated text will likely discuss reduced methane emissions and land use from increased recycling in more detail. Hence, chaining further encourages step-by-step logical thinking that focuses on the end goal over open-ended, meandering text.</p>
<p>Let’s look at some <a id="_idIndexMarker230"/>examples of CoT<a id="_idIndexMarker231"/> prompting and the responses:</p>
<p><strong class="bold">Example 1</strong>:</p>
<p><code>Write a story about a professor exploring a </code><code>mysterious artifact</code></p>
<pre class="console">
Dr. Jones entered the temple, brushing cobwebs out of her face. In the center of the room sat a small stone box...</pre>
<p><code>As she opens the box, a glowing light emerges, illuminating symbols on </code><code>the walls</code></p>
<pre class="console">
Squinting in the sudden brightness, Dr. Jones sees the symbols represent a long-lost ancient language. She reaches towards the box and feels an energy pulling at her fingertips...</pre>
<p><strong class="bold">Example 2</strong>:</p>
<p><code>What are the pros and cons of renewable </code><code>energy sources?</code></p>
<pre class="console">
Some pros of renewable energies are...</pre>
<p><code>You missed one major renewable source. Please include geothermal energy when comparing the pros </code><code>and cons.</code></p>
<pre class="console">
Some pros of renewable energies like solar, wind, geothermal and hydroelectric power are low pollution...</pre>
<p>In this approach, the human can guide and shape the overall narrative or analysis by inserting additional prompts whenever they want to redirect, refine, or expand the model’s response. It allows for a more conversational flow based on an initial theme or direction provided by the human.</p>
<p>Additional <a id="_idIndexMarker232"/>examples of <a id="_idIndexMarker233"/>CoT prompting are available at <a href="https://www.promptingguide.ai/techniques/cot">https://www.promptingguide.ai/techniques/cot</a>.</p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/>ReAct prompting</h2>
<p><strong class="bold">Reasoning and Acting</strong> (<strong class="bold">ReAct</strong>) prompting <a id="_idIndexMarker234"/>is derived <a id="_idIndexMarker235"/>from the ReAct framework, as introduced in the paper <em class="italic">ReAct: Synergizing Reasoning and Acting in Language </em><em class="italic">Models</em> (<a href="https://arxiv.org/abs/2210.03629">https://arxiv.org/abs/2210.03629</a>):</p>
<div><div><img alt="Figure 3.7 – ReAct prompting" src="img/B22045_03_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – ReAct prompting</p>
<p>This framework generates various reasoning traces. Then, based on those traces, it performs relevant actions. Through this process of reasoning and acting, you can also allow the application to have access to the organization’s internal data stores and APIs, or external sources from various tools, websites, web searches, and more, as shown in <em class="italic">Figure 3</em><em class="italic">.7</em>. This process of generating reasoning traces and performing actions is iterative and occurs until the LLM finds the answer to the user’s prompt.</p>
<p>We will discuss ReAct prompting in more detail in <a href="B22045_10.xhtml#_idTextAnchor192"><em class="italic">Chapter 10</em></a>. For now, let’s create a ReAct-SQL tool for deriving a comprehensive response from the model.</p>
<p><code>Provide an updated financial summary report </code><code>for AnyCompany</code></p>
<p><strong class="bold">Assistant response:</strong></p>
<p><em class="italic">Thought</em>: The user is asking for financial information about AnyCompany. To answer this, I need to do the following:</p>
<ul>
<li>Query the database to find financial records for AnyCompany</li>
<li>Write a summary of the financial information obtained</li>
</ul>
<p>So, I will need<a id="_idIndexMarker236"/> to<a id="_idIndexMarker237"/> use the <code>sql_db_query</code> tool.</p>
<p><em class="italic">Action</em>: <code>sql_db_query</code></p>
<p><em class="italic">Action input</em>: <code>SELECT * FROM financial_db WHERE company = '</code><code>AnyCompany';</code></p>
<p><code>[(260B, 55B, </code><code>321B, 2.2T)]</code></p>
<p><em class="italic">Answer</em>:</p>
<pre class="console">
AnyCompany has demonstrated strong financial performance with revenue of $260 billion. With net income of $55 billion, AnyCompany has a 21% net profit margin, indicating it is highly profitable. AnyCompany also has a considerable asset base of $321 billion, providing the company with significant financial resources. With a market capitalization of $2.2 trillion, AnyCompany is the most valuable public company, suggesting investors are confident in AnyCompany continued growth and profitability. Overall, AnyCompany's finances appear very healthy.</pre>
<p>Similar to the preceding form of prompting patterns, new prompt engineering techniques and patterns are burgeoning these days. More details on prompt engineering can be found in the prompt engineering guide at <a href="https://www.promptingguide.ai/techniques">https://www.promptingguide.ai/techniques</a>.</p>
<p>In this section, we uncovered several prompt engineering techniques that are being leveraged in the<a id="_idIndexMarker238"/> realm of building Generative AI applications for various use cases. Next, we will work on designing prompts<a id="_idIndexMarker239"/> for Amazon Bedrock FMs.</p>
<h1 id="_idParaDest-64"><a id="_idTextAnchor064"/>Designing prompts for Amazon Bedrock models</h1>
<p>In this section, we’re <a id="_idIndexMarker240"/>going to cover prompt guidance for some of the models available via Amazon Bedrock. We will kick things off with the Anthropic Claude model and provide details around prompt guidance for this. The majority of the learning for prompt guidance can be inherited from Claude models. Furthermore, for the sake of striking a balance between brevity and detail, we will shine a light on the models from Amazon Titan, AI21 Labs, and Stability AI Stable Diffusion. This will sum up our prompt guidance and associated prompt recommendations for invoking Amazon Bedrock models.</p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/>Prompting Anthropic Claude 3</h2>
<p>Here are some things <a id="_idIndexMarker241"/>to <a id="_idIndexMarker242"/>keep in mind while prompting the Anthropic Claude 3 model:</p>
<ul>
<li><code>You are a seasoned Children's Book Author</code> or <code>You are a </code><code>Business Expert</code>.<p class="list-inset"><code>Imagine you are a kindergarten teacher and have to provide an explanation for rainbows in the sky to </code><code>the children.</code>:</p></li>
</ul>
<div><div><img alt="Figure 3.8 – Anthropic Claude 3 Haiku – simple prompting" src="img/B22045_03_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Anthropic Claude 3 Haiku – simple prompting</p>
<p class="list-inset">Without assigning a <a id="_idIndexMarker243"/>role/persona, the <a id="_idIndexMarker244"/>answer may be complex to understand, as shown in <em class="italic">Figure 3</em><em class="italic">.8</em>.</p>
<p class="list-inset">After adding the role/persona, you can see that the output response aligns more with a child’s complexity level in <em class="italic">Figure 3</em><em class="italic">.9</em>:</p>
<div><div><img alt="Figure 3.9 – Anthropic Claude 3 Haiku – assigning role personas" src="img/B22045_03_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Anthropic Claude 3 Haiku – assigning role personas</p>
<ul>
<li><code>Temperature</code> parameter can be set to a higher value, the output that’s generated might be different, as shown here. However, the clear instructions in the latter provide a more direct output without any additional context, as desired by the user:</li>
</ul>
<div><div><img alt="Figure 3.10 – Providing clear and direct instructions" src="img/B22045_03_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Providing clear and direct instructions</p>
<ul>
<li><strong class="bold">Few-shot prompt examples</strong>: Providing <a id="_idIndexMarker247"/>examples (as<a id="_idIndexMarker248"/> covered in the <em class="italic">Few-shot prompting</em> section) of some common scenarios aids in the overall performance gain of the model and generating succinct responses with proper formatting.</li>
<li><code>&lt;tag&gt;content&lt;/tag&gt;</code>, can assist in providing a definitive structure in the prompt and the output response. We can provide additional context and clarification to Claude stating a piece of information can be found within the tags to be leveraged for generating the output. In such a way, Claude understands how to frame the output response, by extracting the key relevant information from the tags.<p class="list-inset">It’s also recommended to separate the input data from the instructions to generate a more structured prompt for easier and more performant processing by the model.</p><p class="list-inset">Here’s an example of tags:</p></li>
</ul>
<div><div><img alt="Figure 3.11 – XML tags" src="img/B22045_03_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.11 – XML tags</p>
<p class="list-inset">As shown in <em class="italic">Figure 3</em><em class="italic">.11</em>, we provided a <code>&lt;email&gt;</code> tag, which generated a more structured <a id="_idIndexMarker249"/>output<a id="_idIndexMarker250"/> response from the model.</p>
<ul>
<li><strong class="bold">Response limiters and defining the output format</strong>: Anthropic Claude models (especially the 100K and 200K token length models) are capable of providing comprehensive and verbose responses. The user can limit the response length by explicitly stating the word limit, or character count, as part of the prompt to provide a more succinct and relevant output. Let’s look at an example:</li>
</ul>
<div><div><img alt="Figure 3.12 – Response limiters" src="img/B22045_03_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.12 – Response limiters</p>
<p class="list-inset">In <em class="italic">Figure 3</em><em class="italic">.12</em>, we have set the response limiter to 100 words as part of the prompt. Furthermore, specifying the desired output format – be it a list, JSON, paragraph, Markdown, and so on – can lead to more performant and precise output, as desired by the user, which aids in eliminating any irrelevant verbiage from chatty models.</p>
<ul>
<li><code>If you don't know the answer, respond with the following format – My sincere apologies, I'm not aware of the answer</code> can aid in avoiding any form of hallucination from chatty Claude models. Guardrails <a id="_idIndexMarker251"/>for the<a id="_idIndexMarker252"/> topics can also be added such that Claude doesn’t respond to unwanted inputs. This concept will also be discussed in <a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a>.</li>
</ul>
<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/>Prompting Mistral models</h2>
<p>Similar to other <a id="_idIndexMarker253"/>models, when<a id="_idIndexMarker254"/> working with Mistral models, crafting well-designed prompts is crucial to obtaining high-quality and relevant outputs. Here are some key points to keep in mind while designing the prompts for Mistral models:</p>
<ul>
<li>Clearly define the task or objective you want the model to accomplish through the prompt. Are you looking for text classification, summarization, personalization, or something else?</li>
<li>Provide relevant context, examples, or background information to ground the model’s understanding before stating the core prompt. Context helps the model better comprehend the prompt.</li>
<li>Use clear formatting and delimiters such as <code>#</code>, <code>###</code>, or <code>&lt;&lt;&lt; &gt;&gt;&gt;</code> to separate different sections of the prompt, such as instructions, examples, and the main query. This enhances the prompt’s structure.</li>
<li>When possible, demonstrate the desired output through examples in a <em class="italic">few-shot</em> learning style. Showing examples guides the model toward the expected format.</li>
<li>Specify the role the model should take on, such as a customer service agent or technical writer. Defining a persona makes responses more tailored.</li>
<li>For an open-ended<a id="_idIndexMarker255"/> generation, provide <a id="_idIndexMarker256"/>clear instructions on the desired output length and structure through numeric targets such as word counts or the number of sentences/paragraphs.</li>
<li>Ask the model to include confidence scores or assessments when generating outputs to gauge its certainty levels.</li>
<li>Consider chaining multiple Mistral models in a sequence, where the output from one model feeds into the next for enhanced capabilities.</li>
<li>Test and iterate on prompt designs through evaluations to find optimal prompting strategies.</li>
</ul>
<p><em class="italic">Figure 3</em><em class="italic">.13</em> shows the Mixtral 8x7B Instruct model being invoked in Amazon Bedrock Playground.</p>
<p>Please note that the <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code> tokens are utilized to<a id="_idIndexMarker257"/> denote<a id="_idIndexMarker258"/> the <code>[INST]</code> and <code>[/INST]</code> strings tell the model that the content enclosed between them constitutes instructions that the model should adhere to:</p>
<div><div><img alt="Figure 3.13 – Prompting the Mixtral 8x7B Instruct model" src="img/B22045_03_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.13 – Prompting the Mixtral 8x7B Instruct model</p>
<p>The key is to <a id="_idIndexMarker259"/>carefully <a id="_idIndexMarker260"/>structure prompts with clear context, examples, instructions, and formatting to steer Mistral models to generate high-quality, tailored outputs matching your needs.</p>
<h2 id="_idParaDest-67"><a id="_idTextAnchor067"/>Prompt guidance for Amazon Titan text models</h2>
<p>As we learned in <a href="B22045_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, Amazon<a id="_idIndexMarker261"/> Titan text <a id="_idIndexMarker262"/>models are well suited for a plethora of use cases:</p>
<ul>
<li>Dialog and roleplay systems</li>
<li>Text summarization and Q&amp;A</li>
<li>Machine translation</li>
<li>Metadata extraction and analysis</li>
<li>RAG (This will be covered in detail in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>)</li>
<li>Code generation approaches</li>
<li>Text and content generation</li>
</ul>
<p>When generating text outputs with the model, it is often recommended to provide clear instructions on the desired output length and structure to get optimal results. Here are some additional tips for Titan text models:</p>
<ul>
<li>Focus prompts on concise, directed questions to get targeted answers by default.</li>
<li>Systems perform best on single sentences or short paragraphs.</li>
<li>For longer inputs, place instructions at the end to guide high-quality responses.</li>
<li>Adding explicit instructions in the prompt produces more tailored results.</li>
<li>Specify an exact number of words, sentences, bullet points, or paragraphs you want the AI to generate in the prompt. Providing a numerical range (for example, 100-200 words) can also work well. This gives the model a clear target to aim for.</li>
<li>Avoid vague instructions such as <code>keep it short</code> or <code>summarize briefly</code>. These are open to interpretation by the AI. Precise numbers remove ambiguity.</li>
<li>Word count alone may not sufficiently guide output length as sentence lengths can vary. Specifying the number of sentences/paragraphs provides more robust control.</li>
<li>If the model seems unable to generate a quality response for a prompt, program it to default to a message such as <code>Unsure about answer</code> rather than attempt to force a poor response. Here’s an example of such a prompt: <code>Tell me about Quantum Computing. Respond with Unsure about the answer or I don't know in case you are not sure about the question </code><code>being asked</code>.</li>
<li>When relevant, provide context paragraphs for the AI to reference before asking a question. This provides knowledge for an informed response.</li>
<li>Test different output length instructions to find the right balance between conciseness and adequate detail for your use case. Err on the side of more specificity with numbers.</li>
</ul>
<p>You can go back <a id="_idIndexMarker263"/>to<a id="_idIndexMarker264"/> the <em class="italic">Amazon Titan FMs</em> section in <a href="B22045_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a> if you wish to look at example prompts and responses in Titan models.</p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor068"/>AI21 Labs – instruct models</h2>
<p>AI21 Labs models work v<a id="_idIndexMarker265"/>ery well with languages<a id="_idIndexMarker266"/> other than English, such as Spanish, French, German, Portuguese, Italian, and Dutch. The model is proficient in text summarization, text generation, and Q&amp;A tasks. In this section, we will walk through a few key concepts to be inculcated with AI21 models available via Amazon Bedrock:</p>
<ul>
<li><strong class="bold">Output length</strong>: To generate desirable responses from AI12 models, it is advisable to specify the output length – that is, the number of paragraphs, items, and so on or an approximation of the same – instead of using words/characters.</li>
<li><strong class="bold">Provide short yet detailed task descriptions</strong>: Craft clear, detailed task descriptions to minimize ambiguity. AI21 models excel at following precise instructions for even complex jobs.</li>
<li><code>no more than x statements</code>. It is always recommended to state requirements directly and affirmatively.</li>
<li><code>Instruction:</code> header to clarify the prompt. Separate prompt sections with newlines to highlight distinct pieces and for readability purposes.</li>
<li><strong class="bold">Evaluate multiple prompting patterns</strong>: Try both zero-shot and few-shot learning. Pick an ideal approach for your use case. For instance, as depicted in the zero-shot and few-shot examples shown in this chapter, depending on the use case under consideration, you may initiate by providing zero examples and determining the response, and simultaneously compare the response generated from the model <a id="_idIndexMarker267"/>after providing <a id="_idIndexMarker268"/>certain examples to guide the output. In some cases, there may not be a need to provide a ton of examples if the model can generate a desirable response.</li>
</ul>
<p><em class="italic">Figure 3</em><em class="italic">.14</em> depicts a product description summarization example from AI21 Jurassic-2 Ultra within Amazon Bedrock:</p>
<div><div><img alt="Figure 3.14 – Prompting the AI21 Jurassic-2 Ultra model" src="img/B22045_03_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.14 – Prompting the AI21 Jurassic-2 Ultra model</p>
<p>Further details on<a id="_idIndexMarker269"/> prompt engineering and design<a id="_idIndexMarker270"/> with AI21 models, along with examples, can be found at <a href="https://docs.ai21.com/docs/prompt-engineering">https://docs.ai21.com/docs/prompt-engineering</a>.</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor069"/>Prompting Meta Llama models</h2>
<p>Similar to any other <a id="_idIndexMarker271"/>LLM, effective <a id="_idIndexMarker272"/>prompting is essential for getting the most out of Llama models. Since the following is standard prompt guidance for any LLM, we will cover some of the best practices here:</p>
<ul>
<li><strong class="bold">Clarity </strong><strong class="bold">and specificity</strong>:<ul><li>Ensure that your prompts are clear, concise, and unambiguous</li><li>Provide sufficient context and details to guide the model toward the desired output</li><li>Use precise language and avoid vague or open-ended statements</li></ul></li>
<li><strong class="bold">Structure </strong><strong class="bold">and formatting</strong>:<ul><li>Organize your prompts logically and structure them in a way that aligns with the desired output format</li><li>Utilize formatting elements such as bullet points, numbered lists, or headings to enhance readability and comprehension</li><li>Consider providing examples or templates to illustrate the expected format of the output</li></ul></li>
<li><strong class="bold">Task framing</strong>:<ul><li>Frame your<a id="_idIndexMarker273"/> prompts<a id="_idIndexMarker274"/> as specific tasks or instructions for the model to follow</li><li>Clearly specify the desired action, such as summarizing, generating, or analyzing</li><li>Provide context about the intended use case or audience for the output</li></ul></li>
<li><strong class="bold">Iterative refinement</strong>:<ul><li>Prompting is an iterative process, and you may need to refine your prompts based on the model’s responses</li><li>Analyze the output and identify areas for improvement or clarification</li><li>Incorporate feedback and adjust the prompt accordingly to steer the model toward better results</li></ul></li>
<li><strong class="bold">Fine-tuning </strong><strong class="bold">and customization</strong>:<ul><li>Explore the possibility of fine-tuning the Llama model on domain-specific data or examples</li><li>Customize the model’s behavior and outputs by incorporating specific instructions or constraints in the prompt</li><li>Leverage techniques such as prompting with few-shot examples or demonstrations to improve performance</li></ul></li>
<li><strong class="bold">Ethical and </strong><strong class="bold">safety considerations</strong>:<ul><li>Be mindful of potential biases or harmful outputs that the model could generate</li><li>Incorporate explicit instructions or filters to mitigate risks and ensure the model’s responses align with ethical and safety guidelines</li><li>Monitor and evaluate the model’s outputs for any concerning or inappropriate content</li></ul></li>
</ul>
<p>Llama models also<a id="_idIndexMarker275"/> consider <a id="_idIndexMarker276"/>special kinds of tokens. For Llama 3, the following tokens are used:</p>
<ul>
<li>The <code>&lt;|begin_of_text|&gt;</code> token represents the BOS token.</li>
<li>The <code>&lt;|eot_id|&gt;</code> token indicates the end of the current turn or message.</li>
<li>The <code>&lt;|start_header_id|&gt;{role}&lt;|end_header_id|&gt;</code> token encloses the role for a particular message, which can be either <strong class="bold">system</strong>, <strong class="bold">user</strong>, or <strong class="bold">assistant</strong>.</li>
<li>The <code>&lt;|end_of_text|&gt;</code> token is equivalent to the EOS token. Upon generating this token, Llama 3 will stop producing any further tokens.</li>
</ul>
<p>For more details on prompt formats, go to <a href="https://llama.meta.com/docs/model-cards-and-prompt-formats">https://llama.meta.com/docs/model-cards-and-prompt-formats</a>:</p>
<div><div><img alt="Figure 3.15 – Prompting the Llama2 Chat 70B model" src="img/B22045_03_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.15 – Prompting the Llama2 Chat 70B model</p>
<p>As shown <a id="_idIndexMarker277"/>in <em class="italic">Figure 3</em><em class="italic">.15</em>, the<a id="_idIndexMarker278"/> Llama 2 Chat 70B model is being invoked in Amazon Bedrock Playground.</p>
<p>The <code>[INST]</code> and <code>[/INST]</code> strings tell the model that the content enclosed between them constitutes instructions that the model should adhere to.</p>
<p>If you’d like to learn about the different examples and templates for invoking various models, including any new models being added to Amazon Bedrock, go to <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html">https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-templates-and-examples.html</a>.</p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor070"/>Prompt guidance for Stability AI – Stable Diffusion</h2>
<p>Stable Diffusion<a id="_idIndexMarker279"/> models (along with Amazon Titan image models) have been gaining popularity<a id="_idIndexMarker280"/> in image generation use cases.  Here are some key tips for crafting effective prompts when using Stability AI’s Stable Diffusion for image generation:</p>
<ul>
<li><code>A photo of a cat</code> or <code>An illustration of a robot</code>. Being more specific usually produces better results.</li>
<li><code>in impressionist style</code> or <code>a cartoon drawing of</code>. Styles help steer the output.</li>
<li><code>cat:1.5, sitting:1.2, couch:1</code>. Higher weights make elements more prominent.</li>
<li><code>-</code>, improves quality by excluding unwanted elements. For example, if you provide <code>Cars racing on racetrack</code> as a prompt and give <code>red car</code> as a negative prompt, it will exclude red cars from the image’s output, as shown in <em class="italic">Figure 3</em><em class="italic">.16</em>:</li>
</ul>
<div><div><img alt="Figure 3.16 – Negative prompt example" src="img/B22045_03_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.16 – Negative prompt example</p>
<ul>
<li><strong class="bold">Be detailed and specific</strong>: Using more descriptive and distinctive words, rather than general terms, produces more tailored results.</li>
<li><code>--ar</code>, <code>--v</code>, and <code>--n</code> control the aspect ratio, vividness, and level of detail, respectively. Tweak them to refine the output.</li>
<li>Providing additional descriptive details always aids the model in being more performant. The following are examples of such aspects:<ul><li>Specify<a id="_idIndexMarker281"/> the <a id="_idIndexMarker282"/>medium (painting, drawing, CGI, and so on)</li><li>Define colors used</li><li>Describe lighting and shadows</li><li>Include the artist’s name if mimicking a style</li><li>Mention the website if you’re reproducing a specific image</li><li>Add any other descriptive remarks or adjectives</li><li>Specify the desired resolution if needed for print or digital use</li></ul></li>
</ul>
<p>Let’s look at an example.</p>
<p><code>Portrait photo of an Indian old warrior chief, tribal panther make up, front profile, looking straight into the camera, serious eyes, 50mm portrait photography, hard rim lighting photography–beta –ar </code><code>2:3 –beta</code></p>
<p><strong class="bold">Output response from </strong><strong class="bold">SDXL 1.0</strong>:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 3.17 – Image generation output from SDXL 1.0" src="img/B22045_03_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.17 – Image generation output from SDXL 1.0</p>
<p>When prompted <a id="_idIndexMarker283"/>with the <a id="_idIndexMarker284"/>same input, the output response from the Titan Image Generator G1 model is as follows:</p>
<div><div><img alt="Figure 3.18 – Image generation output from Titan Image Generator" src="img/B22045_03_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.18 – Image generation output from Titan Image Generator</p>
<p>Providing this level of detail and context will help produce a more accurate image that matches your vision. Adjust prompts iteratively to refine results.</p>
<p>You might be wondering why the same prompt generated different outputs from two models. The reason for this is that SDXL is trained on a different set of data than Titan Image Generator, so the output you will see from these models will differ. Think of it this way: SDXL and Titan are two people, who learn from two different books for an exam. During the exam, when they were asked the same question, they would have two different viewpoints, and their answer would be based on the books they read.</p>
<p>If you are trying these prompts in your environment, you might also notice another thing. The output image that you are seeing in your environment might differ from the one shown here, even if you provide the same prompts. The reason for this is to do with the added degree of randomness. These models will generate output based on the inference parameters, such as <em class="italic">Prompt Strength</em> and <em class="italic">Seed</em>. We will cover these parameters in detail in <a href="B22045_09.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>. However, in short, prompt strength controls how you want the model output to be influenced by the prompt, and the seed is a way to randomize an output image.</p>
<p>Now that you have a good understanding of prompt guidance for FMs provided by <a id="_idIndexMarker285"/>Amazon <a id="_idIndexMarker286"/>Bedrock, in the next section, we will try to sum up some of the key principles and techniques you must follow when it comes to prompt engineering in various use cases.</p>
<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/>Understanding best practices in prompt engineering</h1>
<p>To summarize, when<a id="_idIndexMarker287"/> crafting prompts, you must adhere to the following key principles:</p>
<ul>
<li><code>What are the use cases of renewable resources? List 5 </code><code>key points</code>.</li><li><em class="italic">Language emphasis</em>: Using simple flowing language with coherent sentences assists in crafting a better prompt and avoiding isolated phrases.</li><li><em class="italic">Get into the model’s head</em>: Craft prompts to nudge it toward helpful behaviors. Think of it as someone who has all the right answers but only for correctly articulated questions.</li></ul></li>
<li><code>Summarize the chapter in </code><code>500 words</code>.</li><li><em class="italic">Provide example responses</em>: Adding some example responses within the prompt with the expected output can refine the responses more desirably – for example, <code>Summarize this chapter in one paragraph (1000 characters): [New study shows decreasing activity in region X leads to impairment.]</code>. Surrounding the example response in brackets indicates the model adheres to the guidelines set <a id="_idIndexMarker288"/>by the user while responding in the desired format.</li><li><em class="italic">Add constraints</em>: Constraining prompt responses by format, additional information inclusion, length, and more can lead to more controlled output.</li></ul></li>
<li><strong class="bold">Strike the right detail balance</strong>: Too little detail fails to guide the model adequately while excessive verbosity limits creative flourishes. Distill prompts down to concise essence:<ul><li><em class="italic">Complex </em><em class="italic">task handling</em>:<ul><li>FMs can hallucinate when dealing with complex tasks. It is advisable to break down the complex task into subtasks or even consider splitting complex tasks into multiple prompts.</li><li>Provide emphasis by using keywords to ask the model to think step-by-step or provide logical reasoning as it is crafting the output. Provide some key examples in the input for complex tasks.</li></ul></li><li><em class="italic">Rinse, lather, repeat</em>: Iteratively break down and try different prompts to optimize model responses for your goals. Continue adjusting while testing and experimenting to achieve the desired results.</li><li><em class="italic">Continuous evaluation</em>: Iteratively reviewing the model’s responses to provide the desired quality is a must when it comes to handling different use cases and complex scenarios.</li></ul></li>
</ul>
<p>Prompts unlock Generative AI’s capabilities but require thoughtfulness to construct properly. Learn your target model’s strengths and limitations, iterate carefully on prompt phrasings, and appreciate these systems’ ever-evolving nature. Wield prompts judiciously and enjoy the fruits of AI’s burgeoning creativity!</p>
<p>Complexity arises in structuring the right prompts to handle intricate goals. But when done well, prompts unlock AI like a skeleton key, opening doors to breathtaking new generative<a id="_idIndexMarker289"/> capabilities. The prompt contains the potential; our role is to shape and guide it with thoughtful prompts.</p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor072"/>Summary</h1>
<p>In this chapter, we learned about several prompt engineering techniques to gain a deeper understanding of prompting patterns and uncovered insights while considering examples of said prompting patterns. Then, we dived into prompt guidance with Amazon Bedrock models for Anthropic Claude, AI21 Labs, Amazon Titan, and Stability AI’s Stable Diffusion.</p>
<p>Lastly, we summarized the practical approach to prompt guidance while looking at Amazon Bedrock models that can be applied to various use cases. Through various examples, we learned how to craft the most effective prompts.</p>
<p>At this point, you should have a good understanding of the importance of prompt engineering. Furthermore, you should be able to analyze various prompt techniques and best practices involved in prompt engineering in the context of building Generative AI applications.</p>
<p>In the next chapter, we’ll learn how to customize a model using fine-tuning and continued pretraining techniques. We will delve into how fine-tuning works, look at various APIs, analyze the results, and perform inference on our fine-tuned model.</p>
</div>
</body></html>