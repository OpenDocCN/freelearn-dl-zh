- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Scalable Knowledge-Graph-Based RAG with Wikipedia API and LlamaIndex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaled datasets can rapidly become challenging to manage. In real-life projects,
    data management generates more headaches than AI! Project managers, consultants,
    and developers constantly struggle to obtain the necessary data to get any project
    running, let alone a RAG-driven generative AI application. Data is often unstructured
    before it becomes organized in one way or another through painful decision-making
    processes. Wikipedia is a good example of how scaling data leads to mostly reliable
    but sometimes incorrect information. Real-life projects often evolve the way Wikipedia
    does. Data keeps piling up in a company, challenging database administrators,
    project managers, and users.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the main problems is seeing how large amounts of data fit together,
    and **knowledge graphs** provide an effective way of visualizing the relationships
    between different types of data. This chapter begins by defining the architecture
    of a knowledge base ecosystem designed for RAG-driven generative AI. The ecosystem
    contains three pipelines: data collection, populating a vector store, and running
    a knowledge graph index-based RAG program. We will then build *Pipeline 1: Collecting
    and preparing the documents*, in which we will build an automated Wikipedia retrieval
    program with the Wikipedia API. We will simply choose a topic based on a Wikipedia
    page and then let the program retrieve the metadata we need to collect and prepare
    the data. The system will be flexible and allow you to choose any topic you wish.
    The use case to first run the program is a marketing knowledge base for students
    who want to upskill for a new job, for example. The next step is to build *Pipeline
    2: Creating and populating the Deep Lake vector store*. We will load the data
    in a vector store leveraging Deep Lake’s in-built automated chunking and OpenAI
    embedding functionality. We will peek into the dataset to explore how this marvel
    of technology does the job.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will build *Pipeline 3: Knowledge graph index-based RAG*, where
    LlamaIndex will automatically build a knowledge graph index. It will be exciting
    to see how the index function churns through our data and produces a graph showing
    semantic relationships contained in our data. We will then query the graph with
    LlamaIndex’s in-built OpenAI functionality to automatically manage user inputs
    and produce a response. We will also see how re-ranking can be done and implement
    metrics to calculate and display the system’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining knowledge graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Wikipedia API to prepare summaries and content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citing Wikipedia sources in an ethical approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Populating a Deep Lake vector store with Wikipedia data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a knowledge graph index with LlamaIndex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying the LlamaIndex knowledge graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with the knowledge graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating retrieval responses with the knowledge graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-ranking the order retrieval responses to choose a better output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating and measuring the outputs with metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining the architecture of RAG for knowledge-based semantic
    search.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of RAG for knowledge-graph-based semantic search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As established, we will build a graph-based RAG program in this chapter. The
    graph will enable us to visually map out the relationships between the documents
    of a RAG dataset. It can be created automatically with LlamaIndex, as we will
    do in the *Pipeline 3: Knowledge graph index-based RAG* section of this chapter.
    The program in this chapter will be designed for any Wikipedia topic, as illustrated
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a graph  Description automatically generated](img/B31169_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: From a Wikipedia topic to interacting with a graph-based vector
    store index'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first implement a marketing agency for which a knowledge graph can
    visually map out the complex relationships between different marketing concepts.
    Then, you can go back and explore any topic you wish once you understand the process.
    In simpler words, we will implement the three pipelines seamlessly to:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a Wikipedia topic related to *marketing*. Then, you can run the process
    with the topic of your choice to explore the ecosystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a corpus of Wikipedia pages with the Wikipedia API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve and store the citations for each page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve and store the URLs for each page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieve and upsert the content of the URLs in a Deep Lake vector store.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build a knowledge base index with LlamaIndex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a user input prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query the knowledge base index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let LlamaIndex’s in-built LLM functionality, based on OpenAI’s embedding models,
    produce a response based on the embedded data in the knowledge graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the LLM’s response with a sentence transformer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the LLM’s response with a human feedback score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide time metrics for the key functions, which you can extend to other functions
    if necessary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run metric calculations and display the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To attain our goal, we will implement three pipelines leveraging the components
    we have already built in the previous chapters, as illustrated in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a graph  Description automatically generated](img/B31169_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Knowledge graph ecosystem for index-based RAG'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 1: Collecting and preparing the documents** will involve building
    a Wikipedia program using the Wikipedia API to retrieve links from a Wikipedia
    page and the metadata for all the pages (summary, URL, and citation data). Then,
    we will load and parse the URLs to prepare the data for upserting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline 2: Creating and populating the Deep Lake vector store** will embed
    and upsert parsed content of the Wikipedia pages prepared by *Pipeline 1* to a
    Deep Lake vector store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline 3: Knowledge graph index-based RAG** will build the knowledge graph
    index using embeddings with LlamaIndex and display it. Then, we will build the
    functionality to query the knowledge base index and let LlamaIndex’s in-built
    LLM generate the response based on the updated dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter’s scenario, we are directly implementing an augmented retrieval
    system leveraging OpenAI’s embedding models more than we are augmenting inputs.
    This implementation shows the many ways we can improve real-time data retrieval
    with LLMs. There are no conventional rules. What works, works!
  prefs: []
  type: TYPE_NORMAL
- en: The ecosystem of the three pipelines will be controlled by a scenario that will
    enable an administrator to either query the vector base or add new Wikipedia pages,
    as we will implement in this chapter. As such, the architecture of the ecosystem
    allows for indefinite scaling since it processes and populates the vector dataset
    one set of Wikipedia pages at a time. The system only uses a CPU and an optimized
    amount of memory. There are limits to this approach since the LlamaIndex knowledge
    graph index is loaded with the entire dataset. We can only load portions of the
    dataset as the vector store grows. Or, we can create one Deep Lake vector store
    per topic and run queries on multiple datasets. These are decisions to make in
    real-life projects that require careful decision-making and planning depending
    on the specific requirements of each project.
  prefs: []
  type: TYPE_NORMAL
- en: We will now dive into the code, beginning a tree-to-graph sandbox.
  prefs: []
  type: TYPE_NORMAL
- en: Building graphs from trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A graph is a collection of nodes (or vertices) connected by edges (or arcs).
    Nodes represent entities, and edges represent relationships or connections between
    these entities. For instance, in our chapter’s use case, nodes could represent
    various marketing strategies, and the edges could show how these strategies are
    interconnected. This helps new customers understand how different marketing tactics
    work together to achieve overall business goals, facilitating clearer communication
    and more effective strategy planning. You can play around with the tree-to-graph
    sandbox before building the pipelines in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: You may open `Tree-2-Graph.ipynb` on GitHub. The provided program is designed
    to visually represent relationships in a tree structure using NetworkX and Matplotlib
    in Python. It specifically creates a directed graph from given pairs, checks and
    marks friendships, and then displays this tree with customized visual attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first defines the main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`build_tree_from_pairs(pairs)`: Constructs a directed graph (tree) from a list
    of node pairs, potentially identifying a root node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`check_relationships(pairs, friends)`: Checks and prints the friendship status
    for each pair'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`draw_tree(G, layout_choice, root, friends)`: Visualizes the tree using `matplotlib`,
    applying different styles to edges based on friendship status and different layout
    options for node positioning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, the program executes the process from tree to graph:'
  prefs: []
  type: TYPE_NORMAL
- en: Node pairs and friendship data are defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tree is built from the pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relationships are checked against the friendship data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tree is drawn using a selected layout, with edges styled differently to
    denote friendship.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, the program first defines a set of node pairs with their pairs
    of friends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `('a', 'z')` are not friends because they are not on the `friends`
    list. Neither are `('b', 'q')`. You can imagine any type of relationship between
    the pairs, such as the same customer age, similar job, same country, or any other
    concept you wish to represent. For instance, the `friends` list could contain
    relationships between friends on social media, friends living in the same country,
    or anything else you can imagine or need!
  prefs: []
  type: TYPE_NORMAL
- en: 'The program then builds the tree and checks the relationships:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows which pairs are friends and which ones are not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be used to provide useful information for similarity searches.
    The program now draws the graph with the `''spring''` layout:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `'spring'` layout attracts nodes attracted by edges, simulating the effect
    of springs. It also ensures that all nodes repel each other to avoid overlapping.
    You can dig into the `draw_tree` function to explore and select other layouts
    listed there. You can also modify the colors and line styles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the pairs of friends are represented with solid lines, and the
    pairs that are not friends are represented with dashes, as shown in the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a tree  Description automatically generated](img/B31169_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Example of a spring layout'
  prefs: []
  type: TYPE_NORMAL
- en: You can play with this sandbox graph with different pairs of nodes. If you imagine
    doing this with hundreds of nodes, you will begin to appreciate the automated
    functionality we will build in this chapter with LlamaIndex’s knowledge graph
    index!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go from the architecture to the code, starting by collecting and preparing
    the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 1: Collecting and preparing the documents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code in this section retrieves the metadata we need from Wikipedia, retrieves
    the documents, cleans them, and aggregates them to be ready for insertion into
    the Deep Lake vector store. This process is illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a document  Description automatically generated](img/B31169_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Pipeline 1 flow chart'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pipeline 1* includes two notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Wikipedia_API.ipynb`, in which we will implement the Wikipedia API to retrieve
    the URLs of the pages related to the root page of the topic we selected, including
    the citations for each page. As mentioned, the topic is “marketing” in our case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Knowledge_Graph_Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`, in which we will implement
    all three pipelines. In Pipeline 1, it will fetch the URLs provided by the `Wikipedia_API`
    notebook, clean them, and load and aggregate them for upserting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will begin by implementing the Wikipedia API.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving Wikipedia data and metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s begin by building a program to interact with the Wikipedia API to retrieve
    information about a specific topic, tokenize the retrieved text, and manage citations
    from Wikipedia articles. You may open `Wikipedia_API.ipynb` in the GitHub repository
    and follow along.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program begins by installing the `wikipediaapi` library we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the tokenization function that will be called to
    count the number of tokens of a summary, as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes a string of text as input and returns the number of tokens
    in the text, using the NLTK library for sophisticated tokenization, including
    punctuation. Next, to start retrieving data, we need to set up an instance of
    the Wikipedia API with a specified language and user agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, English was defined with `''en''`, and you must enter the user
    agent information, such as an email address, for example. We can now define the
    main topic and filename associated with the Wikipedia page of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The three parameters defined are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`topic`: The topic of the retrieval process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filename`: The name of the topic that will customize the files we produce,
    which can be different from the topic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxl`: The maximum number of URL links of the pages we will retrieve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We now need to retrieve the summary of the specified Wikipedia page, check
    if the page exists, and print its summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides the control information requested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The information provided shows if we are on the right track or not before running
    a full search on the main page of the topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Page - Exists: True` confirms that the page exists. If not, the `print("Page
    does not exist")` message will be displayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Number of tokens: 229` provides us with insights into the size of the content
    we are retrieving for project management assessments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of `summary=page.summary` displays a summary of the page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the page exists, fits our topic, and the summary makes sense.
    Before we continue, we check if we are working on the right page to be sure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to retrieve the URLs, links, and summaries on the target page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The function is limited to `maxl`, defined at the beginning of the program.
    The function will retrieve URL links up to `maxl` links, or less if the page contains
    fewer links than the maximum requested. We then check the output before moving
    on to the next step and generating files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We observe that we have the information we need, and the summaries are acceptable:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Link 1`: The link counter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Link`: The actual link to the page retrieved from the main topic page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Summary`: A summary of the link to the page'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next step is to apply the function we just built to generate the text file
    containing citations for the links retrieved from a Wikipedia page and their URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`urls = []` will be appended to have the full list of URLs we need for the
    final step. The output is a file containing the name of the topic, `datetime`,
    and the citations beginning with the citation text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The output, in this case, is a file named `Marketing_citations.txt`. The file
    was downloaded and uploaded to the `/citations` directory of this chapter’s directory
    in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, the citations page has been generated, displayed in this notebook,
    and also saved in the GitHub repository to respect Wikipedia’s citation terms.
    The final step is to generate the file containing the list of URLs we will use
    to fetch the content of the pages we need. We first display the URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms we have the URLs required:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The URLs are written in a file with the topic as a prefix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the output is a file named `Marketing_urls.txt` that contains
    the URLs of the pages we need to fetch. The file was downloaded and uploaded to
    the `/citations` directory of the chapter’s directory in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to prepare the data for upsertion.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for upsertion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The URLs provided by the Wikipedia API in the `Wikipedia_API.ipynb` notebook
    will be processed in the `Knowledge_Graph_ Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`
    notebook you can find in the GitHub directory of the chapter. The *Installing
    the environment* section of this notebook is almost the same section as its equivalent
    section in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*,
    and *Chapter 3*, *Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI*.
    In this chapter, however, the list of URLs was generated by the `Wikipedia_API.ipynb`
    notebook, and we will retrieve it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, go to the *Scenario* section of the notebook to define the strategy
    of the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters will determine the behavior of the three pipelines in the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`graph_name="Marketing"`: The prefix (topic) of the files we will read and
    write.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`db="hub://denis76/marketing01"`: The name of the Deep Lake vector store. You
    can choose the name of the dataset you wish.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vector_store_path = db`: The path to the vector store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_path = db`: The path to the dataset of the vector store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pop_vs=True`: Activates data insertion if `True` and deactivates it if `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ow=True`: Overwrites the existing dataset if `True` and appends it if `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Then, we can launch the *Pipeline 1: Collecting and preparing the documents*
    section of the notebook. The program will download the URL list generated in the
    previous section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'It will then read the file and store the URLs in a list named `urls`. The rest
    of the code in the *Pipeline 1: Collecting and preparing the documents* section
    of this notebook follows the same process as the `Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb`
    notebook from *Chapter 3*. In *Chapter 3*, the URLs of the web pages were entered
    manually in a list.'
  prefs: []
  type: TYPE_NORMAL
- en: The code will fetch the content in the list of URLs. The program then cleans
    and prepares the data to populate the Deep Lake vector store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 2: Creating and populating the Deep Lake vector store'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pipeline in this section of `Deep_Lake_LlamaIndex_OpenAI_RAG.ipynb` was
    built with the code of *Pipeline 2* from *Chapter 3*. We can see that by creating
    pipelines as components, we can rapidly repurpose and adapt them to other applications.
    Also, Activeloop Deep Lake possesses in-built default chunking, embedding, and
    upserting functions, making it seamless to integrate various types of unstructured
    data, as in the case of the Wikipedia documents we are upserting.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the `display_record(record_number)` function shows how seamless
    the process is. The output displays the ID and metadata such as the file information,
    the data collected, the text, and the embedded vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: And with that, we have successfully repurposed the *Pipeline 2* component of
    *Chapter 3* and can now move on and build the graph knowledge index.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 3: Knowledge graph index-based RAG'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It’s time to create a knowledge graph index-based RAG pipeline and interact
    with it. As illustrated in the following figure, we have a lot of work to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a graph  Description automatically generated](img/B31169_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Building knowledge graph-index RAG from scratch'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate the knowledge graph index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the user prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the hyperparameters of LlamaIndex’s in-built LLM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the similarity score packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the similarity score functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run a sample similarity comparison between the similarity functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-rank the output vectors of an LLM response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run evaluation samples and apply metrics and human feedback scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run metric calculations and display them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through these steps and begin by generating the knowledge graph index.
  prefs: []
  type: TYPE_NORMAL
- en: Generating the knowledge graph index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create a knowledge graph index from a set of documents using the `KnowledgeGraphIndex`
    class from the `llama_index.core` module. We will also time the index creation
    process to evaluate performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function begins by recording the start time with `time.time()`. In this
    case, measuring the time is important because it takes quite some time to create
    the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create a `KnowledgeGraphIndex` with embeddings using the `from_documents`
    method. The function uses the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`documents` is the set of documents to index'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_triplets_per_chunk` is set to 2, limiting the number of triplets per chunk
    to optimize memory usage and processing time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`include_embeddings` is set to `True`, indicating that embeddings should be
    included'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The graph index is thus created in a few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The timer is stopped and the creation time is measured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph type is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms the knowledge graph index class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now set up a query engine for our knowledge graph index and configure
    it to manage similarity, response temperature, and output length parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters will determine the behavior of the query engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '`k=3` sets the number of top similar results to take into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temp=0.1` sets the temperature parameter, controlling the randomness of the
    query engine’s response generation. The lower it is, the more precise it is; the
    higher it is, the more creative it is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mt=1024` sets the maximum number of tokens for the output, defining the length
    of the generated responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The query engine is then created with the parameters we defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The graph index and query engine are ready. Let’s display the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying the graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a graph instance, `g`, with `pyvis.network`, a Python library
    used for creating interactive network visualizations. The displayed parameters
    are similar to the ones we defined in the *Building graphs from trees* section
    of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'A directed graph has been created, and now we will save it in an HTML file
    to display it for further use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `graph_name` was defined at the beginning of the notebook, in the *Scenario*
    section. We will now display the graph in the notebook as an HTML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now download the file to display it in your browser to interact with
    it. You can also visualize it in the notebook, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of marketing strategy  Description automatically generated](img/B31169_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: The knowledge graph'
  prefs: []
  type: TYPE_NORMAL
- en: We are all set to interact with the knowledge graph index.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with the knowledge graph index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now define the functionality we need to execute the query, as we have
    done in *Chapter 3* in the *Pipeline 3: Index-based RAG* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '`execute_query` is the function we created that will execute the query: `response
    = graph_query_engine.query(user_input)`. It also measures the time it takes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`user_query="What is the primary goal of marketing for the consumer market?"`,
    which we will use to make the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response = execute_query(user_query)`, which is encapsulated in the request
    code and displays the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output provides the best vectors that we created with the Wikipedia data
    with the time measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: We will now install similarity score packages and define the similarity calculation
    functions we need.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the similarity score packages and defining the functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first retrieve the Hugging Face token from the **Secrets** tab on Google
    Colab, where it was stored in the settings of the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'In August 2024, the token is optional for Hugging Face’s `sentence-transformers`.
    You can ignore the message and comment the code. Next, we install `sentence-transformers`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create a cosine similarity function with embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We import the libraries we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: We have a similarity function and can use it for re-ranking.
  prefs: []
  type: TYPE_NORMAL
- en: Re-ranking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, the program re-ranks the response of a query by reordering
    the top results to select other, possibly better, ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '`user_query=" Which experts are often associated with marketing theory?"` represents
    the query we are making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_time = time.time()` records the start time for the query execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response = execute_query(user_query)` executes the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_time = time.time()` stops the timer, and the query execution time is displayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`for idx, node_with_score in enumerate(response.source_nodes)` iterates through
    the response to retrieve all the nodes in the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`similarity_score3=calculate_cosine_similarity_with_embeddings(text1, text2)`
    calculates the similarity score between the user query and the text in the nodes
    retrieved from the response. All the comparisons are displayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`best_score=similarity_score3` stores the best similarity score found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print(textwrap.fill(str(best_text), 100))` displays the best re-ranked result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The initial response for the `user_query` `"Which experts are often associated
    with marketing theory?"` was:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The response is acceptable. However, the re-ranked response goes deeper and
    mentions the names of marketing experts (highlighted in bold font):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The re-ranked response is longer and contains raw document content instead
    of the summary provided by LlamaIndex’s LLM query engine. The original query engine
    response is better from an LLM perspective. However, it isn’t easy to estimate
    what an end-user will prefer. Some users like short answers, and some like long
    documents. We can imagine many other ways of re-ranking documents, such as modifying
    the prompt, adding documents, and deleting documents. We can even decide to fine-tune
    an LLM, as we will do in *Chapter 9*, *Empowering AI Models: Fine-Tuning RAG Data
    and Human Feedback*. We can also introduce human feedback scores as we did in
    *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*, because, in
    many cases, mathematical metrics will not capture the accuracy of a response (writing
    fiction, long answers versus short input, and other complex responses). But we
    need to try anyway!'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s perform some of the possible metrics for the examples we are going to
    run.
  prefs: []
  type: TYPE_NORMAL
- en: Example metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To evaluate the knowledge graph index’s query engine, we will run ten examples
    and keep track of the scores. `rscores` keeps track of human feedback scores while
    `scores=[]` keeps track of similarity function scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of examples can be increased as much as necessary depending on the
    needs of a project. Each of the ten examples has the same structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`user_query`, which is the input text for the query engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`elapsed_time`, which is the result of the time measurement of the system’s
    response'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response = execute_query(user_query)` executes the query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The user query and output are the same as in the example used for the re-ranking
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this time, we will run a similarity function and also ask a human
    for a score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text1` is the query engine’s response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text2` is the user query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`similarity_score3` is the cosine similarity score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scores.append(similarity_score3)` appends the similarity score to scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`human_feedback` is the human similarity evaluation. We could replace this
    score with a document as we did in *Chapter 5*, *Boosting RAG Performance with
    Expert Human Feedback*, or we could replace the human score with a human text
    response, which will become the ground truth. In both cases, the similarity score
    is recalculated with human feedback content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rscores.append(human_feedback)` appends the human score to `rscores`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review a few of the ten examples’ outputs and add a comment at the end
    of each one.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs are stochastic algorithms. As such, the responses and scores may vary from
    one run to another.
  prefs: []
  type: TYPE_NORMAL
- en: '**Example 1**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User query**: `Which experts are often associated with marketing theory?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response**: Psychologists, cultural anthropologists, and other experts in
    behavioral sciences are often associated with marketing theory.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine similarity score**: `0.809`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human feedback**: `0.75`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comment**: The response is acceptable, but it could be more specific and
    mention the names of experts. However, the prompt is ambiguous and only mentions
    experts in general.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 3**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User query**: `What is the difference between B2B and B2C?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response**: B2B businesses sell products and services to other companies,
    while B2C businesses sell directly to customers.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine Similarity score**: `0.760`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human feedback**: `0.8`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comment**: The response is precise, but in some cases, users like examples.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example 7**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User query**: `What commodity programs does the Agricultural Marketing Service
    (AMS) maintain?`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Response**: The **Agricultural Marketing Service** (**AMS**) maintains programs
    in five commodity areas: cotton and tobacco, dairy, fruit and vegetable, livestock
    and seed, and poultry.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cosine Similarity score**: `0.904`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human feedback**: `0.9`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comment**: This response is accurate and interesting because the information
    is contained in a page linked to the main page. Thus, this is information from
    a linked page to the main page. We could ask Wikipedia to search the links of
    all the linked pages to the main page and go down several levels. However, the
    main information we are looking for may be diluted in less relevant data. The
    decision on the scope of the depth of the data depends on the needs of each project.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now perform metric calculations on the cosine similarity scores and
    the human feedback scores.
  prefs: []
  type: TYPE_NORMAL
- en: Metric calculation and display
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The cosine similarity scores of the examples are stored in `scores`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The ten scores are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We could expand the evaluations to as many other examples, depending on the
    needs of each project. The human feedback scores for the same examples are stored
    in `rscores`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The ten human feedback scores are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply metrics to evaluate the responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Each metric can provide several insights. Let’s go through each of them and
    the outputs obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Central tendency (mean, median)** gives us an idea of what a typical score
    looks like.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variability (standard deviation, variance, range, IQR)** tells us how spread
    out the scores are, indicating the consistency or diversity of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Extremes (minimum, maximum)** show the bounds of our dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distribution (percentiles)** provides insights into how scores are distributed
    across the range of values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s go through these metrics calculated from the cosine similarity scores
    and the human feedback scores and display their outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean (average)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The mean is the sum of all the scores divided by the number
    of scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It gives us the central value of the data, providing an idea of
    the typical score.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B31169_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Output**: `Mean: 0.68`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Median**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The median is the middle value when the scores are ordered
    from smallest to largest.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It provides the central point of the dataset and is less affected
    by extreme values (outliers) compared to the mean.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `Median: 0.71`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The standard deviation measures the average amount by which
    each score differs from the mean.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It gives an idea of how spread out the scores are around the mean.
    A higher value indicates more variability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B31169_07_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Output**: `Standard Deviation: 0.15`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variance**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The variance is the square of the standard deviation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It also measures the spread of the scores, showing how much they
    vary from the mean.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `Variance: 0.02`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minimum**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The minimum is the smallest score in the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It tells us the lowest value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `Minimum: 0.45`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maximum:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The maximum is the largest score in the dataset.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It tells us the highest value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `Maximum: 0.90`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Range**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The range is the difference between the maximum and minimum
    scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It shows the span of the dataset from the lowest to the highest
    value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Range* = *Maximum* - *Minimum*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Output**: `Range: 0.46`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**25**^(th) **Percentile (Q1)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The 25^(th) percentile is the value below which 25% of the
    scores fall.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It provides a point below which a quarter of the data lies.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `25th Percentile (Q1): 0.56`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**75**^(th) **Percentile (Q3)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The 75^(th) percentile is the value below which 75% of the
    scores fall.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It gives a point below which three-quarters of the data lies.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: `75th Percentile (Q3): 0.80`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interquartile Range (IQR)**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Definition**: The IQR is the range between the 25^(th) percentile (Q1) and
    the 75^(th) percentile (Q3).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Purpose**: It measures the middle 50% of the data, providing a sense of the
    data’s spread without being affected by extreme values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation**:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*IQR* = *Q3* – *Q1*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Output**: `Interquartile Range (IQR): 0.24`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We have built a knowledge-graph-based RAG system, interacted with it, and evaluated
    it with some examples and metrics. Let’s sum up our journey.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the creation of a scalable knowledge-graph-based
    RAG system using the Wikipedia API and LlamaIndex. The techniques and tools developed
    are applicable across various domains, including data management, marketing, and
    any field requiring organized and accessible data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Our journey began with data collection in *Pipeline 1*. This pipeline focused
    on automating the retrieval of Wikipedia content. Using the Wikipedia API, we
    built a program to collect metadata and URLs from Wikipedia pages based on a chosen
    topic, such as marketing. In *Pipeline 2*, we created and populated the Deep Lake
    vector store. The retrieved data from *Pipeline 1* was embedded and upserted into
    the Deep Lake vector store. This pipeline highlighted the ease of integrating
    vast amounts of data into a structured vector store, ready for further processing
    and querying. Finally, in *Pipeline 3*, we introduced knowledge graph index-based
    RAG. Using LlamaIndex, we automatically built a knowledge graph index from the
    embedded data. This index visually mapped out the relationships between different
    pieces of information, providing a semantic overview of the data. The knowledge
    graph was then queried using LlamaIndex’s built-in language model to generate
    optimal responses. We also implemented metrics to evaluate the system’s performance,
    ensuring accurate and efficient data retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we had constructed a comprehensive, automated RAG-driven
    knowledge graph system capable of collecting, embedding, and querying vast amounts
    of Wikipedia data with minimal human intervention. This journey showed the power
    and potential of combining multiple AI tools and models to create an efficient
    pipeline for data management and retrieval. You are now all set to implement knowledge
    graph-based RAG systems in real-life projects. In the next chapter, we will learn
    how to implement dynamic RAG for short-term usage.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with yes or no:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the chapter focus on building a scalable knowledge-graph-based RAG system
    using the Wikipedia API and LlamaIndex?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the primary use case discussed in the chapter related to healthcare data
    management?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does *Pipeline 1* involve collecting and preparing documents from Wikipedia
    using an API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is Deep Lake used for creating a relational database in *Pipeline 2*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does *Pipeline 3* utilize LlamaIndex to build a knowledge graph index?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the system designed to only handle a single specific topic, such as marketing,
    without flexibility?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the chapter describe how to retrieve URLs and metadata from Wikipedia pages?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is a GPU required to run the pipelines described in the chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the knowledge graph index visually map out relationships between pieces
    of data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is human intervention required at every step to query the knowledge graph index?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Wikipedia API GitHub repository: [https://github.com/martin-majlis/Wikipedia-API](https://github.com/martin-majlis/Wikipedia-API)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyVis Network: *Interactive Network Visualization in Python*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hogan, A., Blomqvist, E., Cochez, M., et al. *Knowledge Graphs*. `arXiv:2003.02320`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
