<html><head></head><body>
		<div><h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor015"/><st c="0">1</st></h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/><st c="2">What Is Retrieval-Augmented Generation (RAG)</st></h1>
			<p><st c="46">The field of </st><strong class="bold"><st c="60">artificial intelligence</st></strong><st c="83"> (</st><strong class="bold"><st c="85">AI</st></strong><st c="87">) is </st><a id="_idIndexMarker000"/><st c="93">rapidly evolving. </st><st c="111">At the center of it all is </st><strong class="bold"><st c="138">generative AI</st></strong><st c="151">. At the </st><a id="_idIndexMarker001"/><st c="160">center of generative AI is </st><strong class="bold"><st c="187">retrieval-augmented generation</st></strong><st c="217"> (</st><strong class="bold"><st c="219">RAG</st></strong><st c="222">). </st><st c="226">RAG </st><a id="_idIndexMarker002"/><st c="230">is emerging as a significant addition to the generative AI toolkit, harnessing the intelligence and text generation capabilities of </st><strong class="bold"><st c="362">large language models</st></strong><st c="383"> (</st><strong class="bold"><st c="385">LLMs</st></strong><st c="389">) and</st><a id="_idIndexMarker003"/><st c="395"> integrating them with a company’s internal data. </st><st c="445">This offers a method to enhance organizational operations significantly. </st><st c="518">This book focuses on numerous aspects of RAG, examining its role in augmenting the capabilities of LLMs and leveraging internal corporate data for </st><st c="665">strategic advantage.</st></p>
			<p><st c="685">As this book progresses, we will outline the potential of RAG in the enterprise, suggesting how it can make AI applications more responsive and smarter, aligning them with your organizational objectives. </st><st c="890">RAG is well-positioned to become a key facilitator of customized, efficient, and insightful AI solutions, bridging the gap between generative AI’s potential and your specific business needs. </st><st c="1081">Our exploration of RAG will encourage you to unlock the full potential of your corporate data, paving the way for you to enter the era of </st><st c="1219">AI-driven innovation.</st></p>
			<p><st c="1240">In this chapter, we will cover the </st><st c="1276">following topics:</st></p>
			<ul>
				<li><st c="1293">The basics of RAG and how it combines LLMs with a company’s </st><st c="1354">private data</st></li>
				<li><st c="1366">The key advantages of RAG, such as improved accuracy, customization, </st><st c="1436">and flexibility</st></li>
				<li><st c="1451">The challenges and limitations of RAG, including data quality and </st><st c="1518">computational complexity</st></li>
				<li><st c="1542">Important RAG vocabulary terms, with an emphasis on vectors </st><st c="1603">and embeddings</st></li>
				<li><st c="1617">Real-world examples of RAG applications across </st><st c="1665">various industries</st></li>
				<li><st c="1683">How RAG differs from conventional generative AI and </st><st c="1736">model fine-tuning</st></li>
				<li><st c="1753">The overall architecture and stages of a RAG system from user and </st><st c="1820">technical perspectives</st></li>
			</ul>
			<p><st c="1842">By the end of this chapter, you will have a solid foundation in the core RAG concepts and understand the immense potential it offers organizations so that they can extract more value from their data and empower their LLMs. </st><st c="2066">Let’s </st><st c="2072">get started!</st></p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/><st c="2084">Understanding RAG – Basics and principles</st></h1>
			<p><st c="2126">Modern-day LLMs are </st><a id="_idIndexMarker004"/><st c="2147">impressive, but they have never seen your company’s private data (hopefully!). </st><st c="2226">This means the ability of an LLM to help your </st><a id="_idIndexMarker005"/><st c="2272">company fully utilize its data is very limited. </st><st c="2320">This very large barrier has given rise to the concept of RAG, where you are using the power and capabilities of the LLM but combining it with the knowledge and data contained within your company’s internal data repositories. </st><st c="2545">This is the primary motivation for using RAG: to make new data available to the LLM and significantly increase the value you can extract from </st><st c="2687">that data.</st></p>
			<p><st c="2697">Beyond internal data, RAG is</st><a id="_idIndexMarker006"/><st c="2726"> also useful in cases where the LLM has not been trained on the data, even if it is public, such as the most recent research papers or articles about a topic that is strategic to your company. </st><st c="2919">In both cases, we are talking about data that was not present during the training of the LLM. </st><st c="3013">You can have the latest LLM trained on the most tokens ever, but if that data was not present for training, then the LLM will be at a disadvantage in helping you reach your </st><st c="3186">full productivity.</st></p>
			<p><st c="3204">Ultimately, this highlights the fact that, for most organizations, it is a central need to connect new data to an LLM. </st><st c="3324">RAG is the most popular paradigm for doing this. </st><st c="3373">This book focuses on showing you how to set up a RAG application with your data, as well as how to get the most out of it in various situations. </st><st c="3518">We intend to give you an in-depth understanding of RAG and its importance in leveraging an LLM within the context of a company’s private or specific </st><st c="3667">data needs.</st></p>
			<p><st c="3678">Now that you understand the basic motivations behind implementing RAG, let’s review some of the advantages of </st><st c="3789">using it.</st></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/><st c="3798">Advantages of RAG</st></h2>
			<p><st c="3816">Some of the potential </st><a id="_idIndexMarker007"/><st c="3839">advantages of using RAG include improved accuracy and relevance, customization, flexibility, and expanding the model’s knowledge beyond the training data. </st><st c="3994">Let’s take a </st><st c="4007">closer look:</st></p>
			<ul>
				<li><strong class="bold"><st c="4019">Improved accuracy and relevance</st></strong><st c="4051">: RAG can significantly enhance the accuracy and relevance of responses that are generated by LLMs. </st><st c="4152">RAG fetches and incorporates specific information from a database or dataset, typically in real time, and ensures that the output is based on both the model’s pre-existing knowledge and the most current and relevant data that you are </st><st c="4386">providing directly.</st></li>
				<li><strong class="bold"><st c="4405">Customization</st></strong><st c="4419">: RAG allows you to customize and adapt the model’s knowledge to your specific domain or use case. </st><st c="4519">By pointing RAG to databases or datasets directly relevant to your application, you can tailor the model’s outputs so that they align closely with the information and style that matters most for your specific needs. </st><st c="4735">This customization enables the model to provide more targeted and </st><st c="4801">useful responses.</st></li>
				<li><strong class="bold"><st c="4818">Flexibility</st></strong><st c="4830">: RAG provides flexibility in terms of the data sources that the model can access. </st><st c="4914">You can apply RAG to various structured and unstructured data, including databases, web pages, documents, and more. </st><st c="5030">This flexibility allows you to leverage diverse information sources and combine them in novel ways to enhance the model’s capabilities. </st><st c="5166">Additionally, you can update or swap out the data sources as needed, enabling the model to adapt to changing </st><st c="5275">information landscapes.</st></li>
				<li><strong class="bold"><st c="5298">Expanding model knowledge beyond training data</st></strong><st c="5345">: LLMs are limited by the scope of their training data. </st><st c="5402">RAG overcomes this limitation by enabling models to access and utilize information that was not included in their initial training sets. </st><st c="5539">This effectively expands the knowledge base of the model without the need for retraining, making LLMs more versatile and adaptable to new domains or rapidly </st><st c="5696">evolving topics.</st></li>
				<li><strong class="bold"><st c="5712">Removing hallucinations</st></strong><st c="5736">: The LLM is a key component within the RAG system. </st><st c="5789">LLMs have the potential to provide wrong information, also known as hallucinations. </st><st c="5873">These</st><a id="_idIndexMarker008"/><st c="5878"> hallucinations can manifest in several ways, such as made-up facts, incorrect facts, or even nonsensical verbiage. </st><st c="5994">Often, the hallucination is worded in a way that can be very convincing, causing it to be difficult to identify. </st><st c="6107">A well-designed RAG application can remove hallucinations much more easily than when directly using </st><st c="6207">an LLM.</st></li>
			</ul>
			<p><st c="6214">With that, we’ve covered the key advantages of implementing RAG in your organization. </st><st c="6301">Next, let’s discuss some of the challenges you </st><st c="6348">might face.</st></p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/><st c="6359">Challenges of RAG</st></h2>
			<p><st c="6377">There are </st><a id="_idIndexMarker009"/><st c="6388">some challenges to using RAG as well, which include dependency on the quality of the internal data, the need for data manipulation and cleaning, computational overhead, more complex integrations, and the potential for information overload. </st><st c="6628">Let’s review these challenges and gain a better understanding of how they impact RAG pipelines and what can be done </st><st c="6744">about them:</st></p>
			<ul>
				<li><strong class="bold"><st c="6755">Dependency on data quality</st></strong><st c="6782">: When talking about how data can impact an AI model, the saying in data science circles is </st><em class="italic"><st c="6875">garbage in, garbage out.</st></em><st c="6899"> This means that if you give a model bad data, it will give you bad results. </st><st c="6976">RAG is no different. </st><st c="6997">The effectiveness of RAG is directly tied to the quality of the data it retrieves. </st><st c="7080">If the underlying database or dataset contains outdated, biased, or inaccurate information, the outputs generated by RAG will likely suffer from the </st><st c="7229">same issues.</st></li>
				<li><strong class="bold"><st c="7241">Need for data manipulation and cleaning</st></strong><st c="7281">: Data in the recesses of the company often has a lot of value to it, but it is not often in good, accessible shape. </st><st c="7399">For example, data from PDF-based customer statements needs a lot of massaging so that it can be put into a format that can be useful to a </st><st c="7537">RAG pipeline.</st></li>
				<li><strong class="bold"><st c="7550">Computational overhead</st></strong><st c="7573">: A RAG pipeline introduces a host of new computational steps into the response generation process, including data retrieval, processing, and integration. </st><st c="7729">LLMs are getting faster every day, but even the fastest </st><a id="_idIndexMarker010"/><st c="7785">response can be more than a second, and some can take several seconds. </st><st c="7856">If you combine that with other data processing steps, and possibly multiple LLM calls, the result can be a very significant increase in the time it takes to receive a response. </st><st c="8033">This all leads to increased computational overhead, affecting the efficiency and scalability of the entire system. </st><st c="8148">As with any other IT initiative, an organization must balance the benefits of enhanced accuracy and customization against the resource requirements and potential latency introduced by these </st><st c="8338">additional processes.</st></li>
				<li><strong class="bold"><st c="8359">Data storage explosion; complexity in integration and maintenance</st></strong><st c="8425">: Traditionally, your data resides in a data source that’s queried in various ways to be made available to your internal and external systems. </st><st c="8569">But with RAG, your data resides in multiple forms and locations, such as vectors in a vector database, that represent the same data, but in a different format. </st><st c="8729">Add in the complexity of connecting these various data sources to LLMs and relevant technical mechanisms such as vector searches and you have a significant increase in complexity. </st><st c="8909">This increased complexity can be resource-intensive. </st><st c="8962">Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. </st><st c="9083">Organizations need to invest in technical expertise and infrastructure to leverage RAG capabilities effectively while accounting for the rapid increase in complexities these systems bring </st><st c="9271">with them.</st></li>
				<li><strong class="bold"><st c="9281">Potential for information overload</st></strong><st c="9316">: RAG-based systems can pull in too much information. </st><st c="9371">It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found. </st><st c="9513">Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. </st><st c="9671">Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally </st><st c="9786">relevant details.</st></li>
				<li><strong class="bold"><st c="9803">Hallucinations</st></strong><st c="9818">: While we listed removing hallucinations as an advantage of using RAG, hallucinations do pose one of the biggest challenges to RAG pipelines if they’re not dealt with properly. </st><st c="9997">A well-designed RAG application must take measures to</st><a id="_idIndexMarker011"/><st c="10050"> identify and remove hallucinations and undergo significant testing before the final output text is provided to the </st><st c="10166">end user.</st></li>
				<li><strong class="bold"><st c="10175">High levels of complexity within RAG components</st></strong><st c="10223">: A typical RAG application tends to have a high level of complexity, with many components that need to be optimized for the overall application to function properly. </st><st c="10391">The components can interact with each other in several ways, often with many more steps than the basic RAG pipeline you start with. </st><st c="10523">Every component within the pipeline needs significant amounts of trials and testing, including your prompt design and engineering, the LLMs you use and how you use them, the various algorithms and their parameters for retrieval, the interface you use to access your RAG application, and numerous other aspects that you will need to add over the course of </st><st c="10878">your development.</st></li>
			</ul>
			<p><st c="10895">In this section, we explored the key advantages of implementing RAG in your organization, including improved accuracy and relevance, customization, flexibility, and the ability to expand the model’s knowledge beyond its initial training data. </st><st c="11139">We also discussed some of the challenges you might face when deploying RAG, such as dependency on data quality, the need for data manipulation and cleaning, increased computational overhead, complexity in integration and maintenance, and the potential for information overload. </st><st c="11417">Understanding these benefits and challenges provides a foundation for diving deeper into the core concepts and vocabulary used in </st><st c="11547">RAG systems.</st></p>
			<p><st c="11559">To understand the approaches we will introduce, you will need a good understanding of the vocabulary used to discuss these approaches. </st><st c="11695">In the following section, we will familiarize ourselves with some of the foundational concepts so that you can better understand the various components and techniques involved in building effective </st><st c="11893">RAG pipelines.</st></p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/><st c="11907">RAG vocabulary</st></h1>
			<p><st c="11922">Now is as </st><a id="_idIndexMarker012"/><st c="11933">good a time as any to review some vocabulary that should help you become familiar with the various concepts in RAG. </st><st c="12049">In the following subsections, we will familiarize ourselves with some of this vocabulary, including LLMs, prompting concepts, inference, context windows, fine-tuning approaches, vector databases, and vectors/embeddings. </st><st c="12269">This is not an exhaustive list, but understanding these core concepts should help you understand everything else we will teach you about RAG in a more </st><st c="12420">effective way.</st></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/><st c="12434">LLM</st></h2>
			<p><st c="12438">Most of this book</st><a id="_idIndexMarker013"/><st c="12456"> will deal with LLMs. </st><st c="12478">LLMs are generative AI technologies that focus on generating text. </st><st c="12545">We will keep things simple by concentrating on the type of model that most RAG pipelines use, the LLM. </st><st c="12648">However, we would like to clarify that while we will focus primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images, audio, and videos. </st><st c="12832">We will focus on these other types of models and how they are used in RAG in </st><a href="B22475_14.xhtml#_idTextAnchor283"><em class="italic"><st c="12909">Chapter 14</st></em></a><st c="12919">.</st></p>
			<p><st c="12920">Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta Llama models, Google’s Gemini models, and Anthropic’s </st><st c="13045">Claude models.</st></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/><st c="13059">Prompting, prompt design, and prompt engineering</st></h2>
			<p><st c="13108">These terms are sometimes used interchangeably, but technically, while they all have to do with prompting, they do have </st><st c="13229">different meanings:</st></p>
			<ul>
				<li><strong class="bold"><st c="13248">Prompting</st></strong><st c="13258"> is the</st><a id="_idIndexMarker014"/><st c="13265"> act </st><a id="_idIndexMarker015"/><st c="13270">of sending a query or </st><em class="italic"><st c="13292">prompt</st></em><st c="13298"> to </st><st c="13302">an LLM.</st></li>
				<li><strong class="bold"><st c="13309">Prompt design</st></strong><st c="13323"> refers</st><a id="_idIndexMarker016"/><st c="13330"> to the strategy you implement to </st><em class="italic"><st c="13364">design</st></em><st c="13370"> the</st><a id="_idIndexMarker017"/><st c="13374"> prompt you will send to the LLM. </st><st c="13408">Many different prompt design strategies work in different scenarios. </st><st c="13477">We will review many of these in </st><a href="B22475_13.xhtml#_idTextAnchor256"><em class="italic"><st c="13509">Chapter 13</st></em></a><st c="13519">.</st></li>
				<li><strong class="bold"><st c="13520">Prompt engineering</st></strong><st c="13539"> focuses </st><a id="_idIndexMarker018"/><st c="13548">more on the technical aspects</st><a id="_idIndexMarker019"/><st c="13577"> surrounding the prompt that you use to improve the outputs from the LLM. </st><st c="13651">For example, you may break up a complex query into two or three different LLM interactions, </st><em class="italic"><st c="13743">engineering</st></em><st c="13754"> it better to achieve superior results. </st><st c="13794">We will also review prompt engineering in </st><a href="B22475_13.xhtml#_idTextAnchor256"><em class="italic"><st c="13836">Chapter 13</st></em></a><st c="13846">.</st></li>
			</ul>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/><st c="13847">LangChain and LlamaIndex</st></h2>
			<p><st c="13872">This book </st><a id="_idIndexMarker020"/><st c="13883">will focus on using LangChain as the framework for building our RAG pipelines. </st><st c="13962">LangChain is an open source framework</st><a id="_idIndexMarker021"/><st c="13999"> that supports not just RAG but any development that wants to use LLMs within a pipeline approach. </st><st c="14098">With over 15 million monthly downloads, LangChain is the most popular generative AI development framework. </st><st c="14205">It supports RAG particularly well, providing a modular and flexible set of tools that make RAG development significantly more efficient than not using </st><st c="14356">a framework.</st></p>
			<p><st c="14368">While LangChain is currently the most popular framework for developing RAG pipelines, LlamaIndex</st><a id="_idIndexMarker022"/><st c="14465"> is a </st><a id="_idIndexMarker023"/><st c="14471">leading alternative to LangChain, with similar capabilities in general. </st><st c="14543">LlamaIndex is known for its focus on search and retrieval tasks and may be a good option if you require advanced search or need to handle </st><st c="14681">large datasets.</st></p>
			<p><st c="14696">Many other options focus on various niches. </st><st c="14741">Once you have gotten familiar with building RAG pipelines, be sure to look at some of the other options to see if there are frameworks that work for your particular </st><st c="14906">project better.</st></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/><st c="14921">Inference</st></h2>
			<p><st c="14931">We will use</st><a id="_idIndexMarker024"/><st c="14943"> the term </st><strong class="bold"><st c="14953">inference</st></strong><st c="14962"> from time to time. </st><st c="14982">Generally, this refers to the process of the LLM generating outputs or predictions based on given inputs using a pre-trained language model. </st><st c="15123">For example, when you ask ChatGPT a question, the steps it takes to provide you with a response is </st><st c="15222">called inference.</st></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/><st c="15239">Context window</st></h2>
			<p><st c="15254">A</st><a id="_idIndexMarker025"/><st c="15256"> context window, in</st><a id="_idIndexMarker026"/><st c="15275"> the context of LLMs, refers to the maximum number of tokens (words, sub-words, or characters) that the model can process in a single pass. </st><st c="15415">It determines the amount of text the model can </st><em class="italic"><st c="15462">see</st></em><st c="15465"> or </st><em class="italic"><st c="15469">attend to</st></em><st c="15478"> at once when making predictions or </st><st c="15514">generating responses.</st></p>
			<p><st c="15535">The context window size is a key parameter of the model architecture and is typically fixed during model training. </st><st c="15651">It directly relates to the input size of the model as it sets an upper limit on the number of tokens that can be fed into the model at </st><st c="15786">a time.</st></p>
			<p><st c="15793">For example, if a model </st><a id="_idIndexMarker027"/><st c="15818">has a context window size of 4,096 tokens, it means that the model can process and generate sequences of up to 4,096 tokens. </st><st c="15943">When processing longer texts, such as documents or conversations, the input needs to be divided into smaller segments that fit within the context window. </st><st c="16097">This is often done using techniques such as sliding windows </st><st c="16157">or truncation.</st></p>
			<p><st c="16171">The size of the </st><a id="_idIndexMarker028"/><st c="16188">context window has implications for the model’s ability to understand and maintain long-range dependencies and context. </st><st c="16308">Models with larger context windows can capture and utilize more contextual information when generating responses, which can lead to more coherent and contextually relevant outputs. </st><st c="16489">However, increasing the context window size also increases the computational resources required to train and run </st><st c="16602">the model.</st></p>
			<p><st c="16612">In the context of RAG, the context window size is essential because it determines how much information from the retrieved documents can be effectively utilized by the model when generating the final response. </st><st c="16822">Recent advancements in language models have led to the development of models with significantly larger context windows, enabling them to process and retain more information from the retrieved sources. </st><st c="17023">See </st><em class="italic"><st c="17027">Table 1.1</st></em><st c="17036"> to see the context windows of many popular LLMs, both closed and </st><st c="17102">open sourced:</st></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="17115">LLM</st></strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold"><st c="17119">Context </st></strong><strong class="bold"><st c="17128">Window (Tokens)</st></strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17143">ChatGPT-3.5 Turbo </st><st c="17162">0613 (OpenAI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17175">4,096</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17181">Llama </st><st c="17188">2 (Meta)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17196">4,096</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17202">Llama </st><st c="17209">3 (Meta)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17217">8,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17223">ChatGPT-4 (OpenAI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17242">8,192</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17248">ChatGPT-3.5 Turbo </st><st c="17267">0125 (OpenAI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17280">16,385</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17287">ChatGPT-4.0-32k (OpenAI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17312">32,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17319">Mistral (</st><st c="17329">Mistral AI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17341">32,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17348">Mixtral (</st><st c="17358">Mistral AI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17370">32,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17377">DBRX (Databricks)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17395">32,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17402">Gemini 1.0 </st><st c="17414">Pro (Google)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17426">32,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17433">ChatGPT-4.0 </st><st c="17446">Turbo (OpenAI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17460">128,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17468">ChatGPT-4o (OpenAI)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17488">128,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17496">Claude </st><st c="17504">2.1 (Anthropic)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17519">200,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17527">Claude </st><st c="17535">3 (Anthropic)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17548">200,000</st></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><st c="17556">Gemini 1.5 </st><st c="17568">Pro (Google)</st></p>
						</td>
						<td class="No-Table-Style">
							<p><st c="17580">1,000,000</st></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17590">Table 1.1 – Different context windows for LLMs</st></p>
			<p><em class="italic"><st c="17637">Figure 1</st></em><em class="italic"><st c="17646">.1</st></em><st c="17648">, which is</st><a id="_idIndexMarker029"/><st c="17658"> based on </st><em class="italic"><st c="17668">Table 1.1</st></em><st c="17677">, shows that Gemini 1.5 Pro is far</st><a id="_idIndexMarker030"/><st c="17711"> larger than </st><st c="17724">the others.</st></p>
			<div><div><img src="img/B22475_01_01.jpg" alt="Figure 1.1 – Different context windows for LLMs"/><st c="17735"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="17828">Figure 1.1 – Different context windows for LLMs</st></p>
			<p><st c="17875">Note that </st><em class="italic"><st c="17886">Figure 1</st></em><em class="italic"><st c="17894">.1</st></em><st c="17896"> shows models that have generally aged from right to left, meaning the older models tended to have smaller context windows, with the newest models having larger context windows. </st><st c="18074">This trend is likely to continue, pushing the typical context window larger as </st><st c="18153">time progresses.</st></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/><st c="18169">Fine-tuning – full-model fine-tuning (FMFT) and parameter-efficient fine-tuning (PEFT)</st></h2>
			<p><st c="18256">FMFT is </st><a id="_idIndexMarker031"/><st c="18265">where you take a</st><a id="_idIndexMarker032"/><st c="18281"> foundation model and train it further to gain new capabilities. </st><st c="18346">You could simply give it new knowledge for a specific domain, or you could give it a skill, such as being a conversational chatbot. </st><st c="18478">FMFT updates all the parameters and biases in </st><st c="18524">the model.</st></p>
			<p><st c="18534">PEFT, on </st><a id="_idIndexMarker033"/><st c="18544">the </st><a id="_idIndexMarker034"/><st c="18548">other hand, is a type of fine-tuning where you focus only on specific parts of the parameters or biases when you fine-tune the model, but with a similar goal as general fine-tuning. </st><st c="18730">The latest research in this area shows that you can achieve similar results to FMFT with far less cost, time commitment, </st><st c="18851">and data.</st></p>
			<p><st c="18860">While this book does not focus on fine-tuning, it is a very valid strategy to try to use a model fine-tuned with your data to give it more knowledge from your domain or to give it more of a </st><em class="italic"><st c="19051">voice</st></em><st c="19056"> from your domain. </st><st c="19075">For example, you could train it to talk more like a scientist than a generic foundation model, if you’re using this in a scientific field. </st><st c="19214">Alternatively, if you are developing in a legal field, you may want it to sound more like </st><st c="19304">a lawyer.</st></p>
			<p><st c="19313">Fine-tuning also helps the LLM to understand your company’s data better, making it better at generating an effective response during the RAG process. </st><st c="19464">For example, if you have a scientific company, you might fine-tune a model with scientific information and use it for a RAG application that summarizes your research. </st><st c="19631">This may improve your RAG application’s output (the summaries of your research) because your fine-tuned model understands your data better and can provide a more </st><st c="19793">effective summary.</st></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/><st c="19811">Vector store or vector database?</st></h2>
			<p><st c="19844">Both! </st><st c="19851">All </st><a id="_idIndexMarker035"/><st c="19855">vector databases</st><a id="_idIndexMarker036"/><st c="19871"> are vector stores, but not all vector </st><a id="_idIndexMarker037"/><st c="19910">stores are vector databases. </st><st c="19939">OK, while you get out your chalkboard to draw a Vinn diagram, I will continue to explain </st><st c="20028">this statement.</st></p>
			<p><st c="20043">There are ways to store vectors that are not full databases. </st><st c="20105">They are simply storage devices for vectors. </st><st c="20150">So, to</st><a id="_idIndexMarker038"/><st c="20156"> encompass all possible ways to store vectors, LangChain calls them all </st><strong class="bold"><st c="20228">vector stores</st></strong><st c="20241">. Let’s do</st><a id="_idIndexMarker039"/><st c="20251"> the same! </st><st c="20262">Just know that not all the </st><em class="italic"><st c="20289">vector stores</st></em><st c="20302"> that LangChain connects with are officially considered vector databases, but in general, most of them are and many people refer to all of them as vector databases, even when they are not technically full databases from a functionality standpoint. </st><st c="20550">Phew – glad we cleared </st><st c="20573">that up!</st></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/><st c="20581">Vectors, vectors, vectors!</st></h2>
			<p><st c="20608">A vector is</st><a id="_idIndexMarker040"/><st c="20620"> a mathematical representation of your data. </st><st c="20665">They are often referred to as embeddings when talking specifically about </st><strong class="bold"><st c="20738">natural language processing</st></strong><st c="20765"> (</st><strong class="bold"><st c="20767">NLP</st></strong><st c="20770">) and </st><a id="_idIndexMarker041"/><st c="20777">LLMs. </st><st c="20783">Vectors are one of the most important concepts to understand and there are many different parts of a RAG pipeline that </st><st c="20902">utilize vectors.</st></p>
			<p><st c="20918">We just covered many key vocabulary terms that will be important for you to understand the rest of this book. </st><st c="21029">Many of these concepts will be expanded upon in future chapters. </st><st c="21094">In the next section, we will continue to discuss vectors in further depth. </st><st c="21169">And beyond that, we will spend </st><em class="italic"><st c="21200">Chapters 7</st></em><st c="21210"> and </st><em class="italic"><st c="21215">8</st></em><st c="21216"> going over vectors and how they are used to find </st><st c="21266">similar content.</st></p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/><st c="21282">Vectors</st></h1>
			<p><st c="21290">It could be argued</st><a id="_idIndexMarker042"/><st c="21309"> that understanding vectors and all the ways they are used in RAG is the most important part of this entire book. </st><st c="21423">As mentioned previously, vectors are simply the mathematical representations of your external data, and they are often referred to as embeddings. </st><st c="21569">These representations capture semantic information in a format that can be processed by algorithms, facilitating tasks such as similarity search, which is a crucial step in the </st><st c="21746">RAG process.</st></p>
			<p><st c="21758">Vectors typically have a specific dimension based on how many numbers are represented by them. </st><st c="21854">For example, this is a </st><st c="21877">four-dimensional vector:</st></p>
			<pre class="source-code"><st c="21901">
[0.123, 0.321, 0.312, 0.231]</st></pre>
			<p><st c="21930">If you didn’t know</st><a id="_idIndexMarker043"/><st c="21949"> we were talking about vectors and you saw this in Python code, you might recognize this as a list of four floating points, and you aren’t too far off. </st><st c="22101">However, when working with vectors in Python, you want to recognize them as a NumPy array, rather than lists. </st><st c="22211">NumPy arrays are generally more machine-learning-friendly because they are optimized to be processed much faster and more efficiently than Python lists, and they are more broadly recognized as the de facto representation of embeddings across machine learning packages such as SciPy, pandas, scikit-learn, TensorFlow, Keras, Pytorch, and many others. </st><st c="22561">NumPy also enables you to perform vectorized math directly on the NumPy array, such as performing element-wise operations, without having to code in loops and other approaches you might have to use if you were using a different type </st><st c="22794">of sequence.</st></p>
			<p><st c="22806">When working with vectors for vectorization, there are often hundreds or thousands of dimensions, which refers to the number of floating points present in the vector. </st><st c="22974">Higher dimensionality can capture more detailed semantic information, which is crucial for accurately matching query inputs with relevant documents or data in </st><st c="23133">RAG applications.</st></p>
			<p><st c="23150">In </st><a href="B22475_07.xhtml#_idTextAnchor122"><em class="italic"><st c="23154">Chapter 7</st></em></a><st c="23163">, we will cover the key role vectors and vector databases play in RAG implementation. </st><st c="23249">Then, in </st><a href="B22475_08.xhtml#_idTextAnchor152"><em class="italic"><st c="23258">Chapter 8</st></em></a><st c="23267">, we will dive more into the concept of similarity searches, which utilize vectors to search much faster and more efficiently. </st><st c="23394">These are key concepts that will help you gain a much deeper understanding of how to better implement a </st><st c="23498">RAG pipeline.</st></p>
			<p><st c="23511">Understanding vectors can be a crucial underlying concept to understand how to implement RAG, but how is RAG used in practical applications in the enterprise? </st><st c="23671">We will discuss these practical AI applications of RAG in the </st><st c="23733">next section.</st></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/><st c="23746">Implementing RAG in AI applications</st></h1>
			<p><st c="23782">RAG is</st><a id="_idIndexMarker044"/><st c="23789"> rapidly becoming a cornerstone of generative AI platforms in the corporate world. </st><st c="23872">RAG combines the power of retrieving internal or </st><em class="italic"><st c="23921">new</st></em><st c="23924"> data with generative language models to enhance the quality and relevance of the generated text. </st><st c="24022">This technique can be particularly useful for companies across various industries to improve their products, services, and operational efficiencies. </st><st c="24171">The following are some examples of how RAG can </st><st c="24218">be used:</st></p>
			<ul>
				<li><strong class="bold"><st c="24226">Customer support and chatbots</st></strong><st c="24256">: These can exist without RAG, but when integrated with </st><a id="_idIndexMarker045"/><st c="24313">RAG, it can connect those chatbots with past customer interactions, FAQs, support documents, and anything else that was specific to </st><st c="24445">that customer.</st></li>
				<li><strong class="bold"><st c="24459">Technical support</st></strong><st c="24477">: With better access to customer history and information, RAG-enhanced chatbots can provide a significant improvement to current technical </st><st c="24617">support chatbots.</st></li>
				<li><strong class="bold"><st c="24634">Automated reporting</st></strong><st c="24654">: RAG can assist in creating initial drafts or summarizing existing articles, research papers, and other types of unstructured data into more </st><st c="24797">digestible formats.</st></li>
				<li><strong class="bold"><st c="24816">E-commerce support</st></strong><st c="24835">: For e-commerce companies, RAG can help generate dynamic product descriptions and user content, as well as make better </st><st c="24956">product recommendations.</st></li>
				<li><strong class="bold"><st c="24980">Utilizing knowledge bases</st></strong><st c="25006">: RAG improves the searchability and utility of both internal and general knowledge bases by generating summaries, providing direct answers to queries, and retrieving relevant information across various domains such as legal, compliance, research, medical, academia, patents, and </st><st c="25287">technical documents.</st></li>
				<li><strong class="bold"><st c="25307">Innovation scouting</st></strong><st c="25327">: This is like searching general knowledge bases but with a focus on innovation. </st><st c="25409">With this, companies can use RAG to scan and summarize information from quality sources to identify trends and potential areas for innovations that are relevant to that </st><st c="25578">company’s specialization.</st></li>
				<li><strong class="bold"><st c="25603">Training and education</st></strong><st c="25626">: RAG can be used by education organizations and corporate training programs to generate or customize learning materials based on specific needs and knowledge levels of the learners. </st><st c="25810">With RAG, a much deeper level of internal knowledge from the organization can be incorporated into the educational curriculum in very customized ways to the individual </st><st c="25978">or role.</st></li>
			</ul>
			<p><st c="25986">These are just a</st><a id="_idIndexMarker046"/><st c="26003"> few of the ways organizations are using RAG right now to improve their operations. </st><st c="26087">We will dive into each of these areas in more depth in </st><a href="B22475_03.xhtml#_idTextAnchor056"><em class="italic"><st c="26142">Chapter 3</st></em></a><st c="26151">, helping you understand how you can implement all these game-changing initiatives in multiple places in </st><st c="26256">your company.</st></p>
			<p><st c="26269">You might be wondering, “</st><em class="italic"><st c="26295">If I am using an LLM such as ChatGPT to answer my questions in my company, does that mean my company is using </st></em><em class="italic"><st c="26406">RAG already?</st></em><st c="26418">”</st></p>
			<p><st c="26420">The answer </st><st c="26431">is “</st><em class="italic"><st c="26435">No.</st></em><st c="26439">”</st></p>
			<p><st c="26441">If you just log in to ChatGPT and ask questions, that is not the same as implementing RAG. </st><st c="26532">Both ChatGPT and RAG are forms of generative AI, and they are sometimes used together, but they are two different concepts. </st><st c="26656">In the next section, we will discuss the differences between generative AI </st><st c="26731">and RAG.</st></p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/><st c="26739">Comparing RAG with conventional generative AI</st></h1>
			<p><st c="26785">Conventional generative AI has </st><a id="_idIndexMarker047"/><st c="26817">already</st><a id="_idIndexMarker048"/><st c="26824"> shown to be a revolutionary change for companies, helping their employees reach new levels of productivity. </st><st c="26933">LLMs such as ChatGPT are assisting users with a rapidly growing list of applications that include writing business plans, writing and improving code, writing marketing copy, and even providing healthier recipes for a specific diet. </st><st c="27165">Ultimately, much of what users are doing is getting </st><st c="27217">done faster.</st></p>
			<p><st c="27229">However, conventional generative AI</st><a id="_idIndexMarker049"/><st c="27265"> does not know what it does not know. </st><st c="27303">And that includes most of the internal data in your company. </st><st c="27364">Can you imagine what you could do with all the benefits mentioned previously, but combined with all the data within your company – about everything your company has ever done, about your customers and all their interactions, or about all your products and services combined with a knowledge of what a specific customer’s needs are? </st><st c="27696">You do not have to imagine it – that is what </st><st c="27741">RAG does!</st></p>
			<p><st c="27750">Before RAG, most of the services you saw that connected customers or employees with the data resources of the company were just scratching the surface of what is possible compared to if they could access </st><em class="italic"><st c="27955">all</st></em><st c="27958"> the data in the company. </st><st c="27984">With the advent of RAG and generative AI in general, corporations are on the precipice of something really, </st><st c="28092">really big.</st></p>
			<p><st c="28103">Another area you might confuse RAG with is the concept of fine-tuning a model. </st><st c="28183">Let’s discuss what the differences are between these types </st><st c="28242">of approaches.</st></p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/><st c="28256">Comparing RAG with model fine-tuning</st></h1>
			<p><st c="28293">LLMs can be</st><a id="_idIndexMarker050"/><st c="28305"> adapted to your </st><a id="_idIndexMarker051"/><st c="28322">data in </st><st c="28330">two ways:</st></p>
			<ul>
				<li><strong class="bold"><st c="28339">Fine-tuning</st></strong><st c="28351">: With fine-tuning, you are adjusting the weights and/or biases that define the model’s intelligence based on new training data. </st><st c="28481">This directly impacts the model, permanently changing how it will interact with </st><st c="28561">new inputs.</st></li>
				<li><strong class="bold"><st c="28572">Input/prompts</st></strong><st c="28586">: This is where you </st><em class="italic"><st c="28607">use</st></em><st c="28610"> the model, using the prompt/input to introduce new knowledge that the LLM can </st><st c="28689">act upon.</st></li>
			</ul>
			<p><st c="28698">Why not use fine-tuning in all situations? </st><st c="28742">Once you have introduced the new knowledge, the LLM will always have it! </st><st c="28815">It is also how the model was created – by being trained with data, right? </st><st c="28889">That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (such as teaching a model how to converse in a certain way), and less reliable for </st><st c="29091">factual recall.</st></p>
			<p><st c="29106">The reason is complicated, but in general, a model’s knowledge of facts is like a human’s long-term memory. </st><st c="29215">If you memorize a long passage from a speech or book and then try to recall it a few months later, you will likely still understand the context of the information, but you may forget specific details. </st><st c="29416">On the other hand, adding knowledge through the input of the model is like our short-term memory, where the facts, details, and even the order of wording are all very fresh and available for recall. </st><st c="29615">It is this latter scenario that lends itself better in a situation where you want successful factual recall. </st><st c="29724">And given how much more expensive fine-tuning can be, this makes it that much more important to </st><st c="29820">consider RAG.</st></p>
			<p><st c="29833">There is a trade-off, though. </st><st c="29864">While there are generally ways to feed all data you have to a model for fine-tuning, inputs are limited by the context window of the model. </st><st c="30004">This is an area that is actively being addressed. </st><st c="30054">For example, early versions of ChatGPT 3.5 had a 4,096 token context window, which is the equivalent of about five pages of text. </st><st c="30184">When ChatGPT 4 was released, they expanded the context window to 8,192 tokens (10 pages) and there was a Chat 4-32k version that had a context window of 32,768 tokens (40 pages). </st><st c="30363">This issue is so important that they included the context window size in the name of the model. </st><st c="30459">That is a strong indicator of how important the context </st><st c="30515">window is!</st></p>
			<p class="callout-heading"><st c="30525">Interesting fact!</st></p>
			<p class="callout"><st c="30543">What about the latest Gemini 1.5 model? </st><st c="30584">It has a 1 million token context window or over </st><st c="30632">1,000 pages!</st></p>
			<p><st c="30644">As the </st><a id="_idIndexMarker052"/><st c="30652">context windows expand, another issue is created. </st><st c="30702">Early models with expanded context windows were</st><a id="_idIndexMarker053"/><st c="30749"> shown to lose a lot of the details, especially in the </st><em class="italic"><st c="30804">middle</st></em><st c="30810"> of the text. </st><st c="30824">This issue is also being addressed. </st><st c="30860">The Gemini 1.5 model with its 1 million token context window has performed well in tests called </st><em class="italic"><st c="30956">needle in a haystack</st></em><st c="30976"> tests for </st><em class="italic"><st c="30987">remembering</st></em><st c="30998"> all details well throughout the text it can take as input. </st><st c="31058">Unfortunately, the model did not perform as well in the </st><em class="italic"><st c="31114">multiple needles in a haystack</st></em><st c="31144"> tests. </st><st c="31152">Expect more effort in this area as these context windows become larger. </st><st c="31224">Keep this in mind if you need to work with large amounts of text at </st><st c="31292">a time.</st></p>
			<p class="callout-heading"><st c="31299">Note</st></p>
			<p class="callout"><st c="31304">It is important to note that token count differs from word count as tokens include punctuation, symbols, numbers, and other text representations. </st><st c="31451">How a compound word such as </st><em class="italic"><st c="31479">ice cream</st></em><st c="31488"> is treated token-wise depends on the tokenization scheme and it can vary across LLMs. </st><st c="31575">But most well-known LLMs (such as ChatGPT and Gemini) would consider </st><em class="italic"><st c="31644">ice cream</st></em><st c="31653"> as two tokens. </st><st c="31669">Under certain circumstances in NLP, you may argue that it should be one token based on the concept that a token should represent a useful semantic unit for processing, but that is not the case for </st><st c="31866">these models.</st></p>
			<p><st c="31879">Fine-tuning can also be quite expensive, depending on the environment and resources you have available. </st><st c="31984">In recent years, the costs for fine-tuning have come down substantially due to new techniques such as representative fine-tuning, LoRA-related techniques, and quantization. </st><st c="32157">But in many RAG development efforts, fine-tuning is considered an additional cost to already expensive RAG efforts, so it is considered a more expensive addition to </st><st c="32322">the efforts.</st></p>
			<p><st c="32334">Ultimately, when deciding between RAG and fine-tuning, consider your specific use case and requirements. </st><st c="32440">RAG is</st><a id="_idIndexMarker054"/><st c="32446"> generally superior for retrieving factual information that is not present in the LLM’s training data or is private. </st><st c="32563">It allows you to dynamically integrate external knowledge </st><a id="_idIndexMarker055"/><st c="32621">without modifying the model’s weights. </st><st c="32660">Fine-tuning, on the other hand, is more suitable for teaching the model specialized tasks or adapting it to a specific domain. </st><st c="32787">Keep the limitations of context window sizes and the potential for overfitting in mind when fine-tuning a </st><st c="32893">specific dataset.</st></p>
			<p><st c="32910">Now that we have defined what RAG is, particularly when compared to other approaches that use generative AI, let’s review the general architecture of </st><st c="33061">RAG systems.</st></p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/><st c="33073">The architecture of RAG systems</st></h1>
			<p><st c="33105">The following are </st><a id="_idIndexMarker056"/><st c="33124">the stages of a RAG process from a </st><st c="33159">user’s perspective:</st></p>
			<ol>
				<li><st c="33178">A user enters </st><st c="33193">a query/question.</st></li>
				<li><st c="33210">The application thinks for a little while before checking the data it has access to so that it can see what is the </st><st c="33326">most relevant.</st></li>
				<li><st c="33340">The application provides a response that focuses on answering the user’s question, but using data that has been provided to it through the </st><st c="33480">RAG pipeline.</st></li>
			</ol>
			<p><st c="33493">From a technical standpoint, this</st><a id="_idIndexMarker057"/><st c="33527"> captures two of the stages you will code: the </st><strong class="bold"><st c="33574">retrieval</st></strong><st c="33583"> and </st><strong class="bold"><st c="33588">generation</st></strong><st c="33598"> stages. </st><st c="33607">But</st><a id="_idIndexMarker058"/><st c="33610"> there is one other stage, known </st><a id="_idIndexMarker059"/><st c="33643">as </st><strong class="bold"><st c="33646">indexing</st></strong><st c="33654">, which can be and is often executed before the user enters the query. </st><st c="33725">With indexing, you are turning supporting data into vectors, storing them in a vector database, and likely optimizing the search functionality so that the retrieval step is as fast and effective </st><st c="33920">as possible.</st></p>
			<p><st c="33932">Once the user passes their query into the system, the following </st><st c="33997">steps occur:</st></p>
			<ol>
				<li><st c="34009">The user query </st><st c="34025">is vectorized.</st></li>
				<li><st c="34039">The vectorized query is passed to a vector search to retrieve the most relevant data in a vector database representing your </st><st c="34164">external data.</st></li>
				<li><st c="34178">The vector search returns the most relevant results and unique keys referencing the original content those </st><st c="34286">vectors represent.</st></li>
				<li><st c="34304">The unique keys are used to pull out the original data associated with those vectors, often in a batch of </st><st c="34411">multiple documents.</st></li>
				<li><st c="34430">The original data might be filtered or post-processed but will typically then be passed to an LLM based on what you expect the RAG process </st><st c="34570">to do.</st></li>
				<li><st c="34576">The LLM is provided with a prompt that generally says something like “</st><code><st c="34647">You are a helpful assistant for question-answering tasks. </st><st c="34706">Take the following question (the user query) and use this helpful information (the data retrieved in the similarity search) to answer it. </st><st c="34844">If you don't know the answer based on the information provided, just say you </st></code><code><st c="34921">don't know.</st></code><st c="34932">”</st></li>
				<li><st c="34934">The LLM processes that prompt and provides a response based on the external data </st><st c="35015">you provided.</st></li>
			</ol>
			<p><st c="35028">Depending on the scope of the RAG system, these steps can be done in real time, or steps such as </st><a id="_idIndexMarker060"/><st c="35126">indexing can be done before the query so that it is ready to be searched when the </st><st c="35208">time comes.</st></p>
			<p><st c="35219">As mentioned previously, we can break these aspects down into three main stages (see </st><em class="italic"><st c="35305">Figure 1</st></em><em class="italic"><st c="35313">.2</st></em><st c="35315">):</st></p>
			<ul>
				<li><st c="35318">Indexing</st></li>
				<li><st c="35327">Retrieval</st></li>
				<li><st c="35337">Generation</st></li>
			</ul>
			<div><div><img src="img/B22475_01_02.jpg" alt="Figure 1.2 – The three stages of RAG"/><st c="35348"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="35430">Figure 1.2 – The three stages of RAG</st></p>
			<p><st c="35466">As described</st><a id="_idIndexMarker061"/><st c="35479"> previously, these three stages make up the overall user pattern and design of a general RAG system. </st><st c="35580">In </st><a href="B22475_04.xhtml#_idTextAnchor078"><em class="italic"><st c="35583">Chapter 4</st></em></a><st c="35592">, we will dive much deeper into understanding these stages. </st><st c="35652">This will help you tie the concepts of this coding paradigm with their </st><st c="35723">actual implementation.</st></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/><st c="35745">Summary</st></h1>
			<p><st c="35753">In this chapter, we explored RAG and its ability to enhance the capabilities of LLMs by integrating them with an organization’s internal data. </st><st c="35897">We learned how RAG combines the power of LLMs with a company’s private data, enabling the model to utilize information it was not originally trained on, making the LLM’s outputs more relevant and valuable for the specific organization. </st><st c="36133">We also discussed the advantages of RAG, such as improved accuracy and relevance, customization to a company’s domain, flexibility in data sources used, and expansion of the model’s knowledge beyond its original training data. </st><st c="36360">Additionally, we examined the challenges and limitations of RAG, including dependency on data quality, the need for data cleaning, added computational overhead and complexity, and the potential for information overload if not </st><st c="36586">properly filtered.</st></p>
			<p><st c="36604">Midway through this chapter, we defined key vocabulary terms and emphasized the critical importance of understanding vectors. </st><st c="36731">We explored examples of how RAG is being implemented across industries for various applications and compared RAG to conventional generative AI and </st><st c="36878">model fine-tuning.</st></p>
			<p><st c="36896">Finally, we outlined the architecture and stages of a typical RAG pipeline from both the user’s perspective and a technical standpoint while covering the indexing, retrieval, and generation stages of the RAG pipeline. </st><st c="37115">In the next chapter, we will walk through these stages using an actual </st><st c="37186">coding example.</st></p>
		</div>
	<div></body></html>