- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, data is rich with information and has a well-defined structure. If
    you know what you want, then this data is straightforward to work with in a modern
    database system. However, you often don’t know exactly what you need. Without
    specific search terms or phrases, you may not receive optimal search results.
    For example, you might not know the brand or name of your picky pet’s favorite
    food. In such complex cases, traditional information search and retrieval methods
    can fall short.
  prefs: []
  type: TYPE_NORMAL
- en: Modern AI research has given rise to a new class of methods that can encode
    the underlying semantic meaning of something instead of just its raw data. For
    example, AI models can understand that when you ask for `the new action movie
    with that one actor who was also in the movie with green falling numbers`, you’re
    asking for the latest *John Wick* film, which stars Keanu Reeves, who was also
    the star of *The* *Matrix* films.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this result, these methods convert their inputs into a numerical
    format called a **vector embedding**. **Vector databases** provide a means to
    efficiently store, organize, and search these vector representations. This makes
    vector databases valuable tools for retrieval tasks, which are common in AI applications.
    In this chapter, you will learn about vector search, the key concepts and algorithms
    associated with it, and the significance of vector databases. By the end of this
    chapter, you will understand the workings of graph connectivity and its application
    in architecture patterns such as RAG. You will also understand the best practices
    for building vector search systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector embeddings and similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nearest neighbor vector search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for vector databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case studies and real-world applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector search best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While not required, it may help to have some familiarity with graph data structures
    and operations. You may also want to know about the embedding models that are
    used to create vectors, which are discussed in more detail in [*Chapter 4*](B22495_04.xhtml#_idTextAnchor061),
    *Embedding Models*.
  prefs: []
  type: TYPE_NORMAL
- en: What is a vector embedding?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the most basic level, a **vector** is a list of numbers plus an implicit
    structure that determines how those numbers are defined and how you can compare
    them. The number of elements in a vector is the vector’s dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '`[year, make, model, color, mileage]`. These properties form a `[2000, Honda,
    Accord,` `Gold, 122000]`.'
  prefs: []
  type: TYPE_NORMAL
- en: This is a useful model for building intuition on how vectors can encode information.
    However, each element may not always correspond to a concrete idea with a numerable
    set of possible values. The vectors used in AI applications are more abstract
    and have significantly more dimensions. In a way, they smear concrete ideas across
    many dimensions and standardize to a single set of possible values for every dimension.
    For example, vectors from OpenAI’s `text-embedding-ada-002` model always have
    1,536 elements, and each element is a floating-point number between -1 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: The vectors used in AI applications are the output of **embedding models**.
    These are **machine learning** (**ML**) models that are pre-trained to convert
    inputs, typically a string of text tokens, into vectors that encode the semantic
    meaning of the input. For humans, the many dimensions of these vectors are basically
    impossible to decipher. However, the embedding model learns an implicit meaning
    for every dimension during training and can reliably encode that meaning for its
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The exact structure of the vectors varies between embedding models, but a specific
    model always outputs vectors of the same size. To use a vector, it’s imperative
    to know which model created it.
  prefs: []
  type: TYPE_NORMAL
- en: Vector similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beyond storing high-dimensional vector data, vector databases also support various
    operations that let you query and search for the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The most common operation is **nearest neighbor search**, which returns a list
    of stored vectors that are most similar to an input query vector. Common search
    interfaces are familiar territory. For instance, e-commerce searches often prioritize
    products relevant to your query, even if they aren’t exact matches. Nearest neighbor
    search uses the semantic nature of embedding model vectors to make finding *similar*
    vectors the same as finding *relevant* results.
  prefs: []
  type: TYPE_NORMAL
- en: But what does it mean for two vectors to be similar? In short, similar vectors
    are close together, which you can measure as a distance. There are many ways to
    define **distance**, including some that become more relevant in higher dimensions.
    It’s not possible to visualize how distance works for high-dimensional vectors
    but it’s straightforward to see how the ideas work for small vectors and then
    scale them up.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you think back to geometry class, you’ll remember that you can find the
    distance between two coordinate vectors using the distance formula. For example,
    2D coordinates such as `(x, y)` use the distance formula `distance(a, b) = sqrt((a_x
    - b_x)**2 + (a_y - b_y)**2)`. It also works for 3D coordinates, where the formula
    has another component for the extra dimension: `sqrt((a_x - b_x)**2 + (a_y - b_y)**2
    + (a_z - b_z)**2)`. This pattern generalizes to any number of dimensions and is
    referred to as the **Euclidean distance** between two *n*-dimensional points.'
  prefs: []
  type: TYPE_NORMAL
- en: In theory, you can also use Euclidean distance to measure distances between
    high-dimensional vectors such as those used in AI applications. Practically, however,
    the usefulness of Euclidean distance breaks down as you continue to increase the
    number of dimensions. This pattern of intuitions and tools that work in small
    dimensions breaking down at higher dimensions is common and often referred to
    as the **curse** **of dimensionality**.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of Euclidean distance, most applications use a different distance metric
    called **cosine similarity**. Unlike Euclidean distance, which measures the space
    between the *tips* of two vectors, cosine similarity uses a different formula
    that measures the size of the angle between two vectors that share a common base.
    It effectively determines whether two vectors are identical, completely unrelated,
    or (most likely) somewhere in between in a mathematically precise way, as shown
    in *Figure 5**.1*. Similar vectors point in almost the same direction, unrelated
    vectors are orthogonal, and opposite vectors point in opposite directions.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: A comparison of vector measurements'
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity equips you with a tool to measure the distance between two
    vectors. Due to the nature of how vector embeddings carry semantic information,
    it’s also a tool to measure how related or relevant two vectors are to one another.
    If you extend this idea to more vectors, you can figure out how related a given
    vector is to any of the others and even rank them all by relevance. This is the
    core idea behind **vector** **search algorithms**.
  prefs: []
  type: TYPE_NORMAL
- en: The process of comparing many vectors in this way brings its own complexity
    and challenges. To deal with them, search providers have developed various approaches
    to nearest neighbor search that balance trade-offs and optimize for different
    use cases. The next section will discuss two approaches to handle real search
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Exact versus approximate search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, your use case requires that searches return only the true nearest
    neighbors. For example, think about an authentication app that stores biometric
    information about its users as embedded vectors so that they can identify themselves
    later. When they scan their fingerprint or face, the app creates a vector embedding
    of the scanned data and uses it as the query vector in a nearest neighbor search.
    An app like this should never misidentify the user as someone else with a similar
    fingerprint or face.
  prefs: []
  type: TYPE_NORMAL
- en: This use case is perfect for an **exact nearest neighbor** (**ENN**) search,
    which guarantees that the search results are the best possible matches. This type
    of search must always return the closest matching stored vector and ensure that
    it appears before other similar but more distant matches.
  prefs: []
  type: TYPE_NORMAL
- en: 'One straightforward approach is to brute-force the problem: calculate the distance
    between the query vector and every stored vector, then return a list of the results
    sorted from closest to farthest. By checking every vector, you can guarantee that
    the search results include precisely the most relevant vectors in order. While
    effective for small datasets, this method quickly becomes computationally expensive
    and time-consuming as the number of stored vectors increases. Some clever approaches
    can help exact search scale to larger datasets, such as using tree-based indexes
    to avoid calculating similarity for every vector. This makes exact search useful
    for some additional kinds of applications, but ultimately, the problem does not
    scale well and can take a long time on large datasets. In cases where exactness
    is required, you have to accept its constraints and find ways around them.'
  prefs: []
  type: TYPE_NORMAL
- en: For other common cases, however, it is enough to know that your search results
    are *close enough* to be the best match. This use case is called an **approximate
    nearest neighbor** (**ANN**) search and is powerful enough for many everyday applications.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you search for `movies like Inception` in a recommendations
    app, you don’t need the results to include a specific movie. Rather, you probably
    just want a list of a few similar sci-fi thrillers with mind-bending plots. A
    list of results such as `["Minority Report", "Memento", "Shutter Island"]` is
    useful, even if it turns out that the movie *Interstellar* is technically a closer
    semantic match than any of the returned results.
  prefs: []
  type: TYPE_NORMAL
- en: The choice between exact and approximate search comes down to your application’s
    requirements. You may have strict requirements that necessitate an exact search.
    However, you may also have a use case where an exact search, while useful, is
    not necessary to provide value. Or it might not make sense to do an exact search
    at all. In the next section, you’ll learn how to evaluate search algorithms to
    help you determine your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can describe a search algorithm in terms of its precision, recall, and
    latency:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** measures how accurate the search results are. Precise searches
    try to return only matches that are relevant to the query and few, if any, irrelevant
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall** measures how complete the search results are. If a search returns
    a large fraction of all relevant results, then it has a high recall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency** measures how long a search query takes from start to finish. Every
    search takes some amount of time to return results. The exact latency varies between
    searches but, on average, it’s a function of how many vectors are in the search
    space and your precision and recall requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These factors are tightly coupled and require trade-offs that define the nature
    of nearest neighbor searches. For example, an ENN search has perfect precision
    and will include the most relevant results. However, to keep the latency reasonable,
    it might omit some relevant results if there are too many. Because it misses valid
    results, this search would have a relatively low recall. If the ENN search also
    required a high recall, then the search would have to run for longer to ensure
    that enough relevant results are included.
  prefs: []
  type: TYPE_NORMAL
- en: In an ANN search, you can relax your precision requirements, which allows you
    to optimize the other factors instead. You can get more complete results by either
    allowing the search to take more time or by returning more results that potentially
    include false positives. If you can tolerate false positives, for example, by
    filtering them out after the search in your app, then you can use ANN to run fast
    searches that return highly relevant result sets.
  prefs: []
  type: TYPE_NORMAL
- en: You should evaluate your application and determine its top priority regarding
    these factors. Then, you can choose the appropriate search operation and tune
    the algorithm until the other factors are appropriately balanced.
  prefs: []
  type: TYPE_NORMAL
- en: '**Tuning** a search algorithm involves modifying the configuration parameters
    that determine how it constructs and traverses its index data structure. To get
    a better feel for what that means, you’ll spend the next few sections going over
    the concepts and data structures used to enable vector search operations, starting
    with the idea of connectivity.'
  prefs: []
  type: TYPE_NORMAL
- en: Graph connectivity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’ve ever used a city’s public transit network to get around, you may
    have wondered about how the city chose to put the train or bus stops where they
    did. There are many factors at play, but if you look at an ideal case, then you
    can boil the choice down to two related factors: **connectivity** and **latency**.'
  prefs: []
  type: TYPE_NORMAL
- en: Think about the experience of a train rider, let’s call her Alice, visiting
    her friend, Bob, across the city. It would be great if there was a stop right
    next to Bob’s house because, then, Alice could see him right after stepping off
    the train. Of course, you can’t put a train station in front of every house, and
    after a certain point, adding more stops would increase the average trip time.
  prefs: []
  type: TYPE_NORMAL
- en: Every time you change the number of stops or connections, you may affect how
    long it takes to get between any two destinations in the system. Typically, the
    job of planning where to place public transit stops is done with thought and consideration
    by knowledgeable civil engineers, city planners, and other stakeholders. The primary
    goal of a transit network is to take a rider to a stop that is relatively close
    to their true final destination in a reasonable amount of time. By understanding
    their goal and applying specific strategies, city planners try to connect distant
    parts of the city in a way that’s useful and efficient for transit riders.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the goal of an ANN search is to find a vector that is close to a
    given query vector, also in a reasonable amount of time. If you were to take inspiration
    from transit planners, you could use this similarity to your advantage and design
    an effective ANN index.
  prefs: []
  type: TYPE_NORMAL
- en: Navigable small worlds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In essence, both transit planning and nearest neighbor search boil down to a
    problem of building and traversing a graph that trades off connectivity and latency.
    You can use an algorithm called **navigable small worlds** (**NSW**) to build
    such a graph. It takes in vectors one at a time and adds a node to the graph for
    each one. Each node can also have connections to other nodes, called **neighbors**,
    that are assigned during graph construction.
  prefs: []
  type: TYPE_NORMAL
- en: The NSW algorithm is designed to balance how relevant a node’s immediate neighbors
    are with how connected the node is to the rest of the graph. It will mostly assign
    neighbors that are closely related to a node. However, it may also sometimes connect
    two less similar nodes that are relatively far apart on the graph. If you think
    about the transit example, this is like having a bus route that has several stops
    in the same neighborhood but that also runs downtown. Residents can easily get
    to their local destinations. If they need to go outside of the neighborhood, then
    they still have access to the rest of the city.
  prefs: []
  type: TYPE_NORMAL
- en: For an example of an NSW graph, refer to *Figure 5**.2*. Notice that each node
    is connected to a maximum of three neighbors and that, in general, nearby nodes
    are closely connected. Each node represents a vector and nodes connected with
    lines are neighbors. The highlighted connections show the path of a greedy nearest
    neighbor search.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: An NSW graph'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve constructed an NSW graph of your vectors, you can use it as an index
    for ANN searches. You can start at a random node and use a search algorithm to
    follow the neighbor connections until you reach the nearest neighbor. This lets
    you limit your similarity comparison to only a subset of the total search space.
    For example, notice how the search path in *Figure 5**.2* arrives at the nearest
    neighbor without visiting every node in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: How to search a navigable small world
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The exact search algorithm that you use to traverse an NSW graph may vary and
    affect the behavior of the search as a whole. The most common algorithm is a simple
    **greedy search**, where at every step, you find and take the best immediate option
    with no regard to previous or future steps. For example, a greedy search of an
    NSW graph first randomly selects a node to start at and then measures to see how
    close the node is to the query vector. Then, it measures the distance to each
    of the node’s neighbors. If one of the neighbors is closer than the current node,
    then the search moves on to that node and continues with the same measure-and-compare
    process. Otherwise, the search is complete and the current node is an approximate
    nearest neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: In this basic example of NSW with greedy search, the definition of *approximate*
    is very broad and the search may return suboptimal results. This comes down to
    the nature of graph search, which, in this case, is designed to find a local minimum
    of the graph. This local minimum is not guaranteed to be the *global* minimum,
    which is what makes the search approximate rather than exact. A greedy search
    algorithm alone can return false positives if it settles on a local minimum that
    is too far from the global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: You can partially guard against this by tuning the graph’s construction parameters.
    However, due to the dynamic nature of search queries and the underlying data being
    searched, you can’t entirely prevent false positive local minima from existing.
    Instead, you need to find a way to minimize their impact.
  prefs: []
  type: TYPE_NORMAL
- en: One way is to run the search multiple times, starting from different randomized
    entry nodes. This method, called **randomized retries**, collects multiple samples
    from the graph and returns the best result out of all the samples. You can also
    add additional machinery to the algorithm to make it more robust. A common architecture
    pairs the greedy search algorithm with a configurable **priority queue** that
    keeps a sorted list of the nearest neighbors the search has seen. If the search
    encounters a false positive local minimum, the queue lets it backtrack and explore
    other branches of the graph that might lead to a nearer neighbor.
  prefs: []
  type: TYPE_NORMAL
- en: The exact search method you use depends on the dataset and your goals. For example,
    randomized retries are easy to implement and can run in parallel. They are useful
    for subtle, exploratory searches that might match many local minima. However,
    their random nature makes them non-deterministic, and each retry does a full search,
    which can quickly scale your costs. Conversely, priority queues are deterministic
    and precise but are harder to implement and tune.
  prefs: []
  type: TYPE_NORMAL
- en: With this information, you have the basis for a useful vector search index.
    You could stop building the index here and start searching. However, you will
    quickly find that there are issues with this approach, particularly as you scale
    the search space to sizes commonly seen in AI apps. Randomized retries have significant
    computational overhead, and you must do more of them as you scale your data set.
    A priority queue keeps a search from getting stuck in local minima but does not
    prevent it from meandering through many nodes on the way to its target.
  prefs: []
  type: TYPE_NORMAL
- en: To address these issues, you need to go beyond a single NSW graph. In the next
    section, you will see how combining multiple NSW graphs together can circumvent
    meandering searches and make randomized retries less necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical navigable small worlds
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Think back to Alice’s public transit experience. What if, instead of the same
    city, she and Bob lived in different cities on opposite sides of the country?
    Alice could, in theory, limit herself to public transit services by crisscrossing
    the nation via a series of trains, buses, taxis, and bike shares. This would obviously
    take a lot of time and require many stops along the way. That’s because transit
    networks are only effective at the scale of an individual city. Once you zoom
    out farther, you need a different system.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of just using transit, Alice could instead start at her city’s airport
    and fly to Bob’s city. Even if her trip included a layover and multiple flights,
    it would still probably be faster than using transit alone. Once she gets to Bob’s
    city, she can use the subway system to get from the airport to his neighborhood
    quickly and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alice’s trip took place at two distinct levels. First, she started at the level
    of airports, where she was free to travel to any destination airport connected
    to her home airport. At this layer, she had direct access to many different cities,
    but that access was limited to only one location in each city: the airport. She
    used the airports to get closer to Bob without spending too much time planning
    her route and traveling. Once she got to the closest airport to Bob, she dropped
    down into the second layer and gained access to a transit network that could get
    her even closer to Bob.'
  prefs: []
  type: TYPE_NORMAL
- en: This is basically the idea of **hierarchical navigable small worlds** (**HNSW**).
    You can create a hierarchy of layers where each layer is an NSW graph. For example,
    look at *Figure 5**.3* to see a typical HNSW graph structure. The top layer has
    relatively few nodes that are all fairly distant from one another and sparsely
    connected. Each lower layer has all the nodes of the layer above it plus additional
    nodes and connections that make the graph denser and more connected. In this chapter’s
    example, the distinction between transit nodes and airport nodes is a natural
    way to split the layers. The airports are the top layer and the next layer down
    includes both the airports and the transit stops.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: An HNSW graph structure'
  prefs: []
  type: TYPE_NORMAL
- en: An actual HNSW algorithm would decide the *top* layer for each vector probabilistically
    with a node that exists only in lower layers being more likely than one that also
    exists in higher layers. A search starts in the top layer by finding the node
    that’s nearest to the query vector. Then, it moves to the same node but in the
    next layer down and continues the search from there. This continues until it reaches
    the nearest neighbor on the final layer, at which point, the search is complete.
    In *Figure 5**.3*, the highlighted connections show the path of a greedy nearest
    neighbor search across multiple layers.
  prefs: []
  type: TYPE_NORMAL
- en: HNSW is the foundation of many modern vector search applications. It’s battle-tested
    and proven to give useful results in a reasonable amount of time. The algorithm
    is highly suited for ANN use cases with configurable parameters that put you in
    control of how your searches perform.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an idea of the inner workings of vector search, you can see
    how it requires purpose-built logic and data structures. In the next section,
    you’ll learn how vector databases encapsulate all of the technical details in
    order to make vector search available to developers.
  prefs: []
  type: TYPE_NORMAL
- en: The need for vector databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vectors carry deep semantic information and have many potential use cases that
    will make them increasingly common over the next few years. Working with them
    requires specific and complex operations that only process vector data. Additionally,
    the demand for search can often vary substantially from the demand for more structured
    database queries.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these factors mean vector operations and traditional database workloads
    are largely independent. This gives rise to the concept of a vector database that’s
    designed specifically to handle vector data, indexes, and workloads. From a developer’s
    perspective, vector databases can take several forms.
  prefs: []
  type: TYPE_NORMAL
- en: The most basic is a **standalone product** that’s independent from other operational
    databases. This type of vector database has the freedom to focus solely on implementing
    and optimizing vector operations without considering other database operations.
    However, often, vector search applications require additional filtering or metadata
    and may perform more traditional database operations based on search results.
    These use cases require either multiple queries to different databases at runtime
    or an additional syncing layer that copies data from your operational database
    to the vector store.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, a vector database can be baked into an existing database or data
    service. For example, a **general-purpose database management system** might support
    vector search operations in its query language if you’ve defined the appropriate
    vector search index. This allows applications to piggyback off of the existing
    system’s features and access search within the same system. The vector database
    can be scaled and run independently within the system but exposed to the user
    along with traditional operations as part of a unified API. This couples your
    vector store to your existing database but leads to simpler and easier-to-maintain
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of form, vector databases are a key tool in AI applications. They
    are purpose-built to store and query vector data. You can configure them to deliver
    optimal search results and power AI applications.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will cover some ways that vector search can be used to enhance
    ML and AI models, including during training, fine-tuning, and runtime. You’ll
    also learn how vector search itself enables AI applications without additional
    functions or models.
  prefs: []
  type: TYPE_NORMAL
- en: How vector search enhances AI models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AI models encompass a broad class of data structures and techniques. ML forms
    the core of most modern vector-based AI models, aiming to “teach” computers to
    do specific tasks via a training process. In general, ML processes work by feeding
    a curated dataset to a base model that can detect and infer patterns from the
    data. Once a model has learned these patterns, it’s able to recreate or interpolate
    them to process new inputs. These techniques and models are ubiquitous in the
    world of AI and are the secret sauce that powers novel use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, ML training and AI applications can be split into two concerns,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Information retrieval** involves finding relevant information that’s useful
    as input to an AI process. Vector search is very well suited for this task. Embedding
    models can encode the semantics of a huge variety of inputs into a standard vector
    form. Then, you can use search to find matches for an equally huge range of inputs,
    both structured and unstructured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information synthesis** combines multiple pieces of information, possibly
    from different sources, into a coherent and useful result. This is the domain
    of GenAI models. These models can’t reliably find or generate true facts, but
    they can effectively process and reformat input information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector search enhances ML and AI models by providing them with access to the
    most relevant data at every stage, from training to fine-tuning to runtime execution.
  prefs: []
  type: TYPE_NORMAL
- en: During training, you can use a vector database to store and search your training
    data. You can design a process that finds the most relevant data from the corpus
    to use for each training task. For example, when training a language model for
    a specific domain such as medicine, you could use vector search to retrieve the
    most relevant chapters from medical textbooks for each training batch. This ensures
    that the model learns the most pertinent information without being distracted
    by noise.
  prefs: []
  type: TYPE_NORMAL
- en: You can apply the same idea during fine-tuning, which is essentially a secondary
    training stage on top of a more generic base model. For example, you could fine-tune
    the medicine language model to generate reports using a hospital system’s preferred
    style and structure. Vector search could help find human-written reports that
    are relevant to each training topic.
  prefs: []
  type: TYPE_NORMAL
- en: Whether your model is specialized or general purpose, you can customize its
    runtime behavior by modifying the inputs you give to it. Vector search can analyze
    raw input and find related information. Then, you can augment or refine the raw
    input to include the retrieved context. For example, you might maintain a vector
    database of rare diseases and search for anything that matches a user’s description
    in order to get a more tailored diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: AI applications come in many forms, but modern apps increasingly use a runtime
    customization approach to provide relevant context to generative transformer models.
    This architecture is the basis of a technique called **retrieval-augmented generation**
    (**RAG**), which you’ll learn about in greater depth in [*Chapter 8*](B22495_08.xhtml#_idTextAnchor180),
    *Implementing Vector Search in* *AI Applications*.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, you’ve learned the theory and mechanics of vector databases
    and search operations. Next, you’ll look at some examples of real vector database
    use cases that highlight how vectors are the core of modern AI apps.
  prefs: []
  type: TYPE_NORMAL
- en: Case studies and real-world applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vector search is a powerful tool that enables you to build sophisticated systems
    for finding information based on its meaning, rather than just its exact words.
    By understanding the context and relationships between data points, vector search
    helps you retrieve highly relevant results. So far, you have learned about the
    different concepts involved with vector search and some of the different offerings
    that exist in the market, but how do businesses integrate vector search into their
    applications?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will explore three popular methods for leveraging vector
    search: semantic search, RAG, and **robotic process automation** (**RPA**). You
    will look at existing case studies of **MongoDB Atlas Vector Search** that fit
    into each of these buckets, and how these applications deliver value to the end
    user through more accurate search that wasn’t previously possible. Each of the
    following case studies was originally published as a part of the *Building AI
    with MongoDB* series of customer stories ([https://www.mongodb.com/resources/use-cases/artificial-intelligence?tck=blog-genai&section=resources&contentType=case-study](https://www.mongodb.com/resources/use-cases/artificial-intelligence?tck=blog-genai&section=resources&contentType=case-study)).
    These stories are presented here to showcase the variety of vector search use
    cases that can be built on the flexible, scalable, and multifaceted MongoDB Atlas
    platform.'
  prefs: []
  type: TYPE_NORMAL
- en: Okta – natural language access request (semantic search)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Okta**, one of the world''s leading identity security providers, uses a natural
    language RAG interface to allow users to easily request roles for new technologies
    in their organizations. They built a system called **Okta Inbox** using Atlas
    Vector Search and their own custom embedding model that makes it possible for
    users to map natural language queries to the right roles.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: Okta Inbox user request form'
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of leveraging semantic search to solve a problem, where the
    embedding models trained by Okta’s data science team were capable of mapping natural
    language requests to the right user roles to be assigned.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Okta Inbox administrator view'
  prefs: []
  type: TYPE_NORMAL
- en: These requests would get routed to a manager via Slack through an existing workflow.
    The end result is a simple user experience that makes identity management between
    both the requesters and the access managers much simpler, thus making the value
    proposition of Okta as an identity and access management solution even greater.
  prefs: []
  type: TYPE_NORMAL
- en: Okta chose to use Atlas Vector Search to query these vectors since they were
    already using Atlas as their operational data store, and this provided a simplified
    developer experience. You can read more about this case study at [https://www.mongodb.com/solutions/customer-case-studies/okta](https://www.mongodb.com/solutions/customer-case-studies/okta).
  prefs: []
  type: TYPE_NORMAL
- en: One AI – language-based AI (RAG over business data)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**One AI** provides verticalized AI agents and chatbots for different industries.
    These services allow detailed AI-assisted analysis to be performed over documents
    with applications in industries ranging from financial services and real estate
    to manufacturing and retail.'
  prefs: []
  type: TYPE_NORMAL
- en: The chatbots offered by One AI are all built using the MongoDB Atlas platform,
    with over 150 million indexed documents from over 20 different internal services.
    One AI’s goal of bringing AI to everyday life is made feasible by simply adding
    a vector search index to the data that they store in Atlas and making it queryable
    via embedded natural language input.
  prefs: []
  type: TYPE_NORMAL
- en: “A very common use case in language AI is creating vectors that represent language.
    The ability to have that vectorized language representation in the same database
    as other representations, which you can then access via a single query interface,
    solves a core problem for us as an API company.”
  prefs: []
  type: TYPE_NORMAL
- en: —Amit Ben, CEO and founder of One AI
  prefs: []
  type: TYPE_NORMAL
- en: This is a prime example of a multitenant RAG application, where data that is
    indexed and provided for one type of AI service provided by One AI might not be
    relevant to another service. As discussed later in this chapter, this is a common
    data modeling pattern that is easy to build within the Atlas platform. You can
    further read about this case study at [https://www.mongodb.com/solutions/customer-case-studies/one-ai-success-story](https://www.mongodb.com/solutions/customer-case-studies/one-ai-success-story).
  prefs: []
  type: TYPE_NORMAL
- en: Novo Nordisk – automatic clinical study generation (advanced RAG/RPA)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Novo Nordisk** is one of the world’s leading healthcare companies with a
    mission to defeat some of the world’s most serious chronic diseases such as diabetes.
    As a part of the process of getting new medicines approved and delivered to patients,
    they must generate a **clinical study report** (**CSR**). This is a detailed record
    of the methodology, execution, results, and analyses of a clinical trial and is
    meant as a critical source of truth for regulatory authorities and other stakeholders
    in the drug approval process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Example of a CSR'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a CSR takes around 12 weeks to complete, but the content digitalization
    team at Novo Nordisk was able to build a tool using Atlas Vector Search to shorten
    this process to ten minutes. They built a RAG workflow called **NovoScribe** leveraging
    **Claude 3** and **ChatGPT** as their chat completion models, and **Titan** for
    text embedding hosted on the **Amazon Bedrock** service. They used MongoDB Atlas
    Vector Search as a knowledge base to serve relevant data to these models.
  prefs: []
  type: TYPE_NORMAL
- en: Functionally, NovoScribe generates validated text using defined content rules
    and statistical outputs. Atlas Vector Search computes the similarity of each text
    snippet to the relevant statistics, which is then fed into a structured prompt
    to the LLM to produce a CSR that is ready for review by a subject-matter expert,
    including the lineage of all of the data presented.
  prefs: []
  type: TYPE_NORMAL
- en: “What’s great about MongoDB Atlas is that we can store native vector embeddings
    of the report right alongside all of their associated text snippets and metadata.
    This means we can run really powerful and complex queries quickly. For each vector
    embedding we can filter on which source document it’s coming from, who wrote it,
    and when.”
  prefs: []
  type: TYPE_NORMAL
- en: —Tobias Kröpelin, PhD, Novo Nordisk
  prefs: []
  type: TYPE_NORMAL
- en: This project allowed Novo Nordisk to build an advanced clinical report generation
    system by intelligently arranging their data in the right format within MongoDB
    and defining a vector search index against it. They were allowed to go further
    with their data in more ways using novel embedding models and LLMs to dramatically
    improve the process of authoring CSRs as a result. You can read more about this
    case study at [https://www.mongodb.com/solutions/customer-case-studies/novo-nordisk](https://www.mongodb.com/solutions/customer-case-studies/novo-nordisk).
  prefs: []
  type: TYPE_NORMAL
- en: Vector search best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section covers the best practices for improving the accuracy of your vector
    search through intelligent data modeling, deployment model options, and considerations
    for prototype and production use cases. By following the guidance in this section,
    you will be more likely to improve the quality of your vector search results and
    operate your search system in a scalable, production-ready manner.
  prefs: []
  type: TYPE_NORMAL
- en: Data modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the context of MongoDB, `$``vectorSearch` query.
  prefs: []
  type: TYPE_NORMAL
- en: One can broadly think about leveraging metadata as using documents to deliver
    the data back to the user, rather than vectors. Working with documents as the
    results of an aggregation stage means that different aggregation stages can be
    composed together to yield greater functionality than any one alone and can benefit
    from query optimization. This has been the bread and butter of the document model
    since MongoDB was invented, and it continues to be the case today in the age of
    GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: This section will dive deeper into the ways other data can be used prior to,
    alongside, and following vector search to improve the accuracy of your vector-based
    information retrieval system.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most basic yet most effective form of metadata usage is to limit the scope
    of the vector search by considering only vector data that meets a prefilter. This
    restricts the scope of valid documents to be considered, which, for selective
    filters (the most common kind of filter), increases accuracy and reduces query
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: At query time, these prefilters can be considered as a part of a `$vectorSearch`
    query using a `$match` MQL semantic. This means that in addition to point filters
    such as `$eq`, the user can define range filters such as `$gt` or `$lt` to only
    search against documents that fit a range of values rather than matching a specific
    one. This can dramatically reduce the number of valid documents that need to be
    searched, reducing the amount of work that needs to be done and generally improving
    the accuracy of your search. `$match` filters can also leverage logical operators
    such as `$and` and `$or` to allow users to compose filters together and build
    more complex logic into their search applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at two common types of filters, and when and how you might use them.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic filters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Dynamic filters** are pieces of metadata that vary based on the content of
    the search query. These can be attributes of the data, such as when a book was
    published or its price. They are typically selected by a user when executing their
    search along with their plain English query. Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic filters are most common when building a semantic search application
    since they are typically input by the user prior to executing a query within a
    search bar. This contrasts with a RAG interface, which is entirely natural language.
  prefs: []
  type: TYPE_NORMAL
- en: Static filters and multitenancy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are cases where the filter is associated not with the body of the query,
    but by the user’s profile. The user may be querying data that is accessible only
    to their company but is stored in a multi tenanted fashion with many other tenants’
    data. In this case, the user ID or company ID that the user belongs to may be
    used to filter what results are searched against. For cases where there are a
    high number of tenants and few vectors, filters are the recommended approach for
    modeling data rather than storing many bits of data across multiple collections
    and indexes.
  prefs: []
  type: TYPE_NORMAL
- en: It is recommended to set the `exact` flag to `true` in `$vectorSearch` when
    you have a high degree of variation between the number of vectors per tenant and
    a high number of tenants modeled within the same collection or index. This will
    lead to an exhaustive search performed in parallel on all segments corresponding
    to a vector index. In many cases, this will accelerate the search, given the high
    selectivity of the filter and the large number of potential vectors that would
    need to be searched and discarded while running a filtered HNSW search.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the context of RAG, an interesting analogy emerges. Just as chat models require
    intelligent prompt engineering, embedding models require intelligent chunking.
    **Intelligent chunking** requires finding the right level of context that can
    effectively be mapped to a search or natural language query. This may also be
    the right level of context to provide to the LLM, but as you’ll see later in the
    *Parent document retrieval* section, this is not a strict requirement if you intelligently
    model your data.
  prefs: []
  type: TYPE_NORMAL
- en: You will learn more about basic and advanced chunking strategies in [*Chapter
    8*](B22495_08.xhtml#_idTextAnchor180), *Implementing Vector Search in AI Applications*.
    For the sake of this section, let’s consider one basic chunking strategy, **fixed
    token count with overlap**, and how you can experiment to assess what works best
    on your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Fixed token count with overlap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A fixed token count with overlap, which is a common default in many RAG integration
    frameworks such as LangChain, splits unstructured data into chunks based on the
    specified maximum number of tokens per chunk and the desired overlap between chunks.
    This method is more granular than the whole-page ingestion method, and it allows
    for greater experimentation on your specific dataset. It doesn’t involve exploiting
    any structure within the unstructured data. This is a positive in terms of simplicity
    of development but can be a negative when sentences, paragraphs, or other boundaries
    demarcate semantic significance in a way you would want to model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique may be a good fit if you have little control over the source
    data or are working with unstructured data that doesn’t lend itself well to boundary
    chunking methods that leverage document structure, such as HTML tags, because
    this technique is compatible with any text format. *Figure 5**.7* shows an example
    with different colors indicating separate chunks and overlaps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: An example of chunking based on fixed token count with overlap'
  prefs: []
  type: TYPE_NORMAL
- en: Experimentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluating which chunking strategy or embedding model works best for your use
    case requires curating judgment lists of documents along with the queries that
    you would expect to map to those documents. You would also want to play around
    with the different embedding models and chunking strategies that can be applied
    before embedding data to see which works best for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: A given embedding model might perform better or worse with a fixed chunking
    strategy. You can more easily evaluate which combination of chunking and embedding
    models is best suited for your use case. You could have multiple versions of the
    same data, each split and processed differently. By comparing these versions,
    you can determine the optimal splitting method and embedding model for your specific
    search needs.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to determine whether an embedding model is effectively mapping
    your documents to a sample query is to inspect the similarity score that is returned
    for a set of queried documents and see how well that aligns with what good responses
    might be for the actual question, as shown in *Table 5.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Rank** | **Raw document** | **Embedding** | **Cosine similarity** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | “One of the main challenges of building software is managing complexity.”
    | [0.23, 0.45, …] | 0.901 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | “Deep modules provide deep functionality behind a simple interface” |
    [0.86, 0.34, …] | 0.874 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | “Software systems often grow in complexity due to evolving requirements.”
    | [0.46, 0.51, …] | 0.563 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5.1: Vector search results ranked by cosine similarity'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of a fixed token count with overlap strategy, you will have to figure
    out the token count that you would like to start with. The 300–500 token range
    seems sufficient for experimentation in the information retrieval community.
  prefs: []
  type: TYPE_NORMAL
- en: Hybridization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Hybridization** involves modeling multiple sources of relevance within a
    single document and jointly considering them alongside a single vector search
    at query time. This technique embodies the flexibility of the aggregation pipelines
    supported by MongoDB and allows for a great amount of experimentation and tuning
    of your search system leveraging vector search, lexical search, traditional database
    operators, geospatial queries, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, you will explore some of the more popular methods
    for hybridization, as well as some promising avenues of exploration that you might
    find relevant to your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Vector plus lexical
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector search is a sound methodology for exploiting semantic similarity between
    a query and indexed document as defined by the capabilities of an embedding model.
    Lexical search systems such as **BM25**, which **Lucene** and, correspondingly,
    Atlas Search use, are helpful in a completely different way in that they index
    tokens directly and use a bag-of-words style approach that ranks a set of documents
    based on the query terms appearing in each document, regardless of their proximity
    within the document.
  prefs: []
  type: TYPE_NORMAL
- en: Despite being based on an original probabilistic retrieval framework developed
    in the 1980s, this approach is still fairly good at mapping keywords in a query
    to keywords in a document, especially when that word is used outside the context
    of what an embedding model was trained on. Small datasets can contain tokens either
    not seen in the training dataset or with alternative meanings, as shown in *Figure
    5**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Out-of-sample terms'
  prefs: []
  type: TYPE_NORMAL
- en: Some vector search providers provide sparse vector search as an alternative
    to lexical search, which can be made to operate similarly but has been considered
    insufficient for customers’ purposes. It also lacks out-of-the-box support for
    many lexical search features, such as synonym lists, pagination, and faceting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Smaller levels of context are good fits for embedding models, whereas broader
    levels can be well represented by keyword search. MongoDB allows users to experiment
    in this direction as much as possible, while also allowing the joint query pattern
    to be joined on a foreign key, rather than simply a document `_id`. This makes
    it possible to have windowing levels of representation for a given document that
    can be considered by different methodologies. The following code shows how some
    documents containing `paragraph_embeddings` can be indexed and queried using a
    vector search index, while other documents containing `full_page_content` can
    be indexed and queried using a text search index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Jointly considering the result sets from the two queries shown in the preceding
    code is what you call **hybrid search** and can be done using the **reciprocal
    rank fusion method**, as shown at [https://www.mongodb.com/docs/atlas/atlas-search/tutorial/hybrid-search/](https://www.mongodb.com/docs/atlas/atlas-search/tutorial/hybrid-search/).
    In the future, Atlas Vector Search will offer support for dedicated stages that
    make combining result sets based on rank or score much simpler. However, the fundamental
    concepts will remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: Vector plus vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There might be multiple sources of vector relevance in your dataset that you
    would want to consider jointly, similar to how you might jointly consider paragraph
    embeddings and keyword relevance for a whole page. The secondary embedding field
    you are considering might be a derivative field, such as an LLM-generated chapter
    summary that is then embedded, or it could be an entirely different source of
    data. The following code shows a single document with a set of source fields that
    could be embedded and indexed using a vector search index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The results of independent `$vectorSearch` queries could be hybridized and fused
    using a similar pattern to the vector plus lexical search query pattern seen in
    the previous section and would allow for multiple sources of relevance to be used
    to find the most relevant document to a query.
  prefs: []
  type: TYPE_NORMAL
- en: 'In e-commerce search use cases, it is common for a single item to have many
    sources of relevance that can be embedded and stored within the same document
    representing that item. These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Product description
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User reviews (and summaries of user reviews)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these sources of relevance can be embedded and jointly considered using
    the same query pattern as one would use to jointly consider vector and lexical
    relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating user feedback
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`$vectorSearch` and the `$sort` stage using the upvotes or downvotes as a proxy
    for user relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a very naive approach, but the principle behind it can be extended to
    allow for greater personalization of content where similar users are defined by
    similar interactions with different content, which is the basis for the popular
    recommendation system algorithm known as **collaborative filtering**.
  prefs: []
  type: TYPE_NORMAL
- en: While it is still early days in terms of intelligently incorporating user feedback
    into your RAG application, the flexibility of the document model should allow
    for a rich amount of experimentation in this area as your search system, and how
    your users engage with it, evolves over time.
  prefs: []
  type: TYPE_NORMAL
- en: Document lookups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have a sorted result set of documents, possibly produced from multiple
    methodologies in an optimized manner, there are still additional operations that
    can be performed that might leverage relationships inherent within your data.
    With **document lookups**, some data may be easier to model outside of the document
    itself using a foreign lookup key to model tree structures within your data, such
    as hierarchies within documents, organizations, or some other taxonomy.
  prefs: []
  type: TYPE_NORMAL
- en: Parent document retrieval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Parent document retrieval** involves performing a vector search at one level
    of context, and then retrieving a document connected to the most relevant retrieved
    documents via a foreign key. This foreign key is usually a child-parent relationship,
    such as an embedded paragraph belonging to a specific page of a larger body of
    text, where that larger bit of context may be stored in another document completely.'
  prefs: []
  type: TYPE_NORMAL
- en: With this pattern, you can store only the embeddings at the lower level, and
    then look up a higher level of context containing a much larger amount of text.
    This may be useful if you find that the queries are more easily mapped semantically
    to a smaller amount of text, but the amount of data you want to serve to the user
    or an LLM is much larger, which is often the case. The following code example
    for hybridizing lexical and vector search is also an example of parent document
    retrieval, as vector embeddings are searched against to yield a full page of content
    to provide to the LLM. The foreign key is the `page_number`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It’s important to note that like all other metadata, capturing relationships
    between MongoDB documents in this manner must be extracted at ingestion time.
  prefs: []
  type: TYPE_NORMAL
- en: Graph relationships
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can exploit even more relationships between documents using the `$graphLookup`
    stage. This allows an arbitrary number of hops to be jumped from the results of
    `$vectorSearch`. If the customer’s data already contains relationships that can
    be traversed in a hierarchical manner, this is an immediate benefit to them.
  prefs: []
  type: TYPE_NORMAL
- en: Just as you might define a relationship between a document and a page, you might
    recursively chunk a document into ever smaller chunks, relate each chunk to a
    parent document using a `parent_id` field, and embed those chunks. At query time,
    you could search against all of the chunks and recursively jump up all of the
    `parent_id` values to the desired level of resolution to provide to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Successfully deploying your AI application is the final hurdle. This section
    outlines various deployment options and provides guidance on estimating necessary
    resources to ensure optimal performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment options
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest deployment model for getting started with Atlas Vector Search is
    to define a search index definition within your existing cluster or a new cluster.
    This can be configured using **search index management commands** for paid tier
    clusters or the **UI/Atlas Administration API** for shared tier clusters.
  prefs: []
  type: TYPE_NORMAL
- en: When you feel confident in your vector search use case and are ready for increased
    usage or increased scale of ingested data, it is recommended to move to dedicated
    search nodes. **Dedicated search resources** provide a robust and scalable platform
    for serving demanding search workloads.
  prefs: []
  type: TYPE_NORMAL
- en: This will allow for high-availability vector search, more cost-effective resource
    utilization, and resource isolation from your core database in a way that is more
    practical for production workloads, as visualized in *Figure 5**.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: The benefits of dedicated search nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Migrating to dedicated search nodes is a zero-downtime process that allows for
    your existing base cluster to continue to serve vector search queries as new resources
    are spun up and your indexes are built on them. Once that build process completes,
    `$vectorSearch` queries will be routed to your dedicated search nodes and the
    indexes on the original cluster will be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dedicated search nodes can be configured from the **Cluster Configuration UI**
    by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: On the **Create New Cluster/Edit Configuration** page, change the radio button
    for **AWS or Google Cloud** for **Multi-cloud, multi-region & workload isolation**
    to enabled.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Toggle the radio button for **Search Nodes for workload isolation** to enabled.
    Select the number of nodes in the textbox.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the agreement box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the right node for your workload.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Create cluster**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resource requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The current index type supported within Atlas Vector Search is HNSW, which is
    memory-resident. This means that you need approximately 3 KB of memory for every
    768d vector you plan on indexing, scaling linearly with the number and dimensionality
    of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: If you expect your workload will have low query volume, it is recommended to
    select the cheapest option on `M` tier clusters that can allocate 50% of the available
    resources to storing the index in memory. When using dedicated search nodes, 90%
    of the available RAM can be used to host the index. Note that when using `M` tier
    clusters, the index will need to be warmed into the cache using representative
    queries. For dedicated search nodes, the index will be automatically loaded into
    the cache upon an index build.
  prefs: []
  type: TYPE_NORMAL
- en: If you expect your workload to have a high indexing or query concurrency, it
    is recommended to use dedicated search nodes with the high CPU option or to scale
    up the number of dedicated search nodes in your replica set. This will scale up
    the number of available vCPUs to serve the `$vectorSearch` queries in a round-robin
    fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored a variety of concepts related to vector search.
    The chapter delved into how high-dimensional vectors produced from embedding models
    can be useful measures of semantic similarity among the unstructured data passed
    into those models. It examined the HNSW index and how it can be used to accelerate
    vector similarity comparisons between a query vector and many indexed vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter then illustrated how this type of index can be applied in various
    real-world contexts by large organizations, including such architecture patterns
    as RAG, semantic search, and RPA. Finally, the chapter reviewed some of the best
    practices for building vector search systems within MongoDB Atlas, ranging from
    ingestion time considerations, such as metadata extraction, to deployment model
    considerations, such as dedicated search nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will discover the crucial aspects of designing AI/ML
    applications. You will learn how to effectively manage data storage, flow, freshness,
    and retention along with techniques to ensure robust security.
  prefs: []
  type: TYPE_NORMAL
