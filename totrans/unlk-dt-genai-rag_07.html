<html><head></head><body>
		<div id="_idContainer045">
			<h1 id="_idParaDest-123" class="chapter-number"><a id="_idTextAnchor122"/><st c="0">7</st></h1>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/><st c="2">The Key Role Vectors and Vector Stores Play in RAG</st></h1>
			<p><strong class="bold"><st c="52">Vectors</st></strong><st c="60"> are a</st><a id="_idIndexMarker293"/><st c="66"> key component of </st><strong class="bold"><st c="84">retrieval-augmented generation</st></strong><st c="114"> (</st><strong class="bold"><st c="116">RAG</st></strong><st c="119">) to understand, as they are the secret ingredient that helps the entire process work well. </st><st c="212">In this chapter, we dive back into our code from previous chapters with an emphasis on how it is impacted by vectors. </st><st c="330">In simplistic terms, this chapter will talk about what a vector is, how vectors are created, and</st><a id="_idIndexMarker294"/><st c="426"> then where to store them. </st><st c="453">In more technical terms, we will talk about vectors, </st><strong class="bold"><st c="506">vectorization</st></strong><st c="519">, and </st><strong class="bold"><st c="525">vector stores</st></strong><st c="538">. This chapter is all about vector creation and why </st><a id="_idIndexMarker295"/><st c="590">they are important. </st><st c="610">We are going to focus on how vectors relate to RAG, but we encourage you to spend more time and research gaining as in-depth of an understanding about vectors as you can. </st><st c="781">The more you understand vectors, the more effective you will be at improving your </st><span class="No-Break"><st c="863">RAG pipelines.</st></span></p>
			<p><st c="877">The vector discussion is so important, though, that we will span it across two chapters. </st><st c="967">While this chapter focuses on vectors and vector stores, </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="1024">Chapter 8</st></em></span></a><st c="1033"> will focus on vector searches, which is to say how the vectors are used in a </st><span class="No-Break"><st c="1111">RAG system.</st></span></p>
			<p><st c="1122">Specifically, we will cover the following topics in </st><span class="No-Break"><st c="1175">this chapter:</st></span></p>
			<ul>
				<li><st c="1188">Fundamentals of vectors </st><span class="No-Break"><st c="1213">in RAG</st></span></li>
				<li><st c="1219">Where vectors lurk in </st><span class="No-Break"><st c="1242">your code</st></span></li>
				<li><st c="1251">The amount of text you </st><span class="No-Break"><st c="1275">vectorize matters!</st></span></li>
				<li><st c="1293">Not all semantics are </st><span class="No-Break"><st c="1316">created equal!</st></span></li>
				<li><st c="1330">Common </st><span class="No-Break"><st c="1338">vectorization techniques</st></span></li>
				<li><st c="1362">Selecting a </st><span class="No-Break"><st c="1375">vectorization option</st></span></li>
				<li><st c="1395">Getting started with </st><span class="No-Break"><st c="1417">vector stores</st></span></li>
				<li><span class="No-Break"><st c="1430">Vector stores</st></span></li>
				<li><st c="1444">Choosing a </st><span class="No-Break"><st c="1456">vector store</st></span></li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/><st c="1468">Technical requirements</st></h1>
			<p><st c="1491">Going back to the code we have discussed over the past chapters, this chapter focuses on just this line </st><span class="No-Break"><st c="1596">of code:</st></span></p>
			<pre class="source-code"><st c="1604">
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=OpenAIEmbeddings())</st></pre>
			<p><st c="1689">The code for this chapter is </st><span class="No-Break"><st c="1719">here: </st></span><a href="https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07 "><span class="No-Break"><st c="1725">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_07</st></span></a></p>
			<p><st c="1822">The filename </st><span class="No-Break"><st c="1836">is </st></span><span class="No-Break"><strong class="source-inline"><st c="1839">CHAPTER7-1_COMMON_VECTORIZATION_TECHNIQUES.ipynb</st></strong></span><span class="No-Break"><st c="1887">.</st></span></p>
			<p><st c="1888">And </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="1893">Chapter 8</st></em></span></a><st c="1902"> will focus on just this line </st><span class="No-Break"><st c="1932">of code:</st></span></p>
			<pre class="source-code"><st c="1940">
retriever = vectorstore.as_retriever()</st></pre>
			<p><st c="1979">Is that it? </st><st c="1992">Just those two lines of code for two chapters? </st><st c="2039">Yes! </st><st c="2044">That shows you how important vectors are to the RAG system. </st><st c="2104">And to thoroughly understand vectors, we start with the fundamentals and build up </st><span class="No-Break"><st c="2186">from there.</st></span></p>
			<p><st c="2197">Let’s </st><span class="No-Break"><st c="2204">get started!</st></span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/><st c="2216">Fundamentals of vectors in RAG</st></h1>
			<p><st c="2247">In this </st><a id="_idIndexMarker296"/><st c="2256">section, we</st><a id="_idIndexMarker297"/><st c="2267"> will cover several important topics related to vectors and embeddings in the context of </st><strong class="bold"><st c="2356">natural language processing</st></strong><st c="2383"> (</st><strong class="bold"><st c="2385">NLP</st></strong><st c="2388">) and RAG. </st><st c="2400">We will begin by clarifying the relationship between vectors and embeddings, explaining that embeddings are a specific type of vector representation used in NLP. </st><st c="2562">We then discuss the properties of vectors, such as their dimensions and size, and how these characteristics impact the precision and effectiveness of text search and </st><span class="No-Break"><st c="2728">similarity comparisons.</st></span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor126"/><st c="2751">What is the difference between embeddings and vectors?</st></h2>
			<p><st c="2806">Vectors </st><a id="_idIndexMarker298"/><st c="2815">and </st><strong class="bold"><st c="2819">embeddings</st></strong><st c="2829"> are key concepts in NLP and play a crucial role in building language models and RAG systems. </st><st c="2923">But what are they and how do they relate to each other? </st><st c="2979">To put it simply, you can think of embeddings as a specific type of vector representation. </st><st c="3070">When we are talking about the </st><strong class="bold"><st c="3100">large language models</st></strong><st c="3121"> (</st><strong class="bold"><st c="3123">LLMs</st></strong><st c="3127">) we </st><a id="_idIndexMarker299"/><st c="3133">use in RAG, which are part of a larger universe called NLP, the vectors we use are referred to as embeddings. </st><st c="3243">Vectors on the other hand, in general, are used </st><a id="_idIndexMarker300"/><st c="3291">across a broad variety of fields and can represent many other objects beyond just language constructs (such as words, sentences, paragraphs, etc.). </st><st c="3439">When talking about RAG, words such as embeddings, vectors, vector embeddings, and embedding vectors can be </st><span class="No-Break"><st c="3546">used interchangeably!</st></span></p>
			<p><st c="3567">Now that we have that out of the way, let’s talk about what a vector </st><span class="No-Break"><st c="3637">actually is.</st></span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/><st c="3649">What is a vector?</st></h2>
			<p><st c="3667">What is </st><a id="_idIndexMarker301"/><st c="3676">the first thing you think of when you hear the word </st><em class="italic"><st c="3728">vector</st></em><st c="3734">? Many people would say math. </st><st c="3764">That would be accurate; vectors are literally mathematical representations of the text we work with in our data, and they allow us to apply mathematical operations to our data in new and very </st><span class="No-Break"><st c="3956">useful ways.</st></span></p>
			<p><st c="3968">The word </st><em class="italic"><st c="3978">vector</st></em><st c="3984"> might also make you think of speed. </st><st c="4021">That is also accurate; with vectors, we can conduct text search at significantly faster speeds than with any other technology that preceded </st><span class="No-Break"><st c="4161">vector search.</st></span></p>
			<p><st c="4175">Another concept that is often associated with the word </st><em class="italic"><st c="4231">vector</st></em><st c="4237"> is precision. </st><st c="4252">By converting text into embeddings that have semantic representation, we can significantly improve the precision of our search systems in finding what we are </st><span class="No-Break"><st c="4410">looking for.</st></span></p>
			<p><st c="4422">And of course, if you are a fan of the movie </st><em class="italic"><st c="4468">Despicable Me</st></em><st c="4481"> from Illumination, you may think of the villain Vector, who describes himself as “</st><em class="italic"><st c="4564">I go by the name of… Vector. </st><st c="4594">It’s a mathematical term, a quantity represented by an arrow with both direction </st></em><span class="No-Break"><em class="italic"><st c="4675">and magnitude</st></em></span><span class="No-Break"><st c="4688">.”</st></span></p>
			<p><st c="4690">He may be a villain doing questionable things, but he is right about the meaning behind his name! </st><st c="4789">The key thing to take away from this description is that a vector is not just a bunch of numbers; it is a mathematical object that represents both magnitude and direction. </st><st c="4961">This is why it does a better job of representing your text and similarities between text, as it captures a more complex form of them than just </st><span class="No-Break"><st c="5104">simple numbers.</st></span></p>
			<p><st c="5119">This may give you an understanding of what a vector is, but let’s next discuss the important aspects of vectors that will have an impact on your RAG development, starting with </st><span class="No-Break"><st c="5296">vector size.</st></span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/><st c="5308">Vector dimensions and size</st></h2>
			<p><st c="5335">Vector, the </st><a id="_idIndexMarker302"/><st c="5348">villain from </st><em class="italic"><st c="5361">Despicable Me</st></em><st c="5374">, said that vectors are “</st><em class="italic"><st c="5399">a quantity represented by an arrow</st></em><st c="5434">.” But while thinking of arrows representing vectors on a 2D or 3D graph makes it easier to comprehend what a vector is, it is important to</st><a id="_idIndexMarker303"/><st c="5573"> understand that the vectors we work with are often represented in many more than just two or three dimensions. </st><st c="5685">The number of dimensions in the vector is also referred to as the vector size. </st><st c="5764">To see this in our code, we are going to add a new cell right below where we define our variables. </st><st c="5863">This code will print out a small section of the </st><span class="No-Break"><strong class="source-inline"><st c="5911">embedding</st></strong></span><span class="No-Break"><st c="5920"> vector:</st></span></p>
			<pre class="source-code"><st c="5928">
question = "What are the advantages of using RAG?"
</st><st c="5980">question_embedding=embedding_function.embed_query(question)first_5_numbers = question_embedding[:5]
print(f"User question embedding (first 5 dimensions):
    {first_5_numbers}")</st></pre>
			<p><st c="6153">In this code, we take the question that we have used throughout our code examples, </st><strong class="source-inline"><st c="6237">What are the advantages of using RAG?</st></strong><st c="6274">, and we use OpenAI’s embedding API to convert it into a vector representation. </st><st c="6354">The </st><strong class="source-inline"><st c="6358">question_embedding</st></strong><st c="6376"> variable represents this embedding. </st><st c="6413">Using a slice, </st><strong class="source-inline"><st c="6428">[0:5]</st></strong><st c="6433">, we take the first five numbers from </st><strong class="source-inline"><st c="6471">question_embedding</st></strong><st c="6489">, which represent the first five dimensions of the vector, and print them out. </st><st c="6568">The full vector is 1,536 float numbers with 17–20 digits each, so we will minimize how much is printed out to make it a little more manageable to read. </st><st c="6720">The output of this cell will look </st><span class="No-Break"><st c="6754">like this:</st></span></p>
			<pre class="source-code"><st c="6764">
User question embedding (first 5 dim): [
-0.006319054113595048, -0.0023517232115089787, 0.015498643243434815, -0.02267445873596028, 0.017820641897159206]</st></pre>
			<p><st c="6918">We only print out the first five dimensions here, but the embedding is much larger than that. </st><st c="7013">We will talk about a practical way to determine the total number of dimensions in a moment, but first I want to draw your attention to the length of </st><span class="No-Break"><st c="7162">each number.</st></span></p>
			<p><st c="7174">All numbers in these embeddings will be +/-0 with a decimal point, so let’s talk about the number of digits that come after that decimal point. </st><st c="7319">The first number here, </st><strong class="source-inline"><st c="7342">-0.006319054113595048</st></strong><st c="7363">, has 18 digits after the decimal point, the second number has 19, and the fourth number has 17. </st><st c="7460">These digit lengths are related to the precision of the floating-point representation used by OpenAI’s embeddings model, </st><strong class="source-inline"><st c="7581">OpenAIEmbeddings</st></strong><st c="7597">. This model uses what is considered a high-precision floating-point format, providing 64-bit numbers (also known as </st><strong class="bold"><st c="7714">double-precision</st></strong><st c="7730">). </st><st c="7734">This high-precision results in </st><a id="_idIndexMarker304"/><st c="7765">the potential for very fine-grained distinctions and accurate representation of the semantic information captured by the </st><span class="No-Break"><st c="7886">embedding model.</st></span></p>
			<p><st c="7902">In addition, let’s revisit a point made in </st><a href="B22475_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><st c="7946">Chapter 1</st></em></span></a><st c="7955">, that the preceding output looks a lot like a Python list of floating points. </st><st c="8034">It actually is a Python list in this case, as that is what OpenAI returns from their embedding API. </st><st c="8134">This is probably a decision to make it more compatible with the Python coding world. </st><st c="8219">But to avoid confusion, it is important to understand that, typically in the machine learning world, when you see something like this in use that will be used for machine learning-related processing, it is typically a NumPy array, even though a list of numbers and a NumPy array look the same when printed out as output like we </st><span class="No-Break"><st c="8547">just did.</st></span></p>
			<p class="callout-heading"><st c="8556">Fun fact</st></p>
			<p class="callout"><st c="8565">You will eventually hear about the</st><a id="_idIndexMarker305"/><st c="8600"> concept called </st><strong class="bold"><st c="8616">quantization</st></strong><st c="8628"> if you work with generative AI. </st><st c="8661">Much like embeddings, quantization deals with high-precision floating points. </st><st c="8739">However, with quantization, the concept is to convert model parameters, such as weights and activations, from their original high-precision floating-point representation to a lower-precision format. </st><st c="8938">This reduces the memory footprint and computational requirements of the LLM, which can be applied to make it more cost-effective to pre-train, train, and fine-tune the LLM. </st><st c="9111">Quantization can also make it more cost-effective to perform inference with the LLM, which is what it is called when you use the LLM to get responses. </st><st c="9262">When I say </st><em class="italic"><st c="9273">cost-effective</st></em><st c="9287"> in this context, I am referring to being able to do these things in a</st><a id="_idIndexMarker306"/><st c="9357"> smaller, less expensive hardware environment. </st><st c="9404">There is a trade-off, though; quantization is a </st><strong class="bold"><st c="9452">lossy compression technique</st></strong><st c="9479">, which means that some of the information is lost during the conversion process. </st><st c="9561">The reduced precision of the quantized LLMs may result in a loss of accuracy compared to the original </st><span class="No-Break"><st c="9663">high-precision LLMs.</st></span></p>
			<p><st c="9683">When you are using RAG and considering different algorithms to convert your text into embeddings, take note of the length of the embedding values to make sure you are using a high-precision floating-point format if the accuracy and quality of response are of high priority in your </st><span class="No-Break"><st c="9965">RAG system.</st></span></p>
			<p><st c="9976">But how many dimensions are represented by these embeddings? </st><st c="10038">We only show five in the preceding example, but we could have printed them all out and counted them individually. </st><st c="10152">This, of course, seems impractical. </st><st c="10188">We will use the </st><strong class="source-inline"><st c="10204">len()</st></strong><st c="10209"> function to do the counting for us. </st><st c="10246">In the following code, you see that helpful function put to good use, giving us the total size of </st><span class="No-Break"><st c="10344">this embedding:</st></span></p>
			<pre class="source-code"><st c="10359">
embedding_size = len(question_embedding)
print(f"Embedding size: {embedding_size}")</st></pre>
			<p><st c="10443">The output of this code is </st><span class="No-Break"><st c="10471">as follows:</st></span></p>
			<pre class="source-code"><st c="10482">
Embedding size: 1536</st></pre>
			<p><st c="10503">This indicates that this embedding is 1,536 dimensions! </st><st c="10560">Trying to visualize this in your mind is difficult when we typically only think in 3 dimensions at most, but these extra 1,533 dimensions make a significant difference in how precise our embedding semantic representations of the related text </st><span class="No-Break"><st c="10802">can be.</st></span></p>
			<p><st c="10809">When working with vectors across most modern vectorization algorithms, there are often hundreds, or thousands, of dimensions. </st><st c="10936">The number of dimensions is equal to the number of floating points that represent the embedding, meaning a 1,024-dimension vector is represented by 1,024 floating points. </st><st c="11107">There is no hard limit to how long an embedding can be, but some of the modern vectorizing algorithms tend to have preset sizes. </st><st c="11236">The model we are using, OpenAI’s </st><strong class="source-inline"><st c="11269">ada</st></strong><st c="11272"> embedding model, uses 1,536 by default. </st><st c="11313">This is because it is trained to produce a certain-sized embedding, and if you try to truncate that size, it changes the context captured in </st><span class="No-Break"><st c="11454">the embedding.</st></span></p>
			<p><st c="11468">However, this is changing. </st><st c="11496">New vectorizers are now available (such as the OpenAI </st><strong class="source-inline"><st c="11550">text-embedding-3-large</st></strong><st c="11572"> model) that enable you to change vector sizes. </st><st c="11620">These embedding models were trained to provide the same context, relatively speaking across the different vector dimension sizes. </st><st c="11750">This enables a </st><a id="_idIndexMarker307"/><st c="11765">technique called </st><span class="No-Break"><strong class="bold"><st c="11782">adaptive retrieval</st></strong></span><span class="No-Break"><st c="11800">.</st></span></p>
			<p><st c="11801">With adaptive retrieval, you generate multiple sets of embeddings at different sizes. </st><st c="11888">You first search the lower-dimension vectors to get you </st><em class="italic"><st c="11944">close</st></em><st c="11949"> to the final results, because searching lower-dimension vectors is much faster than searching higher-dimension vectors. </st><st c="12070">Once your lower-dimension search gets you into proximity of the content most similar to your input inquiry, your search </st><em class="italic"><st c="12190">adapts</st></em><st c="12196"> to searching the slower search-speed, higher-dimension embeddings to target the most relevant content and finalize the similarity search. </st><st c="12335">Overall, this can increase your search speeds by 30–90%, depending on how you set up the search. </st><st c="12432">The embeddings generated by this technique are called </st><strong class="bold"><st c="12486">Matryoshka embeddings</st></strong><st c="12507">, named</st><a id="_idIndexMarker308"/><st c="12514"> after the Russian nesting dolls, reflecting that the embeddings, like the dolls, are all relatively identical to each other while varying in size. </st><st c="12662">If you ever need to optimize a RAG pipeline in a production environment for heavy usage, you are going to want to consider </st><span class="No-Break"><st c="12785">this technique.</st></span></p>
			<p><st c="12800">The next concept that will be important for you to understand is where in the code your vectors reside, helping you to apply the concepts you are learning about vectors directly to your </st><span class="No-Break"><st c="12987">RAG efforts.</st></span></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor129"/><st c="12999">Where vectors lurk in your code</st></h1>
			<p><st c="13031">One way to </st><a id="_idIndexMarker309"/><st c="13043">indicate the value of vectors in the RAG system is to show you all the places they are used. </st><st c="13136">As discussed earlier, you start with your text data and convert it to vectors during the vectorization process. </st><st c="13248">This occurs in the indexing stage of the RAG system. </st><st c="13301">But, in most cases, you must have somewhere to put those embedding vectors, which brings in the concept of the </st><span class="No-Break"><st c="13412">vector store.</st></span></p>
			<p><st c="13425">During the retrieval stage of the RAG system, you start with a question as input from the user, which is first converted to an embedding vector before the retrieval begins. </st><st c="13599">Lastly, the retrieval process uses a similarity algorithm that determines the proximity between the question embedding and all the embeddings in the vector store. </st><st c="13762">There is one more potential area in which vectors are common and that is when you want to evaluate your RAG responses, but we will cover that in </st><a href="B22475_09.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><st c="13907">Chapter 9</st></em></span></a><st c="13916"> when we cover evaluation techniques. </st><st c="13954">For now, let’s dive deeper into each of these other concepts, starting </st><span class="No-Break"><st c="14025">with vectorization.</st></span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor130"/><st c="14044">Vectorization occurs in two places</st></h2>
			<p><st c="14079">At the very front of </st><a id="_idIndexMarker310"/><st c="14101">the RAG process, you typically have a mechanism for a user to enter a question that is passed to the retriever. </st><st c="14213">We see the processing of this occurring in our </st><span class="No-Break"><st c="14260">code here:</st></span></p>
			<pre class="source-code"><st c="14270">
rag_chain_with_source = RunnableParallel(
    {"context": </st><strong class="bold"><st c="14325">retriever</st></strong><st c="14334">,
     "question":RunnablePassthrough()}
).assign(answer=rag_chain_from_docs)</st></pre>
			<p><st c="14406">The</st><a id="_idIndexMarker311"/><st c="14410"> retriever is a LangChain </st><strong class="source-inline"><st c="14436">retriever</st></strong><st c="14445"> object that facilitates similarity search and retrieval of relevant vectors based on the user query. </st><st c="14547">So when we talk about vectorization, it actually occurs in two places in </st><span class="No-Break"><st c="14620">our code:</st></span></p>
			<ul>
				<li><st c="14629">First, when we vectorize the original data that will be used in the </st><span class="No-Break"><st c="14698">RAG system</st></span></li>
				<li><st c="14708">Second, when we need to vectorize the </st><span class="No-Break"><st c="14747">user query</st></span></li>
			</ul>
			<p><st c="14757">The relationship between these two separate steps is that they are both used in the similarity search. </st><st c="14861">Before we talk about the search, though, let’s first talk about where the latter group of embeddings, the embeddings from the original data, is stored: the </st><span class="No-Break"><st c="15017">vector store.</st></span></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor131"/><st c="15030">Vector databases/stores store and contain vectors</st></h2>
			<p><st c="15080">A vector store</st><a id="_idIndexMarker312"/><st c="15095"> is typically a vector database (but not always, see the following note) that is optimized for storing and serving vectors, and plays a crucial role in an effective RAG system. </st><st c="15272">Technically, you could build a RAG system without using a vector database, but you would miss out on a lot of the optimizations that have been built into these data storage tools, impacting your memory, computation requirements, and search </st><span class="No-Break"><st c="15512">precision unnecessarily.</st></span></p>
			<p class="callout-heading"><st c="15536">Note</st></p>
			<p class="callout"><st c="15541">You often hear the term </st><strong class="bold"><st c="15566">vector databases</st></strong><st c="15582"> when</st><a id="_idIndexMarker313"/><st c="15587"> referring to optimized database-like structures for storing vectors. </st><st c="15657">However, there are tools and other mechanisms that are not databases while serving the same or similar purpose as a vector database. </st><st c="15790">For this reason, we will refer to all of them in a group as </st><em class="italic"><st c="15850">vector stores</st></em><st c="15863">. This is consistent with LangChain documentation, which also refers to the group in aggregate as vector stores, inclusive of all types of mechanisms that store and serve vectors. </st><st c="16043">But you will often hear the terms used interchangeably, and the term </st><em class="italic"><st c="16112">vector database</st></em><st c="16127"> is actually the more popular term used to refer to all of these mechanisms. </st><st c="16204">For the sake of accuracy and to align our terminology with LangChain documentation, in this book, we will use the term </st><span class="No-Break"><em class="italic"><st c="16323">vector store</st></em></span><span class="No-Break"><st c="16335">.</st></span></p>
			<p><st c="16336">In terms of </st><em class="italic"><st c="16349">where vectors lurk in your code</st></em><st c="16380">, the vector store is where most of the vectors generated in </st><a id="_idIndexMarker314"/><st c="16441">your code are stored. </st><st c="16463">When you vectorize your data, those embeddings go into your vector store. </st><st c="16537">When you conduct a similarity search, the embeddings used to represent that data get pulled from the vector store. </st><st c="16652">This makes vector stores a key player in the RAG system and worthy of </st><span class="No-Break"><st c="16722">our attention.</st></span></p>
			<p><st c="16736">Now that we know where the original data embeddings are stored, let’s bring this back to how they are used in relation to the user </st><span class="No-Break"><st c="16868">query embeddings.</st></span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/><st c="16885">Vector similarity compares your vectors</st></h2>
			<p><st c="16925">We have our two primary </st><span class="No-Break"><st c="16950">vectorization occurrences:</st></span></p>
			<ul>
				<li><st c="16976">The embedding for our </st><span class="No-Break"><st c="16999">user query</st></span></li>
				<li><st c="17009">The vector embeddings representing all the data in our </st><span class="No-Break"><st c="17065">vector store</st></span></li>
			</ul>
			<p><st c="17077">Let’s </st><a id="_idIndexMarker315"/><st c="17084">review how these two occurrences relate to each other. </st><st c="17139">When we conduct the highly important vector similarity search that forms the foundation of our retrieval process, we are really just performing a mathematical operation that measures the distance between the user query embedding and the original </st><span class="No-Break"><st c="17385">data embeddings.</st></span></p>
			<p><st c="17401">Multiple mathematical algorithms can be used to perform this distance calculation, which we will review later in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="17515">Chapter 8</st></em></span></a><st c="17524">. But for now, it is important to understand that this distance calculation identifies the closest original data embeddings to the user query embedding and returns the list of those embeddings in the order of their distance (sorted by closest to furthest). </st><st c="17781">Our code is a bit more simplistic, in that the embedding represents the data points (the chunks) in a </st><span class="No-Break"><st c="17883">1:1 relationship.</st></span></p>
			<p><st c="17900">But in many applications, such as with a question-and-answer chatbot where the questions or answers are very long and broken up into smaller chunks, you will likely see those chunks have a foreign key ID that refers back to a larger piece of content. </st><st c="18152">That allows us to retrieve the full piece of content, rather than just the chunk. </st><st c="18234">This will vary depending on the problem your RAG system is trying to solve, but it is important to understand that the architecture of this retrieval system can vary to meet the needs of </st><span class="No-Break"><st c="18421">the application.</st></span></p>
			<p><st c="18437">This covers the most common places you find vectors in your RAG system: where they occur, where they are stored, and how they are used in service of the RAG system. </st><st c="18603">In the next section, we talk about how the size of the data text we are using in the search for our RAG system can vary. </st><st c="18724">You will ultimately make decisions in your code that dictate that size. </st><st c="18796">But from what you already know about vectors, you may start to wonder, if we are vectorizing content of various sizes, how does that impact our ability to compare them and ultimately build the most effective retrieval process we can build? </st><st c="19036">And you would be right to wonder! </st><st c="19070">Let’s discuss the impact of the size of the content that we turn into </st><span class="No-Break"><st c="19140">embeddings next.</st></span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor133"/><st c="19156">The amount of text you vectorize matters!</st></h1>
			<p><st c="19198">The vector we showed earlier</st><a id="_idIndexMarker316"/><st c="19227"> came from the text </st><strong class="source-inline"><st c="19247">What are the advantages of using RAG?</st></strong><st c="19284">. That is a relatively short amount of text, which means a 1,536-dimension vector is going to do a very thorough job representing the context within that text. </st><st c="19444">But if we go back to the code, the content that we vectorize to represent our </st><em class="italic"><st c="19522">data</st></em><st c="19526"> comes </st><span class="No-Break"><st c="19533">from here:</st></span></p>
			<pre class="source-code"><st c="19543">
loader = WebBaseLoader(
    web_paths=("https://kbourne.github.io/chapter1.html",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title",
                    "post-header")
        )
    ),
)
docs = loader.load()</st></pre>
			<p><st c="19749">This pulls in the web page we looked at in previous chapters, which is relatively long compared to the question text. </st><st c="19868">To make that data more manageable, we break that content up into chunks using a text splitter in </st><span class="No-Break"><st c="19965">this code:</st></span></p>
			<pre class="source-code"><st c="19975">
text_splitter = SemanticChunker(embedding_function)
splits = text_splitter.split_documents(docs)</st></pre>
			<p><st c="20072">If you were to pull out the third chunk using </st><strong class="source-inline"><st c="20119">splits[2]</st></strong><st c="20128">, it would look </st><span class="No-Break"><st c="20144">like this:</st></span></p>
			<pre class="source-code"><st c="20154">
There are also generative models that generate images from text prompts, while others generate video from text prompts. </st><st c="20275">There are other models that generate text descriptions from images. </st><st c="20343">We will talk about these other types of models in Chapter 16, Going Beyond the LLM. </st><st c="20427">But for most of the book, I felt it would keep things simple and let you focus on the core principles of RAG if we focus on the type of model that most RAG pipelines use, the LLM. </st><st c="20607">But I did want to make sure it was clear, that while the book focuses primarily on LLMs, RAG can also be applied to other types of generative models, such as those for images and videos. </st><st c="20794">Some popular examples of LLMs are the OpenAI ChatGPT models, the Meta LLaMA models, Google's PaLM and Gemini models, and Anthropic's Claude models. </st><st c="20942">Foundation model\nA foundation model is the base model for most LLMs. </st><st c="21012">In the case of ChatGPT, the foundation model is based on the GPT (Generative Pre-trained Transformer) architecture, and it was fine-tuned for Chat. </st><st c="21160">The specific model used for ChatGPT is not publicly disclosed. </st><st c="21223">The base GPT model cannot talk with you in chatbot-style like ChatGPT does. </st><st c="21299">It had to get further trained to gain that skill.</st></pre>
			<p><st c="21348">I chose the third chunk to show because it is a relatively short chunk. </st><st c="21421">Most of the chunks are much larger. </st><st c="21457">The </st><strong class="bold"><st c="21461">Semantic Chunker text splitter</st></strong><st c="21491"> we use attempts to use semantics to </st><a id="_idIndexMarker317"/><st c="21528">determine how to split up the text, using embeddings to determine those semantics. </st><st c="21611">In theory, this should give us chunks that do a better job of breaking up the data based on context, rather than just an </st><span class="No-Break"><st c="21732">arbitrary size.</st></span></p>
			<p><st c="21747">However, there</st><a id="_idIndexMarker318"/><st c="21762"> is an important concept to understand when it comes to embeddings that will impact the splitter you choose and the size of your embeddings in general. </st><st c="21914">This all stems from the fact that no matter how large the text that you pass to the vectorization algorithm is, it is still going to give you an embedding that is the same size as any of the other embeddings. </st><st c="22123">In this case, that means the user query embedding is going to be 1,536 dimensions, but all those long sections of text in the vector store are also going to be 1,536 dimensions, even though their actual length in text format is quite different. </st><st c="22368">It may be counter-intuitive, but in an amazing turn of events, it </st><span class="No-Break"><st c="22434">works well!</st></span></p>
			<p><st c="22445">When conducting a search with the user query of the vector store, the mathematical representations of the user query embedding and the other embeddings are done in such a way that we are still able to detect the semantic similarities between them, despite the large disparity in their size. </st><st c="22737">This aspect of the vector similarity search is the kind of thing that makes mathematicians love math so much. </st><st c="22847">It just seems to defy all logic that you can turn text of very different sizes into numbers and be able to detect similarities </st><span class="No-Break"><st c="22974">between them.</st></span></p>
			<p><st c="22987">But there is </st><a id="_idIndexMarker319"/><st c="23001">another aspect of this to consider as well—when you compare the results across just the chunks that you break your data into, the size of those chunks will matter. </st><st c="23165">In this case, the larger the amount of content that is being vectorized, the more diluted the embedding will be. </st><st c="23278">On the other hand, the smaller the amount of content that the embedding represents, the less context you will have to match up when you perform a vector similarity search. </st><st c="23450">For each of your RAG implementations, you will need to find a delicate balance between chunk size and </st><span class="No-Break"><st c="23552">context representation.</st></span></p>
			<p><st c="23575">Understanding this will help you make better decisions about how you split data and the vectorization algorithms you choose when trying to improve your RAG system. </st><st c="23740">We will cover some other techniques to get more out of your splitting/chunking strategy in </st><a href="B22475_11.xhtml#_idTextAnchor229"><span class="No-Break"><em class="italic"><st c="23831">Chapter 11</st></em></span></a><st c="23841"> when we talk about LangChain splitters. </st><st c="23882">Next, we will talk about the importance of testing different </st><span class="No-Break"><st c="23943">vectorization models.</st></span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/><st c="23964">Not all semantics are created equal!</st></h1>
			<p><st c="24001">A common mistake made in RAG applications is choosing the first vectorization algorithm that is implemented and just assuming that provides the best results. </st><st c="24160">These algorithms take the semantic meaning of text and represent them mathematically. </st><st c="24246">However, these algorithms are generally large NLP models themselves, and they can vary in capabilities and quality as much as the LLMs. </st><st c="24382">Just as we, as humans, often find it challenging to comprehend the intricacies and nuances of text, these models can grapple with the same challenge, having varying abilities to grasp the complexities inherent in written language. </st><st c="24613">For example, models in the past could not decipher the difference between </st><strong class="source-inline"><st c="24687">bark</st></strong><st c="24691"> (a dog noise) and </st><strong class="source-inline"><st c="24710">bark</st></strong><st c="24714"> (the outer part of most trees), but newer models can detect this based on the surrounding text and the context in which it is used. </st><st c="24847">This area of the field is adapting and evolving just as fast as </st><span class="No-Break"><st c="24911">other areas.</st></span></p>
			<p><st c="24923">In some cases, it is possible that a domain-specific vectorization model, such as one trained on scientific papers, is going to do better in an app that is focused on scientific papers than using a generic vectorization model. </st><st c="25151">Scientists talk in very specific ways, very different from what you might see on social media, and so a giant model trained on general web-based text may not perform well in this </st><span class="No-Break"><st c="25330">specific domain.</st></span></p>
			<p class="callout-heading"><st c="25346">Fun fact</st></p>
			<p class="callout"><st c="25355">You often hear about how you can fine-tune LLMs to improve your domain-specific results. </st><st c="25445">But did you know that you can also fine-tune embedding models? </st><st c="25508">Fine-tuning an embedding model has the potential to improve the way the embedding model understands your domain-specific data and, therefore, has the potential to improve your similarity search results. </st><st c="25711">This has the potential to improve your entire RAG system substantially for </st><span class="No-Break"><st c="25786">your domain.</st></span></p>
			<p><st c="25798">To summarize this section on fundamentals, numerous aspects of vectors can help you or hurt you when trying to build the most effective RAG application for your needs. </st><st c="25967">Of course, it would be poor manners for me to tell you how important the vectorization algorithm is without telling you which ones are available! </st><st c="26113">To address this, in this next section, let’s run through a list of some of the most popular vectorization techniques! </st><st c="26231">We will even do this </st><span class="No-Break"><st c="26252">with code!</st></span></p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/><st c="26262">Code lab 7.1 – Common vectorization techniques</st></h1>
			<p><st c="26309">Vectorization algorithms </st><a id="_idIndexMarker320"/><st c="26335">have evolved significantly over the past few decades. </st><st c="26389">Understanding how these have changed, and why, will help you gain more perspective on how to choose the one that fits your needs the most. </st><st c="26528">Let’s walk through some of these vectorization algorithms, starting with some of the earliest ones and ending with the most recent, more advanced options. </st><st c="26683">This is nowhere close to an exhaustive list, but these select few should be enough to give you a sense of where this part of the field came from and where it is going. </st><st c="26851">Before we start, let’s install and import some new Python packages that play important roles in our coding journey through </st><span class="No-Break"><st c="26974">vectorization techniques:</st></span></p>
			<pre class="source-code"><st c="26999">
%pip install gensim --user
%pip install transformers
%pip install torch</st></pre>
			<p><st c="27071">This code should go near the top of the previous code in the same cell as the other </st><span class="No-Break"><st c="27156">package installations.</st></span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/><st c="27178">Term frequency-inverse document frequency (TF-IDF)</st></h2>
			<p><st c="27229">1972 was probably much sooner a time than what you would expect in a book about a relatively </st><a id="_idIndexMarker321"/><st c="27323">brand-new technology like RAG, but this is where we find the roots of the vectorization </st><a id="_idIndexMarker322"/><st c="27411">techniques we are going to </st><span class="No-Break"><st c="27438">talk about.</st></span></p>
			<p><st c="27449">Karen Ida Boalth Spärck Jones was a self-taught programmer and pioneering British computer scientist who worked on several papers focused on the field of NLP. </st><st c="27609">In 1972, she made one of her most important contributions, introducing the concept of </st><strong class="bold"><st c="27695">inverse document frequency</st></strong><st c="27721"> (</st><strong class="bold"><st c="27723">IDF</st></strong><st c="27726">). </st><st c="27730">The</st><a id="_idIndexMarker323"/><st c="27733"> basic concept as she stated was that “</st><em class="italic"><st c="27772">the specificity of a term can be quantified as an inverse function of the number of documents in which </st></em><span class="No-Break"><em class="italic"><st c="27876">it occurs</st></em></span><span class="No-Break"><st c="27885">.”</st></span></p>
			<p><st c="27887">As a real-world example, consider applying the </st><strong class="source-inline"><st c="27935">df</st></strong><st c="27937"> (document frequency) and </st><strong class="source-inline"><st c="27963">idf</st></strong><st c="27966"> (inverse document frequency) score to some words in Shakespeare’s 37 plays and you will find that the word </st><strong class="source-inline"><st c="28074">Romeo</st></strong><st c="28079"> is the highest-scoring result. </st><st c="28111">This is because it appears very frequently, but only in one </st><em class="italic"><st c="28171">document</st></em><st c="28179">, the </st><strong class="source-inline"><st c="28185">Romeo and Juliet</st></strong><st c="28201"> document. </st><st c="28212">In this case, </st><strong class="source-inline"><st c="28226">Romeo</st></strong><st c="28231"> would be scored </st><strong class="source-inline"><st c="28248">1</st></strong><st c="28249"> for </st><strong class="source-inline"><st c="28254">df</st></strong><st c="28256">, as it appeared in 1 document. </st><strong class="source-inline"><st c="28288">Romeo</st></strong><st c="28293"> would score </st><strong class="source-inline"><st c="28306">1.57</st></strong><st c="28310"> for </st><strong class="source-inline"><st c="28315">idf</st></strong><st c="28318">, higher than any other word because of its high frequency in that one document. </st><st c="28399">Meanwhile, Shakespeare used the word </st><strong class="source-inline"><st c="28436">sweet</st></strong><st c="28441"> occasionally but in every single play, giving it a low score. </st><st c="28504">This gives </st><strong class="source-inline"><st c="28515">sweet</st></strong><st c="28520"> a </st><strong class="source-inline"><st c="28523">df</st></strong><st c="28525"> score of </st><strong class="source-inline"><st c="28535">37</st></strong><st c="28537">, and an </st><strong class="source-inline"><st c="28546">idf</st></strong><st c="28549"> score of </st><strong class="source-inline"><st c="28559">0</st></strong><st c="28560">. What Karen Jones was saying in her paper was that when you see words such as </st><strong class="source-inline"><st c="28639">Romeo</st></strong><st c="28644"> appear in just a small number of the overall number of plays, you can take the plays where those words appear and consider them very important and predictive of what that play is about. </st><st c="28831">In contrast, </st><strong class="source-inline"><st c="28844">sweet</st></strong><st c="28849"> had the opposite effect, as it is uninformative in terms of the importance of the word and of the documents that the word </st><span class="No-Break"><st c="28972">is in.</st></span></p>
			<p><st c="28978">But that’s enough talk. </st><st c="29003">Let’s see how this algorithm looks in code! </st><st c="29047">The scikit-learn library has a function that can be applied to text to vectorize that text using the TF-IDF method. </st><st c="29163">The following code is where we define the </st><strong class="source-inline"><st c="29205">splits</st></strong><st c="29211"> variable, which is what we will use as our data to</st><a id="_idIndexMarker324"/><st c="29262"> train the </st><span class="No-Break"><st c="29273">model on:</st></span></p>
			<pre class="source-code"><st c="29282">
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
tfidf_documents = [split.page_content for split in splits]
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(
    tfidf_documents)
vocab = tfidf_vectorizer.get_feature_names_out()
tf_values = tfidf_matrix.toarray()
idf_values = tfidf_vectorizer.idf_
word_stats = list(zip(vocab, tf_values.sum(axis=0),
    idf_values))
word_stats.sort(key=lambda x: x[2], reverse=True)
print("Word\t\tTF\t\tIDF")
print("----\t\t--\t\t---")
for word, tf, idf in word_stats[:10]:
         print(f"{word:&lt;12}\t{tf:.2f}\t\t{idf:.2f}")</st></pre>
			<p><st c="29927">Unlike </st><a id="_idIndexMarker325"/><st c="29935">the OpenAI embedding model, this model requires you to </st><em class="italic"><st c="29990">train</st></em><st c="29995"> on your </st><em class="italic"><st c="30004">corpus</st></em><st c="30010"> data, which is a fancy term for all the text data you have available to train with. </st><st c="30095">This code is primarily to demonstrate how a TD-IDF model is used compared to our current RAG pipeline retriever, so we won’t review it line by line. </st><st c="30244">But we encourage you to try out the code yourself and try </st><span class="No-Break"><st c="30302">different settings.</st></span></p>
			<p><st c="30321">It should be noted that the vectors this algorithm produces are</st><a id="_idIndexMarker326"/><st c="30385"> called </st><strong class="bold"><st c="30393">sparse vectors</st></strong><st c="30407">, and the vectors we were previously working with in previous code labs were </st><a id="_idIndexMarker327"/><st c="30484">called </st><strong class="bold"><st c="30491">dense vectors</st></strong><st c="30504">. This is an important distinction that we will review in detail in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="30572">Chapter 8</st></em></span></a><span class="No-Break"><st c="30581">.</st></span></p>
			<p><st c="30582">This model uses the corpus data to set up the environment that can then calculate the embeddings for </st><a id="_idIndexMarker328"/><st c="30684">new content that you introduce to it. </st><st c="30722">The output should look like the </st><span class="No-Break"><st c="30754">following table:</st></span></p>
			<pre class="source-code"><st c="30770">
Word                     TF    IDF
000                      0.16  2.95
1024                     0.04  2.95
123                      0.02  2.95
13                       0.04  2.95
15                       0.01  2.95
16                       0.07  2.95
192                      0.06  2.95
1m                       0.08  2.95
200                      0.08  2.95
2024                     0.01  2.95</st></pre>
			<p><st c="30920">In this case, we</st><a id="_idIndexMarker329"/><st c="30937"> see at least a 10-way tie for the </st><strong class="source-inline"><st c="30972">idf</st></strong><st c="30975"> highest value (we are only showing 10, so there are probably more), and all of them are number-based text. </st><st c="31083">This does not seem particularly useful, but this is primarily because our corpus data is so small. </st><st c="31182">Training on more data from the same author or domain can help you build a model that has a better contextual understanding of the </st><span class="No-Break"><st c="31312">underlying content.</st></span></p>
			<p><st c="31331">Now, going back to the original question that we have been using, </st><strong class="source-inline"><st c="31398">What are the advantages of RAG?</st></strong><st c="31429">, we want to use the TF-IDF embeddings to determine what the most relevant </st><span class="No-Break"><st c="31504">documents are:</st></span></p>
			<pre class="source-code"><st c="31518">
tfidf_user_query = ["What are the advantages of RAG?"]
new_tfidf_matrix = tfidf_vectorizer.transform(
    tfidf_user_query)
tfidf_similarity_scores = cosine_similarity(
    new_tfidf_matrix, tfidf_matrix)
tfidf_top_doc_index = tfidf_similarity_scores.argmax()
print("TF-IDF Top Document:\n",
    tfidf_documents[tfidf_top_doc_index])</st></pre>
			<p><st c="31840">This replicates </st><a id="_idIndexMarker330"/><st c="31857">the behavior we see with the retriever, where it uses a similarity algorithm to find the nearest embedding by distance. </st><st c="31977">In this case, we use cosine similarity, which</st><a id="_idIndexMarker331"/><st c="32022"> we will talk about in </st><a href="B22475_08.xhtml#_idTextAnchor152"><span class="No-Break"><em class="italic"><st c="32045">Chapter 8</st></em></span></a><st c="32054">, but just keep in mind that there are many distance algorithms that we can use to calculate this distance. </st><st c="32162">Our output from this code is </st><span class="No-Break"><st c="32191">as follows:</st></span></p>
			<pre class="source-code"><st c="32202">
TF-IDF Top Document:
Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? </st><st c="32562">You do not have to imagine it, that is what RAG does…[TRUNCATED FOR BREVITY]</st></pre>
			<p><st c="32638">If you run our original code, which uses the original vector store and retriever, you will see </st><span class="No-Break"><st c="32734">this output:</st></span></p>
			<pre class="source-code"><st c="32746">
Retrieved Document:
Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? </st><st c="33105">You do not have to imagine it, that is what RAG does…[TRUNCATED FOR BREVITY]</st></pre>
			<p><st c="33181">They match! </st><st c="33194">A small algorithm from 1972 trained on our own data in a fraction of a second is just as good as the massive algorithms developed by OpenAI spending billions of dollars to develop them! </st><st c="33380">Okay, let’s slow down, this is definitely NOT the case! </st><st c="33436">The reality is that in real-world scenarios, you will be working with a much larger dataset than we are and much more complicated user queries, and this will benefit from the use of more sophisticated modern </st><span class="No-Break"><st c="33644">embedding techniques.</st></span></p>
			<p><st c="33665">TF-IDF has </st><a id="_idIndexMarker332"/><st c="33677">been </st><a id="_idIndexMarker333"/><st c="33682">very useful over the years. </st><st c="33710">But was it necessary to learn about an algorithm from 1972 when we are talking about the most advanced generative AI models ever built? </st><st c="33846">The answer is BM25. </st><st c="33866">This is just a teaser, but you will learn more about this very popular </st><strong class="bold"><st c="33937">keyword search</st></strong><st c="33951"> algorithm, one</st><a id="_idIndexMarker334"/><st c="33966"> of the most popular algorithms in use today, in the next chapter. </st><st c="34033">And guess what? </st><st c="34049">It is based on TF-IDF! </st><st c="34072">What TF-IDF has a problem with, though, is capturing context and semantics as well as some of the next models we will talk about. </st><st c="34202">Let’s discuss the next major step up: Word2Vec and </st><span class="No-Break"><st c="34253">related algorithms.</st></span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor137"/><st c="34272">Word2Vec, Sentence2Vec, and Doc2Vec</st></h2>
			<p><strong class="bold"><st c="34308">Word2Vec</st></strong><st c="34317"> and </st><a id="_idIndexMarker335"/><st c="34322">similar models introduced an early application</st><a id="_idIndexMarker336"/><st c="34368"> of unsupervised learning, representing a significant step forward in the NLP field. </st><st c="34453">There are multiple </st><em class="italic"><st c="34472">vec</st></em><st c="34475"> models (word, doc, and sentence), where their training was focused on words, documents, or sentences, respectively. </st><st c="34592">These models differ in the level of text they are </st><span class="No-Break"><st c="34642">trained on.</st></span></p>
			<p><st c="34653">Word2Vec focuses on learning vector representations for individual words, capturing their semantic meaning and relationships. </st><strong class="bold"><st c="34780">Doc2Vec</st></strong><st c="34787">, on the</st><a id="_idIndexMarker337"/><st c="34795"> other hand, learns</st><a id="_idIndexMarker338"/><st c="34814"> vector representations for entire documents, allowing it to capture the overall context and theme of a </st><a id="_idIndexMarker339"/><st c="34918">document. </st><strong class="bold"><st c="34928">Sentence2Vec</st></strong><st c="34940"> is similar to Doc2Vec but operates at the </st><a id="_idIndexMarker340"/><st c="34983">sentence level, learning vector representations for individual sentences. </st><st c="35057">While Word2Vec is useful for tasks such as word similarity and analogy, Doc2Vec and Sentence2Vec are more suitable for document-level tasks such as document similarity, classification, </st><span class="No-Break"><st c="35242">and retrieval.</st></span></p>
			<p><st c="35256">Because we are working with larger documents, and not just words or sentences, we are going to select the Doc2Vec model over Word2Vec or Sentence2Vec and train this model to see how it works as our retriever. </st><st c="35466">Like the TD-IDF model, this model can be trained with our data and then we pass the user query to it to see whether we can get similar results for the most similar </st><span class="No-Break"><st c="35630">data chunks.</st></span></p>
			<p><st c="35642">Add this code in a new cell after the TD-IDF </st><span class="No-Break"><st c="35688">code cell:</st></span></p>
			<pre class="source-code"><st c="35698">
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from sklearn.metrics.pairwise import cosine_similarity
doc2vec_documents = [
    split.page_content for split in splits]
doc2vec_tokenized_documents = [
    doc.lower().split() for doc in doc2vec_documents]
doc2vec_tagged_documents = [TaggedDocument(words=doc,
    tags=[str(i)]) for i, doc in enumerate(
    doc2vec_tokenized_documents)]
doc2vec_model = Doc2Vec(doc2vec_tagged_documents,
    vector_size=100, window=5, min_count=1, workers=4)
doc2vec_model.save("doc2vec_model.bin")</st></pre>
			<p><st c="36220">Much like the </st><a id="_idIndexMarker341"/><st c="36235">TD-IDF model, this code is primarily to demonstrate how a </st><a id="_idIndexMarker342"/><st c="36293">Doc2Vec model is used </st><a id="_idIndexMarker343"/><st c="36315">compared to our current RAG pipeline retriever, so we won’t review it line by line, but we encourage you to try out the code yourself and try different settings. </st><st c="36477">This code focuses on training the Doc2Vec model and saving </st><span class="No-Break"><st c="36536">it locally.</st></span></p>
			<p class="callout-heading"><st c="36547">Fun fact</st></p>
			<p class="callout"><st c="36556">Training language models is a hot topic and can be a well-paid profession these days. </st><st c="36643">Have you ever trained a language model? </st><st c="36683">If your answer was </st><em class="italic"><st c="36702">no</st></em><st c="36704">, you would be wrong. </st><st c="36726">Not only did you just train a language model but you have now trained two! </st><st c="36801">Both TF-IDF and Doc2Vec are language models that you just trained. </st><st c="36868">These are relatively basic versions of model training, but you have to start somewhere, and you </st><span class="No-Break"><st c="36964">just did!</st></span></p>
			<p><st c="36973">In this next code, we </st><a id="_idIndexMarker344"/><st c="36996">will use that model on </st><span class="No-Break"><st c="37019">our data:</st></span></p>
			<pre class="source-code"><st c="37028">
loaded_doc2vec_model = Doc2Vec.load("doc2vec_model.bin")
doc2vec_document_vectors = [loaded_doc2vec_model.dv[
    str(i)] for i in range(len(doc2vec_documents))]
doc2vec_user_query = ["What are the advantages of RAG?"]
doc2vec_tokenized_user_query = [content.lower().split() for content in doc2vec_user_query]
doc2vec_user_query_vector = loaded_doc2vec_model.infer_vector(
    doc2vec_tokenized_user_query[0])
doc2vec_similarity_scores = cosine_similarity([
    doc2vec_user_query_vector], doc2vec_document_vectors)
doc2vec_top_doc_index = doc2vec_similarity_scores.argmax()
print("\nDoc2Vec Top Document:\n",
    doc2vec_documents[doc2vec_top_doc_index])</st></pre>
			<p><st c="37668">We separated </st><a id="_idIndexMarker345"/><st c="37682">the code for creating and saving the model from the usage of the model so that you can see how this model can be saved and referenced later. </st><st c="37823">Here is the output from </st><span class="No-Break"><st c="37847">this code:</st></span></p>
			<pre class="source-code"><st c="37857">
Doc2Vec Top Document:
Once you have introduced the new knowledge, it will always have it! </st><st c="37948">It is also how the model was originally created, by training with data, right? </st><st c="38027">That sounds right in theory, but in practice, fine-tuning has been more reliable in teaching a model specialized tasks (like teaching a model how to converse in a certain way), and less reliable for factual recall…[TRUNCATED FOR BREVITY]</st></pre>
			<p><st c="38264">Comparing this to the results from our original retriever shown previously, this model does not return the same result. </st><st c="38385">However, this model was set up with just 100 dimension vectors in </st><span class="No-Break"><st c="38451">this line:</st></span></p>
			<pre class="source-code"><st c="38461">
doc2vec_model = Doc2Vec(doc2vec_tagged_documents,
    vector_size=100, window=5, min_count=1, workers=4)</st></pre>
			<p><st c="38562">What happens when you change </st><strong class="source-inline"><st c="38592">vector_size</st></strong><st c="38603"> in this line to use 1,536, the same vector size as the </st><span class="No-Break"><st c="38659">OpenAI model?</st></span></p>
			<p><st c="38672">Change the </st><strong class="source-inline"><st c="38684">doc2vec_model</st></strong><st c="38697"> variable definition </st><span class="No-Break"><st c="38718">to this:</st></span></p>
			<pre class="source-code"><st c="38726">
doc2vec_model = Doc2Vec(doc2vec_tagged_documents,
    vector_size=1536, window=5, min_count=1, workers=4)</st></pre>
			<p><st c="38828">The results </st><a id="_idIndexMarker346"/><st c="38841">will change </st><span class="No-Break"><st c="38853">to this:</st></span></p>
			<pre class="source-code"><st c="38861">
Doc2Vec Top Document:
Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer's needs are? </st><st c="39222">You do not have to imagine it, that is what RAG does…[TRUNCATED FOR BREVITY]</st></pre>
			<p><st c="39298">This resulted in the same text as our original results, using OpenAI’s embeddings. </st><st c="39382">However, the results are not consistent. </st><st c="39423">If you trained this model on more data, it will likely improve </st><span class="No-Break"><st c="39486">the results.</st></span></p>
			<p><st c="39498">In theory, the benefit this type of model has over TF-IDF is that it is a neural network-based approach that takes into account surrounding words, whereas TF-IDF is simply a statistical measure that evaluates how relevant a word is to the document (keyword search). </st><st c="39765">But as we said about the TD-IDF model, there are still more powerful models than the </st><em class="italic"><st c="39850">vec</st></em><st c="39853"> models that capture much more context and semantics of the text they are fed. </st><st c="39932">Let’s jump to another generation of </st><span class="No-Break"><st c="39968">models, transformers.</st></span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/><st c="39989">Bidirectional encoder representations from transformers</st></h2>
			<p><st c="40045">At this point, with </st><strong class="bold"><st c="40066">bidirectional encoder representations from transformers</st></strong><st c="40121"> (</st><strong class="bold"><st c="40123">BERT</st></strong><st c="40127">), we are</st><a id="_idIndexMarker347"/><st c="40137"> fully </st><a id="_idIndexMarker348"/><st c="40144">into using neural networks to better understand the underlying semantics of the corpus, yet another big step forward for NLP algorithms. </st><st c="40281">BERT is also among the first to apply a specific type of neural network, the </st><strong class="bold"><st c="40358">transformer</st></strong><st c="40369">, which</st><a id="_idIndexMarker349"/><st c="40376"> was a major step in the progression that led to the development of the LLMs we are familiar with today. </st><st c="40481">OpenAI’s popular ChatGPT models are also transformers but were trained on a much larger corpus and with different techniques </st><span class="No-Break"><st c="40606">from BERT.</st></span></p>
			<p><st c="40616">That said, BERT is still a very capable model. </st><st c="40664">You can use BERT as a standalone model that you import, avoiding having to rely on APIs such as OpenAI’s embedding service. </st><st c="40788">Being able to use a local model in your code can be a big advantage in certain network-constrained environments, instead of relying on an API service such </st><span class="No-Break"><st c="40943">as OpenAI.</st></span></p>
			<p><st c="40953">One of the defining characteristics of the transformer models is the use of a self-attention mechanism to capture dependencies between words in a text. </st><st c="41106">BERT also has multiple layers of transformers, allowing it to learn even more complex representations. </st><st c="41209">Compared to our Doc2Vec model, BERT is pre-trained already on large amounts of data, such as Wikipedia and BookCorpus with the objective of predicting the </st><span class="No-Break"><st c="41364">next sentence.</st></span></p>
			<p><st c="41378">Much like</st><a id="_idIndexMarker350"/><st c="41388"> the</st><a id="_idIndexMarker351"/><st c="41392"> previous two models, we provide code for you to compare retrieved results </st><span class="No-Break"><st c="41467">using BERT:</st></span></p>
			<pre class="source-code"><st c="41478">
from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity
bert_documents = [split.page_content for split in splits]
bert_tokenizer = BertTokenizer.from_pretrained(
    'bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')
bert_vector_size = bert_model.config.hidden_size
print(f"Vector size of BERT (base-uncased) embeddings:
    {bert_vector_size}\n")
bert_tokenized_documents = [bert_tokenizer(doc,
    return_tensors='pt', max_length=512, truncation=True)
    for doc in bert_documents]
bert_document_embeddings = []
with torch.no_grad():
    for doc in bert_tokenized_documents:
        bert_outputs = bert_model(**doc)
        bert_doc_embedding =
            bert_outputs.last_hidden_state[0, 0, :].numpy()
        bert_document_embeddings.append(bert_doc_embedding)
bert_user_query = ["What are the advantages of RAG?"]
bert_tokenized_user_query = bert_tokenizer(
    bert_user_query[0], return_tensors='pt',
    max_length=512, truncation=True)
bert_user_query_embedding = []
with torch.no_grad():
    bert_outputs = bert_model(
        **bert_tokenized_user_query)
         bert_user_query_embedding =
             bert_outputs.last_hidden_state[
                 0, 0, :].numpy()
bert_similarity_scores = cosine_similarity([
    bert_user_query_embedding], bert_document_embeddings)
bert_top_doc_index = bert_similarity_scores.argmax()
print("BERT Top Document:\n", bert_documents[
    bert_top_doc_index])</st></pre>
			<p><st c="42859">There is </st><a id="_idIndexMarker352"/><st c="42869">one</st><a id="_idIndexMarker353"/><st c="42872"> very important difference in this code compared to the usage of the last couple of models. </st><st c="42964">Here, we are not tuning the model on our own data. </st><st c="43015">This BERT model has already been trained on a large dataset. </st><st c="43076">It is possible to fine-tune the model further with our data, which is recommended if you want to use this model. </st><st c="43189">The results will reflect this lack of training, but we won’t let that prevent us from showing you how </st><span class="No-Break"><st c="43291">it works!</st></span></p>
			<p><st c="43300">For this code, we are printing out the vector size for comparison to the others. </st><st c="43382">Like the other models, we can see the top retrieved result. </st><st c="43442">Here is </st><span class="No-Break"><st c="43450">the output:</st></span></p>
			<pre class="source-code"><st c="43461">
Vector size of BERT (base-uncased) embeddings: 768
BERT Top Document:
Or if you are developing in a legal field, you may want it to sound more like a lawyer. </st><st c="43620">Vector Store or Vector Database?</st></pre>
			<p><st c="43652">The vector size is a respectable </st><strong class="source-inline"><st c="43686">768</st></strong><st c="43689">. I don’t even need metrics to tell you that the top document it found is not the best chunk to answer the question </st><strong class="source-inline"><st c="43805">What are the advantages </st></strong><span class="No-Break"><strong class="source-inline"><st c="43829">of RAG?</st></strong></span><span class="No-Break"><st c="43836">.</st></span></p>
			<p><st c="43837">This</st><a id="_idIndexMarker354"/><st c="43842"> model is </st><a id="_idIndexMarker355"/><st c="43852">powerful and has the potential to work better than the previous models, but we would need to do some extra work (fine-tuning) to get it to do a better job with our data when comparing it to the previous types of embedding models we have discussed so far. </st><st c="44107">That may not be the case with all data, but typically, in a specialized domain like this, fine-tuning should be considered as an option for your embedding model. </st><st c="44269">This is especially true if you are using a smaller local model rather than a large, hosted API such as OpenAI’s </st><span class="No-Break"><st c="44381">embeddings API.</st></span></p>
			<p><st c="44396">Running through these three different models illustrates how much embedding models have changed over the past 50 years. </st><st c="44517">Hopefully, this exercise has shown you how important the decision is for what embedding model you select. </st><st c="44623">We will conclude our discussion of embedding models by bringing us full circle back to the original embedding model we were using, the OpenAI embedding model from OpenAI’s API service. </st><st c="44808">We will discuss the OpenAI model, as well as its peers on other </st><span class="No-Break"><st c="44872">cloud services.</st></span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor139"/><st c="44887">OpenAI and other similar large-scale embedding services</st></h2>
			<p><st c="44943">Let’s talk a </st><a id="_idIndexMarker356"/><st c="44957">little more about the BERT model we just used, relative to OpenAI’s embedding model. </st><st c="45042">This was the </st><strong class="source-inline"><st c="45055">'bert-base-uncased'</st></strong><st c="45074"> version, which</st><a id="_idIndexMarker357"/><st c="45089"> is a pretty robust 110M parameter transformer model, especially compared to the previous models we used. </st><st c="45195">We have come a long way since the TD-IDF model. </st><st c="45243">Depending on the environment you are working in, this may test your computational limitations. </st><st c="45338">This was the largest model my computer could run of the BERT options. </st><st c="45408">But if you have a more powerful environment, you can change the model in these two lines </st><span class="No-Break"><st c="45497">to </st></span><span class="No-Break"><strong class="source-inline"><st c="45500">'bert-large-uncased'</st></strong></span><span class="No-Break"><st c="45520">:</st></span></p>
			<pre class="source-code"><st c="45522">
tokenizer = BertTokenizer.from_pretrained(
    'bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')</st></pre>
			<p><st c="45641">You can see the full list of BERT options </st><span class="No-Break"><st c="45684">here: </st></span><a href="https://huggingface.co/google-bert/bert-base-uncased "><span class="No-Break"><st c="45690">https://huggingface.co/google-bert/bert-base-uncased</st></span></a></p>
			<p><st c="45742">The </st><strong class="source-inline"><st c="45747">'bert-large-uncased'</st></strong><st c="45767"> model has 340M parameters, more than three times the size of </st><strong class="source-inline"><st c="45829">'bert-base-uncased'</st></strong><st c="45848">. If your environment cannot handle this size of a model, it will crash your kernel and you will have to reload all your imports and relevant notebook cells. </st><st c="46006">This just tells you how large these models can get. </st><st c="46058">But just to be clear, these two </st><a id="_idIndexMarker358"/><st c="46090">BERT models are 110M and 340M parameters, which is in millions, </st><span class="No-Break"><st c="46154">not billions.</st></span></p>
			<p><st c="46167">The </st><a id="_idIndexMarker359"/><st c="46172">OpenAI embedding model that we have been using is based on the </st><strong class="bold"><st c="46235">GPT-3</st></strong><st c="46240"> architecture, which has 175 billion parameters. </st><st c="46289">That is a </st><em class="italic"><st c="46299">billion</st></em><st c="46306"> with a </st><em class="italic"><st c="46314">B</st></em><st c="46315">. We will be talking about their newer embedding models later in this chapter, which are based on the </st><strong class="bold"><st c="46417">GPT-4</st></strong><st c="46422"> architecture and have one trillion parameters (with a </st><em class="italic"><st c="46477">T</st></em><st c="46478">!). </st><st c="46482">Needless to say, these models are massive and dwarf any of the other models we have discussed. </st><st c="46577">BERT and OpenAI are both transformers, but BERT was trained on 3.3 billion words, whereas the full corpus for GPT-3 is estimated to be around 17 trillion words (45 TB </st><span class="No-Break"><st c="46744">of text).</st></span></p>
			<p><st c="46753">OpenAI currently has three different embedding models available. </st><st c="46819">We have been using the older model to save API costs based on GPT-3, </st><strong class="source-inline"><st c="46888">'text-embedding-ada-002'</st></strong><st c="46912">, but it is a very capable embedding model. </st><st c="46956">The other two newer models that are based on GPT-4 are </st><strong class="source-inline"><st c="47011">'text-embedding-3-small'</st></strong><st c="47035"> and </st><strong class="source-inline"><st c="47040">'text-embedding-3-large'</st></strong><st c="47064">. Both of these models support the Matryoshka embeddings we talked about earlier, which allow you to use an adaptive retrieval approach for </st><span class="No-Break"><st c="47204">your retrieval.</st></span></p>
			<p><st c="47219">OpenAI is not the only cloud provider that offers a text-embedding API though. </st><strong class="bold"><st c="47299">Google Cloud Platform</st></strong><st c="47320"> (</st><strong class="bold"><st c="47322">GCP</st></strong><st c="47325">) offers</st><a id="_idIndexMarker360"/><st c="47334"> text embedding API services, with the latest version released on April 9, 2024, called </st><strong class="source-inline"><st c="47422">'text-embedding-preview-0409'</st></strong><st c="47451">. The </st><strong class="source-inline"><st c="47457">'text-embedding-preview-0409'</st></strong><st c="47486"> model is the only other large-scale cloud-hosted embedding model that I am aware of at this time that supports Matryoshka embeddings, beyond OpenAI’s </st><span class="No-Break"><st c="47637">newer models.</st></span></p>
			<p><strong class="bold"><st c="47650">Amazon Web Services</st></strong><st c="47670"> (</st><strong class="bold"><st c="47672">AWS</st></strong><st c="47675">) has </st><a id="_idIndexMarker361"/><st c="47682">embedding models based </st><a id="_idIndexMarker362"/><st c="47705">on their </st><strong class="bold"><st c="47714">Titan model</st></strong><st c="47725">, as</st><a id="_idIndexMarker363"/><st c="47729"> well as </st><strong class="bold"><st c="47738">Cohere’s embedding models</st></strong><st c="47763">. </st><strong class="bold"><st c="47765">Titan Text Embeddings V2</st></strong><st c="47789"> is</st><a id="_idIndexMarker364"/><st c="47792"> expected to launch soon and is also expected to support </st><span class="No-Break"><st c="47849">Matryoshka embedding.</st></span></p>
			<p><st c="47870">That concludes our whirlwind adventure through 50 years of embedding generation technology! </st><st c="47963">The models highlighted were selected to represent the progression of embedding </st><a id="_idIndexMarker365"/><st c="48042">capabilities over the past 50 years, but these are just a tiny sliver of the actual number </st><a id="_idIndexMarker366"/><st c="48133">of ways there are to generate embeddings. </st><st c="48175">Now that your knowledge about embedding capabilities has been expanded, let’s turn to the factors you can consider when making the actual decisions on which model </st><span class="No-Break"><st c="48338">to use.</st></span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/><st c="48345">Factors in selecting a vectorization option</st></h1>
			<p><st c="48389">Selecting the </st><a id="_idIndexMarker367"/><st c="48404">right vectorization option is a crucial decision when building a RAG system. </st><st c="48481">Key considerations include the quality of the embeddings for your specific application, the associated costs, network availability, speed of embedding generation, and compatibility between embedding models. </st><st c="48688">There are numerous other options beyond what we shared above that you can explore for your specific needs when it comes to selecting an embedding model. </st><st c="48841">Let’s review </st><span class="No-Break"><st c="48854">these considerations.</st></span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor141"/><st c="48875">Quality of the embedding</st></h2>
			<p><st c="48900">When considering the quality of your embeddings, you cannot rely on just the generic metrics you have</st><a id="_idIndexMarker368"/><st c="49002"> seen for each model. </st><st c="49024">For example, OpenAI has been tested on the </st><strong class="bold"><st c="49067">Massive Text Embedding Benchmark</st></strong><st c="49099"> (</st><strong class="bold"><st c="49101">MTEB</st></strong><st c="49105">), scoring 61.0% with their </st><strong class="source-inline"><st c="49134">'text-embedding-ada-002'</st></strong><st c="49158"> model, whereas the </st><strong class="source-inline"><st c="49178">'text-embedding-3-large'</st></strong><st c="49202"> model scored 64.6%. </st><st c="49223">The metrics can be useful, especially when trying to hone in on a model of a certain quality, but this does not mean the model will be 3.6% better for your specific model. </st><st c="49395">It does not even mean it will necessarily be better at all. </st><st c="49455">Do not rely on generic tests completely. </st><st c="49496">What ultimately matters is how well your embeddings work for your specific application of them. </st><st c="49592">This includes embedding models that you train with your own data. </st><st c="49658">If you work on an application that involves a specific domain, such as science, legal, or technology, it is very likely you can find or train a model that will work better with your specific domain data. </st><st c="49862">When you start your project, try multiple embedding models within your RAG system and then use the evaluation techniques we share in </st><a href="B22475_09.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><st c="49995">Chapter 9</st></em></span></a><st c="50004"> to compare results from using each model to determine which is best for </st><span class="No-Break"><st c="50077">your application.</st></span></p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/><st c="50094">Cost</st></h2>
			<p><st c="50099">The costs for these embedding services vary from free to relatively expensive. </st><st c="50179">OpenAI’s most expensive embedding model costs $0.13 per million tokens. </st><st c="50251">This means that for a page that has 800 tokens, it will cost you $0.000104, or slightly more than 1% of 1 cent. </st><st c="50363">That </st><a id="_idIndexMarker369"/><st c="50368">may not sound like much, but for most applications using embeddings, especially in the enterprise, these costs get multiplied rapidly, pushing the costs into the $1,000s or $10,000s for even a small project. </st><st c="50576">But other embedding APIs cost less and may fit your needs just as well. </st><st c="50648">And of course, if you build your own model like I described earlier in this chapter, you will only have the costs of the hardware or hosting costs for that model. </st><st c="50811">That can cost much less over time and may meet your needs </st><span class="No-Break"><st c="50869">as well.</st></span></p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/><st c="50877">Network availability</st></h2>
			<p><st c="50898">There are a variety of scenarios you will want to consider in terms of network availability. </st><st c="50992">Almost all applications will have some scenarios where the network will not be available. </st><st c="51082">Network availability impacts your users’ access to your application interface, but it can also impact network calls you make from your application to other services. </st><st c="51248">In this latter case, this could be a situation where your users can access your application’s interface but the application cannot reach OpenAI’s embedding service to generate an embedding for your user query. </st><st c="51458">What will you do in this case? </st><st c="51489">If you are using a model that is within your environment, this avoids this problem. </st><st c="51573">This is a consideration of availability and the impact it has on </st><span class="No-Break"><st c="51638">your users.</st></span></p>
			<p><st c="51649">Keep in mind that you cannot just switch the embedding model for your user query, just in case you were thinking you could use a </st><em class="italic"><st c="51779">fallback</st></em><st c="51787"> mechanism and have a local embedding model available as a secondary option when the network is unavailable. </st><st c="51896">If you use a proprietary API-only embedding model to vectorize your embeddings, you are committed to that embedding model, and your RAG system will be reliant on the availability of that API. </st><st c="52088">OpenAI does not offer their embedding models to use locally. </st><st c="52149">See the upcoming </st><em class="italic"><st c="52166">Embedding </st></em><span class="No-Break"><em class="italic"><st c="52176">compatibility</st></em></span><span class="No-Break"><st c="52189"> subsection!</st></span></p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/><st c="52201">Speed</st></h2>
			<p><st c="52207">The speed of generating embeddings is an important consideration, as it can impact the responsiveness and user experience of your application. </st><st c="52351">When using a hosted API service such as OpenAI, you are making network calls to generate embeddings. </st><st c="52452">While these network calls are relatively fast, there is still some latency involved compared to generating embeddings locally within your own environment. </st><st c="52607">However, it’s important to note that local embedding generation is not always faster, as the speed also depends on the specific </st><a id="_idIndexMarker370"/><st c="52735">model being used. </st><st c="52753">Some models may have slower inference times, negating the benefits of local processing. </st><st c="52841">Key aspects to consider when determining the speed of your embedding option(s) are network latency, model inference time, hardware resources, and, in some cases where multiple embeddings are involved, the ability to generate embeddings in batches and optimize </st><span class="No-Break"><st c="53101">with that.</st></span></p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/><st c="53111">Embedding compatibility</st></h2>
			<p><st c="53135">Pay close attention; this is a very important consideration and fact about embeddings! </st><st c="53223">In any case when you are comparing embeddings, such as when you are detecting the similarity between a user query embedding and the embeddings stored in the vector store, </st><em class="italic"><st c="53394">they must be created by the same embedding model</st></em><st c="53442">. These models generate unique vector signatures specific to only that model. </st><st c="53520">This is even true of models at the same service provider. </st><st c="53578">With OpenAI, for example, all three embedding models are not compatible with each other. </st><st c="53667">If you use any of OpenAI’s embedding models to vectorize your embeddings stored in your vector store, you have to call the OpenAI API and use that same model when you vectorize the user query to conduct a </st><span class="No-Break"><st c="53872">vector search.</st></span></p>
			<p><st c="53886">As your application expands in size, changing or updating an embedding model has major cost implications, since it means you will have to generate all new embeddings to use a new embedding model. </st><st c="54083">This may even drive you to use a local model rather than a hosted API service since generating new embeddings with a model you have control over tends to cost </st><span class="No-Break"><st c="54242">much less.</st></span></p>
			<p><st c="54252">While generic benchmarks can provide guidance, it’s essential to evaluate multiple embedding models within your specific domain and application to determine the best fit. </st><st c="54424">Costs can vary significantly, depending on the service provider and the volume of embeddings required. </st><st c="54527">Network availability and speed are important factors, especially when using hosted API services, as they can impact the responsiveness and user experience of your application. </st><st c="54703">Compatibility between embedding models is also crucial, as embeddings generated by different models cannot be </st><span class="No-Break"><st c="54813">directly compared.</st></span></p>
			<p><st c="54831">As your application grows, changing or updating vector embedding models can have significant cost implications. </st><st c="54944">Local embedding generation can offer more control and potentially lower costs, but the speed depends on the specific model and available hardware resources. </st><st c="55101">Thorough testing and benchmarking are necessary to find the optimal balance of quality, cost, speed, and other relevant factors for your application. </st><st c="55251">Now that we have explored the considerations for selecting a vectorization option, let’s dive into the topic of how they are stored with </st><span class="No-Break"><st c="55388">vector stores.</st></span></p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor146"/><st c="55402">Getting started with vector stores</st></h1>
			<p><st c="55437">Vector stores, combined </st><a id="_idIndexMarker371"/><st c="55462">with other data stores (databases, data warehouses, data lakes, and any other data sources) are the fuel for your RAG system engine. </st><st c="55595">Not to state the obvious, but without a place to store your RAG-focused data, which typically involves the creating, management, filtering, and search of vectors, you will not be able to build a capable RAG system. </st><st c="55810">What you use and how it is implemented will have significant implications for how your entire RAG system performs, making it a critical decision and effort. </st><st c="55967">To start this section, let’s first go back to the original concept of </st><span class="No-Break"><st c="56037">a database.</st></span></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/><st c="56048">Data sources (other than vector)</st></h2>
			<p><st c="56081">In our</st><a id="_idIndexMarker372"/><st c="56088"> basic RAG example so far, we are keeping it simple (for now) and have not connected it to an additional database resource. </st><st c="56212">You could consider the web page that the content is pulled from as the database, although the most accurate description in this context is probably to call it an unstructured data source. </st><st c="56400">Regardless, it is very likely your application will expand to the point of needing database-like support. </st><st c="56506">This may come in the form of a traditional SQL database, or it may be in the form of a giant data lake (large repository of all types of raw, primarily unstructured data), where the data is preprocessed into a more usable format representing the data source that supports your </st><span class="No-Break"><st c="56783">RAG system.</st></span></p>
			<p><st c="56794">The architecture of your data storage may be based on a </st><strong class="bold"><st c="56851">relational database management system</st></strong><st c="56888"> (</st><strong class="bold"><st c="56890">RDBMS</st></strong><st c="56895">), a </st><a id="_idIndexMarker373"/><st c="56901">variety of different types of NoSQL, NewSQL (aimed at giving you the best of the two previous approaches), or various versions of data warehouses and data lakes. </st><st c="57063">From the perspective of this book, we will approach the data sources these systems represent as an abstract concept of the </st><strong class="bold"><st c="57186">data source</st></strong><st c="57197">. But what is important </st><a id="_idIndexMarker374"/><st c="57221">to consider here is that your decision on what vector store to use will likely be highly influenced by the existing architecture of your data source. </st><st c="57371">The current technical skills of your staff will likely play a key role in these decisions </st><span class="No-Break"><st c="57461">as well.</st></span></p>
			<p><st c="57469">As an example, you may be </st><a id="_idIndexMarker375"/><st c="57496">using </st><strong class="bold"><st c="57502">PostgreSQL</st></strong><st c="57512"> for your RDBMS and have a team of expert engineers with significant expertise in fully utilizing and optimizing PostgreSQL. </st><st c="57637">In this case, you </st><a id="_idIndexMarker376"/><st c="57655">will want to consider the </st><strong class="bold"><st c="57681">pgvector</st></strong><st c="57689"> extension for PostgreSQL, which turns PostgreSQL tables into vector stores, extending many of the PostgreSQL capabilities your team is familiar with into the vector world. </st><st c="57862">Concepts such as indexing and writing SQL specifically for PostgreSQL are already going to be familiar, and that will help get your team up to speed quickly with how this extends to pgvector. </st><st c="58054">If you are building your entire data infrastructure from scratch, which is rare in enterprise, then you may go a different route optimized for speed, cost, accuracy, or all of the above! </st><st c="58241">But for most companies, you will need to consider compatibility with existing infrastructure in the vector store selection </st><span class="No-Break"><st c="58364">decision criteria.</st></span></p>
			<p class="callout-heading"><st c="58382">Fun fact – What about applications such as SharePoint?</st></p>
			<p class="callout"><st c="58437">SharePoint is</st><a id="_idIndexMarker377"/><st c="58451"> typically considered a </st><strong class="bold"><st c="58475">content management system</st></strong><st c="58500"> (</st><strong class="bold"><st c="58502">CMS</st></strong><st c="58505">) and may not fit strictly into</st><a id="_idIndexMarker378"/><st c="58537"> the definitions of the other data sources we mentioned previously. </st><st c="58605">But SharePoint and similar applications contain massive repositories of unstructured data, including PDF, Word, Excel, and PowerPoint documents that represent a huge portion of a company’s knowledge base, especially in large enterprise environments. </st><st c="58855">Combine this with the fact that generative AI has shown a proclivity to tap into unstructured data unlike any other technology preceding it, and you have the makings of an incredible data source for RAG systems. </st><st c="59067">These types of applications also have sophisticated APIs that can conduct data extraction as you pull the documents, such as pulling text out of a Word document and putting it into your database before vectorization. </st><st c="59284">In many large companies, due to the high value of the data in these applications and the relative ease of extracting that data using the APIs, this has been one of the first sources of data for RAG systems. </st><st c="59491">So yes, you can definitely include SharePoint and similar applications in your list of potential </st><span class="No-Break"><st c="59588">data sources!</st></span></p>
			<p><st c="59601">We will talk more about pgvector and other vector store options in a moment, but it is important to understand how these decisions can be very specific to each situation and that considerations other than just the vector store itself will play an important role in what you ultimately decide to </st><span class="No-Break"><st c="59897">work with.</st></span></p>
			<p><st c="59907">Regardless of what option you choose, or are starting with, this will be a key component that feeds the data to your RAG system. </st><st c="60037">This leads us to the vector stores themselves, which we can </st><span class="No-Break"><st c="60097">discuss next.</st></span></p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/><st c="60110">Vector stores</st></h2>
			<p><st c="60124">Vector stores, also</st><a id="_idIndexMarker379"/><st c="60144"> known as vector databases or vector search engines, are specialized storage systems designed to efficiently store, manage, and retrieve vector representations of data. </st><st c="60313">Unlike traditional databases that organize data in rows and columns, vector stores are optimized for operations in high-dimensional vector spaces. </st><st c="60460">They play a crucial role in an effective RAG system by enabling fast similarity search, which is essential for identifying the most relevant pieces of information in response to a </st><span class="No-Break"><st c="60640">vectorized query.</st></span></p>
			<p><st c="60657">The architecture of a vector store typically consists of three </st><span class="No-Break"><st c="60721">main components:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="60737">Indexing layer</st></strong><st c="60752">: This </st><a id="_idIndexMarker380"/><st c="60760">layer organizes the vectors in a manner that speeds up search queries. </st><st c="60831">It employs indexing techniques such as tree-based partitioning (e.g., KD-trees) or hashing (e.g., locality-sensitive hashing) to facilitate fast retrieval of vectors that are near each other in the </st><span class="No-Break"><st c="61029">vector space.</st></span></li>
				<li><strong class="bold"><st c="61042">Storage layer</st></strong><st c="61056">: The storage layer</st><a id="_idIndexMarker381"/><st c="61076"> efficiently manages the data storage on disk or in memory, ensuring optimal performance </st><span class="No-Break"><st c="61165">and scalability.</st></span></li>
				<li><strong class="bold"><st c="61181">Processing layer (optional)</st></strong><st c="61209">: Some vector stores include a processing layer to handle</st><a id="_idIndexMarker382"/><st c="61267"> vector transformations, similarity computations, and other analytics operations in </st><span class="No-Break"><st c="61351">real time.</st></span></li>
			</ul>
			<p><st c="61361">While it is technically possible to build a RAG system without using a vector store, doing so would result in suboptimal performance and scalability. </st><st c="61512">Vector stores are specifically designed to handle the unique challenges of storing and serving high-dimensional vectors, offering optimizations that significantly improve memory usage, computation requirements, and </st><span class="No-Break"><st c="61727">search precision.</st></span></p>
			<p><st c="61744">As we’ve mentioned previously, it is important to note that while the terms </st><em class="italic"><st c="61821">vector database</st></em><st c="61836"> and </st><em class="italic"><st c="61841">vector store</st></em><st c="61853"> are often used interchangeably, not all vector stores are necessarily databases. </st><st c="61935">There are other tools and mechanisms that serve the same or similar purpose as a vector database. </st><st c="62033">For the sake of accuracy and consistency with LangChain documentation, we will use the term </st><em class="italic"><st c="62125">vector store</st></em><st c="62137"> to refer to all mechanisms that store and serve vectors, including vector databases and other </st><span class="No-Break"><st c="62232">non-database solutions.</st></span></p>
			<p><st c="62255">Next, let’s discuss the vector store options to give you a better grasp of what </st><span class="No-Break"><st c="62336">is available.</st></span></p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor149"/><st c="62349">Common vector store options</st></h2>
			<p><st c="62377">When </st><a id="_idIndexMarker383"/><st c="62383">choosing a vector store, consider factors such as scalability requirements, ease of setup and maintenance, performance needs, budget constraints, and the level of control and flexibility you require over the underlying infrastructure. </st><st c="62618">Additionally, evaluate the integration options and supported programming languages to ensure compatibility with your existing </st><span class="No-Break"><st c="62744">technology stack.</st></span></p>
			<p><st c="62761">There are quite a few vector stores, some from established database companies and communities, many that are new start-ups, many more that are appearing each day, and in all likelihood, some that will go out of business by the time you are reading this. </st><st c="63016">It is a very active space! </st><st c="63043">Stay vigilant and use the information in this chapter to understand the aspects that are most important to your specific RAG applications and then look at the current marketplace to determine which option works best </st><span class="No-Break"><st c="63259">for you.</st></span></p>
			<p><st c="63267">We will focus on the vector stores that have established integration with LangChain, and even then, we will pair them down to not overwhelm you while also giving you enough options so that you can get a sense of what kinds of options are available. </st><st c="63517">Keep in mind that these vector stores are adding features and improvements all the time. </st><st c="63606">Be sure to look up their latest versions before making a selection! </st><st c="63674">It could make all the difference you need to change your mind and make a </st><span class="No-Break"><st c="63747">better choice!</st></span></p>
			<p><st c="63761">In the following subsections, we will walk through some common vector store options that integrate with LangChain, along with what you should consider about each one during the </st><span class="No-Break"><st c="63939">selection process.</st></span></p>
			<h3><st c="63957">Chroma</st></h3>
			<p><strong class="bold"><st c="63964">Chroma</st></strong><st c="63971"> is an </st><a id="_idIndexMarker384"/><st c="63978">open source vector database. </st><st c="64007">It offers fast search performance and supports easy integration with LangChain through its Python SDK. </st><st c="64110">Chroma </st><a id="_idIndexMarker385"/><st c="64117">stands out for its simplicity and ease of use, with a straightforward API and support for dynamic filtering of collections during search. </st><st c="64255">It also offers built-in support for document chunking and indexing, making it convenient for working with large text datasets. </st><st c="64382">Chroma is a good choice if you prioritize simplicity and want an open source solution that can be self-hosted. </st><st c="64493">However, it may not have as many advanced features as some other options, such as distributed search, support for multiple indexing algorithms, and built-in hybrid search capabilities that combine vector similarity with </st><span class="No-Break"><st c="64713">metadata filtering.</st></span></p>
			<h3><st c="64732">LanceDB</st></h3>
			<p><strong class="bold"><st c="64740">LanceDB</st></strong><st c="64748"> is a </st><a id="_idIndexMarker386"/><st c="64754">vector database designed for efficient</st><a id="_idIndexMarker387"/><st c="64792"> similarity search and retrieval. </st><st c="64826">It stands out for its hybrid search capabilities, combining vector similarity search with traditional keyword-based search. </st><st c="64950">LanceDB supports various distance metrics and indexing algorithms, including </st><strong class="bold"><st c="65027">Hierarchical navigable small world</st></strong><st c="65061"> (</st><strong class="bold"><st c="65063">HNSW</st></strong><st c="65067">) for efficient approximate nearest neighbor search. </st><st c="65121">It </st><a id="_idIndexMarker388"/><st c="65124">integrates with LangChain and offers fast search performance and support for various indexing techniques. </st><st c="65230">LanceDB is a good choice if you want a dedicated vector database with good performance and integration with LangChain. </st><st c="65349">However, it may not have as large of a community or ecosystem compared to some </st><span class="No-Break"><st c="65428">other options.</st></span></p>
			<h3><st c="65442">Milvus</st></h3>
			<p><strong class="bold"><st c="65449">Milvus</st></strong><st c="65456"> is an </st><a id="_idIndexMarker389"/><st c="65463">open source vector database that provides</st><a id="_idIndexMarker390"/><st c="65504"> scalable similarity search and supports various indexing algorithms. </st><st c="65574">It provides a cloud-native architecture and supports Kubernetes-based deployments for scalability and high availability. </st><st c="65695">Milvus offers features such as multi-vector indexing, allowing you to search across multiple vector fields simultaneously, and provides a plugin system for extending its functionality. </st><st c="65880">It integrates well with LangChain and offers distributed deployment and horizontal scalability. </st><st c="65976">Milvus is a good fit if you need a scalable and feature-rich open source vector store. </st><st c="66063">However, it may require more setup and management compared to </st><span class="No-Break"><st c="66125">managed services.</st></span></p>
			<h3><st c="66142">pgvector</st></h3>
			<p><strong class="bold"><st c="66151">pgvector</st></strong><st c="66160"> is an</st><a id="_idIndexMarker391"/><st c="66166"> extension for PostgreSQL that adds support</st><a id="_idIndexMarker392"/><st c="66209"> for vector similarity search and integrates with LangChain as a vector store. </st><st c="66288">It leverages the power and reliability of PostgreSQL, the world’s most advanced open source relational database, and benefits from PostgreSQL’s mature ecosystem, extensive documentation, and strong community support. </st><st c="66505">pgvector seamlessly integrates vector similarity search with traditional relational database features, enabling hybrid </st><span class="No-Break"><st c="66624">search capabilities.</st></span></p>
			<p><st c="66644">Recent updates have improved the level of performance for pgvector to bring it in line with other dedicated vector database services. </st><st c="66779">Given that PostgreSQL is the most popular database in the world (a battle-tested mature technology that has a huge community) and that the vector extension pgvector gives you all the capabilities of other vector databases, this combination offers a great solution for any company already </st><span class="No-Break"><st c="67067">using PostgreSQL.</st></span></p>
			<h3><st c="67084">Pinecone</st></h3>
			<p><strong class="bold"><st c="67093">Pinecone</st></strong><st c="67102"> is a </st><a id="_idIndexMarker393"/><st c="67108">fully managed vector database service</st><a id="_idIndexMarker394"/><st c="67145"> that offers high performance, scalability, and easy integration with LangChain. </st><st c="67226">It provides a fully managed and serverless experience, abstracting away the complexities of infrastructure management. </st><st c="67345">Pinecone offers features such as real-time indexing, allowing you to update and search vectors with low latency, and supports hybrid search, combining vector similarity with metadata filtering. </st><st c="67539">It also provides features such as distributed search and support for multiple indexing algorithms. </st><st c="67638">Pinecone is a good choice if you want a managed solution with good performance and minimal setup. </st><st c="67736">However, it may be more expensive compared to </st><span class="No-Break"><st c="67782">self-hosted options.</st></span></p>
			<h3><st c="67802">Weaviate</st></h3>
			<p><strong class="bold"><st c="67811">Weaviate</st></strong><st c="67820"> is an </st><a id="_idIndexMarker395"/><st c="67827">open source vector search engine that supports</st><a id="_idIndexMarker396"/><st c="67873"> various vector indexing and similarity search algorithms. </st><st c="67932">It follows a schema-based approach, allowing you to define a semantic data model for your vectors. </st><st c="68031">Weaviate supports CRUD operations, data validation, and authorization mechanisms, and offers modules for common machine learning tasks such as text classification and image similarity search. </st><st c="68223">It integrates with LangChain and offers features such as schema management, real-time indexing, and a GraphQL API. </st><st c="68338">Weaviate is a good fit if you want an open source vector search engine with advanced features and flexibility. </st><st c="68449">However, it may require more setup and configuration compared to </st><span class="No-Break"><st c="68514">managed services.</st></span></p>
			<p><st c="68531">In the</st><a id="_idIndexMarker397"/><st c="68538"> preceding subsections, we discussed various vector store options that integrate with LangChain, providing an overview of their features, strengths, and considerations for selection. </st><st c="68721">This </st><a id="_idIndexMarker398"/><st c="68726">emphasizes the importance of evaluating factors such as scalability, ease of use, performance, budget, and compatibility with existing technology stacks when choosing a vector store. </st><st c="68909">While broad, this list is still very short compared to the overall number of options available to integrate with LangChain and to use as vector stores </st><span class="No-Break"><st c="69060">in general.</st></span></p>
			<p><st c="69071">The vector stores mentioned span a range of capabilities, including fast similarity search, support for various indexing algorithms, distributed architectures, hybrid search combining vector similarity with metadata filtering, and integration with other services and databases. </st><st c="69350">Given the rapid evolution of the vector store landscape, new options are emerging frequently. </st><st c="69444">Use this information as a base, but when you are ready to build your next RAG system, we highly recommend you visit the LangChain documentation on available vector stores, and consider which option best suits your needs at </st><span class="No-Break"><st c="69667">that time.</st></span></p>
			<p><st c="69677">In the next section, we will talk more in-depth about considerations when choosing a vector store for your </st><span class="No-Break"><st c="69785">RAG system.</st></span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor150"/><st c="69796">Choosing a vector store</st></h1>
			<p><st c="69820">Selecting</st><a id="_idIndexMarker399"/><st c="69830"> the right vector store for a RAG system involves considering several factors, including the scale of the data, the required search performance (speed and accuracy), and the complexity of the vector operations. </st><st c="70041">Scalability is crucial for applications dealing with large datasets, requiring a mechanism that can efficiently manage and retrieve vectors from a growing corpus. </st><st c="70204">Performance considerations involve evaluating the database’s search speed and its ability to return highly </st><span class="No-Break"><st c="70311">relevant results.</st></span></p>
			<p><st c="70328">Moreover, the ease of integration with existing RAG models and the flexibility to support various vector operations are also critical. </st><st c="70464">Developers should look for vector stores that offer robust APIs, comprehensive documentation, and strong community or vendor support. </st><st c="70598">As listed previously, there are many popular vector stores, each offering unique features and optimizations tailored to different use cases and </st><span class="No-Break"><st c="70742">performance needs.</st></span></p>
			<p><st c="70760">When choosing a vector store, it’s essential to align the selection with the overall architecture and operational requirements of the RAG system. </st><st c="70907">Here are some </st><span class="No-Break"><st c="70921">key considerations:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="70940">Compatibility with existing infrastructure</st></strong><st c="70983">: When evaluating vector stores, it’s crucial to consider how well they integrate with your existing data infrastructure, such as databases, data warehouses, and data lakes. </st><st c="71158">Assess the compatibility of the vector store with your current technology stack and the skills of your development team. </st><st c="71279">For example, if you have strong expertise in a particular database system such as PostgreSQL, a vector store extension such as pgvector</st><a id="_idIndexMarker400"/><st c="71414"> might be a suitable choice, as it allows for seamless integration and leverages your team’s </st><span class="No-Break"><st c="71507">existing knowledge.</st></span></li>
				<li><strong class="bold"><st c="71526">Scalability and performance</st></strong><st c="71554">: What is the vector store’s ability to handle the expected growth of your data and the performance requirements of your RAG system? </st><st c="71688">Assess the indexing and search capabilities of the vector store, ensuring it can deliver the desired level of performance and accuracy. </st><st c="71824">If you anticipate a large-scale deployment, distributed vector databases such as Milvus or Elasticsearch with vector plugins might be more appropriate, as they are designed to handle high data volumes and provide efficient </st><span class="No-Break"><st c="72047">search throughput.</st></span></li>
				<li><strong class="bold"><st c="72065">Ease of use and maintenance</st></strong><st c="72093">: What is the learning curve associated with the vector store, taking into account the available documentation, community support, and vendor support? </st><st c="72245">Understand the effort required for setup, configuration, and ongoing maintenance of the vector store. </st><st c="72347">Fully managed services such as Pinecone can simplify deployment and management, reducing the operational burden on your team. </st><st c="72473">On the other hand, self-hosted solutions such as Weaviate provide more control and flexibility, allowing for customization and fine-tuning to meet your </st><span class="No-Break"><st c="72625">specific requirements.</st></span></li>
				<li><strong class="bold"><st c="72647">Data security and compliance</st></strong><st c="72676">: Evaluate the security features and access controls provided by the vector store, ensuring they align with your industry’s compliance requirements. </st><st c="72826">If you deal with sensitive data, assess the encryption and data protection capabilities of the vector store. </st><st c="72935">Consider the vector store’s ability to meet data privacy regulations and standards, such as GDPR or HIPAA, depending on your </st><span class="No-Break"><st c="73060">specific needs.</st></span></li>
				<li><strong class="bold"><st c="73075">Cost and licensing</st></strong><st c="73094">: What is the pricing model of the vector store? </st><st c="73144">Is it based on data volume, search operations, or a combination of factors? </st><st c="73220">Consider the long-term cost-effectiveness of the vector store, taking into account the scalability and growth projections of your RAG system. </st><st c="73362">Assess the licensing fees, infrastructure costs, and maintenance expenses associated with the vector store. </st><st c="73470">Open source solutions may have lower upfront costs but require more in-house expertise </st><a id="_idIndexMarker401"/><st c="73557">and resources for maintenance, while managed services may have higher subscription fees but offer simplified management </st><span class="No-Break"><st c="73677">and support.</st></span></li>
				<li><strong class="bold"><st c="73689">Ecosystem and integrations</st></strong><st c="73716">: When selecting a vector store, it’s important to evaluate the ecosystem and integrations it supports. </st><st c="73821">Consider the availability of client libraries, SDKs, and APIs for different programming languages, as this can greatly simplify the development process and enable seamless integration with your existing code base. </st><st c="74035">Assess the compatibility of the vector store with other tools and frameworks commonly used in RAG systems, such as NLP libraries or machine learning frameworks. </st><st c="74196">The general size of the supporting community is important as well; make sure it is at a critical mass to grow and thrive. </st><st c="74318">A vector store with a robust ecosystem and extensive integrations can provide more flexibility and opportunities for extending the functionality of your </st><span class="No-Break"><st c="74471">RAG system.</st></span></li>
			</ul>
			<p><st c="74482">By carefully evaluating these factors and aligning them with your specific requirements, you can make an informed decision when choosing a vector store for your RAG system. </st><st c="74656">It’s important to conduct thorough research, benchmark different options, and consider the long-term implications of your choice in terms of scalability, performance, </st><span class="No-Break"><st c="74823">and maintainability.</st></span></p>
			<p><st c="74843">Remember that the choice of vector store is not a one-size-fits-all decision, and it may evolve as your RAG system grows and your requirements change. </st><st c="74995">It’s crucial to periodically reassess your vector store selection and adjust as needed to ensure optimal performance and alignment with your overall </st><span class="No-Break"><st c="75144">system architecture.</st></span></p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/><st c="75164">Summary</st></h1>
			<p><st c="75172">The integration of vectors and vector stores into RAG systems is foundational for enhancing the efficiency and accuracy of information retrieval and generation tasks. </st><st c="75340">By carefully selecting and optimizing your vectorization approach, as well as your vector store, you can significantly improve the performance of your RAG system. </st><st c="75503">Vectorization techniques and vector stores are only part of how vectors play a role in RAG systems; they also play a major role in our retrieval stage. </st><st c="75655">In the next chapter, we will address the retrieval role vectors play, going in-depth on the subject of vector similarity search algorithms </st><span class="No-Break"><st c="75794">and services.</st></span></p>
		</div>
	<div id="charCountTotal" value="75807"/></body></html>