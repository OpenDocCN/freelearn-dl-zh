- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Domain Adaptation for Large Language Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we examined how **Parameter-Efficient Fine-Tuning**
    (**PEFT**) enhances **large language models** (**LLMs**) for specific tasks such
    as question-answering. In this chapter, we will be introduced to domain adaptation,
    a distinct fine-tuning approach. Unlike task-specific tuning, domain adaptation
    equips models to interpret language that’s unique to specific industries or domains,
    addressing the gap in LLMs’ understanding of specialized language.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, we’ll introduce *Proxima Investment Group*, a hypothetical
    digital-only investment firm aiming to adapt an LLM to its specific financial
    language using internal data. We’ll demonstrate how modifying the LLM to process
    the specific terminology and nuances typical in Proxima’s environment enhances
    the model’s relevance and effectiveness in the financial domain.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also explore the practical steps Proxima might take, such as selecting
    relevant internal datasets for training, applying PEFT methods such as **Low-Rank
    Adaptation** (**LoRA**) to adapt the model efficiently, and using masking techniques
    to refine the model’s comprehension. Then, we’ll explore how Proxima can evaluate
    the success of this domain adaptation, assessing the model’s performance in tasks
    such as analyzing financial trends, responding to client inquiries, and generating
    reports that align with Proxima’s internal standards and market position.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will clearly understand the theoretical underpinnings
    of domain adaptation and its real-world application, particularly in a complex
    sector such as finance, where the model’s depth of domain understanding can significantly
    impact business outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by demystifying the concept, exploring its technical underpinnings,
    and discussing its importance in accomplishing domain-specific business objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying domain adaptation – understanding its history and importance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of generative LLMs, domain adaptation specifically tailors models
    such as **BLOOM**, which have been pre-trained on extensive, generalized datasets
    (such as news articles and Wikipedia entries) for enhanced understanding of texts
    from targeted sectors, including biomedical, legal, and financial fields. This
    type of refinement can be pivotal as LLMs, despite their vast pre-training, may
    not inherently capture the intricate details and specialized terminology inherent
    to these domains. This adaptation involves a deliberate process of realigning
    the model’s learned patterns to the linguistic characteristics, terminologies,
    and contextual nuances prevalent in the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: Domain adaptation operates within the ambit of **transfer learning**. In this
    broader paradigm, a model’s learnings from one task are repurposed to improve
    its efficacy on a related yet distinct task. This approach capitalizes on the
    model’s pre-learned features to improve its efficiency and accuracy on the subsequent
    task, markedly reducing its reliance on large volumes of domain-specific data
    and computational resources. Specifically, we begin with a model that’s been trained
    on broad datasets and use it as a starting point to adapt to specialized domains
    thereby augmenting their accuracy, relevance, and applicability to more targeted
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, several methodologies can be employed to tailor the model to specific
    domains, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continued pre-training**: The model undergoes additional pre-training on
    domain-specific corpora, allowing its parameters to be adapted incrementally to
    the target domain’s linguistic features, as highlighted in research by Gururangan
    et al. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermediate task training**: Here, the model is trained on intermediate
    tasks, utilizing domain-specific data before being fine-tuned for downstream applications.
    This step facilitates a more robust adaptation to the domain (Pruksachatkun et
    al., 2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation**: Techniques such as **back translation** (Xie et al.,
    2019) and **token replacement** (Anaby-Tavor et al., 2020) are leveraged to generate
    synthetic domain-specific training examples from limited actual data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Back translation** entails translating an existing text from one language
    (for example, English) into another (for example, French) and then translating
    it back to the original language. This process generates paraphrased versions
    of the original text while preserving its semantics.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token replacement** involves altering individual words within a sentence
    to generate new sentences. This alteration usually aims to preserve the semantic
    meaning of the original sentence while introducing variations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-task learning**: This framework concurrently optimizes the model for
    both generic and domain-specific tasks during the adaptation phase, as demonstrated
    by Clark et al. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As domain adaptation techniques evolve, they increasingly enhance model performance
    in specialized fields, even with reduced amounts of domain-specific data. As discussed
    in [*Chapter 4*](B21773_04.xhtml#_idTextAnchor123), more recent developments have
    focused on the computational efficiency of these techniques. Adaptation methods
    such as LoRA facilitate significant model adjustments with minimal parameter changes
    without requiring comprehensive retraining. It is important to note that a model's
    performance will always vary based on various factors like the quality of the
    dataset, available computational resources, and other implementation details.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have some insight into domain adaptation techniques and their focus
    on computational efficiency, we can apply these concepts practically. Our practice
    project will leverage BLOOM, a state-of-the-art, open source LLM, to demonstrate
    domain adaptation for the finance sector. Leveraging PEFT, we aim to fine-tune
    BLOOM with minimal computational resources, illustrating the practical application
    of these advanced adaptation methods in enhancing model performance within the
    finance domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Practice project: Transfer learning for the finance domain'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This project aims to fine-tune BLOOM on a curated corpus of specific documents
    to imbue it with the ability to interpret and articulate concepts specific to
    Proxima and its products.
  prefs: []
  type: TYPE_NORMAL
- en: Our methodology is inspired by strategies for domain adaptation across various
    fields, including biomedicine, finance, and law. A noteworthy study conducted
    by Cheng et al. in 2023 called *Adapting Large Language Models via Reading Comprehension*
    presents a novel approach for enhancing LLMs’ proficiency in domain-specific tasks.
    This approach repurposed extensive pre-training corpora into formats conducive
    to reading comprehension tasks, significantly improving the models’ functionality
    in specialized domains. In our case, we will apply a similar but simplified approach
    to continued pre-training by fine-tuning the pre-trained BLOOM model using a bespoke
    dataset specific to Proxima, effectively continuing the model’s training. This
    process adjusts the model parameters incrementally to ensure that it understands
    the language unique to Proxima’s products and offerings better.
  prefs: []
  type: TYPE_NORMAL
- en: Training methodologies for financial domain adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Four our continued training strategy, we’ll employ **causal language modeling**
    (**CLM**). This approach is part of a broader set of training methodologies that
    optimize model performance for various objectives. Before moving to implementation,
    let''s try to disambiguate our chosen approach from other popular strategies to
    better understand the CLM methodology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked Language Modeling**(**MLM**): A cornerstone of Transformer-based models
    such as BERT, MLM randomly masks parts of the input text and challenges the model
    to predict the masked tokens. By considering the entire context around the mask
    (both before and after), MLM enables a model to develop a bidirectional understanding
    of language, enriching its grasp of context and semantics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next-Sentence Prediction**(**NSP**): This methodology further broadens a
    model’s narrative understanding by training it to discern whether two sentences
    logically follow each other. NSP is instrumental in teaching models about text
    structure and coherence, enabling them to construct and comprehend logical sequences
    within larger bodies of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CLM**: Our chosen path for BLOOM’s adaptation diverges here, embracing CLM
    for its focused, sequential prediction capabilities. Unlike MLM, which looks both
    ways (before and after the masked token), CLM adopts a unidirectional approach,
    predicting each subsequent token based solely on the preceding context. This method
    is intrinsically aligned with natural language generation, making it especially
    suitable for crafting coherent, contextually rich narratives in the target domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In selecting CLM for BLOOM’s adaptation, we’ll extend the model’s generative
    capabilities to produce text sequences that are not only logically structured
    but also deeply embedded with the nuance of the target domain. CLM’s unidirectional
    nature ensures that each token that’s generated is informed by a cohesive understanding
    of the preceding text, enabling the model to generate detailed, accurate, and
    domain-specific texts.
  prefs: []
  type: TYPE_NORMAL
- en: Once fine-tuning is complete, we can evaluate the efficacy of the domain-adapted
    BLOOM model based on its proficiency in generating contextually relevant and domain-specific
    narratives. We’ll compare the adapted model’s performance against the original
    model with a special focus on the model’s fluency, accuracy, and overall comprehension
    of the target domain.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve done previously, we’ll leverage Google Colab for our initial prototyping
    phase. As *Chapters 4* and *5* described, Google Colab offers a preconfigured
    environment that simplifies the process of testing our methodologies before we
    consider promoting them to production environments. All the code in this chapter
    is available in the `Chapter 6` folder of this book’s GitHub repository ([https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin with the initial setup, which involves loading a smaller variation
    of **BLOOM-1b1** using the Transformers library. We’ll also import the methods
    that we’ll need to apply PEFT. For this example, we’ll rely on a few libraries
    that can be installed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once installed, we can begin importing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to load the tokenizer and model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed previously, we’re incorporating PEFT for efficient adaptation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The PEFT technique, specifically through `AdaLoraConfig`, allows us to introduce
    a compact, efficient layer so that we can adapt the model to new contexts – here,
    the finance domain – with a significantly reduced number of trainable parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We must integrate the adapter to finalize the PEFT model setup, effectively
    creating a model variant that’s optimized for our domain-specific training while
    focusing on efficiency. We can quantify this by examining the number of trainable
    parameters our model will use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code provides us with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trainable** **parameters**: 1,769,760'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total parameters in the** **model**: 1,067,084,088'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Percentage of trainable** **parameters**: 0.166%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that out of over 1 billion parameters in the BLOOM-1b1 model, only
    about 1.77 million parameters are being fine-tuned for the finance domain adaptation.
    This small percentage (0.166%) of trainable parameters highlights the efficiency
    of PEFT, allowing significant model adaptability with minimal adjustments. This
    is crucial for practical applications as it reduces both computational costs and
    the time required for training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll move on to preparing the data. We’ll assume we have assembled texts
    encompassing the breadth of knowledge about specialized Proxima products and offerings
    such as the **Proxima Passkey**. CLM training requires distinct testing and training
    phases to evaluate the model’s ability to accurately predict the next token in
    a sequence. This ensures it generalizes well beyond the training data to unseen
    text. During training, the loss calculation measures the difference between the
    model’s predicted token probabilities and the actual tokens. It guides the model
    to adjust its parameters to minimize this loss, improving its predictive accuracy
    over iterations. As such, we must define training and testing texts as our dataset.
    An example dataset is included in this book’s GitHub repository (linked earlier
    in the chapter).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must apply preprocessing and tokenization. Texts are cleaned, standardized,
    and then converted into a numerical format (`512` tokens so that it aligns with
    the model’s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TrainingArguments` class configures the training process, setting parameters
    such as the batch size, number of epochs, and the directory for saving model checkpoints.
    This configuration is crucial for efficient learning and model evaluation. Meanwhile,
    the `Trainer` class orchestrates the model’s training process. Again, continued
    training gradually adapts the model’s parameters to generate and understand text
    related to the Proxima Passkey:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Generally, our configuration specifies the training parameters and initializes
    the `Trainer` class while focusing on domain adaptation. The `TrainingArguments`
    class is tailored to manage the training process efficiently, including logging
    and model-saving strategies. Remember that the batch size we choose for training
    the model balances the GPU’s memory capacity and how quickly the model learns
    from the dataset. A larger batch size allows more data to be processed at once,
    speeding up training but requiring more memory, which can be a limitation if the
    GPU has restricted capacity. Conversely, a smaller batch size means the model
    updates its weights more frequently with fewer samples, which can benefit learning
    but results in slower overall progress through the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'With training complete, we can use the adapted model to generate text based
    on prompts related to the Proxima Passkey. The model considers the prompt, generates
    a sequence of tokens representing the continuation, and then decodes this sequence
    back into human-readable text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Notice the `model.generate()` function, which takes tokenized input and produces
    a sequence of tokens as output. These tokens are then decoded into text.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we adapted the BLOOM language model so that it specializes
    in the finance domain. This involved loading the pre-trained model, applying a
    PEFT adapter for efficient domain adaptation, and preparing a financial dataset
    for model training through standardization and tokenization. After fine-tuning
    BLOOM with this domain-specific data, we used the model to generate text relevant
    to the finance sector. The final step is to evaluate this adapted model’s performance
    compared to the original pre-trained version, focusing on its effectiveness in
    accurately handling financial language and concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and outcome analysis – the ROUGE metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quantitative and qualitative evaluations are essential to assess the adapted
    BLOOM model against the original, especially in the context of Proxima’s language.
    Quantitatively, the model’s output is compared against a reference dataset that
    mirrors Proxima’s product language using the **ROUGE** metric. This comparison
    helps measure the overlap in key terms and styles. Additionally, it’s beneficial
    to develop specific metrics for evaluating the model’s proficiency in terms of
    financial terminology and concepts relevant to Proxima:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The ROUGE score would be calculated by comparing the two texts in this example.
    The score measures the overlap between the predicted output and the reference
    text in terms of **n-grams** (sequences of words). For instance, **ROUGE-N** (where
    *N* can be 1, 2, or L) calculates the overlap of n-grams between the predicted
    and reference texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ROUGE-1** evaluates the overlap of unigrams (individual words) between the
    predicted and reference texts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-2** assesses the overlap of bigrams (two-word phrases) between the
    texts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-L** focuses on the longest common subsequence, which is useful for
    evaluating sentence-level structure similarity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ROUGE scores range from 0 to 1 and quantify the similarity between the
    predicted text and a reference text, providing insights into how well a model’s
    output matches the expected content. Scores closer to 1 indicate higher similarity
    or overlap, while scores near 0 suggest little to no commonality. These scores
    are divided into three key components – precision, recall, and the F1 score:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** measures the proportion of words in the predicted text that are
    also found in the reference text. A high precision score indicates that most of
    the words generated by the model are relevant and appear in the reference, signifying
    accuracy in the model’s output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall** assesses the proportion of words from the reference text that are
    captured in the model’s prediction. High recall implies that the model effectively
    includes most of the relevant content from the reference in its output, indicating
    comprehensiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **F1 score** is the harmonic mean of precision and recall, balancing the
    two. It is especially useful for understanding the model’s overall accuracy in
    generating text that is both relevant (precision) and comprehensive (recall).
    The F1 score is crucial when equal importance is given to precision and recall
    in evaluating the model’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Metric** | **Recall (r)** | **Precision (p)** | **F1** **Score (f)** |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-1 | 0.35 | 0.333 | 0.341 |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-2 | 0.053 | 0.048 | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| ROUGE-L | 0.35 | 0.333 | 0.341 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.1: ROUGE metric outcomes'
  prefs: []
  type: TYPE_NORMAL
- en: These scores indicate a moderate level of unigram overlap (ROUGE-1) between
    the texts but a significantly lower bigram overlap (ROUGE-2). The similarity between
    the ROUGE-1 and ROUGE-L scores suggests the model captures individual key terms
    to some extent but may struggle with longer phrase structures, pointing to areas
    for model improvement.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while the model demonstrates a basic grasp of key individual terms
    (as shown by ROUGE-1 and ROUGE-L), its ability to replicate more complex structures
    or phrases from the reference text (as indicated by ROUGE-2) is quite limited.
    This suggests that while the model has some understanding of the domain-specific
    language, further fine-tuning is required for it to effectively replicate the
    more nuanced and structured aspects of the reference texts. Keep in mind that,
    as we have seen in other chapters, semantic similarity is also a good measure
    of domain-specific language understanding and does not rely on lexical overlap
    the way ROUGE does.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitatively, domain experts should review the model’s outputs to judge their
    relevance and accuracy in the context of Proxima’s products and institutional
    language. These experts can provide insights into the nuances of the model’s performance,
    which might not be captured by quantitative metrics alone. Comparing their feedback
    on the outputs from both the original and adapted models will highlight how well
    the adaptation has aligned BLOOM with Proxima’s specific communication needs.
    This dual approach ensures a comprehensive evaluation, blending statistical analysis
    with real-world applicability and relevance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the domain adaptation process for the BLOOM LLM,
    which is specifically tailored to enhance its proficiency in the financial sector,
    particularly in understanding and generating content related to Proxima’s product
    offerings. We began by introducing the concept of domain adaptation within the
    broader scope of transfer learning, emphasizing its significance in fine-tuning
    general-purpose models to grasp the intricacies of specialized fields.
  prefs: []
  type: TYPE_NORMAL
- en: The adaptation process involved integrating PEFT techniques into BLOOM and preprocessing
    a financial dataset for model training. This included standardizing text lengths
    through truncation and padding and tokenizing the texts for consistency in model
    input. The adapted model’s performance was then quantitatively assessed against
    a reference dataset using the ROUGE metric, providing insights into its ability
    to capture key financial terminologies and phrases. Qualitative evaluation by
    domain experts was also suggested as a complementary method to gauge the model’s
    practical effectiveness in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this chapter detailed a common approach to refining an LLM for a specific
    domain, illustrating both the methodology and the importance of a nuanced evaluation
    to ascertain the success of such adaptations. In the next chapter, we will explore
    how to adapt an LLM without fine-tuning using prompt engineering. We will discover
    how to contextualize and guide model outputs to produce similar results comparable
    to fine-tuned models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey,
    D., & Smith, N. A. (2020). *Don’t stop pretraining: Adapt language models to domains
    and tasks*. In arXiv [cs.CL]. [http://arxiv.org/abs/2004.10964/](http://arxiv.org/abs/2004.10964/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pruksachatkun, Y., Phang, J., Liu, H., Htut, P. M., Zhang, X., Pang, R. Y.,
    Vania, C., Kann, K., & Bowman, S. R. (2020a). *Intermediate-task transfer learning
    with pretrained language models: When and why does it work?* Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., & Le, Q. V. (n.d.). *Unsupervised
    Data Augmentation for Consistency Training*. Arxiv.org. Retrieved March 16, 2024,
    from [http://arxiv.org/abs/1904.12848](http://arxiv.org/abs/1904.12848).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A., Kour, G., Shlomov,
    S., Tepper, N., & Zwerdling, N. (2020). *Do not have enough data? Deep learning
    to the rescue!* Proceedings of the ... AAAI Conference on Artificial Intelligence.
    AAAI Conference on Artificial Intelligence, 34(05), 7383–7390\. [https://doi.org/10.1609/aaai.v34i05.6233](https://doi.org/10.1609/aaai.v34i05.6233).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clark, K., Luong, M.-T., Khandelwal, U., Manning, C. D., & Le, Q. V. (2019).
    *BAM! Born-again multi-task networks for natural language understanding*. In arXiv
    [cs.CL]. [http://arxiv.org/abs/1907.04829](http://arxiv.org/abs/1907.04829).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
