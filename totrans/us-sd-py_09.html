<html><head></head><body>
		<div><h1 id="_idParaDest-111" class="chapter-number"><a id="_idTextAnchor178"/>9</h1>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor179"/>Using Textual Inversion</h1>
			<p><strong class="bold">Textual inversion</strong> (<strong class="bold">TI</strong>) is <a id="_idIndexMarker277"/>another way to provide additional capabilities to a pretrained model. Unlike <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>), discussed in <a href="B21263_08.xhtml#_idTextAnchor153"><em class="italic">Chapter 8</em></a>, which is a<a id="_idIndexMarker278"/> fine-tuning technique applied to the text encoder and the UNet attention weights, TI is a technique to add new <strong class="bold">embedding</strong> space<a id="_idIndexMarker279"/> based on the trained data.</p>
			<p>In the context of Stable Diffusion, <strong class="bold">text embedding</strong> refers<a id="_idIndexMarker280"/> to the representation of text data as numerical vectors in a high dimensional space, allowing for manipulation and processing by machine learning algorithms. Specifically, in the case of Stable Diffusion, text embeddings <a id="_idIndexMarker281"/>are typically created using the <strong class="bold">Contrastive Language-Image Pretraining</strong> (<strong class="bold">CLIP</strong>) [6] model.</p>
			<p>To train a TI model, you only need a minimal set of three to five images, resulting in a compact <code>pt</code> or <code>bin</code> file, typically just a few kilobytes in size. This makes TI a highly efficient method for incorporating new elements, concepts, or styles into your pretrained checkpoint model while maintaining exceptional portability.</p>
			<p>In this chapter, we will first start using TI with the TI loader from the <code>diffusers</code> package, then delve into the core of TI to uncover how it works internally, and finally build a custom TI loader to have the TI weight applied to the image generation.</p>
			<p>Here are the topics we are going to cover:</p>
			<ul>
				<li>Diffusers inference using TI</li>
				<li>How TI works</li>
				<li>Build a custom TI loader</li>
			</ul>
			<p>By the end of this chapter, you will be able to start using any type of TI shared by the community and also build your application to load TI.</p>
			<p>Let’s start leveraging the power of Stable Diffusion<a id="_idTextAnchor180"/> TI.</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor181"/>Diffusers inference using TI</h1>
			<p>Before <a id="_idIndexMarker282"/>diving into how TI works internally, let’s take <a id="_idIndexMarker283"/>a look at how to use TI using Diffusers.</p>
			<p>There are countless pretrained TIs shared in the Hugging Face’s Stable Diffusion concepts library [3] and CIVITAI [4]. For example, one of the most downloaded TIs from the Stable Diffusion concepts library is <code>sd-concepts-library/midjourney-style</code> [5]. We can start using it by simply referencing this name in the code; Diffusers will download the model data automatically:</p>
			<ol>
				<li>Let’s initialize a Stable Diffusion pipeline:<pre class="source-code">
# initialize model</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
import torch</pre><pre class="source-code">
model_id = "stablediffusionapi/deliberate-v2"</pre><pre class="source-code">
pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    model_id,</pre><pre class="source-code">
    torch_dtype=torch.float16</pre><pre class="source-code">
).to("cuda")</pre></li>
				<li>Generate an image without TI involved:<pre class="source-code">
# without using TI</pre><pre class="source-code">
prompt = "a high quality photo of a futuristic city in deep \ </pre><pre class="source-code">
space, midjourney-style"</pre><pre class="source-code">
image = pipe(</pre><pre class="source-code">
    prompt,</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image</pre><p class="list-inset">In the prompt, <code>midjourney-style</code> is used, which will be given as the name of the TI. Without applying the name, we will see an image generated, as shown in <em class="italic">Figure 9</em><em class="italic">.1</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_09_01.jpg" alt="Figure 9.1: A futuristic city in deep space without TI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1: A futuristic city in deep space without TI</p>
			<ol>
				<li value="3">Generate<a id="_idIndexMarker284"/> an image<a id="_idIndexMarker285"/> with TI involved.<p class="list-inset">Now, let’s load the TI into the Stable Diffusion pipeline and give it a name, <code>midjourney-style</code>, to represent the newly added embeddings:</p><pre class="source-code">
pipe.load_textual_inversion(</pre><pre class="source-code">
    "sd-concepts-library/midjourney-style",</pre><pre class="source-code">
    token = "midjourney-style"</pre><pre class="source-code">
)</pre><p class="list-inset">The<a id="_idIndexMarker286"/> preceding code will download<a id="_idIndexMarker287"/> the TI automatically and then add it to the pipeline model. Execute the same prompt and pipeline again, and we will get a completely new image, as shown in <em class="italic">Figure 9</em><em class="italic">.2</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_09_02.jpg" alt="Figure 9.2: A futuristic city in deep space with TI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2: A futuristic city in deep space with TI</p>
			<p>Yes, it looks and<a id="_idIndexMarker288"/> feels like an image generated by <a id="_idIndexMarker289"/>Midjourney, but it is actually generated by Stable Diffusion. The “<em class="italic">inversion</em>” in the name of the TI indicates that we can inverse any new name to the new embeddings, for example, if we give a new token a name such as <code>colorful-magic-style</code>:</p>
			<pre class="source-code">
pipe.load_textual_inversion(
    "sd-concepts-library/midjourney-style",
    token = "colorful-magic-style"
)</pre>
			<p>We will get the<a id="_idIndexMarker290"/> same image <a id="_idIndexMarker291"/>because we use <code>midjourney-style</code> as the name of the TI. This time, we “inverse” <code>colorful-magic-style</code> into the new embeddings. However, the <code>load_textual_inversion</code> function provided by Diffusers does not provide a <code>weight</code> parameter for users to load a TI with a certain weight. We will add the weighted TI in our own TI loader later in this chapter.</p>
			<p>Before that, let’s dig into the heart of TI and see how it works<a id="_idTextAnchor182"/> internally.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor183"/>How TI works</h1>
			<p>Simply put, training a<a id="_idIndexMarker292"/> TI is finding a text embedding that matches the target image the best, such as its style, object, or face. The key is to find a new embedding that never existed in the current text encoder. As <em class="italic">Figure 9</em><em class="italic">.3</em>, from its original paper [1], shows:</p>
			<div><div><img src="img/B21263_09_03.jpg" alt="Figure 9.3: The outline of the text embedding and inversion process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3: The outline of the text embedding and inversion process</p>
			<p>The only job of the <a id="_idIndexMarker293"/>training is to find a new embedding represented by v *. and use S * as the token string placeholder; the string can be replaced by any string that does not exist in the tokenizer later. Once the new corresponding embedding vector is founded, the train is done. The output of the training is usually a vector with 768 numbers. That is why the TI file is tiny; it is just a couple of kilobytes.</p>
			<p>It is like the pretrained UNet is a pile of matrix magic boxes, one key (embedding) can unlock a box to have a pattern, a style, or an object. The number of boxes is way more than the limited keys from the text encoder provided. The training of a TI is done by providing a new key to unlock the unknown magic box. Throughout the training and inferencing, the original checkpoint model is untouched.</p>
			<p>In a precise way, the finding of the new embedding can be defined as follows:</p>
			<p>v * = arg v min E z∼E(x),y,ϵ∼N(0,1),t[||ϵ − ϵ θ(z t, t, c θ(y))|| 2 2]</p>
			<p>Let’s go through the formula from left to right, one by one:</p>
			<ul>
				<li>v * denotes the new embedding we are looking for.</li>
				<li>The arg : min notation is often used in statistics and optimization to denote the set of values that minimize a function. It is a useful notation because it allows us to talk about the minimum value of a function without having to specify the actual value of the minimum.</li>
				<li>The (E) is the loss expectation.</li>
				<li>z ∼ E(x) denotes that the input image will be encoded to latent space.</li>
				<li>y is the input text.</li>
				<li>e ∼  N(0,1) says that the initial noise latent is a strict Gaussian with <code>0</code> mean and <code>1</code> variance.</li>
				<li>c θ(y) represents a text encoder model that maps an input text string y into embedding vectors.</li>
				<li>ϵ θ(z t, t, c θ(y)) means that we provide the noised latent image z in the t step, step t itself and text embeddings c θ(y), and then generate noise vector from the UNet model.</li>
				<li>The 2 in || 2 means the square of the Euclidean distance. The 2 in || 2 means the data is in 2<a id="_idIndexMarker294"/> dimensions.</li>
			</ul>
			<p>Together, the formula shows how we can use Stable Diffusion’s training process to approximate a new embedding, v *, that generates the minimum loss.</p>
			<p>Next, let’s build a custom TI lo<a id="_idTextAnchor184"/>ader function.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor185"/>Building a custom TI loader</h1>
			<p>In this section, we are <a id="_idIndexMarker295"/>going to build a TI loader by implementing the preceding understanding into code and giving a TI weight parameter for the loader function.</p>
			<p>Before writing the function code, let’s first take a look at how a TI looks internally. Before running the following code, you will need to first download the TI file t<a id="_idTextAnchor186"/>o your storage.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor187"/>TI in the pt file format</h2>
			<p>Load a TI in<a id="_idIndexMarker296"/> the <code>pt</code> file format:</p>
			<pre class="source-code">
# load a pt TI
import torch
loaded_learned_embeds = torch.load("badhandsv5-neg.pt",
    map_location="cpu")
keys = list(loaded_learned_embeds.keys())
for key in keys:
    print(key,":",loaded_learned_embeds[key])</pre>
			<p>We can clearly see the key and paired value from the TI file:</p>
			<pre class="source-code">
string_to_token : {'*': 265}
string_to_param : {'*': tensor([[ 0.0399, -0.2473,  0.1252,  ...,  0.0455,  0.0845, -0.1463],
        [-0.1385, -0.0922, -0.0481,  ...,  0.1766, -0.1868,  0.3851]],
       requires_grad=True)}
name : bad-hands-5
step : 1364
sd_checkpoint : 7ab762a7
sd_checkpoint_name : blossom-extract</pre>
			<p>The most<a id="_idIndexMarker297"/> important value is the <code>tensor</code> object with the <code>string_to_param</code> key. We can take the tensor value out of it by using the following code:</p>
			<pre class="source-code">
string_to_token = loaded_learned_embeds['string_to_token']
string_to_param = loaded_learned_embeds['string_to_param']
# separate token and the embeds
trained_token = list(string_to_token.keys())[0]
embeds = string_to_par<a id="_idTextAnchor188"/>am[trained_token]</pre>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor189"/>TI in bin file format</h2>
			<p>Most of the TI from the<a id="_idIndexMarker298"/> Hugging Face concepts library is in the <code>bin</code> format. The <code>bin</code> structure is even simpler than the <code>pt</code> one:</p>
			<pre class="source-code">
import torch
loaded_learned_embeds = torch.load("midjourney_style.bin",
    map_location="cpu")
keys = list(loaded_learned_embeds.keys())
for key in keys:
    print(key,":",loaded_learned_embeds[key])</pre>
			<p>We will see this – a dictionary with just one key and one value:</p>
			<pre class="source-code">
&lt;midjourney-style&gt; : tensor([-5.9785e-02, -3.8523e-02,  5.1913e-02,  8.0925e-03, -6.2018e-02,
         1.3361e-01,  1.3679e-01,  8.2224e-02, -2.0598e-01,  1.8543e-02,
         1.9180e-01, -1.5537e-01, -1.5216e-01, -1.2607e-01, -1.9420e-01,
         1.0445e-01,  1.6942e-01,  4.2150e-02, -2.7406e-01,  1.8115e-01,
...
])</pre>
			<p>Extracting the <a id="_idIndexMarker299"/>tensor object is as simple as doing the following:</p>
			<pre class="source-code">
keys = list(loaded_learned_embeds.keys())
embeds =  loaded_learned_embed<a id="_idTextAnchor190"/>s[keys[0]] * weight</pre>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor191"/>Detailed steps to build a TI loader</h2>
			<p>Here are the detailed<a id="_idIndexMarker300"/> steps to load a TI with weight:</p>
			<ol>
				<li><code>emb_params</code> key to store the embedding tensor.<p class="list-inset">Use this function to load a TI in the model initialization stage or image generation stage:</p><pre class="source-code">
def load_textual_inversion(</pre><pre class="source-code">
    learned_embeds_path,</pre><pre class="source-code">
    token,</pre><pre class="source-code">
    text_encoder,</pre><pre class="source-code">
    tokenizer,</pre><pre class="source-code">
    weight = 0.5,</pre><pre class="source-code">
    device = "cpu"</pre><pre class="source-code">
):</pre><pre class="source-code">
    loaded_learned_embeds = \</pre><pre class="source-code">
        torch.load(learned_embeds_path, map_location=device)</pre><pre class="source-code">
    if "string_to_token" in loaded_learned_embeds:</pre><pre class="source-code">
        string_to_token = \</pre><pre class="source-code">
            loaded_learned_embeds['string_to_token']</pre><pre class="source-code">
        string_to_param = \</pre><pre class="source-code">
            loaded_learned_embeds['string_to_param']</pre><pre class="source-code">
        # separate token and the embeds</pre><pre class="source-code">
        trained_token = list(string_to_token.keys())[0]</pre><pre class="source-code">
        embeds = string_to_param[trained_token]</pre><pre class="source-code">
        embeds = embeds[0] * weight</pre><pre class="source-code">
    elif "emb_params" in loaded_learned_embeds:</pre><pre class="source-code">
        embeds = loaded_learned_embeds["emb_params"][0] * weight</pre><pre class="source-code">
    else:</pre><pre class="source-code">
        keys = list(loaded_learned_embeds.keys())</pre><pre class="source-code">
        embeds =  loaded_learned_embeds[keys[0]] * weight</pre><pre class="source-code">
    # ...</pre><p class="list-inset">Let’s <a id="_idIndexMarker301"/>break down the preceding code:</p><ul><li><code>torch.load(learned_embeds_path, map_location=device)</code> loads the learned embeddings from the specified file using PyTorch’s <code>torch.load</code> function</li><li><code>if "string_to_token" in loaded_learned_embeds</code> then checks for a specific file structure where embeddings are stored in a dictionary with the <code>string_to_token</code> and <code>string_to_param</code> keys, and extracts the token and embeddings from this structure</li><li><code>elif "emb_params" in loaded_learned_embeds</code> then handles a different structure where embeddings are directly stored under the <code>emb_params</code> key</li><li><code>else:</code> then handles a generic structure by assuming the embeddings are stored under the first key of the dictionary</li></ul><p class="list-inset">In essence, the weight serves as a multiplier for each element of the embedding vector, fine-tuning the intensity of the TI effect. For example, a weight value of <code>1.0</code> would apply the TI at full strength, while a value of <code>0.5</code> would apply it at half strength.</p></li>
				<li>Cast data to the<a id="_idIndexMarker302"/> same type of Stable Diffusion text encoder:<pre class="source-code">
dtype = text_encoder.get_input_embeddings().weight.dtype</pre><pre class="source-code">
embeds.to(dtype)</pre></li>
				<li>Add the token to the tokenizer:<pre class="source-code">
token = token if token is not None else trained_token</pre><pre class="source-code">
num_added_tokens = tokenizer.add_tokens(token)</pre><pre class="source-code">
if num_added_tokens == 0:</pre><pre class="source-code">
    raise ValueError(</pre><pre class="source-code">
        f"""The tokenizer already contains the token {token}.</pre><pre class="source-code">
        Please pass a different `token` that is not already in </pre><pre class="source-code">
        the tokenizer."""</pre><pre class="source-code">
    )</pre><p class="list-inset">The code will raise an exception if the added token already exists to prevent overriding existing tokens.</p></li>
				<li>Get the<a id="_idIndexMarker303"/> token ID and add the new embedding to the text encoder:</li>
			</ol>
			<pre class="source-code">
# resize the token embeddings
text_encoder.resize_token_embeddings(len(tokenizer))
# get the id for the token and assign the embeds
token_id = tokenizer.convert_tokens_to_ids(token)
text_encoder.get_input_embeddings().weight.data[token_id] = embeds</pre>
			<p>That is all the code needs to load most of the existing TI from both the Hu<a id="_idTextAnchor192"/>gging Face and Civitai.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor193"/>Putting all of the code together</h2>
			<p>Let’s put all of the <a id="_idIndexMarker304"/>code blocks together into one function – <code>load_textual_inversion</code>:</p>
			<pre class="source-code">
def load_textual_inversion(
    learned_embeds_path,
    token,
    text_encoder,
    tokenizer,
    weight = 0.5,
    device = "cpu"
):
    loaded_learned_embeds = torch.load(learned_embeds_path, 
        map_location=device)
    if "string_to_token" in loaded_learned_embeds:
        string_to_token = loaded_learned_embeds['string_to_token']
        string_to_param = loaded_learned_embeds['string_to_param']
        # separate token and the embeds
        trained_token = list(string_to_token.keys())[0]
        embeds = string_to_param[trained_token]
        embeds = embeds[0] * weight
    elif "emb_params" in loaded_learned_embeds:
        embeds = loaded_learned_embeds["emb_params"][0] * weight
    else:
        keys = list(loaded_learned_embeds.keys())
        embeds =  loaded_learned_embeds[keys[0]] * weight
    # cast to dtype of text_encoder
    dtype = text_encoder.get_input_embeddings().weight.dtype
    embeds.to(dtype)
    # add the token in tokenizer
    token = token if token is not None else trained_token
    num_added_tokens = tokenizer.add_tokens(token)
    if num_added_tokens == 0:
        raise ValueError(
            f"""The tokenizer already contains the token {token}.
            Please pass a different `token` that is not already in the 
            tokenizer."""
        )
    # resize the token embeddings
    text_encoder.resize_token_embeddings(len(tokenizer))
    # get the id for the token and assign the embeds
    token_id = tokenizer.convert_tokens_to_ids(token)
    text_encoder.get_input_embeddings().weight.data[token_id] = embeds
    return (tokenizer,text_encoder)</pre>
			<p>To use it, we need to <a id="_idIndexMarker305"/>have both <code>tokenizer</code> and <code>text_encoder</code> from the pipeline object:</p>
			<pre class="source-code">
text_encoder = pipe.text_encoder
tokenizer = pipe.tokenizer</pre>
			<p>Then load it by calling the newly created function:</p>
			<pre class="source-code">
load_textual_inversion(
    learned_embeds_path = "learned_embeds.bin",
    token = "colorful-magic-style",
    text_encoder = text_encoder,
    tokenizer = tokenizer,
    weight = 0.5,
    device = "cuda"
)</pre>
			<p>Now, use the same inference code to generate an image. Note that this time, we are using TI with a weight of <code>0.5</code>, so, let’s see whether anything is different compared with the original one with a weight of <code>1.0</code>:</p>
			<pre class="source-code">
prompt = "a high quality photo of a futuristic city in deep space, colorful-magic-style"
image = pipe(
    prompt,
    num_inference_steps = 50,
    generator = torch.Generator("cuda").manual_seed(1)
).images[0]
image</pre>
			<p>The result seems <a id="_idIndexMarker306"/>quite good (see <em class="italic">Figure 9</em><em class="italic">.4</em>):</p>
			<div><div><img src="img/B21263_09_04.jpg" alt="Figure 9.4: A futuristic city in deep space, with TI loaded by a custom function"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4: A futuristic city in deep space, with TI loaded by a custom function</p>
			<p>The result seems <a id="_idIndexMarker307"/>even better than the one using Diffusers’ one-line TI loader. Another advantage of our custom loader is that we can now freely give w<a id="_idTextAnchor194"/>eight to the loaded model.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor195"/>Summary</h1>
			<p>This chapter discussed what Stable Diffusion TI is and the difference between it and LoRA. Then, we introduced a quick way to load any TI into Diffusers to apply a new pattern, style, or object in the generation pipeline.</p>
			<p>Then, we dove into the core of TI and learned about how it is trained and how it works. Based on the understanding of how it works, we went a step further to implement a TI loader with the capability of accepting a TI weight.</p>
			<p>Lastly, we provided a piece of sample code to call the custom TI loader and then generate an image with a weight of <code>0.5</code>.</p>
			<p>In the next chapter, we'll explore ways to maximize the power of prompts and unlock their full potential.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor196"/>References</h1>
			<ol>
				<li>Rinon et al., <em class="italic">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</em>: <a href="https://arxiv.org/abs/2208.01618">https://arxiv.org/abs/2208.01618</a> and <a href="https://textual-inversion.github.io/&#13;">https://textual-inversion.github.io/</a></li>
				<li>Hugging Face, <em class="italic">Textual </em><em class="italic">Inversion</em>: <a href="https://huggingface.co/docs/diffusers/main/en/training/text_inversion#how-it-works">https://huggingface.co/docs/diffusers/main/en/training/text_inversion#how-it-works</a></li>
				<li>St<em class="italic">able Diffusion concepts library</em>: <a href="https://huggingface.co/sd-concepts-library">https://huggingface.co/sd-concepts-library</a>  <a href="https://huggingface.co/sd-concepts-library&#13;"/></li>
				<li>Civitai: <a href="https://civitai.com&#13;">https://civitai.com</a></li>
				<li><em class="italic">Midjourney style on Stable </em><em class="italic">Diffusion</em>: <a href="https://huggingface.co/sd-concepts-library/midjourney-style&#13;">https://huggingface.co/sd-concepts-library/midjourney-style</a></li>
				<li>OpenAI’s CLIP: <a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a></li>
			</ol>
		</div>
	</body></html>