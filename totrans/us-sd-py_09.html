<html><head></head><body>
		<div id="_idContainer059">
			<h1 id="_idParaDest-111" class="chapter-number"><a id="_idTextAnchor178"/>9</h1>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor179"/>Using Textual Inversion</h1>
			<p><strong class="bold">Textual inversion</strong> (<strong class="bold">TI</strong>) is <a id="_idIndexMarker277"/>another way to provide additional capabilities to a pretrained model. Unlike <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>), discussed in <a href="B21263_08.xhtml#_idTextAnchor153"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, which is a<a id="_idIndexMarker278"/> fine-tuning technique applied to the text encoder and the UNet attention weights, TI is a technique to add new <strong class="bold">embedding</strong> space<a id="_idIndexMarker279"/> based on the <span class="No-Break">trained data.</span></p>
			<p>In the context of Stable Diffusion, <strong class="bold">text embedding</strong> refers<a id="_idIndexMarker280"/> to the representation of text data as numerical vectors in a high dimensional space, allowing for manipulation and processing by machine learning algorithms. Specifically, in the case of Stable Diffusion, text embeddings <a id="_idIndexMarker281"/>are typically created using the <strong class="bold">Contrastive Language-Image Pretraining</strong> (<strong class="bold">CLIP</strong>) [<span class="No-Break">6] model.</span></p>
			<p>To train a TI model, you only need a minimal set of three to five images, resulting in a compact <strong class="source-inline">pt</strong> or <strong class="source-inline">bin</strong> file, typically just a few kilobytes in size. This makes TI a highly efficient method for incorporating new elements, concepts, or styles into your pretrained checkpoint model while maintaining <span class="No-Break">exceptional portability.</span></p>
			<p>In this chapter, we will first start using TI with the TI loader from the <strong class="source-inline">diffusers</strong> package, then delve into the core of TI to uncover how it works internally, and finally build a custom TI loader to have the TI weight applied to the <span class="No-Break">image generation.</span></p>
			<p>Here are the topics we are going <span class="No-Break">to cover:</span></p>
			<ul>
				<li>Diffusers inference <span class="No-Break">using TI</span></li>
				<li>How <span class="No-Break">TI works</span></li>
				<li>Build a custom <span class="No-Break">TI loader</span></li>
			</ul>
			<p>By the end of this chapter, you will be able to start using any type of TI shared by the community and also build your application to <span class="No-Break">load TI.</span></p>
			<p>Let’s start leveraging the power of Stable <span class="No-Break">Diffusion<a id="_idTextAnchor180"/> TI.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor181"/>Diffusers inference using TI</h1>
			<p>Before <a id="_idIndexMarker282"/>diving into how TI works internally, let’s take <a id="_idIndexMarker283"/>a look at how to use TI <span class="No-Break">using Diffusers.</span></p>
			<p>There are countless pretrained TIs shared in the Hugging Face’s Stable Diffusion concepts library [3] and CIVITAI [4]. For example, one of the most downloaded TIs from the Stable Diffusion concepts library is <strong class="source-inline">sd-concepts-library/midjourney-style</strong> [5]. We can start using it by simply referencing this name in the code; Diffusers will download the model <span class="No-Break">data automatically:</span></p>
			<ol>
				<li>Let’s initialize a Stable <span class="No-Break">Diffusion pipeline:</span><pre class="source-code">
# initialize model</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
import torch</pre><pre class="source-code">
model_id = "stablediffusionapi/deliberate-v2"</pre><pre class="source-code">
pipe = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    model_id,</pre><pre class="source-code">
    torch_dtype=torch.float16</pre><pre class="source-code">
).to("cuda")</pre></li>
				<li>Generate an image without <span class="No-Break">TI involved:</span><pre class="source-code">
# without using TI</pre><pre class="source-code">
prompt = "a high quality photo of a futuristic city in deep \ </pre><pre class="source-code">
space, midjourney-style"</pre><pre class="source-code">
image = pipe(</pre><pre class="source-code">
    prompt,</pre><pre class="source-code">
    num_inference_steps = 50,</pre><pre class="source-code">
    generator = torch.Generator("cuda").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
image</pre><p class="list-inset">In the prompt, <strong class="source-inline">midjourney-style</strong> is used, which will be given as the name of the TI. Without applying the name, we will see an image generated, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B21263_09_01.jpg" alt="Figure 9.1: A futuristic city in deep space without TI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1: A futuristic city in deep space without TI</p>
			<ol>
				<li value="3">Generate<a id="_idIndexMarker284"/> an image<a id="_idIndexMarker285"/> with <span class="No-Break">TI involved.</span><p class="list-inset">Now, let’s load the TI into the Stable Diffusion pipeline and give it a name, <strong class="source-inline">midjourney-style</strong>, to represent the newly <span class="No-Break">added embeddings:</span></p><pre class="source-code">
pipe.load_textual_inversion(</pre><pre class="source-code">
    "sd-concepts-library/midjourney-style",</pre><pre class="source-code">
    token = "midjourney-style"</pre><pre class="source-code">
)</pre><p class="list-inset">The<a id="_idIndexMarker286"/> preceding code will download<a id="_idIndexMarker287"/> the TI automatically and then add it to the pipeline model. Execute the same prompt and pipeline again, and we will get a completely new image, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B21263_09_02.jpg" alt="Figure 9.2: A futuristic city in deep space with TI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2: A futuristic city in deep space with TI</p>
			<p>Yes, it looks and<a id="_idIndexMarker288"/> feels like an image generated by <a id="_idIndexMarker289"/>Midjourney, but it is actually generated by Stable Diffusion. The “<em class="italic">inversion</em>” in the name of the TI indicates that we can inverse any new name to the new embeddings, for example, if we give a new token a name such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">colorful-magic-style</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
pipe.load_textual_inversion(
    "sd-concepts-library/midjourney-style",
    token = "colorful-magic-style"
)</pre>
			<p>We will get the<a id="_idIndexMarker290"/> same image <a id="_idIndexMarker291"/>because we use <strong class="source-inline">midjourney-style</strong> as the name of the TI. This time, we “inverse” <strong class="source-inline">colorful-magic-style</strong> into the new embeddings. However, the <strong class="source-inline">load_textual_inversion</strong> function provided by Diffusers does not provide a <strong class="source-inline">weight</strong> parameter for users to load a TI with a certain weight. We will add the weighted TI in our own TI loader later in <span class="No-Break">this chapter.</span></p>
			<p>Before that, let’s dig into the heart of TI and see how it <span class="No-Break">works<a id="_idTextAnchor182"/> internally.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor183"/>How TI works</h1>
			<p>Simply put, training a<a id="_idIndexMarker292"/> TI is finding a text embedding that matches the target image the best, such as its style, object, or face. The key is to find a new embedding that never existed in the current text encoder. As <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.3</em>, from its original paper [<span class="No-Break">1], shows:</span></p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B21263_09_03.jpg" alt="Figure 9.3: The outline of the text embedding and inversion process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3: The outline of the text embedding and inversion process</p>
			<p>The only job of the <a id="_idIndexMarker293"/>training is to find a new embedding represented by <span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span>. and use <span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span> as the token string placeholder; the string can be replaced by any string that does not exist in the tokenizer later. Once the new corresponding embedding vector is founded, the train is done. The output of the training is usually a vector with 768 numbers. That is why the TI file is tiny; it is just a couple <span class="No-Break">of kilobytes.</span></p>
			<p>It is like the pretrained UNet is a pile of matrix magic boxes, one key (embedding) can unlock a box to have a pattern, a style, or an object. The number of boxes is way more than the limited keys from the text encoder provided. The training of a TI is done by providing a new key to unlock the unknown magic box. Throughout the training and inferencing, the original checkpoint model <span class="No-Break">is untouched.</span></p>
			<p>In a precise way, the finding of the new embedding can be defined <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">a</span><span class="_-----MathTools-_Math_Variable_v-normal">r</span><span class="_-----MathTools-_Math_Variable_v-normal">g</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Variable_v-bold">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Variable_v-bold">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Number">1</span><span class="_-----MathTools-_Math_Number">)</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number">2</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">]</span></span></p>
			<p>Let’s go through the formula from left to right, one <span class="No-Break">by one:</span></p>
			<ul>
				<li><span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span> denotes the new embedding we are <span class="No-Break">looking for.</span></li>
				<li>The <span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">g</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Space">: </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Variable">i</span><span class="_-----MathTools-_Math_Variable">n</span> notation is often used in statistics and optimization to denote the set of values that minimize a function. It is a useful notation because it allows us to talk about the minimum value of a function without having to specify the actual value of <span class="No-Break">the minimum.</span></li>
				<li>The <span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">E</span><span class="_-----MathTools-_Math_Base">)</span> is the <span class="No-Break">loss expectation.</span></li>
				<li><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">E</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">x</span><span class="_-----MathTools-_Math_Variable">)</span> denotes that the input image will be encoded to <span class="No-Break">latent space.</span></li>
				<li><span class="_-----MathTools-_Math_Variable">y</span> is the <span class="No-Break">input text.</span></li>
				<li><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∼</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-bold-italic">N</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Number">0,1</span><span class="_-----MathTools-_Math_Number">)</span> says that the initial noise latent is a strict Gaussian with <strong class="source-inline">0</strong> mean and <span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break"> variance.</span></li>
				<li><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span> represents a text encoder model that maps an input text string <span class="_-----MathTools-_Math_Variable">y</span> into <span class="No-Break">embedding vectors.</span></li>
				<li><span class="_-----MathTools-_Math_Symbol_Extended">ϵ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">z</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Operator">,</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Base">)</span> means that we provide the noised latent image <span class="_-----MathTools-_Math_Variable">z</span> in the <span class="_-----MathTools-_Math_Variable">t</span> step, step <span class="_-----MathTools-_Math_Variable">t</span> itself and text embeddings <span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">θ</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">y</span><span class="_-----MathTools-_Math_Variable">)</span>, and then generate noise vector from the <span class="No-Break">UNet model.</span></li>
				<li>The 2 in <span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> means the square of the Euclidean distance. The 2 in <span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base">|</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> means the data is in <span class="No-Break">2</span><span class="No-Break"><a id="_idIndexMarker294"/></span><span class="No-Break"> dimensions.</span></li>
			</ul>
			<p>Together, the formula shows how we can use Stable Diffusion’s training process to approximate a new embedding, <span class="_-----MathTools-_Math_Variable">v</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal">*</span>, that generates the <span class="No-Break">minimum loss.</span></p>
			<p>Next, let’s build a custom TI <span class="No-Break">lo<a id="_idTextAnchor184"/>ader function.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor185"/>Building a custom TI loader</h1>
			<p>In this section, we are <a id="_idIndexMarker295"/>going to build a TI loader by implementing the preceding understanding into code and giving a TI weight parameter for the <span class="No-Break">loader function.</span></p>
			<p>Before writing the function code, let’s first take a look at how a TI looks internally. Before running the following code, you will need to first download the TI file t<a id="_idTextAnchor186"/>o <span class="No-Break">your storage.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor187"/>TI in the pt file format</h2>
			<p>Load a TI in<a id="_idIndexMarker296"/> the <strong class="source-inline">pt</strong> <span class="No-Break">file format:</span></p>
			<pre class="source-code">
# load a pt TI
import torch
loaded_learned_embeds = torch.load("badhandsv5-neg.pt",
    map_location="cpu")
keys = list(loaded_learned_embeds.keys())
for key in keys:
    print(key,":",loaded_learned_embeds[key])</pre>
			<p>We can clearly see the key and paired value from the <span class="No-Break">TI file:</span></p>
			<pre class="source-code">
string_to_token : {'*': 265}
string_to_param : {'*': tensor([[ 0.0399, -0.2473,  0.1252,  ...,  0.0455,  0.0845, -0.1463],
        [-0.1385, -0.0922, -0.0481,  ...,  0.1766, -0.1868,  0.3851]],
       requires_grad=True)}
name : bad-hands-5
step : 1364
sd_checkpoint : 7ab762a7
sd_checkpoint_name : blossom-extract</pre>
			<p>The most<a id="_idIndexMarker297"/> important value is the <strong class="source-inline">tensor</strong> object with the <strong class="source-inline">string_to_param</strong> key. We can take the tensor value out of it by using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
string_to_token = loaded_learned_embeds['string_to_token']
string_to_param = loaded_learned_embeds['string_to_param']
# separate token and the embeds
trained_token = list(string_to_token.keys())[0]
embeds = string_to_par<a id="_idTextAnchor188"/>am[trained_token]</pre>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor189"/>TI in bin file format</h2>
			<p>Most of the TI from the<a id="_idIndexMarker298"/> Hugging Face concepts library is in the <strong class="source-inline">bin</strong> format. The <strong class="source-inline">bin</strong> structure is even simpler than the <span class="No-Break"><strong class="source-inline">pt</strong></span><span class="No-Break"> one:</span></p>
			<pre class="source-code">
import torch
loaded_learned_embeds = torch.load("midjourney_style.bin",
    map_location="cpu")
keys = list(loaded_learned_embeds.keys())
for key in keys:
    print(key,":",loaded_learned_embeds[key])</pre>
			<p>We will see this – a dictionary with just one key and <span class="No-Break">one value:</span></p>
			<pre class="source-code">
&lt;midjourney-style&gt; : tensor([-5.9785e-02, -3.8523e-02,  5.1913e-02,  8.0925e-03, -6.2018e-02,
         1.3361e-01,  1.3679e-01,  8.2224e-02, -2.0598e-01,  1.8543e-02,
         1.9180e-01, -1.5537e-01, -1.5216e-01, -1.2607e-01, -1.9420e-01,
         1.0445e-01,  1.6942e-01,  4.2150e-02, -2.7406e-01,  1.8115e-01,
...
])</pre>
			<p>Extracting the <a id="_idIndexMarker299"/>tensor object is as simple as doing <span class="No-Break">the following:</span></p>
			<pre class="source-code">
keys = list(loaded_learned_embeds.keys())
embeds =  loaded_learned_embed<a id="_idTextAnchor190"/>s[keys[0]] * weight</pre>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor191"/>Detailed steps to build a TI loader</h2>
			<p>Here are the detailed<a id="_idIndexMarker300"/> steps to load a TI <span class="No-Break">with weight:</span></p>
			<ol>
				<li><strong class="bold">Load the embeddings</strong>: We will reuse the code from the preceding two conditions, and add another condition that some TIs use the <strong class="source-inline">emb_params</strong> key to store the <span class="No-Break">embedding tensor.</span><p class="list-inset">Use this function to load a TI in the model initialization stage or image <span class="No-Break">generation stage:</span></p><pre class="source-code">
def load_textual_inversion(</pre><pre class="source-code">
    learned_embeds_path,</pre><pre class="source-code">
    token,</pre><pre class="source-code">
    text_encoder,</pre><pre class="source-code">
    tokenizer,</pre><pre class="source-code">
    weight = 0.5,</pre><pre class="source-code">
    device = "cpu"</pre><pre class="source-code">
):</pre><pre class="source-code">
    loaded_learned_embeds = \</pre><pre class="source-code">
        torch.load(learned_embeds_path, map_location=device)</pre><pre class="source-code">
    if "string_to_token" in loaded_learned_embeds:</pre><pre class="source-code">
        string_to_token = \</pre><pre class="source-code">
            loaded_learned_embeds['string_to_token']</pre><pre class="source-code">
        string_to_param = \</pre><pre class="source-code">
            loaded_learned_embeds['string_to_param']</pre><pre class="source-code">
        # separate token and the embeds</pre><pre class="source-code">
        trained_token = list(string_to_token.keys())[0]</pre><pre class="source-code">
        embeds = string_to_param[trained_token]</pre><pre class="source-code">
        embeds = embeds[0] * weight</pre><pre class="source-code">
    elif "emb_params" in loaded_learned_embeds:</pre><pre class="source-code">
        embeds = loaded_learned_embeds["emb_params"][0] * weight</pre><pre class="source-code">
    else:</pre><pre class="source-code">
        keys = list(loaded_learned_embeds.keys())</pre><pre class="source-code">
        embeds =  loaded_learned_embeds[keys[0]] * weight</pre><pre class="source-code">
    # ...</pre><p class="list-inset">Let’s <a id="_idIndexMarker301"/>break down the <span class="No-Break">preceding code:</span></p><ul><li><strong class="source-inline">torch.load(learned_embeds_path, map_location=device)</strong> loads the learned embeddings from the specified file using PyTorch’s <span class="No-Break"><strong class="source-inline">torch.load</strong></span><span class="No-Break"> function</span></li><li><strong class="source-inline">if "string_to_token" in loaded_learned_embeds</strong> then checks for a specific file structure where embeddings are stored in a dictionary with the <strong class="source-inline">string_to_token</strong> and <strong class="source-inline">string_to_param</strong> keys, and extracts the token and embeddings from <span class="No-Break">this structure</span></li><li><strong class="source-inline">elif "emb_params" in loaded_learned_embeds</strong> then handles a different structure where embeddings are directly stored under the <span class="No-Break"><strong class="source-inline">emb_params</strong></span><span class="No-Break"> key</span></li><li><strong class="source-inline">else:</strong> then handles a generic structure by assuming the embeddings are stored under the first key of <span class="No-Break">the dictionary</span></li></ul><p class="list-inset">In essence, the weight serves as a multiplier for each element of the embedding vector, fine-tuning the intensity of the TI effect. For example, a weight value of <strong class="source-inline">1.0</strong> would apply the TI at full strength, while a value of <strong class="source-inline">0.5</strong> would apply it at <span class="No-Break">half strength.</span></p></li>
				<li>Cast data to the<a id="_idIndexMarker302"/> same type of Stable Diffusion <span class="No-Break">text encoder:</span><pre class="source-code">
dtype = text_encoder.get_input_embeddings().weight.dtype</pre><pre class="source-code">
embeds.to(dtype)</pre></li>
				<li>Add the token to <span class="No-Break">the tokenizer:</span><pre class="source-code">
token = token if token is not None else trained_token</pre><pre class="source-code">
num_added_tokens = tokenizer.add_tokens(token)</pre><pre class="source-code">
if num_added_tokens == 0:</pre><pre class="source-code">
    raise ValueError(</pre><pre class="source-code">
        f"""The tokenizer already contains the token {token}.</pre><pre class="source-code">
        Please pass a different `token` that is not already in </pre><pre class="source-code">
        the tokenizer."""</pre><pre class="source-code">
    )</pre><p class="list-inset">The code will raise an exception if the added token already exists to prevent overriding <span class="No-Break">existing tokens.</span></p></li>
				<li>Get the<a id="_idIndexMarker303"/> token ID and add the new embedding to the <span class="No-Break">text encoder:</span></li>
			</ol>
			<pre class="source-code">
# resize the token embeddings
text_encoder.resize_token_embeddings(len(tokenizer))
# get the id for the token and assign the embeds
token_id = tokenizer.convert_tokens_to_ids(token)
text_encoder.get_input_embeddings().weight.data[token_id] = embeds</pre>
			<p>That is all the code needs to load most of the existing TI from both the Hu<a id="_idTextAnchor192"/>gging Face <span class="No-Break">and Civitai.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor193"/>Putting all of the code together</h2>
			<p>Let’s put all of the <a id="_idIndexMarker304"/>code blocks together into one function – <span class="No-Break"><strong class="source-inline">load_textual_inversion</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
def load_textual_inversion(
    learned_embeds_path,
    token,
    text_encoder,
    tokenizer,
    weight = 0.5,
    device = "cpu"
):
    loaded_learned_embeds = torch.load(learned_embeds_path, 
        map_location=device)
    if "string_to_token" in loaded_learned_embeds:
        string_to_token = loaded_learned_embeds['string_to_token']
        string_to_param = loaded_learned_embeds['string_to_param']
        # separate token and the embeds
        trained_token = list(string_to_token.keys())[0]
        embeds = string_to_param[trained_token]
        embeds = embeds[0] * weight
    elif "emb_params" in loaded_learned_embeds:
        embeds = loaded_learned_embeds["emb_params"][0] * weight
    else:
        keys = list(loaded_learned_embeds.keys())
        embeds =  loaded_learned_embeds[keys[0]] * weight
    # cast to dtype of text_encoder
    dtype = text_encoder.get_input_embeddings().weight.dtype
    embeds.to(dtype)
    # add the token in tokenizer
    token = token if token is not None else trained_token
    num_added_tokens = tokenizer.add_tokens(token)
    if num_added_tokens == 0:
        raise ValueError(
            f"""The tokenizer already contains the token {token}.
            Please pass a different `token` that is not already in the 
            tokenizer."""
        )
    # resize the token embeddings
    text_encoder.resize_token_embeddings(len(tokenizer))
    # get the id for the token and assign the embeds
    token_id = tokenizer.convert_tokens_to_ids(token)
    text_encoder.get_input_embeddings().weight.data[token_id] = embeds
    return (tokenizer,text_encoder)</pre>
			<p>To use it, we need to <a id="_idIndexMarker305"/>have both <strong class="source-inline">tokenizer</strong> and <strong class="source-inline">text_encoder</strong> from the <span class="No-Break">pipeline object:</span></p>
			<pre class="source-code">
text_encoder = pipe.text_encoder
tokenizer = pipe.tokenizer</pre>
			<p>Then load it by calling the newly <span class="No-Break">created function:</span></p>
			<pre class="source-code">
load_textual_inversion(
    learned_embeds_path = "learned_embeds.bin",
    token = "colorful-magic-style",
    text_encoder = text_encoder,
    tokenizer = tokenizer,
    weight = 0.5,
    device = "cuda"
)</pre>
			<p>Now, use the same inference code to generate an image. Note that this time, we are using TI with a weight of <strong class="source-inline">0.5</strong>, so, let’s see whether anything is different compared with the original one with a weight <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">1.0</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
prompt = "a high quality photo of a futuristic city in deep space, colorful-magic-style"
image = pipe(
    prompt,
    num_inference_steps = 50,
    generator = torch.Generator("cuda").manual_seed(1)
).images[0]
image</pre>
			<p>The result seems <a id="_idIndexMarker306"/>quite good (see <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">):</span></p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B21263_09_04.jpg" alt="Figure 9.4: A futuristic city in deep space, with TI loaded by a custom function"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4: A futuristic city in deep space, with TI loaded by a custom function</p>
			<p>The result seems <a id="_idIndexMarker307"/>even better than the one using Diffusers’ one-line TI loader. Another advantage of our custom loader is that we can now freely give w<a id="_idTextAnchor194"/>eight to the <span class="No-Break">loaded model.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor195"/>Summary</h1>
			<p>This chapter discussed what Stable Diffusion TI is and the difference between it and LoRA. Then, we introduced a quick way to load any TI into Diffusers to apply a new pattern, style, or object in the <span class="No-Break">generation pipeline.</span></p>
			<p>Then, we dove into the core of TI and learned about how it is trained and how it works. Based on the understanding of how it works, we went a step further to implement a TI loader with the capability of accepting a <span class="No-Break">TI weight.</span></p>
			<p>Lastly, we provided a piece of sample code to call the custom TI loader and then generate an image with a weight <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.5</strong></span><span class="No-Break">.</span></p>
			<p>In the next chapter, we'll explore ways to maximize the power of prompts and unlock their <span class="No-Break">full potential.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor196"/>References</h1>
			<ol>
				<li>Rinon et al., <em class="italic">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion</em>: <a href="https://arxiv.org/abs/2208.01618">https://arxiv.org/abs/2208.01618</a> <span class="No-Break">and </span><a href="https://textual-inversion.github.io/&#13;"><span class="No-Break">https://textual-inversion.github.io/</span></a></li>
				<li>Hugging Face, <em class="italic">Textual </em><span class="No-Break"><em class="italic">Inversion</em></span><span class="No-Break">: </span><a href="https://huggingface.co/docs/diffusers/main/en/training/text_inversion#how-it-works"><span class="No-Break">https://huggingface.co/docs/diffusers/main/en/training/text_inversion#how-it-works</span></a></li>
				<li>St<em class="italic">able Diffusion concepts library</em>: <a href="https://huggingface.co/sd-concepts-library">https://huggingface.co/sd-concepts-library</a>  <a href="https://huggingface.co/sd-concepts-library&#13;"/></li>
				<li><span class="No-Break">Civitai: </span><a href="https://civitai.com&#13;"><span class="No-Break">https://civitai.com</span></a></li>
				<li><em class="italic">Midjourney style on Stable </em><span class="No-Break"><em class="italic">Diffusion</em></span><span class="No-Break">: </span><a href="https://huggingface.co/sd-concepts-library/midjourney-style&#13;"><span class="No-Break">https://huggingface.co/sd-concepts-library/midjourney-style</span></a></li>
				<li>OpenAI’s <span class="No-Break">CLIP: </span><a href="https://github.com/openai/CLIP"><span class="No-Break">https://github.com/openai/CLIP</span></a></li>
			</ol>
		</div>
	</body></html>