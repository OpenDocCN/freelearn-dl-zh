<html><head></head><body>
<div class="calibre1" id="_idContainer192">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">10</span></h1>
<h1 class="chaptertitle" id="_idParaDest-136"><span class="kobospan" id="kobo.2.1">Building Multimodal Applications with LLMs</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">In this chapter, we are going beyond LLMs, to introduce the concept of multimodality while building agents. </span><span class="kobospan" id="kobo.3.2">We will see the logic behind the combination of foundation models in different AI domains – language, images, and audio – into one single agent that can adapt to a variety of tasks. </span><span class="kobospan" id="kobo.3.3">By the end of this chapter, you will be able to build your own multimodal agent, providing it with the tools and LLMs needed to perform various AI tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.4.1">Throughout this chapter, we will cover the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.5.1">Introduction to multimodality and </span><strong class="screentext"><span class="kobospan" id="kobo.6.1">large multimodal models</span></strong><span class="kobospan" id="kobo.7.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.8.1">LMMs</span></strong><span class="kobospan" id="kobo.9.1">)</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.10.1">Examples of emerging LMMs</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.11.1">How to build a multimodal agent with single-modal LLMs using LangChain</span></li>
</ul>
<h1 class="heading" id="_idParaDest-137"><span class="kobospan" id="kobo.12.1">Technical requirements</span></h1>
<p class="normal"><span class="kobospan" id="kobo.13.1">To complete the tasks in this chapter, you will need the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.14.1">A Hugging Face account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.15.1">An OpenAI account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.16.1">Python 3.7.1 or later version.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.17.1">Python packages. </span><span class="kobospan" id="kobo.17.2">Make sure to have the following Python packages installed: </span><code class="inlinecode"><span class="kobospan" id="kobo.18.1">langchain</span></code><span class="kobospan" id="kobo.19.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.20.1">python-dotenv</span></code><span class="kobospan" id="kobo.21.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.22.1">huggingface_hub</span></code><span class="kobospan" id="kobo.23.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.24.1">streamlit</span></code><span class="kobospan" id="kobo.25.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.26.1">pytube</span></code><span class="kobospan" id="kobo.27.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.28.1">openai</span></code><span class="kobospan" id="kobo.29.1">, and </span><code class="inlinecode"><span class="kobospan" id="kobo.30.1">youtube_search</span></code><span class="kobospan" id="kobo.31.1">. </span><span class="kobospan" id="kobo.31.2">Those can be easily installed via </span><code class="inlinecode"><span class="kobospan" id="kobo.32.1">pip install</span></code><span class="kobospan" id="kobo.33.1"> in your terminal.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.34.1">You can find all the code and examples in the book’s GitHub repository at </span><a href="Chapter_10.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.35.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.36.1">.</span></p>
<h1 class="heading" id="_idParaDest-138"><span class="kobospan" id="kobo.37.1">Why multimodality?</span></h1>
<p class="normal"><span class="kobospan" id="kobo.38.1">In the context of Generative AI, multimodality</span><a id="_idIndexMarker712" class="calibre3"/><span class="kobospan" id="kobo.39.1"> refers to a model’s capability of processing data in various formats. </span><span class="kobospan" id="kobo.39.2">For example, a multimodal model can communicate with humans via text, speech, images, or even videos, making the interaction extremely smooth and “human-like.”</span></p>
<p class="normal1"><span class="kobospan" id="kobo.40.1">In </span><em class="italic"><span class="kobospan" id="kobo.41.1">Chapter 1</span></em><span class="kobospan" id="kobo.42.1">, we defined </span><strong class="screentext"><span class="kobospan" id="kobo.43.1">large foundation models</span></strong><span class="kobospan" id="kobo.44.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.45.1">LFMs</span></strong><span class="kobospan" id="kobo.46.1">) as a type of pre-trained generative AI model</span><a id="_idIndexMarker713" class="calibre3"/><span class="kobospan" id="kobo.47.1"> that offers immense versatility by being adaptable for various specific tasks. </span><span class="kobospan" id="kobo.47.2">LLMs, on the other hand, are a subset of foundation models that are able to process one type of data: natural language. </span><span class="kobospan" id="kobo.47.3">Even though LLMs have proven to be not only excellent text understanders and generators but also reasoning engines to power applications and copilots, it soon became clear that we could aim at even more powerful applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.48.1">The dream is to have intelligent systems that are capable of handling multiple data formats – text, images, audio, video, etc – always powered by the reasoning engine, which makes them able to plan and execute actions with an agentic approach. </span><span class="kobospan" id="kobo.48.2">Such an AI system would</span><a id="_idIndexMarker714" class="calibre3"/><span class="kobospan" id="kobo.49.1"> be a further milestone toward the reaching of </span><strong class="screentext"><span class="kobospan" id="kobo.50.1">artificial general intelligence</span></strong><span class="kobospan" id="kobo.51.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.52.1">AGI</span></strong><span class="kobospan" id="kobo.53.1">).</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.54.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.55.1">AGI is a hypothetical type of </span><strong class="screentext"><span class="kobospan" id="kobo.56.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.57.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.58.1">AI</span></strong><span class="kobospan" id="kobo.59.1">) that can perform any intellectual</span><a id="_idIndexMarker715" class="calibre3"/><span class="kobospan" id="kobo.60.1"> task that a human can. </span><span class="kobospan" id="kobo.60.2">AGI would have a general cognitive ability, similar to human intelligence, and be able to learn from experience, reason, plan, communicate, and solve problems across different domains. </span><span class="kobospan" id="kobo.60.3">An AGI system would also be able to “perceive” the world as we do, meaning that it could process data in different formats, from text to images to sounds. </span><span class="kobospan" id="kobo.60.4">Hence, AGI implies multimodality.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.61.1">Creating AGI is a primary goal of some AI research and a common topic in science fiction. </span><span class="kobospan" id="kobo.61.2">However, there is no consensus on how to achieve AGI, what criteria to use to measure it, or when it might be possible. </span><span class="kobospan" id="kobo.61.3">Some researchers argue that AGI could be achieved in years or decades, while others maintain that it might take a century or longer, or that it might never be achieved.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.62.1">However, AGI is not seen as the ultimate milestone in AI development. </span><span class="kobospan" id="kobo.62.2">In fact, in recent months another definition has emerged in the context of AI – that is, Strong AI or Super AI, referring to an AI system</span><a id="_idIndexMarker716" class="calibre3"/><span class="kobospan" id="kobo.63.1"> that is more capable than a human.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.64.1">At the time of writing this book (February 2024), LMMs such as GPT-4 Turbo with Vision are a reality. </span><span class="kobospan" id="kobo.64.2">However, those are not the only ways to reach multimodality. </span><span class="kobospan" id="kobo.64.3">In this chapter, we are going to examine how to merge multiple AI systems to reach a multimodal AI assistant. </span><span class="kobospan" id="kobo.64.4">The idea is that if we combine single-modal models, one for each data format we want to process, and then use an LLM as the brain of our agent to let it interact in dynamic ways with those models (that will be its tools), we can still achieve this goal. </span><span class="kobospan" id="kobo.64.5">The following diagram shows the structure of a multimodal application that integrates various single-modal tools to perform a task – in this case, describing a picture aloud. </span><span class="kobospan" id="kobo.64.6">The application uses image analysis to examine the picture, text generation to create some text that describes what it observes in the picture, and text-to-speech to convey this text to the user through speech. </span></p>
<p class="normal1"><span class="kobospan" id="kobo.65.1">The LLM acts as the “reasoning engine” of the application, invoking</span><a id="_idIndexMarker717" class="calibre3"/><span class="kobospan" id="kobo.66.1"> the proper tools needed to accomplish the user’s query.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.67.1"><img alt="A person talking to a speech bubble  Description automatically generated" src="../Images/B21714_10_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.68.1">Figure 10.1: Illustration of multimodal application with single-modal tools</span></p>
<p class="normal1"><span class="kobospan" id="kobo.69.1">In the upcoming section, we are going to explore various approaches to building multimodal applications, all based on the idea of combining existing single-modal tools or models.</span></p>
<h1 class="heading" id="_idParaDest-139"><span class="kobospan" id="kobo.70.1">Building a multimodal agent with LangChain</span></h1>
<p class="normal"><span class="kobospan" id="kobo.71.1">So far, we’ve covered the main </span><a id="_idIndexMarker718" class="calibre3"/><span class="kobospan" id="kobo.72.1">aspects of multimodality</span><a id="_idIndexMarker719" class="calibre3"/><span class="kobospan" id="kobo.73.1"> and how to achieve it with modern LFMs. </span><span class="kobospan" id="kobo.73.2">As we saw throughout Part 2 of this book, LangChain offers a variety of components that we leveraged massively, such as chains, agents, tools, and so on. </span><span class="kobospan" id="kobo.73.3">As a result, we already have all the ingredients we need to start building</span><a id="_idIndexMarker720" class="calibre3"/><span class="kobospan" id="kobo.74.1"> our multimodal</span><a id="_idIndexMarker721" class="calibre3"/><span class="kobospan" id="kobo.75.1"> agent.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.76.1">However, in this chapter, we will adopt three approaches to tackle the problem:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.77.1">The agentic, out-of-the-box approach</span></strong><span class="kobospan" id="kobo.78.1">: Here we will leverage the Azure Cognitive Services toolkit, which offers native integrations toward a set of AI models that can be consumed via API, and that covers various domains such as image, audio, OCR, etc.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.79.1">The agentic, custom approach</span></strong><span class="kobospan" id="kobo.80.1">: Here, we are going to select single models and tools (including defining custom tools) and concatenate them into a single agent that can leverage all of them.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.81.1">The hard-coded approach</span></strong><span class="kobospan" id="kobo.82.1">: Here, we are going to build</span><a id="_idIndexMarker722" class="calibre3"/><span class="kobospan" id="kobo.83.1"> separate chains and combine them into a sequential chain.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.84.1">In the upcoming sections, we will cover all these approaches with concrete examples.</span></p>
<h1 class="heading" id="_idParaDest-140"><span class="kobospan" id="kobo.85.1">Option 1: Using an out-of-the-box toolkit for Azure AI Services</span></h1>
<p class="normal"><span class="kobospan" id="kobo.86.1">Formerly known as Azure Cognitive Services, Azure AI Services </span><a id="_idIndexMarker723" class="calibre3"/><span class="kobospan" id="kobo.87.1">are a set of cloud-based APIs</span><a id="_idIndexMarker724" class="calibre3"/><span class="kobospan" id="kobo.88.1"> and AI services developed by Microsoft that enable developers and data scientists to add cognitive capabilities to their apps. </span><span class="kobospan" id="kobo.88.2">AI Services are meant to provide every developer with AI models to be integrated with programming languages such as Python, C#, or JavaScript.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.89.1">Azure AI Services cover various domains of AI, including speech, natural language, vision, and decision-making. </span><span class="kobospan" id="kobo.89.2">All those services come with models that can be consumed via API, and you can decide to:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.90.1">Leverage powerful pre-built models available as they are and ready to use.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.91.1">Customize those pre-built models with custom data so that they are tailored to your use case.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.92.1">Hence, considered all together, Azure AI Services can achieve the goal of multimodality, if properly orchestrated by an LLM as a reasoning engine, which is exactly the framework LangChain built.</span></p>
<h2 class="heading1" id="_idParaDest-141"><span class="kobospan" id="kobo.93.1">Getting Started with AzureCognitiveServicesToolkit</span></h2>
<p class="normal"><span class="kobospan" id="kobo.94.1">In fact, LangChain has a native</span><a id="_idIndexMarker725" class="calibre3"/><span class="kobospan" id="kobo.95.1"> integration with Azure AI Services called </span><strong class="screentext"><span class="kobospan" id="kobo.96.1">AzureCognitiveServicesToolkit</span></strong><span class="kobospan" id="kobo.97.1">, which can be passed as a parameter to an agent and leverage the multimodal capabilities of those models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.98.1">The toolkit makes it easier to incorporate Azure AI services’ capabilities – such as image analysis, form recognition, speech-to-text, and text-to-speech – within your application. </span><span class="kobospan" id="kobo.98.2">It can be used within an agent, which is then empowered to use the AI services to enhance its functionality and provide richer responses.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.99.1">Currently, the integration supports the following tools:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.100.1">AzureCogsImageAnalysisTool</span></strong><span class="kobospan" id="kobo.101.1">: Used to analyze and extract</span><a id="_idIndexMarker726" class="calibre3"/><span class="kobospan" id="kobo.102.1"> metadata from images.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.103.1">AzureCogsSpeech2TextTool</span></strong><span class="kobospan" id="kobo.104.1">: Used to convert speech</span><a id="_idIndexMarker727" class="calibre3"/><span class="kobospan" id="kobo.105.1"> to text.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.106.1">AzureCogsText2SpeechTool</span></strong><span class="kobospan" id="kobo.107.1">: Used to synthetize text</span><a id="_idIndexMarker728" class="calibre3"/><span class="kobospan" id="kobo.108.1"> to speech with neural voices.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.109.1">AzureCogsFormRecognizerTool</span></strong><span class="kobospan" id="kobo.110.1">: Used to</span><a id="_idIndexMarker729" class="calibre3"/><span class="kobospan" id="kobo.111.1"> perform </span><strong class="screentext"><span class="kobospan" id="kobo.112.1">optical character recognition</span></strong><span class="kobospan" id="kobo.113.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.114.1">OCR</span></strong><span class="kobospan" id="kobo.115.1">).</span></li>
</ul>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.116.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.117.1">OCR is a technology </span><a id="_idIndexMarker730" class="calibre3"/><span class="kobospan" id="kobo.118.1">that converts different types of documents, such as scanned paper documents, PDFs, or images captured by a digital camera, into editable and searchable data. </span><span class="kobospan" id="kobo.118.2">OCR can save time, cost, and resources by automating data entry and storage processes. </span><span class="kobospan" id="kobo.118.3">It can also enable access to and editing of the original content of historical, legal, or other types of documents.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.119.1">For example, if you ask an agent what you can make with some ingredients, and provide an image of eggs and flour, the agent can use the Azure AI Services Image Analysis tool to extract the caption, objects, and tags from the image, and then use the provided LLM to suggest some recipes based on the ingredients. </span><span class="kobospan" id="kobo.119.2">To implement this, let’s first set up our toolkit.</span></p>
<h3 class="heading2" id="_idParaDest-142"><span class="kobospan" id="kobo.120.1">Setting up the toolkit</span></h3>
<p class="normal"><span class="kobospan" id="kobo.121.1">To get started</span><a id="_idIndexMarker731" class="calibre3"/><span class="kobospan" id="kobo.122.1"> with the toolkit, you can follow these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.123.1">You first need to create a multi-service</span><a id="_idIndexMarker732" class="calibre3"/><span class="kobospan" id="kobo.124.1"> instance of Azure AI Services in Azure following the instructions at </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&amp;pivots=azportal" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.125.1">https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&amp;pivots=azportal</span></span></a><span class="kobospan" id="kobo.126.1">.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.127.1">A multi-service resource allows you to access multiple AI services with a single key and endpoint to be passed to LangChain as environmental variables. </span><span class="kobospan" id="kobo.127.2">You can find your keys and endpoint under the </span><strong class="screentext"><span class="kobospan" id="kobo.128.1">Keys and Endpoint</span></strong><span class="kobospan" id="kobo.129.1"> tab in your resource panel:</span></li>
</ol>
<figure class="mediaobject"><span class="kobospan" id="kobo.130.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_10_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.131.1">Figure 10.2: Screenshot of a multi-service instance of Azure AI Services</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.132.1">Once the resource is set, we can start building our LegalAgent. </span><span class="kobospan" id="kobo.132.2">To do so, the first thing we need to do is set the AI services environmental variables in order to configure the toolkit. </span><span class="kobospan" id="kobo.132.3">To do so, I’ve saved the following variables in my </span><code class="inlinecode"><span class="kobospan" id="kobo.133.1">.env</span></code><span class="kobospan" id="kobo.134.1"> file:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.135.1">AZURE_COGS_KEY = </span><span class="hljs-string"><span class="kobospan" id="kobo.136.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.137.1">your-api-key"</span></span><span class="kobospan" id="kobo.138.1">
AZURE_COGS_ENDPOINT = </span><span class="hljs-string"><span class="kobospan" id="kobo.139.1">"your-endpoint</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.140.1">AZURE_COGS_REGION = "</span></span><span class="kobospan" id="kobo.141.1">your-region</span><span class="hljs-string"><span class="kobospan" id="kobo.142.1">"</span></span>
</code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.143.1">Then, you can load them as always alongside the other environmental variables:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.144.1">import</span></span><span class="kobospan" id="kobo.145.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.146.1">from</span></span><span class="kobospan" id="kobo.147.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.148.1">import</span></span><span class="kobospan" id="kobo.149.1"> load_dotenv
load_dotenv()
azure_cogs_key = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.150.1">"AZURE_COGS_KEY"</span></span><span class="kobospan" id="kobo.151.1">]
azure_cogs_endpoint = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.152.1">"AZURE_COGS_ENDPOINT"</span></span><span class="kobospan" id="kobo.153.1">]
azure_cogs_region = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.154.1">"AZURE_COGS_REGION"</span></span><span class="kobospan" id="kobo.155.1">]
openai_api_key = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.156.1">'OPENAI_API_KEY'</span></span><span class="kobospan" id="kobo.157.1">]
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.158.1">Now, we can configure our toolkit</span><a id="_idIndexMarker733" class="calibre3"/><span class="kobospan" id="kobo.159.1"> and also see which tools we have, alongside their description:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.160.1">from</span></span><span class="kobospan" id="kobo.161.1"> langchain.agents.agent_toolkits </span><span class="hljs-keyword"><span class="kobospan" id="kobo.162.1">import</span></span><span class="kobospan" id="kobo.163.1"> AzureCognitiveServicesToolkit
toolkit = AzureCognitiveServicesToolkit()
[(tool.name, tool.description) </span><span class="hljs-keyword"><span class="kobospan" id="kobo.164.1">for</span></span><span class="kobospan" id="kobo.165.1"> tool </span><span class="hljs-keyword"><span class="kobospan" id="kobo.166.1">in</span></span><span class="kobospan" id="kobo.167.1"> toolkit.get_tools()]
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.168.1">The following is the corresponding output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.169.1">[('azure_cognitive_services_form_recognizer',
  'A wrapper around Azure Cognitive Services Form Recognizer. </span><span class="kobospan" id="kobo.169.2">Useful for when you need to extract text, tables, and key-value pairs from documents. </span><span class="kobospan" id="kobo.169.3">Input should be a url to a document.'),
 ('azure_cognitive_services_speech2text',
  'A wrapper around Azure Cognitive Services Speech2Text. </span><span class="kobospan" id="kobo.169.4">Useful for when you need to transcribe audio to text. </span><span class="kobospan" id="kobo.169.5">Input should be a url to an audio file.'),
 ('azure_cognitive_services_text2speech',
  'A wrapper around Azure Cognitive Services Text2Speech. </span><span class="kobospan" id="kobo.169.6">Useful for when you need to convert text to speech. </span><span class="kobospan" id="kobo.169.7">'),
 ('azure_cognitive_services_image_analysis',
  'A wrapper around Azure Cognitive Services Image Analysis. </span><span class="kobospan" id="kobo.169.8">Useful for when you need to analyze images. </span><span class="kobospan" id="kobo.169.9">Input should be a url to an image.')]
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="6"><span class="kobospan" id="kobo.170.1">Now, it’s time to initialize our agent. </span><span class="kobospan" id="kobo.170.2">For this purpose, we will use a </span><code class="inlinecode"><span class="kobospan" id="kobo.171.1">STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION</span></code><span class="kobospan" id="kobo.172.1"> agent that, as we saw in previous chapters, also allows for multi-tools input, since we will also add</span><a id="_idIndexMarker734" class="calibre3"/><span class="kobospan" id="kobo.173.1"> further tools in the </span><em class="italic"><span class="kobospan" id="kobo.174.1">Leveraging multiple tools</span></em><span class="kobospan" id="kobo.175.1"> section:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.176.1">from</span></span><span class="kobospan" id="kobo.177.1"> langchain.agents </span><span class="hljs-keyword"><span class="kobospan" id="kobo.178.1">import</span></span><span class="kobospan" id="kobo.179.1"> initialize_agent, AgentType
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.180.1">from</span></span><span class="kobospan" id="kobo.181.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.182.1">import</span></span><span class="kobospan" id="kobo.183.1"> OpenAI
llm = OpenAI()
Model = ChatOpenAI()
agent = initialize_agent(
    tools=toolkit.get_tools(),
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.184.1">True</span></span><span class="kobospan" id="kobo.185.1">,
)
</span></code></pre>
</li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.186.1">Now we have all the ingredients to start testing our agent.</span></p>
<h3 class="heading2" id="_idParaDest-143"><span class="kobospan" id="kobo.187.1">Leveraging a single tool</span></h3>
<p class="normal"><span class="kobospan" id="kobo.188.1">To start easy, let’s simply ask the agent</span><a id="_idIndexMarker735" class="calibre3"/><span class="kobospan" id="kobo.189.1"> to describe the following picture, which will only require the </span><code class="inlinecode"><span class="kobospan" id="kobo.190.1">image_analysis</span></code><span class="kobospan" id="kobo.191.1"> tool to be accomplished:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.192.1"><img alt="A person holding a slingshot  Description automatically generated" src="../Images/B21714_10_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.193.1">Figure 10.3: Sample picture of a slingshot (source: </span><a href="https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg" class="calibre3"><span class="kobospan" id="kobo.194.1">https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg</span></a><span class="kobospan" id="kobo.195.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.196.1">Let’s pass the URL of this image</span><a id="_idIndexMarker736" class="calibre3"/><span class="kobospan" id="kobo.197.1"> as input to our model, as per the description of the </span><code class="inlinecode"><span class="kobospan" id="kobo.198.1">azure_cognitive_services_image_analysis</span></code><span class="kobospan" id="kobo.199.1"> tool:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.200.1">description = agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.201.1">"what shows the following image?:"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.202.1">"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"</span></span><span class="kobospan" id="kobo.203.1">)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.204.1">print</span></span><span class="kobospan" id="kobo.205.1">(description)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.206.1">We then get the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.207.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.207.2">Action:
```
{
  "action": "azure_cognitive_services_image_analysis",
  "action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"
}
```
Observation: Caption: a person holding a slingshot
Tags: person, tool, nail, hand, holding, needle
Thought: I know what the image is.
</span><span class="kobospan" id="kobo.207.3">Action:
```
{
  "action": "Final Answer",
  "action_input": "The image is of a person holding a slingshot."
</span><span class="kobospan" id="kobo.207.4">}
```
&gt; Finished chain.
</span><span class="kobospan" id="kobo.207.5">The image is of a person holding a slingshot.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.208.1">As you can see, the agent</span><a id="_idIndexMarker737" class="calibre3"/><span class="kobospan" id="kobo.209.1"> was able to retrieve the proper tool to address the user’s question. </span><span class="kobospan" id="kobo.209.2">In this case, the question was very simple, so I want to challenge the same tool with a trickier question.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.210.1">The goal is to replicate the GPT-4 capabilities in its common-sense reasoning while working with images, as the following illustration from GPT-4’s earliest experiments shows:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.211.1"><img alt="A close up of a cell phone  Description automatically generated" src="../Images/B21714_10_04.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.212.1">Figure 10.4: Example of visual capabilities and common sense reasoning of GPT-4 (source: </span><a href="https://openai.com/research/gpt-4" class="calibre3"><span class="kobospan" id="kobo.213.1">https://openai.com/research/gpt-4</span></a><span class="kobospan" id="kobo.214.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.215.1">So let’s ask our model something</span><a id="_idIndexMarker738" class="calibre3"/><span class="kobospan" id="kobo.216.1"> more challenging. </span><span class="kobospan" id="kobo.216.2">Let’s ask it to reason about the consequences of letting the slingshot go:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.217.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.218.1">"what happens if the person lets the slingshot go?:"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.219.1">"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"</span></span><span class="kobospan" id="kobo.220.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.221.1">We then obtain the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.222.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.222.2">Action:
```
{
  "action": "azure_cognitive_services_image_analysis",
  "action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"
}
```
Observation: Caption: a person holding a slingshot
Tags: person, tool, nail, hand, holding, needle
Thought: I know what to respond
Action:
```
{
  "action": "Final Answer",
  "action_input": "If the person lets the slingshot go, it will fly through the air."
</span><span class="kobospan" id="kobo.222.3">}
```
&gt; Finished chain.
</span><span class="kobospan" id="kobo.222.4">'If the person lets go of the slingshot, the object being launched by it would be released and propelled forward by the tension of the stretched rubber bands.'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.223.1">It might seem an easy question, but the agent’s answers imply an extremely refined common sense reasoning: thanks to the metadata extracted from the image leveraging the </span><code class="inlinecode"><span class="kobospan" id="kobo.224.1">image_analysis</span></code><span class="kobospan" id="kobo.225.1"> tool, the LLM was able to set up reasoning in terms of consequences given an action (the person letting the slingshot go).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.226.1">In this example, the agent only leveraged</span><a id="_idIndexMarker739" class="calibre3"/><span class="kobospan" id="kobo.227.1"> one tool. </span><span class="kobospan" id="kobo.227.2">But what happens if we ask it something that requires at least two tools?</span></p>
<h3 class="heading2" id="_idParaDest-144"><span class="kobospan" id="kobo.228.1">Leveraging multiple tools</span></h3>
<p class="normal"><span class="kobospan" id="kobo.229.1">Let’s say, for example, that we want</span><a id="_idIndexMarker740" class="calibre3"/><span class="kobospan" id="kobo.230.1"> the model to read a story aloud to us based on a picture.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.231.1"><img alt="Meet Côtelette, the Kyrgyz stray dog that followed cyclists all season ..." src="../Images/B21714_10_05.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.232.1">Figure 10.5: Example of an input image for a story-telling agent (source: </span><a href="https://i.redd.it/diawvlriobq11.jpg" class="calibre3"><span class="kobospan" id="kobo.233.1">https://i.redd.it/diawvlriobq11.jpg</span></a><span class="kobospan" id="kobo.234.1">)</span></p>
<p class="normal1"><span class="kobospan" id="kobo.235.1">Let’s ask the agent</span><a id="_idIndexMarker741" class="calibre3"/><span class="kobospan" id="kobo.236.1"> to do so:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.237.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.238.1">"Tell me a story related to the following picture and read the story aloud to me: https://i.redd.it/diawvlriobq11.jpg"</span></span><span class="kobospan" id="kobo.239.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.240.1">We obtain the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.241.1">Thought:The human requested a story related to the picture. </span><span class="kobospan" id="kobo.241.2">I will use the Azure Cognitive Services Image Analysis tool to analyze the picture and come up with a story.
</span><span class="kobospan" id="kobo.241.3">Action:
```
{
  "action": "azure_cognitive_services_image_analysis",
  "action_input": {
    "query": "https://i.redd.it/diawvlriobq11.jpg"
  }
}
```
Observation: Caption: a dog standing on a snowy hill
Objects: dog
Tags: outdoor, animal, mammal, snow, freezing, dog breed, dog, mountain, covered, winter
Thought:Based on the Azure Cognitive Services Image Analysis tool's analysis of the picture, I will come up with a story involving a dog in a snowy winter wonderland.
</span><span class="kobospan" id="kobo.241.4">Action:
```
{
  "action": "azure_cognitive_services_text2speech",
  "action_input": {
    "query": "Once upon a time, in a snowy winter wonderland, there lived a brave little dog named Snowy. </span><span class="kobospan" id="kobo.241.5">Snowy loved to explore the vast snowy hills and mountains surrounding his home. </span><span class="kobospan" id="kobo.241.6">One day, Snowy stumbled upon a group of lost animals who were stranded in the snow. </span><span class="kobospan" id="kobo.241.7">They were cold, hungry, and scared. </span><span class="kobospan" id="kobo.241.8">Without hesitation, Snowy stepped up to help. </span><span class="kobospan" id="kobo.241.9">He led the group to a nearby shelter and found food and warmth for everyone. </span><span class="kobospan" id="kobo.241.10">Snowy's bravery and kindness earned him a reputation as a hero in the winter wonderland. </span><span class="kobospan" id="kobo.241.11">From then on, he became known as Snowy the Savior of the Snow."
  </span><span class="kobospan" id="kobo.241.12">}
}
```
Observation: C:\Users\vaalt\AppData\Local\Temp\tmp_cecwqht.wav
Thought:The story involving a dog in a snowy winter wonderland seems to have satisfied the human's request for a story related to the picture. </span><span class="kobospan" id="kobo.241.13">I will finalize the response by providing a closing statement.
</span><span class="kobospan" id="kobo.241.14">Action:
```
{
  "action": "Final Answer",
  "action_input": "I hope you enjoyed the story of Snowy the Savior of the Snow, the brave little dog who helped rescue a group of lost animals in the snowy winter wonderland. </span><span class="kobospan" id="kobo.241.15">Thank you for the opportunity to share this tale with you."
</span><span class="kobospan" id="kobo.241.16">}
```
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.242.1">As you can see, the agent was able to invoke two tools to accomplish the request:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.243.1">It first started with the </span><code class="inlinecode"><span class="kobospan" id="kobo.244.1">image_analysis</span></code><span class="kobospan" id="kobo.245.1"> tool to generate the image caption used to produce the story.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.246.1">Then, it invoked the </span><code class="inlinecode"><span class="kobospan" id="kobo.247.1">text2speech</span></code><span class="kobospan" id="kobo.248.1"> tool to read it aloud to the user.</span></li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.249.1">The agent saved the audio file</span><a id="_idIndexMarker742" class="calibre3"/><span class="kobospan" id="kobo.250.1"> in a temporary file, and you can listen to it directly by clicking on the URL. </span><span class="kobospan" id="kobo.250.2">Alternatively, you can save the output as a Python variable and execute it as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.251.1">from</span></span><span class="kobospan" id="kobo.252.1"> IPython </span><span class="hljs-keyword"><span class="kobospan" id="kobo.253.1">import</span></span><span class="kobospan" id="kobo.254.1"> display
audio = agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.255.1">"Tell me a story related to the following picture and read the story aloud to me: https://i.redd.it/diawvlriobq11.jpg"</span></span><span class="kobospan" id="kobo.256.1">)
display.display(audio)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.257.1">Finally, we can also modify the default prompt that comes with the agent type, to make it more customized with respect to our specific use case. </span><span class="kobospan" id="kobo.257.2">To do so, we first need to inspect the template and then decide which part we can modify. </span><span class="kobospan" id="kobo.257.3">To inspect the template, you can run the following command:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.258.1">print</span></span><span class="kobospan" id="kobo.259.1">(agent.agent.llm_chain.prompt.messages[</span><span class="hljs-number"><span class="kobospan" id="kobo.260.1">0</span></span><span class="kobospan" id="kobo.261.1">].prompt.template)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.262.1">Here is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.263.1">Respond to the human as helpfully and accurately as possible. </span><span class="kobospan" id="kobo.263.2">You have access to the following tools:
{tools}
Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).
</span><span class="kobospan" id="kobo.263.3">Valid "action" values: "Final Answer" or youtube_search, CustomeYTTranscribe
Provide only ONE action per $JSON_BLOB, as shown:
```
{{
  "action": $TOOL_NAME,
  "action_input": $INPUT
}}
```
Follow this format:
Question: input question to answer
Thought: consider previous and subsequent steps
Action:
```
$JSON_BLOB
...
</span><span class="kobospan" id="kobo.263.4">```
Begin! </span><span class="kobospan" id="kobo.263.5">Reminder to ALWAYS respond with a valid json blob of a single action. </span><span class="kobospan" id="kobo.263.6">Use tools if necessary. </span><span class="kobospan" id="kobo.263.7">Respond directly if appropriate. </span><span class="kobospan" id="kobo.263.8">Format is Action:```$JSON_BLOB```then Observation:.
</span><span class="kobospan" id="kobo.263.9">Thought:
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.264.1">Let’s modify the prefix of the prompt and pass it as </span><code class="inlinecode"><span class="kobospan" id="kobo.265.1">kwargs</span></code><span class="kobospan" id="kobo.266.1"> to our agent:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.267.1">PREFIX = """
You are a story teller for children. 
</span><span class="kobospan" id="kobo.267.2">You read aloud stories based on pictures that the user pass you.
 </span><span class="kobospan" id="kobo.267.3">You always start your story with a welcome message targeting children, with the goal of make them laugh.
 </span><span class="kobospan" id="kobo.267.4">You can use multiple tools to answer the question.
 </span><span class="kobospan" id="kobo.267.5">ALWAYS use the tools.
 </span><span class="kobospan" id="kobo.267.6">You have access to the following tools:"""
agent = initialize_agent(toolkit.get_tools(), model, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True,
                         agent_kwargs={
                            'prefix':PREFIX})
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.268.1">As you can see, now the agent </span><a id="_idIndexMarker743" class="calibre3"/><span class="kobospan" id="kobo.269.1">acts more similar to a storyteller with a specific style. </span><span class="kobospan" id="kobo.269.2">You can customize your prompt as you wish, always keeping in mind that each pre-built agent has its own prompt template, hence it is always recommended to first inspect it before customizing it.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.270.1">Now that we have explored the out-of-the-box capabilities of the toolkit, let’s build an end-to-end application.</span></p>
<h3 class="heading2" id="_idParaDest-145"><span class="kobospan" id="kobo.271.1">Building an end-to-end application for invoice analysis</span></h3>
<p class="normal"><span class="kobospan" id="kobo.272.1">Analyzing invoices might require</span><a id="_idIndexMarker744" class="calibre3"/><span class="kobospan" id="kobo.273.1"> a lot of manual work</span><a id="_idIndexMarker745" class="calibre3"/><span class="kobospan" id="kobo.274.1"> if not assisted by digital processes. </span><span class="kobospan" id="kobo.274.2">To address this, we will build an AI assistant that is able to analyze invoices for us and tell us any</span><a id="_idIndexMarker746" class="calibre3"/><span class="kobospan" id="kobo.275.1"> relevant information aloud. </span><span class="kobospan" id="kobo.275.2">We will call this application </span><strong class="screentext"><span class="kobospan" id="kobo.276.1">CoPenny</span></strong><span class="kobospan" id="kobo.277.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.278.1">With CoPenny, individuals and enterprises could reduce the time of invoice analysis, as well as build toward document process automation and, more generally, digital process automation.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.279.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.280.1">Document process automation is a strategy that uses technology to streamline and automate various document-related tasks and processes within an organization. </span><span class="kobospan" id="kobo.280.2">It involves the use of software tools, including document capture, data extraction, workflow automation, and integration with other systems. </span><span class="kobospan" id="kobo.280.3">For example, document process automation can help you extract, validate, and analyze data from invoices, receipts, forms, and other types of documents. </span><span class="kobospan" id="kobo.280.4">Document process automation can save you time and money, improve accuracy and efficiency, and provide valuable insights and reports from your document data.</span></p>
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.281.1">Digital process automation</span></strong><span class="kobospan" id="kobo.282.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.283.1">DPA</span></strong><span class="kobospan" id="kobo.284.1">) is a broader term that refers</span><a id="_idIndexMarker747" class="calibre3"/><span class="kobospan" id="kobo.285.1"> to automating any business process with digital technology. </span><span class="kobospan" id="kobo.285.2">DPA can help you connect your apps, data, and services and boost your team’s productivity with cloud flows. </span><span class="kobospan" id="kobo.285.3">DPA can also help you create more sophisticated and intuitive customer experiences, collaborate across your organization, and innovate with AI and ML.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.286.1">To start building our application, we can follow these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.287.1">Using </span><code class="inlinecode"><span class="kobospan" id="kobo.288.1">AzureCognitiveServicesToolkit</span></code><span class="kobospan" id="kobo.289.1">, we will leverage the </span><code class="inlinecode"><span class="kobospan" id="kobo.290.1">azure_cognitive_services_form_recognizer</span></code><span class="kobospan" id="kobo.291.1"> and </span><code class="inlinecode"><span class="kobospan" id="kobo.292.1">azure_cognitive_services_text2speech</span></code><span class="kobospan" id="kobo.293.1"> tools, so we can limit the agent’s “powers” only to those two:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.294.1">toolkit = AzureCognitiveServicesToolkit().get_tools()
</span><span class="hljs-comment"><span class="kobospan" id="kobo.295.1">#those tools are at the first and third position in the list</span></span><span class="kobospan" id="kobo.296.1">
tools = [toolkit[</span><span class="hljs-number"><span class="kobospan" id="kobo.297.1">0</span></span><span class="kobospan" id="kobo.298.1">], toolkit[</span><span class="hljs-number"><span class="kobospan" id="kobo.299.1">2</span></span><span class="kobospan" id="kobo.300.1">]]
tools
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.301.1">The following is</span><a id="_idIndexMarker748" class="calibre3"/><span class="kobospan" id="kobo.302.1"> the corresponding</span><a id="_idIndexMarker749" class="calibre3"/><span class="kobospan" id="kobo.303.1"> output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.304.1">[AzureCogsFormRecognizerTool(name='azure_cognitive_services_form_recognizer', description='A wrapper around Azure Cognitive Services Form Recognizer. </span><span class="kobospan" id="kobo.304.2">Useful for when you need to extract text, tables, and key-value pairs from documents. </span><span class="kobospan" id="kobo.304.3">Input should be a url to a document.', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, azure_cogs_key='', azure_cogs_endpoint='', doc_analysis_client=&lt;azure.ai.formrecognizer._document_analysis_client.DocumentAnalysisClient object at 0x000001FEA6B80AC0&gt;), AzureCogsText2SpeechTool(name='azure_cognitive_services_text2speech', description='A wrapper around Azure Cognitive Services Text2Speech. </span><span class="kobospan" id="kobo.304.4">Useful for when you need to convert text to speech. </span><span class="kobospan" id="kobo.304.5">', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, azure_cogs_key='', azure_cogs_region='', speech_language='en-US', speech_config=&lt;azure.cognitiveservices.speech.SpeechConfig object at 0x000001FEAF932CE0&gt;)]
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="2"><span class="kobospan" id="kobo.305.1">Let’s now initialize the agent with the default prompt and see the results. </span><span class="kobospan" id="kobo.305.2">For this purpose, we will use a sample invoice as a template with which to query the agent:</span></li>
</ol>
<figure class="mediaobject"><span class="kobospan" id="kobo.306.1"><img alt="A close-up of a receipt  Description automatically generated" src="../Images/B21714_10_06.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.307.1">Figure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.308.1">Let’s start by asking the model to tell us all the men’s </span><strong class="screentext"><span class="kobospan" id="kobo.309.1">stock-keeping units</span></strong><span class="kobospan" id="kobo.310.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.311.1">SKUs</span></strong><span class="kobospan" id="kobo.312.1">) on the invoice:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.313.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.314.1">"what are all men's skus?"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.315.1">"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"</span></span><span class="kobospan" id="kobo.316.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.317.1">We then get the following</span><a id="_idIndexMarker750" class="calibre3"/><span class="kobospan" id="kobo.318.1"> output (showing a truncated</span><a id="_idIndexMarker751" class="calibre3"/><span class="kobospan" id="kobo.319.1"> output; you can find the whole output in the book’s GitHub repository):</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.320.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.320.2">Action:
```
{
  "action": "azure_cognitive_services_form_recognizer",
  "action_input": {
    "query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"
  }
}
```
Observation: Content: PURCHASE ORDER TEMPLATE […]
&gt; Finished chain.
</span><span class="kobospan" id="kobo.320.3">"The men's skus are B222 and D444."
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="4"><span class="kobospan" id="kobo.321.1">We can also ask for multiple information (women’s SKUs, shipping address, and delivery dates) as follows (note that the delivery date is not specified, as we want our agent not to hallucinate):
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.322.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.323.1">"give me the following information about the invoice: women's SKUs, shipping address and delivery date."</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.324.1">"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"</span></span><span class="kobospan" id="kobo.325.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.326.1">This gives us the following output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.327.1">"The women's SKUs are A111 Women's Tall - M. </span><span class="kobospan" id="kobo.327.2">The shipping address is Company Name 123 Main Street Hamilton, OH 44416 (321) 456-7890. </span><span class="kobospan" id="kobo.327.3">The delivery date is not mentioned in the invoice."
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="5"><span class="kobospan" id="kobo.328.1">Finally, let’s also leverage the text2speech tool to produce the audio of the response:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.329.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.330.1">"extract women's SKUs in the following invoice, then read it aloud:"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.331.1">"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"</span></span><span class="kobospan" id="kobo.332.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.333.1">As per the previous example, you can listen to the audio by clicking on the URL in the chain, or using Python’s </span><code class="inlinecode"><span class="kobospan" id="kobo.334.1">Display</span></code><span class="kobospan" id="kobo.335.1"> function if you save it as a variable.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="6"><span class="kobospan" id="kobo.336.1">Now, we want our agent</span><a id="_idIndexMarker752" class="calibre3"/><span class="kobospan" id="kobo.337.1"> to be better tailored</span><a id="_idIndexMarker753" class="calibre3"/><span class="kobospan" id="kobo.338.1"> toward our goal. </span><span class="kobospan" id="kobo.338.2">To do so, let’s customize the prompt giving specific instructions. </span><span class="kobospan" id="kobo.338.3">In particular, we want the agent to produce the audio output without the user explicitly asking for it:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.339.1">PREFIX = </span><span class="hljs-string"><span class="kobospan" id="kobo.340.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.341.1">You are an AI assistant that help users to interact with invoices.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.342.1">You extract information from invoices and read it aloud to users.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.343.1">You can use multiple tools to answer the question.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.344.1">Always divide your response in 2 steps:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.345.1">1. </span><span class="kobospan" id="kobo.345.2">Extracting the information from the invoice upon user's request</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.346.1">2. </span><span class="kobospan" id="kobo.346.2">Converting the transcript of the previous point into an audio file</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.347.1">ALWAYS use the tools.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.348.1">ALWAYS return an audio file using the proper tool.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.349.1">You have access to the following tools:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.350.1">"""</span></span><span class="kobospan" id="kobo.351.1">
agent = initialize_agent(tools, model, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = </span><span class="hljs-literal"><span class="kobospan" id="kobo.352.1">True</span></span><span class="kobospan" id="kobo.353.1">,
                         agent_kwargs={
                            </span><span class="hljs-string"><span class="kobospan" id="kobo.354.1">'prefix'</span></span><span class="kobospan" id="kobo.355.1">:PREFIX})
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.356.1">Let’s run the agent:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.357.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.358.1">"what are women's SKUs in the following invoice?:"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.359.1">"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"</span></span><span class="kobospan" id="kobo.360.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.361.1">This yields</span><a id="_idIndexMarker754" class="calibre3"/><span class="kobospan" id="kobo.362.1"> the following</span><a id="_idIndexMarker755" class="calibre3"/><span class="kobospan" id="kobo.363.1"> output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.364.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.364.2">I will need to use the azure_cognitive_services_form_recognizer tool to extract the information from the invoice.
</span><span class="kobospan" id="kobo.364.3">Action:
```
{
  "action": "azure_cognitive_services_form_recognizer",
  "action_input": {
    "query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"
  }
}
```
Observation: Content: PURCHASE ORDER TEMPLATE […]
Observation: C:\Users\vaalt\AppData\Local\Temp\tmpx1n4obf3.wav
Thought:Now that I have provided the answer, I will wait for further inquiries.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.365.1">As you can see, now the agent saved the output into an audio file, even when the user didn’t ask explicitly for it.</span></p>
<p class="normal1"><code class="inlinecode"><span class="kobospan" id="kobo.366.1">AzureCognitiveServicesToolkit</span></code><span class="kobospan" id="kobo.367.1"> is a powerful integration that allows for native consumption of Azure AI Services. </span><span class="kobospan" id="kobo.367.2">However, there are some pitfalls of this approach, including the limited</span><a id="_idIndexMarker756" class="calibre3"/><span class="kobospan" id="kobo.368.1"> number of AI </span><a id="_idIndexMarker757" class="calibre3"/><span class="kobospan" id="kobo.369.1">services. </span><span class="kobospan" id="kobo.369.2">In the next section, we are going to explore yet another option to achieve multimodality, with a more flexible approach while still keeping an agentic strategy.</span></p>
<h1 class="heading" id="_idParaDest-146"><span class="kobospan" id="kobo.370.1">Option 2: Combining single tools into one agent</span></h1>
<p class="normal"><span class="kobospan" id="kobo.371.1">In this leg of our journey</span><a id="_idIndexMarker758" class="calibre3"/><span class="kobospan" id="kobo.372.1"> toward multimodality, we will leverage different tools as plug-ins to our </span><code class="inlinecode"><span class="kobospan" id="kobo.373.1">STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION</span></code><span class="kobospan" id="kobo.374.1"> agent. </span><span class="kobospan" id="kobo.374.2">Our goal is to build a copilot agent that will help us generate reviews about YouTube videos, as well as post those reviews on our social media with a nice description and related picture. </span><span class="kobospan" id="kobo.374.3">In all of that, we want to make little or no effort, so we need our agent to perform the following steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.375.1">Search and transcribe a YouTube video based on our input.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.376.1">Based on the transcription, generate a review with a length and style defined by the user query.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.377.1">Generate an image related to the video and the review.</span></li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.378.1">We will call our copilot </span><strong class="screentext"><span class="kobospan" id="kobo.379.1">GPTuber</span></strong><span class="kobospan" id="kobo.380.1">. </span><span class="kobospan" id="kobo.380.2">In the following</span><a id="_idIndexMarker759" class="calibre3"/><span class="kobospan" id="kobo.381.1"> subsections, we will examine each tool and then put them all together.</span></p>
<h2 class="heading1" id="_idParaDest-147"><span class="kobospan" id="kobo.382.1">YouTube tools and Whisper</span></h2>
<p class="normal"><span class="kobospan" id="kobo.383.1">The first step of our agent will be to search and transcribe the YouTube video based on our input. </span><span class="kobospan" id="kobo.383.2">To do so, there are two tools we need to leverage:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.384.1">YouTubeSearchTool</span></strong><span class="kobospan" id="kobo.385.1">: An out-of-the-box tool</span><a id="_idIndexMarker760" class="calibre3"/><span class="kobospan" id="kobo.386.1"> offered by LangChain and adapted from </span><a href="https://github.com/venuv/langchain_yt_tools" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.387.1">https://github.com/venuv/langchain_yt_tools</span></span></a><span class="kobospan" id="kobo.388.1">. </span><span class="kobospan" id="kobo.388.2">You can import and try the tool by running the following code, specifying the topic of the video and the number of videos you want the tool to return:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.389.1">from</span></span><span class="kobospan" id="kobo.390.1"> langchain.tools </span><span class="hljs-keyword"><span class="kobospan" id="kobo.391.1">import</span></span><span class="kobospan" id="kobo.392.1"> YouTubeSearchTool
tool = YouTubeSearchTool()
result = tool.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.393.1">"Avatar: The Way of Water,1"</span></span><span class="kobospan" id="kobo.394.1">)
result:
</span></code></pre>
</li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.395.1">Here is the output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.396.1">"['/watch?v=d9MyW72ELq0&amp;pp=ygUYQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVy']"
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.397.1">The tool returns</span><a id="_idIndexMarker761" class="calibre3"/><span class="kobospan" id="kobo.398.1"> the URL of the video. </span><span class="kobospan" id="kobo.398.2">To watch it, you can add it to </span><a href="https://youtube.com" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.399.1">https://youtube.com domain</span></span></a><span class="kobospan" id="kobo.400.1">.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.401.1">CustomYTTranscribeTool</span></strong><span class="kobospan" id="kobo.402.1">: This is a custom tool</span><a id="_idIndexMarker762" class="calibre3"/><span class="kobospan" id="kobo.403.1"> that I’ve adapted from </span><a href="https://github.com/venuv/langchain_yt_tools" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.404.1">https://github.com/venuv/langchain_yt_tools</span></span></a><span class="kobospan" id="kobo.405.1">. </span><span class="kobospan" id="kobo.405.2">It consists of transcribing the audio file retrieved from the previous tool using a speech-to-text model. </span><span class="kobospan" id="kobo.405.3">In our case, we will</span><a id="_idIndexMarker763" class="calibre3"/><span class="kobospan" id="kobo.406.1"> be leveraging OpenAI’s </span><strong class="screentext"><span class="kobospan" id="kobo.407.1">Whisper</span></strong><span class="kobospan" id="kobo.408.1">.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.409.1">Whisper is a transformer-based model introduced by OpenAI in September 2022. </span><span class="kobospan" id="kobo.409.2">It works as follows:</span></p>
<ol class="romanlist">
<li class="romanlist1" value="1"><span class="kobospan" id="kobo.410.1">It splits the input audio into 30-second chunks, converting them into spectrograms (visual representations of sound frequencies).</span></li>
<li class="romanlist1"><span class="kobospan" id="kobo.411.1">It then passes them to an encoder.</span></li>
<li class="romanlist1"><span class="kobospan" id="kobo.412.1">The encoder then produces a sequence of hidden states that capture the information in the audio.</span></li>
<li class="romanlist1"><span class="kobospan" id="kobo.413.1">A decoder then predicts the corresponding text caption, using special tokens to indicate the task (such as language identification, speech transcription, or speech translation) and the output language.</span></li>
<li class="romanlist1"><span class="kobospan" id="kobo.414.1">The decoder can also generate timestamps for each word or phrase in the caption.</span></li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.415.1">Unlike most OpenAI models, Whisper is open-source.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.416.1">Since this model takes as input only files and not URLs, within the custom tool, there is a function defined as </span><code class="inlinecode"><span class="kobospan" id="kobo.417.1">yt_get</span></code><span class="kobospan" id="kobo.418.1"> (you can find it in the GitHub repository) that, starting from the video URL, downloads it into a </span><code class="inlinecode"><span class="kobospan" id="kobo.419.1">.mp4</span></code><span class="kobospan" id="kobo.420.1"> file. </span><span class="kobospan" id="kobo.420.2">Once downloaded, you can try Whisper with the following lines of code:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.421.1">import</span></span><span class="kobospan" id="kobo.422.1"> openai
audio_file = </span><span class="hljs-built_in"><span class="kobospan" id="kobo.423.1">open</span></span><span class="kobospan" id="kobo.424.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.425.1">"Avatar The Way of Water  Official Trailer.mp4"</span></span><span class="kobospan" id="kobo.426.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.427.1">'rb'</span></span><span class="kobospan" id="kobo.428.1">)
result = openai.Audio.transcribe(</span><span class="hljs-string"><span class="kobospan" id="kobo.429.1">"whisper-1"</span></span><span class="kobospan" id="kobo.430.1">, audio_file)
audio_file.close()
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.431.1">print</span></span><span class="kobospan" id="kobo.432.1">(result.text)
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.433.1">Here is the corresponding output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.434.1">♪ Dad, I know you think I'm crazy. </span><span class="kobospan" id="kobo.434.2">But I feel her. </span><span class="kobospan" id="kobo.434.3">I hear her heartbeat. </span><span class="kobospan" id="kobo.434.4">She's so close. </span><span class="kobospan" id="kobo.434.5">♪ So what does her heartbeat sound like? </span><span class="kobospan" id="kobo.434.6">♪ Mighty. </span><span class="kobospan" id="kobo.434.7">♪ We cannot let you bring your war here. </span><span class="kobospan" id="kobo.434.8">Outcast, that's all I see. </span><span class="kobospan" id="kobo.434.9">I see you. </span><span class="kobospan" id="kobo.434.10">♪ The way of water connects all things. </span><span class="kobospan" id="kobo.434.11">Before your birth. </span><span class="kobospan" id="kobo.434.12">And after your death. </span><span class="kobospan" id="kobo.434.13">This is our home! </span><span class="kobospan" id="kobo.434.14">I need you with me. </span><span class="kobospan" id="kobo.434.15">And I need you to be strong. </span><span class="kobospan" id="kobo.434.16">♪ Strongheart. </span><span class="kobospan" id="kobo.434.17">♪
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.435.1">By embedding Whisper</span><a id="_idIndexMarker764" class="calibre3"/><span class="kobospan" id="kobo.436.1"> in this custom tool, we can transcribe the output of the first tool into a transcript that will serve as input to the next tool. </span><span class="kobospan" id="kobo.436.2">You can see the code and logic behind this embedding and the whole tool in this book’s GitHub repository at </span><a href="Chapter_10.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.437.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.438.1">, which is a modified version from </span><a href="https://github.com/venuv/langchain_yt_tools" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.439.1">https://github.com/venuv/langchain_yt_tools</span></span></a><span class="kobospan" id="kobo.440.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.441.1">Since we already have two tools, we can start building our tools list</span><a id="_idIndexMarker765" class="calibre3"/><span class="kobospan" id="kobo.442.1"> and initializing</span><a id="_idIndexMarker766" class="calibre3"/><span class="kobospan" id="kobo.443.1"> our agent, using the following code:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.444.1">llm = OpenAI(temperature=</span><span class="hljs-number"><span class="kobospan" id="kobo.445.1">0</span></span><span class="kobospan" id="kobo.446.1">)
tools = []
tools.append(YouTubeSearchTool())
tools.append(CustomYTTranscribeTool())
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.447.1">True</span></span><span class="kobospan" id="kobo.448.1">)
agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.449.1">"search a video trailer of Avatar: the way of water. </span><span class="kobospan" id="kobo.449.2">Return only 1 video. </span><span class="kobospan" id="kobo.449.3">transcribe the youtube video and return the transcription."</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.450.1">The following is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.451.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.451.2">I need to find a specific video and transcribe it.
</span><span class="kobospan" id="kobo.451.3">Action: youtube_search
Action Input: "Avatar: the way of water,1"
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUYQXZhdGFyOiB0aGUgd2F5IG9mIHdhdGVy']
Thought:I found the video I was looking for, now I need to transcribe it.
</span><span class="kobospan" id="kobo.451.4">Action: CustomeYTTranscribe
Action Input: […]
Observation: ♪ Dad, I know you think I'm crazy. </span><span class="kobospan" id="kobo.451.5">[…]
Thought:I have the transcription of the video trailer for Avatar: the way of water.
</span><span class="kobospan" id="kobo.451.6">Final Answer: The transcription of the video trailer for Avatar: the way of water is: "♪ Dad, I know you think I'm crazy. </span><span class="kobospan" id="kobo.451.7">[…]
&gt; Finished chain.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.452.1">Great! </span><span class="kobospan" id="kobo.452.2">We were able to generate the transcription of this video. </span><span class="kobospan" id="kobo.452.3">The next step will be to generate a review alongside a picture. </span><span class="kobospan" id="kobo.452.4">While the review can be written directly from the LLM and passed as a parameter to the model (so we don’t need another tool), the image generation will need an additional tool. </span><span class="kobospan" id="kobo.452.5">For this purpose, we are going to use OpenAI’s DALL·E.</span></p>
<h2 class="heading1" id="_idParaDest-148"><span class="kobospan" id="kobo.453.1">DALL·E and text generation</span></h2>
<p class="normal"><span class="kobospan" id="kobo.454.1">Introduced by OpenAI</span><a id="_idIndexMarker767" class="calibre3"/><span class="kobospan" id="kobo.455.1"> in January 2021, DALL·E is a transformer-based model that can create images from text descriptions. </span><span class="kobospan" id="kobo.455.2">It is based on GPT-3, which is also used for natural language processing tasks. </span><span class="kobospan" id="kobo.455.3">It is trained on a large dataset of text-image pairs from the web and uses a vocabulary of tokens for both text and image concepts. </span><span class="kobospan" id="kobo.455.4">DALL·E can produce multiple images for the same text, showing different interpretations and variations.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.456.1">LangChain offers native integration with DALL·E, which you can use as a tool by running the following code (always setting the environmental variable of your </span><code class="inlinecode"><span class="kobospan" id="kobo.457.1">OPENAI_API_KEY</span></code><span class="kobospan" id="kobo.458.1"> from the </span><code class="inlinecode"><span class="kobospan" id="kobo.459.1">.env</span></code><span class="kobospan" id="kobo.460.1"> file):</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.461.1">from</span></span><span class="kobospan" id="kobo.462.1"> langchain.agents </span><span class="hljs-keyword"><span class="kobospan" id="kobo.463.1">import</span></span><span class="kobospan" id="kobo.464.1"> load_tools
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.465.1">from</span></span><span class="kobospan" id="kobo.466.1"> langchain.agents </span><span class="hljs-keyword"><span class="kobospan" id="kobo.467.1">import</span></span><span class="kobospan" id="kobo.468.1"> initialize_agent
tools = load_tools([</span><span class="hljs-string"><span class="kobospan" id="kobo.469.1">'dalle-image-generator'</span></span><span class="kobospan" id="kobo.470.1">])
agent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.471.1">True</span></span><span class="kobospan" id="kobo.472.1">)
agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.473.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.474.1">Create an image of a halloween night. </span><span class="kobospan" id="kobo.474.2">Return only the image url."</span></span><span class="kobospan" id="kobo.475.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.476.1">Here is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.477.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.477.2">I need to use an image generator to create an image of a halloween night.
</span><span class="kobospan" id="kobo.477.3">Action: Dall-E Image Generator
Action Input: "An image of a spooky halloween night with a full moon, bats flying in the sky, and a haunted house in the background."
</span><span class="kobospan" id="kobo.477.4">Observation: [link_to_the_blob]
Thought:I have successfully generated an image of a halloween night.
</span><span class="kobospan" id="kobo.477.5">Final Answer: The image url is [link_to_the_blob]
&gt; Finished chain.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.478.1">The following is the image</span><a id="_idIndexMarker768" class="calibre3"/><span class="kobospan" id="kobo.479.1"> that was generated, as requested:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.480.1"><img alt="A house with bats flying in the sky  Description automatically generated" src="../Images/B21714_10_07.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.481.1">Figure 10.7: Image generated by DALL·E upon the user’s input</span></p>
<p class="normal1"><span class="kobospan" id="kobo.482.1">Great! </span><span class="kobospan" id="kobo.482.2">Now let’s also</span><a id="_idIndexMarker769" class="calibre3"/><span class="kobospan" id="kobo.483.1"> see whether our agent is capable of generating a review of a video based on the transcription:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.484.1">agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.485.1">"search a video trailer of Avatar: the way of water. </span><span class="kobospan" id="kobo.485.2">Return only 1 video. </span><span class="kobospan" id="kobo.485.3">transcribe the youtube video and return a review of the trailer."</span></span><span class="kobospan" id="kobo.486.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.487.1">We get the</span><a id="_idIndexMarker770" class="calibre3"/><span class="kobospan" id="kobo.488.1"> following</span><a id="_idIndexMarker771" class="calibre3"/><span class="kobospan" id="kobo.489.1"> output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.490.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.490.2">I need to use both youtube_search and CustomeYTTranscribe tools to answer this question.
</span><span class="kobospan" id="kobo.490.3">Action: youtube_search
Action Input: "Avatar: the way of water,1"
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUYQXZhdGFyOiB0aGUgd2F5IG9mIHdhdGVy']
Thought:I found the video I need to transcribe.
</span><span class="kobospan" id="kobo.490.4">Action: CustomeYTTranscribe
Action Input: […]
Observation: ♪ Dad, I know you think I'm crazy. </span><span class="kobospan" id="kobo.490.5">[…]
Thought:I need to write a review of the trailer.
</span><span class="kobospan" id="kobo.490.6">Action: Write a review
Action Input: "Avatar: The Way of Water trailer is visually stunning and captivating. </span><span class="kobospan" id="kobo.490.7">The trailer gives a glimpse into the world of Pandora and the connection between the Na'vi and the way of water. </span><span class="kobospan" id="kobo.490.8">The music and sound effects add to the overall immersive experience. </span><span class="kobospan" id="kobo.490.9">The trailer leaves the audience wanting more and eagerly anticipating the release of the movie."
</span><span class="kobospan" id="kobo.490.10">Observation: Write a review is not a valid tool, try one of [youtube_search, CustomeYTTranscribe].
</span><span class="kobospan" id="kobo.490.11">Thought:I need to use a different tool to write a review.
</span><span class="kobospan" id="kobo.490.12">Action: Write a review
Action Input: N/A
Observation: Write a review is not a valid tool, try one of [youtube_search, CustomeYTTranscribe].
</span><span class="kobospan" id="kobo.490.13">...
</span><span class="kobospan" id="kobo.490.14">Thought:I can write the review manually.
</span><span class="kobospan" id="kobo.490.15">Final Answer: Avatar: The Way of Water trailer is visually stunning and captivating. </span><span class="kobospan" id="kobo.490.16">The trailer gives a glimpse into the world of Pandora and the connection between the Na'vi and the way of water. </span><span class="kobospan" id="kobo.490.17">The music and sound effects add to the overall immersive experience. </span><span class="kobospan" id="kobo.490.18">The trailer leaves the audience wanting more and eagerly anticipating the release of the movie.
</span><span class="kobospan" id="kobo.490.19">&gt; Finished chain.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.491.1">Note how the agent was initially looking for a tool to make a review, to then realize that there is no tool yet that can do it manually thanks to its parametric knowledge. </span><span class="kobospan" id="kobo.491.2">This is a great example of how LLMs are reasoning engines and endowed with common sense reasoning. </span><span class="kobospan" id="kobo.491.3">As always, you can find</span><a id="_idIndexMarker772" class="calibre3"/><span class="kobospan" id="kobo.492.1"> the entire chain of thoughts</span><a id="_idIndexMarker773" class="calibre3"/><span class="kobospan" id="kobo.493.1"> in the book’s repository.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.494.1">The next step will be to put it all together and see whether the agent is capable of orchestrating all the tools, with some assistance in terms of prompt engineering.</span></p>
<h2 class="heading1" id="_idParaDest-149"><span class="kobospan" id="kobo.495.1">Putting it all together</span></h2>
<p class="normal"><span class="kobospan" id="kobo.496.1">Now that we have all the</span><a id="_idIndexMarker774" class="calibre3"/><span class="kobospan" id="kobo.497.1"> ingredients, we need to put them together into one single agent. </span><span class="kobospan" id="kobo.497.2">To do so, we can follow these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.498.1">First, we need to add the DALL·E tool to the list of tools:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.499.1">tools = []
tools.append(YouTubeSearchTool())
tools.append(CustomYTTranscribeTool())
tools.append(load_tools([</span><span class="hljs-string"><span class="kobospan" id="kobo.500.1">'dalle-image-generator'</span></span><span class="kobospan" id="kobo.501.1">])[</span><span class="hljs-number"><span class="kobospan" id="kobo.502.1">0</span></span><span class="kobospan" id="kobo.503.1">])
[tool.name </span><span class="hljs-keyword"><span class="kobospan" id="kobo.504.1">for</span></span><span class="kobospan" id="kobo.505.1"> tool </span><span class="hljs-keyword"><span class="kobospan" id="kobo.506.1">in</span></span><span class="kobospan" id="kobo.507.1"> tools]
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.508.1">This gives us the following output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.509.1">['youtube_search', 'CustomeYTTranscribe', 'Dall-E Image Generator']
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="2"><span class="kobospan" id="kobo.510.1">The next step will be to test the agent with the default prompt, and then try to refine the instructions with some prompt engineering. </span><span class="kobospan" id="kobo.510.2">Let’s start with a pre-configured agent (you can find all the steps in the GitHub repository):
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.511.1">agent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.512.1">True</span></span><span class="kobospan" id="kobo.513.1">)
agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.514.1">"search a video trailer of Avatar: the way of water. </span><span class="kobospan" id="kobo.514.2">Return only 1 video. </span><span class="kobospan" id="kobo.514.3">transcribe the youtube video and return a review of the trailer. </span><span class="kobospan" id="kobo.514.4">Generate an image based on the video transcription"</span></span><span class="kobospan" id="kobo.515.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.516.1">This gives us the following output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.517.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.517.2">I need to search for a video trailer of "Avatar: The Way of Water" and transcribe it to generate a review. </span><span class="kobospan" id="kobo.517.3">Then, I can use the transcription to generate an image based on the video content.
</span><span class="kobospan" id="kobo.517.4">Action: youtube_search
Action Input: "Avatar: The Way of Water trailer,1"
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUgQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXI%3D']
Thought:I found a video trailer of "Avatar: The Way of Water" with the given search query. </span><span class="kobospan" id="kobo.517.5">Now I need to transcribe the video and generate a review.
</span><span class="kobospan" id="kobo.517.6">Action: CustomeYTTranscribe
Action Input: '/watch?v=d9MyW72ELq0&amp;pp=ygUgQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXI%3D'[YTTRANSCIBE***], […]
Observation: ♪ Dad, I know you think I'm crazy. </span><span class="kobospan" id="kobo.517.7">[…]
Thought:I have transcribed the video and now I can generate a review based on the transcription. </span><span class="kobospan" id="kobo.517.8">Additionally, I can use the transcription to generate an image.
</span><span class="kobospan" id="kobo.517.9">Action: Dall-E Image Generator
Action Input: "Review of Avatar: The Way of Water trailer: The trailer for Avatar: The Way of Water is captivating and mysterious. </span><span class="kobospan" id="kobo.517.10">It starts with a dialogue between a father and his son, discussing a connection to someone or something. </span><span class="kobospan" id="kobo.517.11">The visuals are stunning, showcasing breathtaking landscapes and vibrant underwater scenes. </span><span class="kobospan" id="kobo.517.12">The trailer hints at an impending war and the protagonist's role as an outcast. </span><span class="kobospan" id="kobo.517.13">The theme of water and its connection to all things is also emphasized. </span><span class="kobospan" id="kobo.517.14">Overall, the trailer leaves the viewer excited and intrigued about the upcoming movie."
</span><span class="kobospan" id="kobo.517.15">Observation: [image_url]
Thought:I have generated a review of the "Avatar: The Way of Water" trailer based on the transcription of the video. </span><span class="kobospan" id="kobo.517.16">Additionally, I have generated an image based on the review. </span><span class="kobospan" id="kobo.517.17">Now I can provide the final answer.
</span><span class="kobospan" id="kobo.517.18">Final Answer: The "Avatar: The Way of Water" trailer is captivating and mysterious, featuring stunning visuals of landscapes and underwater scenes. </span><span class="kobospan" id="kobo.517.19">It hints at an impending war and explores the theme of water and its connection to all things. </span><span class="kobospan" id="kobo.517.20">The trailer leaves viewers excited and intrigued about the upcoming movie.
</span><span class="kobospan" id="kobo.517.21">&gt; Finished chain.
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.518.1">The following is the accompanying visual output:</span></p>
<p class="packt_figref"><span class="kobospan" id="kobo.519.1"><img alt="A person with dreadlocks and green eyes  Description automatically generated" src="../Images/B21714_10_08.png" class="calibre4"/></span></p>
<p class="packt_figref"><span class="kobospan" id="kobo.520.1">Figure 10.8: Image generated by DALL·E based on the trailer review</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.521.1">Well, even without</span><a id="_idIndexMarker775" class="calibre3"/><span class="kobospan" id="kobo.522.1"> any prompt engineering, the agent was able to orchestrate the tools and return the desired results!</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.523.1">Now, let’s try to make it more tailored toward our purpose. </span><span class="kobospan" id="kobo.523.2">Similar to the CoPenny application, we don’t want the user to specify every time to generate a review alongside an image. </span><span class="kobospan" id="kobo.523.3">So let’s modify the default prompt as follows:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.524.1">PREFIX = </span><span class="hljs-string"><span class="kobospan" id="kobo.525.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.526.1">You are an expert reviewer of movie trailer.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.527.1">You adapt the style of the review depending on the channel the user want to use, namely Instagram, LinkedIn, Facebook.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.528.1">You can use multiple tools to answer the question.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.529.1">ALWAYS search for the youtube video related to the trailer. </span><span class="kobospan" id="kobo.529.2">Search ONLY 1 video.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.530.1">ALWAYS transcribe the youtube trailer and use it to generate the review.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.531.1">ALWAYS generate an image alongside the review, based on the transcription of the trailer.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.532.1">ALWAYS use all the available tools for the various steps.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.533.1">You have access to the following tools:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.534.1">"""</span></span><span class="kobospan" id="kobo.535.1">
agent = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = </span><span class="hljs-literal"><span class="kobospan" id="kobo.536.1">True</span></span><span class="kobospan" id="kobo.537.1">,
                         agent_kwargs={
                            </span><span class="hljs-string"><span class="kobospan" id="kobo.538.1">'prefix'</span></span><span class="kobospan" id="kobo.539.1">:PREFIX})
agent.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.540.1">"Generate a review of the trailer of Avatar: The Way of Water. </span><span class="kobospan" id="kobo.540.2">I want to publish it on Instagram."</span></span><span class="kobospan" id="kobo.541.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.542.1">The output obtained</span><a id="_idIndexMarker776" class="calibre3"/><span class="kobospan" id="kobo.543.1"> is as follows:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.544.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.544.2">To generate a review for Instagram, I will need to search for the trailer of "Avatar: The Way of Water" on YouTube and transcribe it. </span><span class="kobospan" id="kobo.544.3">I will also need to generate an image based on the transcription. </span><span class="kobospan" id="kobo.544.4">Let's start by searching for the YouTube video.
</span><span class="kobospan" id="kobo.544.5">Action: youtube_search
Action Input: "Avatar: The Way of Water trailer", 1
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUhQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXIi']
Thought:I have found a YouTube video of the "Avatar: The Way of Water" trailer. </span><span class="kobospan" id="kobo.544.6">Now I will transcribe it and generate an image based on the transcription.
</span><span class="kobospan" id="kobo.544.7">Action: CustomeYTTranscribe
Action Input: '/watch?v=d9MyW72ELq0&amp;pp=ygUhQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXIi'[YTTRANSCIBE***],[…]
Observation: ♪ Dad, I know you think I'm crazy.[…]
Action: Dall-E Image Generator
Action Input: "A scene from the movie 'Avatar: The Way of Water' with the text 'The Way of Water connects all things. </span><span class="kobospan" id="kobo.544.8">This is our home!'"
</span><span class="kobospan" id="kobo.544.9">Observation: [image_url]
Thought:I have generated an image for the Instagram review of the trailer of "Avatar: The Way of Water". </span><span class="kobospan" id="kobo.544.10">Now I can write the review.
</span><span class="kobospan" id="kobo.544.11">Final Answer: "Avatar: The Way of Water" is an upcoming movie that promises to take us on a breathtaking journey. </span><span class="kobospan" id="kobo.544.12">The trailer captivated me with its stunning visuals and powerful storytelling. </span><span class="kobospan" id="kobo.544.13">The tagline "The Way of Water connects all things. </span><span class="kobospan" id="kobo.544.14">This is our home!" </span><span class="kobospan" id="kobo.544.15">resonated with me, highlighting the movie's theme of unity and the importance of preserving our planet. </span><span class="kobospan" id="kobo.544.16">I can't wait to dive into this immersive cinematic experience. </span><span class="kobospan" id="kobo.544.17">#AvatarTheWayOfWater #MovieReview #ComingSoon
&gt; Finished chain.
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.545.1">This is accompanied</span><a id="_idIndexMarker777" class="calibre3"/><span class="kobospan" id="kobo.546.1"> by the following visual output:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.547.1"><img alt="A mountain with a lake and trees  Description automatically generated with medium confidence" src="../Images/B21714_10_09.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.548.1">Figure 10.9: Image generated by DALL·E based on a trailer review</span></p>
<p class="normal1"><span class="kobospan" id="kobo.549.1">Wow! </span><span class="kobospan" id="kobo.549.2">Not only was the agent</span><a id="_idIndexMarker778" class="calibre3"/><span class="kobospan" id="kobo.550.1"> able to use all the tools with the proper scope but it also adapted the style to the type of channel we want to share our review on – in this case, Instagram.</span></p>
<h1 class="heading" id="_idParaDest-150"><span class="kobospan" id="kobo.551.1">Option 3: Hard-coded approach with a sequential chain</span></h1>
<p class="normal"><span class="kobospan" id="kobo.552.1">The third and last option</span><a id="_idIndexMarker779" class="calibre3"/><span class="kobospan" id="kobo.553.1"> offers yet another way of implementing</span><a id="_idIndexMarker780" class="calibre3"/><span class="kobospan" id="kobo.554.1"> a multimodal application, which performs the following tasks:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.555.1">Generates a story based on a topic given by the user.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.556.1">Generates a social media post to promote the story.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.557.1">Generates an image to go along with the social media post.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.558.1">We will call</span><a id="_idIndexMarker781" class="calibre3"/><span class="kobospan" id="kobo.559.1"> this application </span><strong class="screentext"><span class="kobospan" id="kobo.560.1">StoryScribe</span></strong><span class="kobospan" id="kobo.561.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.562.1">To implement this, we will build</span><a id="_idIndexMarker782" class="calibre3"/><span class="kobospan" id="kobo.563.1"> separate LangChain chains</span><a id="_idIndexMarker783" class="calibre3"/><span class="kobospan" id="kobo.564.1"> for those single tasks, and then combine them into a </span><code class="inlinecode"><span class="kobospan" id="kobo.565.1">SequentialChain</span></code><span class="kobospan" id="kobo.566.1">. </span><span class="kobospan" id="kobo.566.2">As we saw in </span><em class="italic"><span class="kobospan" id="kobo.567.1">Chapter 1</span></em><span class="kobospan" id="kobo.568.1">, this is a type of chain that allows you to execute multiple chains in a sequence. </span><span class="kobospan" id="kobo.568.2">You can specify the order of the chains and how they pass their outputs to the next chain. </span><span class="kobospan" id="kobo.568.3">So, we first need to create individual chains, then combine them and run as a unique chain. </span><span class="kobospan" id="kobo.568.4">Let’s follow these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.569.1">We’ll start by initializing the story generator chain:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.570.1">from</span></span><span class="kobospan" id="kobo.571.1"> langchain.chains </span><span class="hljs-keyword"><span class="kobospan" id="kobo.572.1">import</span></span><span class="kobospan" id="kobo.573.1"> SequentialChain, LLMChain
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.574.1">from</span></span><span class="kobospan" id="kobo.575.1"> langchain.prompts </span><span class="hljs-keyword"><span class="kobospan" id="kobo.576.1">import</span></span><span class="kobospan" id="kobo.577.1"> PromptTemplate
story_template = </span><span class="hljs-string"><span class="kobospan" id="kobo.578.1">"""You are a storyteller. </span><span class="kobospan" id="kobo.578.2">Given a topic, a genre and a target audience, you generate a story.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.579.1">Topic: {topic}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.580.1">Genre: {genre}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.581.1">Audience: {audience}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.582.1">Story: This is a story about the above topic, with the above genre and for the above audience:"""</span></span><span class="kobospan" id="kobo.583.1">
story_prompt_template = PromptTemplate(input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.584.1">"topic"</span></span><span class="kobospan" id="kobo.585.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.586.1">"genre"</span></span><span class="kobospan" id="kobo.587.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.588.1">"audience"</span></span><span class="kobospan" id="kobo.589.1">], template=story_template)
story_chain = LLMChain(llm=llm, prompt=story_prompt_template, output_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.590.1">"story"</span></span><span class="kobospan" id="kobo.591.1">)
result = story_chain({</span><span class="hljs-string"><span class="kobospan" id="kobo.592.1">'topic'</span></span><span class="kobospan" id="kobo.593.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.594.1">'friendship story'</span></span><span class="kobospan" id="kobo.595.1">,</span><span class="hljs-string"><span class="kobospan" id="kobo.596.1">'genre'</span></span><span class="kobospan" id="kobo.597.1">:</span><span class="hljs-string"><span class="kobospan" id="kobo.598.1">'adventure'</span></span><span class="kobospan" id="kobo.599.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.600.1">'audience'</span></span><span class="kobospan" id="kobo.601.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.602.1">'young adults'</span></span><span class="kobospan" id="kobo.603.1">})
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.604.1">print</span></span><span class="kobospan" id="kobo.605.1">(result[</span><span class="hljs-string"><span class="kobospan" id="kobo.606.1">'story'</span></span><span class="kobospan" id="kobo.607.1">])
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.608.1">This gives us the following output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.609.1">John and Sarah had been best friends since they were kids. </span><span class="kobospan" id="kobo.609.2">They had grown up together, shared secrets, and been through thick and thin.[…]
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="2"><span class="kobospan" id="kobo.610.1">Note that I’ve set the </span><code class="inlinecode"><span class="kobospan" id="kobo.611.1">output_key= "story"</span></code><span class="kobospan" id="kobo.612.1"> parameter so that it can be easily linked as output to the next chain, which will be the social post generator:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.613.1">template = </span><span class="hljs-string"><span class="kobospan" id="kobo.614.1">"""You are an influencer that, given a story, generate a social media post to promote the story.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.615.1">The style should reflect the type of social media used.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.616.1">Story:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.617.1">{story}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.618.1">Social media: {social}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.619.1">Review from a New York Times play critic of the above play:"""</span></span><span class="kobospan" id="kobo.620.1">
prompt_template = PromptTemplate(input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.621.1">"story"</span></span><span class="kobospan" id="kobo.622.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.623.1">"social"</span></span><span class="kobospan" id="kobo.624.1">], template=template)
social_chain = LLMChain(llm=llm, prompt=prompt_template, output_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.625.1">'post'</span></span><span class="kobospan" id="kobo.626.1">)
post = social_chain({</span><span class="hljs-string"><span class="kobospan" id="kobo.627.1">'story'</span></span><span class="kobospan" id="kobo.628.1">: result[</span><span class="hljs-string"><span class="kobospan" id="kobo.629.1">'story'</span></span><span class="kobospan" id="kobo.630.1">], </span><span class="hljs-string"><span class="kobospan" id="kobo.631.1">'social'</span></span><span class="kobospan" id="kobo.632.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.633.1">'Instagram'</span></span><span class="kobospan" id="kobo.634.1">})
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.635.1">print</span></span><span class="kobospan" id="kobo.636.1">(post[</span><span class="hljs-string"><span class="kobospan" id="kobo.637.1">'post'</span></span><span class="kobospan" id="kobo.638.1">])
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.639.1">The following output is then obtained:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.640.1">"John and Sarah's journey of discovery and friendship is a must-see! </span><span class="kobospan" id="kobo.640.2">From the magical world they explore to the obstacles they overcome, this play is sure to leave you with a newfound appreciation for the power of friendship. </span><span class="kobospan" id="kobo.640.3">#FriendshipGoals #AdventureAwaits #MagicalWorlds"
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.641.1">Here, I used the output</span><a id="_idIndexMarker784" class="calibre3"/><span class="kobospan" id="kobo.642.1"> of </span><code class="inlinecode"><span class="kobospan" id="kobo.643.1">story_chain</span></code><span class="kobospan" id="kobo.644.1"> as input</span><a id="_idIndexMarker785" class="calibre3"/><span class="kobospan" id="kobo.645.1"> to </span><code class="inlinecode"><span class="kobospan" id="kobo.646.1">social_chain</span></code><span class="kobospan" id="kobo.647.1">. </span><span class="kobospan" id="kobo.647.2">When we combine all the chains together, this step will be automatically performed by the sequential chain.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.648.1">Finally, let’s initialize an image generator chain:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.649.1">from</span></span><span class="kobospan" id="kobo.650.1"> langchain.utilities.dalle_image_generator </span><span class="hljs-keyword"><span class="kobospan" id="kobo.651.1">import</span></span><span class="kobospan" id="kobo.652.1"> DallEAPIWrapper
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.653.1">from</span></span><span class="kobospan" id="kobo.654.1"> langchain.llms </span><span class="hljs-keyword"><span class="kobospan" id="kobo.655.1">import</span></span><span class="kobospan" id="kobo.656.1"> OpenAI
template = </span><span class="hljs-string"><span class="kobospan" id="kobo.657.1">"""Generate a detailed prompt to generate an image based on the following social media post:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.658.1">Social media post:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.659.1">{post}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.660.1">"""</span></span><span class="kobospan" id="kobo.661.1">
prompt = PromptTemplate(
    input_variables=[</span><span class="hljs-string"><span class="kobospan" id="kobo.662.1">"post"</span></span><span class="kobospan" id="kobo.663.1">],
    template=template,
)
image_chain = LLMChain(llm=llm, prompt=prompt, output_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.664.1">'image'</span></span><span class="kobospan" id="kobo.665.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.666.1">Note that the output</span><a id="_idIndexMarker786" class="calibre3"/><span class="kobospan" id="kobo.667.1"> of the chain will be the prompt</span><a id="_idIndexMarker787" class="calibre3"/><span class="kobospan" id="kobo.668.1"> to pass to the DALL·E model.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="4"><span class="kobospan" id="kobo.669.1">In order to generate the image, we need to use the </span><code class="inlinecode"><span class="kobospan" id="kobo.670.1">DallEAPIWrapper()</span></code><span class="kobospan" id="kobo.671.1"> module available in LangChain:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.672.1">from</span></span><span class="kobospan" id="kobo.673.1"> langchain.utilities.dalle_image_generator </span><span class="hljs-keyword"><span class="kobospan" id="kobo.674.1">import</span></span><span class="kobospan" id="kobo.675.1"> DallEAPIWrapper
image_url = DallEAPIWrapper().run(image_chain.run(</span><span class="hljs-string"><span class="kobospan" id="kobo.676.1">"a cartoon-style cat playing piano"</span></span><span class="kobospan" id="kobo.677.1">))
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.678.1">import</span></span><span class="kobospan" id="kobo.679.1"> cv2
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.680.1">from</span></span><span class="kobospan" id="kobo.681.1"> skimage </span><span class="hljs-keyword"><span class="kobospan" id="kobo.682.1">import</span></span><span class="kobospan" id="kobo.683.1"> io
image = io.imread(image_url)
cv2.imshow(</span><span class="hljs-string"><span class="kobospan" id="kobo.684.1">'image'</span></span><span class="kobospan" id="kobo.685.1">, image)
cv2.waitKey(</span><span class="hljs-number"><span class="kobospan" id="kobo.686.1">0</span></span><span class="kobospan" id="kobo.687.1">)  
cv2.destroyAllWindows()
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.688.1">This generates the following output:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.689.1"><img alt="A child giving a flower to a child  Description automatically generated" src="../Images/B21714_10_10.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.690.1">Figure 10.10: Picture generated by DALL·E given a social media post</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="5"><span class="kobospan" id="kobo.691.1">The final step will</span><a id="_idIndexMarker788" class="calibre3"/><span class="kobospan" id="kobo.692.1"> be to put it all together</span><a id="_idIndexMarker789" class="calibre3"/><span class="kobospan" id="kobo.693.1"> into a sequential chain:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.694.1">overall_chain = SequentialChain(input_variables = [</span><span class="hljs-string"><span class="kobospan" id="kobo.695.1">'topic'</span></span><span class="kobospan" id="kobo.696.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.697.1">'genre'</span></span><span class="kobospan" id="kobo.698.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.699.1">'audience'</span></span><span class="kobospan" id="kobo.700.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.701.1">'social'</span></span><span class="kobospan" id="kobo.702.1">],
                chains=[story_chain, social_chain, image_chain],
                output_variables = [</span><span class="hljs-string"><span class="kobospan" id="kobo.703.1">'post'</span></span><span class="kobospan" id="kobo.704.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.705.1">'image'</span></span><span class="kobospan" id="kobo.706.1">], verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.707.1">True</span></span><span class="kobospan" id="kobo.708.1">)
overall_chain({</span><span class="hljs-string"><span class="kobospan" id="kobo.709.1">'topic'</span></span><span class="kobospan" id="kobo.710.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.711.1">'friendship story'</span></span><span class="kobospan" id="kobo.712.1">,</span><span class="hljs-string"><span class="kobospan" id="kobo.713.1">'genre'</span></span><span class="kobospan" id="kobo.714.1">:</span><span class="hljs-string"><span class="kobospan" id="kobo.715.1">'adventure'</span></span><span class="kobospan" id="kobo.716.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.717.1">'audience'</span></span><span class="kobospan" id="kobo.718.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.719.1">'young adults'</span></span><span class="kobospan" id="kobo.720.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.721.1">'social'</span></span><span class="kobospan" id="kobo.722.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.723.1">'Instagram'</span></span><span class="kobospan" id="kobo.724.1">}, return_only_outputs=</span><span class="hljs-literal"><span class="kobospan" id="kobo.725.1">True</span></span><span class="kobospan" id="kobo.726.1">)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.727.1">Here is our output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.728.1">{'post': '\n\n"John and Sarah\'s journey of discovery and friendship is a must-see! </span><span class="kobospan" id="kobo.728.2">[…],
'image': '\nPrompt:\n\nCreate a digital drawing of John and Sarah standing side-by-side,[…]'}
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.729.1">Since we passed the </span><code class="inlinecode"><span class="kobospan" id="kobo.730.1">output_variables = ['post, 'image']</span></code><span class="kobospan" id="kobo.731.1"> parameter to the chain, those will be the two outputs of the chain. </span><span class="kobospan" id="kobo.731.2">With </span><code class="inlinecode"><span class="kobospan" id="kobo.732.1">SequentialChain</span></code><span class="kobospan" id="kobo.733.1">, we have the flexibility to decide as many output variables as we want, so that we can construct our output as we please.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.734.1">Overall, there are several ways</span><a id="_idIndexMarker790" class="calibre3"/><span class="kobospan" id="kobo.735.1"> to reach multimodality</span><a id="_idIndexMarker791" class="calibre3"/><span class="kobospan" id="kobo.736.1"> within your application, and LangChain offers many components that make it easier. </span><span class="kobospan" id="kobo.736.2">Now, let’s compare these approaches.</span></p>
<h1 class="heading" id="_idParaDest-151"><span class="kobospan" id="kobo.737.1">Comparing the three options</span></h1>
<p class="normal"><span class="kobospan" id="kobo.738.1">We examined three options</span><a id="_idIndexMarker792" class="calibre3"/><span class="kobospan" id="kobo.739.1"> to achieve</span><a id="_idIndexMarker793" class="calibre3"/><span class="kobospan" id="kobo.740.1"> this result: options 1 and 2 follow the “agentic” approach, using, respectively, pre-built toolkit and single tools</span><a id="_idIndexMarker794" class="calibre3"/><span class="kobospan" id="kobo.741.1"> combined; option 3, on the other</span><a id="_idIndexMarker795" class="calibre3"/><span class="kobospan" id="kobo.742.1"> hand, follows a hard-coded</span><a id="_idIndexMarker796" class="calibre3"/><span class="kobospan" id="kobo.743.1"> approach, letting</span><a id="_idIndexMarker797" class="calibre3"/><span class="kobospan" id="kobo.744.1"> the developer decide the order of actions to be done.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.745.1"> All three come with pros and cons, so let’s wrap up some final considerations:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.746.1">Flexibility vs control</span></strong><span class="kobospan" id="kobo.747.1">: The agentic approach lets the LLM decide which actions to take and in which order. </span><span class="kobospan" id="kobo.747.2">This implies greater flexibility for the end user since there are no constraints in terms of queries that can be done. </span><span class="kobospan" id="kobo.747.3">On the other hand, having no control over the agent’s chain of thoughts could lead to mistakes that would need several tests of prompt engineering. </span><span class="kobospan" id="kobo.747.4">Plus, as LLMs are non-deterministic, it is also hard to recreate mistakes to retrieve the wrong thought process. </span><span class="kobospan" id="kobo.747.5">Under this point of view, the hard-coded approach is safer, since the developer has full control over the order of execution of the actions.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.748.1">Evaluations</span></strong><span class="kobospan" id="kobo.749.1">: The agentic approach leverages the tools to generate the final answer so that we don’t have to bother to plan these actions. </span><span class="kobospan" id="kobo.749.2">However, if the final output doesn’t satisfy us, it might be cumbersome to understand what is the main source of the error: it might be a wrong plan, rather than a tool that is not doing its job correctly, or maybe a wrong prompt overall. </span><span class="kobospan" id="kobo.749.3">On the other hand, with the hard-coded approach, each chain has its own model that can be tested separately, so that it is easier to identify the step of the process where the main error has occurred.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.750.1">Maintenance</span></strong><span class="kobospan" id="kobo.751.1">: With the agentic approach, there is one component to maintain: the agent itself. </span><span class="kobospan" id="kobo.751.2">We have in fact one prompt, one agent, and one LLM, while the toolkit or list of tools is pre-built and we don’t need to maintain them. </span><span class="kobospan" id="kobo.751.3">On the other hand, with the hard-coded approach, for each chain, we need a separate prompt, model, and testing activities.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.752.1">To conclude, there is no golden rule to decide which approach to follow: it’s up to the developer to decide depending on the relative weight of the above parameters. </span><span class="kobospan" id="kobo.752.2">As a general rule of thumb, the first step</span><a id="_idIndexMarker798" class="calibre3"/><span class="kobospan" id="kobo.753.1"> should be to define</span><a id="_idIndexMarker799" class="calibre3"/><span class="kobospan" id="kobo.754.1"> the problem to solve and then evaluate the complexity of each</span><a id="_idIndexMarker800" class="calibre3"/><span class="kobospan" id="kobo.755.1"> approach with respect to that problem. </span><span class="kobospan" id="kobo.755.2">If, for example, it is a task that can be entirely</span><a id="_idIndexMarker801" class="calibre3"/><span class="kobospan" id="kobo.756.1"> addressed with the Cognitive Services toolkit without even doing prompt</span><a id="_idIndexMarker802" class="calibre3"/><span class="kobospan" id="kobo.757.1"> engineering, that could</span><a id="_idIndexMarker803" class="calibre3"/><span class="kobospan" id="kobo.758.1"> be the easiest way to proceed; on the other hand, if it requires a lot of control over the single components as well as on the sequence of execution, a hard-coded approach is preferable.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.759.1">In the next section, we are going to build a sample front-end using Streamlit, built on top of StoryScribe.</span></p>
<h1 class="heading" id="_idParaDest-152"><span class="kobospan" id="kobo.760.1">Developing the front-end with Streamlit</span></h1>
<p class="normal"><span class="kobospan" id="kobo.761.1">Now that we have seen the logic</span><a id="_idIndexMarker804" class="calibre3"/><span class="kobospan" id="kobo.762.1"> behind an LLM-powered</span><a id="_idIndexMarker805" class="calibre3"/><span class="kobospan" id="kobo.763.1"> StoryScribe, it is time to give our application a GUI. </span><span class="kobospan" id="kobo.763.2">To do so, we will once again leverage Streamlit. </span><span class="kobospan" id="kobo.763.3">As always, you can find the whole Python code in the GitHub book repository at </span><a href="Chapter_10.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.764.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.765.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.766.1">As per the previous sections, you need to create a </span><code class="inlinecode"><span class="kobospan" id="kobo.767.1">.py</span></code><span class="kobospan" id="kobo.768.1"> file to run in your terminal via </span><code class="inlinecode"><span class="kobospan" id="kobo.769.1">streamlit run file.py</span></code><span class="kobospan" id="kobo.770.1">. </span><span class="kobospan" id="kobo.770.2">In our case, the file will be named </span><code class="inlinecode"><span class="kobospan" id="kobo.771.1">storyscribe.py</span></code><span class="kobospan" id="kobo.772.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.773.1">The following are the main steps to set up the front-end:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.774.1">Configuring the application webpage:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.775.1">st.set_page_config(page_title=</span><span class="hljs-string"><span class="kobospan" id="kobo.776.1">"StoryScribe"</span></span><span class="kobospan" id="kobo.777.1">, page_icon=</span><span class="hljs-string"><span class="kobospan" id="kobo.778.1">"</span><span class="kobospan" id="kobo.779.1"><img alt="" role="presentation" src="../Images/Book.png" class="calibre4"/></span><span class="kobospan" id="kobo.780.1">"</span></span><span class="kobospan" id="kobo.781.1">)
st.header(</span><span class="hljs-string"><span class="kobospan" id="kobo.782.1">'</span><span class="kobospan" id="kobo.783.1"><img alt="" role="presentation" src="../Images/Book.png" class="calibre4"/></span><span class="kobospan" id="kobo.784.1"> Welcome to StoryScribe, your story generator and promoter!'</span></span><span class="kobospan" id="kobo.785.1">)
load_dotenv()
openai_api_key = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.786.1">'OPENAI_API_KEY'</span></span><span class="kobospan" id="kobo.787.1">]
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.788.1">Initialize the dynamic variables to be used within the placeholders of the prompts:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.789.1">topic = st.sidebar.text_input(</span><span class="hljs-string"><span class="kobospan" id="kobo.790.1">"What is topic?"</span></span><span class="kobospan" id="kobo.791.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.792.1">'A dog running on the beach'</span></span><span class="kobospan" id="kobo.793.1">)
genre = st.sidebar.text_input(</span><span class="hljs-string"><span class="kobospan" id="kobo.794.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.795.1">What is the genre?"</span></span><span class="kobospan" id="kobo.796.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.797.1">'Drama'</span></span><span class="kobospan" id="kobo.798.1">)
audience = st.sidebar.text_input(</span><span class="hljs-string"><span class="kobospan" id="kobo.799.1">"What is your audience?"</span></span><span class="kobospan" id="kobo.800.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.801.1">'Young adult'</span></span><span class="kobospan" id="kobo.802.1">)
social = st.sidebar.text_input(</span><span class="hljs-string"><span class="kobospan" id="kobo.803.1">"What is your social?"</span></span><span class="kobospan" id="kobo.804.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.805.1">'Instagram'</span></span><span class="kobospan" id="kobo.806.1">)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.807.1">Initialize all the chains</span><a id="_idIndexMarker806" class="calibre3"/><span class="kobospan" id="kobo.808.1"> and the overall </span><a id="_idIndexMarker807" class="calibre3"/><span class="kobospan" id="kobo.809.1">chain (I will omit here all the prompt templates; you can find them in the GitHub repository of the book):
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.810.1">story_chain = LLMChain(llm=llm, prompt=story_prompt_template, output_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.811.1">"story"</span></span><span class="kobospan" id="kobo.812.1">)
social_chain = LLMChain(llm=llm, prompt=social_prompt_template, output_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.813.1">'post'</span></span><span class="kobospan" id="kobo.814.1">)
image_chain = LLMChain(llm=llm, prompt=prompt, output_key=</span><span class="hljs-string"><span class="kobospan" id="kobo.815.1">'image'</span></span><span class="kobospan" id="kobo.816.1">)
overall_chain = SequentialChain(input_variables = [</span><span class="hljs-string"><span class="kobospan" id="kobo.817.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.818.1">topic'</span></span><span class="kobospan" id="kobo.819.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.820.1">'genre'</span></span><span class="kobospan" id="kobo.821.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.822.1">'audience'</span></span><span class="kobospan" id="kobo.823.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.824.1">'social'</span></span><span class="kobospan" id="kobo.825.1">],
                chains=[story_chain, social_chain, image_chain],
                output_variables = [</span><span class="hljs-string"><span class="kobospan" id="kobo.826.1">'story'</span></span><span class="kobospan" id="kobo.827.1">,</span><span class="hljs-string"><span class="kobospan" id="kobo.828.1">'post'</span></span><span class="kobospan" id="kobo.829.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.830.1">'image'</span></span><span class="kobospan" id="kobo.831.1">], verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.832.1">True</span></span><span class="kobospan" id="kobo.833.1">)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.834.1">Run the overall chain and print the results:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.835.1">if</span></span><span class="kobospan" id="kobo.836.1"> st.button(</span><span class="hljs-string"><span class="kobospan" id="kobo.837.1">'Create your post!'</span></span><span class="kobospan" id="kobo.838.1">):
    result = overall_chain({</span><span class="hljs-string"><span class="kobospan" id="kobo.839.1">'topic'</span></span><span class="kobospan" id="kobo.840.1">: topic,</span><span class="hljs-string"><span class="kobospan" id="kobo.841.1">'genre'</span></span><span class="kobospan" id="kobo.842.1">:genre, </span><span class="hljs-string"><span class="kobospan" id="kobo.843.1">'audience'</span></span><span class="kobospan" id="kobo.844.1">: audience, </span><span class="hljs-string"><span class="kobospan" id="kobo.845.1">'social'</span></span><span class="kobospan" id="kobo.846.1">: social}, return_only_outputs=</span><span class="hljs-literal"><span class="kobospan" id="kobo.847.1">True</span></span><span class="kobospan" id="kobo.848.1">)
    image_url = DallEAPIWrapper().run(result[</span><span class="hljs-string"><span class="kobospan" id="kobo.849.1">'image'</span></span><span class="kobospan" id="kobo.850.1">])
    st.subheader(</span><span class="hljs-string"><span class="kobospan" id="kobo.851.1">'Story'</span></span><span class="kobospan" id="kobo.852.1">)
    st.write(result[</span><span class="hljs-string"><span class="kobospan" id="kobo.853.1">'story'</span></span><span class="kobospan" id="kobo.854.1">])
    st.subheader(</span><span class="hljs-string"><span class="kobospan" id="kobo.855.1">'Social Media Post'</span></span><span class="kobospan" id="kobo.856.1">)
    st.write(result[</span><span class="hljs-string"><span class="kobospan" id="kobo.857.1">'post'</span></span><span class="kobospan" id="kobo.858.1">])
    st.image(image_url)
</span></code></pre>
</li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.859.1">In this case, I’ve set the </span><code class="inlinecode"><span class="kobospan" id="kobo.860.1">output_variables = ['story','post', 'image']</span></code><span class="kobospan" id="kobo.861.1"> parameter so that we will have also the story</span><a id="_idIndexMarker808" class="calibre3"/><span class="kobospan" id="kobo.862.1"> itself as output. </span><span class="kobospan" id="kobo.862.2">The final result looks</span><a id="_idIndexMarker809" class="calibre3"/><span class="kobospan" id="kobo.863.1"> like the following:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.864.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_10_11.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.865.1">Figure 10.11: Front-end of StoryScribe showing the story output</span></p>
<p class="normal1"><span class="kobospan" id="kobo.866.1">The following picture is the resulting Instagram post:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.867.1"><img alt="A screenshot of a painting  Description automatically generated" src="../Images/B21714_10_12.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.868.1">Figure 10.12: Front-end of StoryScribe showing the social media post along with the generated image</span></p>
<p class="normal1"><span class="kobospan" id="kobo.869.1">With just a few lines</span><a id="_idIndexMarker810" class="calibre3"/><span class="kobospan" id="kobo.870.1"> of code, we were able to set up a simple front-end</span><a id="_idIndexMarker811" class="calibre3"/><span class="kobospan" id="kobo.871.1"> for StoryScribe with multimodal capabilities.</span></p>
<h1 class="heading" id="_idParaDest-153"><span class="kobospan" id="kobo.872.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.873.1">In this chapter, we introduced the concept of multimodality and how to achieve it even without multimodal models. </span><span class="kobospan" id="kobo.873.2">We explored three different ways of achieving the objective of a multimodal application: an agentic approach with a pre-built toolkit, an agentic approach with the combination of single tools, and a hard-coded approach with chained models.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.874.1">We delved into the concrete implementation of three applications with the above methods, examining the pros and cons of each approach. </span><span class="kobospan" id="kobo.874.2">We saw, for example, how an agentic approach gives higher flexibility to the end user at the price of less control of the backend plan of action.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.875.1">Finally, we implemented a front-end with Streamlit to build a consumable application with the hard-coded approach.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.876.1">With this chapter, we conclude Part 2 of the book, where we examined hands-on scenarios and built LLMs-powered applications. </span><span class="kobospan" id="kobo.876.2">In the next chapter, we will focus on how to customize your LLMs even more with the process of fine-tuning, leveraging open-source models, and using custom data for this purpose.</span></p>
<h1 class="heading" id="_idParaDest-154"><span class="kobospan" id="kobo.877.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.878.1">Source code for YouTube tools: </span><a href="https://github.com/venuv/langchain_yt_tools" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.879.1">https://github.com/venuv/langchain_yt_tools</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.880.1">LangChain YouTube tool: </span><a href="https://python.langchain.com/docs/integrations/tools/youtube" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.881.1">https://python.langchain.com/docs/integrations/tools/youtube</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.882.1">LangChain AzureCognitiveServicesToolkit: </span><a href="https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.883.1">https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.884.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.885.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.886.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.887.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>