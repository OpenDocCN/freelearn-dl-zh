<html><head></head><body><div><div><h1 id="_idParaDest-413"><em class="italic"><a id="_idTextAnchor374"/>Chapter 16</em>: Diving Deeper with the IMU</h1>
			<p>In <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, we read data from an <strong class="bold">inertial measurement unit</strong> (<strong class="bold">IMU</strong>). We've now learned a bit more about processing sensor data, using math and pipelines to make decisions. </p>
			<p>In this chapter, we will learn how to get calibrated data from the IMU, combine data from the sensors, and use this to make a robot have absolute orientation-based behavior. On the way, we'll see algorithms for better precision/speed or accuracy.</p>
			<p>By the end of the chapter, you will be able to detect a robot's absolute orientation, display it on a screen, and incorporate this with the <strong class="bold">Proportional-Integral-Derivative</strong> (<strong class="bold">PID</strong>) behaviors.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Programming a virtual robot</li>
				<li>Detecting rotation with the gyroscope</li>
				<li>Detecting pitch and roll with the accelerometer</li>
				<li>Detecting a heading with the magnetometer</li>
				<li>Getting a rough heading from the magnetometer</li>
				<li>Combining sensors for orientation</li>
				<li>Driving a robot from IMU data</li>
			</ul>
			<h1 id="_idParaDest-414"><a id="_idTextAnchor375"/>Technical requirements</h1>
			<p>For this chapter, you will need the following items:</p>
			<ul>
				<li>The robot from at least <a href="B15660_14_Final_ASB_ePub.xhtml#_idTextAnchor315"><em class="italic">Chapter 14</em></a>, <em class="italic">Line-Following with a Camera in Python</em></li>
				<li>The robot code from <a href="B15660_14_Final_ASB_ePub.xhtml#_idTextAnchor315"><em class="italic">Chapter 14</em></a>, <em class="italic">Line-Following with a Camera in Python</em>, at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter14">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter14</a></li>
				<li>The IMU code from <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter12">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter12</a></li>
				<li>A wide driving space without many magnets</li>
				<li>A magnetic compass</li>
			</ul>
			<p>For the complete code for this chapter, go to <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter16">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter16</a>.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/2LztwOO">https://bit.ly/2LztwOO</a></p>
			<h1 id="_idParaDest-415"><a id="_idTextAnchor376"/>Programming a virtual robot</h1>
			<p>We will first detect <a id="_idIndexMarker1016"/>our robot's orientation; it would be useful to show this as a 3D robot model. This part builds upon the <em class="italic">Representing coordinate and rotation systems</em> section in <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>. In this section, we will construct a simple model of our robot in VPython.</p>
			<h2 id="_idParaDest-416"><a id="_idTextAnchor377"/>Modeling the robot in VPython</h2>
			<p>We'll use <a id="_idIndexMarker1017"/>shapes, known <a id="_idIndexMarker1018"/>as <strong class="bold">primitives</strong>, to model the robot. They <a id="_idIndexMarker1019"/>have a position, rotation, size, and color. The height-and-width parameters match the VPython-world coordinate system (see <em class="italic">Figure 12.14 – The robot body coordinate system</em> in <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>), so we must rotate things to match the robot body coordinate system.</p>
			<p>First, we need to collect some robot measurements. The following diagram shows where they are. Once the major measurements are made, estimates can be used for smaller measurements:</p>
			<div><div><img src="img/B15660_16_01.jpg" alt="" width="352" height="423"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.1 – Measurements for the virtual robot</p>
			<p><em class="italic">Figure 16.1</em> shows the <a id="_idIndexMarker1020"/>measurements across the robot. A top <a id="_idIndexMarker1021"/>view and a left view show to cover the different aspects. This includes the width and height of the base—note that we are treating it as a rectangle for this purpose. The wheels' size and position, along with the castor-wheel size and position, are needed. Measure or guess these for your robot. For our purposes, guesses are good enough. Positions come from the middle of the chassis.</p>
			<p>Let's write the code to make the basic shape, as follows:</p>
			<ol>
				<li>Create a file called <code>virtual_robot.py</code> and start it by adding in the <code>vpython</code> import and our robot view, as follows:<pre><strong class="bold">import vpython as vp</strong>
<strong class="bold">from robot_pose import robot_view</strong></pre></li>
				<li>We'll put the virtual bot in a function ready to use in a few different behaviors, like this:<pre><strong class="bold">def make_robot():</strong></pre></li>
				<li>We put the <a id="_idIndexMarker1022"/>robot's measurements from <em class="italic">Figure 16.1</em> in <a id="_idIndexMarker1023"/>variables. I've used <strong class="bold">millimeters</strong> (<strong class="bold">mm</strong>) for all of them. The code is shown in the following snippet:<pre><strong class="bold">    chassis_width = 155</strong>
<strong class="bold">    chassis_thickness = 3</strong>
<strong class="bold">    chassis_length = 200</strong>
<strong class="bold">    wheel_thickness = 26</strong>
<strong class="bold">    wheel_diameter = 70</strong>
<strong class="bold">    axle_x = 30</strong>
<strong class="bold">    axle_z = -20</strong>
<strong class="bold">    castor_position = vp.vector(-80, -6, -30)</strong>
<strong class="bold">    castor_radius = 14</strong>
<strong class="bold">    castor_thickness = 12</strong></pre></li>
				<li>The base is a box with the position defaulting to (<code>0</code>, <code>0</code>, <code>0</code>). The code is shown in the following snippet:<pre><strong class="bold">    base = vp.box(length=chassis_length,</strong>
<strong class="bold">                  height=chassis_thickness,</strong>
<strong class="bold">                  width=chassis_width)</strong></pre></li>
				<li>Rotate this box to match the body coordinate system by 90 degrees around the <em class="italic">x</em> axis, putting the <em class="italic">z </em>axis up, as follows:<pre><strong class="bold">    base.rotate(angle=vp.radians(90), </strong>
<strong class="bold">                axis=vp.vector(1, 0, 0))</strong></pre></li>
				<li>We'll use two cylinders for the wheels. The distance from each wheel to the middle is roughly half the chassis width. Let's use it to create the wheels' <em class="italic">y</em> positions, as follows:<pre><strong class="bold">   wheel_dist = chassis_width/2</strong></pre></li>
				<li>We set wheel positions to line up with the ends of the motor axles. The left wheel has a <em class="italic">y</em> coordinate; <code>-wheel_dist</code> moves it left of the platform, as illustrated in the following code snippet:<pre><code>cylinder</code> axis says which way it is <a id="_idIndexMarker1025"/>pointing. We set <em class="italic">y</em> to <em class="italic">-1</em> to point it left.</p></li>
				<li>Now, we set the right wheel, with a positive <code>wheel_dist</code> and <em class="italic">y</em> as <em class="italic">1</em> for the axis so that it points to the right, as illustrated in the following code snippet:<pre><strong class="bold">    wheel_r = vp.cylinder(radius=wheel_diameter/2,</strong>
<strong class="bold">          length=wheel_thickness,</strong>
<strong class="bold">          pos=vp.vector(axle_x, wheel_dist, axle_z),</strong>
<strong class="bold">          axis=vp.vector(0, 1, 0))</strong></pre></li>
				<li>I've used a cylinder for the rear castor wheel, as illustrated in the following code snippet:<pre><strong class="bold">    castor = vp.cylinder(radius=castor_radius,</strong>
<strong class="bold">          length=castor_thickness,</strong>
<strong class="bold">          pos=castor_position,</strong>
<strong class="bold">          axis=vp.vector(0, 1, 0))</strong></pre></li>
				<li>Now, we join all of these parts into a compound object—a single 3D object, like this:<pre><strong class="bold">    return vp.compound([base, wheel_l, wheel_r, castor])</strong></pre></li>
				<li>For testing it, let's make a tiny <code>main</code> section. This code checks if you've launched it directly, so the following code won't run when we import the virtual robot as a library:<pre><strong class="bold">if __name__ == "__main__":</strong></pre></li>
				<li>Set the view, putting the camera just in front of the robot, as follows:<pre><strong class="bold">    robot_view()</strong></pre></li>
				<li>We'll <a id="_idIndexMarker1026"/>add axes to show where things are, like <a id="_idIndexMarker1027"/>this:<pre><strong class="bold">    x_arrow = vp.arrow(axis=vp.vector(200, 0, 0), color=vp.color.red)</strong>
<strong class="bold">    y_arrow = vp.arrow(axis=vp.vector(0, 200, 0), color=vp.color.green)</strong>
<strong class="bold">    z_arrow = vp.arrow(axis=vp.vector(0, 0, 200), color=vp.color.blue)</strong></pre></li>
				<li>And then, we'll draw the robot, as follows:<pre><strong class="bold">    make_robot()</strong></pre></li>
				<li>Upload and test this code with <code>vpython virtual_robot.py</code>.</li>
				<li>Open up a browser to port <code>9020</code> on your robot to see your virtual robot. You should see a figure like the following:<div><img src="img/B15660_16_02.jpg" alt="" width="441" height="405"/></div><p class="figure-caption">Figure 16.2 – Screenshot of the 3D virtual robot from VPython</p><p>In <em class="italic">Figure 16.2</em>, we can <a id="_idIndexMarker1028"/>see the <em class="italic">x</em> axis facing forward in <a id="_idIndexMarker1029"/>red<a id="_idTextAnchor378"/>, the <em class="italic">y</em> axis going right in green, and the <em class="italic">z</em> axis going up in blue. This follows a right-hand-rule coordinate system. It shows the virtual robot viewed from the front, with a wheel on either side. It's gray, boxy, and basic, but it will do for our remaining experiments. </p></li>
				<li>You can right-click and drag this around to get another view. The mouse wheel will also zoom in or out. The following screenshot shows the rear castor:</li>
			</ol>
			<div><div><img src="img/B15660_16_03.jpg" alt="" width="488" height="439"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.3 – A different view of the virtual robot</p>
			<p><em class="italic">Figure 16.3</em> shows a left-hand view of this virtual robot. </p>
			<p>Close the browser <a id="_idIndexMarker1030"/>tab, then press <em class="italic">Ctrl</em> + <em class="italic">C</em> to finish this <a id="_idIndexMarker1031"/>program when done. Let's just check you've been able to follow along.</p>
			<h3 id="_idParaDest-417">Troubleshooting</h3>
			<p>If you <a id="_idIndexMarker1032"/>haven't got this to work, let's check a few things, as follows:</p>
			<ol>
				<li value="1">If you receive errors saying <strong class="bold">no such module vpython</strong>, ensure that VPython is installed. Follow the steps in <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, in the  <em class="italic">Reading the temperature</em> section. You need the code from the whole of <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, for this chapter to work.</li>
				<li>If you receive errors saying <strong class="bold">no such command vpython</strong>, ensure you have followed the <em class="italic">Simplifying the VPython command line</em> section from <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>. The alias for VPython is necessary to be able to see a display.</li>
				<li>If you see syntax errors, please check your code for typos.</li>
				<li>If you cannot reach the display (and have checked <em class="italic">Step 1</em>), ensure you use port <code>9020</code> on your robot (mine is <a href="http://myrobot.local:9020">http://myrobot.local:9020</a>).</li>
				<li>Be patient—VPython can take a minute or two to start up.</li>
			</ol>
			<p>Now that we have a <a id="_idIndexMarker1033"/>visual robot to play with, we can revisit the gyroscope and try to make the onscreen robot move like our real robot.</p>
			<h1 id="_idParaDest-418"><a id="_idTextAnchor379"/>Detecting rotation with the gyroscope</h1>
			<p>We've had <a id="_idIndexMarker1034"/>some raw data from the gyroscope, but to use it <a id="_idIndexMarker1035"/>more effectively, we'll need to perform two operations, calibrating the gyroscope, and then integrating it, as shown in the following diagram:</p>
			<div><div><img src="img/B15660_16_04.jpg" alt="" width="539" height="347"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.4 – The gyroscope data flow</p>
			<p><em class="italic">Figure 16.4</em> shows the <a id="_idIndexMarker1036"/>data flow, and we will look closer at the concepts <a id="_idIndexMarker1037"/>later in this section. The first operation is shown at the top, which shows the gyroscope data going through an offset calibration to take out errors. This gives us a calibrated gyroscope, with a rate of change in degrees per second (per axis)—shown by the arrow around the circle. The gyroscope makes a relative measurement.</p>
			<p>The lower part of the diagram is the second operation, combining delta time with the calibrated gyroscope (gyro). We need to <strong class="bold">integrate</strong> that to find an absolute measurement. An integrator multiplies an input value by delta time and adds this to a previous result.  In this case, we multiply the gyroscope rate by delta time to produce a movement for that period (shown by the multiplication symbol in a box). The circle above it has a small slither of pie with dashed lines, denoting the amount moved.</p>
			<p>We add the movement <a id="_idIndexMarker1038"/>to the last value for that axis, shown by the <a id="_idIndexMarker1039"/>plus symbol box. The circle above it shows a solid gray pie segment for the existing position and a new segment with dashed lines. When added, they make the total value for that axis—shown by the circle with a large, solid gray pie segment representing the addition's result.  The system feeds the pitch, roll, or yaw result back into the next cycle.</p>
			<p>Before we do this, we need to correct the errors in the gyroscope. </p>
			<h2 id="_idParaDest-419"><a id="_idTextAnchor380"/>Calibrating the gyroscope</h2>
			<p>As they <a id="_idIndexMarker1040"/>come from the factory, <strong class="bold">microelectromechanical systems</strong> (<strong class="bold">MEMS</strong>) gyroscopes usually <a id="_idIndexMarker1041"/>have minor flaws that cause them to give slightly off readings. These flaws will cause drift in our integration.</p>
			<p>We can make code to detect these and <a id="_idIndexMarker1042"/>compensate; we call this <strong class="bold">calibration</strong>. Proceed as follows:</p>
			<ol>
				<li value="1">Create a file named <code>calibrate_gyro.py</code>.</li>
				<li>We need VPython for vectors, time for a little sleep, and to set up the IMU, as illustrated in the following code snippet:<pre><strong class="bold">from robot_imu import RobotImu</strong>
<strong class="bold">import time</strong>
<strong class="bold">import vpython as vp</strong>
<strong class="bold">imu = RobotImu()</strong></pre></li>
				<li>We need vectors to hold the minimum and maximum values of the gyroscope, as illustrated in the following code snippet:<pre><strong class="bold">gyro_min = vp.vector(0, 0, 0)</strong>
<strong class="bold">gyro_max = vp.vector(0, 0, 0)</strong></pre></li>
				<li>Now, for the loop, we'll do a bunch of readings over time, as follows:<pre><strong class="bold">for n in range(500):</strong>
<strong class="bold">    gyro = imu.read_gyroscope()</strong></pre></li>
				<li>To calibrate, we measure for a while to get the minimum and maximum values for each axis. The Python <code>min</code> function returns the lower of the two values given to it, as follows:<pre><strong class="bold">    gyro_min.x = min(gyro_min.x, gyro.x)</strong>
<strong class="bold">    gyro_min.y = min(gyro_min.y, gyro.y)</strong>
<strong class="bold">    gyro_min.z = min(gyro_min.z, gyro.z)</strong></pre></li>
				<li>We do the same for the maximum values, using the Python <code>max</code> function, as follows:<pre><strong class="bold">    gyro_max.x = max(gyro_max.x, gyro.x)</strong>
<strong class="bold">    gyro_max.y = max(gyro_max.y, gyro.y)</strong>
<strong class="bold">    gyro_max.z = max(gyro_max.z, gyro.z)</strong></pre></li>
				<li>The middle of <a id="_idIndexMarker1043"/>these is an estimate of how far we are from zero. We can calculate this by adding the vectors and dividing by 2, as follows:<pre><strong class="bold">    offset = (gyro_min + gyro_max) / 2</strong></pre></li>
				<li>Sleep a little before the next loop, as follows:<pre><strong class="bold">    time.sleep(.01)</strong></pre></li>
				<li>We print the values so we can use them, as follows:<pre><strong class="bold">print(f"Zero offset: {offset}.")</strong></pre></li>
				<li>This code is ready to run. Upload and run this with Python 3, leaving the robot still on a flat, stable surface until the program exits.</li>
				<li>You should see console output ending with something like this:<pre><strong class="bold">pi@myrobot:~ $ python3 calibrate_gyro.py</strong>
<strong class="bold">Zero offset: &lt;-0.583969, 0.675573, -0.530534&gt;.</strong></pre></li>
			</ol>
			<p>What we've measured here is how much the gyroscope changes, on average, when stationary. This is calculated as an offset for each axis. By subtracting this from the measurements, we will mostly offset any continuous errors from the gyroscope. Let's put this somewhere we can use it, as follows: </p>
			<ol>
				<li value="1">Create a file called <code>imu_settings.py</code>.</li>
				<li>We'll import the <code>vector</code> type, and then set our calibration readings. You probably only need to run this once, or if you change IMU device. Please use the readings you got from your robot. Run the following code:<pre>from vpython import vector
gyro_offsets = vector(-0.583969, 0.675573, -0.530534)</pre></li>
				<li>Next, we upgrade our <code>RobotImu</code> class to handle these offsets—open <code>robot_imu.py</code>.</li>
				<li>We will make our class <a id="_idIndexMarker1044"/>accept offsets if we pass them, or use zero if we leave them. Make the highlighted changes to the <code>__init__</code> method of <code>RobotImu</code>, as follows<a id="_idTextAnchor381"/>:<pre>    def __init__(self<strong class="bold">, gyro_offsets=None</strong>):
        self._imu = ICM20948()
<strong class="bold">        self.gyro_offsets = gyro_offsets or vector(0, 0, 0)</strong></pre></li>
				<li>We need to modify the <code>read_gyroscope</code> method to account for these too, as follows:<pre>    def read_gyroscope(self):
        _, _, _, gyro_x, gyro_y, gyro_z = self._imu.read_accelerometer_gyro_data()
        return vector(x, y, z)<strong class="bold"> - self.gyro_offsets</strong></pre></li>
			</ol>
			<p>Now, to see if this works, let's use it to move a virtual robot.</p>
			<h2 id="_idParaDest-420"><a id="_idTextAnchor382"/>Rotating the virtual robot with the gyroscope</h2>
			<p>We've mentioned <a id="_idIndexMarker1045"/>how we will integrate the <a id="_idIndexMarker1046"/>gyroscope measurements. Take a look at the following diagram to see how this will work for a single axis:</p>
			<div><div><img src="img/B15660_16_05.jpg" alt="" width="867" height="549"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.5 – Integrating a gyroscope axis</p>
			<p><em class="italic">Figure 16.5</em> shows a dashed circle, indicating a turning circle of an axis. The crosshair through the circle shows its center. A thick arrow above and to the left of the circle indicates the current <a id="_idIndexMarker1047"/>heading. A shaded area shows the <a id="_idIndexMarker1048"/>change in rotation in degrees over some time, which we add to the current heading to get to the new heading estimate—another thick arrow. </p>
			<p>We multiply the turning rate by time to get a movement; it is an estimate since we don't have intermediate values.</p>
			<p>The concept of time-since-last-measurement is an important one, seen in the PID in <a href="B15660_14_Final_ASB_ePub.xhtml#_idTextAnchor315"><em class="italic">Chapter 14</em></a>, <em class="italic">Line-Following with a Camera in Python</em>. It's <a id="_idIndexMarker1049"/>more commonly known as the delta time. </p>
			<p>We can combine what we know about the gyroscope with the virtual robot and make it rotate on the screen. Let's use this to rotate our virtual robot, as follows:</p>
			<ol>
				<li value="1">Create a new file named <code>visual_gyroscope.py</code>. We have many imports here to bring the components together, as can be seen in the following code snippet:<pre><strong class="bold">import vpython as vp</strong>
<strong class="bold">from robot_imu import RobotImu</strong>
<strong class="bold">import time</strong>
<strong class="bold">import imu_settings</strong>
<strong class="bold">import virtual_robot</strong></pre></li>
				<li>This time, when we set up the <code>RobotImu</code>, we will do so with the settings we made, as follows:<pre><strong class="bold">imu = RobotImu(gyro_offsets=imu_settings.gyro_offsets)</strong></pre></li>
				<li>We are going to integrate three axes: <code>pitch</code>, <code>roll</code>, and <code>yaw</code>. Let's start them at zero, like this:<pre><strong class="bold">pitch = 0</strong>
<strong class="bold">roll = 0</strong>
<strong class="bold">yaw = 0</strong></pre></li>
				<li>We should now <a id="_idIndexMarker1050"/>set up the virtual <a id="_idIndexMarker1051"/>robot and the view for it, as follows:<pre><strong class="bold">model = virtual_robot.make_robot()</strong>
<strong class="bold">virtual_robot.robot_view()</strong></pre></li>
				<li>We are going to be tracking delta time, so we start by taking the latest time, like this:<pre><strong class="bold">latest = time.time()</strong></pre></li>
				<li>We then start the main loop for this behavior. Since this is animating in VPython, we need to set the loop rate and tell it to update, as follows:<pre><strong class="bold">while True:</strong>
<strong class="bold">    vp.rate(1000)</strong></pre></li>
				<li>We now calculate the delta time (<code>dt</code>), storing a new latest time, as follows:<pre><strong class="bold">    current = time.time()</strong>
<strong class="bold">    dt = current - latest</strong>
<strong class="bold">    latest = current</strong></pre></li>
				<li>The code reads the gyroscope in the <code>gyro</code> vector, as follows:<pre><strong class="bold">    gyro = imu.read_gyroscope()</strong></pre></li>
				<li>We integrate the current rate (in degrees per second) multiplied by <code>dt</code> (in seconds), as illustrated in the following code snippet:<pre><strong class="bold">    roll += gyro.x * dt</strong>
<strong class="bold">    pitch += gyro.y * dt</strong>
<strong class="bold">    yaw += gyro.z * dt</strong></pre></li>
				<li>We reset the model's orientation to prepare it for rotation, like this:<pre><strong class="bold">    model.up = vp.vector(0, 1, 0)</strong>
<strong class="bold">    model.axis = vp.vector(1, 0, 0)</strong></pre></li>
				<li>We perform the <a id="_idIndexMarker1052"/>rotations by each axis. We <a id="_idIndexMarker1053"/>must convert these into radians, as follows:<pre><code>1</code>, <code>0</code>, <code>0</code>).</p></li>
				<li>This code is now ready to run; this will make the virtual robot change position when we rotate the robot in the real world! Upload the files and run with <code>vpython visual_gyroscope.py</code>.</li>
				<li>As before, wait a minute or so, and point your browser to <code>myrobot.local:9020</code>. You should see the following display:<div><img src="img/B15660_16_06.jpg" alt="" width="1650" height="862"/></div><p class="figure-caption">Figure 16.6 – The robot rotated</p><p><em class="italic">Figure 16.6</em> shows the <a id="_idIndexMarker1054"/>virtual robot rotated to an <a id="_idIndexMarker1055"/>angle by having moved the real robot. Move your robot around a bit—try to approximate what you see here.</p></li>
				<li>You'll notice that as you move the robot and return it, it won't line up correctly anymore—this is the accumulating errors or drift that gyroscope integration causes.</li>
			</ol>
			<p>From this experiment, while seeing some great movement you've also noticed that a gyroscope alone can't <a id="_idIndexMarker1056"/>accurately track rotations. We are going to <a id="_idIndexMarker1057"/>need to leverage the other sensors in the IMU device to improve this.</p>
			<p>Let's check it is working before proceeding.</p>
			<h3 id="_idParaDest-421">Troubleshooting</h3>
			<p>If it is not quite <a id="_idIndexMarker1058"/>working, try some of these steps:</p>
			<ol>
				<li value="1">This code requires the use of the <code>vpython</code> command and a browser to see the results.</li>
				<li>If the robot is still moving when held still, retry the calibration and offsets. The gyroscope's nature is that this won't be perfect—we'll fix that further on.</li>
				<li>If the robot appears to spin uncontrollably or jump around, ensure you've remembered to convert to radians.</li>
				<li>If the robot is rotating the wrong way (left/right instead of up/down), check the rotations' axis parameters.</li>
			</ol>
			<p>Now that you have this working, let's move on to the accelerometer so that we can see forces acting on our robot!</p>
			<h1 id="_idParaDest-422"><a id="_idTextAnchor383"/>Detecting pitch and roll with the accelerometer</h1>
			<p>In <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, we were getting <a id="_idIndexMarker1059"/>a vector from the accelerometer, but we need to calculate angles to consider <a id="_idIndexMarker1060"/>using it alongside the gyroscope and magnetometer. To use this to rotate things, we need to turn this vector into pitch-and-roll angles. </p>
			<h2 id="_idParaDest-423"><a id="_idTextAnchor384"/>Getting pitch and roll from the accelerometer vector</h2>
			<p>The <a id="_idIndexMarker1061"/>accelerometer describes what is <a id="_idIndexMarker1062"/>going on in <strong class="bold">Cartesian coordinates</strong>. We need to convert these into a pair of pitch-and-roll angles perpendicular to each other. In <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, the <em class="italic">Coordinate and rotation systems</em> section shows roll as taking place around the <em class="italic">x</em> axis, and pitch as taking place around the <em class="italic">y</em> axis. </p>
			<p>A crude but effective way to consider this is as two planes. When rotating around the <em class="italic">x</em> axis, you can take a vector in the <em class="italic">yz</em> plane and find its angle. When turning around the <em class="italic">y</em> axis, then you consider the <em class="italic">xz</em> plane instead. Take a look at the next diagram: </p>
			<div><div><img src="img/B15660_16_07.jpg" alt="" width="1650" height="932"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.7 – The accelerometer vector and angles</p>
			<p>In <em class="italic">Figure 16.7</em>, the <a id="_idIndexMarker1063"/>background has <em class="italic">x</em>, <em class="italic">y</em>, and <em class="italic">z</em> axes and a <a id="_idIndexMarker1064"/>sphere, with circles around the <em class="italic">xz</em> and <em class="italic">yz</em> planes. </p>
			<p>The accelerometer vector is shown as vector <strong class="bold">A</strong>. By using only the <em class="italic">xz</em> components, we project this vector onto an <em class="italic">xz</em> circle at point <strong class="bold">C</strong>; so, the angle from the <em class="italic">z</em> axis to <strong class="bold">C</strong> is the pitch. We project <strong class="bold">A</strong> again onto a <em class="italic">yz</em> circle at point <strong class="bold">B</strong>; this angle from the <em class="italic">z</em> axis to <strong class="bold">B</strong> is the roll.</p>
			<p>When we have two components (<em class="italic">x</em> and <em class="italic">z</em>, for example) on a plane, they can be used in the <code>atan2</code> function (present in most programming languages) to get an angle from them. A slight quirk here is that the orientation of the different sensor components means we must negate the pitch. The following diagram shows the process:</p>
			<div><div><img src="img/B15660_16_08.jpg" alt="" width="508" height="128"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.8 – Accelerometer data flow</p>
			<p><em class="italic">Figure 16.8</em> shows the raw accelerometer data going into the arctangent to get the angles and outputting the accelerometer pitch/roll values.</p>
			<p>Let's turn the <a id="_idIndexMarker1065"/>accelerometer readings into pitch <a id="_idIndexMarker1066"/>and roll, and then put them into a graph, as follows:</p>
			<ol>
				<li value="1">First, open up <code>robot_imu.py</code>.</li>
				<li>Extend the imports to include the trigonometric functions, as follows:<pre>from vpython import vector<strong class="bold">, degrees, atan2</strong></pre></li>
				<li>After the <code>read_accelerometer</code> method, add the following code to perform the required math:<pre>    def read_accelerometer_pitch_and_roll(self):
        accel = self.read_accelerometer()
        pitch = degrees(-atan2(accel.x, accel.z))
        roll = degrees(atan2(accel.y, accel.z))
        return pitch, roll</pre></li>
				<li>Let's show these angles on a graph, which will also reveal a major flaw with using the accelerometer on its own. Create a <code>plot_pitch_and_roll.py</code> file.</li>
				<li>Start with imports, as follows:<pre>import vpython as vp
import time
from robot_imu import RobotImu
imu = RobotImu()</pre></li>
				<li>We create the graphs, like this:<pre>vp.graph(xmin=0, xmax=60, scroll=True)
graph_pitch = vp.gcurve(color=vp.color.red)
graph_roll = vp.gcurve(color=vp.color.green)</pre></li>
				<li>Now, we <a id="_idIndexMarker1067"/>set up a start time so that we <a id="_idIndexMarker1068"/>can make a time-based graph, as follows:<pre>start = time.time()
while True:
    vp.rate(100)
    elapsed = time.time() - start</pre></li>
				<li>We can now get our new pitch-and-roll values, as follows:<pre>    pitch, roll = imu.read_accelerometer_pitch_and_roll()</pre></li>
				<li>And then, we can put both sets into graphs, like this:<pre>    graph_pitch.plot(elapsed, pitch)
    graph_roll.plot(elapsed, roll)</pre></li>
				<li>Upload both the <code>robot_imu.py</code> and <code>plot_pitch_and_roll.py</code> files. Run this with <code>vpython plot_accel_pitch_and_roll.py</code>, and point your browser at port <code>9020</code> on the robot. This should result in the following:<div><img src="img/B15660_16_09.jpg" alt="" width="1369" height="814"/></div></li>
			</ol>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 16.9 – Accelerometer pitch-and-roll graph</p>
			<p><em class="italic">Figure 16.9</em> shows a screenshot of <a id="_idIndexMarker1069"/>the graph. The red <a id="_idIndexMarker1070"/>curve in the graph represents pitch, around the <em class="italic">y</em> axis, while the green curve represents roll, around the <em class="italic">x</em> axis. Although it shows swings between +90 and -90 degrees, what is also clear is that the graph has a lot of noise, so much so that movements of less than a second are blotted out by it.</p>
			<p>We need to clean this up. A common way to do so is through a complementary filter, combining a new value with a previous value to filter out fast vibration noise. We will create such a filter, but it makes sampling slower.</p>
			<p>Let's check that this is working.</p>
			<h3 id="_idParaDest-424">Troubleshooting</h3>
			<p>If it's not quite <a id="_idIndexMarker1071"/>working, let's try a few fixes, as follows:</p>
			<ol>
				<li value="1">If it's very noisy, try a more severe turn, and try keeping your hands steady. This graph will be noisy due to the nature of the accelerometer alone.</li>
				<li>If you see the graph break up or misbehave outside of the 0-90-degree range, ensure you are using the <code>atan2</code> function—this mathematically performs the trigonometric CAST rule.</li>
				<li>Notice that the <code>read_accelerometer_pitch_and_roll</code> method has a negative sign in front of the <code>atan2</code> function.</li>
				<li>If things misbehave at 180 degrees—this is a known and expected problem with this system—try to avoid hitting this yet.</li>
			</ol>
			<p>Now, we have the pitch and roll, but it's quite rough—a suitable way to fix this is to combine sensors through a filter. We have another sensor that is giving us an integrated pitch-and-roll value: the gyroscope. </p>
			<h2 id="_idParaDest-425"><a id="_idTextAnchor385"/>Smoothing the accelerometer</h2>
			<p>We can combine what we <a id="_idIndexMarker1072"/>know about integrating the gyroscope with the accelerometer to make a smooth combination.</p>
			<p>Since we will use the delta-time concept more, a class to help will save us some work later.</p>
			<h3 id="_idParaDest-426">Delta time</h3>
			<p>We saw before how we <a id="_idIndexMarker1073"/>tracked the elapsed time for graphing and the delta time between updates for integrating. Let's create the code to help, as follows:</p>
			<ol>
				<li value="1">Make a <code>delta_timer.py</code> file and start by importing <code>time</code>, as follows:<pre><strong class="bold">import time</strong></pre></li>
				<li>We'll make a <code>DeltaTimer</code> class that will keep track of things, as follows:<pre><code>last</code> and <code>start</code> variables with the current time.</p></li>
				<li>The central part of this is an <code>update</code> method. Every loop calls this; let's start by getting the current time, as follows:<pre><strong class="bold">    def update(self):</strong>
<strong class="bold">        current = time.time()</strong></pre></li>
				<li>The delta time will be the difference between the current time and last time, as illustrated in the following code snippet:<pre><strong class="bold">        dt = current - self.last</strong></pre></li>
				<li>The elapsed time is the <a id="_idIndexMarker1074"/>difference between the current time and the start time, as illustrated in the following code snippet:<pre><strong class="bold">        elapsed = current - self.start</strong></pre></li>
				<li>We now need to update the last time for the delta time and return the parts, as follows:<pre><strong class="bold">        self.last = current</strong>
<strong class="bold">        return dt, elapsed</strong></pre></li>
			</ol>
			<p>We can use this class whenever we need a delta time and an elapsed time for graphing. Let's start by using it to combine the accelerometer and gyroscope.</p>
			<h2 id="_idParaDest-427"><a id="_idTextAnchor386"/>Fusing accelerometer and gyroscope data</h2>
			<p>By <a id="_idIndexMarker1075"/>combining the sensors, we can let each of them complement the other's weaknesses. The accelerometer acts as an absolute measurement for pitch and roll to counteract the drift seen by the gyroscope. The gyroscope does not experience the same noise as the accelerometer but can make fast measurements. Let's see how to combine them in the following diagram:</p>
			<div><div><img src="img/B15660_16_10.jpg" alt="" width="659" height="234"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.10 – Gyroscope and accelerometer fusion data flow</p>
			<p><em class="italic">Figure 16.10</em> shows the data flow diagram, using a complementary filter to fuse gyroscope and accelerometer data. We'll take pitch as an example. First, the system feeds gyroscope data and delta time into an integrator. The integrator adds this to a previous position. We can then use 95% of this term to account for larger movement changes. The filter's other 5% is the <a id="_idIndexMarker1076"/>accelerometer measurement. This 5% will drag the measurement to the average accelerometer reading while filtering out the chaotic noise element. The output is a filtered pitch or roll, fed back into the integrator for the next cycle.</p>
			<p>Let's put this into code, starting with the filter, as follows:</p>
			<ol>
				<li value="1">Open up <code>robot_imu.py</code>.</li>
				<li>Add the <code>ComplementaryFilter</code> class, as follows:<pre><strong class="bold">class ComplementaryFilter:</strong></pre></li>
				<li>We can construct this filter with the left side's value, storing this and calculating the complement (one minus the left side) to make the right side, as follows:<pre><strong class="bold">    def __init__(self, filter_left=0.9):</strong>
<strong class="bold">        self.filter_left = filter_left</strong>
<strong class="bold">        self.filter_right = 1.0 - filter_left</strong></pre></li>
				<li>This class has a <code>filter</code> method that takes the two sides and combines them using the filter values, as follows:<pre><strong class="bold">    def filter(self, left, right):</strong>
<strong class="bold">        return self.filter_left * left + \</strong>
<strong class="bold">               self.filter_right * right</strong></pre><p>That finishes the filter. </p></li>
				<li>The next thing we'll want is code to combine the IMU sensors via filters – to fuse them. We'll add a class for this to <code>robot_imu.py</code>, as follows:<pre><strong class="bold">class ImuFusion:</strong></pre></li>
				<li>In the constructor, we will store the <code>RobotImu</code> instance, create a filter, and seed the pitch-and-roll values, as follows:<pre><strong class="bold">    def __init__(self, imu, filter_value=0.95):</strong>
<strong class="bold">        self.imu = imu</strong>
<strong class="bold">        self.filter = ComplementaryFilter(filter_value).filter</strong>
<strong class="bold">        self.pitch = 0</strong>
<strong class="bold">        self.roll = 0</strong></pre></li>
				<li>The core <a id="_idIndexMarker1077"/>part of this code is an <code>update</code> function. The function takes a <code>dt</code> (delta time) parameter. It will not return anything and just updates the pitch/roll members, as follows:<pre><strong class="bold">    def update(self, dt):</strong></pre></li>
				<li>We start by taking the pitch-and-roll values from the accelerometer, as follows:<pre><strong class="bold">        accel_pitch, accel_roll = self.imu.read_accelerometer_pitch_and_roll()</strong></pre></li>
				<li>We also want the gyroscope values, so we run the following command:<pre><strong class="bold">        gyro = self.imu.read_gyroscope()</strong></pre></li>
				<li>Now, we combine the gyroscope <em class="italic">y</em> reading and accelerometer pitch to get the pitch value, as follows:<pre><strong class="bold">        self.pitch = self.filter(self.pitch + gyro.y * dt, accel_pitch)</strong></pre><p>Notice here the multiply and addition operations from the preceding data flow.</p></li>
				<li>We do the same for the roll, as follows:<pre><strong class="bold">        self.roll = self.filter(self.roll + gyro.x * dt, accel_roll)</strong></pre></li>
			</ol>
			<p>Now, we have prepared the <code>RobotImu</code> class with filters and fused the sensors. Let's give this code a test drive with a graph, as follows:</p>
			<ol>
				<li value="1">In the <code>plot_pitch_and_roll.py</code> file, we'll add the <code>DeltaTimer</code>, <code>ImuFusion</code>, and gyroscope calibration imports. Note in the following code snippet that <code>import time</code> has been removed:<pre>import vpython as vp
from robot_imu import RobotImu<strong class="bold">, ImuFusion</strong>
<strong class="bold">from delta_timer import DeltaTimer</strong>
<strong class="bold">import imu_settings</strong></pre></li>
				<li>Next, we set up the <code>RobotImu</code> with the gyroscope settings, and then create the <code>fusion</code> instance, as illustrated in the following code snippet:<pre>imu = RobotImu(<strong class="bold">gyro_offsets=imu_settings.gyro_offsets</strong>)
<strong class="bold">fusion = ImuFusion(imu)</strong></pre></li>
				<li>We need a <code>dt</code> (delta time) for <a id="_idIndexMarker1078"/>the fusion calculations and an elapsed time for the graph. The <code>DeltaTimer</code> class provides these. We put this close before the loop starts, replacing the assignment of <code>start</code>, as follows:<pre>timer = DeltaTimer()</pre></li>
				<li>Now, in the loop where we calculate <code>elapsed</code>, we use the delta timer, as follows:<pre>while True:
    vp.rate(100)
    <strong class="bold">dt, elapsed = timer.update()</strong></pre></li>
				<li>Now, replace the reading of the accelerometer with code to update the fusion with the delta time so that it makes its calculations, as follows:<pre><strong class="bold">fusion.update(dt)</strong></pre></li>
				<li>We can now fetch pitch-and-roll values from the <code>fusion</code> object, as follows:<pre>    graph_pitch.plot(elapsed, <strong class="bold">fusion.</strong>pitch)
    graph_roll.plot(elapsed, <strong class="bold">fusion.</strong>roll)</pre></li>
				<li>Upload <code>robot_imu.py</code>, <code>delta_timer.py</code> and <code>plot_pitch_and_roll.py</code> to the robot.</li>
				<li>Run <code>vpython plot_pitch_and_roll.py</code>, again and point your browser at port <code>9020</code> on the robot.</li>
			</ol>
			<p>Superficially, it should look similar to the accelerometer <a id="_idIndexMarker1079"/>pitch-and-roll graph in <em class="italic">Figure 16.9</em>. However, as you move the robot around, you should notice that there is far less noise—the graph is smoother—and that when you place the robot down or hold it still, it levels off. It should quickly account for rapid turns. The system is smooth and accurate!</p>
			<h3 id="_idParaDest-428">Troubleshooting</h3>
			<p>If you have issues, try these <a id="_idIndexMarker1080"/>troubleshooting checks:</p>
			<ol>
				<li value="1">As always, if you see syntax errors or strange behavior, check the code carefully.</li>
				<li>If things move strangely, ensure you are using <code>0.95</code> (and not <code>95</code>) for the filter value.</li>
				<li>Ensure you've uploaded all the files.</li>
				<li>This system will need a second or two after the graph starts to settle.</li>
			</ol>
			<p>You've now seen how to get an accurate and smooth pitch and roll from these sensors. A robot on wheels may not encounter many reasons to use pitch and roll, but one of them will be to make a compass work. Let's dig further into the magnetometer.</p>
			<h1 id="_idParaDest-429"><a id="_idTextAnchor387"/>Detecting a heading with the magnetometer</h1>
			<p>We saw in <a href="B15660_12_Final_ASB_ePub.xhtml#_idTextAnchor251"><em class="italic">Chapter 12</em></a>, <em class="italic">IMU Programming with Python</em>, how to plot a vector from the magnetometer, and <a id="_idIndexMarker1081"/>how magnetic metal (such as bits of <a id="_idIndexMarker1082"/>steel and iron) will interfere with it. Even the pin headers on the IMU board interfere. We can calibrate to compensate for this. </p>
			<p>Getting <em class="italic">X</em>, <em class="italic">Y</em>, and <em class="italic">Z</em> components aren't that useful; we want a heading relative to a magnetic North. We can see how to use this for precise turns.</p>
			<p>This section needs a space, with very few magnets present. Laptops, phones, speakers, and disk drives interfere with this sensor. Use a map compass to reveal magnetic fields in your space. I recommend making the standoff <em class="italic">stalk</em> on the robot as long as the cable allows, putting more standoffs in; the robot's motors have a strong magnetic field of their own.</p>
			<p>Please avoid starting with the robot facing South—this will cause some odd results, which we will investigate and fix later. Starting with the robot roughly North is a good idea.</p>
			<h2 id="_idParaDest-430"><a id="_idTextAnchor388"/>Calibrating the magnetometer</h2>
			<p>We are going to <a id="_idIndexMarker1083"/>perform a calibration known <a id="_idIndexMarker1084"/>as the <strong class="bold">hard iron offset calculation</strong>. Hard iron refers to any magnetic things near the magnetometer that move with it. We move the robot around to sample the field strength in each axis. We will use the middle of all readings for an axis to compensate, and add this to the IMU settings; this will seem similar to the gyroscope calibration but requires you to move the robot around.</p>
			<p>Let's write the code, as follows:</p>
			<ol>
				<li value="1">Create a file named <code>magnetometer_calibration.py</code>, starting with imports and the <code>RobotImu</code> setup, as follows:<pre><strong class="bold">import vpython as vp</strong>
<strong class="bold">from robot_imu import RobotImu</strong>
<strong class="bold">imu = RobotImu()</strong></pre></li>
				<li>We will find minimum and maximum vectors, as we did for the gyroscope, as illustrated in the following code snippet:<pre><strong class="bold">mag_min = vp.vector(0, 0, 0)</strong>
<strong class="bold">mag_max = vp.vector(0, 0, 0)</strong></pre></li>
				<li>We are going to show the system as a set of three scatter charts with colored-dot clusters. Each of the three clusters is a plot combining two axes: <em class="italic">xy</em>, <em class="italic">yz</em>, and <em class="italic">xz</em>. Our goal is to make the sets line up by calibrating the device, as follows:<pre><strong class="bold">scatter_xy = vp.gdots(color=vp.color.red)</strong>
<strong class="bold">scatter_yz = vp.gdots(color=vp.color.green)</strong>
<strong class="bold">scatter_zx = vp.gdots(color=vp.color.blue) </strong></pre></li>
				<li>We start the main loop and read the magnetometer, as follows:<pre><strong class="bold">while True:</strong>
<strong class="bold">    vp.rate(100)</strong>
<strong class="bold">    mag = imu.read_magnetometer()</strong></pre></li>
				<li>Now, we update <a id="_idIndexMarker1085"/>the minimums, in the same way we did for the gyroscope, as follows:<pre><strong class="bold">    mag_min.x = min(mag_min.x, mag.x)</strong>
<strong class="bold">    mag_min.y = min(mag_min.y, mag.y)</strong>
<strong class="bold">    mag_min.z = min(mag_min.z, mag.z)</strong></pre></li>
				<li>And then, we update the maximums, as follows:<pre><strong class="bold">    mag_max.x = max(mag_max.x, mag.x)</strong>
<strong class="bold">    mag_max.y = max(mag_max.y, mag.y)</strong>
<strong class="bold">    mag_max.z = max(mag_max.z, mag.z)</strong></pre></li>
				<li>We then calculate the offset in the same way as we did for the gyroscope, as follows:<pre><strong class="bold">    offset = (mag_max + mag_min) / 2</strong></pre></li>
				<li>This <code>print</code> statement shows the current values and offsets:<pre><strong class="bold">    print(f"Magnetometer: {mag}. Offsets: {offset}.")</strong></pre></li>
				<li>Now, we create the plots. They will guide you in getting enough calibration data and show where the axes do not line up. The code is shown in the following snippet:<pre><strong class="bold">    scatter_xy.plot(mag.x, mag.y)</strong>
<strong class="bold">    scatter_yz.plot(mag.y, mag.z)</strong>
<strong class="bold">    scatter_zx.plot(mag.z, mag.x)</strong></pre></li>
				<li>Upload this and <a id="_idIndexMarker1086"/>run it with VPython. You should see the following screen:<div><img src="img/B15660_16_11.jpg" alt="" width="1650" height="687"/></div><p class="figure-caption">Figure 16.11 – Initial magnetometer calibration screen</p><p><em class="italic">Figure 16.11</em> shows the clusters as three colored blobs—the bottom right is red, (for <em class="italic">xy</em>), the top is blue (for <em class="italic">yz</em>), and on the right is green (for <em class="italic">zx</em>). These clusters will start in a different position for you, depending on the orientation of your robot.</p></li>
				<li>You need to move the robot around, rotating it slowly around the whole <em class="italic">y</em> axis (around the wheels). The green graph should be more like an ellipse, as illustrated in the following screenshot:<div><img src="img/B15660_16_12.jpg" alt="" width="1525" height="679"/></div><p class="figure-caption">Figure 16.12 – The magnetometer partially calibrated</p><p><em class="italic">Figure 16.12</em> shows the <a id="_idIndexMarker1087"/>ellipse for the green values, and more data for the red and blue scatter plots. The slower you are, the better the data is.</p></li>
				<li>Rotate the robot around the <em class="italic">x</em> axis (the length of the robot), and then around the <em class="italic">z </em>axis (around its height). The more angles you move it through, the better. Fill in the gaps by making 3D figures of 8. Eventually, it should look like the graph in the following screenshot:<div><img src="img/B15660_16_13.jpg" alt="" width="1408" height="689"/></div><p class="figure-caption">Figure 16.13 – Magnetometer calibration: a good combination</p><p><em class="italic">Figure 16.13</em> shows how a good collection of data should look, with circles of red, green, and blue. Note that there are outliers due to waving the robot too close to other magnets—beware of these!</p></li>
				<li>You can close the <a id="_idIndexMarker1088"/>browser now, having collected a load of calibration data. </li>
				<li>The console will show the calibration offsets, as follows:<pre><strong class="bold">Magnetometer: &lt;30.6, -36.9, 10.35&gt;. Offsets: &lt;21.15, 3.15, 0.225&gt;.</strong>
<strong class="bold">Magnetometer: &lt;31.5, -36.75, 11.25&gt;. Offsets: &lt;21.15, 3.15, 0.225&gt;.</strong></pre><p>At the start, those offsets change a lot; however, as you collect more data, they will settle and stabilize, even when the magnetometer readings are changing.</p></li>
			</ol>
			<p>We now have some calibration numbers; my example gave <code>21.15, 3.15, 0.225</code>. Let's make sure that everyone has some values.</p>
			<h3 id="_idParaDest-431">Troubleshooting</h3>
			<p>This calibration may not <a id="_idIndexMarker1089"/>have worked—let's see why, as follows:</p>
			<ol>
				<li value="1">If the numbers don't appear to be settling, continue moving the robot. You must try to do full 360-degree movements with it to get a full range.</li>
				<li>If strange dots appear outside of the circle, move somewhere else and restart the calibration—this is likely to be a magnetic field where you are testing, and will throw your results off.</li>
				<li>There is a possibility your browser will get slow or run out of memory trying to do this—while I say move slowly, you cannot put this aside while running as it will continue adding dots.</li>
				<li>If you don't get circles at all—lines or small patches—double-check that you have the right plot combinations of <em class="italic">xy</em>, <em class="italic">yz</em>, and <em class="italic">zx</em>.</li>
			</ol>
			<p>You should now be getting calibration offsets. Let's use these values to line up the scatter plots.</p>
			<h3 id="_idParaDest-432">Testing the calibration values</h3>
			<p>To see if these are <a id="_idIndexMarker1090"/>effective we'll put them back into the code, and the same operation should show the dot clusters lining up. It starts by allowing us to set offsets in the <code>RobotImu</code> interface. Proceed as follows:</p>
			<ol>
				<li value="1">Open up the <code>robot_imu.py</code> file.</li>
				<li>In the <code>__init__</code> method, we need to store the offsets. I've highlighted the new code, as follows:<pre>  def __init__(self, gyro_offsets=None<strong class="bold">,</strong>
<strong class="bold">               mag_offsets=None</strong>):
      self._imu = ICM20948()
      self.gyro_offsets = gyro_offsets or vector(0, 0, 0)
<strong class="bold">      self.</strong> <strong class="bold">mag_offsets = mag_offsets or vector(0, 0, 0)</strong></pre></li>
				<li>The <code>read_magnetometer</code> method needs to subtract the magnetometer offsets, like this:<pre>    def read_magnetometer(self):
        mag_x, mag_y, mag_z = self._imu.read_magnetometer_data()
        return vector(mag_x, -mag_z, -mag_y)<strong class="bold"> - self.</strong> <strong class="bold">mag_offsets</strong></pre></li>
			</ol>
			<p>Our scripts can now include an offset for the magnetometer. We'll put these in the same settings file we used for the gyroscope calibrations. Proceed as follows:</p>
			<ol>
				<li value="1">Open <code>imu_settings.py</code>.</li>
				<li>Add the magnetometer calibration readings from your robot, as follows:<pre>from vpython import vector
gyro_offsets = vector(0.557252, -0.354962, -0.522901)
<strong class="bold">mag_offsets = vector(21.15, 3.15, 0.225)</strong></pre></li>
				<li>Now, we can <a id="_idIndexMarker1091"/>use them in our scatter plot. Open up <code>magnetometer_calibration.py</code> and add our settings to the imports, as follows:<pre>import vpython as vp
from robot_imu import RobotImu
<strong class="bold">from imu_settings import mag_offsets</strong></pre></li>
				<li>When we have created our <code>RobotImu</code> we can apply the offset, like this:<pre>imu = RobotImu(<strong class="bold">mag_offsets=mag_offsets</strong>)</pre></li>
				<li>Send the files to the robot, and rerun <code>magnetometer_calibration.py</code>. You'll need to make rotations and figures of 8 again to get many sample points at different orientations. When you have collected the data you should have overlapping circles, as depicted in the following screenshot:</li>
			</ol>
			<div><div><img src="img/B15660_16_14.jpg" alt="" width="1378" height="678"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.14 – Calibrated magnetometer</p>
			<p><em class="italic">Figure 16.14</em> shows the <a id="_idIndexMarker1092"/>red, blue, and green circles superimposed. Congratulations—you have calibrated your magnetometer!</p>
			<p>With your calibrated magnetometer, we can try further experiments with more useful values. But first, let's troubleshoot any problems.</p>
			<h3 id="_idParaDest-433">What to do if the circles aren't together</h3>
			<p>You may have reached <a id="_idIndexMarker1093"/>this point, and the circles are not converging. Try these troubleshooting steps if this is the case:</p>
			<ol>
				<li value="1">You will need to rerun the calibration code. Before you do so, comment out the line that applies (sets) the offsets on the <code>RobotImu</code> class. Running the calibration code when you have offsets active will cause it to offset incorrectly.</li>
				<li>Check your calibration and IMU code carefully for errors.</li>
				<li>Ensure there are no strong magnets or big chunks of metal near the robot—speakers or hard disks, for example. Try to do this about a meter away from such things. Even your laptop or mobile phone can interfere.</li>
				<li>Ensure you go through each axis slowly and try the figure of 8. Keep going until you can see three ellipses.</li>
				<li>You can use the console output, rotating the robot and moving around in every orientation, and then seeing if the offset values output here settle.</li>
				<li>When the outputs <a id="_idIndexMarker1094"/>settle, try applying the offset again, and run the calibration to see if the circles converge.</li>
			</ol>
			<p>After going through these, you should have the calibration values you need to continue. Let's put this back into the vector output we had and determine a heading.</p>
			<h1 id="_idParaDest-434"><a id="_idTextAnchor389"/>Getting a rough heading from the magnetometer</h1>
			<p>Now that we've got <a id="_idIndexMarker1095"/>calibration settings, we can <a id="_idIndexMarker1096"/>start using magnetometer readings to estimate where North is, like a compass. The words <em class="italic">heading</em> and <em class="italic">yaw</em> mean the same thing —which way we face relative to a reference point—in this case, magnetic North. Let's see how we can do this. Have a look at the following screenshot:</p>
			<div><div><img src="img/B15660_16_15.jpg" alt="" width="634" height="127"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.15 – Getting an approximate heading from the magnetometer</p>
			<p><em class="italic">Figure 16.15</em> shows a method we will build. It takes the magnetometer with calibration data applied and uses <code>atan2</code>, as we did with the gyroscope to approximate the heading. We can also add a rough compass with it too.</p>
			<p>Let's make this, as follows:</p>
			<ol>
				<li value="1">Create a <code>plot_mag_heading.py</code> file. Start with the imports, as follows:<pre><strong class="bold">import vpython as vp</strong>
<strong class="bold">from robot_imu import RobotImu</strong>
<strong class="bold">from delta_timer import DeltaTimer</strong>
<strong class="bold">import imu_settings</strong></pre></li>
				<li>We can initialize the <code>RobotImu</code> with the settings, like this:<pre><strong class="bold">imu = RobotImu(mag_offsets=imu_settings.mag_offsets)</strong></pre></li>
				<li>To make a compass display, we need a dial (cylinder) and needle (arrow) in red, as follows:<pre><strong class="bold">vp.cylinder(radius=1, axis=vp.vector(0, 0, 1), </strong>
<strong class="bold">            pos=vp.vector(0, 0, -1))</strong>
<strong class="bold">needle = vp.arrow(axis=vp.vector(1, 0, 0), </strong>
<strong class="bold">                  color=vp.color.red)</strong></pre></li>
				<li>Next, let's make a <a id="_idIndexMarker1097"/>graph to show the <a id="_idIndexMarker1098"/>heading, and a delta timer for elapsed time, as follows:<pre><strong class="bold">vp.graph(xmin=0, xmax=60, scroll=True)</strong>
<strong class="bold">graph_yaw = vp.gcurve(color=vp.color.blue)</strong>
<strong class="bold">timer = DeltaTimer()</strong></pre></li>
				<li>We start the main loop with a rate and fetch the elapsed time, as follows:<pre><strong class="bold">while True:</strong>
<strong class="bold">    vp.rate(100)</strong>
<strong class="bold">    dt, elapsed = timer.update()</strong></pre></li>
				<li>Now, we read the magnetometer by running the following command:<pre><strong class="bold">    mag = imu.read_magnetometer()</strong></pre></li>
				<li>We can take the <em class="italic">xy</em> plane and find the <code>atan2</code> function of these values of this to get a heading, as follows:<pre><strong class="bold">    yaw = -vp.atan2(mag.y, mag.x)</strong></pre></li>
				<li>Then, we plot this on the graph in degrees, like this:<pre><strong class="bold">    graph_yaw.plot(elapsed, vp.degrees(yaw))</strong></pre></li>
				<li>We can also set the <a id="_idIndexMarker1099"/>needle axis to our direction, using <code>sin</code>/<code>cos</code> to convert it back into a <a id="_idIndexMarker1100"/>unit direction, as follows:<pre><strong class="bold">    needle.axis = vp.vector(vp.sin(yaw), vp.cos(yaw), 0)</strong></pre></li>
				<li>Save, upload, and run this in VPython. Send your browser to port <code>9020</code> on the robot.</li>
				<li>If you rotate the robot around, you will see a display like this:</li>
			</ol>
			<div><div><img src="img/B15660_16_16.jpg" alt="" width="925" height="1147"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.16 – Magnetometer heading estimate</p>
			<p><em class="italic">Figure 16.16</em> shows a compass, with <a id="_idIndexMarker1101"/>the top being <a id="_idIndexMarker1102"/>what the robot perceives as North, and a red arrow. Below this is a blue graph, ranging between + and –180 degrees. As you move the robot, you will see this move, with 0 degrees being North. You will need the robot to be on a flat surface, though. </p>
			<p>Note that the compass is reading where North is relative to the robot—not where the robot is relative to North!</p>
			<p>This output is starting to appear reasonable. It can point North and make some compass measurements, and we have a heading.</p>
			<p>It is a little chaotic, and you can make it incorrect by any pitch or roll. Again, by fusing this data with data from the other sensors, we can improve this.</p>
			<h1 id="_idParaDest-435"><a id="_idTextAnchor390"/>Combining sensors for orientation</h1>
			<p>We've seen how <a id="_idIndexMarker1103"/>we combined the accelerometer and <a id="_idIndexMarker1104"/>gyroscope to get smooth readings for pitch and roll. We can combine the sensors again to correctly orient and smooth the magnetometer readings too. This system allows us to approximate the absolute orientation of the robot.</p>
			<p>Take a look at the following data flow to see what we are doing—it builds on the previous stages:</p>
			<div><div><img src="img/B15660_16_17.jpg" alt="" width="787" height="303"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.17 – Fusing all three sensors</p>
			<p><em class="italic">Figure 16.17</em> starts on the left with data from our previous stages. We have the filtered pitch and roll in gray because it's also an output. There's the calibrated gyroscope yaw, delta time, and also the calibrated magnetometer as inputs. The filtered pitch and roll go through the tilt-compensate box, where we rotate the magnetometer vector. The magnetometer data then goes through an <em class="italic">xy</em>-to-polar box, using the <code>atan2</code> function to get a heading.</p>
			<p>Above this, the calibrated gyroscope yaw and delta time go into an integrator, which adds to a previous yaw reading. The integrator and magnetometer heading output go into a complementary filter, with the integrator output being dominant. This filter output is then a heading/yaw output, which will be stable and quick to respond, and will return to the absolute heading. We now have three angles—pitch, roll, and yaw!</p>
			<p>Let's modify the code to do this, as follows;</p>
			<ol>
				<li value="1">Open up <code>robot_imu.py</code> and head to the <code>ImuFusion</code> class.</li>
				<li>We will need to convert back to radians, so we need to add this to the imports from VPython, as follows:<pre>from icm20948 import ICM20948
from vpython import vector, degrees, atan2<strong class="bold">, radians</strong></pre></li>
				<li>In <a id="_idIndexMarker1105"/>the <code>__init__</code> method, we <a id="_idIndexMarker1106"/>should add a variable to store the yaw in, as follows:<pre>    def __init__(self, imu, filter_value=0.95):
        self.imu = imu
        self.filter = ComplementaryFilter(filter_value).filter
        self.pitch = 0
        self.roll = 0
<strong class="bold">        self.yaw = 0</strong></pre><p>We are going to use the same filter for now.</p></li>
				<li>In the <code>update</code> method, after calculating the pitch and roll, add the following line to get the magnetometer reading:<pre>        mag = self.imu.read_magnetometer()</pre></li>
				<li>The <code>mag</code> variable is a vector. We rotate this using pitch and tilt to level the <em class="italic">xy</em> components, as follows:<pre>        mag = mag.rotate(radians(self.pitch), vector(0, 1, 0))
        mag = mag.rotate(radians(self.roll), vector(1, 0, 0))</pre></li>
				<li>We can now calculate the magnetometer yaw from this, as follows:<pre>        mag_yaw = -degrees(atan2(mag.y, mag.x))</pre></li>
				<li>To stabilize this, we can now use the complementary filter with the gyroscope, as follows:<pre>        self.yaw = self.filter(self.yaw + gyro.z * dt, mag_yaw)</pre></li>
			</ol>
			<p>The <code>self.yaw</code> value will now have the compensated yaw (or heading) value, allowing this IMU to act as a compass. To make use of it, let's visualize it in three ways—as a graph, a compass, and the movement of the robot. Proceed as follows:</p>
			<ol>
				<li value="1">Put this in a new file called <code>visual_fusion.py</code>. The code will be very familiar. Only the <a id="_idIndexMarker1107"/>magnetometer offsets and <a id="_idIndexMarker1108"/>yaw values are new. The imports are shown in the following code snippet:<pre><strong class="bold">import vpython as vp</strong>
<strong class="bold">from robot_imu import RobotImu, ImuFusion</strong>
<strong class="bold">from delta_timer import DeltaTimer</strong>
<strong class="bold">import imu_settings</strong>
<strong class="bold">import virtual_robot</strong></pre></li>
				<li>Prepare the <code>RobotImu</code> with magnetometer offsets, and initialize <code>fusion</code>, as follows:<pre><strong class="bold">imu = RobotImu(gyro_offsets=imu_settings.gyro_offsets,</strong>
<strong class="bold">               mag_offsets=imu_settings.mag_offsets)</strong>
<strong class="bold">fusion = ImuFusion(imu)</strong></pre></li>
				<li>We are going to use a VPython canvas for the virtual robot, and a separate one for the compass. Each canvas lets us contain a 3D scene. Let's make the current canvas a robot view and put it on the left. The robot model will be associated with this. The code is shown in the following snippet:<pre><strong class="bold">robot_view = vp.canvas(align="left")</strong>
<strong class="bold">model = virtual_robot.make_robot()</strong>
<strong class="bold">virtual_robot.robot_view()</strong></pre></li>
				<li>To accompany the robot view, we'll create a <code>compass</code> canvas, using the same cylinder and arrow as previously. Note that the most recent canvas is associated with the shapes created after it. The code is shown in the following snippet:<pre><strong class="bold">compass = vp.canvas(width=400, height=400)</strong>
<strong class="bold">vp.cylinder(radius=1, axis=vp.vector(0, 0, 1), </strong>
<strong class="bold">            pos=vp.vector(0, 0, -1))</strong>
<strong class="bold">needle = vp.arrow(axis=vp.vector(1, 0, 0), </strong>
<strong class="bold">                  color=vp.color.red)</strong></pre></li>
				<li>Set up graphs for pitch, roll, and yaw, as follows:<pre><strong class="bold">vp.graph(xmin=0, xmax=60, scroll=True)</strong>
<strong class="bold">graph_roll = vp.gcurve(color=vp.color.red)</strong>
<strong class="bold">graph_pitch = vp.gcurve(color=vp.color.green)</strong>
<strong class="bold">graph_yaw = vp.gcurve(color=vp.color.blue)</strong></pre></li>
				<li>Create a delta <a id="_idIndexMarker1109"/>timer, start the loop, and <a id="_idIndexMarker1110"/>fetch the time update, as follows:<pre><strong class="bold">timer = DeltaTimer()</strong>
<strong class="bold">while True:</strong>
<strong class="bold">    vp.rate(100)</strong>
<strong class="bold">    dt, elapsed = timer.update()</strong></pre></li>
				<li>We now update <code>fusion</code> with the time (it will read the IMU and perform calculations), as follows:<pre><strong class="bold">    fusion.update(dt)</strong></pre></li>
				<li>Now, we need to reset the virtual robot model before we rotate it, as follows:<pre><strong class="bold">    model.up = vp.vector(0, 1, 0)</strong>
<strong class="bold">    model.axis = vp.vector(1, 0, 0)</strong></pre></li>
				<li>And then, we need to perform three rotations—roll, pitch, and yaw, as follows:<pre><strong class="bold">    model.rotate(angle=vp.radians(fusion.roll), axis=vp.vector(1, 0, 0))</strong>
<strong class="bold">    model.rotate(angle=vp.radians(fusion.pitch), axis=vp.vector(0, 1, 0))</strong>
<strong class="bold">    model.rotate(angle=vp.radians(fusion.yaw), axis=vp.vector(0, 0, 1))</strong></pre></li>
				<li>We position the compass needle—note that our yaw is in degrees, so we convert it, as follows:<pre><strong class="bold">    needle.axis = vp.vector(</strong>
<strong class="bold">            vp.sin(vp.radians(fusion.yaw)), </strong>
<strong class="bold">            vp.cos(vp.radians(fusion.yaw)), </strong>
<strong class="bold">            0)</strong></pre></li>
				<li>Then, we plot the <a id="_idIndexMarker1111"/>three-graph axes, as <a id="_idIndexMarker1112"/>follows:<pre><strong class="bold">    graph_roll.plot(elapsed, fusion.roll)</strong>
<strong class="bold">    graph_pitch.plot(elapsed, fusion.pitch)</strong>
<strong class="bold">    graph_yaw.plot(elapsed, fusion.yaw)</strong></pre></li>
				<li>Upload <code>robot_imu.py</code> and <code>visual_fusion.py</code> to the robot. Start with <code>vpython visual_fusion.py</code> and point your browser at port <code>9020</code> on the robot.</li>
			</ol>
			<p>You should see the visual robot, compass, and a graph for all three axes displayed, and each should be both relatively stable and responsive, as depicted in the following screenshot: </p>
			<div><div><img src="img/B15660_16_18.jpg" alt="" width="1203" height="937"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.18 – Pitch, roll, and yaw graph</p>
			<p>The graph in <em class="italic">Figure 16.18</em> is a screenshot of the display. In the top left is the virtual robot—you <a id="_idIndexMarker1113"/>can change its view by <a id="_idIndexMarker1114"/>right-clicking. The top left shows the compass. Below that is a scrolling pitch, yaw, and roll graph. The roll is in red, pitch is in green, and yaw is in blue. The graphs will initially settle and then match your robot movements. When moving in one axis there is a small effect on the others, but they can move independently.</p>
			<p>At +/-180 degrees, the graph will misbehave though. Let's see how to fix that.</p>
			<h3 id="_idParaDest-436">Fixing the 180-degree problem</h3>
			<p>The fundamental thing to <a id="_idIndexMarker1115"/>realize is that angles on a circle are cyclical; 200 degrees and -160 degrees are equivalent, and -180 degrees and 180 degrees are also equal. We've not made the filter or code aware of this, so when we reach the 180-degree point and the <code>atan2</code> function is flipping between -179.988 and 179.991 (or some similar very close mark), the graph becomes chaotic, treating the difference of less than 1 degree as 360 degrees, and then trying to filter between them.</p>
			<p>This problem needs some changes to fix it. First, we can state that we intend angles to be numerically below 180 and above -180 and constrain them this way. Since we intend to use the complementary filter with angles, we can specialize it, as follows:</p>
			<ol>
				<li value="1">At the top of <code>robot_imu.py</code>, inside the <code>ComplementaryFilter</code> class, let's add a method to format the angle, like this:<pre>    @staticmethod
    def format_angle(angle):</pre></li>
				<li>If the angle is below -180, we want to wrap it around by adding <code>360</code>, as follows:<pre>        if angle &lt; -180:
            angle += 360</pre></li>
				<li>If the angle is above 180, we wrap it around by subtracting <code>360</code>, as follows:<pre>        if angle &gt; 180:
            angle -= 360
        return angle</pre></li>
				<li>We will replace the inside of <a id="_idIndexMarker1116"/>the <code>filter</code> function with something to constrain these angles more intelligently. When we filter, we start by formatting the incoming angles, as follows:<pre>    def filter(self, left, right):
<strong class="bold">        left = self.format_angle(left)</strong>
<strong class="bold">        right = self.format_angle(right)</strong></pre></li>
				<li>We also want to put the filtered angles in the same range. If there is a difference of more than 350 degrees, we can assume that something has wrapped around; so, we add 360 to the lowest one to filter them together, as follows:<pre><strong class="bold">        if left - right &gt; 350:</strong>
<strong class="bold">            right += 360</strong>
<strong class="bold">        elif right - left &gt; 350:</strong>
<strong class="bold">            left += 360</strong>
<strong class="bold">        filtered = </strong>self.filter_left * left + \
               self.filter_right * right</pre></li>
				<li>This operation could leave an answer outside of the range. So, we format it back, like this:<pre><strong class="bold">        return format_angle(filtered)</strong></pre></li>
				<li>This filter is in use already, so we can rerun <code>visual_fusion.py</code> and try turning back through 180 <a id="_idIndexMarker1117"/>degrees again. When you point your browser at the port, after settling, the robot there should be rotating with yours—and settling, not drifting!</li>
			</ol>
			<p>Note that this system still doesn't deal well with facing South when it starts. We've solved at least one problem with the system and smoothed out its flaws. </p>
			<p>This behavior is exciting: you can now get a robot on screen to mirror how you rotate it. However, while moving on the screen is fun, we'd like to see this used in the real world. Let's engage some motors!</p>
			<h1 id="_idParaDest-437"><a id="_idTextAnchor391"/>Driving a robot from IMU data</h1>
			<p>In previous <a id="_idIndexMarker1118"/>chapters, we saw how to use the PID algorithm, and in this <a id="_idIndexMarker1119"/>chapter, how to detect a pitch, roll, and yaw from a magnetometer. Our robot can't move its pitch or roll, but it can change its heading.</p>
			<p>In this demonstration, we'll get the robot to stay on course—to try to track North regardless of where we turn it. Let's see how. Have a look at the following diagram:</p>
			<div><div><img src="img/B15660_16_19.jpg" alt="" width="650" height="306"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.19 – Drive to heading behavior</p>
			<p><em class="italic">Figure 16.19</em> shows the flow of data. The left of the diagram starts with a measured heading, and a heading setpoint going into a PID—the error value will be the difference between the two. The <a id="_idIndexMarker1120"/>measured heading has come from the <strong class="bold">IMU + Fusion</strong> algorithm. We use <a id="_idIndexMarker1121"/>the PID output to drive the motors so that they move at a fixed speed plus or minus the value, so the robot will turn to reduce the error. The robot moving will feed back into the <strong class="bold">IMU + Fusion</strong> algorithm, looping through the PID.</p>
			<p>Let's take the preceding flow and use it to build the code, as follows: </p>
			<ol>
				<li value="1">Start a <code>drive_north_behavior.py</code> file with the following imports:<pre><strong class="bold">from robot_imu import RobotImu, ImuFusion</strong>
<strong class="bold">from delta_timer import DeltaTimer</strong>
<strong class="bold">from pid_controller import PIController</strong>
<strong class="bold">from robot import Robot</strong>
<strong class="bold">import imu_settings</strong></pre></li>
				<li>We now initialize the <code>RobotImu</code>, <code>fusion</code>, and the <code>DeltaTimer</code>, as follows:<pre><strong class="bold">imu = RobotImu(mag_offsets=imu_settings.mag_offsets,</strong>
<strong class="bold">               gyro_offsets=imu_settings.gyro_offsets)</strong>
<strong class="bold">fusion = ImuFusion(imu)</strong>
<strong class="bold">timer = DeltaTimer()</strong></pre></li>
				<li>We can then set up a PID (or PI) controller and the robot, as follows:<pre><strong class="bold">pid = PIController(0.7, 0.01)</strong>
<strong class="bold">robot = Robot()</strong></pre></li>
				<li>And then, a couple of constants—the robot's base speed, and the heading setpoint in degrees from North, as illustrated in th<a id="_idTextAnchor392"/>e following code snippet:<pre><strong class="bold">base_speed = 70</strong>
<strong class="bold">heading_set_point = 0</strong></pre></li>
				<li>The main loop here updates the timer and IMU fusion. Note in the following code snippet that there's not a visual rate here:<pre><strong class="bold">while True:</strong>
<strong class="bold">    dt, elapsed = timer.update()</strong>
<strong class="bold">    fusion.update(dt)</strong></pre></li>
				<li>We now calculate the <a id="_idIndexMarker1122"/>error, and feed the PID with that <a id="_idIndexMarker1123"/>and the delta time, as follows:<pre><strong class="bold">    heading_error = fusion.yaw - heading_set_point</strong>
<strong class="bold">    steer_value = pid.get_value(heading_error, delta_time=dt)</strong></pre></li>
				<li>We print the values to debug, and set our motor speeds, as follows:<pre><strong class="bold">    print(f"Error: {heading_error}, Value:{steer_value:2f}, t: {elapsed}")</strong>
<strong class="bold">    robot.set_left(base_speed + steer_value)</strong>
<strong class="bold">    robot.set_right(base_speed - steer_value)</strong></pre></li>
			</ol>
			<p>Upload this to the robot, turn on the motors, and run with regular Python 3. The robot will try to drive North. If you turn it off course it will correct back to North, and the more you turn it, the faster the motors will try to turn back. Playing with this behavior is quite fun!</p>
			<p>Press <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop this when you are done, and play with different heading set points.</p>
			<p>In this section, you've reinforced building data flow diagrams and writing code from them. You've further demonstrated that by converting sensor data to a number like this, you can build a PID-based behavior with it. You've then taken the heading that we've calculated and used it with the PID to create compass-based movement from your robot.</p>
			<h1 id="_idParaDest-438"><a id="_idTextAnchor393"/>Summary</h1>
			<p>In this chapter, you've seen how to combine the IMU sensors to approximate an absolute orientation in space. You've seen how to render these in graphs and how to display them onscreen with a virtual robot. You've then seen how to hook this sensor system up to a PID controller and motor to get the robot to drive.</p>
			<p>You've learned a little of the math needed to convert between vector components and angles, in 3D, along with how to use complementary filters to compensate for noise in one system and drift in another. You've started to see multiple sensors fused together to make inferences about the world. Your block diagram and data flow skills have been exercised, and you have had more practice with the PID algorithm.</p>
			<p>In the next chapter, we will look at how you can control your robot and choose behaviors from a menu with a smartphone.</p>
			<h1 id="_idParaDest-439"><a id="_idTextAnchor394"/>Exercises</h1>
			<p>Here are some ideas to further your understanding, and give you some ideas for more interesting things to do with the concepts from this chapter:</p>
			<ul>
				<li>A reader can use more colors and complicated shapes to make a better robot model. It's not the purpose of this chapter, but it is a fun and rewarding way to get more familiar with VPython.</li>
				<li>Our magnetometer settings were hardcoded, going into a Python file. It is good practice to load settings from a data file. A good starting point can be found at <a href="http://zetcode.com/python/yaml/">http://zetcode.com/python/yaml/</a>.</li>
				<li>Could the visual robot be used to display or debug the other sensors and integrations?</li>
				<li>Could you combine the absolute positioning here with the encoders to make a square with very accurate turns?</li>
			</ul>
			<h1 id="_idParaDest-440"><a id="_idTextAnchor395"/>Further reading</h1>
			<p>For more information on the topics covered in this chapter, refer to the following:</p>
			<ul>
				<li>The <strong class="bold">World Wide Web Consortium</strong> (<strong class="bold">W3C</strong>) has a guide on magnetometer devices in browsers, which makes for interesting reading on techniques, but also on how code on a smartphone might be able to perform these same algorithms to get the phone orientation: <a href="https://www.w3.org/TR/magnetometer">https://www.w3.org/TR/magnetometer</a>.</li>
				<li>I've mentioned the <code>atan2</code> function a lot; this page has further information on it: <a href="https://en.wikipedia.org/wiki/Atan2">https://en.wikipedia.org/wiki/Atan2</a>.</li>
				<li>I recommend Paul McWhorter's Arduino experiments with an IMU, and his introduction to VPython—his guide was an instrumental part in the research for this book: <a href="https://toptechboy.com/arduino-based-9-axis-inertial-measurement-unit-imu-based-on-bno055-sensor/">https://toptechboy.com/arduino-based-9-axis-inertial-measurement-unit-imu-based-on-bno055-sensor/</a>.</li>
				<li>This paper takes things a bit further and introduces a <strong class="bold">Global Positioning System</strong> (<strong class="bold">GPS</strong>) for further sensor fusion: <a href="https://www.researchgate.net/publication/51873462_Data_Fusion_Algorithms_for_Multiple_Inertial_Measurement_Units">https://www.researchgate.net/publication/51873462_Data_Fusion_Algorithms_for_Multiple_Inertial_Measurement_Units</a>.</li>
				<li>If you wish to dig deeper into sensor fusion, and algorithms to combine them while filtering errors, Kalman filters are the way to go. This article is a starting point: <a href="https://towardsdatascience.com/sensor-fusion-part-1-kalman-filter-basics-4692a653a74c">https://towardsdatascience.com/sensor-fusion-part-1-kalman-filter-basics-4692a653a74c</a>.</li>
			</ul>
		</div>
	</div></body></html>