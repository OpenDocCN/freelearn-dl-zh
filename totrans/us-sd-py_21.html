<html><head></head><body>
		<div><h1 id="_idParaDest-243" class="chapter-number"><a id="_idTextAnchor405"/>21</h1>
			<h1 id="_idParaDest-244"><a id="_idTextAnchor406"/>Diffusion Model Transfer Learning</h1>
			<p>This book is mainly focused on using Stable Diffusion with Python, and when doing so, we will need to fine-tune a model for our specific needs. As we discussed in previous chapters, there are many ways to customize the model, such as the following:</p>
			<ul>
				<li>Unlocking UNet to fine-tune all parameters</li>
				<li>Training a textual inversion to add new keyword embeddings</li>
				<li>Locking UNet and training a LoRA model for customized styles</li>
				<li>Training a ControlNet model to guide image generation with control guidance</li>
				<li>Training an adaptor to use the image as one of the guidance embeddings</li>
			</ul>
			<p>It is impossible to cover all the model training topics in simply one chapter. Another book would be needed to discuss the details of model training.</p>
			<p>Nevertheless, we still want to use this chapter to drill down to the core concepts of model training. Instead of listing sample code on how to fine-tune a diffusion model, or using the scripts from the <code>Diffusers</code> package, we want to introduce you to the core concepts of training so that you fully understand the common training process. In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introducing the foundations of training a model by training a linear model from scratch using PyTorch</li>
				<li>Introducing the Hugging Face Accelerate package to train a model in multiple GPUs</li>
				<li>Building code to train a Stable Diffusion V1.5 LoRA model using PyTorch and Accelerator step by step</li>
			</ul>
			<p>By the end of this chapter, you’ll be familiar with the overall training process and key concepts, and you’ll be able to read sample code from other repositories and build your own training code to customize a model from a pre-trained model.</p>
			<p>Writing code to train one model is the best way to learn how to train a model. Let’s start work on it.</p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor407"/>Technical requirements</h1>
			<p>Training a model requires more GPU power and VRAM than model inference. Prepare a GPU with at least 8 GB of VRAM – the more, the better. You can also train a model using multiple GPUs.</p>
			<p>It is recommended to install the latest version of the following packages:</p>
			<pre class="source-code">
pip install torch torchvision torchaudio
pip install bitsandbytes
pip install transformers
pip install accelerate
pip install diffusers
pip install peft
pip install datasets</pre>
			<p>Here are the specified packages with the versions I used to write the code samples:</p>
			<pre class="source-code">
pip install torch==2.1.2 torchvision==0.16.1 torchaudio==2.1.1
pip install bitsandbytes==0.41.0
pip install transformers==4.36.1
pip install accelerate==0.24.1
pip install diffusers==0.26.0
pip install peft==0.6.2
pip install datasets==2.16.0</pre>
			<p>The training code was tested in the Ubuntu 22.04 x64 version.</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor408"/>Training a neural network model with PyTorch</h1>
			<p>The <a id="_idIndexMarker608"/>target of this section is to build and train one simple neural network model using PyTorch. The model will be a simple one-layer model, with no additional fancy layers. It is simple but with all the elements required to train a Stable Diffusion LoRA, as we will see later in this chapter.</p>
			<p>Feel free to skip this section if you are familiar with PyTorch model training. If it is your first time to start training a model, this simple model training will help you thoroughly understand the process of model training.</p>
			<p>Before starting, make sure you have installed all the required packages mentioned in the <em class="italic">Technical </em><em class="italic">requirements</em> section.<a id="_idTextAnchor409"/></p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor410"/>Preparing the training data</h2>
			<p>Let’s<a id="_idIndexMarker609"/> assume we want to train a model with four weights and output one digital result show, as shown in the following:</p>
			<p>y = w 1 × x 1 + w 2 × x 2 + w 3 × x 3 + w 4 × x 4</p>
			<p>The four weights, w 1, w 2, w 3, w 4, are the model weights we want to have from the training data (Think of these weights as the Stable Diffusion model weight). Because we need to have some real data to train the model, I will use the weights <code>[2,3,4,7]</code> to generate some sample data:</p>
			<pre class="source-code">
import numpy as np
w_list = np.array([2,3,4,7])</pre>
			<p>Let’s create 10 groups of input sample data, <code>x_sample</code>; each <code>x_sample</code> is an array with four elements, the same length as the weight:</p>
			<pre class="source-code">
import random
x_list = []
for _ in range(10):
    x_sample = np.array([random.randint(1,100) for _ in range(
        len(w_list))])
    x_list.append(x_sample)</pre>
			<p>In the following section, we will use a neural network model to predict a list of weights; for the sake of training, let’s assume that the true weights are unknown after generating the training data.</p>
			<p>In the preceding code snippet, we utilize <code>numpy</code> to leverage its dot product operator, <code>@</code>, to compute the output, <code>y</code>. Now, let’s generate <code>y_list</code> containing <code>10</code> elements:</p>
			<pre class="source-code">
y_list = []
for x_sample in x_list:
    y_temp = x_sample@w_list
    y_list.append(y_temp)</pre>
			<p>You can print <code>x_list</code> and <code>y_list</code> to take a look at the training data.</p>
			<p>Our <a id="_idIndexMarker610"/>training data is ready; there’s no need to download anything else. Next, let’s define the model itself and prepare for trainin<a id="_idTextAnchor411"/>g.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor412"/>Preparing for training</h2>
			<p>Our model could<a id="_idIndexMarker611"/> be the world’s simplest model ever, a simple linear dot product, as defined in the following code:</p>
			<pre class="source-code">
import torch
import torch.nn as nn
class MyLinear(nn.Module):
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.randn(4))
    def forward(self, x:torch.Tensor):
        return self.w @ x</pre>
			<p>The <code>torch.randn(4)</code> code is to generate a tensor with a four-weight number. No other code is needed; our NN model is ready now, named <code>MyLinear</code>.</p>
			<p>To train a model, we will need to initialize it, similar to initializing random weights in an LLM or diffusion model:</p>
			<pre class="source-code">
model = MyLinear()</pre>
			<p>Almost all neural network model training follows these steps:</p>
			<ol>
				<li>Forward a pass to predict the result.</li>
				<li>Compute the difference between the predicted result and the ground truth, known as the loss value.</li>
				<li>Perform backpropagation to calculate the gradient loss value.</li>
				<li>Update the model parameters.</li>
			</ol>
			<p>Before<a id="_idIndexMarker612"/> kicking off the training, define a loss function and an optimizer. The loss function, <code>loss_fn</code>, will help calculate a loss value based on the predicted result and ground truth result. <code>optimizer</code> will be used to update the weights.</p>
			<pre class="source-code">
loss_fn = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.00001)</pre>
			<p><code>lr</code> represents the learning rate, a crucial hyperparameter to set. Determining the best <strong class="bold">learning rate</strong> (<strong class="bold">lr</strong>) often<a id="_idIndexMarker613"/> involves trial and error, depending on the characteristics of your model, dataset, and problem. To find a reasonable learning rate, you need to do the following:</p>
			<ul>
				<li><strong class="bold">Start with a small learning rate</strong>: A common practice is to start with a small learning rate, such as 0.001, and gradually increase or decrease it based on the observed convergence behavior.</li>
				<li><strong class="bold">Use learning rate schedules</strong>: You can use learning rate schedules to adjust the learning rate dynamically during <a id="_idIndexMarker614"/>training. One common approach is step decay, where the learning rate decreases after a fixed number of epochs. Another <a id="_idIndexMarker615"/>popular method is exponential decay, in which the learning rate decreases exponentially over time. (We won’t use it in the world’s simplest model.)</li>
			</ul>
			<p>Also, don’t forget to convert the input and output to the torch Tensor object:</p>
			<pre class="source-code">
x_input = torch.tensor(x_list, dtype=torch.float32)
y_output = torch.tensor(y_list, dtype=torch.float32)</pre>
			<p>All the preparations are done, so let’s start training a<a id="_idTextAnchor413"/> model.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor414"/>Training a model</h2>
			<p>We will set the<a id="_idIndexMarker616"/> epoch number to 100, which means looping through our training data 100 times:</p>
			<pre class="source-code">
# start train model
num_epochs = 100
for epoch in range(num_epochs):
    for i, x in enumerate(x_input):
        # forward
        y_pred = model(x)
        # calculate loss
        loss = loss_fn(y_pred,y_output[i])
        # zero out the cached parameter.
        optimizer.zero_grad()
        # backward
        loss.backward()
        # update paramters
        optimizer.step()
    if (epoch+1) % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 
            num_epochs, loss.item()))
print("train done")</pre>
			<p>Let’s break down the<a id="_idIndexMarker617"/> preceding code:</p>
			<ul>
				<li><code>y_pred = model(x)</code>: This line applies the model to the current input data sample, <code>x</code>, generating a prediction, <code>y_pred</code>.</li>
				<li><code>loss = loss_fn(y_pred,y_output[i])</code>: This line calculates the loss (also known as the error or cost) by comparing the predicted output, <code>y_pred</code>, with the actual output, <code>y_output[i]</code> , using a specified loss function, <code>loss_fn</code>.</li>
				<li><code>optimizer.zero_grad()</code>: This line resets the gradients calculated during the backward pass to zero. This is important because it prevents gradient values from carrying over between different samples.</li>
				<li><code>loss.backward()</code>: This line performs the backpropagation algorithm, computing gradients for all parameters with respect to the loss.</li>
				<li><code>optimizer.step()</code>: This line updates the model’s parameters based on the computed gradients and the chosen optimization method.</li>
			</ul>
			<p>Putting all the code together and running it, we will see the following output:</p>
			<pre class="source-code">
Epoch [10/100], Loss: 201.5572
Epoch [20/100], Loss: 10.8380
Epoch [30/100], Loss: 3.5255
Epoch [40/100], Loss: 1.7397
Epoch [50/100], Loss: 0.9160
Epoch [60/100], Loss: 0.4882
Epoch [70/100], Loss: 0.2607
Epoch [80/100], Loss: 0.1393
Epoch [90/100], Loss: 0.0745
Epoch [100/100], Loss: 0.0398
train done</pre>
			<p>The loss value converges quickly and approaches <code>0</code> after <code>100</code> epochs. Execute the following code to see the current weight:</p>
			<pre class="source-code">
model.w</pre>
			<p>You can see that<a id="_idIndexMarker618"/> the weights update as follows:</p>
			<pre class="source-code">
Parameter containing:
tensor([1.9761, 3.0063, 4.0219, 6.9869], requires_grad=True)</pre>
			<p>This is quite close to <code>[2,3,4,7]</code>! The model was successfully trained to find the right weight numbers.</p>
			<p>In the case of Stable Diffusion and multiple GPU training, we can get help from the Hugging Face Accelerate package [4]. Let’s start using <code>Accele<a id="_idTextAnchor415"/>rate</code> next.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor416"/>Training a model with Hugging Face’s Accelerate</h1>
			<p>Hugging Face’s <code>Accelerate</code> is a library that provides a high-level API over different PyTorch distributed <a id="_idIndexMarker619"/>frameworks, aiming to simplify the process of distributed and mixed-precision training. It is designed to keep changes to your training loop to a minimum and allow the same functions to work for any distributed setup. Let’s see what <code>Accelerate</code> can bring to<a id="_idTextAnchor417"/> the table.</p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor418"/>Applying Hugging Face’s Accelerate</h2>
			<p>Let’s apply <code>Accelerate</code> to our simple but working model. Accelerate is designed to be used together with PyTorch, so we don’t need to change too much code. Here are the steps to use <code>Accelerate</code> to<a id="_idIndexMarker620"/> train a model:</p>
			<ol>
				<li>Generate the default configuration file:<pre class="source-code">
from accelerate import utils</pre><pre class="source-code">
utils.write_basic_config()</pre></li>
				<li>Initialize an <code>Accelerate</code> instance, and send the model instance and data to the device managed by Accelerate:<pre class="source-code">
from accelerate import Accelerator</pre><pre class="source-code">
accelerator = Accelerator()</pre><pre class="source-code">
device = accelerator.device</pre><pre class="source-code">
x_input.to(device)</pre><pre class="source-code">
y_output.to(device)</pre><pre class="source-code">
model.to(device)</pre></li>
				<li>Replace <code>loss.backward</code> with <code>accelerator.backward(loss)</code> :<pre class="source-code">
# loss.backward</pre><pre class="source-code">
accelerator.backward(loss)</pre></li>
			</ol>
			<p>Next, we will update the training code using<a id="_idTextAnchor419"/> Accelerate.</p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor420"/>Putting code together</h2>
			<p>We will keep<a id="_idIndexMarker621"/> all the other code the same; here is the complete training code except for the data preparations and model initializing:</p>
			<pre class="source-code">
# start train model using Accelerate
from accelerate import utils
utils.write_basic_config()
from accelerate import Accelerator
accelerator = Accelerator()
device = accelerator.device
x_input.to(device)
y_output.to(device)
model.to(device)
model, optimizer = accelerator.prepare(
    model, optimizer
)
num_epochs = 100
for epoch in range(num_epochs):
    for i, x in enumerate(x_input):
        # forward
        y_pred = model(x)
        # calculate loss
        loss = loss_fn(y_pred,y_output[i])
        # zero out the cached parameter.
        optimizer.zero_grad()
        # backward
        #loss.backward()
        accelerator.backward(loss)
        # update paramters
        optimizer.step()
    if (epoch+1) % 10 == 0:
        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1,
            num_epochs, loss.item()))
print("train done")</pre>
			<p>Running the<a id="_idIndexMarker622"/> preceding code, we should get the same output as when we run the training model without the Hugging Face <code>Accelerate</code> library. And the loss value c<a id="_idTextAnchor421"/>onverges as well.</p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor422"/>Training a model with multiple GPUs using Accelerate</h2>
			<p>There are <a id="_idIndexMarker623"/>many types of multiple GPU training; in our case, we will use the data parallel style [1]. Simply put, we will load the whole model data into each GPU and split the training data across multiple GPUs.</p>
			<p>In PyTorch, we can achieve this with the following code:</p>
			<pre class="source-code">
import torch.nn as nn
from torch.nn.parallel import DistributedDataParallel
model = MyLinear()
ddp_model = DistributedDataParallel(model)
# Hugging Face Accelerate wraps this operation automatically using the prepare() function like this:
from accelerate import Accelerator
accelerator = Accelerator()
model = MyLinear()
model = accelerator.prepare(model)</pre>
			<p>For the world’s simplest model, we will load the whole model to each GPU and split the 10 groups’ training data into 5 groups each. Each GPU will take five groups of data at the same time. After each step, all loss gradient numbers will be merged using the <code>allreduce</code> operation. The <code>allreduce</code> operation simply added all the loss data from all GPUs, added it up, and then sent it back to each GPU to update the weights, as shown in the following Python code:</p>
			<pre class="source-code">
def allreduce(data):
    for i in range(1, len(data)):
        data[0][:] += data[i].to(data[0].device)
    for i in range(1, len(data)):
        data[i][:] = data[0].to(data[i].device)</pre>
			<p>Accelerate will launch two independent processes to train. To avoid creating two training <a id="_idIndexMarker624"/>datasets, let’s generate one dataset and save it to local storage using the <code>pickle</code> package:</p>
			<pre class="source-code">
import numpy as np
w_list = np.array([2,3,4,7])
import random
x_list = []
for _ in range(10):
    x_sample = np.array([random.randint(1,100)
        for _ in range(len(w_list))]
    )
    x_list.append(x_sample)
y_list = []
for x_sample in x_list:
    y_temp = x_sample@w_list
    y_list.append(y_temp)
train_obj = {
    'w_list':w_list.tolist(),
    'input':x_list,
    'output':y_list
}
import pickle
with open('train_data.pkl','wb') as f:
    pickle.dump(train_obj,f)</pre>
			<p>Then, wrap<a id="_idIndexMarker625"/> the whole model and training code in a <code>main</code> function and save it in a new Python file named <code>train_model_in_2gpus.py</code>:</p>
			<pre class="source-code">
import torch
import torch.nn as nn
from accelerate import utils
from accelerate import Accelerator
# start a accelerate instance
utils.write_basic_config()
accelerator = Accelerator()
device = accelerator.device
def main():
    # define the model
    class MyLinear(nn.Module):
        def __init__(self):
            super().__init__()
            self.w = nn.Parameter(torch.randn(len(w_list)))
        def forward(self, x:torch.Tensor):
            return self.w @ x
    # load training data
    import pickle
    with open("train_data.pkl",'rb') as f:
        loaded_object = pickle.load(f)
    w_list = loaded_object['w_list']
    x_list = loaded_object['input']
    y_list = loaded_object['output']
    # convert data to torch tensor
    x_input = torch.tensor(x_list, dtype=torch.float32).to(device)
    y_output = torch.tensor(y_list, dtype=torch.float32).to(device)
    # initialize model, loss function, and optimizer
    Model = MyLinear().to(device)
    loss_fn = nn.MSELoss()
    optimizer = torch.optim.SGD(model.parameters(), lr = 0.00001)
    # wrap model and optimizer using accelerate
    model, optimizer = accelerator.prepare(
        model, optimizer
    )
    num_epochs = 100
    for epoch in range(num_epochs):
        for i, x in enumerate(x_input):
            # forward
            y_pred = model(x)
            # calculate loss
            loss = loss_fn(y_pred,y_output[i])
            # zero out the cached parameter.
            optimizer.zero_grad()
            # backward
            #loss.backward()
            accelerator.backward(loss)
            # update paramters
            optimizer.step()
        if (epoch+1) % 10 == 0:
            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 
                num_epochs, loss.item()))
    # take a look at the model weights after trainning
    model = accelerator.unwrap_model(model)
    print(model.w)
if __name__ == "__main__":
    main()</pre>
			<p>Then, start the training using this command:</p>
			<pre class="source-code">
accelerate launch --num_processes=2 train_model_in_2gpus.py</pre>
			<p>You should see something like this:</p>
			<pre class="source-code">
Parameter containing:
tensor([1.9875, 3.0020, 4.0159, 6.9961], device='cuda:0', requires_grad=True)</pre>
			<p>If so, congratulations! You<a id="_idIndexMarker626"/> just successfully trained an AI model in two GPUs. With the knowledge you’ve learned, let’s now start to train a Stable Di<a id="_idTextAnchor423"/>ffusion V1.5 LoRA.</p>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor424"/>Training a Stable Diffusion V1.5 LoRA</h1>
			<p>The Hugging Face <a id="_idIndexMarker627"/>document provides complete guidance on training a LoRA by calling a pre-defined script [2] provided by Diffusers. However, we don’t want to stop at “using” the script. The training code from Diffusers includes a lot of edge-case handling and additional code that is hard to read and learn. In this section, we will write up each line of the training code to fully understand what happens in each step.</p>
			<p>In the following sample, we will use eight images with associated captions to train a LoRA. The image and image captions are provided in the <code>train_data</code> folder of the code for this chapter.</p>
			<p>Our training code structure will be like this:</p>
			<pre class="source-code">
# import packages
import torch
from accelerate import utils
from accelerate import Accelerator
from diffusers import DDPMScheduler,StableDiffusionPipeline
from peft import LoraConfig
from peft.utils import get_peft_model_state_dict
from datasets import load_dataset
from torchvision import transforms
import math
from diffusers.optimization import get_scheduler
from tqdm.auto import tqdm
import torch.nn.functional as F
from diffusers.utils import convert_state_dict_to_diffusers
# train code
def main():
    accelerator = Accelerator(
        gradient_accumulation_steps = gradient_accumulation_steps,
        mixed_precision = "fp16"
    )
    Device = accelerator.device
    ...
    # almost all training code will be land inside of this main function.
if __name__ == "__main__":
    main()</pre>
			<p>Right below the <code>main()</code> function, we initialize the <code>accelerate</code> instance. The <code>Accelerator</code> instance is initialized with two hyperparameters:</p>
			<ul>
				<li><code>gradient_accumulation_steps</code>: This is the number of training steps to accumulate <a id="_idIndexMarker628"/>gradients before we update the model parameters. Gradient accumulation allows you to effectively train with a larger batch size than would be possible with a single GPU, while still fitting the model parameters in memory.</li>
				<li><code>mixed_precision</code>: This specifies the precision to use during training. The <code>"fp16"</code> value means that half-precision floating point values will be used for the intermediate computations, which can lead to faster training times and lower memory usage.</li>
			</ul>
			<p>The <code>Accelerator</code> instance also has an attribute device, which is the device (GPU or CPU) on which the model will be trained. The device attribute can be used to move the model and tensors to the appropriate device before training.</p>
			<p>Now, let’s start defin<a id="_idTextAnchor425"/>ing hyperparameters.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor426"/>Defining training hyperparameters</h2>
			<p>Hyperparameters are <a id="_idIndexMarker629"/>parameters that are not learned from the data but, instead, are set before the commencement of the learning process. They are user-defined settings that govern the training process of a machine learning algorithm. In our LoRA training case, we will have the following settings:</p>
			<pre class="source-code">
# hyperparameters
output_dir = "."
pretrained_model_name_or_path   = "runwayml/stable-diffusion-v1-5"
lora_rank = 4
lora_alpha = 4
learning_rate = 1e-4
adam_beta1, adam_beta2 = 0.9, 0.999
adam_weight_decay = 1e-2
adam_epsilon = 1e-08
dataset_name = None
train_data_dir = "./train_data"
top_rows = 4
output_dir = "output_dir"
resolution = 768
center_crop = True
random_flip = True
train_batch_size = 4
gradient_accumulation_steps = 1
num_train_epochs = 200
# The scheduler type to use. Choose between ["linear", "cosine", # "cosine_with_restarts", "polynomial","constant", "constant_with_ 
# warmup"]
lr_scheduler_name = "constant" #"cosine"#
max_grad_norm = 1.0
diffusion_scheduler = DDPMScheduler</pre>
			<p>Let’s break down the preceding settings:</p>
			<ul>
				<li><code>output_dir</code>: This is the directory where the model outputs will be saved.</li>
				<li><code>pretrained_model_name_or_path</code>: This is the name or path of the pretrained model to be used as the starting point for training.</li>
				<li><code>lora_rank</code>: This is the<a id="_idIndexMarker630"/> number of layers in the <code>32</code> might not be effective enough, while ranks above <code>256</code> might be overkill for most tasks. In our case, since we use only eight images to train the LoRA, setting the rank to <code>4</code> is enough.</li>
				<li><code>lora_alpha</code>: This, conversely, controls the strength of the updates made to the pretrained model’s weights during fine-tuning. Specifically, the weight changes generated during fine-tuning are multiplied by a scaling factor equal to Alpha divided by Rank, before being added back to the original model weights. Therefore, increasing Alpha relative to Rank. Setting Alpha equal to Rank is a common starting practice.</li>
				<li><code>learning_rate</code>: This parameter controls how quickly the model learns from its mistakes during training. Specifically, it sets the step size for each iteration, determining how aggressively the model adjusts its parameters to minimize the <code>loss</code> function.</li>
				<li><code>adam_beta1</code> and <code>adam_beta2</code>: These are the parameters used in the Adam optimizer to control the decay rates of the moving averages of the gradient and squared gradient, respectively.</li>
				<li><code>adam_weight_decay</code>: This is the weight decay used in the Adam optimizer to prevent overfitting.</li>
				<li><code>adam_epsilon</code>: This is a<a id="_idIndexMarker632"/> small value added to the denominator for numerical stability in the Adam optimizer.</li>
				<li><code>dataset_name</code>: This is the name of the dataset to be used for training. Particularly, this is the Hugging Face dataset ID, such as <code>lambdalabs/pokemon-blip-captions</code>.</li>
				<li><code>train_data_dir</code>: This is the directory where the training data is stored.</li>
				<li><code>top_rows</code>: This is the number of rows used for training. It is used to select the top rows for training; if you have a dataset with 1,000 rows, set it to 8 to train the training code with the top 8 rows.</li>
				<li><code>output_dir</code>: This is the directory where the outputs will be saved during training.</li>
				<li><code>resolution</code>: This is the resolution of the input images.</li>
				<li><code>center_crop</code>: This is a Boolean flag indicating whether to perform center cropping on the input images.</li>
				<li><code>random_flip</code>: This is a Boolean flag indicating whether to perform random horizontal flipping on the input images.</li>
				<li><code>train_batch_size</code>: This is the batch size used during training.</li>
				<li><code>gradient_accumulation_steps</code>: This is the number of training steps to accumulate gradients before updating the model parameters.</li>
				<li><code>num_train_epochs</code>: This is the number of training epochs to perform.</li>
				<li><code>lr_scheduler_name</code>: This is the name of the learning rate scheduler to use.</li>
				<li><code>max_grad_norm</code>: This is the maximum norm of the gradients to clip to prevent exploding gradients.</li>
				<li><code>diffusion_scheduler</code>: This is<a id="_idIndexMarker633"/> the name of the di<a id="_idTextAnchor427"/>ffusion scheduler to use. </li>
			</ul>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor428"/>Preparing the Stable Diffusion components</h2>
			<p>When training a LoRA, the <a id="_idIndexMarker634"/>process involves inference, adding the loss value, and backpropagation - a procedure reminiscent of the inference process. To facilitate this, let’s use the <code>StableDiffusionPipeline</code> from <code>Diffusers</code> package to get <code>tokenizer</code>, <code>text_encoder</code>, <code>vae</code>, and <code>unet</code>:</p>
			<pre class="source-code">
noise_scheduler = DDPMScheduler.from_pretrained(
    pretrained_model_name_or_path, subfolder="scheduler")
weight_dtype = torch.float16
pipe = StableDiffusionPipeline.from_pretrained(
    pretrained_model_name_or_path,
    torch_dtype = weight_dtype
).to(device)
tokenizer, text_encoder = pipe.tokenizer, pipe.text_encoder
vae, unet = pipe.vae, pipe.unet</pre>
			<p>During LoRA training, those components will facilitate the forward pass, but their weights won’t be updated during backpropagation, so we need to set <code>requires_grad_</code> to <code>False</code>, as shown here:</p>
			<pre class="source-code">
# freeze parameters of models, we just want to train a LoRA only
unet.requires_grad_(False)
vae.requires_grad_(False)
text_encoder.requires_grad_(False)</pre>
			<p>The LoRA weights are the part we want to train; let’s use PEFT’s [3] <code>LoraConfig</code> to initialize the LoRA configurations.</p>
			<p><code>PEFT</code> is a<a id="_idIndexMarker635"/> library <a id="_idIndexMarker636"/>developed by Hugging Face that provides parameter-efficient ways to adapt large pre-trained models to specific downstream applications. The key idea behind PEFT is to fine-tune only a small fraction of a model’s parameters instead of fine-tuning all of them, resulting in significant savings in terms of computation and memory usage. This makes it possible to fine-tune very large models even on consumer hardware with limited resources.</p>
			<p>LoRA is one of the PEFT methods supported by the PEFT library. With LoRA, instead of updating all the weights of a given layer during fine-tuning, only a low-rank approximation of the weight updates is learned, reducing the number of additional parameters required per layer. This approach allows you to fine-tune just 0.16% of the total parameters of a model while achieving similar performance to full fine-tuning.</p>
			<p>To use LoRA with a pre-trained transformer model, you need to instantiate a <code>LoraConfig</code> object and pass it to the appropriate component of your model. The <code>LoraConfig</code> class has several attributes that control its behavior, including the dimension/rank of the decomposition, dropout rates, and other hyperparameters. Once configured, you can then train your model using standard techniques, such as gradient descent. Here is the code to create a LoRA configuration object:</p>
			<pre class="source-code">
# configure LoRA parameters use PEFT
unet_lora_config = LoraConfig(
    r = lora_rank,
    lora_alpha = lora_alpha,
    init_lora_weights = "gaussian",
    target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
)</pre>
			<p>Next, let’s add the LoRA adapter to the UNet model using the <code>unet_lora_config</code> configuration:</p>
			<pre class="source-code">
# Add adapter and make sure the trainable params are in float32.
unet.add_adapter(unet_lora_config)
for param in unet.parameters():
    # only upcast trainable parameters (LoRA) into fp32
    if param.requires_grad:
        param.data = param.to(torch.float32)</pre>
			<p>Inside the <code>for</code> loop, if <a id="_idIndexMarker637"/>the parameters require gradients (i.e., they are trainable), their data type is explicitly cast to <code>torch.float32</code>. This ensures that only the trainable parameters are in the <code>float32</code> f<a id="_idTextAnchor429"/>ormat for efficient training.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor430"/>Loading the training data</h2>
			<p>Let’s load up some<a id="_idIndexMarker638"/> data using the following code:</p>
			<pre class="source-code">
if dataset_name:
    # Downloading and loading a dataset from the hub. data will be 
    # saved to ~/.cache/huggingface/datasets by default
    dataset = load_dataset(dataset_name)
else:
    dataset = load_dataset(
        "imagefolder",
        data_dir = train_data_dir
    )
train_data = dataset["train"]
dataset["train"] = train_data.select(range(top_rows))
# Preprocessing the datasets. We need to tokenize inputs and targets.
dataset_columns = list(dataset["train"].features.keys())
image_column, caption_column = dataset_columns[0],dataset_columns[1]</pre>
			<p>Let’s break <a id="_idIndexMarker639"/>down the preceding code:</p>
			<ul>
				<li><code>if dataset_name:</code>: If <code>dataset_name</code> is provided, the code tries to load a dataset from Hugging Face’s dataset hub using the <code>load_dataset</code> function. If no <code>dataset_name</code> is provided, it assumes that the dataset is stored locally and loads it using the <code>imagefolder</code> dataset type.</li>
				<li><code>train_data = dataset["train"]</code>: The train split of the dataset is assigned to the <code>train_data</code> variable.</li>
				<li><code>dataset["train"] = train_data.select(range(top_rows))</code>: The first top rows of the train dataset are selected and assigned back to the train split of the dataset. This is useful when working with a small subset of the dataset for faster experimentation.</li>
				<li><code>dataset_columns = list(dataset["train"].features.keys())</code>: The keys of the <code>dataset["train"]</code> feature dictionary are extracted and assigned to the <code>dataset_columns</code> variable. These keys represent the image and caption columns in the dataset.</li>
				<li><code>image_column, caption_column = dataset_columns[0], dataset_columns[1]</code>: The first and second columns are assigned to the <code>image_column</code> and <code>caption_column</code> variables, respectively. This assumes that the dataset has exactly two columns – the first for images and the second for captions.</li>
			</ul>
			<p>We will need a<a id="_idIndexMarker640"/> function to convert the input text to token IDs; we define the function like this:</p>
			<pre class="source-code">
def tokenize_captions(examples, is_train=True):
    '''Preprocessing the datasets.We need to tokenize input captions and transform the images.'''
    captions = []
    for caption in examples[caption_column]:
        if isinstance(caption, str):
            captions.append(caption)
    inputs = tokenizer(
        captions,
        max_length = tokenizer.model_max_length,
        padding = "max_length",
        truncation = True,
        return_tensors = "pt"
    )
    return inputs.input_ids</pre>
			<p>And then, we<a id="_idIndexMarker641"/> train the data transform pipeline:</p>
			<pre class="source-code">
# Preprocessing the datasets.
train_transforms = transforms.Compose(
    [
        transforms.Resize(
            resolution,
            interpolation=transforms.InterpolationMode.BILINEAR
        ),
        transforms.CenterCrop(resolution) if center_crop else 
            transforms.RandomCrop(resolution),
        transforms.RandomHorizontalFlip() if random_flip else 
            transforms.Lambda(lambda x: x),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]) # [0,1] -&gt; [-1,1]
    ]
)</pre>
			<p>The preceding code defines a set of image transformations that will be applied to the training dataset during the training of a machine learning or deep learning model. These transformations are defined using the <code>transforms</code> module from the <code>PyTorch</code> library.</p>
			<p>Here’s a breakdown of what each line does:</p>
			<ul>
				<li><code>transforms.Compose()</code>: This is a function that “chains” multiple transformations together. It takes a list of transformation functions as input and applies them in order.</li>
				<li><code>transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR)</code>: This line resizes the image to the given resolution pixels while keeping the aspect ratio. The interpolation method used is bilinear interpolation.</li>
				<li><code>transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution)</code>: This line crops the image to a square of resolution x resolution. If <code>center_crop</code> is <code>True</code>, the crop is taken from the center of the image. If <code>center_crop</code> is <code>False</code>, the crop is taken randomly.</li>
				<li><code>transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda x: x)</code>: This line horizontally flips the image randomly <a id="_idIndexMarker642"/>with a probability of 0.5. If <code>random_flip</code> is <code>False</code>, it leaves the image unchanged.</li>
				<li><code>transforms.ToTensor()</code>: This line converts the image from a PIL image or NumPy array to a PyTorch tensor.</li>
				<li><code>transforms.Normalize([0.5], [0.5])</code>: This line scales the pixel values of the image between -1 and 1. It is commonly used to normalize image data before passing it to a neural network.</li>
			</ul>
			<p>By chaining these transformations together using <code>transforms.Compose</code>, you can easily preprocess your image data and apply multiple transformations to your dataset.</p>
			<p>We need the following code to use the chained transformation object:</p>
			<pre class="source-code">
def preprocess_train(examples):
    '''prepare the train data'''
    images = [image.convert("RGB") for image in examples[
        image_column]]
    examples["pixel_values"] = [train_transforms(image) 
        for image in images]
    examples["input_ids"] = tokenize_captions(examples)
    return examples
# only do this in the main process
with accelerator.main_process_first():
    # Set the training transforms
    train_dataset = dataset["train"].with_transform(preprocess_train)
def collate_fn(examples):
    pixel_values = torch.stack([example["pixel_values"] 
        for example in examples])
    pixel_values = pixel_values.to(memory_format = \
        torch.contiguous_format).float()
    input_ids = torch.stack([example["input_ids"] 
        for example in examples])
    return {"pixel_values": pixel_values, "input_ids": input_ids}
# DataLoaders creation:
train_dataloader = torch.utils.data.DataLoader(
    train_dataset,
    shuffle = True
    collate_fn = collate_fn
    batch_size = train_batch_size
)</pre>
			<p>The <a id="_idIndexMarker643"/>preceding code first defines a function called <code>preprocess_train</code>, which preprocesses the train data. It first converts the images to the RGB format, and then it applies a series of image transformations (resize, center/random crop, random horizontal flip, and normalization) to them using the <code>train_transforms</code> object. It then tokenizes the input captions using the <code>tokenize_captions</code> function. The resulting preprocessed data is added to the <code>examples</code> dictionary as the <code>pixel_values</code> and <code>input_ids</code> keys.</p>
			<p>The with <code>accelerator.main_process_first()</code> line is used to ensure that the code inside the block is executed only in the main process. In this case, it sets the training transforms for <code>train_dataset</code>.</p>
			<p>The <code>collate_fn</code> function is used to collate the dataset examples into a batch to be fed to the model. It takes a list of examples and stacks <code>pixel_values</code> and <code>input_ids</code> together. The resulting tensors are then converted to the <code>float32</code> format and returned as a dictionary.</p>
			<p>Finally, <code>train_dataloader</code> is created using the <code>torch.utils.data.DataLoader</code> class, which loads <code>train_dataset</code> with the specified batch size, shuffle, and collate functions.</p>
			<p>In PyTorch, DataLoader is a utility class that abstracts the process of loading data in batches for training or evaluation. It is used to load data in batches, which are sequences of data points used to train a machine learning model.</p>
			<p>In the provided code, <code>train_dataloader</code> is an instance of PyTorch’s <code>DataLoader</code> class. It is used to load the training data in batches. More specifically, it loads the data from <code>train_dataset</code> in batches of a predefined batch size, shuffles the data for each epoch, and applies a user-defined <code>collate_fn</code> function to preprocess the data before feeding it to the model.</p>
			<p><code>train_dataloader</code> is necessary for the efficient training of the model. By loading data in batches, it allows the model to process multiple data points in parallel, which can significantly reduce training time. Additionally, shuffling the data for each epoch helps prevent overfitting by ensuring that the model sees different data points in each epoch.</p>
			<p>In the provided code, the <code>collate_fn</code> function is used to preprocess the data before it is fed to the model. It takes a list of examples and returns a dictionary containing the pixel values and input IDs for each example. The <code>collate_fn</code> function is applied to each batch of data by <code>DataLoader</code> before it is fed to the model. This allows for more efficient <a id="_idIndexMarker644"/>processing of the data by applying the same prepro<a id="_idTextAnchor431"/>cessing steps to each batch of data.</p>
			<h2 id="_idParaDest-258"><a id="_idTextAnchor432"/>Defining the training components</h2>
			<p>To prepare and define the training components, let’s <a id="_idIndexMarker645"/>first initialize an <code>AdamW</code> optimizer. <code>AdamW</code> is an optimization algorithm to train machine learning models. It is a variant of the popular <code>Adam</code> optimizer, which uses adaptive learning rates for each model parameter. The <code>AdamW</code> optimizer is similar to the <code>Adam</code> optimizer, but it includes an additional weight decay term in the gradient update step. This weight decay term is added to the gradient of the loss function during optimization, which helps to prevent overfitting by adding a regularization term to the loss function.</p>
			<p>We can initialize an <code>AdamW</code> optimizer using the following code:</p>
			<pre class="source-code">
# initialize optimizer
lora_layers = filter(lambda p: p.requires_grad, unet.parameters())
optimizer = torch.optim.AdamW(
    lora_layers,
    lr = learning_rate,
    betas = (adam_beta1, adam_beta2),
    weight_decay = adam_weight_decay,
    eps = adam_epsilon
)</pre>
			<p>The <code>filter</code> function is used to iterate through all the parameters of the <code>unet</code> model and selects only those parameters that require gradient computation. The <code>filter</code> function returns a generator object that contains the parameters that require gradient computation. This generator object is assigned to the <code>lora_layers</code> variable, which will be used to optimize the model parameters during training.</p>
			<p>The <code>AdamW</code> optimizer is initialized with the following hyperparameters:</p>
			<ul>
				<li><code>lr</code>: The learning rate, which controls the step size at each iteration while moving toward a minimum of a loss function</li>
				<li><code>betas</code>: A tuple containing the exponential decay rates for the moving average of the gradient (β1) and the squared gradient (β2)</li>
				<li><code>weight_decay</code>: The weight decay term added to the gradient of the loss function during optimization</li>
				<li><code>eps</code>: A small value added to the denominator to improve numerical stability</li>
			</ul>
			<p>Second, we<a id="_idIndexMarker646"/> define a learning rate scheduler – <code>lr_scheduler</code>. Instead of defining one manually, we can use the <code>get_scheduler</code> function provided by the <code>Diffusers</code> package (<code>from diffusers.optimization </code><code>import get_scheduler</code>):</p>
			<pre class="source-code">
# learn rate scheduler from diffusers's get_scheduler
lr_scheduler = get_scheduler(
    lr_scheduler_name,
    optimizer = optimizer
)</pre>
			<p>This code creates a learning rate scheduler object using the <code>get_scheduler</code> function from the <code>Diffusers</code> library. The learning rate scheduler determines how the learning rate (i.e., the step size in gradient descent) changes during training.</p>
			<p>The <code>get_scheduler</code> function takes two arguments:</p>
			<ul>
				<li><code>lr_scheduler_name</code>: The name of the learning rate scheduler algorithm to use. In our sample, the name is <code>constant</code>, defined at the beginning of the code.</li>
				<li><code>optimizer</code>: The PyTorch optimizer object that the learning rate scheduler will be applied to. This is the <code>AdamW</code> optimizer we just initialized.</li>
			</ul>
			<p>We have just prepared all the elements to kick off training and we have written lots of code to prepare the dataset, although the actual training code isn’t that lon<a id="_idTextAnchor433"/>g. Let’s write the training code next.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor434"/>Training a Stable Diffusion V1.5 LoRA</h2>
			<p>Training a LoRA will usually <a id="_idIndexMarker647"/>take a while, and we’d better create a progress bar to track the training progress:</p>
			<pre class="source-code">
# set step count and progress bar
max_train_steps = num_train_epochs*len(train_dataloader)
progress_bar = tqdm(
    range(0, max_train_steps),
    initial = 0,
    desc = "Steps",
    # Only show the progress bar once on each machine.
    Disable = not accelerator.is_local_main_process,
)</pre>
			<p>Here is the core training code:</p>
			<pre class="source-code">
# start train
for epoch in range(num_train_epochs):
    unet.train()
    train_loss = 0.0
    for step, batch in enumerate(train_dataloader):
        # step 1. Convert images to latent space
        # latents = vae.encode(batch["pixel_values"].to(
            dtype=weight_dtype)).latent_dist.sample()
        latents = latents * vae.config.scaling_factor
        # step 2. Sample noise that we'll add to the latents, 
        latents provide the shape info.
        noise = torch.randn_like(latents)
        # step 3. Sample a random timestep for each image
        batch_size = latents.shape[0]
        timesteps = torch.randint(
            low = 0,
            high = noise_scheduler.config.num_train_timesteps,
            size = (batch_size,),
            device = latents.device
        )
        timesteps = timesteps.long()
        # step 4. Get the text embedding for conditioning
        encoder_hidden_states = text_encoder(batch["input_ids"])[0]
        # step 5. Add noise to the latents according to the noise 
        # magnitude at each timestep
        # (this is the forward diffusion process), 
        # provide to unet to get the prediction result
        noisy_latents = noise_scheduler.add_noise(
            latents, noise, timesteps)
        # step 6. Get the target for loss depend on the prediction 
        # type
        if noise_scheduler.config.prediction_type == "epsilon":
            target = noise
        elif noise_scheduler.config.prediction_type == "v_prediction":
            target = noise_scheduler.get_velocity(
                latents, noise, timesteps)
        else:
            raise ValueError(f"Unknown prediction type {
                noise_scheduler.config.prediction_type}")
        # step 7. Predict the noise residual and compute loss
        model_pred = unet(noisy_latents, timesteps, 
            encoder_hidden_states).sample
        # step 8. Calculate loss
        loss = F.mse_loss(model_pred.float(), target.float(), 
            reduction="mean")
        # step 9. Gather the losses across all processes for logging 
        # (if we use distributed training).
        avg_loss = accelerator.gather(loss.repeat(
            train_batch_size)).mean()
        train_loss += avg_loss.item() / gradient_accumulation_steps
        # step 10. Backpropagate
        accelerator.backward(loss)
        if accelerator.sync_gradients:
            params_to_clip = lora_layers
            accelerator.clip_grad_norm_(params_to_clip, max_grad_norm)
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        # step 11. check optimization step and update progress bar
        if accelerator.sync_gradients:
            progress_bar.update(1)
            train_loss = 0.0
        logs = {"epoch": epoch,"step_loss": loss.detach().item(), 
            "lr": lr_scheduler.get_last_lr()[0]}
        progress_bar.set_postfix(**logs)</pre>
			<p>The preceding code is a typical training loop for Stable Diffusion model training. Here’s a breakdown of what each part of the code does:</p>
			<ul>
				<li>The outer loop (<code>for epoch in range(num_train_epochs)</code>) iterates over the number of training epochs. An epoch is one complete pass through the entire training dataset.</li>
				<li><code>unet.train()</code> sets the model to training mode. This is important because some layers, such as dropout and batch normalization, behave differently during training and testing. In the<a id="_idIndexMarker648"/> training phase, these layers behave differently than in the evaluation phase. For example, dropout layers will drop out nodes with a certain probability during training to prevent overfitting, but they will not drop out any nodes during evaluation. Similarly, <code>BatchNorm</code> layers will use batch statistics during training, but will use accumulated statistics during evaluation. So, if you don’t call <code>unet.train()</code>, these layers will not behave correctly for the training phase, which could lead to incorrect results.</li>
				<li>The inner loop (<code>for step, batch in enumerate(train_dataloader)</code>) iterates over the training data. <code>train_dataloader</code> is a <code>DataLoader</code> object that provides batches of training data.</li>
				<li>In <em class="italic">step 1</em>, the model encodes the input images into a latent space using a <code>latents</code>), which are scaled by a factor.</li>
				<li>In <em class="italic">step 2</em>, random noise is added<a id="_idIndexMarker650"/> to the latent vectors. This noise is sampled from a standard normal distribution and has the same shape as the latent vectors.</li>
				<li>In <em class="italic">step 3</em>, random timesteps are sampled for each image in the batch. This is part of a time-dependent noise addition process.</li>
				<li>In <em class="italic">step 4</em>, the text encoder is used to get the text embedding for conditioning.</li>
				<li>In <em class="italic">step 5</em>, noise is added to the latent vectors according to the noise magnitude at each timestep.</li>
				<li>In <em class="italic">step 6</em>, the target for the loss calculation is determined based on the prediction type. It can be either the noise or the velocity of the noise.</li>
				<li>In <em class="italic">steps 7</em> and <em class="italic">8</em>, The model makes a prediction using the noisy latent vectors, the timesteps, and the text embeddings. The loss is then calculated as the mean squared error between the model’s prediction and the target.</li>
				<li>In <em class="italic">step 9</em>, the loss is gathered across all processes for logging. This is necessary in the case of distributed training, where the model is trained on multiple GPUs. So that we can see the loss value changes in the middle of a training process.</li>
				<li>In <em class="italic">step 10</em>, the gradients of the loss with respect to the model parameters are computed (<code>accelerator.backward(loss)</code>), and the gradients are clipped if necessary. This is to prevent the gradients from becoming too large, which can cause numerical instability. The optimizer updates the model parameters based on the gradients (<code>optimizer.step()</code>), and the learning rate scheduler updates the learning rate (<code>lr_scheduler.step()</code>). The gradients are then reset to zero (<code>optimizer.zero_grad()</code>).</li>
				<li>In <em class="italic">step 11</em>, if the gradients are synchronized, the training loss is reset to zero and the progress bar is updated.</li>
				<li>Finally, the training loss, learning rate, and current epoch are logged to monitor the training process. The progress bar is updated with these logs.</li>
			</ul>
			<p>Once you understand<a id="_idIndexMarker651"/> the preceding steps, you can not only train a Stable Diffusion LoRA but also train any other models.</p>
			<p>Lastly, we will need to save the LoRA we just trained:</p>
			<pre class="source-code">
# Save the lora layers
accelerator.wait_for_everyone()
if accelerator.is_main_process:
    unet = unet.to(torch.float32)
    unwrapped_unet = accelerator.unwrap_model(unet)
    unet_lora_state_dict = convert_state_dict_to_diffusers(
        get_peft_model_state_dict(unwrapped_unet))
    weight_name = f"""lora_{pretrained_model_name_or_path.split('/')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{diffusion_scheduler.__name__}_{formatted_date}.safetensors"""
    StableDiffusionPipeline.save_lora_weights(
        save_directory = output_dir,
        unet_lora_layers = unet_lora_state_dict,
        safe_serialization = True,
        weight_name = weight_name
    )
accelerator.end_training()</pre>
			<p>Let’s break down the preceding code:</p>
			<ul>
				<li><code>accelerator.wait_for_everyone()</code>: This line is used in distributed training to make sure all processes have reached this point in the code. It’s a synchronization point.</li>
				<li><code>if accelerator.is_main_process:</code>: This checks whether the current process is the main one. In distributed training, you typically only want to save the model once, not once for each process.</li>
				<li><code>unet = unet.to(torch.float32)</code>: This line converts the data type of the model’s weights to <code>float32</code>. This is typically done to save memory, as <code>float32</code> uses less memory than <code>float64</code> but still provides sufficient precision for most deep learning tasks.</li>
				<li><code>unwrapped_unet = accelerator.unwrap_model(unet)</code>: This unwraps the model<a id="_idIndexMarker652"/> from the accelerator, which is a wrapper used for distributed training.</li>
				<li><code>unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))</code>: This line gets the state dictionary of the model, which contains the weights of the model, and then converts it to a format suitable for Diffusers.</li>
				<li><code>weight_name = f"lora_{pretrained_model_name_or_path.split('/')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{diffusion_scheduler.__name__}_{formatted_date}.safetensors"</code>: This line creates a name for the file where the weights will be saved. The name includes various details about the training process.</li>
				<li><code>StableDiffusionPipeline.save_lora_weights(...)</code>: This line saves the weights of the model to a file. The <code>save_directory</code> argument specifies the directory where the file will be saved, <code>unet_lora_layers</code> is the state dictionary of the model, <code>safe_serialization</code> indicates that the weights should be saved in a way that is safe to load later, and <code>weight_name</code> is the name of the file.</li>
				<li><code>accelerator.end_training()</code>: This line signals the end of the training process. This is typically used to clean up resources used during training.</li>
			</ul>
			<p>We have the complete <a id="_idIndexMarker653"/>training code in the associated code folder for this chapter, named <code>train_sd16_lora.py</code>. We are not done yet; we still need to kick off the training using the <code>accelerator</code> command in<a id="_idTextAnchor435"/>stead of entering <code>python </code><code>py_file.py</code> directly.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor436"/>Kicking off the training</h2>
			<p>If you have one <a id="_idIndexMarker654"/>GPU, simply run the following command:</p>
			<pre class="source-code">
accelerate launch --num_processes=1 ./train_sd16_lora.py</pre>
			<p>For two GPUs, increase <code>--num_processes</code> to <code>2</code>, like this:</p>
			<pre class="source-code">
accelerate launch --num_processes=2 ./train_sd16_lora.py</pre>
			<p>If you have more than two GPUs and want to train on assigned GPUs (e.g., you have three GPUs and want the training code run on the second and third GPUs), use this command:</p>
			<pre class="source-code">
CUDA_VISIBLE_DEVICES=1,2 accelerate launch --num_processes=2 ./train_sd16_lora.py</pre>
			<p>To use the first and third GPUs, simply update the <code>CUDA_VISIBLE_DEVICES</code> settings to <code>0,2</code>, like this:</p>
			<pre class="source-code">
CUDA_VISIBLE_DEVICES=0,2 accelerate<a id="_idTextAnchor437"/> launch --num_processes=2 ./train_sd16_lora.py</pre>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor438"/>Verifying the result</h2>
			<p>This is the most exciting moment<a id="_idIndexMarker655"/> to witness the power of model training. First, let’s load up the LoRA but set its weight to <code>0.0</code> with <code>adapter_weights = [</code><code>0.0]</code>:</p>
			<pre class="source-code">
from diffusers import StableDiffusionPipeline
import torch
from diffusers.utils import make_image_grid
from diffusers import EulerDiscreteScheduler
lora_name = "lora_file_name.safetensors"
lora_model_path = f"./output_dir/{lora_name}"
device = "cuda:0"
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.bfloat16
).to(device)
pipe.load_lora_weights(
    pretrained_model_name_or_path_or_dict=lora_model_path,
    adapter_name = "az_lora"
)
prompt = "a toy bike. macro photo. 3d game asset"
nagtive_prompt = "low quality, blur, watermark, words, name"
pipe.set_adapters(
    ["az_lora"],
    adapter_weights = [0.0]
)
pipe.scheduler = EulerDiscreteScheduler.from_config(
    pipe.scheduler.config)
images = pipe(
    prompt = prompt,
    nagtive_prompt = nagtive_prompt,
    num_images_per_prompt = 4,
    generator = torch.Generator(device).manual_seed(12),
    width = 768,
    height = 768,
    guidance_scale = 8.5
).images
pipe.to("cpu")
torch.cuda.empty_cache()
make_image_grid(images, cols = 2, rows = 2)</pre>
			<p>Running the <a id="_idIndexMarker656"/>preceding code, we will get the images shown in <em class="italic">Figure 21</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21263_21_01.jpg" alt="Figure 21.1: A toy bike, a macro photo, a 3D game asset, and an image generated without using LoRA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 21.1: A toy bike, a macro photo, a 3D game asset, and an image generated without using LoRA</p>
			<p>The result is not <a id="_idIndexMarker657"/>that good. Now, let’s enable the trained LoRA with <code>adapter_weights = [1.0]</code>. Run the code again, and you should see the images shown in <em class="italic">Figure 21</em><em class="italic">.2</em>:</p>
			<div><div><img src="img/B21263_21_02.jpg" alt="Figure 21.2: A toy bike, a macro photo, a 3D game asset, and an image generated with LoRA training using eight images﻿"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 21.2: A toy bike, a macro photo, a 3D game asset, and an image generated with LoRA training using eight images</p>
			<p>The result is <a id="_idIndexMarker658"/>way better than the images without using the Lo<a id="_idTextAnchor439"/>RA! If you see similar results, congratulations!</p>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor440"/>Summary</h1>
			<p>This has been a long chapter, but learning about the power of model training is worth the length. Once we have mastered the training skill, we can train any models based on our needs. The whole training process isn’t easy, as there are so many details and trivial things to deal with. However, writing the training code is the only way to fully understand how model training works; considering the fruitful outcome, it is worth spending time to figure it out from the bottom up.</p>
			<p>Due to the length limitation of one chapter, I can only cover the entire LoRA training process, but once you succeed with LoRA training, you can find more training samples from Diffusers, change the code based on your specific needs, or simply write your training code, especially if you are working on a new model’s architecture.</p>
			<p>In this chapter, we began by training one simple model; the model itself isn’t that interesting, but it helped you to understand the core steps of model training using PyTorch. Then, we moved on to leverage the Accelerator package to train a model in multiple GPUs. Finally, we touched on the real Stable Diffusion model and trained a full-functioning LoRA, using simply eight images.</p>
			<p>In the next and final chapter, we’ll discuss something less technical, AI, and its relationship with us, privacy, and how t<a id="_idTextAnchor441"/>o keep pace with its fast-changing advancements.</p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor442"/>References</h1>
			<ol>
				<li>What is <strong class="bold">Distributed Data Parallel</strong> (<strong class="bold">DDP</strong>): <a href="https://pytorch.org/tutorials/beginner/ddp_series_theory.html">https://pytorch.org/tutorials/beginner/ddp_series_theory.html</a></li>
				<li>Launch the LoRA training script: <a href="https://huggingface.co/docs/diffusers/en/training/lora#launch-the-script&#13;">https://huggingface.co/docs/diffusers/en/training/lora#launch-the-script</a></li>
				<li>Hugging Face PEFT: <a href="https://huggingface.co/docs/peft/en/index">https://huggingface.co/docs/peft/en/index</a></li>
				<li>Hugging Face Accelerate: <a href="https://huggingface.co/docs/accelerate/en/index&#13;">https://huggingface.co/docs/accelerate/en/index</a></li>
			</ol>
		</div>
	</body></html>