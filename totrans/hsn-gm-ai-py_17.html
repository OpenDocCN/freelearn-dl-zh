<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">From DRL to AGI</h1>
                </header>
            
            <article>
                
<p class="mce-root">Our journey through this book has been an exploration of the evolution of reinforcement and <strong>deep reinforcement learning</strong> (<strong>DRL</strong>). We have looked at many methods that you can use to solve a variety of problems in a variety of environments, but in general, we have stuck to a single environment; however, the true goal of DRL is to be able to build an agent that can learn across many different environments, an agent that can generalize its knowledge across tasks, much like we animals do. That type of agent, the type that can generalize across multiple tasks without human intervention, is known as an artificial general intelligence, or AGI. This field is currently exploding in growth for a variety of reasons and will be our focus in this final chapter.</p>
<p>In this chapter, we will look at how DRL builds the AGI agent. We will first look at the concept of meta learning, or learning to learn. Then we will learn how meta learning can be applied to reinforcement learning, looking at an example of model-agnostic meta learning as applied to DRL. Moving past the meta, we move on to hindsight experience replay, a method of using trajectory hindsight in order to improve learning across tasks. Next, we will move on to <strong>generative adversarial imitation learning</strong> (<span><strong>GAIL</strong>) </span>and see how this is implemented. We will finish the chapter with a new concept that is being applied to DRL known as imagination and reasoning.  </p>
<p>Here is a brief summary of the topics we will cover in this chapter:</p>
<ul>
<li>Learning meta learning</li>
<li style="font-weight: 400">Introducing meta reinforcement learning</li>
<li style="font-weight: 400">Using hindsight experience replay</li>
<li>Imagination and reasoning</li>
<li style="font-weight: 400">Understanding imagination-augmented agents</li>
</ul>
<p>In this last chapter, we will cover a wide variety of complex examples quickly. Each section of this chapter could warrant an entire chapter or book on its own. If any of this material piques your interest, be sure to do further research online; some areas may or may not have developed since this material was written. In the next section, we will look at ML and MRL.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning meta learning</h1>
                </header>
            
            <article>
                
<p>The word "meta" is defined as "referring to itself or its type or genre". When talking about meta learning, we are talking about understanding the learning process of learning—that is, instead of thinking about how an agent learns a task, we want to think about how an agent could learn to learn across tasks. It is both an interesting and yet abstract problem, so we first want to explore what meta learning is. In the next section, we will explore how machine learning can learn to learn.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning 2 learn</h1>
                </header>
            
            <article>
                
<p>Any good learning model should be trained across a variety of tasks and then generalized to fit the best distribution of those tasks. While we have covered very little with respect to general machine learning, consider the simple image classification problem with a deep learning model. We would typically train such a model with one goal or task, perhaps to identify whether a dataset contains a cat or dog, but not both, and nothing else. With meta learning, the cat/dog dataset would be one training entry in a set of image classification tasks that could cover a broad range of tasks, from recognizing flowers to cars. The following example images demonstrate this concept further:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-709 image-border" src="assets/41ab7d2e-ccde-4a1b-902e-263ba4a1f743.png" style="width:49.08em;height:25.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Example of meta learning applied to image classification (Image source Google)</span></div>
<p>The concept, then, involves training the model to classify the tasks of identifying a mouse or moose and cat or wolverine, and then, with meta learning, apply that to the task of identifying a tiger or bear. Essentially, we train the model in the same way that we test the model recursively—iteratively using collections of few-shot samples to expose to the network in learning sets or mini batches. You will have heard the term few-shot learning used in the context of meta learning to describe the small number of samples used for each task that are exposed to the model as a way of generalizing learning. The way that the model is updated through this process has been classified into the following three current schools of thought:</p>
<ul>
<li><strong>Metric based</strong>: Solutions are so named because they depend on training a metric to gauge and monitor performance. This often requires the learner to learn the kernel that defines the distribution the network is trying to model rather than tune the network explicitly. What we find is that adversarial learning, using two somewhat opposing networks, can balance and refine this learning to learn the metric in a form of encoding or embedding. Some great examples of this type of approach are convolutional siamese networks for few-shot learning, matching networks, full-context embeddings, relational networks, and prototypical networks.</li>
<li class="mce-root"><strong>Model based</strong><span>: Solutions define a group of methods that rely on some form of memory augmentation or context. Memory-augmented neural networks, or MANNs, are the primary implementation you will find using this solution. This concept is further based on a neural Turing machine (NTM), which describes a controller network that learns to read and write from memory-soft attention. An example of how this looks is taken from the following NTM architecture diagram:</span></li>
</ul>
<div style="color: black" class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/1cb35d13-5976-48a3-add9-d2cbe0caa944.png" style="width:23.67em;height:16.50em;"/></div>
<div style="color: black" class="CDPAlignCenter CDPAlign packt_figref">Example of a neural Turing machine</div>
<ul>
<li>The NTM architecture is used to power the MANN model for meta learning. Training a MANN requires some attention to the details of how the model memorizes and encodes the tasks. This model is often trained in such a way as to lengthen the amount of time that it takes for disparate training data to be reintroduced and remembered. Essentially, the agent is trained longer and longer for individual tasks, and then forced to recall memory of prelearned tasks later in further training. Interestingly enough, this is a method we humans will often use to focus on learning specific complex tasks. Then, we retest this knowledge later in order to reinforce this knowledge in memory. This same concept very much applies to MANN, and many believe that NTM or memory is a key ingredient to any meta learning pattern.</li>
<li><strong>Optimization based</strong>: Solutions are as much a combination of the two previous solutions as it is an antipattern. In optimization-based problems, we consider the root of the problem, and therefore the problem of optimizing our function, using not only gradient descent but also introducing gradient descent through context or time. Gradient descent through context or time is also known as <strong>backpropagation through time</strong> (<strong>BPTT</strong>), and is something we briefly touched on when we looked at recurrent networks. By introducing recurrent networks or <strong>long short-term memory</strong> (<strong>LSTM</strong>) layers into a network, we encourage the network to remember gradient context. Another way to think of this is that the network learns the history of the gradients it applied during training. The meta learner therefore gets trained using the process shown in the following diagram:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5ced2008-0997-412b-ab93-d79c698f80bb.png" style="width:42.42em;height:22.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Training the meta learned with LSTM </div>
<p>The diagram was sourced from the paper <em>Optimization as a Model for Few-Shot Learning</em>, by Sachin Ravi and Hugo Larochelle, and it is a much-simplified version of the original. In the diagram, we can see how the meta learner is trained outside the regular model, often in an outside loop, where the inside loop is defined as the training process of the individual classification, regression, or other forms of learning-based task. </p>
<p> While there are three different forms of meta learning, we will pay particular attention to the optimization form, and in particular, a method that works by assuming an agnostic model, which we will explore in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model-agnostic meta learning</h1>
                </header>
            
            <article>
                
<p><strong>Model-agnostic meta learning</strong> (<strong>MAML</strong>) is described as a general optimization method that will work with any machine learning method that uses gradient descent for optimization or learning. The intuition here is that we want to find a loss approximation that best matches the task we are currently undertaking. MAML does this by adding context through our model training tasks. That context is used to refine the model training parameters and thereby allow our model to better apply gradient loss for a specific task.  </p>
<div class="packt_infobox">This example uses the MNIST dataset, a set of 60,000 handwritten digits that is commonly used for base image classification tasks. While the dataset has been solved with high accuracy using a number of methods, it is often the base comparison for image classification tasks.</div>
<p class="mce-root"><span>This will likely still sound abstract, so in the next exercise, we pull down a PyTorch ML framework called <kbd>learn2learn</kbd> and show how MAML can be used:</span></p>
<ol>
<li>We will first create a new virtual environment and then download a package called <kbd>learn2learn</kbd>, a meta learning framework that provides a great implementation of MAML in PyTorch. Make sure that you create a new environment and install PyTorch and the Gym environment, as we previously did. You can install <kbd>learn2learn</kbd> with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install learn2learn  # after installing new environment with torch</strong><br/><br/><strong>pip install tqdm # used for displaying progress</strong></pre>
<ol start="2">
<li>In order to see how <kbd>learn2learn</kbd> is used in a basic task, we are going to review the basic MNIST training sample found in the repository, but we won't look at every section of the code example that has been provided in the source code <kbd>Chapter_14_learn.py</kbd>. Open the sample up and review the top section of the code, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">import learn2learn as l2l<br/>class Net(nn.Module):<br/>    def __init__(self, ways=3):<br/>        super(Net, self).__init__()<br/>        self.conv1 = nn.Conv2d(1, 20, 5, 1)<br/>        self.conv2 = nn.Conv2d(20, 50, 5, 1)<br/>        self.fc1 = nn.Linear(4 * 4 * 50, 500)<br/>        self.fc2 = nn.Linear(500, ways)<br/><br/>    def forward(self, x):<br/>        x = F.relu(self.conv1(x))<br/>        x = F.max_pool2d(x, 2, 2)<br/>        x = F.relu(self.conv2(x))<br/>        x = F.max_pool2d(x, 2, 2)<br/>        x = x.view(-1, 4 * 4 * 50)<br/>        x = F.relu(self.fc1(x))<br/>        x = self.fc2(x)<br/>        return F.log_softmax(x, dim=1)</pre>
<ol start="3">
<li>This top section of code shows the <kbd>learn2learn</kbd> <kbd>import</kbd> statement and the definition of the <kbd>Net</kbd> class. This is the network model we will be training. Note how the model is composed of two convolutional/pooling layers followed by a fully connected linear layer to an output layer. Note the use of <kbd>ways</kbd> as an input variable that defines the number of outputs from the last output layer.</li>
<li>Next, we will scroll down to the <kbd>main</kbd> function. This is where all the main setup and initialization occurs. This sample being more robust than most, it provides for input parameters that you can use instead of altering the hyperparameters in the code. The top of the <kbd>main</kbd> function is shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def main(lr=0.005, maml_lr=0.01, iterations=1000, ways=5, shots=1, tps=32, fas=5, device=torch.device("cpu"),<br/>         download_location="/tmp/mnist"):<br/>    transformations = transforms.Compose([<br/>        transforms.ToTensor(),<br/>        transforms.Normalize((0.1307,), (0.3081,)),<br/>        lambda x: x.view(1, 1, 28, 28),<br/>    ])<br/><br/>    mnist_train = l2l.data.MetaDataset(MNIST(download_location, train=True, download=True, transform=transformations))<br/>    # mnist_test = MNIST(file_location, train=False, download=True, transform=transformations)<br/><br/>    train_gen = l2l.data.TaskGenerator(mnist_train, ways=ways, tasks=10000)<br/>    # test_gen = l2l.data.TaskGenerator(mnist_test, ways=ways)<br/><br/>    model = Net(ways)<br/>    model.to(device)<br/>    <strong>meta_model = l2l.algorithms.MAML(model, lr=maml_lr)</strong><br/>    opt = optim.Adam(meta_model.parameters(), lr=lr)<br/>    loss_func = nn.NLLLoss(reduction="sum")</pre>
<ol start="5">
<li>Although we haven't gone through an image classification example before, hopefully the code will be relatively understandable and familiar to you. The main point to note is the construction of the <kbd>meta_model</kbd> using the <kbd>l2l.algorithms.MAML</kbd> model on the highlighted line of code. Note how the <kbd>meta_model</kbd> wraps the <kbd>model</kbd> network by using it as an input.</li>
<li>From here, we will scroll down to the familiar training loop we have seen so many times before. This time, however, there are some interesting differences. Look specifically at the code just inside the first iteration loop, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">iteration_error = 0.0<br/>iteration_acc = 0.0<br/>for _ in range(tps):<br/>    learner = meta_model.clone()<br/>    train_task = train_gen.sample()<br/>    valid_task = train_gen.sample(task=train_task.sampled_task)</pre>
<p style="padding-left: 60px">Note how we are constructing a <kbd>learner</kbd> clone of the <kbd>meta_model</kbd> learner. The <kbd>learner</kbd> clone becomes our target learning network. The last two lines show the construction of a sampler for the training and validation tasks.</p>
<ol start="7">
<li>Next, let's see how <kbd>learner</kbd> is used to compute the loss again in an iterative manner using another loop, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">for step in range(fas):<br/>    train_error, _ = compute_loss(train_task, device, learner, loss_func, batch=shots * ways)<br/>    learner.adapt(train_error)</pre>
<ol start="8">
<li>At this point, run the sample and observe the output to get a sense of how training is done.  </li>
</ol>
<p>Now that we understand some of the basic code setup, we are going to move on to explore how the sample trains and computes loss in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a meta learner</h1>
                </header>
            
            <article>
                
<p>The <kbd>learn2learn</kbd> framework provides the MAML framework for building the learner model we can use to learn to learn; however, it is not automatic and does require a bit of setup and thought regarding how loss is computed for your particular set of tasks. We have already seen where we compute loss—now we will look closer at how loss is computed across tasks. Reopen <kbd>Chapter_14_learn.py</kbd> and go through the following exercise:</p>
<ol>
<li>Scroll back down to the innermost training loop within the <kbd>main</kbd> function.</li>
<li>The inner loop here is called a <strong>fast adaptive training loop</strong>, since we are showing our network a few or mini batches or shots of data for training. Computing the loss of the network is done using the <kbd>compute_loss</kbd> function, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def compute_loss(task, device, learner, loss_func, batch=5):<br/>    loss = 0.0<br/>    acc = 0.0<br/>    dataloader = DataLoader(task, batch_size=batch, shuffle=False, num_workers=0)<br/>    for i, (x, y) in enumerate(dataloader):<br/>        x, y = x.squeeze(dim=1).to(device), y.view(-1).to(device)<br/>        output = learner(x)<br/>        curr_loss = loss_func(output, y)<br/>        acc += accuracy(output, y)<br/>        loss += curr_loss / x.size(0)<br/>    loss /= len(dataloader)<br/>    return loss, acc</pre>
<ol start="3">
<li>Note how the computation of loss is done iteratively over task training batches by iterating through the <kbd>dataloader</kbd> list. We then compute the average loss for all tasks by taking the total loss, <kbd>loss</kbd>, and dividing it by the number of dataloaders.  </li>
</ol>
<ol start="4">
<li>This average <kbd>loss</kbd> and accuracy, <kbd>acc</kbd>, are returned from the <kbd>compute_loss</kbd> function. From that learning instance, the learner is then adapted or updated using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">train_error, _ = compute_loss(train_task, device, learner, loss_func, batch=shots * ways)<br/>learner.adapt(train_error)</pre>
<ol start="5">
<li>After the fast adaptive looping and updating the learner through each loop, we can then validate the learner with the following code:</li>
</ol>
<pre style="padding-left: 60px">valid_error, valid_acc = compute_loss(valid_task, device, learner, loss_func, batch=shots * ways)<br/>iteration_error += valid_error<br/>iteration_acc += valid_acc</pre>
<ol start="6">
<li>The <kbd>valid_error</kbd> validation error and <kbd>valid_acc</kbd> accuracy are then accumulated on the total <kbd>iteration_error</kbd> error and <kbd>iteration_acc</kbd> accuracy values.  </li>
<li>We finish by calculating the average iteration and accuracy errors, <kbd>iteration_error</kbd> or <kbd>iteration_acc</kbd> values, and then propagating that error back through the networks with the following code:</li>
</ol>
<pre style="padding-left: 60px">iteration_error /= tps<br/>iteration_acc /= tps<br/>tqdm_bar.set_description("Loss : {:.3f} Acc : {:.3f}".format(iteration_error.item(), iteration_acc))<br/><br/># Take the meta-learning step<br/>opt.zero_grad()<br/>iteration_error.backward()<br/>opt.step()</pre>
<ol start="8">
<li>The training for this example is quite quick, so run the example again and observe how quickly the algorithm can train across meta learning tasks.</li>
</ol>
<p>Each meta learning step involves pushing the loss back through the network using BPTT, since the meta network is composed of recurrent layers. That detail is abstracted for us here, but hopefully you can appreciate how seamlessly we were able to introduce meta learning into training this regular image classification task. In the next section, we will look at how we can apply meta learning to reinforcement learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing meta reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Now, that we understand the concept of meta learning, we can move on to meta reinforcement learning. Meta-RL—or RL^2 (RL Squared), as it has been called—is quickly evolving, but the additional complexity still makes this method currently inaccessible. While the concept is very similar to vanilla meta, it still introduces a number of subtle nuances for RL. Some of these can be difficult to understand, so hopefully the following diagram can help. It was taken from a paper titled <em>Reinforcement Learning, Fast and Slow</em> by <em>Botvinick, et al. 2019</em> (<a href="https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0">https://www.cell.com/action/showPdf?pii=S1364-6613%2819%2930061-0</a>):</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-703 image-border" src="assets/0f6a295d-b2dc-4130-a376-e93868d180ba.png" style="width:42.17em;height:16.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Meta reinforcement learning</div>
<p>In the diagram, you can see that familiar inner and outer loops that are characteristic of meta learning. This means that we also go from evaluating a policy <img class="fm-editor-equation" src="assets/7715985a-bfda-45c7-9fad-ac9d84f9253d.png" style="width:0.92em;height:0.92em;"/> for any observed state to also now including the last action, last reward, and observed state in meta-RL. These differences are summarized as follows:</p>
<ul>
<li>Reinforcement learning = <img class="fm-editor-equation" src="assets/c6e1f0bd-6030-4027-a98a-9007f4c48774.png" style="width:6.00em;height:1.83em;"/> distribution over <img class="fm-editor-equation" src="assets/3088d7ca-eb64-4bfb-8ef9-a79bedb4d2d2.png" style="width:1.17em;height:1.42em;"/></li>
<li>Meta reinforcement learning = <img class="fm-editor-equation" src="assets/591c6379-1708-4e19-af52-7ec21db9225e.png" style="width:12.83em;height:1.83em;"/> <span>distribution over </span><img class="fm-editor-equation" src="assets/1d1e849d-5700-4005-bba1-841cb7b6a3e7.png" style="width:1.17em;height:1.42em;"/></li>
</ul>
<p>As we have seen with regular meta learning, there are a number of variations that are used and experimented on within meta-RL, but they all share the following three common elements:</p>
<ul>
<li><strong>Model memory</strong>: We add memory to our model in the form of recurrent network layers or LSTM. Typically, the outer loop is composed of the memory component engaged by LSTM layers.</li>
</ul>
<ul>
<li><strong>Distribution of MDPs</strong>: The agent/algorithm needs to be trained across multiple different MDPs, which is typically done by exposing it to different or randomized environments.  </li>
<li><strong>Meta-learning algorithm</strong>: The agent needs a meta learning algorithm for itself to learn to learn. </li>
</ul>
<div class="packt_infobox">The Unity Obstacle Tower Challenge was most likely developed to encourage developers to build a meta-RL agent, but as we have seen, the winning entry used a variation of hierarchical reinforcement learning. While HRL is designed to accomplish the same function as meta-RL, it lacks the automatic generation of memory.</div>
<p>In order to get a sense of the diversity of meta-RL algorithms, we will look at a list of what appear to be the most current methods used:</p>
<ul>
<li><strong>Optimizing weights</strong>: This is essentially MAML or another variation called Reptile. MAML is currently one of the more popular variations used, and one we will explore later in detail.</li>
<li><strong>Meta-learning hyperparameters</strong>: There are a few hyperparameters that we use internally to balance learning within RL. These are the gamma and alpha values that we have tuned before, but imagine if they could be autotuned with meta-learning.</li>
<li><strong>Meta-learning loss</strong>: This considers that the loss function itself may need to be tuned, and uses a pattern to evolve it over iterations. This method uses evolutionary strategies that are outside the scope of this book.</li>
<li><strong>Meta-learning exploration</strong>: This uses meta learning to build more effective exploration strategies. This, in turn, reduces the amount of time exploring and increases effective training performance.</li>
<li><strong>Episodic control</strong>: This provides the agent with a method to keep important episodes in memory and forget others. This sounds a lot like prioritized experience replay, but the method of control here is within the calculation of loss and not from replay.</li>
<li><strong>Evolutionary algorithms</strong>: These are gradient-free, optimization-based solutions that use a form of genetic search to find solution methods. The collision of evolutionary algorithms and deep learning is an ongoing endeavor that many have tried and failed with. Both methods are very powerful and capable on their own, so it is perhaps only a matter of time before they get combined into a working model.  </li>
</ul>
<p>As you can see, there is plenty of variation in meta-RL methods, and we will look at how one method is implemented in detail in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MAML-RL</h1>
                </header>
            
            <article>
                
<p>The <kbd>learn2learn</kbd> repository holds another great example of how to use their library for a few variations of this method. A good method for us to look at will be an implementation of Meta-SGD, which further extends MAML by adopting per-parameter learning rates using vanilla policy gradients, and is often referred to as MetaSGD-VPG. This concept was originally presented in the paper <em>Meta Reinforcement Learning with Task Embedding and Shared Policy</em>, which was itself presented at IJCAI-19.   </p>
<div class="packt_tip">Make sure that you have completed all the installation steps from the last exercise before proceeding. If you have troubles running the sample, repeat the installation in a new virtual environment. Some issues may be related to the version of PyTorch you are using, so check that your version is compatible.</div>
<p class="mce-root">Open<span> </span><span>up</span><span> </span><kbd><span>Chapter_14_MetaSGD-VPG.py</span></kbd><span> </span><span>and go through the following steps:</span></p>
<ol>
<li>You will need to install the cherry RL package first by entering the following command in your virtual environment window:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install cherry-rl</strong></pre>
<ol start="2">
<li>We won't review the entire code listing, just the critical sections. First, let's look at the <kbd>main</kbd> function, which starts the initialization and hosts the training. The start of this function is shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def main(<br/>        experiment='dev',<br/>        env_name='Particles2D-v1',<br/>        adapt_lr=0.1,<br/>        meta_lr=0.01,<br/>        adapt_steps=1,<br/>        num_iterations=200,<br/>        meta_bsz=20,<br/>        adapt_bsz=20,<br/>        tau=1.00,<br/>        gamma=0.99,<br/>        num_workers=2,<br/>        seed=42,<br/>):<br/>    random.seed(seed)<br/>    np.random.seed(seed)<br/>    th.manual_seed(seed)<br/><br/>    def make_env():<br/>        return gym.make(env_name)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">In the definition of the <kbd>main</kbd> function, we can see all the relevant hyperparameters as well as their selected defaults. Note that the two new groups of hyperparameters for the adaption and meta learning steps are prefixed with <kbd>adapt</kbd> and <kbd>meta</kbd>, respectively.  </p>
<ol start="3">
<li>Next, we will look at the initialization of the environment, policy, meta learner, and optimizer using the following code:</li>
</ol>
<pre style="padding-left: 60px">env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])<br/>env.seed(seed)<br/>env = ch.envs.Torch(env)<br/>policy = DiagNormalPolicy(env.state_size, env.action_size)<br/>meta_learner = l2l.algorithms.MetaSGD(policy, lr=meta_lr)<br/>baseline = LinearValue(env.state_size, env.action_size)<br/>opt = optim.Adam(policy.parameters(), lr=meta_lr)<br/>all_rewards = []</pre>
<ol start="4">
<li class="mce-root"><span>Here, we can see the three training loops. First, the outer iteration loop controls</span> <span>the number of meta learning repetitions. Inside that loop, we have the task setup and configuration loop; remember that we want each learning session to require a different but related task. The third, innermost loop is where the adaption occurs, and we push the loss back through the model. The code for all three loops is shown here:</span></li>
</ol>
<pre style="padding-left: 60px">for iteration in range(num_iterations):<br/>    iteration_loss = 0.0<br/>    iteration_reward = 0.0<br/>    for task_config in tqdm(env.sample_tasks(meta_bsz)): <br/>        learner = meta_learner.clone()<br/>        env.set_task(task_config)<br/>        env.reset()<br/>        task = ch.envs.Runner(env)<br/><br/>        # Fast Adapt<br/>        for step in range(adapt_steps):<br/>            train_episodes = task.run(learner, episodes=adapt_bsz)<br/>            loss = maml_a2c_loss(train_episodes, learner, baseline, gamma, tau)<br/>            learner.adapt(loss)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="5">
<li>After the fast adaptive looping takes place, we then jump back to the second loop and calculate the validation loss with the following code:</li>
</ol>
<pre style="padding-left: 60px">valid_episodes = task.run(learner, episodes=adapt_bsz)<br/>loss = maml_a2c_loss(valid_episodes, learner, baseline, gamma, tau)<br/>iteration_loss += loss<br/>iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz</pre>
<ol start="6">
<li>The validation loss is computed over the second loop for each different task. This loss is then accumulated into the iteration loss, <kbd>iteration_loss</kbd>, value. Leaving the second loop, we then print out some stats and calculate the adaption loss, <kbd>adaption_loss</kbd>, and push that as a gradient back through the network for training with the following code:</li>
</ol>
<pre style="padding-left: 60px">adaptation_loss = iteration_loss / meta_bsz<br/>print('adaptation_loss', adaptation_loss.item())<br/><br/>opt.zero_grad()<br/>adaptation_loss.backward()<br/>opt.step()</pre>
<ol start="7">
<li>Remember that the divisors in both loss equations (iteration and adaption) both use a similar value of <kbd>20</kbd>, <kbd>meta_bsz</kbd> <kbd>= 20</kbd>, and <kbd>adapt_bsz = 20</kbd>. The base loss function is defined by the <kbd>maml_a2c_loss</kbd> and <kbd>compute_advantages</kbd> functions, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):<br/>    # Update baseline<br/>    returns = ch.td.discount(gamma, rewards, dones)<br/>    baseline.fit(states, returns)<br/>    values = baseline(states)<br/>    next_values = baseline(next_states)<br/>    bootstraps = values * (1.0 - dones) + next_values * dones<br/>    next_value = th.zeros(1, device=values.device)<br/>    return ch.pg.generalized_advantage(tau=tau,<br/>                                       gamma=gamma,<br/>                                       rewards=rewards,<br/>                                       dones=dones,<br/>                                       values=bootstraps,<br/>                                       next_value=next_value)<br/><br/>def maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):    <br/>    states = train_episodes.state()<br/>    actions = train_episodes.action()<br/>    rewards = train_episodes.reward()<br/>    dones = train_episodes.done()<br/>    next_states = train_episodes.next_state()<br/>    log_probs = learner.log_prob(states, actions)<br/>    advantages = compute_advantages(baseline, tau, gamma, rewards,<br/>                                    dones, states, next_states)<br/>    advantages = ch.normalize(advantages).detach()<br/>    return a2c.policy_loss(log_probs, advantages)</pre>
<div class="packt_infobox">Note how the cherry RL library saves us the implementation of some tricky code. Fortunately, we should already know what the cherry functions <kbd>ch.td.discount</kbd> and <kbd>ch.pg.generalized_advantage</kbd> are, as we encountered them in previous chapters, and so we won't need to review them here.</div>
<ol start="8">
<li>Run the example as you normally would and observe the output. An example of the generated output is shown in the following code:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-712 image-border" src="assets/55603a29-3aae-4937-ae15-b26db7666157.png" style="width:74.17em;height:43.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from Chapter_14_MetaSQG-VPG.py</div>
<p>Note the amount of training time that the sample expects to take when running on a CPU when it first starts. While the prediction comes down significantly from five days to just over two in less than an hour, it still demonstrates the computational requirements for this type of training. Therefore, if you plan to do any serious work on meta-RL, you will likely want to use a very fast GPU for training. When testing on a very fast GPU, the preceding sample took 1,000 times less time to process. Yes, you read that right, 1,000 times less time. While you likely may not experience such a vast difference, any upgrade from a CPU to GPU will be significant.</p>
<p>There is a strong belief that is held <span>by many of those in the RL community </span><span>that meta-RL is the next big leap that we need to solve in order to get closer to AGI. Most of the development of this field is still guided by what is currently the state of the art, and how and when changes will dictate the future of RL. With this in mind, we are going to look at some other potential next-level steps, starting in the next section with HER.  </span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using hindsight experience replay</h1>
                </header>
            
            <article>
                
<p>Hindsight experience replay was introduced by OpenAI as a method to deal with sparse rewards, but the algorithm has also been shown to successfully generalize across tasks due in part to the novel mechanism by which HER works. The analogy used to explain HER is a game of shuffleboard, the object of which is to slide a disc down a long table to reach a goal target. When first learning the game, we will <span>often</span><span> </span><span>repeatedly fail, with the disc falling off the table or playing area. Except, it is presumed that we learn by expecting to fail and give ourselves a reward when we do so. Then, internally, we can work backward by reducing our failure reward and thereby increasing other non-failure rewards. In some ways, this method resembles Pierarchy (a form of HRL that we looked at earlier), but without the extensive pretraining parts.</span></p>
<div class="packt_infobox">The next collection of samples in the following sections has again been sourced from <a href="https://github.com/higgsfield">https://github.com/higgsfield</a>, and are the result of a young man named Dulat Yerzat from Almaty, Kazakhstan.</div>
<p>Open the samples for <kbd>Chapter_14_wo_HER.py</kbd> and <kbd>Chapter_14_HER.py</kbd>. These two samples are comparisons of simple DQN networks that are applied with and without HER. Go through the following steps:</p>
<ol>
<li>Both examples are almost the same, aside from the implementation of HER, so the comparison will help us understand how the code works. Next, the environment has been simplified and custom built to perform that simple bit shifting of a random set of bits. The code to create the environment is as follows:</li>
</ol>
<pre style="padding-left: 60px">class Env(object):<br/>    def __init__(self, num_bits):<br/>        self.num_bits = num_bits<br/>    <br/>    def reset(self):<br/>        self.done = False<br/>        self.num_steps = 0<br/>        self.state = np.random.randint(2, size=self.num_bits)<br/>        self.target = np.random.randint(2, size=self.num_bits)<br/>        return self.state, self.target<br/>    <br/>    def step(self, action):<br/>        if self.done:<br/>            raise RESET        <br/>        self.state[action] = 1 - self.state[action]        <br/>        if self.num_steps &gt; self.num_bits + 1:<br/>            self.done = True<br/>        self.num_steps += 1        <br/>        if np.sum(self.state == self.target) == self.num_bits:<br/>            self.done = True<br/>            return np.copy(self.state), 0, self.done, {}<br/>        else:<br/>            return np.copy(self.state), -1, self.done, {}</pre>
<ol start="2">
<li>We never really went over how to build a custom environment, but as you can see, it can be quite simple. Next, we will look at the simple DQN model that we will use to train, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">class Model(nn.Module):<br/>    def __init__(self, num_inputs, num_outputs, hidden_size=256):<br/>        super(Model, self).__init__()<br/>        <br/>        self.linear1 = nn.Linear(num_inputs, hidden_size)<br/>        self.linear2 = nn.Linear(hidden_size, num_outputs)<br/>    <br/>    def forward(self, state, goal):<br/>        x = torch.cat([state, goal], 1)<br/>        x = F.relu(self.linear1(x))<br/>        x = self.linear2(x)<br/>        return x</pre>
<ol start="3">
<li>That is about as simple a DQN model as you can get. Next, let's compare the two examples by viewing the code side by side, as shown in the following screenshot:</li>
</ol>
<p class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-713 image-border" src="assets/1c6c16c6-4389-42fd-a144-2d488bc5b30f.png" style="width:50.75em;height:28.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Comparison of code examples in VS</span></div>
<ol start="4">
<li>The new section of code is also shown here:</li>
</ol>
<pre style="padding-left: 60px">new_episode = []<br/>  for state, reward, done, next_state, goal in episode:<br/>    for t in np.random.choice(num_bits, new_goals):<br/>      try:<br/>        episode[t]<br/>      except:<br/>        continue<br/>      new_goal = episode[t][-2]<br/>      if np.sum(next_state == new_goal) == num_bits:<br/>        reward = 0<br/>      else:<br/>        reward = -1<br/>      replay_buffer.push(state, action, reward, next_state, done, new_goal)<br/>      new_episode.append((state, reward, done, next_state, new_goal)) </pre>
<ol start="5">
<li>What we see here is the addition of another loop not unlike meta-RL, but this time, instead of wrapping the inner loop, it sits as a sibling. The second loop is activated after an episode is completed from the first inner loop. It then loops through every event in the previous episode and adjusts the goals or targets based on the returned reward based on the new goal. This is essentially the hindsight part.</li>
<li>The remaining parts of this example resemble many of our previous examples, and should be quite familiar by now. One interesting part, though, is the <kbd>get_action</kbd> function, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def get_action(model, state, goal, epsilon=0.1):<br/>    if random.random() &lt; 0.1:<br/>        return random.randrange(env.num_bits)<br/>    <br/>    state = torch.FloatTensor(state).unsqueeze(0).to(device)<br/>    goal = torch.FloatTensor(goal).unsqueeze(0).to(device)<br/>    q_value = model(state, goal)<br/>    return q_value.max(1)[1].item()</pre>
<p style="padding-left: 60px">Note here that we are using an <kbd>epsilon</kbd> value that is defaulted to <kbd>.1</kbd> to denote the tendency for exploration. In fact, you might notice that this example uses no variable exploration.</p>
<ol start="7">
<li>Continuing with the differences, the next key difference is the <kbd>compute_td_loss</kbd> function, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">def compute_td_error(batch_size):<br/>    if batch_size &gt; len(replay_buffer):<br/>        return None<br/><br/>    state, action, reward, next_state, done, goal = replay_buffer.sample(batch_size)<br/><br/>    state = torch.FloatTensor(state).to(device)<br/>    reward = torch.FloatTensor(reward).unsqueeze(1).to(device)<br/>    action = torch.LongTensor(action).unsqueeze(1).to(device)<br/>    next_state = torch.FloatTensor(next_state).to(device)<br/>    goal = torch.FloatTensor(goal).to(device)<br/>    <strong>mask = torch.FloatTensor(1 - np.float32(done)).unsqueeze(1).to(device)</strong><br/><br/>    q_values = model(state, goal)<br/>    q_value = q_values.gather(1, action)<br/><br/>    next_q_values = target_model(next_state, goal)<br/>    target_action = next_q_values.max(1)[1].unsqueeze(1)<br/>    next_q_value = target_model(next_state, goal).gather(1, target_action)<br/><br/>    <strong>expected_q_value = reward + 0.99 * next_q_value * mask</strong><br/><br/>    loss = (q_value - expected_q_value.detach()).pow(2).mean()<br/><br/>    optimizer.zero_grad()<br/>    loss.backward()<br/>    optimizer.step()<br/>    <br/>    return loss</pre>
<ol start="8">
<li> Run the example without HER first and observe the results, then run the example with HER. The output for the example with HER is shown in the following excerpt:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-714 image-border" src="assets/79196e9f-67de-45fe-aec1-383d28af335a.png" style="width:90.83em;height:39.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Example output from Chapter_14_HER.py</div>
<p>Compared to the example without HER, the preceding output is significantly better. You will have to run both examples yourself to see the exact difference. Note how the calculation of loss remains consistently variable and doesn't converge, while the mean reward increases. In the next section, we move to what is expected to be the next wave in RL—imagination and reasoning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Imagination and reasoning in RL</h1>
                </header>
            
            <article>
                
<p>Something that we can observe from our own experience of learning is how imagination can benefit the learning process. Pure imagination is the stuff of <span>deep abstract thoughts </span><span>and </span><span>dreams,</span><span> </span><span>often closer to a hallucination than any way to solve a real problem. Except, this same imagination can be used to span gaps in our understanding of knowledge and allow us to reason out possible solutions. Say that we are trying to solve the problem of putting a puzzle together, and all we have are three remaining, mostly black pieces, as shown in the following image:</span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-715 image-border" src="assets/88801726-c736-427f-9580-328e2c8886a0.png" style="width:33.25em;height:29.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Imagining what the three missing puzzle pieces may look like</div>
<p>Given the simplicity of the preceding diagram, it is quite easy for us to imagine what those puzzle pieces may look like. We are able to fill in those gaps quite easily using our imagination from previous observations and reasoning. This use of imagination to fill in gaps is something we use all the time, and it is often said that the more imaginative you are, the more intelligent you are as well. Now it remains to be seen if this path to AI will indeed prove that theory, but it certainly looks like a possibility.</p>
<p>Imagination is not pulled from a vacuum, and likewise, in order to give our agents imagination, we have to essentially bootstrap their memory or previous learnings. We will do this in the next exercise in order for us to later generate the imagination from these learnings. Open sample <kbd>Chapter_14_Imagine_A2C.py</kbd> and go through the following steps:</p>
<ol>
<li>The base agent we will use to generate the training bootstrap for our imagination will be a simple A2C Vanilla PG method. Let's first scroll down in the file and look at the <kbd>ActorCritic</kbd> class that defines our agent:</li>
</ol>
<pre style="padding-left: 60px">class ActorCritic(OnPolicy):<br/>    def __init__(self, in_shape, num_actions):<br/>        super(ActorCritic, self).__init__()<br/>        <br/>        self.in_shape = in_shape<br/>        <br/>        self.features = nn.Sequential(<br/>            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(16, 16, kernel_size=3, stride=2),<br/>            nn.ReLU(),<br/>        )<br/>        <br/>        self.fc = nn.Sequential(<br/>            nn.Linear(self.feature_size(), 256),<br/>            nn.ReLU(),<br/>        )<br/>        <br/>        self.critic = nn.Linear(256, 1)<br/>        self.actor = nn.Linear(256, num_actions)<br/>        <br/>    def forward(self, x):<br/>        x = self.features(x)<br/>        x = x.view(x.size(0), -1)<br/>        x = self.fc(x)<br/>        logit = self.actor(x)<br/>        value = self.critic(x)<br/>        return logit, value<br/>    <br/>    def feature_size(self):<br/>        return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1, -1).size(1)</pre>
<ol start="2">
<li>What we can see is a simple PG agent that will be powered by an A2C synchronous actor-critic. Next, we come to another new class called <kbd>RolloutStorage</kbd>. Rollout storage is similar in concept to experience replay, but it also enables us to have an ongoing calculation of returns, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">class RolloutStorage(object):<br/>    def __init__(self, num_steps, num_envs, state_shape):<br/>        self.num_steps = num_steps<br/>        self.num_envs = num_envs<br/>        self.states = torch.zeros(num_steps + 1, num_envs, *state_shape)<br/>        self.rewards = torch.zeros(num_steps, num_envs, 1)<br/>        self.masks = torch.ones(num_steps + 1, num_envs, 1)<br/>        self.actions = torch.zeros(num_steps, num_envs, 1).long()<br/>        #self.use_cuda = False<br/>            <br/>    def cuda(self):<br/>        #self.use_cuda = True<br/>        self.states = self.states.cuda()<br/>        self.rewards = self.rewards.cuda()<br/>        self.masks = self.masks.cuda()<br/>        self.actions = self.actions.cuda()<br/>        <br/>    def insert(self, step, state, action, reward, mask):<br/>        self.states[step + 1].copy_(state)<br/>        self.actions[step].copy_(action)<br/>        self.rewards[step].copy_(reward)<br/>        self.masks[step + 1].copy_(mask)<br/>        <br/>    def after_update(self):<br/>        self.states[0].copy_(self.states[-1])<br/>        self.masks[0].copy_(self.masks[-1])<br/>        <br/>    def compute_returns(self, next_value, gamma):<br/>        returns = torch.zeros(self.num_steps + 1, self.num_envs, 1)<br/>        #if self.use_cuda:<br/>        # returns = returns.cuda()<br/>        returns[-1] = next_value<br/>        for step in reversed(range(self.num_steps)):<br/>            returns[step] = returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]<br/>        return returns[:-1]</pre>
<ol start="3">
<li>If we scroll down to the <kbd>main</kbd> function, we can see that there are 16 synchronous environments that are being run with the following code:</li>
</ol>
<pre style="padding-left: 60px">def main():<br/>    mode = "regular"<br/>    num_envs = 16<br/><br/><br/>    def make_env():<br/>        def _thunk():<br/>            env = MiniPacman(mode, 1000)<br/>            return env<br/><br/><br/>        return _thunk<br/><br/><br/>    envs = [make_env() for i in range(num_envs)]<br/>    envs = SubprocVecEnv(envs)<br/><br/>    state_shape = envs.observation_space.shape</pre>
<ol start="4">
<li>We will talk more about the <kbd>RolloutStorage</kbd> class later. For now, move down to the training section of code. It is the typical double-loop code, the outside loop controlling episodes and the inside loop controlling steps, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">for i_update in range(num_frames):<br/>        for step in range(num_steps):<br/>            action = actor_critic.act(autograd.Variable(state))</pre>
<p style="padding-left: 60px">The rest of the training code should be familiar, but it should be worth reviewing in detail on your own.  </p>
<ol start="5">
<li>The next major difference we want to observe is at the end of the outer training loop. This last block of code is where the loss is calculated and pushed back through the network:</li>
</ol>
<pre style="padding-left: 60px">optimizer.zero_grad()<br/>loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef<br/>loss.backward()<br/><strong>nn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)</strong><br/>optimizer.step()</pre>
<ol start="6">
<li>Note the highlighted line in the preceding code block. This is unique in that we are clipping the gradient to a maximum value that is likely to avoid exploding gradients. The last section of code at the end renders out the playing area and <span>shows </span>the agent playing the game. </li>
</ol>
<div class="packt_infobox">Exploding gradients are when a gradient value becomes so large that it causes the network to forget knowledge. The network weights start to be trained in wild fluctuations and any previous knowledge will often be lost.</div>
<ol start="7">
<li>Run the code as you normally would and observe the output.</li>
</ol>
<p>Running the preceding code will also create a saved-state dictionary of memories that we will use to populate the imagination later. You must run this last exercise to completion if you want to continue working with later exercises. In the next section, we will explore how these latent traces can be used to generate an agent's imagination.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating imagination</h1>
                </header>
            
            <article>
                
<p>In the current version of this algorithm, we first need to bootstrap the memories that we populate in the agent with a previous run by an agent, or perhaps a human. This is really no different than imitation learning or behavioral cloning, except we are using an on-policy agent that we will later use as an off-policy base for our imagination. Before we combine imagination into our agent, we can see how the predicted next state will look compared to what the agent's actual state will be. Let's see how this works by opening up the next example <kbd>Chapter_14_Imagination.py</kbd> and go through the following steps:</p>
<ol>
<li>This example works by loading the previous saved-state dictionary we generated in the last exercise. Make sure that this data is generated and saved with a prefix of <kbd>actor_critic_</kbd> files in the same folder before continuing.</li>
<li>The purpose of this code is to extract the saved-state observation dictionary we recorded earlier. We then want to extract the observation and use it to imagine what the next state will look like. Then we can compare how well the imagined and next state resemble each other. This amount of resemblance will in turn be used to train an imagination loss later. We can see how the previous model is loaded by looking at the following line of code:</li>
</ol>
<pre style="padding-left: 60px">actor_critic.load_state_dict(torch.load("actor_critic_" + mode))</pre>
<ol start="3">
<li>The preceding line of code reloads our previously trained model. Now we want to use the imagination to (for instance) reasonably fill in the areas where the agent may not have explored. Scrolling down, we can see the training loop that will learn the imagination part of the agent:</li>
</ol>
<pre style="padding-left: 60px">for frame_idx, states, actions, rewards, next_states, dones in play_games(envs, num_updates):<br/>    states = torch.FloatTensor(states)<br/>    actions = torch.LongTensor(actions)<br/>    batch_size = states.size(0)<br/>    <br/>    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])<br/>    onehot_actions[range(batch_size), actions] = 1<br/>    inputs = autograd.Variable(torch.cat([states, onehot_actions], 1))</pre>
<ol start="4">
<li>This loop loops through the previously played games and encodes the actions using one-hot encoding. Scrolling down, we can see how the <kbd>imagined_state</kbd> state and the <kbd>imagined_reward</kbd> reward are learned:</li>
</ol>
<pre style="padding-left: 60px">imagined_state, imagined_reward = env_model(inputs)<br/><br/>target_state = pix_to_target(next_states)<br/>target_state = autograd.Variable(torch.LongTensor(target_state))<br/>target_reward = rewards_to_target(mode, rewards)<br/>target_reward = autograd.Variable(torch.LongTensor(target_reward))<br/><br/>optimizer.zero_grad()<br/>image_loss = criterion(imagined_state, target_state)<br/>reward_loss = criterion(imagined_reward, target_reward)<br/>loss = image_loss + reward_coef * reward_loss<br/>loss.backward()<br/>optimizer.step()<br/>    <br/>losses.append(loss.item())<br/>all_rewards.append(np.mean(rewards))</pre>
<p style="padding-left: 60px">This is the section of code that learns to correctly imagine the target state and reward from playing through the previously observed observations. Of course, the more observations, the better the imagination, but at some point, too many observations will eliminate all of the imagination entirely. Balancing this new trade-off will require a bit of trial and error on your own.</p>
<ol start="5">
<li>Scrolling down to the bottom of the file, you can see where an example of the imagination and target states are outputted with the following code:</li>
</ol>
<pre style="padding-left: 60px">while not done:<br/>    steps += 1<br/>    actions = get_action(state)<br/>    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])<br/>    onehot_actions[range(batch_size), actions] = 1<br/>    state = torch.FloatTensor(state).unsqueeze(0)<br/>    <br/>    inputs = autograd.Variable(torch.cat([state, onehot_actions], 1))      <br/>    imagined_state, imagined_reward = env_model(inputs)<br/>    imagined_state = F.softmax(imagined_state)<br/>    iss.append(imagined_state)<br/>    <br/>    next_state, reward, done, _ = env.step(actions[0])<br/>    ss.append(state)<br/>    state = next_state<br/>    <br/>    imagined_image = target_to_pix(imagined_state.view(batch_size, -1, len(pixels))[0].max(1)[1].data.cpu().numpy())<br/>    imagined_image = imagined_image.reshape(15, 19, 3)<br/>    state_image = torch.FloatTensor(next_state).permute(1, 2, 0).cpu().numpy()<br/>            <br/>    plt.figure(figsize=(10,3))<br/>    plt.subplot(131)<br/>    plt.title("Imagined")<br/>    plt.imshow(imagined_image)<br/>    plt.subplot(132)<br/>    plt.title("Actual")<br/>    plt.imshow(state_image)<br/>    plt.show()<br/>    time.sleep(0.3)<br/>    <br/>    if steps &gt; 30:<br/>       break</pre>
<ol start="6">
<li>The following example screenshot shows the best the original author was able to get by training the agent for a considerable amount of time:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-716 image-border" src="assets/9d845819-71d5-4b23-b772-b452c54d1bec.png" style="width:29.75em;height:12.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example comparison of imagined versus actual</div>
<ol start="7">
<li>Run the example and your output: depending on your previous amount of training, it may not look as good. Again, the quality of the imagination will be based on the previous experiences and amount of training to refine the imagination itself.</li>
<li>One last thing to note is how the imagined image is getting extracted. This is done using an inverted CNN in the <kbd>BasicBlock</kbd> class that converts the encoding back to an image of the correct resolution. The code for the <kbd>BasicBlock</kbd> class is shown here:</li>
</ol>
<pre style="padding-left: 60px">class BasicBlock(nn.Module):<br/>    def __init__(self, in_shape, n1, n2, n3):<br/>        super(BasicBlock, self).__init__()<br/>        <br/>        self.in_shape = in_shape<br/>        self.n1 = n1<br/>        self.n2 = n2<br/>        self.n3 = n3<br/>        <br/>        self.maxpool = nn.MaxPool2d(kernel_size=in_shape[1:])<br/>        self.conv1 = nn.Sequential(<br/>            nn.Conv2d(in_shape[0] * 2, n1, kernel_size=1, stride=2, padding=6),<br/>            nn.ReLU(),<br/>            nn.Conv2d(n1, n1, kernel_size=10, stride=1, padding=(5, 6)),<br/>            nn.ReLU(),<br/>        )<br/>        self.conv2 = nn.Sequential(<br/>            nn.Conv2d(in_shape[0] * 2, n2, kernel_size=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(n2, n2, kernel_size=3, stride=1, padding=1),<br/>            nn.ReLU(),<br/>        )<br/>        self.conv3 = nn.Sequential(<br/>            nn.Conv2d(n1 + n2, n3, kernel_size=1),<br/>            nn.ReLU()<br/>        )<br/>        <br/>    def forward(self, inputs):<br/>        x = self.pool_and_inject(inputs)<br/>        x = torch.cat([self.conv1(x), self.conv2(x)], 1)<br/>        x = self.conv3(x)<br/>        x = torch.cat([x, inputs], 1)<br/>        return x<br/>    <br/>    def pool_and_inject(self, x):<br/>        pooled = self.maxpool(x)<br/>        tiled = pooled.expand((x.size(0),) + self.in_shape)<br/>        out = torch.cat([tiled, x], 1)<br/>        return out</pre>
<p>As we can see, training the imagination process itself is not that difficult. The real difficulty is putting this all together in a running agent, and we will see how this is done in the next section when we learn about I2A.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding imagination-augmented agents</h1>
                </header>
            
            <article>
                
<p>The concept of <strong>imagination-augmented agents</strong> (<strong>I2A</strong>) was released in a paper titled <em>Imagination-Augmented Agents for Deep Reinforcement Learning</em> in February 2018 by T. Weber, et al. We have already talked about why imagination is important for learning and learning to learn. Imagination allows us to fill in the gaps in our learning and make leaps in our knowledge, if you will.</p>
<p>Giving agents an imagination allows us to combine model-based and model-free learning. Most of the agent algorithms we have used in this book have been model-free, meaning that we have no representative model of the environment. Early on, we did cover model-based RL with MC and DP, but most of our efforts have been fixed on model-free agents. The benefit of having a model of the environment is that the agent can then plan. Without a model, our agent just becomes reactionary through trial and error attempts. Adding imagination allows us to combine some aspects of using a model of the environment while being model free. Essentially, we hope to achieve the best of both worlds using imagination.</p>
<p>We have already explored the core role of imagination in the I2A architecture. This was the part we looked at in the last section that generated the imagined features and reward, essentially the model part. The following diagram illustrates the I2A architecture, the imagination core part, and the rollout encoder:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-717 image-border" src="assets/e5fd38c1-0c19-416d-b92b-70aa02ef2919.png" style="width:35.17em;height:20.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Summary of the I2A architecture</div>
<p>The I2A architecture demonstrates the complexity of the systems that we can start to build on top of DRL in the hopes of adding additional learning advantages, such as imagination. In order to really understand this architecture, we should look at a code example. Open up <kbd>Chapter_14_I2A.py</kbd> and go through the following steps:</p>
<ol>
<li>We have already covered the first part of the architecture, so at this stage, we can start with the policy itself. Look at the I2A policy class:</li>
</ol>
<pre style="padding-left: 60px">class I2A(OnPolicy):<br/>    def __init__(self, in_shape, num_actions, num_rewards, hidden_size, imagination, full_rollout=True):<br/>        super(I2A, self).__init__()<br/>        <br/>        self.in_shape = in_shape<br/>        self.num_actions = num_actions<br/>        self.num_rewards = num_rewards<br/>        <br/>        self.imagination = imagination<br/>        <br/>        self.features = nn.Sequential(<br/>            nn.Conv2d(in_shape[0], 16, kernel_size=3, stride=1),<br/>            nn.ReLU(),<br/>            nn.Conv2d(16, 16, kernel_size=3, stride=2),<br/>            nn.ReLU(),<br/>        )<br/>        <br/>        self.encoder = RolloutEncoder(in_shape, num_rewards, hidden_size)<br/>        <br/>        if full_rollout:<br/>            self.fc = nn.Sequential(<br/>                nn.Linear(self.feature_size() + num_actions * hidden_size, 256),<br/>                nn.ReLU(),<br/>            )<br/>        else:<br/>            self.fc = nn.Sequential(<br/>                nn.Linear(self.feature_size() + hidden_size, 256),<br/>                nn.ReLU(),<br/>            )<br/>        <br/>        self.critic = nn.Linear(256, 1)<br/>        self.actor = nn.Linear(256, num_actions)<br/>        <br/>    def forward(self, state):<br/>        batch_size = state.size(0)<br/>        <br/>        imagined_state, imagined_reward = self.imagination(state.data)<br/>        hidden = self.encoder(autograd.Variable(imagined_state), autograd.Variable(imagined_reward))<br/>        hidden = hidden.view(batch_size, -1)<br/>        <br/>        state = self.features(state)<br/>        state = state.view(state.size(0), -1)<br/>        <br/>        x = torch.cat([state, hidden], 1)<br/>        x = self.fc(x)<br/>        <br/>        logit = self.actor(x)<br/>        value = self.critic(x)<br/>        <br/>        return logit, value<br/>        <br/>    def feature_size(self):<br/>        return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1, -1).size(1)</pre>
<ol start="2">
<li>For the most part, this is a fairly simple PG policy, except with the addition of imagination elements. Note how, in the <kbd>forward</kbd> function, the forward pass refers to the imagination needed to extract the <kbd>imagined_state</kbd> and <kbd>imagined_reward</kbd> values.</li>
<li>Next, we scroll down a little bit more and come to the <kbd>ImaginationCore</kbd> class. The class encapsulates the functionality we have seen before, but all wrapped in a single class, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">class ImaginationCore(object):<br/>    def __init__(self, num_rolouts, in_shape, num_actions, num_rewards, env_model, distil_policy, full_rollout=True):<br/>        self.num_rolouts = num_rolouts<br/>        self.in_shape = in_shape<br/>        self.num_actions = num_actions<br/>        self.num_rewards = num_rewards<br/>        self.env_model = env_model<br/>        self.distil_policy = distil_policy<br/>        self.full_rollout = full_rollout<br/>        <br/>    def __call__(self, state):<br/>        state = state.cpu()<br/>        batch_size = state.size(0)<br/><br/>        rollout_states = []<br/>        rollout_rewards = []<br/><br/>        if self.full_rollout:<br/>            state = state.unsqueeze(0).repeat(self.num_actions, 1, 1, 1, 1).view(-1, *self.in_shape)<br/>            action = torch.LongTensor([[i] for i in range(self.num_actions)]*batch_size)<br/>            action = action.view(-1)<br/>            rollout_batch_size = batch_size * self.num_actions<br/>        else:<br/>            action = self.distil_policy.act(autograd.Variable(state, volatile=True))<br/>            action = action.data.cpu()<br/>            rollout_batch_size = batch_size<br/><br/>        for step in range(self.num_rolouts):<br/>            onehot_action = torch.zeros(rollout_batch_size, self.num_actions, *self.in_shape[1:])<br/>            onehot_action[range(rollout_batch_size), action] = 1<br/>            inputs = torch.cat([state, onehot_action], 1)<br/><br/>            imagined_state, imagined_reward = self.env_model(autograd.Variable(inputs, volatile=True))<br/><br/>            imagined_state = F.softmax(imagined_state).max(1)[1].data.cpu()<br/>            imagined_reward = F.softmax(imagined_reward).max(1)[1].data.cpu()<br/><br/>            imagined_state = target_to_pix(imagined_state.numpy())<br/>            imagined_state = torch.FloatTensor(imagined_state).view(rollout_batch_size, *self.in_shape)<br/><br/>            onehot_reward = torch.zeros(rollout_batch_size, self.num_rewards)<br/>            onehot_reward[range(rollout_batch_size), imagined_reward] = 1<br/><br/>            rollout_states.append(imagined_state.unsqueeze(0))<br/>            rollout_rewards.append(onehot_reward.unsqueeze(0))<br/><br/>            state = imagined_state<br/>            action = self.distil_policy.act(autograd.Variable(state, volatile=True))<br/>            action = action.data.cpu()<br/>        <br/>        return torch.cat(rollout_states), torch.cat(rollout_rewards)</pre>
<ol start="4">
<li>Now that we have seen how these big pieces work, it is time to get to the <kbd>main</kbd> function. We will start by looking at the first dozen or so lines of code:</li>
</ol>
<pre style="padding-left: 60px">envs = [make_env() for i in range(num_envs)]<br/>envs = SubprocVecEnv(envs)<br/>state_shape = envs.observation_space.shape<br/>num_actions = envs.action_space.n<br/>num_rewards = len(task_rewards[mode])<br/><br/>full_rollout = True<br/><br/>env_model = EnvModel(envs.observation_space.shape, num_pixels, num_rewards)<br/>env_model.load_state_dict(torch.load("env_model_" + mode))<br/>distil_policy = ActorCritic(envs.observation_space.shape, envs.action_space.n)<br/>distil_optimizer = optim.Adam(distil_policy.parameters())<br/><br/>imagination = ImaginationCore(1, state_shape, num_actions, num_rewards, env_model, distil_policy, full_rollout=full_rollout)<br/><br/>actor_critic = I2A(state_shape, num_actions, num_rewards, 256, imagination, full_rollout=full_rollout)</pre>
<p style="padding-left: 60px">Note the flow of code. The code goes from instantiating an environment model <kbd>env_model</kbd> and the <kbd>distil_policy</kbd> from an <kbd>ActorCritic</kbd> class. Then the code sets up the optimizer and instantiates the <kbd>imagination</kbd> object of the <kbd>ImaginationCore</kbd> type with inputs of <kbd>env_model</kbd> and <kbd>distil_policy</kbd>. The last line creates the <kbd>actor_critic I2A</kbd> policy using the <kbd>imagination</kbd> object as input.</p>
<ol start="5">
<li>Jump down to the training loop. Note that it looks fairly standard:</li>
</ol>
<pre style="padding-left: 60px">for i_update in tqdm(range(num_frames)):<br/>    for step in range(num_steps):<br/>        action = actor_critic.act(autograd.Variable(current_state))<br/>        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())<br/>        reward = torch.FloatTensor(reward).unsqueeze(1)<br/>        episode_rewards += reward<br/>        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)<br/>        final_rewards *= masks<br/>        final_rewards += (1-masks) * episode_rewards<br/>        episode_rewards *= masks</pre>
<ol start="6">
<li>After the inner episode loop is complete, we then jump down to the loss calculation and update the code, as shown here:</li>
</ol>
<pre style="padding-left: 60px">_, next_value = actor_critic(autograd.Variable(rollout.states[-1], volatile=True))<br/>next_value = next_value.data<br/><br/>returns = rollout.compute_returns(next_value, gamma)<br/>logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(<br/>autograd.Variable(rollout.states[:-1]).view(-1, *state_shape),<br/>            autograd.Variable(rollout.actions).view(-1, 1)<br/>        )<br/>    <br/>distil_logit, _, _, _ = distil_policy.evaluate_actions(<br/>            autograd.Variable(rollout.states[:-1]).view(-1, *state_shape),<br/>            autograd.Variable(rollout.actions).view(-1, 1)<br/>        )<br/>        <br/>distil_loss = 0.01 * (F.softmax(logit).detach() * F.log_softmax(distil_logit)).sum(1).mean()<br/><br/>values = values.view(num_steps, num_envs, 1)<br/>action_log_probs = action_log_probs.view(num_steps, num_envs, 1)<br/>advantages = autograd.Variable(returns) - values<br/><br/>value_loss = advantages.pow(2).mean()<br/>action_loss = -(autograd.Variable(advantages.data) * action_log_probs).mean()<br/><br/>optimizer.zero_grad()<br/>loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef<br/>loss.backward()<br/>nn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)<br/>optimizer.step()<br/>distil_optimizer.zero_grad()<br/>distil_loss.backward()<br/>optimizer.step()</pre>
<ol start="7">
<li>One thing to pay attention to here is that we are using two loss gradients to push back the loss to the <kbd>distil</kbd> model, which adjusts the <kbd>distil</kbd> model parameters and the <kbd>actor_critic</kbd> model or policy and its parameters. Without getting too bogged down in details, the main concept here is that we train the <kbd>distil</kbd> model to learn the imagination and the other loss for general policy training.</li>
<li>Run the example again. Wait until it starts and then you may want to shut it down after a few rounds, because, this sample can take upwards of an hour per iteration on a slower CPU, possibly longer. The following is an example screenshot of the start of training:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-718 image-border" src="assets/a625ef87-3a79-47f0-a2fb-c13ca20068f5.png" style="width:48.33em;height:25.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example of Chapter_14_I2A.py training</div>
<p>Now, if you want to run this exercise to completion, you should use a GPU at the very least. Ten thousand hours of CPU training would take a year, and is not something you will likely want to spend time on. If you do use a GPU, you will have to modify the sample to support a GPU, and this will require uncommenting sections and setting up PyTorch so that it can run with CUDA.</p>
<p>This completes this section and the content for this chapter, as well as the book. In the next section, we will look at the last set of exercises.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>The following is a mix of simple and very difficult exercises. Choose those exercises that you feel appropriate to your interests, abilities, and resources. Some of the exercises in the following list could require considerable resources, so pick those that are within your time/resource budget:</p>
<ol>
<li>Tune the hyperparameters for sample <kbd>Chapter_14_learn.py</kbd>.<strong> </strong>This sample is a standard deep learning model, but the parameters should be familiar enough to figure out on your own.</li>
<li>Tune the hyperparameters for sample <kbd>Chapter_14_MetaSGD-VPG.py</kbd>, as you normally would.</li>
<li>Tune the hyperparameters for sample <strong><kbd>Chapter_14_Imagination.py</kbd></strong>. There are a few new hyperparameters in this sample that you should familiarize yourself with.</li>
<li>Tune the hyperparameters for the <kbd>Chapter_14_wo_HER.py</kbd><span> and </span><kbd>Chapter_14_HER.py</kbd> examples. It can be very beneficial for your understanding to train the sample with and without HER using the same techniques.<strong> </strong></li>
<li>Tune the hyperparameters for the <kbd>Chapter_14_Imagine_A2C.py</kbd> example. What effect does this have on running the <kbd>Chapter_14_Imagination.py</kbd> example later?</li>
<li>Upgrade the HER example (<kbd>Chapter_14_HER.py</kbd>) to use a different PG or value/DQN method.</li>
<li>Upgrade the <kbd>Chapter_14_MetaSGD-VPG.py</kbd> example to use a more advanced PG or DQN method.</li>
<li>Adapt the <kbd>Chapter_14_MetaSGD-VPG.py</kbd> example to train on different environments that use continuous or possibly even discrete actions.</li>
</ol>
<ol start="9">
<li>Train the <kbd>Chapter_14_I2A.py </kbd>sample to completion. You will need to configure the example to run with CUDA, as well as install PyTorch with CUDA.</li>
<li>Tune the hyperparameters for the <kbd>Chapter_14_I2A.py </kbd>sample. You may decide to do only partial training runs using just the CPU, which is acceptable. Therefore, you could train a couple of iterations at a time and still optimize those new hyperparameters.  </li>
</ol>
<p>Do the exercises of most interest to you and remember to have fun. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked beyond DRL and into the realm of AGI, or at least where we hope we are going with AGI. More importantly, though, we looked at what the next phase of DRL is, how we can tackle its current shortcomings, and where it could go next. We looked at meta learning and what it means to learn to learn. Then we covered the excellent <kbd>learn2learn</kbd> library and saw how it could be used on a simple deep learning problem and then a more advanced meta-RL problem with MAML. From there, we looked at another new approach to learning using hindsight with HER. From hindsight, we moved to imagination and reasoning and how this could be incorporated into an agent. Then we finished the chapter by looking at I2A—imagination-augmented agents—and how imagination can help fill in the gaps in our knowledge.</p>
<p>I just want to thank you for taking the time to work through this book with us. It has been an amazing journey covering almost the entire RL and DRL alphabet of concepts, terms, and acronyms. This book started with the basics of RL and went deep, very deep, into DRL. Provided you have the mathematical background, you <span>can</span><span> </span><span>l</span><span>ikely venture out on your own now, and build your own latest and greatest agent. RL, and in particular DRL, suffers from the myth that you require extensive computational resources to make valuable contributions. While for certain research this is certainly the case, there are a lot of other more rudimentary elements that still need a better understanding that can be improved upon. The field of DRL is still relatively new, and it is quite likely that we have missed things along the way. Therefore, whatever your resources, you likely still could make a valuable contribution to DRL in the coming years. If you do plan to pursue this dream, I wish you success and hope that this book contributes to your journey.</span></p>


            </article>

            
        </section>
    </body></html>