- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Integration Pattern: Real-Time Intent Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we discussed the batch-processing integration pattern,
    where we focused on efficiently processing large volumes of data and generating
    data to be used by downstream systems. In this chapter, we will shift our focus
    to **real-time integration patterns**.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time interactions require applications to be optimized for latency, rather
    than processing large batch requests efficiently. In other words, we need to ensure
    that the output is generated as quickly as possible to provide an optimized user
    experience. The most common use case for this pattern is real-time agents exposed
    through chat or voice interfaces.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an intent classification use case, which is a common scenario
    for chatbots. In this context, an **artificial intelligence** (**AI**) system
    is responsible for identifying the user’s intent, such as checking a balance,
    scheduling an appointment, or making a purchase. Based on the identified intent,
    the system can then perform the appropriate tasks or provide relevant responses.
  prefs: []
  type: TYPE_NORMAL
- en: In today’s application experiences, customers expect seamless and personalized
    experiences when interacting with businesses. One way to achieve this is by implementing
    an intelligent system that can accurately interpret user intents based on natural
    language inputs. This capability is particularly valuable in customer service,
    e-commerce, and conversational AI applications, where understanding the user’s
    intent is crucial for providing relevant and contextual responses.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore a real-time intent classification use case, leveraging
    the power of Google’s **Gemini Pro**, *a state-of-the-art* generative AI model,
    to build a system that can accurately categorize user inputs into predefined intents.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll walk through the entire process, from data preparation to deployment and
    integration with downstream systems, following the integration framework discussed
    in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Use case definition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entry point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt pre-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result post-processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Result presentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s consider a scenario where we’re working with an e-commerce company that
    wants to improve its customer service experience. The company receives a large
    volume of customer inquiries through various channels, such as email, chat, and
    social media. Currently, these inquiries are handled manually by a team of customer
    service representatives, which can be time-consuming and prone to inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating intent classification into customer engagement flows, companies
    can optimize their customer service operations. This advanced natural language
    processing technique automatically categorizes incoming customer inquiries into
    predefined intents, such as “order status,” “product inquiry,” “return request,”
    or “general feedback.” The classification layer acts as an intelligent entry point
    for customer service interactions, enabling more efficient and accurate routing
    of inquiries.
  prefs: []
  type: TYPE_NORMAL
- en: This automated categorization serves as the foundation for a scalable customer
    service infrastructure. Once an inquiry is classified, it can be seamlessly directed
    to the most appropriate team or agent, ensuring that customers receive expert
    assistance tailored to their specific needs. For high-volume, straightforward
    inquiries, the system can even trigger automated responses, providing instant
    solutions to common issues. This not only dramatically improves response times
    but also enhances overall customer satisfaction by delivering quick, relevant
    assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the implementation of intent classification significantly improves
    the quality of life for customer service agents.
  prefs: []
  type: TYPE_NORMAL
- en: By receiving pre-categorized inquiries, organizations can leverage specialist
    agents to focus on their areas of expertise, reducing the cognitive load of constantly
    switching between different types of issues. This specialization allows agents
    to provide more in-depth, high-quality support, leading to better resolution rates
    and increased job satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: There is an additional benefit in terms of analytics, as the data gathered from
    intent classification can offer valuable insights into customer needs and pain
    points, enabling companies to continually refine their products, services, and
    support strategies to better meet customer expectations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will dive deep into an approach that will get you
    started on an intent classification example using GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To build our intent classification system, we’ll leverage a serverless, event-driven
    architecture built on **Google Cloud** (for example: [https://cloud.google.com/architecture/serverless-functions-blueprint](https://cloud.google.com/architecture/serverless-functions-blueprint)).
    This approach aligns with cloud-native principles and allows for seamless integration
    with other cloud services.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Intent classification example architecture diagram'
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture consists of the following key components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingestion layer**: This layer is responsible for accepting incoming user
    inputs from various channels, such as web forms, chat interfaces, or API endpoints.
    We’ll use **Google Cloud Functions** as the entry point for our system, which
    can be triggered by events from services like **Cloud Storage**, **Pub/Sub**,
    or **Cloud Run**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI processing layer**: In this layer, we’ll integrate Google’s Gemini Pro
    through **Vertex AI**. Vertex AI provides a managed environment for deploying
    and scaling machine learning models, ensuring high availability and performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intent classification model**: This is the core component of our system,
    responsible for analyzing the user input and determining the corresponding intent.
    We’ll leverage Google Gemini Pro’s natural language understanding capabilities
    for our intent classification model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orchestration and routing**: Based on the classified intent, we’ll need to
    route the user input to the appropriate downstream system or service. This could
    involve integrating with **customer relationship management** (**CRM**) systems,
    knowledge bases, or other enterprise applications. We’ll use Cloud Functions or
    Cloud Run to orchestrate this routing process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and logging**: To ensure the reliability and performance of our
    system, we’ll implement robust monitoring and logging mechanisms. We’ll leverage
    services like **Cloud Logging**, **Cloud Monitoring**, and **Cloud Operations**
    to gain visibility into our system’s behavior and quickly identify and resolve
    any issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adopting this architecture, the intent classification system won’t just be
    scalable but also flexible enough to adapt to varying workloads and integration
    requirements. We’ll be able to handle high volumes of customer inquiries in real
    time and deliver swift and consistent responses that improve the overall customer
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: The serverless nature of this architecture brings several additional benefits.
    It allows for automatic scaling based on demand, ensuring that we can handle sudden
    spikes in customer inquiries without manual intervention. This elasticity not
    only improves system reliability but also optimizes costs, as we only pay for
    the resources we actually use.
  prefs: []
  type: TYPE_NORMAL
- en: This event-driven design facilitates easy integration with other systems and
    services. As our customer service ecosystem evolves, we can easily add new triggers
    or outputs to our intent classification system.
  prefs: []
  type: TYPE_NORMAL
- en: This could include integrating with new communication channels, connecting to
    additional backend systems, or incorporating advanced analytics for deeper insights
    into customer behavior and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we’ll dive deeper into each component of our architecture,
    exploring the specific Google Cloud services we’ll use, best practices for implementation,
    and strategies for optimizing performance and cost-efficiency. We’ll also discuss
    a concrete example that will help you get started.
  prefs: []
  type: TYPE_NORMAL
- en: Entry point
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For real-time interactive applications, the entry points where prompts originate
    need to be highly streamlined, with simplicity and ease of use in mind. These
    prompts often originate from unpredictable contexts, so interfaces have to feel
    natural across device types and usage scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: In our use case, the entry point could be a web form, chat interface, or API
    endpoint where customers submit their inquiries. These inputs will be sent to
    a cloud function, which acts as the ingestion layer for our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a sample user query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Prompt pre-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a real-time system, every step in the prompt pre-processing workflow adds
    precious latency, commonly measured in milliseconds or microseconds depending
    on your application’s SLAs, to the overall response time. Higher-latency experiences
    can be detrimental to the user experience. Therefore, pre-processing should be
    kept as lightweight as possible.
  prefs: []
  type: TYPE_NORMAL
- en: For our intent classification use case, the prompt pre-processing may involve
    simple text normalization, such as removing punctuation, converting to lowercase,
    or handling abbreviations. Additionally, we may apply some basic filtering to
    remove any potentially harmful or inappropriate content before sending the prompt
    to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s dive deep into an example prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The previous prompt defines the template for the intent classification task.
    The prompt provides context that explains that the assistant is helping users
    of an online financial services company perform various actions, such as signing
    up, checking balances, investing in CDs, withdrawing funds, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this prompt instructs the model to carefully analyze the user’s
    input message and classify the intent into one of the predefined categories. For
    each intent category, the prompt specifies the JSON object that should be returned,
    including any additional information that needs to be extracted from the user’s
    message.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if the user’s intent is to invest in a CD, the assistant should
    return the JSON object in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This means that the virtual assistant should not only identify the intent as
    `"invest_cd"` but also extract relevant information like the investment amount
    and term from the user’s message and include it in the `"content"` field.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt also provides instructions for handling intents that do not fall
    into any of the predefined categories (the `"Other"` case).
  prefs: []
  type: TYPE_NORMAL
- en: By providing this detailed prompt template, the system can effectively guide
    the language model to perform the intent classification task for financial services
    scenarios, ensuring that the model’s responses are structured and formatted correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the inference stage, we’ll leverage Google’s Gemini Pro model hosted on Vertex
    AI. Within the cloud function triggered by the user input, we’ll invoke the Vertex
    AI endpoint hosting the Gemini Pro model, passing the pre-processed input as the
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gemini Pro will process the input and return the predicted intent, leveraging
    its natural language understanding capabilities. Since we’re using an out-of-the-box
    model, the underlying infrastructure and resource allocation are abstracted away,
    ensuring that individual requests are processed efficiently while adhering to
    the service’s performance and cost objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Result post-processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For our intent classification use case, the post-processing step may involve
    formatting the predicted intent into a suitable response format, such as JSON
    or a human-readable string. Additionally, we may apply some basic filtering or
    ranking mechanisms to ensure that the most relevant and helpful responses are
    prioritized.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]json")'
  prefs: []
  type: TYPE_NORMAL
- en: end_index = text.find("[PRE5]json"
  prefs: []
  type: TYPE_NORMAL
- en: 'if start_index != -1 and end_index != -1:'
  prefs: []
  type: TYPE_NORMAL
- en: 'json_string = text[start_index + 7: end_index]  # Extract the JSON string'
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: json_string = text
  prefs: []
  type: TYPE_NORMAL
- en: 'try:'
  prefs: []
  type: TYPE_NORMAL
- en: json_data = json.loads(json_string)
  prefs: []
  type: TYPE_NORMAL
- en: return json_data
  prefs: []
  type: TYPE_NORMAL
- en: 'except json.JSONDecodeError:'
  prefs: []
  type: TYPE_NORMAL
- en: return None
  prefs: []
  type: TYPE_NORMAL
- en: 'extract_json that is designed to handle cases where the language model’s output
    contains JSON data wrapped in backticks: json[PRE6]'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `extract_json` function takes a string text as input and attempts to extract
    the JSON portion from within the backticks. Here’s a breakdown of how the function
    works:'
  prefs: []
  type: TYPE_NORMAL
- en: The function first looks for the string [PRE7]json"[PRE8]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'def process_intent(intent):'
  prefs: []
  type: TYPE_NORMAL
- en: 'if intent["intent"] == "signup":'
  prefs: []
  type: TYPE_NORMAL
- en: '#If a user is trying to sign up you could'
  prefs: []
  type: TYPE_NORMAL
- en: '#redirect the to a sign up page for example.'
  prefs: []
  type: TYPE_NORMAL
- en: return("Sign up process")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "change_password":'
  prefs: []
  type: TYPE_NORMAL
- en: '#If a user is looking into changing their password,'
  prefs: []
  type: TYPE_NORMAL
- en: '#you could either do it through the chatbot,'
  prefs: []
  type: TYPE_NORMAL
- en: '#or redirect to a password change page.'
  prefs: []
  type: TYPE_NORMAL
- en: return("Change password")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "check_balance":'
  prefs: []
  type: TYPE_NORMAL
- en: '#In this case you could have a function that'
  prefs: []
  type: TYPE_NORMAL
- en: '#would query a database to obtain the'
  prefs: []
  type: TYPE_NORMAL
- en: '#balance (as long as the user is logged in or not)'
  prefs: []
  type: TYPE_NORMAL
- en: return("Check account balance")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "invest_cd":'
  prefs: []
  type: TYPE_NORMAL
- en: '#For the investment intent, this could redirect'
  prefs: []
  type: TYPE_NORMAL
- en: '#to a page where investment options can be selected.'
  prefs: []
  type: TYPE_NORMAL
- en: return("Invest in a CD")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "withdraw_funds":'
  prefs: []
  type: TYPE_NORMAL
- en: return("Withdraw funds")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "transfer_funds":'
  prefs: []
  type: TYPE_NORMAL
- en: return("Transfer funds")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "account_info":'
  prefs: []
  type: TYPE_NORMAL
- en: return("Account information")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "lost_card":'
  prefs: []
  type: TYPE_NORMAL
- en: return("Report lost card")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "support":'
  prefs: []
  type: TYPE_NORMAL
- en: return("Contact support")
  prefs: []
  type: TYPE_NORMAL
- en: 'elif intent["intent"] == "other":'
  prefs: []
  type: TYPE_NORMAL
- en: return("Other kind of intent")
  prefs: []
  type: TYPE_NORMAL
- en: 'else:'
  prefs: []
  type: TYPE_NORMAL
- en: return("If a intent was classified as something else you should investigate
    what is going on.")
  prefs: []
  type: TYPE_NORMAL
- en: intent = process_intent(extract_json(result.text))
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: process_intent function can be extended or modified to include additional logic
    or actions based on the specific requirements of the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: import gradio as gr
  prefs: []
  type: TYPE_NORMAL
- en: 'def chat(message, history):'
  prefs: []
  type: TYPE_NORMAL
- en: response = generate(prompt_template.format(query=message))
  prefs: []
  type: TYPE_NORMAL
- en: intent_action = process_intent(extract_json(response.text))
  prefs: []
  type: TYPE_NORMAL
- en: history.append((message, intent_action))
  prefs: []
  type: TYPE_NORMAL
- en: return "", history
  prefs: []
  type: TYPE_NORMAL
- en: 'with gr.Blocks() as demo:'
  prefs: []
  type: TYPE_NORMAL
- en: gr.Markdown("Fintech Assistant")
  prefs: []
  type: TYPE_NORMAL
- en: chatbot = gr.Chatbot(show_label=False)
  prefs: []
  type: TYPE_NORMAL
- en: message = gr.Textbox(placeholder="Enter your question")
  prefs: []
  type: TYPE_NORMAL
- en: message.submit(chat, [message, chatbot],[message, chatbot]  )
  prefs: []
  type: TYPE_NORMAL
- en: demo.launch(debug=True)
  prefs: []
  type: TYPE_NORMAL
- en: '```'
  prefs: []
  type: TYPE_NORMAL
- en: The previous code illustrates the result presentation stage for the intent classification
    system using the Gradio library.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the `chat(message, history)` function is the core of the chatbot
    interface. It takes two arguments: `message` (the user’s input message) and `history`
    (a list containing the previous messages and responses). Here’s what the function
    does:'
  prefs: []
  type: TYPE_NORMAL
- en: It calls the `generate` function (not shown in the provided code) to get the
    response from the intent classification model, passing the user’s message as part
    of the prompt template. It then processes the model’s response using the `extract_json`
    function (not shown) to extract the predicted intent data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The extracted intent data is passed to the `process_intent` function (which
    is not shown) to determine the appropriate action or response based on the predicted
    intent. The user’s message and the generated response are appended to the history
    list, which keeps track of the conversation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The function returns an empty string for the response message and the updated
    history list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code then creates a Gradio interface using the `gr.Blocks` context manager.
    Inside the context, it does the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Displays a title using the gr.Markdown component.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates a `gr.Chatbot` component to display the conversation history.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Creates a `gr.Textbox` component for the user to enter their message.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Binds the chat function to the `submit` event of the `Textbox` component. When
    the user submits their message, the chat function is called with the user’s message
    and the current history as arguments.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates the `Textbox` and `Chatbot` components with the new message and updated
    history, respectively.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Launches the Gradio interface in debug mode using `demo.launch(debug=True)`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is an interactive chatbot interface where users can enter their messages
    as illustrated in *Figure 7.2*, and the system will process the message, predict
    the intent, and provide an appropriate response based on the `process_intent`
    function. The conversation history is displayed in the `Chatbot` component, allowing
    users to track the flow of the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Example Gradio interface'
  prefs: []
  type: TYPE_NORMAL
- en: Logging and monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-time systems require tight instrumentation around per-request metrics,
    such as latencies, errors, and resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: In our architecture, we’ll leverage services like Cloud Logging ([https://cloud.google.com/logging/docs/overview](https://cloud.google.com/logging/docs/overview))
    and Cloud Monitoring ([https://cloud.google.com/monitoring/docs/monitoring-overview](https://cloud.google.com/monitoring/docs/monitoring-overview))
    to gain visibility into the system’s behavior and quickly identify and resolve
    any issues. We can monitor metrics like request latency, error rates, and resource
    utilization, and set up alerts for anomalies or performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: By following this integration pattern and leveraging the power of Google’s Gemini
    Pro, businesses can unlock the power of generative AI to build intelligent systems
    that accurately classify user intents, enhance customer experiences, and streamline
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the GitHub directory of this chapter for the complete code that demonstrates
    how all the pieces described above fit together.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the integration pattern for building a real-time
    intent classification system using Google’s Gemini Pro generative AI model. We
    started by introducing the concept of real-time integration patterns, which prioritize
    low latency over efficiency and volume, as opposed to batch-processing integration
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The use case we developed is an e-commerce company that wants to improve its
    customer service experience by automatically categorizing incoming customer inquiries
    into predefined intents, such as order status, product inquiry, return request,
    or general feedback. This classification can then be used to route the inquiry
    to the appropriate team or provide automated responses for common issues.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture proposed is a serverless, event-driven architecture on Google
    Cloud, consisting of an ingestion layer (Cloud Functions), an AI processing layer
    (Vertex AI with Gemini Pro), an intent classification model, orchestration and
    routing (Cloud Functions or Cloud Run), and monitoring and logging (Cloud Logging,
    Cloud Monitoring, and Cloud Operations).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive deep into another very important real-time
    use case, a **Retrieval Augmented Generation** (**RAG**) example where we are
    going to leverage generative AI to answer questions based on documents provided
    by us.
  prefs: []
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genpat](Chapter_07.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code134841911667913109.png)'
  prefs: []
  type: TYPE_IMG
