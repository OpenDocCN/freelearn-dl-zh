<html><head></head><body>
<div id="_idContainer301">
<h1 class="chapter-number" id="_idParaDest-157"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-158"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.2.1">Creating Single- and Multi-Agent Systems</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In previous chapters, we discussed a number of components or tools that can be associated with LLMs to extend their capabilities. </span><span class="koboSpan" id="kobo.3.2">In </span><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapters 5</span></em><span class="koboSpan" id="kobo.5.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.6.1">6</span></em><span class="koboSpan" id="kobo.7.1">, we addressed in detail how external memory can be used to enrich the context. </span><span class="koboSpan" id="kobo.7.2">This allows the model to obtain additional information to be able to answer user questions when it does not know the answer (when it hasn’t seen the document during pre-training or it relates to information after the date of their training). </span><span class="koboSpan" id="kobo.7.3">Similarly, in </span><a href="B21257_07.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.9.1">, we saw that knowledge graphs can be used to extend the model’s knowledge. </span><span class="koboSpan" id="kobo.9.2">These components attempt to solve one of the most problematic limitations of LLMs, namely, hallucinations (an output produced by the model that is not factually correct). </span><span class="koboSpan" id="kobo.9.3">In addition, we saw that the use of graphs allows the model to conduct graph reasoning and thus adds </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">new capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">In </span><a href="B21257_08.xhtml#_idTextAnchor137"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.13.1">, we saw the intersection of RL and LLMs. </span><span class="koboSpan" id="kobo.13.2">One of the problems associated with LLMs is that they could produce harmful content (such as biased or toxic content or misinformation). </span><span class="koboSpan" id="kobo.13.3">RL algorithms allow us to align the behavior of the model with human preferences, thus allowing us to reduce the risk of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">harmful content.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">We can use similar approaches to make the model more capable of performing tasks or following instructions. </span><span class="koboSpan" id="kobo.15.2">In the future, these reinforcement learning algorithms could be useful in overcoming an important limitation of LLMs: a lack of </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">continual learning.</span></span></p>
<p><span class="koboSpan" id="kobo.17.1">The definition of tools, as we will see, is quite broad. </span><span class="koboSpan" id="kobo.17.2">In fact, any software or algorithm can be a tool. </span><span class="koboSpan" id="kobo.17.3">As we have already seen in previous chapters, LLMs can execute code or connect to </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">application programming Interfaces</span></strong><span class="koboSpan" id="kobo.19.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.20.1">APIs</span></strong><span class="koboSpan" id="kobo.21.1">). </span><span class="koboSpan" id="kobo.21.2">But this means that they can also invoke other models to perform tasks that they are unable to accomplish on </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">their own.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">In any case, all these elements have set the seed for what is called the agent revolution, in which an LLM can interact with the environment and perform tasks in the real world (be it the internet or, in the future, beyond the constraint of </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">a computer).</span></span></p>
<p><span class="koboSpan" id="kobo.25.1">In this chapter, we focus on LLMs, its various tools, and how these can be combined to interact with the environment. </span><span class="koboSpan" id="kobo.25.2">We will start with the definition of an autonomous agent and continue with what the tools (APIs, models, and so on) are and how they can be organized. </span><span class="koboSpan" id="kobo.25.3">We will see how using prompt engineering techniques (which we addressed in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.26.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.27.1">) allows us to create different types of agents. </span><span class="koboSpan" id="kobo.27.2">After that, we will discuss several strategies that have been used previously in the literature to connect an LLM to </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">its tools.</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">This will allow us to see in detail how some technical limitations and challenges have been solved. </span><span class="koboSpan" id="kobo.29.2">We will then talk in detail about HuggingGPT (an LLM connected to hundreds of models), which was a turning point in agent creation. </span><span class="koboSpan" id="kobo.29.3">We will see how HuggingGPT allows an LLM to solve complex tasks using other expert models. </span><span class="koboSpan" id="kobo.29.4">Then, we will see how instead of a single agent, we can create multi-agent platforms. </span><span class="koboSpan" id="kobo.29.5">The interaction of different agents will allow us to solve increasingly complex tasks and issues. </span><span class="koboSpan" id="kobo.29.6">In addition, we will see how these approaches can be applied to complex domains, such as healthcare, chemistry, and law. </span><span class="koboSpan" id="kobo.29.7">We will then put what we have seen into practice using HuggingGPT. </span><span class="koboSpan" id="kobo.29.8">Next, we will extend this concept with a multi-agent platform that will allow us to understand how modern </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">systems work.</span></span></p>
<p><span class="koboSpan" id="kobo.31.1">Once we have seen how agents or multi-agents work, we will discuss in detail the new business paradigms that are emerging, such as </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">Software as a Service</span></strong><span class="koboSpan" id="kobo.33.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.34.1">SaaS</span></strong><span class="koboSpan" id="kobo.35.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">Model as a Service</span></strong><span class="koboSpan" id="kobo.37.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.38.1">MaaS</span></strong><span class="koboSpan" id="kobo.39.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">Data as a Service</span></strong><span class="koboSpan" id="kobo.41.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.42.1">DaaS</span></strong><span class="koboSpan" id="kobo.43.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.44.1">Results as a Service</span></strong><span class="koboSpan" id="kobo.45.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.46.1">RaaS</span></strong><span class="koboSpan" id="kobo.47.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">Outcome as a Service</span></strong><span class="koboSpan" id="kobo.49.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.50.1">OaaS</span></strong><span class="koboSpan" id="kobo.51.1">). </span><span class="koboSpan" id="kobo.51.2">As we will see in this chapter, each of these business models has advantages </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">and disadvantages.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.55.1">Introduction to </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">autonomous agents</span></span></li>
<li><span class="koboSpan" id="kobo.57.1">HuggingGPT and </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">other approaches</span></span></li>
<li><span class="koboSpan" id="kobo.59.1">Working </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">with HuggingGPT</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.61.1">Multi-agent system</span></span></li>
<li><span class="koboSpan" id="kobo.62.1">SaaS, MaaS, DaaS, </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">and RaaS</span></span></li>
</ul>
<h1 id="_idParaDest-159"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.64.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.65.1">The code in this chapter requires the use of a GPU. </span><span class="koboSpan" id="kobo.65.2">For the section on using HuggingGPT in particular, both a GPU and plenty of space on the hard disk drive are required (several models will be downloaded, including diffusion models. </span><span class="koboSpan" id="kobo.65.3">For this, it will be necessary to use Git </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">Large File Storage</span></strong><span class="koboSpan" id="kobo.67.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">LFS</span></strong><span class="koboSpan" id="kobo.69.1">), which allows downloading wide files via Git). </span><span class="koboSpan" id="kobo.69.2">Anaconda should be installed to obtain the various libraries (the necessary libraries will be set up directly during installation). </span><span class="koboSpan" id="kobo.69.3">For readers who do not have these resources, the </span><em class="italic"><span class="koboSpan" id="kobo.70.1">Using HuggingGPT on the web</span></em><span class="koboSpan" id="kobo.71.1"> section shows how you can use HuggingGPT on the web. </span><span class="koboSpan" id="kobo.71.2">For local use of HuggingGPT, it is necessary to have an OpenAI token, while for web use, it is also necessary to have a Hugging Face token. </span><span class="koboSpan" id="kobo.71.3">The multi-agent system is based on Python libraries (NumPy, scikit-learn, SentenceTransformers, </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">and Transformers).</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">HuggingGPT should be run on a GPU. </span><span class="koboSpan" id="kobo.73.2">The multi-agent system should be run on a GPU, but it could also be run on a CPU; this is, however, highly discouraged. </span><span class="koboSpan" id="kobo.73.3">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr9"><span class="No-Break"><span class="koboSpan" id="kobo.75.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr9</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.76.1">.</span></span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.77.1">Introduction to autonomous agents</span></h1>
<p><span class="koboSpan" id="kobo.78.1">In the context of AI, </span><strong class="bold"><span class="koboSpan" id="kobo.79.1">autonomous agents</span></strong><span class="koboSpan" id="kobo.80.1"> refer to </span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.81.1">systems or entities that can perform tasks or make decisions independently without the need for human intervention. </span><span class="koboSpan" id="kobo.81.2">These agents are designed to perceive their environment, reason about it, make decisions based on their goals, and take action accordingly to achieve those goals. </span><span class="koboSpan" id="kobo.81.3">Autonomous agents are considered an important step toward </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">artificial general intelligence</span></strong><span class="koboSpan" id="kobo.83.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.84.1">AGI</span></strong><span class="koboSpan" id="kobo.85.1">), which</span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.86.1"> is expected to conduct autonomous planning </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">and actions.</span></span></p>
<p><span class="koboSpan" id="kobo.88.1">The main reason for using LLMs as agents lies in the fact that LLMs have shown some reasoning and thus planning capabilities. </span><span class="koboSpan" id="kobo.88.2">LLMs use reasoning to interpret input, draw inferences, and make decisions (showing some extent of deductive, inductive, and abductive reasoning). </span><span class="koboSpan" id="kobo.88.3">This allows LLMs to apply general rules to specific cases (deductive reasoning), learn patterns from examples (inductive reasoning), and infer explanations from incomplete data (abductive reasoning). </span><span class="koboSpan" id="kobo.88.4">In addition, LLMs are capable of conducting step reasoning by chaining ideas, thus enabling them to be able to solve equations or debug code. </span><span class="koboSpan" id="kobo.88.5">Also, solving some problems (such as math problems) requires following a series of steps. </span><span class="koboSpan" id="kobo.88.6">Intrinsically, an LLM must often decompose a task into a series of actions, anticipate the results of these actions, and adjust its behavior in response to the results. </span><span class="koboSpan" id="kobo.88.7">These capabilities, however, are limited to the context provided by the user or knowledge gained during pre-training, and for fields such as medicine or finance, this is not enough to solve most problems. </span><span class="koboSpan" id="kobo.88.8">Therefore, the natural response to this limitation is to extend the capabilities of the LLM with external tools, or otherwise connect an LLM to the </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">external environment.</span></span></p>
<p><span class="koboSpan" id="kobo.90.1">The purpose of some studies and research is therefore to extend the capabilities of LLMs with a set of tools. </span><span class="koboSpan" id="kobo.90.2">These works and derived libraries try to equip LLMs with human capabilities, such as memory and planning, to make them behave like humans and complete various </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">tasks effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">As the capabilities of LLMs have </span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.93.1">developed, interest in these agents has grown, and numerous articles and frameworks have </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">been published.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer259">
<span class="koboSpan" id="kobo.95.1"><img alt="Figure 9.1 – Growing interest in LLM autonomous agents (https://arxiv.org/pdf/2308.11432)" src="image/B21257_09_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.96.1">Figure 9.1 – Growing interest in LLM autonomous agents (</span><a href="https://arxiv.org/pdf/2308.11432"><span class="koboSpan" id="kobo.97.1">https://arxiv.org/pdf/2308.11432</span></a><span class="koboSpan" id="kobo.98.1">)</span></p>
<p><span class="koboSpan" id="kobo.99.1">The first aspect to consider when building these types of systems is the design of the architecture and how to use it to perform tasks. </span><span class="koboSpan" id="kobo.99.2">Autonomous agents must perform different roles, perceive the environment, and learn from it. </span><span class="koboSpan" id="kobo.99.3">The purpose of the architecture is to assist an LLM in maximizing its capabilities in order to be used as an agent. </span><span class="koboSpan" id="kobo.99.4">To this end, several modules have been developed, which can be divided into four main groups: profiling, memory, planning, </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">and action.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer260">
<span class="koboSpan" id="kobo.101.1"><img alt="Figure 9.2 – Possible modules to build LLM-based autonomous agents (https://arxiv.org/pdf/2308.11432)" src="image/B21257_09_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.102.1">Figure 9.2 – Possible modules to build LLM-based autonomous agents (</span><a href="https://arxiv.org/pdf/2308.11432"><span class="koboSpan" id="kobo.103.1">https://arxiv.org/pdf/2308.11432</span></a><span class="koboSpan" id="kobo.104.1">)</span></p>
<p><span class="koboSpan" id="kobo.105.1">Let’s go through each of these in a bit </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">more detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.107.1">Profiling module</span></strong><span class="koboSpan" id="kobo.108.1">: Often, agents</span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.109.1"> perform tasks in specific roles (also called personas), such as coders, domain experts, teachers, or assistants. </span><span class="koboSpan" id="kobo.109.2">The profiling module deals with defining these roles (characteristics, role, psychological and social information, and relationships with other agents) in a specific prompt given to the LLM. </span><span class="koboSpan" id="kobo.109.3">These profiles can then be handwritten (handwritten profiles are manually crafted personas or roles defined by developers or domain experts); for example, for a system for software development, we can create different job roles (“you are a software engineer responsible for code review”). </span><span class="koboSpan" id="kobo.109.4">Handwritten profiles allow a high degree of control, enriching context, and can be highly domain-specific (addressing nuances, soft skills, sophisticated knowledge). </span><span class="koboSpan" id="kobo.109.5">Although the handwritten approach is very flexible, it is time-consuming and has limited scalability. </span><span class="koboSpan" id="kobo.109.6">So, some studies have explored systems where LLMs automatically generate profiles (using few-shot examples, rules, and templates, or specific external datasets as job descriptions). </span><span class="koboSpan" id="kobo.109.7">This approach is much more scalable and adaptable to different situations (especially if the system is to be dynamic or if feedback is received from users). </span><span class="koboSpan" id="kobo.109.8">On the other hand, however, there is less control (the system loses nuance and depth, with the risk of being generic), the quality is variable (depending on the prompt engineering technique, some examples might be of poor quality), and it still requires verification by </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">a</span></span><span class="No-Break"><a id="_idIndexMarker1078"/></span><span class="No-Break"><span class="koboSpan" id="kobo.111.1"> human.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Memory module</span></strong><span class="koboSpan" id="kobo.113.1">: The memory module</span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.114.1"> stores information perceived by the system from the environment or other sources; these memories then facilitate future actions. </span><span class="koboSpan" id="kobo.114.2">Dedicated memory components can also be sophisticated and inspired by human cognition, with components dedicated to perceptual, short- or long-term information. </span><span class="koboSpan" id="kobo.114.3">Commonly found memories are then entered into the system prompt (so the context length of the LLM is the limit for the memory that can be used for the agent). </span><span class="koboSpan" id="kobo.114.4">An example is the history of chats with a user that is needed for task accomplishment. </span><span class="koboSpan" id="kobo.114.5">As another example, an agent assisting in the development of a game will have just-occurring events and other descriptions as short-term memory. </span><strong class="bold"><span class="koboSpan" id="kobo.115.1">Hybrid memory</span></strong><span class="koboSpan" id="kobo.116.1"> is a </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.117.1">way of extending memory, where past events and thoughts are saved and found again to facilitate the agent’s behavior. </span><span class="koboSpan" id="kobo.117.2">Hybrid memory combines short-term (within an LLM context) and long-term (external) memory to extend the agent’s capacity beyond the LLM’s context window. </span><span class="koboSpan" id="kobo.117.3">These thoughts, conversations, or other information can be saved via RAG or other systems (database, knowledge graph, and so on). </span><span class="koboSpan" id="kobo.117.4">When needed, relevant information is retrieved and injected into the LLM prompt, allowing the agent to act on prior knowledge without exceeding context limits. </span><span class="koboSpan" id="kobo.117.5">For example, in RAG, a search mechanism pulls relevant documents or memory fragments based on the current query, making responses more informed and consistent over time. </span><span class="koboSpan" id="kobo.117.6">In addition, this module should cover three operations: memory reading (extracting useful information for the agent’s action), memory writing (storing information about the environment that may be useful in the future while avoiding duplicates and memory overflow), and memory reflection (evaluating and inferring more abstract, complex, and high-level information). </span><span class="koboSpan" id="kobo.117.7">Specifically, memory reading retrieves information to support the agent’s decisions (increasing context continuity and consistency), memory writing allows for saving information that is useful for the agent’s interaction with the environment (thus reducing redundancy and allowing for overcoming the limitations of a noneditable memory), and memory </span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.118.1">reflection allows for deriving insights from the analysis of stored information, thus allowing for adjusting behavior to </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">achieve goals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.120.1">Planning module</span></strong><span class="koboSpan" id="kobo.121.1">: The planning module</span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.122.1"> is generally used to deconstruct complex tasks into more manageable tasks, to make LLMs behave more reasonably, powerfully, and reliably. </span><span class="koboSpan" id="kobo.122.2">The planning module can include or not </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">include feedback.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.124.1">In planning without feedback, the agent does not receive feedback that influences its future behavior after it has conducted an action. </span><span class="koboSpan" id="kobo.124.2">In single-path reasoning, the task is divided into several intermediate steps connected in a cascading sequence. </span><strong class="bold"><span class="koboSpan" id="kobo.125.1">Chain of thought</span></strong><span class="koboSpan" id="kobo.126.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.127.1">CoT</span></strong><span class="koboSpan" id="kobo.128.1">) reasoning is often employed to develop a step-by-step plan for this</span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.129.1"> strategy. </span><span class="koboSpan" id="kobo.129.2">In contrast, multi-path reasoning involves a tree-like structure where each intermediate step can branch into multiple subsequent steps. </span><span class="koboSpan" id="kobo.129.3">These approaches typically </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.130.1">leverage </span><strong class="bold"><span class="koboSpan" id="kobo.131.1">self-consistent CoT</span></strong><span class="koboSpan" id="kobo.132.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.133.1">CoT-SC</span></strong><span class="koboSpan" id="kobo.134.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.135.1">tree of thoughts</span></strong><span class="koboSpan" id="kobo.136.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.137.1">ToT</span></strong><span class="koboSpan" id="kobo.138.1">) frameworks, enabling the evaluation of all intermediate steps to </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.139.1">identify the optimal strategy. </span><span class="koboSpan" id="kobo.139.2">The tree can be even coupled with sophisticated strategies such as </span><strong class="bold"><span class="koboSpan" id="kobo.140.1">Monte Carlo Tree Search</span></strong><span class="koboSpan" id="kobo.141.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.142.1">MCTS</span></strong><span class="koboSpan" id="kobo.143.1">) or an</span><a id="_idIndexMarker1086"/> <span class="No-Break"><span class="koboSpan" id="kobo.144.1">external planner.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.145.1">Planning with feedback is mainly used for long-term tasks, where it is difficult to generate an effective plan from the beginning or the dynamics may change. </span><span class="koboSpan" id="kobo.145.2">So, you can incorporate feedback from the environment and observations. </span><span class="koboSpan" id="kobo.145.3">For example, the ReAct framework uses thought-act-observation triplets. </span><span class="koboSpan" id="kobo.145.4">Another alternative is using human feedback or another model to improve the agent’s planning ability. </span></p></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer261">
<span class="koboSpan" id="kobo.146.1"><img alt="Figure 9.3 – Comparison between the strategies of single-path and multi-path reasoning (https://arxiv.org/pdf/2308.11432)" src="image/B21257_09_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.147.1">Figure 9.3 – Comparison between the strategies of single-path and multi-path reasoning (</span><a href="https://arxiv.org/pdf/2308.11432"><span class="koboSpan" id="kobo.148.1">https://arxiv.org/pdf/2308.11432</span></a><span class="koboSpan" id="kobo.149.1">)</span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Action module</span></strong><span class="koboSpan" id="kobo.151.1">: The action module</span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.152.1"> is responsible for translating the planning into a specific outcome; this module is then responsible for the interaction. </span><span class="koboSpan" id="kobo.152.2">In general, this module focuses on the execution of the task and then actions with a specific goal. </span><span class="koboSpan" id="kobo.152.3">The module is also responsible for communicating with other agents (if they are present), exploring the environment, finding the necessary memory, and executing the plan. </span><span class="koboSpan" id="kobo.152.4">To accomplish these goals, the LLM can use either the knowledge gained from the LLM during the pre-training phase or external tools (external models, APIs, databases, or other tools). </span><span class="koboSpan" id="kobo.152.5">Pre-training knowledge allows the LLM to carry out many tasks using learned information, such as generating text, answering questions, or making decisions based on prior data. </span><span class="koboSpan" id="kobo.152.6">However, for more dynamic, real-time, or specialized tasks, the action module uses external tools such as APIs, databases, software applications, or other models. </span><span class="koboSpan" id="kobo.152.7">These tools enable the agent to access up-to-date information, manipulate data, perform calculations, or trigger operations in external systems. </span><span class="koboSpan" id="kobo.152.8">Together, pre-trained knowledge and external tools allow the agent to interact meaningfully with its environment, carry out goals, and adapt based on the outcomes of its actions. </span><span class="koboSpan" id="kobo.152.9">The action of the model has an impact on the environment or internal state of the model, and this is evaluated and taken into account by </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">this</span></span><span class="No-Break"><a id="_idIndexMarker1088"/></span><span class="No-Break"><span class="koboSpan" id="kobo.154.1"> module.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.155.1">Apart from system architecture, we should also consider strategies to develop better agents. </span><span class="koboSpan" id="kobo.155.2">Typically, one of the most used strategies is conducting fine-tuning of the model. </span><span class="koboSpan" id="kobo.155.3">Fine-tuning plays a key role in improving agent performance by adapting a general-purpose LLM to specific tasks, domains, or behavioral goals. </span><span class="koboSpan" id="kobo.155.4">It helps align the model with human values (safety), improve instruction following, or specialize in areas such as education or e-commerce. </span><span class="koboSpan" id="kobo.155.5">In most cases, human-annotated datasets are used for specific tasks. </span><span class="koboSpan" id="kobo.155.6">As we discussed in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.156.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.157.1">, this can be for security reasons (alignment with human values), to make it more responsive to following instructions (instruction tuning), or to train to a specific domain or task. </span><span class="koboSpan" id="kobo.157.2">To fine-tune an</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.158.1"> agent, in the WebShop example (</span><a href="https://arxiv.org/pdf/2308.11432"><span class="koboSpan" id="kobo.159.1">https://arxiv.org/pdf/2308.11432</span></a><span class="koboSpan" id="kobo.160.1">), the authors of the paper collected 1.2 million world products from </span><a href="http://amazon.com"><span class="koboSpan" id="kobo.161.1">amazon.com</span></a><span class="koboSpan" id="kobo.162.1"> and created a simulated e-commerce website. </span><span class="koboSpan" id="kobo.162.2">After that, they collected human behaviors on the website (when users browse and perform actions on the website, their behaviors are registered), thus creating a dataset for fine-tuning specifically for an agent dedicated to helping with product selection. </span><span class="koboSpan" id="kobo.162.3">Or, in the EduChat example (</span><a href="https://arxiv.org/pdf/2308.11432"><span class="koboSpan" id="kobo.163.1">https://arxiv.org/pdf/2308.11432</span></a><span class="koboSpan" id="kobo.164.1">), to </span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.165.1">create an agent for educational scenarios, the authors collected an annotated dataset covering various educational scenarios (the dataset was evaluated and edited by specialized personnel, such </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">as psychologists).</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">Collecting these datasets is expensive and requires specialized personnel in several cases. </span><span class="koboSpan" id="kobo.167.2">Therefore, an alternative is to use an LLM to annotate the dataset. </span><span class="koboSpan" id="kobo.167.3">When this approach is followed, there is a trade-off between quality and cost: the dataset is not as good as that annotated by humans, but the costs are much reduced. </span><span class="koboSpan" id="kobo.167.4">For example, in ToolBench (an agent system where the LLM is connected to APIs), the authors of that work (</span><a href="https://arxiv.org/pdf/2308.11432"><span class="koboSpan" id="kobo.168.1">https://arxiv.org/pdf/2308.11432</span></a><span class="koboSpan" id="kobo.169.1">) collected</span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.170.1"> more than 16,000 real-world APIs and then annotated this dataset with ChatGPT. </span><span class="koboSpan" id="kobo.170.2">Then, they fine-tuned LLaMA on this dataset. </span><span class="koboSpan" id="kobo.170.3">The fine-tuned model was much more performant in using </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">these APIs.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer262">
<span class="koboSpan" id="kobo.172.1"><img alt="Figure 9.4 – Construction of ToolBench (https://arxiv.org/pdf/2307.16789)" src="image/B21257_09_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.173.1">Figure 9.4 – Construction of ToolBench (</span><a href="https://arxiv.org/pdf/2307.16789"><span class="koboSpan" id="kobo.174.1">https://arxiv.org/pdf/2307.16789</span></a><span class="koboSpan" id="kobo.175.1">)</span></p>
<p><span class="koboSpan" id="kobo.176.1">Alternatively, you can collect a large amount of data that is not annotated, so that the model figures out on its own during fine-tuning . </span><span class="koboSpan" id="kobo.176.2">For example, Mind2Web</span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.177.1"> collected a large amount of data for web </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">browsing (</span></span><a href="https://arxiv.org/abs/2306.06070"><span class="No-Break"><span class="koboSpan" id="kobo.179.1">https://arxiv.org/abs/2306.06070</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.180.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.181.1">The trade-off between annotated and self-annotated datasets is that LLM-labeled data may lack the accuracy, nuance, or reliability of human annotation, potentially affecting performance. </span><span class="koboSpan" id="kobo.181.2">Still, it allows broader coverage and faster iteration. </span><span class="koboSpan" id="kobo.181.3">In practice, combining both methods—using LLMs for bulk labeling and humans for validation or high-stakes tasks—offers a balance between quality and cost, making fine-tuning more accessible while still enhancing </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">agent capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.183.1">Because interactions with the model are typically conducted with the prompt, many developers simply use prompt engineering without the need for fine-tuning. </span><span class="koboSpan" id="kobo.183.2">The rationale is that the necessary knowledge already exists in the parameters of the LLM and we want to use a prompt that allows the model to use it to its best advantage. </span><span class="koboSpan" id="kobo.183.3">Other approaches add agents that act as critics, other agents that debate, or </span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">other variations.</span></span></p>
<p><span class="koboSpan" id="kobo.185.1">What we have seen so far enables us to understand what an autonomous agent is and how it is composed. </span><span class="koboSpan" id="kobo.185.2">As we have seen, an agent has, at its core, an LLM and a sophisticated ecosystem around it that can be composed of different elements as the researcher chooses. </span><span class="koboSpan" id="kobo.185.3">In the following sections, we will look in detail at different approaches to autonomous agents that allow us to understand some of the solutions that have been implemented in </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">the literature.</span></span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.187.1">Toolformer</span></h2>
<p><span class="koboSpan" id="kobo.188.1">Toolformer (Schick, 2023) is </span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.189.1">a pioneering work using the idea that an LLM can access external tools to solve tasks (search engines, calculators, and calendars) without sacrificing their generality or requiring large-scale human annotation. </span><span class="koboSpan" id="kobo.189.2">The key innovation of Toolformer lies in treating tool use as a generalizable skill, not bound to a specific task. </span><span class="koboSpan" id="kobo.189.3">Rather than designing separate systems for each tool or task, Toolformer teaches the model to make intelligent decisions about which tool to use, when to use it, and how to use it, all within a unified language </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">modeling framework.</span></span></p>
<p><span class="koboSpan" id="kobo.191.1">According to the authors, an LLM should learn the use of tools according to two principles: in a self-supervised way and preserving the generality of the model. </span><span class="koboSpan" id="kobo.191.2">Toolformer is designed to learn in a largely self-supervised manner, addressing a major bottleneck in AI development: the cost and effort of human-labeled data. </span><span class="koboSpan" id="kobo.191.3">Instead of manually annotating data with tool usage, the model is shown a few examples of how tools (API calls) work. </span><span class="koboSpan" id="kobo.191.4">It then automatically annotates a large, unlabeled dataset with tool-use opportunities during language modeling. </span><span class="koboSpan" id="kobo.191.5">These annotated sequences are used to fine-tune the model, enabling it to learn tool interactions naturally. </span><span class="koboSpan" id="kobo.191.6">This is important because there is a cost associated with annotating a dataset, but it also teaches an LLM how to use the tools. </span><span class="koboSpan" id="kobo.191.7">A central goal is to ensure that the LLM retains its broad capabilities across tasks while gaining the ability to use tools. </span><span class="koboSpan" id="kobo.191.8">Tool use is not hardcoded for specific prompts—it becomes part of the model’s general skillset. </span><span class="koboSpan" id="kobo.191.9">The LLM learns when a tool improves performance and chooses to invoke it only when necessary, maintaining flexibility and avoiding over-dependence. </span><span class="koboSpan" id="kobo.191.10">In short, tool use is not associated with a specific task but becomes a general concept. </span><span class="koboSpan" id="kobo.191.11">The idea behind Toolformer is it is a model that treats a tool as a call to an API. </span><span class="koboSpan" id="kobo.191.12">This abstraction simplifies integration and scales easily to different tools. </span><span class="koboSpan" id="kobo.191.13">For instance, the model might decide to call a calculator API when faced with a math problem or a search engine when external knowledge is needed. </span><span class="koboSpan" id="kobo.191.14">Given a series of human-written examples of how an API can be used, the authors used an LLM to annotate a huge language modeling dataset with potential API calls. </span><span class="koboSpan" id="kobo.191.15">After that, the authors conduct fine-tuning of the model to improve the model’s capabilities. </span><span class="koboSpan" id="kobo.191.16">With this approach, an LLM learns how to control a variety of tools and when it should </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">use them.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer263">
<span class="koboSpan" id="kobo.193.1"><img alt="Figure 9.5 – Toolformer approach (https://arxiv.org/pdf/2302.04761)" src="image/B21257_09_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.194.1">Figure 9.5 – Toolformer approach (</span><a href="https://arxiv.org/pdf/2302.04761"><span class="koboSpan" id="kobo.195.1">https://arxiv.org/pdf/2302.04761</span></a><span class="koboSpan" id="kobo.196.1">)</span></p>
<h2 id="_idParaDest-162"><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.197.1">HuggingGPT</span></h2>
<p><span class="koboSpan" id="kobo.198.1">HuggingGPT (Shen, 2023) introduces a</span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.199.1"> powerful concept: using language as a generic interface that enables LLMs to collaborate with external AI models across various modalities, such as vision, speech, and structured data. </span><span class="koboSpan" id="kobo.199.2">Instead of being limited to textual tasks, the LLM gains the ability to manage and orchestrate other models to solve complex, real-world problems. </span><span class="koboSpan" id="kobo.199.3">HuggingGPT is based on two ideas: an LLM is limited if it cannot access information beyond text (such as vision and speech), and in the real world, complex tasks can be decomposed into smaller tasks that are more manageable. </span><span class="koboSpan" id="kobo.199.4">For specific tasks, LLMs have excellent capabilities in zero-shot or few-shot learning, but generalist models are less capable than specific trained models. </span><span class="koboSpan" id="kobo.199.5">So, for the authors, the solution is that an LLM must be able to coordinate with external models to harness their powers. </span><span class="koboSpan" id="kobo.199.6">In the article, they focus on finding suitable middleware to bridge the connections between LLMs and AI models. </span><span class="koboSpan" id="kobo.199.7">In other words, the idea is that LLMs can dialogue with other models and thus exploit their capabilities. </span><span class="koboSpan" id="kobo.199.8">The intuition behind it is that each AI model can be described in the form of language by summarizing its function. </span><span class="koboSpan" id="kobo.199.9">In other words, each model can be described functionally and textually. </span><span class="koboSpan" id="kobo.199.10">This description can then be used by an LLM. </span><span class="koboSpan" id="kobo.199.11">For the authors, this represents the introduction of a new concept: </span><em class="italic"><span class="koboSpan" id="kobo.200.1">Language as a generic interface for LLMs to collaborate with AI models</span></em><span class="koboSpan" id="kobo.201.1">. </span><span class="koboSpan" id="kobo.201.2">In this system, the LLM acts as the “brain,” responsible for interpreting the user’s request, decomposing it into subtasks, selecting the appropriate models based on their textual descriptions, scheduling and coordinating model execution, integrating results, and generating a </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">final response.</span></span></p>
<p><span class="koboSpan" id="kobo.203.1">Since interaction with an LLM is through a prompt, a model’s function description can be entered in the LLM prompt. </span><span class="koboSpan" id="kobo.203.2">An LLM then can be seen as the brain that manages AI models for planning, scheduling, and cooperation. </span><span class="koboSpan" id="kobo.203.3">So, an LLM does not accomplish the task directly but invokes specific models to solve tasks. </span><span class="koboSpan" id="kobo.203.4">For example, if a user asks, “</span><em class="italic"><span class="koboSpan" id="kobo.204.1">What animal is in the image?</span></em><span class="koboSpan" id="kobo.205.1">”, the LLM processes the question and reasons what type of model it should use (i.e., an image classifier); the model is invoked, which returns an output (the animal present), and the LLM generates a textual output to answer “</span><em class="italic"><span class="koboSpan" id="kobo.206.1">the animal is </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.207.1">a chicken</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">.”</span></span></p>
<p><span class="koboSpan" id="kobo.209.1">At this point, the main problem is collecting these textual descriptions of the functions of the models. </span><span class="koboSpan" id="kobo.209.2">Fortunately, the </span><strong class="bold"><span class="koboSpan" id="kobo.210.1">machine learning</span></strong><span class="koboSpan" id="kobo.211.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.212.1">ML</span></strong><span class="koboSpan" id="kobo.213.1">) community</span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.214.1"> provides quality descriptions for specific tasks and the models used to solve them (language, vision, speech, and so on). </span><span class="koboSpan" id="kobo.214.2">So, what we need is to tie LLMs to the community (GitHub, Hugging Face, and </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.216.1">In short, HuggingGPT is an LLM-powered agent designed to solve a variety of complex tasks autonomously. </span><span class="koboSpan" id="kobo.216.2">HuggingGPT connects an LLM (in the original article, it is ChatGPT) with the ML community (Hugging Face, but the principle can be generalized); the LLM can take different modalities as input and accomplish different tasks. </span><span class="koboSpan" id="kobo.216.3">The LLM acts as a brain, divides the user’s request into subtasks, and then assigns them to specialized models (in accordance with the model description); it then executes these models and integrates the results. </span><span class="koboSpan" id="kobo.216.4">These principles are highlighted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer264">
<span class="koboSpan" id="kobo.218.1"><img alt="Figure 9.6 – HuggingGPT general scheme (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.219.1">Figure 9.6 – HuggingGPT general scheme (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.220.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.221.1">)</span></p>
<p><span class="koboSpan" id="kobo.222.1">The whole </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.223.1">HuggingGPT process can then be divided into </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">four steps:</span></span></p>
<ol>
<li class="upper-roman"><span class="koboSpan" id="kobo.225.1">Task planning: ChatGPT analyzes the requests by the user (understands the intention) and transforms the question into possible </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">solvable tasks.</span></span></li>
<li class="upper-roman"><span class="koboSpan" id="kobo.227.1">Model selection: ChatGPT selects the appropriate models (expert models) that are present in Hugging Face (the models are selected based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">provided description).</span></span></li>
<li class="upper-roman"><span class="koboSpan" id="kobo.229.1">Task execution: The model is invoked and executed, and then the results are returned </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">to ChatGPT.</span></span></li>
<li class="upper-roman"><span class="koboSpan" id="kobo.231.1">Response generation: ChatGPT integrates the results of the models and generates </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">the answers.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.233.1">In Toolformer, we have an LLM </span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.234.1">where the model calls a tool via an API call. </span><span class="koboSpan" id="kobo.234.2">HuggingGPT uses a similar approach but without the need for fine-tuning. </span><span class="koboSpan" id="kobo.234.3">In HuggingGPT, an LLM can be seen as a controller that routes user requests to expert models. </span><span class="koboSpan" id="kobo.234.4">In other words, the LLM understands the task and plans the action, but this action is then conducted by expert models (the LLM just integrates the results). </span><span class="koboSpan" id="kobo.234.5">The LLM here is just a facilitator that organizes the cooperation of different models to solve different tasks in different domains. </span><span class="koboSpan" id="kobo.234.6">The LLM then maintains its generality and can choose which tool to use and when to use it (in this case, the models are the tools). </span><span class="koboSpan" id="kobo.234.7">For example, if an LLM does not have capabilities in a certain mode, it exploits the capabilities of an expert model to be able to accomplish the task. </span><span class="koboSpan" id="kobo.234.8">The LLM just needs to know which model to call to solve a specific task. </span><span class="koboSpan" id="kobo.234.9">HuggingGPT thus represents a flexible system, where we only need textual descriptions to provide to the LLM, and </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.235.1">then the LLM will integrate the different </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">expert models.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer265">
<span class="koboSpan" id="kobo.237.1"><img alt="Figure 9.7 – HuggingGPT process (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.238.1">Figure 9.7 – HuggingGPT process (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.239.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.240.1">)</span></p>
<h3><span class="koboSpan" id="kobo.241.1">Task planning</span></h3>
<p><span class="koboSpan" id="kobo.242.1">In the first step, </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">task planning</span></strong><span class="koboSpan" id="kobo.244.1">, the LLM must</span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.245.1"> understand the task and break it down into subtasks. </span><span class="koboSpan" id="kobo.245.2">In the real world, user requests can be complex and their intentions intricate, requiring task decomposition. </span><span class="koboSpan" id="kobo.245.3">This is because a single model may not be capable of solving the entire task; instead, multiple models might be necessary to address different aspects. </span><span class="koboSpan" id="kobo.245.4">An LLM then needs to decompose the task into a series of subtasks and understand the dependency between these tasks and in what order they should be executed. </span><span class="koboSpan" id="kobo.245.5">This is conducted by creating a </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">specific prompt.</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">To standardize the system, the authors of HuggingGPT used a set of specific instructions. </span><span class="koboSpan" id="kobo.247.2">An LLM must then adhere to these specifications in order to conduct task planning. </span><span class="koboSpan" id="kobo.247.3">They designed a standardized template for tasks and instructed the LLM to conduct task parsing through slot filling. </span><span class="koboSpan" id="kobo.247.4">The LLM is guided to fill this template using slot filling, allowing for the consistent parsing and execution of subtasks. </span><span class="koboSpan" id="kobo.247.5">There are four slots that the template </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">must fill:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.249.1">Task ID</span></strong><span class="koboSpan" id="kobo.250.1">: The model </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.251.1">provides a unique identifier for each task. </span><span class="koboSpan" id="kobo.251.2">This ID is used to identify both the task and dependent tasks, as well as all the resources that </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">are generated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.253.1">Task type</span></strong><span class="koboSpan" id="kobo.254.1">: This slot includes the task type; each task can be of various types (language, visual, video, audio, and </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">so on).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.256.1">Task dependencies</span></strong><span class="koboSpan" id="kobo.257.1">: This slot defines the prerequisites for each task (the model only launches a task if all its prerequisites </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">are complete).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.259.1">Task arguments</span></strong><span class="koboSpan" id="kobo.260.1">: This slot contains all the arguments that are required for the execution of a task (from text to images or other resources). </span><span class="koboSpan" id="kobo.260.2">These contents can be derived from the user’s query or from the results of </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">other tasks.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer266">
<span class="koboSpan" id="kobo.262.1"><img alt="Figure 9.8 – HuggingGPT type of task (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.263.1">Figure 9.8 – HuggingGPT type of task (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.264.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.265.1">)</span></p>
<p><span class="koboSpan" id="kobo.266.1">The authors use</span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.267.1"> demonstrations to direct the model to perform a task (such as image-to-text, summarization, and so on). </span><span class="koboSpan" id="kobo.267.2">As we saw in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.268.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.269.1">, adding demonstrations allows the model to map the task (few-shot prompting and in-context learning). </span><span class="koboSpan" id="kobo.269.2">These demonstrations tell the model how it should divide the task, in what order, and whether there are dependencies. </span><span class="koboSpan" id="kobo.269.3">In addition, to support complex tasks, the authors include chat logs (previous discussions that were conducted with the user) as a kind of tool. </span><span class="koboSpan" id="kobo.269.4">This way, the model can be aware if additional resources or requests have been indicated that can help with </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">the task.</span></span></p>
<p><span class="koboSpan" id="kobo.271.1">The prompt provides all the information needed for the LLM. </span><span class="koboSpan" id="kobo.271.2">In the prompt, we provide instructions on its task (planning the task breakdown), where to retrieve information, examples of how it should perform the task, and what output </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">we expect.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer267">
<span class="koboSpan" id="kobo.273.1"><img alt="Figure 9.9 – Details of the prompt design in HuggingGPT (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.274.1">Figure 9.9 – Details of the prompt design in HuggingGPT (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.275.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.276.1">)</span></p>
<h3><span class="koboSpan" id="kobo.277.1">Model selection</span></h3>
<p><span class="koboSpan" id="kobo.278.1">After planning the task, the model </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.279.1">proceeds to select appropriate models for the task, or </span><strong class="bold"><span class="koboSpan" id="kobo.280.1">model selection</span></strong><span class="koboSpan" id="kobo.281.1">. </span><span class="koboSpan" id="kobo.281.2">Once we have a list of subtasks, we need to choose the appropriate model. </span><span class="koboSpan" id="kobo.281.3">This is possible because we have descriptions of the models and what they do. </span><span class="koboSpan" id="kobo.281.4">The authors of this work have collected descriptions of expert models from the ML community (e.g., Hugging Face). </span><span class="koboSpan" id="kobo.281.5">In fact, on Hugging Face, it is often the model’s developers themselves who describe the model in terms of functionality, architecture, supported languages and domains, licensing, and </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">so on.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer268">
<span class="koboSpan" id="kobo.283.1"><img alt="Figure 9.10 – Screenshot of an example of the description of a model on Hugging Face (https://huggingface.co/docs/transformers/model_doc/bert)" src="image/B21257_09_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.284.1">Figure 9.10 – Screenshot of an example of the description of a model on Hugging Face (</span><a href="https://huggingface.co/docs/transformers/model_doc/bert"><span class="koboSpan" id="kobo.285.1">https://huggingface.co/docs/transformers/model_doc/bert</span></a><span class="koboSpan" id="kobo.286.1">)</span></p>
<p><span class="koboSpan" id="kobo.287.1">Model assignment </span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.288.1">is thus formulated as a single-choice model, in which an LLM must choose which model is the best among those available given a particular context. </span><span class="koboSpan" id="kobo.288.2">Then, considering the user’s requirements and the context, an LLM can choose which expert model is best suited to perform the task. </span><span class="koboSpan" id="kobo.288.3">Of course, there is a limit to the context length, and you cannot enter all the model descriptions without exceeding this length. </span><span class="koboSpan" id="kobo.288.4">To address this, the HuggingGPT system applies a two-stage filtering and ranking process.  </span><span class="koboSpan" id="kobo.288.5">First, models are filtered based on the task type identified during task planning (e.g., language, vision, or audio). </span><span class="koboSpan" id="kobo.288.6">Only models that are relevant to the specific subtask type are retained, narrowing down the pool significantly. </span><span class="koboSpan" id="kobo.288.7">Among the filtered models, the system sorts them based on the number of downloads, which acts as a proxy for quality, reliability, and community trust. </span><span class="koboSpan" id="kobo.288.8">The assumption is that widely used models are more likely to perform well. </span><span class="koboSpan" id="kobo.288.9">Finally, the system selects the top-k model descriptions (where k is a configurable hyperparameter) and includes them in the prompt. </span><span class="koboSpan" id="kobo.288.10">The LLM then performs single-choice model selection, evaluating the context and user requirements to choose the most appropriate model from the shortlist. </span><span class="koboSpan" id="kobo.288.11">This strategy offers a balanced trade-off: it keeps the prompt within manageable token limits while still allowing the LLM enough options to make an informed and effective </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">model selection.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer269">
<span class="koboSpan" id="kobo.290.1"><img alt="Figure 9.11 – Details of the prompt design in HuggingGPT for model selection (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.291.1">Figure 9.11 – Details of the prompt design in HuggingGPT for model selection (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.292.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.293.1">)</span></p>
<h3><span class="koboSpan" id="kobo.294.1">Model execution</span></h3>
<p><span class="koboSpan" id="kobo.295.1">Once a specific model has</span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.296.1"> been assigned to a specific task, the model must be executed. </span><span class="koboSpan" id="kobo.296.2">Note that these models are used only in inference. </span><span class="koboSpan" id="kobo.296.3">These models are used through the Hugging Face API. </span><span class="koboSpan" id="kobo.296.4">To speed up execution, HuggingGPT uses hybrid inference endpoints. </span><span class="koboSpan" id="kobo.296.5">The selected model takes the task arguments as input and then sends the results back to the language model (ChatGPT). </span><span class="koboSpan" id="kobo.296.6">Moreover, if the model has no resource dependencies, its inference can be parallelized. </span><span class="koboSpan" id="kobo.296.7">In other words, tasks that are not dependent on each other can be executed simultaneously. </span><span class="koboSpan" id="kobo.296.8">Otherwise, the system takes into account how much the output of one model and the input of another are connected (e.g., if one task must have the output of another subtask in order to be carried out). </span><span class="koboSpan" id="kobo.296.9">To perform inference, HuggingGPT uses hybrid inference endpoints, primarily relying on Hugging Face APIs. </span><span class="koboSpan" id="kobo.296.10">When models are available and functional via these APIs, the system executes them remotely. </span><span class="koboSpan" id="kobo.296.11">However, if API endpoints are unavailable or slow or face network issues, local inference is used as a fallback. </span><span class="koboSpan" id="kobo.296.12">This hybrid setup ensures flexibility and robustness </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">in execution.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">The authors note: “</span><em class="italic"><span class="koboSpan" id="kobo.299.1">Despite HuggingGPT’s ability to develop the task order through task planning, it can still be challenging to effectively manage resource dependencies between tasks in the task execution stage</span></em><span class="koboSpan" id="kobo.300.1">.” </span><span class="koboSpan" id="kobo.300.2">To solve this problem, the authors simply used a unique symbol, </span><strong class="source-inline"><span class="koboSpan" id="kobo.301.1">&lt;resource&gt;</span></strong><span class="koboSpan" id="kobo.302.1">, to handle the dependencies. </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">&lt;resource&gt;</span></strong><span class="koboSpan" id="kobo.304.1"> is a special</span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.305.1"> token that represents the resource required for a task (this matches the task identifier), and if the required task is completed, the token is replaced with </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">the resource.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer270">
<span class="koboSpan" id="kobo.307.1"><img alt="Figure 9.12 – Model execution (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.308.1">Figure 9.12 – Model execution (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.309.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.310.1">)</span></p>
<h3><span class="koboSpan" id="kobo.311.1">Response generation</span></h3>
<p><span class="koboSpan" id="kobo.312.1">Once all the tasks are executed, the</span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.313.1"> response must be generated. </span><span class="koboSpan" id="kobo.313.2">HuggingGPT integrates all the information that was obtained in the previous steps (task planning, model selection, and task execution) into a kind of concise summary (the tasks, the models used, and the results of the models). </span><span class="koboSpan" id="kobo.313.3">Note that the model integrates results of several other models, especially those obtained by inference and that may be of different formats. </span><span class="koboSpan" id="kobo.313.4">These results are presented in a structured format (as in, bounding boxes, probabilities, and so on), and HuggingGPT takes these results and transforms them into natural language to respond to a user. </span><span class="koboSpan" id="kobo.313.5">So, HuggingGPT not only gets results for the task but also responds to the user in a </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">human-friendly way.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer271">
<span class="koboSpan" id="kobo.315.1"><img alt="Figure 9.13 – Response generation (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.316.1">Figure 9.13 – Response generation (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.317.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.318.1">)</span></p>
<p><span class="koboSpan" id="kobo.319.1">Qualitatively, we can </span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.320.1">see that the model is capable of solving several tasks. </span><span class="koboSpan" id="kobo.320.2">Thus, the model is able to divide the task into various subtasks, choose appropriate models, retrieve the results, and integrate them efficiently. </span><span class="koboSpan" id="kobo.320.3">For example, the model can do image captioning, pose generation, and even pose conditional image generation tasks. </span><span class="koboSpan" id="kobo.320.4">Not only that but the tasks can be multimodal (such as text-to-video generation, adding audio to a video, and so on). </span><span class="koboSpan" id="kobo.320.5">One of the most interesting aspects is that all of this is conducted without any additional LLM training. </span><span class="koboSpan" id="kobo.320.6">In fact, everything is done in inference (for both LLMs and models in inference). </span><span class="koboSpan" id="kobo.320.7">The advantage is that you can integrate additional models for additional tasks without any training; you only need to add a functional description of the </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">new models.</span></span></p>
<p><span class="koboSpan" id="kobo.322.1">For example, in this case, we can see the execution of a multimodal task (text, video, and audio). </span><span class="koboSpan" id="kobo.322.2">The model is asked to perform two tasks: generate a video from a description and dub the video. </span><span class="koboSpan" id="kobo.322.3">The model performs these two actions in parallel. </span><span class="koboSpan" id="kobo.322.4">In the bottom part of the following figure, the model must instead perform the two tasks in series: the model first generates text from the image and then </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">generates audio.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer272">
<span class="koboSpan" id="kobo.324.1"><img alt="Figure 9.14 – Qualitative analysis of multi-model cooperation on video and audio modalities (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.325.1">Figure 9.14 – Qualitative analysis of multi-model cooperation on video and audio modalities (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.326.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.327.1">)</span></p>
<p><span class="koboSpan" id="kobo.328.1">The authors of the study </span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.329.1">also explore more complex tasks where an LLM must organize the cooperation of multiple models to succeed in solving the task. </span><span class="koboSpan" id="kobo.329.2">HuggingGPT can organize the cooperation of multiple models through the task planning step. </span><span class="koboSpan" id="kobo.329.3">The results show that HuggingGPT can cope with complex tasks in a multi-round conversation scenario (where the user divides their requests into several rounds). </span><span class="koboSpan" id="kobo.329.4">Moreover, the model can solve complex tasks by assigning an expert model to each task. </span><span class="koboSpan" id="kobo.329.5">For example, “</span><em class="italic"><span class="koboSpan" id="kobo.330.1">Describe the image in as much detail as possible</span></em><span class="koboSpan" id="kobo.331.1">” requires the model to solve five tasks (image caption, image classification, object detection, segmentation, and visual question-answering tasks). </span><span class="koboSpan" id="kobo.331.2">These five tasks are not solved by one model but by five different models that are called and executed. </span><span class="koboSpan" id="kobo.331.3">Each of these</span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.332.1"> models then provides information that must be integrated into a detailed answer. </span><span class="koboSpan" id="kobo.332.2">These models work in parallel in inference and then the final information </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">is merged.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer273">
<span class="koboSpan" id="kobo.334.1"><img alt="Figure 9.15 – Case study on complex tasks (https://arxiv.org/pdf/2303.17580)" src="image/B21257_09_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.335.1">Figure 9.15 – Case study on complex tasks (</span><a href="https://arxiv.org/pdf/2303.17580"><span class="koboSpan" id="kobo.336.1">https://arxiv.org/pdf/2303.17580</span></a><span class="koboSpan" id="kobo.337.1">)</span></p>
<h3><span class="koboSpan" id="kobo.338.1">HuggingGPT limitations</span></h3>
<p><span class="koboSpan" id="kobo.339.1">However, some </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">limitations</span></span><span class="No-Break"><a id="_idIndexMarker1110"/></span><span class="No-Break"><span class="koboSpan" id="kobo.341.1"> remain:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.342.1">Efficiency</span></strong><span class="koboSpan" id="kobo.343.1">: HuggingGPT requires multiple calls from an LLM; this occurs in three of the four process steps (task planning, model selection, and response generation). </span><span class="koboSpan" id="kobo.343.2">These interactions are expensive and can lead to response latency and degradation of the user experience. </span><span class="koboSpan" id="kobo.343.3">In addition, closed-source models (GPT-3.5 and GPT-4) were used in the original article, leading to additional costs. </span><span class="koboSpan" id="kobo.343.4">Technically, the same approach could have been carried out with models that are </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">open source.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.345.1">Planning</span></strong><span class="koboSpan" id="kobo.346.1">: Planning depends on the capabilities of the LLM. </span><span class="koboSpan" id="kobo.346.2">Obviously, the more capable an LLM, the better the system’s capabilities, but an LLM has limited reasoning capabilities, so planning may not always be optimal or feasible. </span><span class="koboSpan" id="kobo.346.3">You could then test different LLMs or use LLMs that are fine-tuned to create an efficient plan or models that have been fine-tuned to the </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">reasoning chain.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.348.1">Context lengths</span></strong><span class="koboSpan" id="kobo.349.1">: The context length of a model has a definite limit, and for complex tasks, this is a problem. </span><span class="koboSpan" id="kobo.349.2">In the original article, the authors note that 32K for some tasks is enough (especially if several models are connected). </span><span class="koboSpan" id="kobo.349.3">The solution, then, may be to use models with a longer context length. </span><span class="koboSpan" id="kobo.349.4">To date, though, it seems that models don’t use long context efficiently. </span><span class="koboSpan" id="kobo.349.5">Another solution might be to </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">use summarization.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.351.1">Instability</span></strong><span class="koboSpan" id="kobo.352.1">: This stems from the stochastic nature of the LLM. </span><span class="koboSpan" id="kobo.352.2">Although LLMs are trained to generate text, and in this case we provide context, the model can ignore context and hallucinate. </span><span class="koboSpan" id="kobo.352.3">The authors of the article note that the model may fail to conform to instructions or give incorrect answers during the prediction. </span><span class="koboSpan" id="kobo.352.4">This generates program flow errors or incorrect answers. </span><span class="koboSpan" id="kobo.352.5">Hallucinations are still an open problem for LLMs, but there are strategies to </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">mitigate them.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.354.1">HuggingGPT, then, is a</span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.355.1"> system capable of solving complex tasks by orchestrating different expert models using the language as an interface. </span><span class="koboSpan" id="kobo.355.2">The LLM acts here only as a controller and manager of the various AI models. </span><span class="koboSpan" id="kobo.355.3">Its only tasks are to orchestrate the models and then generate a response. </span><span class="koboSpan" id="kobo.355.4">The model then generates a plan, selects the models, and then integrates the results into the final response. </span><span class="koboSpan" id="kobo.355.5">By itself, the LLM does not perform any tasks but demands resolution from the various expert models. </span><span class="koboSpan" id="kobo.355.6">All this is conducted in inference without any training. </span><span class="koboSpan" id="kobo.355.7">The user then provides their question, and the system conducts the process and then responds in natural language, thus making the interaction human-friendly </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">and fluid.</span></span></p>
<p><span class="koboSpan" id="kobo.357.1">In the following subsections, we will examine various models designed to overcome the limitations of HuggingGPT or address critical challenges in other specialized domains. </span><span class="koboSpan" id="kobo.357.2">Through these explorations, you will gain insight into different strategies and learn how these agents can be applied to </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">real-world scenarios.</span></span></p>
<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.359.1">ChemCrow</span></h2>
<p><span class="koboSpan" id="kobo.360.1">We previously saw HuggingGPT as a system that </span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.361.1">orchestrates different tools (models), acting as a generalist model for general tasks. </span><span class="koboSpan" id="kobo.361.2">In this subsection, we want to discuss a similar system applied to a specialized field. </span><span class="koboSpan" id="kobo.361.3">ChemCrow (Bran, 2023) follows a similar design philosophy to HuggingGPT, but applies it to a </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">specialized field—chemistry.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">The limitation of generalist LLMs is that they have generalist knowledge and therefore are neither specialized for a field nor updated with the latest information. </span><span class="koboSpan" id="kobo.363.2">This can be a problematic limitation for many application fields (especially specialized ones such as science, finance, and healthcare). </span><span class="koboSpan" id="kobo.363.3">In addition, LLMs conduct calculations using a bag of heuristics and not by a rigorous process. </span><span class="koboSpan" id="kobo.363.4">For fields such as chemistry, this is a problem, so it is natural to think about extending the models’ capabilities with external tools. </span><span class="koboSpan" id="kobo.363.5">External tools then provide the exact answer and compensate for the deficiencies of LLMs in specific domains. </span><span class="koboSpan" id="kobo.363.6">Thus, having an integration of an LLM with several tools can allow an LLM to be used even in fields where its inherent characteristics constitute a limitation to </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">its applicability.</span></span></p>
<p><span class="koboSpan" id="kobo.365.1">One field that can benefit from the use of LLMs is scientific research. </span><span class="koboSpan" id="kobo.365.2">On the one hand, LLMs have shown some ability to understand chemistry, and on the other hand, there are many specialized models for chemistry, or at least for specific applications. </span><span class="koboSpan" id="kobo.365.3">Many of these tools have been developed by the open source community and are accessible through APIs. </span><span class="koboSpan" id="kobo.365.4">Nevertheless, integrating these tools is not easy and requires expertise in computational coding, which is often not among the skills of chemistry researchers. </span><span class="koboSpan" id="kobo.365.5">Inspired by previous work, the authors of this study (Bran, 2023) proposed what they call an LLM-powered chemistry engine (ChemCrow) to “</span><em class="italic"><span class="koboSpan" id="kobo.366.1">streamline the reasoning process for various common chemical tasks across areas such as drug and materials design and synthesis</span></em><span class="koboSpan" id="kobo.367.1">.” </span><span class="koboSpan" id="kobo.367.2">ChemCrow is very similar to what we have seen with HuggingGPT, in which we have a central LLM (GPT-4) that orchestrates a number of tools (in this case, highly specialized for chemistry). </span><span class="koboSpan" id="kobo.367.3">The central LLM is prompted with specific instructions and information in order to perform the tasks specifically and respond in a specific format. </span><span class="koboSpan" id="kobo.367.4">To guide the LLM’s reasoning and tool use, ChemCrow adopts a structured prompting format known as Thought, Action, Action Input, and Observation, to prompt the model to reason about the task (and its current state), how the current state relates to the final </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.368.1">goal, and how to plan the </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">next steps:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.370.1">Thought</span></strong><span class="koboSpan" id="kobo.371.1">: The model reflects on the current problem, considers its progress, and outlines reasoning toward the </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">final goal</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.373.1">Action</span></strong><span class="koboSpan" id="kobo.374.1">: It selects the appropriate tool to use next (e.g., a molecule generator or a </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">reaction predictor)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.376.1">Action input</span></strong><span class="koboSpan" id="kobo.377.1">: It specifies what input should be sent to the </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">chosen tool</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.379.1">Observation</span></strong><span class="koboSpan" id="kobo.380.1">: It records the tool’s output, which is then incorporated into the next </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">reasoning cycle</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer274">
<span class="koboSpan" id="kobo.382.1"><img alt="Figure 9.16 – Overview of ChemCrow (https://arxiv.org/pdf/2304.05376)" src="image/B21257_09_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.383.1">Figure 9.16 – Overview of ChemCrow (</span><a href="https://arxiv.org/pdf/2304.05376"><span class="koboSpan" id="kobo.384.1">https://arxiv.org/pdf/2304.05376</span></a><span class="koboSpan" id="kobo.385.1">)</span></p>
<p><span class="koboSpan" id="kobo.386.1">So, in this system, the </span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.387.1">model proceeds with a Thought step (which can be thought of as action planning) and uses a tool and an input to this tool (selecting and using the model). </span><span class="koboSpan" id="kobo.387.2">The model gets the results, observes them, and conducts a Thought step again until the answer is reached. </span><span class="koboSpan" id="kobo.387.3">The process is similar to what we saw in the previous section, but there is a greater emphasis on reasoning and a specialization of the model. </span><span class="koboSpan" id="kobo.387.4">Also, among the tools are not only models but also the ability to search the internet or the literature; the model can also run code. </span><span class="koboSpan" id="kobo.387.5">So, we also have an extension of the capabilities and flexibility of the system. </span><span class="koboSpan" id="kobo.387.6">Thus, the authors of the study see this system as a kind of researcher’s assistant to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">chemical tasks.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer275">
<span class="koboSpan" id="kobo.389.1"><img alt="Figure 9.17 – Human/model interaction leading to the discovery of a novel molecule (https://arxiv.org/pdf/2304.05376)" src="image/B21257_09_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.390.1">Figure 9.17 – Human/model interaction leading to the discovery of a novel molecule (</span><a href="https://arxiv.org/pdf/2304.05376"><span class="koboSpan" id="kobo.391.1">https://arxiv.org/pdf/2304.05376</span></a><span class="koboSpan" id="kobo.392.1">)</span></p>
<p><span class="koboSpan" id="kobo.393.1">So, the idea is to </span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.394.1">combine LLM reasoning skills with expert knowledge and chemical computational tools. </span><span class="koboSpan" id="kobo.394.2">The results show that similar approaches can lead to real-world applications in specific fields, such </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">as chemistry.</span></span></p>
<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.396.1">SwiftDossier</span></h2>
<p><span class="koboSpan" id="kobo.397.1">SwiftDossier is a notable example of</span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.398.1"> applying agent-based systems in the scientific and healthcare domains, with a particular focus on addressing one of the most critical challenges in these areas: hallucinations. </span><span class="koboSpan" id="kobo.398.2">In fields such as medicine and pharmaceuticals, hallucinated outputs—that is, confident but false or unverifiable information—can lead to serious legal, ethical, and safety risks. </span><span class="koboSpan" id="kobo.398.3">An LLM has a huge memory but generates text stochastically, without obviously verifying its sources. </span><span class="koboSpan" id="kobo.398.4">This is problematic for the pharmaceutical industry or potential use in medicine. </span><span class="koboSpan" id="kobo.398.5">To solve this problem in SwiftDossier, RAGs and LLM-powered agents are used to force model generation. </span><span class="koboSpan" id="kobo.398.6">Instead of relying solely on the LLM’s internal knowledge—which is vast but generated stochastically and without source verification—the system forces the model to ground its responses in external, reliable data sources. </span><span class="koboSpan" id="kobo.398.7">The system uses a different set of tools to be able to answer different questions: scientific articles, internet access, databases, and other ML models. </span><span class="koboSpan" id="kobo.398.8">Using this set of tools, an LLM can succeed in generating reports and minimize</span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.399.1"> the risk </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">of hallucination.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer276">
<span class="koboSpan" id="kobo.401.1"><img alt="Figure 9.18 – SwiftDossier architecture (https://arxiv.org/pdf/2409.15817)" src="image/B21257_09_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.402.1">Figure 9.18 – SwiftDossier architecture (</span><a href="https://arxiv.org/pdf/2409.15817"><span class="koboSpan" id="kobo.403.1">https://arxiv.org/pdf/2409.15817</span></a><span class="koboSpan" id="kobo.404.1">)</span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.405.1">ChemAgent</span></h2>
<p><span class="koboSpan" id="kobo.406.1">In the two examples seen previously, we </span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.407.1">have an agent to which tools are added to make up for the knowledge deficiencies of a generalist LLM. </span><span class="koboSpan" id="kobo.407.2">In other words, we try to make up for the shortcomings of an LLM by using either external information or tools to conduct operations. </span><span class="koboSpan" id="kobo.407.3">Moreover, if the task itself is complex, several approaches try to decompose it into more manageable subtasks. </span><span class="koboSpan" id="kobo.407.4">An agent first produces a schedule and then executes the various subtasks, thus combining reasoning and execution. </span><span class="koboSpan" id="kobo.407.5">Despite all this, an LLM may still generate errors, especially in complex domains such </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">as chemistry.</span></span></p>
<p><span class="koboSpan" id="kobo.409.1">LLMs, while powerful general-purpose tools, face several challenges in the chemistry domain, where tasks require precise reasoning, accurate calculations, and deep domain knowledge. </span><span class="koboSpan" id="kobo.409.2">These challenges arise due to the limitations in how LLMs generate text and code, and they become more pronounced in scientific applications where small errors can lead to </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">significant inaccuracies:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.411.1">Struggles with domain-specific formulas</span></strong><span class="koboSpan" id="kobo.412.1">: LLMs may misinterpret or incorrectly apply specialized chemical equations or notation, especially when the required formulas are not commonly found in general </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">training data</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.414.1">Incorrect intermediate reasoning steps</span></strong><span class="koboSpan" id="kobo.415.1">: In complex, multi-step tasks (e.g., synthesis planning or property prediction), an error in just one step can cascade and lead to faulty </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">final outputs</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.417.1">Errors in code generation</span></strong><span class="koboSpan" id="kobo.418.1">: When combining textual reasoning with code (typically Python), LLMs often hallucinate functions, use incorrect libraries, produce syntax errors, or generate code that fails to execute—especially for scientific calculations that require precise library calls and </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">numerical stability</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer277">
<span class="koboSpan" id="kobo.420.1"><img alt="Figure 9.19 – Examples of LLM failure in chemistry domain (https://arxiv.org/pdf/2501.06590)" src="image/B21257_09_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.421.1">Figure 9.19 – Examples of LLM failure in chemistry domain (</span><a href="https://arxiv.org/pdf/2501.06590"><span class="koboSpan" id="kobo.422.1">https://arxiv.org/pdf/2501.06590</span></a><span class="koboSpan" id="kobo.423.1">)</span></p>
<p><span class="koboSpan" id="kobo.424.1">Human beings, unlike </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.425.1">LLMs, learn from their past experiences and mistakes. </span><span class="koboSpan" id="kobo.425.2">For LLMs, it is not possible to learn after the end of pre-training (fine-tuning is an expensive approach and cannot be used repeatedly), so continual learning remains an open problem of AI. </span><span class="koboSpan" id="kobo.425.3">Humans, on the other hand, can remember strategies used for similar problems; once they encounter new problems, they learn new strategies that can be used in the future.  </span><span class="koboSpan" id="kobo.425.4">Therefore, in ChemAgent, the authors try to find a way to simulate this process. </span><span class="koboSpan" id="kobo.425.5">They propose a dynamic library that allows iterative problem-solving to be facilitated by continuously updating and refining its content. </span><span class="koboSpan" id="kobo.425.6">The library serves as a repository for decomposed chemical tasks. </span><span class="koboSpan" id="kobo.425.7">In other words, a task is broken down into various subtasks and then the solutions are saved in the library for future use. </span><span class="koboSpan" id="kobo.425.8">Once a new task arrives, the library is updated with the new subtasks and corresponding solutions, keeping the library relevant and improving its usefulness over time. </span><span class="koboSpan" id="kobo.425.9">Inspired by human cognition, the system has three different memory components: planning memory (high-level strategies), execution memory (specific task solutions), and knowledge memory (fundamental chemistry principles). </span><span class="koboSpan" id="kobo.425.10">These memory components are stored externally, allowing the system to find the information again when needed, and are </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">dynamically updated.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer278">
<span class="koboSpan" id="kobo.427.1"><img alt="Figure 9.20 – ChemAgent framework (https://arxiv.org/pdf/2501.06590)" src="image/B21257_09_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.428.1">Figure 9.20 – ChemAgent framework (</span><a href="https://arxiv.org/pdf/2501.06590"><span class="koboSpan" id="kobo.429.1">https://arxiv.org/pdf/2501.06590</span></a><span class="koboSpan" id="kobo.430.1">)</span></p>
<p><span class="koboSpan" id="kobo.431.1">ChemAgent thus doesn’t </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.432.1">just passively use what it finds in memory but rather allows the system to update the memory dynamically. </span><span class="koboSpan" id="kobo.432.2">It also uses memory partitioning to improve the various stages of problem-solving. </span><span class="koboSpan" id="kobo.432.3">ChemAgent divides the process into planning and execution (to which it associates a specific memory for each step) and adds memory that functions as a reference for fundamental chemistry principles and formulas. </span><span class="koboSpan" id="kobo.432.4">When a problem occurs, it is divided into a series of subtasks, which are solved, and these solutions are saved </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">in memory.</span></span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.434.1">Multi-agent for law</span></h2>
<p><span class="koboSpan" id="kobo.435.1">Another area that could benefit from the</span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.436.1"> use of agents is the legal sector. </span><span class="koboSpan" id="kobo.436.2">Legal services are essential to protect citizens’ rights, but they can be particularly expensive and there are not always enough lawyers. </span><span class="koboSpan" id="kobo.436.3">Moreover, fair judgment is a fundamental right, but human beings also exhibit bias. </span><span class="koboSpan" id="kobo.436.4">Using agents in this field could revolutionize legal services by lowering costs and allowing more equitable access. </span><span class="koboSpan" id="kobo.436.5">In the legal field, hallucinations are particularly problematic and should be, if not eliminated, reduced as much as possible. </span><span class="koboSpan" id="kobo.436.6">Hallucinations arise from both the stochastic nature of the models and the quality of the data with which they are trained. </span><span class="koboSpan" id="kobo.436.7">Therefore, action must be taken on two axes in order to mitigate </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">the phenomenon.</span></span></p>
<p><span class="koboSpan" id="kobo.438.1">In this subsection, we want to present two law-focused approaches to present some interesting elements that have been used. </span><span class="koboSpan" id="kobo.438.2">Again, the principle is the same: everything revolves around a central element, which is an LLM. </span><span class="koboSpan" id="kobo.438.3">For example, Chatlaw</span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.439.1"> focuses on data quality to mitigate the risk of LLM hallucination. </span><span class="koboSpan" id="kobo.439.2">Also, to make the most of the quality dataset the authors have collected, they use a knowledge graph. </span><span class="koboSpan" id="kobo.439.3">In addition, instead of using a single agent, they use a multi-agent system. </span><span class="koboSpan" id="kobo.439.4">Using multiple agents allows the system to simulate different areas of expertise, thanks to the flexibility of prompts when interacting with LLMs. </span><span class="koboSpan" id="kobo.439.5">The use of multi-agents makes it possible to emulate the process within a law firm. </span><span class="koboSpan" id="kobo.439.6">The authors developed a protocol to allow effective collaboration among agents: “</span><em class="italic"><span class="koboSpan" id="kobo.440.1">four independent intelligent agent roles responsible for initial information gathering, in-depth material research, legal advice, and final consultation report writing</span></em><span class="koboSpan" id="kobo.441.1">.” </span><span class="koboSpan" id="kobo.441.2">In this way, the process is more thorough. </span><span class="koboSpan" id="kobo.441.3">Again,  they used only one</span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.442.1"> LLM for the whole system (the authors </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">used GPT-4).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer279">
<span class="koboSpan" id="kobo.444.1"><img alt="Figure 9.21 – Chatlaw, a multi-agent collaboration (https://arxiv.org/pdf/2306.16092v2)" src="image/B21257_09_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.445.1">Figure 9.21 – Chatlaw, a multi-agent collaboration (</span><a href="https://arxiv.org/pdf/2306.16092v2"><span class="koboSpan" id="kobo.446.1">https://arxiv.org/pdf/2306.16092v2</span></a><span class="koboSpan" id="kobo.447.1">)</span></p>
<p><span class="koboSpan" id="kobo.448.1">Another interesting approach is one in which the authors  (Hamilton, 2023; </span><a href="https://arxiv.org/pdf/2301.05327"><span class="koboSpan" id="kobo.449.1">https://arxiv.org/pdf/2301.05327</span></a><span class="koboSpan" id="kobo.450.1">) mimic the judgment of a court using an LLM. </span><span class="koboSpan" id="kobo.450.2">Here, too, a multi-agent system is used, in which each agent represents a judge. </span><span class="koboSpan" id="kobo.450.3">Each judge produces an opinion and then a </span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.451.1">majority opinion is obtained. </span><span class="koboSpan" id="kobo.451.2">So, when a case is sent to nine judges, the system receives nine opinions, and then it produces a single opinion. </span><span class="koboSpan" id="kobo.451.3">This approach then relies on conducting nine evaluations in parallel and the consistency of</span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.452.1"> these evaluations (the majority </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">vote wins).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer280">
<span class="koboSpan" id="kobo.454.1"><img alt="Figure 9.22 – Multi-judge system (https://arxiv.org/pdf/2301.05327)" src="image/B21257_09_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.455.1">Figure 9.22 – Multi-judge system (</span><a href="https://arxiv.org/pdf/2301.05327"><span class="koboSpan" id="kobo.456.1">https://arxiv.org/pdf/2301.05327</span></a><span class="koboSpan" id="kobo.457.1">)</span></p>
<p><span class="koboSpan" id="kobo.458.1">This work shows how to </span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.459.1">leverage an LLM to create multiple agents that work together to be able to mitigate hallucinations. </span><span class="koboSpan" id="kobo.459.2">The authors are further evidence of the flexibility that can be achieved by using an LLM as the center of the system. </span><span class="koboSpan" id="kobo.459.3">A limitation of this study is the use of homogeneous judges (it would be better to build the ensemble with different models, to avoid the various judges having the same bias), risking </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">repetitive opinions.</span></span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.461.1">Multi-agent for healthcare applications</span></h2>
<p><span class="koboSpan" id="kobo.462.1">Interdisciplinary research is complex and usually </span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.463.1">requires teams composed of researchers with different areas of expertise. </span><span class="koboSpan" id="kobo.463.2">Typically, scientific research is conducted by teams where each researcher deals with a particular aspect and masters different techniques. </span><span class="koboSpan" id="kobo.463.3">For example, AlphaFold 2</span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.464.1"> is the product of 34 researchers with different expertise (computer science, bioinformatics, and structural biology). </span><span class="koboSpan" id="kobo.464.2">Obviously, recruiting large teams of experts takes time (and it is not always easy to find people with the right expertise) and is expensive. </span><span class="koboSpan" id="kobo.464.3">Only a few institutions and companies can afford the most ambitious projects. </span><span class="koboSpan" id="kobo.464.4">Recently created LLMs, though, have increasingly broad knowledge of scientific topics, and we saw previously that this knowledge can be connected to the use of tools. </span><span class="koboSpan" id="kobo.464.5">ChemCrow is an example of how to solve a chemical problem, but it cannot tackle an open-ended, interdisciplinary research problem. </span><span class="koboSpan" id="kobo.464.6">Recently, efforts have been made to solve this problem by creating pipelines that can handle the end-to-end process. </span><span class="koboSpan" id="kobo.464.7">For example, an AI scientist (Lu, 2024) carries out a process that starts with conceptualizing an idea and ends with</span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.465.1"> writing a scientific paper on ML. </span><span class="koboSpan" id="kobo.465.2">The AI scientist is given a broad research direction, produces an idea, conducts the literature search, plans and executes experiments, writes a manuscript, and, finally, proofreads it. </span><span class="koboSpan" id="kobo.465.3">All this is done by an LLM-like agent that is connected to</span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.466.1"> tools and </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">proceeds sequentially.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer281">
<span class="koboSpan" id="kobo.468.1"><img alt="Figure 9.23 – Illustration of the AI scientist process (https://arxiv.org/pdf/2408.06292)" src="image/B21257_09_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.469.1">Figure 9.23 – Illustration of the AI scientist process (</span><a href="https://arxiv.org/pdf/2408.06292"><span class="koboSpan" id="kobo.470.1">https://arxiv.org/pdf/2408.06292</span></a><span class="koboSpan" id="kobo.471.1">)</span></p>
<p><span class="koboSpan" id="kobo.472.1">Other works also show similar processes, but they are still localized to specific fields and linear processes. </span><span class="koboSpan" id="kobo.472.2">For scientific research, we want to find ways to combine different expertise. </span><span class="koboSpan" id="kobo.472.3">Swanson (2024), therefore, proposes a Virtual Lab</span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.473.1"> for human-AI collaboration with the purpose of performing interdisciplinary science on complex questions. </span><span class="koboSpan" id="kobo.473.2">In the Virtual Lab, a human leads a set of interdisciplinary agents to manage a complex process. </span><span class="koboSpan" id="kobo.473.3">The different agents have different expertise and are run by an LLM. </span><span class="koboSpan" id="kobo.473.4">Each of these agents interacts with both other agents and a human being. </span><span class="koboSpan" id="kobo.473.5">In this way, the authors of the study build a flexible architecture. </span><span class="koboSpan" id="kobo.473.6">Here, the human being provides guidance to the agents, while the agents are the ones that decide on search directions and design solutions to the problem. </span><span class="koboSpan" id="kobo.473.7">Each agent is controlled by a prompt (which contains information about the role, expertise, goal, and available tools) provided to an LLM (GPT-4 in the article). </span><span class="koboSpan" id="kobo.473.8">The Virtual Lab then conducts the research in group or </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">individual meetings.</span></span></p>
<p><span class="koboSpan" id="kobo.475.1">The human provides the</span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.476.1"> question and agenda to start the discussion. </span><span class="koboSpan" id="kobo.476.2">In team meetings, agents discuss the research question and work together toward the global goal. </span><span class="koboSpan" id="kobo.476.3">In individual meetings, a single agent has to solve a task (such as writing code) and the agent works alone or together with another agent who provides critical </span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.477.1">feedback. </span><span class="koboSpan" id="kobo.477.2">With a series of global and individual meetings, the team solves a </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">research question.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer282">
<span class="koboSpan" id="kobo.479.1"><img alt="Figure 9.24 – Architecture of a Virtual Lab (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)" src="image/B21257_09_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.480.1">Figure 9.24 – Architecture of a Virtual Lab (</span><a href="https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full"><span class="koboSpan" id="kobo.481.1">https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full</span></a><span class="koboSpan" id="kobo.482.1">)</span></p>
<p><span class="koboSpan" id="kobo.483.1">In the </span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.484.1">Virtual Lab, there</span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.485.1"> is a </span><strong class="bold"><span class="koboSpan" id="kobo.486.1">Principal Investigator</span></strong><span class="koboSpan" id="kobo.487.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.488.1">PI</span></strong><span class="koboSpan" id="kobo.489.1">) whose purpose is to maximize the impact of the research, and who automatically creates a set of appropriate scientist agents (biologists or computer scientists) for the project (based on the project description provided by the PI). </span><span class="koboSpan" id="kobo.489.2">The PI defines each agent’s role, expertise, and goal in a prompt. </span><span class="koboSpan" id="kobo.489.3">In addition, there may be an agent dedicated to project critique. </span><span class="koboSpan" id="kobo.489.4">After that, the meetings begin. </span><span class="koboSpan" id="kobo.489.5">Each meeting follows a set of inputs organized into a structure: agenda (a description of what is to be discussed), agenda questions (a set of questions to be answered in the meeting), agenda rules (a set of optional rules to make the meeting smoother), summaries (optional summaries of previous meetings), contexts (additional information that can help the meeting), and rounds (the number of rounds of discussion to prevent the discussion from continuing endlessly). </span><span class="koboSpan" id="kobo.489.6">In the team meeting, all agents participate in the discussion, the human writes the agenda (optionally, along with rules and questions), and different rounds of discussion follow. </span><span class="koboSpan" id="kobo.489.7">The </span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.490.1">PI starts and then each of the scientist agents (plus the critic agent) gives their thoughts on the discussion. </span><span class="koboSpan" id="kobo.490.2">At the end, the PI summarizes the points posed by the agents, makes a decision on the agents’ inputs, and asks follow-up questions. </span><span class="koboSpan" id="kobo.490.3">After the various rounds, the PI writes a final summary that the human </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">can read.</span></span></p>
<p><span class="koboSpan" id="kobo.492.1">In individual meetings, the </span><a id="_idIndexMarker1137"/><span class="koboSpan" id="kobo.493.1">human provides the agenda and selects the agent, and the agent performs the task (there may, in addition, be the critic agent, who provides critiques). </span><span class="koboSpan" id="kobo.493.2">After a series of rounds between the agent and critic, the </span><a id="_idIndexMarker1138"/><span class="koboSpan" id="kobo.494.1">agent provides the response. </span><span class="koboSpan" id="kobo.494.2">In addition, parallel meetings may be conducted, in which multiple agents perform the same task, and in a final meeting with the PI, the final answer is </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">arrived at.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer283">
<span class="koboSpan" id="kobo.496.1"><img alt="Figure 9.25 – Virtual Lab parallel meetings (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)" src="image/B21257_09_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.497.1">Figure 9.25 – Virtual Lab parallel meetings (</span><a href="https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full"><span class="koboSpan" id="kobo.498.1">https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full</span></a><span class="koboSpan" id="kobo.499.1">)</span></p>
<p><span class="koboSpan" id="kobo.500.1">In this way, the authors have</span><a id="_idIndexMarker1139"/><span class="koboSpan" id="kobo.501.1"> created a flexible framework that combines heterogeneous agents that work in both single and collaborative settings. </span><span class="koboSpan" id="kobo.501.2">It should be noted that in this approach, there is a human in the loop; that is, a </span><a id="_idIndexMarker1140"/><span class="koboSpan" id="kobo.502.1">human being is at the center of the system and actively collaborates with the AI. </span><span class="koboSpan" id="kobo.502.2">This process mimics (though, of course, in a simplified way) the work and decision-making process of a human team when it has to solve a complex problem. </span><span class="koboSpan" id="kobo.502.3">To test the usefulness of this work, the authors tested the Virtual Lab on designing antibodies or nanobodies that can bind to the spike protein of the KP.3 variant of SARS-CoV-2. </span><span class="koboSpan" id="kobo.502.4">This is a complex problem because SARS-CoV-2 evolves rapidly, so a fast system must be found to design antibodies that can block it. </span><span class="koboSpan" id="kobo.502.5">The Virtual Lab started by creating a team that could tackle the problem (the PI created the right team of researchers for the problem). </span><span class="koboSpan" id="kobo.502.6">In a team meeting, the direction of the project was described and the principal details were discussed. </span><span class="koboSpan" id="kobo.502.7">There was then a team meeting about which tools could be used and were selected, as well as a series of individual meetings where the researchers used the various tools to create the antibody design workflow. </span><span class="koboSpan" id="kobo.502.8">In a meeting with the PI, the workflow </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">was defined.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer284">
<span class="koboSpan" id="kobo.504.1"><img alt="Figure 9.26 – Virtual Lab for antibody design (https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full)" src="image/B21257_09_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.505.1">Figure 9.26 – Virtual Lab for antibody design (</span><a href="https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full"><span class="koboSpan" id="kobo.506.1">https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full</span></a><span class="koboSpan" id="kobo.507.1">)</span></p>
<p><span class="koboSpan" id="kobo.508.1">The Virtual Lab</span><a id="_idIndexMarker1141"/><span class="koboSpan" id="kobo.509.1"> managed </span><a id="_idIndexMarker1142"/><span class="koboSpan" id="kobo.510.1">to design antibodies that they then validated experimentally. </span><span class="koboSpan" id="kobo.510.2">The system managed to create a complex workflow that used serial models to design antibodies (thus solving a real and complex problem). </span><span class="koboSpan" id="kobo.510.3">Building this would usually require a multidisciplinary team because the problem needs to be solved with different expertise. </span><span class="koboSpan" id="kobo.510.4">Thus, having agents with different expertise allows the problem to be discussed from different angles, to which a fundamental element of scientific research (critique) is added. </span><span class="koboSpan" id="kobo.510.5">This is done through a series of meetings, where the AI is a partner to the human being. </span><span class="koboSpan" id="kobo.510.6">What we see here is the creation of a multi-agent and heterogeneous system with multiple rounds of meetings (group and individual) to create a system that is flexible and sophisticated at the </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">same time.</span></span></p>
<p><span class="koboSpan" id="kobo.512.1">There are still limitations at this stage. </span><span class="koboSpan" id="kobo.512.2">For example, the models have knowledge up to a certain cut-off point, so they may not be aware of the latest published tools and could thus suggest old models (or ones that have problems in implementation). </span><span class="koboSpan" id="kobo.512.3">The solution to this problem might be to use RAG or an internet search. </span><span class="koboSpan" id="kobo.512.4">Another limitation is that the system is not exactly self-contained; it comes with both an agenda and a set of prompts that have been carefully designed. </span><span class="koboSpan" id="kobo.512.5">In this system, human beings are still involved and must provide guidance. </span><span class="koboSpan" id="kobo.512.6">Without guidance, the AI models may give vague answers or not make decisions unless specifically requested. </span><span class="koboSpan" id="kobo.512.7">Also, sometimes they do not accomplish the task or they deviate from what they are supposed to do. </span><span class="koboSpan" id="kobo.512.8">In any case, this system is flexible and can be applied agnostically to many </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">other problems.</span></span></p>
<p><span class="koboSpan" id="kobo.514.1">Combining different expertise </span><a id="_idIndexMarker1143"/><span class="koboSpan" id="kobo.515.1">with human feedback seems to be the key to better results. </span><span class="koboSpan" id="kobo.515.2">In a similar vein, Agent Laboratory is</span><a id="_idIndexMarker1144"/><span class="koboSpan" id="kobo.516.1"> designed to generate an entire research workflow (from literature review and experimentation to report writing), all from an initial human-provided research idea. </span><span class="koboSpan" id="kobo.516.2">In this system, the process begins with the collection and analysis of relevant papers, followed by collaborative planning and data preparation, a series of experiments, and report generation. </span><span class="koboSpan" id="kobo.516.3">The process can be divided into </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">three stages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.518.1">Literature review</span></strong><span class="koboSpan" id="kobo.519.1">: In this stage, articles are collected for the given research idea. </span><span class="koboSpan" id="kobo.519.2">A PhD agent utilizes the arXiv API to retrieve related papers, synthesizes them, and provides insights. </span><span class="koboSpan" id="kobo.519.3">This agent uses search APIs, summarization models, and bibliographic management systems as tools. </span><span class="koboSpan" id="kobo.519.4">The process is iterated until it reaches a certain number of </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">relevant articles.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.521.1">Experimentation</span></strong><span class="koboSpan" id="kobo.522.1">: The first step is plan formulation, where a plan is generated based on the literature review and the research goal. </span><span class="koboSpan" id="kobo.522.2">At this stage, the PhD and Postdoc agents collaborate and discuss how to achieve the goals, generating a plan that defines which ML models to implement, which datasets to use, and other necessary experimental steps. </span><span class="koboSpan" id="kobo.522.3">Once the plan is finalized, the data preparation phase begins, during which the code for data preparation is generated based on the defined plan. </span><span class="koboSpan" id="kobo.522.4">An ML engineer agent has access to Hugging Face datasets, and the code is then compiled and submitted. </span><span class="koboSpan" id="kobo.522.5">During the running experiments phase, the ML engineer agent executes the experimental plan. </span><span class="koboSpan" id="kobo.522.6">At this stage, the code is generated, tested, and refined. </span><span class="koboSpan" id="kobo.522.7">The results are then interpreted. </span><span class="koboSpan" id="kobo.522.8">At the end of this phase, the PhD and Postdoc agents discuss the results. </span><span class="koboSpan" id="kobo.522.9">If they agree on the validity of the findings, they submit the results, which will serve as the basis of </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">the report.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.524.1">Report writing</span></strong><span class="koboSpan" id="kobo.525.1">: In the report writing phase, the PhD and professor agents synthesize the research findings into a comprehensive academic report. </span><span class="koboSpan" id="kobo.525.2">Starting with an initial scaffold (abstract, introduction, background, related work, methods, experimental setup, results, and discussion), they begin generating the text (which is written in LaTeX for easy revision and correction). </span><span class="koboSpan" id="kobo.525.3">During writing, the system accesses the literature and iteratively corrects the article for accuracy, clarity, and alignment with the research goals. </span><span class="koboSpan" id="kobo.525.4">Finally, a sort of paper review is conducted to ensure the article is correct. </span><span class="koboSpan" id="kobo.525.5">Note that during this process, the system receives feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">from humans.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.527.1">The key </span><a id="_idIndexMarker1145"/><span class="koboSpan" id="kobo.528.1">features of this system</span><a id="_idIndexMarker1146"/><span class="koboSpan" id="kobo.529.1"> are that the agents perform repetitive tasks (e.g., literature searches and coding) autonomously but allow for human input where creativity or judgment is essential. </span><span class="koboSpan" id="kobo.529.2">The agents communicate intermediate results with each other to ensure cohesion among the parties. </span><span class="koboSpan" id="kobo.529.3">At each stage, there is iterative improvement through reflection </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">and feedback.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer285">
<span class="koboSpan" id="kobo.531.1"><img alt="Figure 9.27 – Agent Laboratory workflow (https://arxiv.org/pdf/2501.04227)" src="image/B21257_09_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.532.1">Figure 9.27 – Agent Laboratory workflow (</span><a href="https://arxiv.org/pdf/2501.04227"><span class="koboSpan" id="kobo.533.1">https://arxiv.org/pdf/2501.04227</span></a><span class="koboSpan" id="kobo.534.1">)</span></p>
<p><span class="koboSpan" id="kobo.535.1">Agent Laboratory </span><a id="_idIndexMarker1147"/><span class="koboSpan" id="kobo.536.1">is designed to</span><a id="_idIndexMarker1148"/><span class="koboSpan" id="kobo.537.1"> explore ideas quickly and help researchers in being able to explore multiple lines of research at the same time. </span><span class="koboSpan" id="kobo.537.2">The structure of Agent Laboratory allows it to conduct the entire workflow from an idea suggested by a human researcher. </span><span class="koboSpan" id="kobo.537.3">In this work, they focus on not only the accuracy of the results but also on trying to find a more efficient way of solving the task (previous work required too much </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">computational cost).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer286">
<span class="koboSpan" id="kobo.539.1"><img alt="Figure 9.28 – Agent Laboratory scheme (https://arxiv.org/pdf/2501.04227)" src="image/B21257_09_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.540.1">Figure 9.28 – Agent Laboratory scheme (</span><a href="https://arxiv.org/pdf/2501.04227"><span class="koboSpan" id="kobo.541.1">https://arxiv.org/pdf/2501.04227</span></a><span class="koboSpan" id="kobo.542.1">)</span></p>
<p><span class="koboSpan" id="kobo.543.1">The authors point out that incorporating human feedback at various stages significantly improved the quality of the research outputs. </span><span class="koboSpan" id="kobo.543.2">Furthermore, they state that ML code generated by Agent Laboratory achieved performance comparable to existing state-of-the-art methods and that the reports generated were of notably good quality for humans </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">reading them.</span></span></p>
<p><span class="koboSpan" id="kobo.545.1">These systems show that by </span><a id="_idIndexMarker1149"/><span class="koboSpan" id="kobo.546.1">incorporating human feedback, sophisticated tasks can be solved. </span><span class="koboSpan" id="kobo.546.2">However, these systems are dependent on human feedback because LLMs to date are not capable of true reasoning. </span><span class="koboSpan" id="kobo.546.3">There are several limitations to this: the system may struggle with designing innovative experiments beyond standard methodologies, particularly in areas requiring creative problem-solving or novel approaches. </span><span class="koboSpan" id="kobo.546.4">The system still generates errors in the code (bugs or inefficiencies), it continues to maintain a high computational cost (several LLM calls), communication between agents is not yet perfect, report generation is still suboptimal in comparison to an expert, it generalizes poorly to highly specialized or niche research</span><a id="_idIndexMarker1150"/><span class="koboSpan" id="kobo.547.1"> areas (they are poorly represented in training data and literature), and several ethical issues </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">remain open.</span></span></p>
<p><span class="koboSpan" id="kobo.549.1">In this section, we looked at different systems with a single agent or multiple agents. </span><span class="koboSpan" id="kobo.549.2">In the next section, we will see how HuggingGPT works in practice and how we can create </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">multi-agent systems.</span></span></p>
<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.551.1">Working with HuggingGPT</span></h1>
<p><span class="koboSpan" id="kobo.552.1">There are two </span><a id="_idIndexMarker1151"/><span class="koboSpan" id="kobo.553.1">ways you can </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">use HuggingGPT:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.555.1">Clone the </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">repository locally</span></span></li>
<li><span class="koboSpan" id="kobo.557.1">Use the </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">web service</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.559.1">Here, we will look at the two methods. </span><span class="koboSpan" id="kobo.559.2">The main difference is that when we clone the repository locally, we download all the models, and the system execution will be conducted locally. </span><span class="koboSpan" id="kobo.559.3">In contrast, the web service method requires that the execution is conducted in a service. </span><span class="koboSpan" id="kobo.559.4">In both cases, all models are used in inference; the difference lies in where the models are executed and the resources employed. </span><span class="koboSpan" id="kobo.559.5">Additionally, both approaches support the use of a </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">web-based GUI.</span></span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.561.1">Using HuggingGPT locally</span></h2>
<p><span class="koboSpan" id="kobo.562.1">To clone</span><a id="_idIndexMarker1152"/><span class="koboSpan" id="kobo.563.1"> HuggingGPT (the corresponding repository is called Jarvis), it is useful to use Git LFS. </span><span class="koboSpan" id="kobo.563.2">Git LFS</span><a id="_idIndexMarker1153"/><span class="koboSpan" id="kobo.564.1"> is an open source extension of Git. </span><span class="koboSpan" id="kobo.564.2">Git is designed to manage code repositories but not large binary files (such as videos, datasets, or high-resolution images). </span><span class="koboSpan" id="kobo.564.3">Git LFS is crucial for repositories that include large assets (e.g., datasets, videos, or binaries) because Git is otherwise inefficient at handling large files. </span><span class="koboSpan" id="kobo.564.4">Git LFS solves this problem by storing large files outside the regular repository objects and replacing them with lightweight references (pointers) in the Git repository. </span><span class="koboSpan" id="kobo.564.5">Git LFS keeps repository size manageable by storing large files outside the repository’s regular objects, allows for better standardization when using large objects, and improves performance during operation with GitHub repositories (such as cloning, pushing, and pulling). </span><span class="koboSpan" id="kobo.564.6">The pointer contains various metadata about the file (e.g., size, hash, and location), and when we clone a repository, Git LFS downloads the files by exploiting the information in these pointers. </span><span class="koboSpan" id="kobo.564.7">This then allows us to separate operations on the code from those conducted on the large files. </span><span class="koboSpan" id="kobo.564.8">In general, it is common to use Git LFS for projects involving ML, game development, or video editing, because it allows for simplification and speeding up of the download process. </span><span class="koboSpan" id="kobo.564.9">In ML projects, the model weights are very large and can be frequently updated; using Git LFS allows us to efficiently track and manage these files—such as downloaded models—without bloating the main repository without bloating the repository. </span><span class="koboSpan" id="kobo.564.10">As we mentioned, HuggingGPT uses several large models (for example, there are different diffusion models that can occupy several gigabytes), and Git LFS allows for </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">easier management.</span></span></p>
<p><span class="koboSpan" id="kobo.566.1">To install Git LFS, you can go to the official </span><a id="_idIndexMarker1154"/><span class="koboSpan" id="kobo.567.1">website (</span><a href="https://git-lfs.github.com/"><span class="koboSpan" id="kobo.568.1">https://git-lfs.github.com/</span></a><span class="koboSpan" id="kobo.569.1">) and download the installer for your operating system (Windows, macOS, or Linux). </span><span class="koboSpan" id="kobo.569.2">Run the downloaded installer. </span><span class="koboSpan" id="kobo.569.3">On macOS, double-click the </span><strong class="source-inline"><span class="koboSpan" id="kobo.570.1">.pkg</span></strong><span class="koboSpan" id="kobo.571.1"> file or use the Homebrew </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">package manager:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.573.1">
brew install git-lfs</span></pre> <p><span class="koboSpan" id="kobo.574.1">Run the following command to enable Git LFS for </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">your user:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.576.1">
git lfs install</span></pre> <p><span class="koboSpan" id="kobo.577.1">Once you have installed Git LFS as a Git extension on your computer, it will automatically recognize and track when there are large files in the repository and manage them. </span><span class="koboSpan" id="kobo.577.2">It modifies or creates a few Git configuration entries (such as in </span><strong class="source-inline"><span class="koboSpan" id="kobo.578.1">~/.gitconfig</span></strong><span class="koboSpan" id="kobo.579.1">) so that future clones and repositories you create can use LFS without </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">extra hassle.</span></span></p>
<p><span class="koboSpan" id="kobo.581.1">Cloning an LFS-enabled repository is as simple as if it were a regular repository (Git LFS takes care of the files in the background and large files are </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">managed automatically):</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.583.1">
git clone https://github.com/example/repo.git</span></pre> <p><span class="koboSpan" id="kobo.584.1">If we want, we can easily conduct large </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">file tracking:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.586.1">
git lfs track "*.bin"
git add .gitattributes
git commit -m "Track large .bin files with LFS"</span></pre> <p><span class="koboSpan" id="kobo.587.1">Git LFS is </span><a id="_idIndexMarker1155"/><span class="koboSpan" id="kobo.588.1">compatible with classical Git commands. </span><span class="koboSpan" id="kobo.588.2">Pull/push operations are conducted as in normal Git workflows—no special steps are required unless a repository demands specific credentials </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">or tokens.</span></span></p>
<p><span class="koboSpan" id="kobo.590.1">At this point, we</span><a id="_idIndexMarker1156"/><span class="koboSpan" id="kobo.591.1"> can proceed with the installation of HuggingGPT. </span><span class="koboSpan" id="kobo.591.2">The HuggingGPT repository is stored </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">at </span></span><a href="https://github.com/microsoft/JARVIS"><span class="No-Break"><span class="koboSpan" id="kobo.593.1">https://github.com/microsoft/JARVIS</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.594.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer287">
<span class="koboSpan" id="kobo.595.1"><img alt="Figure 9.29 – Microsoft HuggingGPT" src="image/B21257_09_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.596.1">Figure 9.29 – Microsoft HuggingGPT</span></p>
<p><span class="koboSpan" id="kobo.597.1">The first step is to clone </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">the repository:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.599.1">
git clone https://github.com/example/microsoft/JARVIS.git</span></pre> <div>
<div class="IMG---Figure" id="_idContainer288">
<span class="koboSpan" id="kobo.600.1"><img alt="Figure 9.30 – Microsoft HuggingGPT cloning" src="image/B21257_09_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.601.1">Figure 9.30 – Microsoft HuggingGPT cloning</span></p>
<p><span class="koboSpan" id="kobo.602.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.603.1">git clone</span></strong><span class="koboSpan" id="kobo.604.1"> command </span><a id="_idIndexMarker1157"/><span class="koboSpan" id="kobo.605.1">initiates the download of the repository from the remote URL. </span><span class="koboSpan" id="kobo.605.2">The terminal output indicates the repository being downloaded: objects (metadata and changes) and delta compression (a process that minimizes the amount of data transmitted by only sending differences between versions). </span><span class="koboSpan" id="kobo.605.3">Notice </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">the following:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.607.1">Receiving objects: 100% (150/150), done.</span></strong><span class="koboSpan" id="kobo.608.1">: This confirms that all objects (files and history) have </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">been received</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.610.1">Resolving deltas: 100% (85/85), done.</span></strong><span class="koboSpan" id="kobo.611.1">: Git reconstructs the actual repository state by applying the changes (</span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">deltas) received</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.613.1">Once we have cloned the repository, we can go to the local repository (the </span><span class="No-Break"><span class="koboSpan" id="kobo.614.1">local folder):</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.615.1">
cd JARVIS/hugginggpt/server</span></pre> <p><span class="koboSpan" id="kobo.616.1">This step is in preparation for creating or managing the </span><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">conda</span></strong><span class="koboSpan" id="kobo.618.1"> environment, ensuring that the actions are performed in the context of the relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">project directory.</span></span></p>
<p><span class="koboSpan" id="kobo.620.1">Then, we create a new </span><strong class="source-inline"><span class="koboSpan" id="kobo.621.1">conda</span></strong><span class="koboSpan" id="kobo.622.1"> environment named </span><strong class="source-inline"><span class="koboSpan" id="kobo.623.1">jarvis</span></strong><span class="koboSpan" id="kobo.624.1"> (or we can choose another name) and specify that it should use Python </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">version 3.8:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.626.1">
conda create -n jarvis python=3.8</span></pre> <p><span class="koboSpan" id="kobo.627.1">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">-n</span></strong><span class="koboSpan" id="kobo.629.1"> means we want a new environment for our project, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">python=3.8</span></strong><span class="koboSpan" id="kobo.631.1"> means we are explicitly defining the Python version to be 3.8 for </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">this environment.</span></span></p>
<p><span class="koboSpan" id="kobo.633.1">A </span><strong class="source-inline"><span class="koboSpan" id="kobo.634.1">conda</span></strong><span class="koboSpan" id="kobo.635.1"> environment</span><a id="_idIndexMarker1158"/><span class="koboSpan" id="kobo.636.1"> allows us to isolate dependencies and avoid conflicts with global Python installations or </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">other projects.</span></span></p>
<p><span class="koboSpan" id="kobo.638.1">Note that </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">conda</span></strong><span class="koboSpan" id="kobo.640.1"> is handling the </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">following processes:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.642.1">Collecting package metadata</span></strong><span class="koboSpan" id="kobo.643.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">conda</span></strong><span class="koboSpan" id="kobo.645.1"> fetches information about the required packages and dependencies from its repositories. </span><span class="koboSpan" id="kobo.645.2">This ensures compatibility between Python 3.8 and any other libraries to </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">be installed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.647.1">Solving the environment</span></strong><span class="koboSpan" id="kobo.648.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.649.1">conda</span></strong><span class="koboSpan" id="kobo.650.1"> resolves potential dependency conflicts and finalizes the list of packages to </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">be installed.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.652.1">Since you may have installed </span><strong class="source-inline"><span class="koboSpan" id="kobo.653.1">conda</span></strong><span class="koboSpan" id="kobo.654.1"> previously, we</span><a id="_idIndexMarker1159"/><span class="koboSpan" id="kobo.655.1"> just need to </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">update it:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.657.1">
conda update -n base -c defaults conda</span></pre> <div>
<div class="IMG---Figure" id="_idContainer289">
<span class="koboSpan" id="kobo.658.1"><img alt="Figure 9.31 – Updating conda" src="image/B21257_09_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.659.1">Figure 9.31 – Updating conda</span></p>
<p><span class="koboSpan" id="kobo.660.1">After solving the </span><a id="_idIndexMarker1160"/><span class="koboSpan" id="kobo.661.1">environment and preparing to create it, </span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">conda</span></strong><span class="koboSpan" id="kobo.663.1"> installs the required base packages for the new environment. </span><span class="koboSpan" id="kobo.663.2">Each package is listed alongside the repository (</span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">pkgs/main</span></strong><span class="koboSpan" id="kobo.665.1">) and its specific version (in this case, we are </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">using macOS).</span></span></p>
<p><span class="koboSpan" id="kobo.667.1">The terminal prompts us with </span><strong class="source-inline"><span class="koboSpan" id="kobo.668.1">Proceed ([y]/n)?</span></strong><span class="koboSpan" id="kobo.669.1">. </span><span class="koboSpan" id="kobo.669.2">Remember to respond with </span><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">y</span></strong><span class="koboSpan" id="kobo.671.1"> to confirm the installation of </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">these packages.</span></span></p>
<p><span class="koboSpan" id="kobo.673.1">Note </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">these elements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.675.1">Preparing transaction</span></strong><span class="koboSpan" id="kobo.676.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">conda</span></strong><span class="koboSpan" id="kobo.678.1"> ensures that the necessary dependencies are ready to be installed </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">without conflicts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.680.1">Verifying transaction</span></strong><span class="koboSpan" id="kobo.681.1">: It checks the integrity of the package metadata and ensures compatibility between </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">all packages</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.683.1">Executing transaction</span></strong><span class="koboSpan" id="kobo.684.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.685.1">conda</span></strong><span class="koboSpan" id="kobo.686.1"> installs the packages into the </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">specified environment</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.688.1">Once these steps are completed, the new environment (</span><strong class="source-inline"><span class="koboSpan" id="kobo.689.1">jarvis</span></strong><span class="koboSpan" id="kobo.690.1">) is ready </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">for use.</span></span></p>
<p><span class="koboSpan" id="kobo.692.1">Upon successful creation, </span><strong class="source-inline"><span class="koboSpan" id="kobo.693.1">conda</span></strong><span class="koboSpan" id="kobo.694.1"> provides </span><a id="_idIndexMarker1161"/><span class="koboSpan" id="kobo.695.1">the user with commands for managing the </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">new environment.</span></span></p>
<p><span class="koboSpan" id="kobo.697.1">To activate this environment, use </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.699.1">
conda activate jarvis</span></pre> <p><span class="koboSpan" id="kobo.700.1">To deactivate an active environment, use </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">the following:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.702.1">
conda deactivate</span></pre> <p><span class="koboSpan" id="kobo.703.1">Remember that the activation switches the user’s terminal session to use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.704.1">jarvis</span></strong><span class="koboSpan" id="kobo.705.1"> environment, isolating its dependencies and Python version. </span><span class="koboSpan" id="kobo.705.2">Notice that the prompt changes from </span><strong class="source-inline"><span class="koboSpan" id="kobo.706.1">(base)</span></strong><span class="koboSpan" id="kobo.707.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.708.1">(jarvis)</span></strong><span class="koboSpan" id="kobo.709.1">, indicating that the terminal is now operating within</span><a id="_idIndexMarker1162"/><span class="koboSpan" id="kobo.710.1"> the </span><strong class="source-inline"><span class="koboSpan" id="kobo.711.1">jarvis</span></strong><span class="koboSpan" id="kobo.712.1"> environment. </span><span class="koboSpan" id="kobo.712.2">The environment’s isolated Python version (3.8) and its dependencies are now being used. </span><span class="koboSpan" id="kobo.712.3">Any libraries or tools installed from this point will remain confined to this environment, avoiding interference with </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">other projects.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer290">
<span class="koboSpan" id="kobo.714.1"><img alt="Figure 9.32 – conda activation" src="image/B21257_09_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.715.1">Figure 9.32 – conda activation</span></p>
<p><span class="koboSpan" id="kobo.716.1">At this point, we begin to</span><a id="_idIndexMarker1163"/><span class="koboSpan" id="kobo.717.1"> install the </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">various requirements:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.719.1">
conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia</span></pre> <p><span class="koboSpan" id="kobo.720.1">The following command uses </span><strong class="source-inline"><span class="koboSpan" id="kobo.721.1">pip</span></strong><span class="koboSpan" id="kobo.722.1"> to install dependencies listed in a </span><strong class="source-inline"><span class="koboSpan" id="kobo.723.1">requirements.txt</span></strong><span class="koboSpan" id="kobo.724.1"> file (most often, a list of packages is provided in a requirements file). </span><span class="koboSpan" id="kobo.724.2">These requirements are necessary to </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">install HuggingGPT:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.726.1">
pip install -r requirements.txt</span></pre> <p><span class="koboSpan" id="kobo.727.1">The following comment in HuggingGPT emphasizes that Git LFS must be installed. </span><span class="koboSpan" id="kobo.727.2">This script (provided as part of the project) automates the download of model files required for local or hybrid inference modes. </span><span class="koboSpan" id="kobo.727.3">As a reminder, local means the model runs entirely on the local machine and hybrid means the inference involves a mix of local and remote execution, as was described in the HuggingGPT paper (</span><a href="https://arxiv.org/abs/2303.17580"><span class="koboSpan" id="kobo.728.1">https://arxiv.org/abs/2303.17580</span></a><span class="koboSpan" id="kobo.729.1">) and in the </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">preceding section:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.731.1">
# download models. </span><span class="koboSpan" id="kobo.731.2">Make sure that `git-lfs` is installed.
</span><span class="koboSpan" id="kobo.731.3">bash download.sh # required when `inference_mode` is `local` or `hybrid`</span></pre> <p><span class="koboSpan" id="kobo.732.1">Once we have installed </span><a id="_idIndexMarker1164"/><span class="koboSpan" id="kobo.733.1">everything, we can start </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">the execution:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.735.1">
python model_server.py --config config/config.default.yaml # required when `inference_mode` is `local` or `hybrid`.
</span><span class="koboSpan" id="kobo.735.2">python awesome_chat.py --config config/config.default.yaml --mode server # for text-davinci-003</span></pre> <p><span class="koboSpan" id="kobo.736.1">There are different scripts in </span><span class="No-Break"><span class="koboSpan" id="kobo.737.1">the repository:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.738.1">model_server.py</span></strong><span class="koboSpan" id="kobo.739.1">: This script runs a model server, which processes ML models based on the configuration file (</span><strong class="source-inline"><span class="koboSpan" id="kobo.740.1">config/config.default.yaml</span></strong><span class="koboSpan" id="kobo.741.1">). </span><span class="koboSpan" id="kobo.741.2">The configuration file specifies parameters such as inference mode (local or hybrid), paths to the models, and </span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">hardware requirements.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.743.1">awesome_chat.py</span></strong><span class="koboSpan" id="kobo.744.1">: This script starts a server for text generation or </span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">chatbot functionality.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer291">
<span class="koboSpan" id="kobo.746.1"><img alt="Figure 9.33 – Microsoft HuggingGPT finalizing installation" src="image/B21257_09_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.747.1">Figure 9.33 – Microsoft HuggingGPT finalizing installation</span></p>
<p><span class="koboSpan" id="kobo.748.1">Since we have </span><a id="_idIndexMarker1165"/><span class="koboSpan" id="kobo.749.1">initialized </span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">awesome_chat.py</span></strong><span class="koboSpan" id="kobo.751.1">, we can use a user-friendly </span><span class="No-Break"><span class="koboSpan" id="kobo.752.1">web page.</span></span></p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.753.1">Using HuggingGPT on the web</span></h2>
<p><span class="koboSpan" id="kobo.754.1">If you do not want </span><a id="_idIndexMarker1166"/><span class="koboSpan" id="kobo.755.1">to install HuggingGPT, you can use the online suite instead (on Hugging Face Gradio: </span><a href="https://huggingface.co/gradio"><span class="koboSpan" id="kobo.756.1">https://huggingface.co/gradio</span></a><span class="koboSpan" id="kobo.757.1">). </span><strong class="bold"><span class="koboSpan" id="kobo.758.1">Hugging Face Gradio</span></strong><span class="koboSpan" id="kobo.759.1"> is a</span><a id="_idIndexMarker1167"/><span class="koboSpan" id="kobo.760.1"> Python library that simplifies the process of creating user-friendly web-based interfaces for ML models and other </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">Python applications.</span></span></p>
<p><span class="koboSpan" id="kobo.762.1">With Gradio, developers can quickly build interactive demos for tasks such as text generation, image classification, and audio processing. </span><span class="koboSpan" id="kobo.762.2">These interfaces allow users to test models directly in their browser by providing inputs (e.g., text, images, or audio) and viewing real-time outputs. </span><span class="koboSpan" id="kobo.762.3">Gradio is highly customizable, supports integration with popular ML frameworks (such as PyTorch, TensorFlow, and Hugging Face models), and enables easy sharing of demos through public links or embedding in </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">web applications.</span></span></p>
<p><span class="koboSpan" id="kobo.764.1">The authors created a </span><a id="_idIndexMarker1168"/><span class="koboSpan" id="kobo.765.1">Gradio interface (launching Jarvis from local allows such an interface). </span><span class="koboSpan" id="kobo.765.2">The Gradio space can be accessed </span><span class="No-Break"><span class="koboSpan" id="kobo.766.1">here: </span></span><a href="https://huggingface.co/spaces/microsoft/HuggingGPT"><span class="No-Break"><span class="koboSpan" id="kobo.767.1">https://huggingface.co/spaces/microsoft/HuggingGPT</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.768.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.769.1">As said, HuggingGPT is a system that connects LLMs with the ML community. </span><span class="koboSpan" id="kobo.769.2">As seen previously in the description of the system and its installation, the web interface also does exactly the same: connect an LLM with a set of ML models that are hosted on Hugging Face. </span><span class="koboSpan" id="kobo.769.3">In the web interface, only a few models are deployed on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.770.1">local/inference</span></strong><span class="koboSpan" id="kobo.771.1"> endpoint due to hardware limitations (this interface serves as an example to understand and see in action how the </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">system works).</span></span></p>
<p><span class="koboSpan" id="kobo.773.1">Note that we need two tokens, which a user needs to obtain from </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">each website:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.775.1">Hugging Face token</span></strong><span class="koboSpan" id="kobo.776.1">: This is a personal authentication key that allows users to securely access </span><a id="_idIndexMarker1169"/><span class="koboSpan" id="kobo.777.1">Hugging Face’s services, including their API, models, datasets, and other resources hosted on the platform. </span><span class="koboSpan" id="kobo.777.2">This token acts as an identifier for your account, ensuring that your requests to Hugging Face’s systems are authorized and linked to your account. </span><span class="koboSpan" id="kobo.777.3">The token is then used to authenticate and use the models in inference. </span><span class="koboSpan" id="kobo.777.4">Hugging Face enforces rate limits for some services, especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">web inference.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.779.1">OpenAI key</span></strong><span class="koboSpan" id="kobo.780.1">: This is a unique authentication key </span><a id="_idIndexMarker1170"/><span class="koboSpan" id="kobo.781.1">provided by OpenAI that enables developers to securely access and interact with OpenAI’s APIs and services, such as GPT (e.g., GPT-3.5 or GPT-4), DALL·E, Codex, and Whisper. </span><span class="koboSpan" id="kobo.781.2">This key acts as a personalized credential that identifies your account and authorizes your usage of OpenAI’s platform. </span><span class="koboSpan" id="kobo.781.3">The key is required to authenticate requests sent to OpenAI’s API endpoints. </span><span class="koboSpan" id="kobo.781.4">OpenAI uses your API key to track your usage (e.g., the number of API calls made and tokens processed) and bill your account accordingly. </span><span class="koboSpan" id="kobo.781.5">In this case, the connection to GPT-4 </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">is used.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.783.1">Once we have our tokens ready, we can enter our question and </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">click </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.785.1">Submit</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer292">
<span class="koboSpan" id="kobo.787.1"><img alt="Figure 9.34 – HuggingGPT interface" src="image/B21257_09_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.788.1">Figure 9.34 – HuggingGPT interface</span></p>
<p><span class="koboSpan" id="kobo.789.1">We can see that there are</span><a id="_idIndexMarker1171"/><span class="koboSpan" id="kobo.790.1"> two </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">main panels:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.792.1">Left panel</span></strong><span class="koboSpan" id="kobo.793.1">: A text input box labeled </span><strong class="bold"><span class="koboSpan" id="kobo.794.1">Chatbot</span></strong><span class="koboSpan" id="kobo.795.1"> is provided. </span><span class="koboSpan" id="kobo.795.2">This field is intended for user inputs, such as questions or commands, to interact with the </span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">HuggingGPT system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.797.1">Right panel</span></strong><span class="koboSpan" id="kobo.798.1">: There is an empty box next to the chatbot reserved for responses or outputs generated </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">by HuggingGPT.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.800.1">Below the chatbox, there is a button labeled </span><strong class="bold"><span class="koboSpan" id="kobo.801.1">Send</span></strong><span class="koboSpan" id="kobo.802.1">, allowing users to submit their queries </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">to HuggingGPT.</span></span></p>
<p><span class="koboSpan" id="kobo.804.1">Note that the system already provides ready-made examples that we </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">can use:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer293">
<span class="koboSpan" id="kobo.806.1"><img alt="Figure 9.35 – HuggingGPT interface provided examples" src="image/B21257_09_35.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.807.1">Figure 9.35 – HuggingGPT interface provided examples</span></p>
<p><span class="koboSpan" id="kobo.808.1">We enter our tokens for both</span><a id="_idIndexMarker1172"/><span class="koboSpan" id="kobo.809.1"> OpenAI and Hugging Face. </span><span class="koboSpan" id="kobo.809.2">Using the text input box labeled </span><strong class="bold"><span class="koboSpan" id="kobo.810.1">Chatbot</span></strong><span class="koboSpan" id="kobo.811.1">, we can send natural language queries to HuggingGPT (“</span><em class="italic"><span class="koboSpan" id="kobo.812.1">Can you tell me which kind of pizza you see in the picture?</span></em><span class="koboSpan" id="kobo.813.1">”) and send the query with the </span><strong class="bold"><span class="koboSpan" id="kobo.814.1">Send</span></strong><span class="koboSpan" id="kobo.815.1"> button. </span><span class="koboSpan" id="kobo.815.2">In addition, images or other multimedia elements can be added (in our case, we have added a picture of </span><span class="No-Break"><span class="koboSpan" id="kobo.816.1">a pizza):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer294">
<span class="koboSpan" id="kobo.817.1"><img alt="Figure 9.36 – Example of HuggingGPT interaction" src="image/B21257_09_36.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.818.1">Figure 9.36 – Example of HuggingGPT interaction</span></p>
<p><span class="koboSpan" id="kobo.819.1">In the panel on the </span><a id="_idIndexMarker1173"/><span class="koboSpan" id="kobo.820.1">right-hand side of the figure, we see the process that the system is working through: </span><em class="italic"><span class="koboSpan" id="kobo.821.1">1 pepperoni pizza on a wooden table.</span></em><span class="koboSpan" id="kobo.822.1"> This indicates that the system successfully processed the input image and identified the object depicted as </span><em class="italic"><span class="koboSpan" id="kobo.823.1">pepperoni pizza</span></em><span class="koboSpan" id="kobo.824.1">. </span><span class="koboSpan" id="kobo.824.2">This is a typical object detection task, and the system is using a model to identify the object (it is not an LLM that conducts the image recognition but a specialized model that is invoked by </span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">the LLM).</span></span></p>
<p><span class="koboSpan" id="kobo.826.1">The chatbot provides a detailed answer based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">inference results:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.828.1">
Sure, based on the inference results, the pizza in the picture is a pepperoni pizza.</span></pre> <p><span class="koboSpan" id="kobo.829.1">HuggingGPT explains </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">the process:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.831.1">The first step involves the use of an image-to-text model to get a description of the image. </span><strong class="bold"><span class="koboSpan" id="kobo.832.1">ViT-GPT2-COCO-EN</span></strong><span class="koboSpan" id="kobo.833.1"> is a </span><a id="_idIndexMarker1174"/><span class="koboSpan" id="kobo.834.1">vision-language model that combines</span><a id="_idIndexMarker1175"/><span class="koboSpan" id="kobo.835.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.836.1">Vision Transformer</span></strong><span class="koboSpan" id="kobo.837.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.838.1">ViT</span></strong><span class="koboSpan" id="kobo.839.1">) for image encoding</span><a id="_idIndexMarker1176"/><span class="koboSpan" id="kobo.840.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.841.1">GPT-2</span></strong><span class="koboSpan" id="kobo.842.1"> for natural language generation, fine-tuned on </span><a id="_idIndexMarker1177"/><span class="koboSpan" id="kobo.843.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.844.1">COCO dataset</span></strong><span class="koboSpan" id="kobo.845.1"> for image captioning tasks. </span><span class="koboSpan" id="kobo.845.2">The model generates descriptive captions in English for input images, effectively translating visual content into coherent textual descriptions. </span><span class="koboSpan" id="kobo.845.3">It leverages the power of ViT for extracting detailed image features and GPT-2’s language generation capabilities to produce accurate and contextually </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">rich captions.</span></span></li>
<li><span class="koboSpan" id="kobo.847.1">Then, HuggingGPT uses an object detection model to identify objects within an image. </span><span class="koboSpan" id="kobo.847.2">This object detection model also provides a similar response because it identifies both a pizza and a</span><a id="_idIndexMarker1178"/><span class="koboSpan" id="kobo.848.1"> dining table. </span><strong class="bold"><span class="koboSpan" id="kobo.849.1">DETR-ResNet-101</span></strong><span class="koboSpan" id="kobo.850.1"> is a vision model designed for object detection and image segmentation. </span><span class="koboSpan" id="kobo.850.2">It combines</span><a id="_idIndexMarker1179"/><span class="koboSpan" id="kobo.851.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.852.1">ResNet-101</span></strong><span class="koboSpan" id="kobo.853.1"> backbone (a convolutional neural network) for feature extraction with a </span><strong class="bold"><span class="koboSpan" id="kobo.854.1">transformer-based architecture</span></strong><span class="koboSpan" id="kobo.855.1"> for </span><a id="_idIndexMarker1180"/><span class="koboSpan" id="kobo.856.1">detecting and localizing objects in an image. </span><strong class="bold"><span class="koboSpan" id="kobo.857.1">DEtection TRansformer</span></strong><span class="koboSpan" id="kobo.858.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.859.1">DETR</span></strong><span class="koboSpan" id="kobo.860.1">) uses transformers to model global</span><a id="_idIndexMarker1181"/><span class="koboSpan" id="kobo.861.1"> relationships in an image, allowing for more accurate object detection without the need for traditional region </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">proposal networks.</span></span></li>
<li><span class="koboSpan" id="kobo.863.1">Then, a </span><a id="_idIndexMarker1182"/><span class="koboSpan" id="kobo.864.1">visual-answering model confirms what type of pizza is in the</span><a id="_idIndexMarker1183"/><span class="koboSpan" id="kobo.865.1"> image. </span><strong class="bold"><span class="koboSpan" id="kobo.866.1">ViLT-B/32-Finetuned-VQA</span></strong><span class="koboSpan" id="kobo.867.1"> is a vision-and-language</span><a id="_idIndexMarker1184"/><span class="koboSpan" id="kobo.868.1"> transformer model fine-tuned for </span><strong class="bold"><span class="koboSpan" id="kobo.869.1">Visual Question-Answering</span></strong><span class="koboSpan" id="kobo.870.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.871.1">VQA</span></strong><span class="koboSpan" id="kobo.872.1">) tasks. </span><span class="koboSpan" id="kobo.872.2">It combines a lightweight </span><strong class="bold"><span class="koboSpan" id="kobo.873.1">Vision-and-Language Transformer</span></strong><span class="koboSpan" id="kobo.874.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.875.1">ViLT</span></strong><span class="koboSpan" id="kobo.876.1">) architecture </span><a id="_idIndexMarker1185"/><span class="koboSpan" id="kobo.877.1">with a patch-based image tokenizer and transformer layers to process both visual and textual inputs jointly. </span><span class="koboSpan" id="kobo.877.2">The B/32 refers to the use of a 32 x 32 pixel patch size for image encoding. </span><span class="koboSpan" id="kobo.877.3">Fine-tuned specifically for VQA datasets, the model is designed to answer natural language questions about input images by reasoning over the visual and </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">textual information.</span></span></li>
<li><span class="koboSpan" id="kobo.879.1">Finally, the LLM observes that the three models are in agreement and thus is confident </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">in responding.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.881.1">To recap, HuggingGPT</span><a id="_idIndexMarker1186"/><span class="koboSpan" id="kobo.882.1"> receives a request from the user and selects patterns. </span><span class="koboSpan" id="kobo.882.2">These patterns are executed, and outputs are collected. </span><span class="koboSpan" id="kobo.882.3">The system analyzes what these outputs are and generates a </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">final response.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer295">
<span class="koboSpan" id="kobo.884.1"><img alt="Figure 9.37 – Example of HuggingGPT response" src="image/B21257_09_37.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.885.1">Figure 9.37 – Example of HuggingGPT response</span></p>
<p><span class="koboSpan" id="kobo.886.1">HuggingGPT shows with a simple example how a multimodal task can be solved with an LLM. </span><span class="koboSpan" id="kobo.886.2">This is all done using information in the prompt and a set </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">of tools.</span></span></p>
<p><span class="koboSpan" id="kobo.888.1">In this section, we have seen a single LLM (a single agent) process a task, divide it into subtasks, and execute different models. </span><span class="koboSpan" id="kobo.888.2">A more elegant approach is to use multiple agents that approach a task from different perspectives, collaborate, and interact to solve a task. </span><span class="koboSpan" id="kobo.888.3">In the next subsection, we will see how this can </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">be achieved.</span></span></p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.890.1">Multi-agent system</span></h1>
<p><span class="koboSpan" id="kobo.891.1">In this section, we see how we can create a </span><a id="_idIndexMarker1187"/><span class="koboSpan" id="kobo.892.1">system that considers different agents and a set of tools (such as ML models). </span><span class="koboSpan" id="kobo.892.2">The entire code can be found in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.893.1">Multi_Model–Travel_Planning_System.py</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.894.1"> script.</span></span></p>
<p><span class="koboSpan" id="kobo.895.1">As a general overview, the system implements a travel planning assistant that uses several agents to create personalized travel plans. </span><span class="koboSpan" id="kobo.895.2">The system then combines weather prediction, hotel recommendations, itinerary planning, and email summarization. </span><span class="koboSpan" id="kobo.895.3">In other words, we have four different agents, each dealing with a different aspect of </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">travel planning:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.897.1">WeatherAnalysisAgent</span></strong><span class="koboSpan" id="kobo.898.1">: Uses a random forest regressor to predict the best time to visit a location based on historical weather data. </span><span class="koboSpan" id="kobo.898.2">Trains on past weather data (month, latitude, longitude, and weather score) and predicts the best months for travel based on weather scores. </span><span class="koboSpan" id="kobo.898.3">This agent then uses an ML model to conduct predictions (a model that is trained specifically for </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">the system).</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.900.1">HotelRecommenderAgent</span></strong><span class="koboSpan" id="kobo.901.1">: Uses Sentence Transformer embeddings to find hotels based on user preferences. </span><span class="koboSpan" id="kobo.901.2">Stores hotel descriptions and converts them into embeddings, after which it matches user preferences with the most relevant hotels using semantic similarity. </span><span class="koboSpan" id="kobo.901.3">This agent, based on user preferences, searches its library for </span><span class="No-Break"><span class="koboSpan" id="kobo.902.1">possible solutions.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.903.1">ItineraryPlannerAgent</span></strong><span class="koboSpan" id="kobo.904.1">: Uses GPT-2 (text-generation pipeline) to create personalized travel itineraries. </span><span class="koboSpan" id="kobo.904.2">The agent generates trip plans based on destination, weather prediction, and </span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">hotel recommendations.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.906.1">SummaryAgent</span></strong><span class="koboSpan" id="kobo.907.1">: Uses GPT-2 to generate a summary email for the client. </span><span class="koboSpan" id="kobo.907.2">This summary includes the hotel cost (per night cost × duration) and additional daily expenses. </span><span class="koboSpan" id="kobo.907.3">After that, it generates a personalized email with trip details, cost breakdown, and </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">itinerary highlights.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.909.1">The following figure presents a schema of the agents and </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">the process:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer296">
<span class="koboSpan" id="kobo.911.1"><img alt="Figure 9.38 – Activity diagram of the AI Travel Planning System workflow showing the full sequence from data loading and agent initialization to trip planning and result output" src="image/B21257_09_38.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.912.1">Figure 9.38 – Activity diagram of the AI Travel Planning System workflow showing the full sequence from data loading and agent initialization to trip planning and result output</span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.913.1">TravelPlanningSystem</span></strong><span class="koboSpan" id="kobo.914.1"> links all agents </span><a id="_idIndexMarker1188"/><span class="koboSpan" id="kobo.915.1">together and is basically the main controller of the system. </span><span class="koboSpan" id="kobo.915.2">The system thus mimics </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">this flow:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.917.1">The user provides the destination, preferences, </span><span class="No-Break"><span class="koboSpan" id="kobo.918.1">and duration.</span></span></li>
<li><span class="koboSpan" id="kobo.919.1">The weather agent predicts the best time </span><span class="No-Break"><span class="koboSpan" id="kobo.920.1">to visit.</span></span></li>
<li><span class="koboSpan" id="kobo.921.1">The hotel agent finds </span><span class="No-Break"><span class="koboSpan" id="kobo.922.1">matching accommodation.</span></span></li>
<li><span class="koboSpan" id="kobo.923.1">The itinerary agent creates </span><span class="No-Break"><span class="koboSpan" id="kobo.924.1">daily plans.</span></span></li>
<li><span class="koboSpan" id="kobo.925.1">The summary agent generates an email and </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">calculates costs.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.927.1">Going into detail, we can see that agents here are defined as classes. </span><strong class="source-inline"><span class="koboSpan" id="kobo.928.1">WeatherAnalysisAgent</span></strong><span class="koboSpan" id="kobo.929.1"> is an ML-based component that analyzes historical weather data and predicts the best months to visit a given location. </span><span class="koboSpan" id="kobo.929.2">It does this using a Random Forest Regressor. </span><span class="koboSpan" id="kobo.929.3">We can see it as an agent using an ML model to perform a task. </span><span class="koboSpan" id="kobo.929.4">This snippet is initializing </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">the agent:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.931.1">
class WeatherAnalysisAgent:
def __init__(self):
           self.model = RandomForestRegressor(n_estimators=100)</span></pre> <p><span class="koboSpan" id="kobo.932.1">This agent creates a </span><strong class="source-inline"><span class="koboSpan" id="kobo.933.1">RandomForestRegressor</span></strong><span class="koboSpan" id="kobo.934.1"> model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.935.1">n_estimators=100</span></strong><span class="koboSpan" id="kobo.936.1"> means the model consists of 100 decision trees) that must learn patterns from historical weather data, and then must predict weather scores for different months </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">and locations:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.938.1">
def train(self, historical_data: Dict):
        X = np.array([[d['month'], d['latitude'], d['longitude']] for d in historical_data])
        y = np.array([d['weather_score'] for d in historical_data])
        self.model.fit(X, y)</span></pre> <p><span class="koboSpan" id="kobo.939.1">As mentioned before, this model is not trained (i.e., it is not used in inference) but is trained on the spot. </span><span class="koboSpan" id="kobo.939.2">For this, we have within our class a </span><strong class="source-inline"><span class="koboSpan" id="kobo.940.1">train</span></strong><span class="koboSpan" id="kobo.941.1"> method. </span><span class="koboSpan" id="kobo.941.2">Random forest uses month, latitude, and longitude for a location to learn to predict a </span><strong class="source-inline"><span class="koboSpan" id="kobo.942.1">weather_score</span></strong><span class="koboSpan" id="kobo.943.1"> value (a numerical score representing how good the weather is in that month). </span><span class="koboSpan" id="kobo.943.2">In this snippet, the data is processed and the model </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">is trained.</span></span></p>
<p><span class="koboSpan" id="kobo.945.1">At this point, we</span><a id="_idIndexMarker1189"/><span class="koboSpan" id="kobo.946.1"> can use </span><strong class="source-inline"><span class="koboSpan" id="kobo.947.1">predict_best_time</span></strong><span class="koboSpan" id="kobo.948.1"> as a method that predicts the best months to visit a location based on the trained weather model. </span><span class="koboSpan" id="kobo.948.2">In this case, the method takes only two inputs (the latitude and longitude of the location) and returns </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">its predictions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.950.1">
def predict_best_time(self, location: Dict) -&gt; Dict:
        # Predicts the best time to visit a location based on weather patterns
        predictions = []
        for month in range(1, 13):
            # predict returns a 2D array, we take the first (and only) element
            prediction = self.model.predict([[
                month,
                location['latitude'],
                location['longitude']
            ]]).item()  # .item() converts numpy array to scalar
            predictions.append({'month': month, 'score': float(prediction)})
        return {
            'best_months': sorted(predictions, key=lambda x: x['score'], reverse=True)[:3],
            'location': location
        }</span></pre> <p><span class="koboSpan" id="kobo.951.1">Note that we initialize predictions, which will contain all scores for 12 months (in fact, predictions are conducted in a loop through all 12 months, from January to December). </span><span class="koboSpan" id="kobo.951.2">Finally, we reorder the list from best to worst to identify the best months to visit. </span><span class="koboSpan" id="kobo.951.3">The method then returns a list of the top three months with the highest predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">weather scores.</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.953.1">HotelRecommenderAgent</span></strong><span class="koboSpan" id="kobo.954.1"> is a hotel </span><a id="_idIndexMarker1190"/><span class="koboSpan" id="kobo.955.1">recommendation system that utilizes semantic similarity to match hotels with user preferences and uses natural language processing (</span><strong class="bold"><span class="koboSpan" id="kobo.956.1">NLP</span></strong><span class="koboSpan" id="kobo.957.1">) to understand and compare hotel descriptions and </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">user preferences:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.959.1">
class HotelRecommenderAgent:
    def __init__(self):
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')
        self.hotels_db = []
        self.hotels_embeddings = None</span></pre> <p><span class="koboSpan" id="kobo.960.1">During agent initialization, </span><strong class="source-inline"><span class="koboSpan" id="kobo.961.1">all-MiniLM-L6-v2</span></strong><span class="koboSpan" id="kobo.962.1"> (a pre-trained NLP model designed for semantic similarity) is loaded. </span><span class="koboSpan" id="kobo.962.2">This model is an embedder (as described in </span><a href="B21257_05.xhtml#_idTextAnchor077"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.963.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.964.1">), converting text (hotel descriptions and user preferences) into vector embeddings (numerical representations in a multi-dimensional space). </span><span class="koboSpan" id="kobo.964.2">Once we have vectors, we can measure the similarity between two vectors (user preferences and hotel descriptions). </span><span class="koboSpan" id="kobo.964.3">The agent retrieves the available hotels (</span><strong class="source-inline"><span class="koboSpan" id="kobo.965.1">self.hotels_db</span></strong><span class="koboSpan" id="kobo.966.1">) and can store precomputed embeddings (numerical vectors) for all hotel </span><span class="No-Break"><span class="koboSpan" id="kobo.967.1">descriptions (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.968.1">self.hotels_embeddings</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.969.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.970.1">Next, in the following snippet, we have </span><strong class="source-inline"><span class="koboSpan" id="kobo.971.1">add_hotels</span></strong><span class="koboSpan" id="kobo.972.1">, which adds hotels to the database and computes the embedding for the description, and then adds it to our embeddings database. </span><strong class="source-inline"><span class="koboSpan" id="kobo.973.1">find_hotels</span></strong><span class="koboSpan" id="kobo.974.1"> finds hotels that match the user’s preferences using </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">semantic similarity:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.976.1">
def add_hotels(self, hotels: List[Dict]):
        self.hotels_db = hotels
        descriptions = [h['description'] for h in hotels]
        self.hotels_embeddings = self.encoder.encode(descriptions)
    def find_hotels(self, preferences: str, top_k: int = 5) -&gt; List[Dict]:
        pref_embedding = self.encoder.encode([preferences])
        similarities = np.dot(self.hotels_embeddings, pref_embedding.T).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1]
        return [
            {**self.hotels_db[i], 'similarity_score': float(similarities[i])}
            for i in top_indices
        ]</span></pre> <p><span class="koboSpan" id="kobo.977.1">What happens is that we conduct embedding of a user’s preferences and then compute the cosine similarity with all stored hotel vectors. </span><span class="koboSpan" id="kobo.977.2">In this case, we then select the five hotels that are closest to our hotel description (</span><strong class="source-inline"><span class="koboSpan" id="kobo.978.1">top_k=5</span></strong><span class="koboSpan" id="kobo.979.1"> means selecting the top </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">five hotels).</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.981.1">ItineraryPlannerAgent</span></strong><span class="koboSpan" id="kobo.982.1"> is responsible </span><a id="_idIndexMarker1191"/><span class="koboSpan" id="kobo.983.1">for automatically generating travel itineraries based on destination information (city or attractions), weather predictions (best months to visit), hotel recommendations (selected accommodation), and trip duration (number of days). </span><span class="koboSpan" id="kobo.983.2">It uses a natural language model (GPT-2) to generate customized travel itineraries based on </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">these inputs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.985.1">
class ItineraryPlannerAgent:
    def __init__(self):
        # Uses a language model for generating itineraries
        self.planner = pipeline(
            "text-generation",
            model="gpt2",  # In production, use a more powerful model
            max_length=500,
            truncation=True,
            pad_token_id=50256
        )</span></pre> <p><span class="koboSpan" id="kobo.986.1">The agent initializes an NLP model (GPT-2 Model, which is a pre-trained language model for text generation) using the Hugging Face transformers library. </span><span class="koboSpan" id="kobo.986.2">We select a pipeline that is focused on text generation (</span><strong class="source-inline"><span class="koboSpan" id="kobo.987.1">"text-generation"</span></strong><span class="koboSpan" id="kobo.988.1"> means the model will generate text based on a prompt). </span><span class="koboSpan" id="kobo.988.2">Other parameters mean we limit the generated text to 500 tokens (</span><strong class="source-inline"><span class="koboSpan" id="kobo.989.1">max_length=500</span></strong><span class="koboSpan" id="kobo.990.1">) and we </span><span class="No-Break"><span class="koboSpan" id="kobo.991.1">ensure truncation.</span></span></p>
<p><span class="koboSpan" id="kobo.992.1">Since we interact with </span><a id="_idIndexMarker1192"/><span class="koboSpan" id="kobo.993.1">LLMs through prompts, we have a method that allows us to create a structured prompt that we will then use to interact with the model. </span><span class="koboSpan" id="kobo.993.2">This prompt is designed to be able to generate a travel plan, where it enters some specific information: the length of stay (duration), the destination, weather information (the best mounts we identified earlier), hotel selection (which were identified with the previous agent), and a list </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">of attractions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.995.1">
def _create_prompt(self, destination_info: Dict, weather_info: Dict,
                   hotel_info: Dict, duration: int) -&gt; str:
    return f"""Create a {duration}-day itinerary for {destination_info['name']}.
</span><span class="koboSpan" id="kobo.995.2">Weather: {weather_info['best_months'][0]['month']} is the best month.
</span><span class="koboSpan" id="kobo.995.3">Hotel: Staying at {hotel_info[0]['name']}.
</span><span class="koboSpan" id="kobo.995.4">Attractions: {', '.join(destination_info['attractions'])}."""</span></pre> <p><span class="koboSpan" id="kobo.996.1">At this point, we can create the itinerary; the </span><strong class="source-inline"><span class="koboSpan" id="kobo.997.1">create_itinerary</span></strong><span class="koboSpan" id="kobo.998.1"> method precisely takes the previous prompt that contains all the information we need (destination, weather, hotel selection, and trip duration). </span><span class="koboSpan" id="kobo.998.2">Inside the </span><strong class="source-inline"><span class="koboSpan" id="kobo.999.1">create_itinerary</span></strong><span class="koboSpan" id="kobo.1000.1"> method is a method called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1001.1">_create_prompt</span></strong><span class="koboSpan" id="kobo.1002.1"> to generate the prompt. </span><span class="koboSpan" id="kobo.1002.2">The GPT-2 model takes the input prompt and produces a </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">detailed itinerary:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1004.1">
def create_itinerary(self, destination_info: Dict, weather_info: Dict,
                     hotel_info: Dict, duration: int) -&gt; Dict:
    prompt = self._create_prompt(destination_info, weather_info, hotel_info, duration)
    #Generate the itinerary
    response = self.planner(prompt)[0]['generated_text']
    return {
        'itinerary': response,
        'duration': duration,
        'destination': destination_info['name']
    }</span></pre> <p><span class="koboSpan" id="kobo.1005.1">The final agent, that is, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1006.1">SummaryAgent</span></strong><span class="koboSpan" id="kobo.1007.1">, is responsible for summarizing trip details, calculating the total estimated cost, and generating a personalized email for the client using GPT-2. </span><span class="koboSpan" id="kobo.1007.2">Our agent </span><a id="_idIndexMarker1193"/><span class="koboSpan" id="kobo.1008.1">is initialized similar to the previous agent; the only difference is that in this case, the generation length is </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">greater (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1010.1">max_length=1000</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1012.1">
class SummaryAgent:
    def __init__(self):
        # In production, use a more powerful LLM like GPT-4 or Claude
        self.llm = pipeline(
            "text-generation",
            model="gpt2",
            max_length=1000,
            truncation=True,
            pad_token_id=50256
        )</span></pre> <p><strong class="source-inline"><span class="koboSpan" id="kobo.1013.1">calculate_total_price</span></strong><span class="koboSpan" id="kobo.1014.1"> is a tool that is used by the agent to be able to calculate the total cost of the trip (remember that LLMs are not good at arithmetic, so it is better to use an </span><span class="No-Break"><span class="koboSpan" id="kobo.1015.1">external tool):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1016.1">
def calculate_total_price(self, hotel_info: Dict, duration: int) -&gt; float:
        # Calculate total trip price
        hotel_cost = hotel_info[0]['price'] * duration
        # Estimate additional costs (activities, meals, transport)
        daily_expenses = 100  # Simplified example
        additional_costs = daily_expenses * duration
        return hotel_cost + additional_costs</span></pre> <p><span class="koboSpan" id="kobo.1017.1">The agent does a series of very </span><span class="No-Break"><span class="koboSpan" id="kobo.1018.1">simple calculations:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1019.1">The hotel price per night is multiplied by the duration of </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">the stay</span></span></li>
<li><span class="koboSpan" id="kobo.1021.1">A fixed daily expense of $100 is used to estimate costs for meals, transport, activities, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1022.1">sightseeing tickets</span></span></li>
<li><span class="koboSpan" id="kobo.1023.1">Hotel and additional costs are added to return the </span><span class="No-Break"><span class="koboSpan" id="kobo.1024.1">final estimate</span></span></li>
</ul>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">create_email</span></strong><span class="koboSpan" id="kobo.1026.1"> allows </span><a id="_idIndexMarker1194"/><span class="koboSpan" id="kobo.1027.1">you to create the email summary that will be sent to </span><span class="No-Break"><span class="koboSpan" id="kobo.1028.1">the customer:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1029.1">
def create_email(self, trip_data: Dict, client_name: str) -&gt; Dict:
        total_price = self.calculate_total_price(
            trip_data['recommended_hotels'],
            trip_data['itinerary']['duration']
        )
        prompt = f"""
        Dear {client_name},
        Based on your preferences, I'm pleased to present your travel plan:
        Destination: {trip_data['itinerary']['destination']}
        Duration: {trip_data['itinerary']['duration']} days
        Best time to visit: Month {trip_data['weather_analysis']['best_months'][0]['month']}
        Recommended Hotel: {trip_data['recommended_hotels'][0]['name']}
        Itinerary Overview:
        {trip_data['itinerary']['itinerary']}
        Estimated Total Cost: ${total_price}
        Please let me know if you would like any adjustments.
</span><span class="koboSpan" id="kobo.1029.2">        """
        # Generate email using LLM
        response = self.llm(prompt)[0]['generated_text']
        return {
            'email_content': response,
            'total_price': total_price,
            'summary_data': {
                'destination': trip_data['itinerary']['destination'],
                'duration': trip_data['itinerary']['duration'],
                'hotel': trip_data['recommended_hotels'][0]['name'],
                'best_month': trip_data['weather_analysis']['best_months'][0]['month']
            }
        }</span></pre> <p><span class="koboSpan" id="kobo.1030.1">As we can see, the email will be structured to include costs (we use the method described previously) and the other information we obtained earlier. </span><span class="koboSpan" id="kobo.1030.2">Note that we use </span><span class="No-Break"><span class="koboSpan" id="kobo.1031.1">a template.</span></span></p>
<p><span class="koboSpan" id="kobo.1032.1">Remember, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1033.1">TravelPlanningSystem</span></strong><span class="koboSpan" id="kobo.1034.1"> is the </span><a id="_idIndexMarker1195"/><span class="koboSpan" id="kobo.1035.1">main controller that integrates all AI agents for automated </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">travel planning:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1037.1">
class TravelPlanningSystem:
    def __init__(self):
        self.weather_agent = WeatherAnalysisAgent()
        self.hotel_agent = HotelRecommenderAgent()
        self.itinerary_agent = ItineraryPlannerAgent()
        self.summary_agent = SummaryAgent()</span></pre> <p><span class="koboSpan" id="kobo.1038.1">In the first step, we initialize our four agents. </span><span class="koboSpan" id="kobo.1038.2">Each agent will handle a specific task. </span><span class="koboSpan" id="kobo.1038.3">If you noticed, we have used a modular system. </span><span class="koboSpan" id="kobo.1038.4">The advantages of this are </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1040.1">Each component operates independently, making the </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">system scalable</span></span></li>
<li><span class="koboSpan" id="kobo.1042.1">Components can be updated or replaced without </span><span class="No-Break"><span class="koboSpan" id="kobo.1043.1">affecting others</span></span></li>
<li><span class="koboSpan" id="kobo.1044.1">It </span><a id="_idIndexMarker1196"/><span class="koboSpan" id="kobo.1045.1">follows the </span><strong class="bold"><span class="koboSpan" id="kobo.1046.1">Single Responsibility Principle</span></strong><span class="koboSpan" id="kobo.1047.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1048.1">SRP</span></strong><span class="koboSpan" id="kobo.1049.1">) for clean </span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">code architecture</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1051.1">At this point, we can start the setup – getting the best hotels and the best months </span><span class="No-Break"><span class="koboSpan" id="kobo.1052.1">to visit:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1053.1">
def setup(self, historical_weather_data: Dict, hotels_database: List[Dict]):
        # Initialize and train the models
        self.weather_agent.train(historical_weather_data)
        self.hotel_agent.add_hotels(hotels_database)</span></pre> <p><span class="koboSpan" id="kobo.1054.1">Finally, you have to</span><a id="_idIndexMarker1197"/><span class="koboSpan" id="kobo.1055.1"> coordinate the entire trip and then generate the summary email with cost estimates and </span><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">the itinerary:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1057.1">
def plan_trip(self, destination: Dict, preferences: str, duration: int, client_name: str) -&gt; Dict:
        # 1. </span><span class="koboSpan" id="kobo.1057.2">Weather analysis and best time prediction
        weather_analysis = self.weather_agent.predict_best_time(destination)
        # 2. </span><span class="koboSpan" id="kobo.1057.3">Hotel search
        recommended_hotels = self.hotel_agent.find_hotels(preferences)
        # 3. </span><span class="koboSpan" id="kobo.1057.4">Itinerary creation
        itinerary = self.itinerary_agent.create_itinerary(
            destination,
            weather_analysis,
            recommended_hotels,
            duration
        )
        # 4. </span><span class="koboSpan" id="kobo.1057.5">Create summary email and calculate price
        trip_data = {
            'weather_analysis': weather_analysis,
            'recommended_hotels': recommended_hotels,
            'itinerary': itinerary
        }
        summary = self.summary_agent.create_email(trip_data, client_name)
        return {
            **trip_data,
            'summary': summary
        }</span></pre> <p><span class="koboSpan" id="kobo.1058.1">Now that we have created</span><a id="_idIndexMarker1198"/><span class="koboSpan" id="kobo.1059.1"> the multi-agent platform, we have to execute it. </span><span class="koboSpan" id="kobo.1059.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">main()</span></strong><span class="koboSpan" id="kobo.1061.1"> function serves as the entry point for running the </span><em class="italic"><span class="koboSpan" id="kobo.1062.1">Travel Planning System</span></em><span class="koboSpan" id="kobo.1063.1">. </span><span class="koboSpan" id="kobo.1063.2">It demonstrates the system’s functionality by doing </span><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1065.1">Initializing sample data (weather history </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">and hotels)</span></span></li>
<li><span class="koboSpan" id="kobo.1067.1">Setting up and training </span><span class="No-Break"><span class="koboSpan" id="kobo.1068.1">AI models</span></span></li>
<li><span class="koboSpan" id="kobo.1069.1">Executing the travel </span><span class="No-Break"><span class="koboSpan" id="kobo.1070.1">planning process</span></span></li>
<li><span class="koboSpan" id="kobo.1071.1">Printing the generated </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">trip summary</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1073.1">We provide the system with various information about the weather, destination, hotels, and so on.  </span><span class="koboSpan" id="kobo.1073.2">After that, the system is initialized and executed. </span><span class="koboSpan" id="kobo.1073.3">At this point, it prints travel summary details and the personalized email generated by GPT-2, and it shows the estimated total </span><span class="No-Break"><span class="koboSpan" id="kobo.1074.1">trip cost:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1075.1">
def main():
    # Example data with a full year of weather information
    historical_weather_data = […    ]
    # Sample hotel database
    hotels_database = […]
    # Initialize the system
    system = TravelPlanningSystem()
    system.setup(historical_weather_data, hotels_database)
    # Plan a trip
    destination = {
        'name': 'Rome',
        'latitude': 41.9028,
        'longitude': 12.4964,
        'attractions': ['Colosseum', 'Vatican', 'Trevi Fountain']
    }
    preferences = """Looking for a luxury hotel in the city center,
    preferably with spa facilities and fine dining options"""
    client_name = "John Smith"
    # Generate trip plan
    trip_plan = system.plan_trip(destination, preferences, duration=3, client_name=client_name)
    # Print results in a readable format
    print("\nTRAVEL PLANNING RESULTS:")
    print("-" * 50)
    print(f"Client: {client_name}")
    print(f"Destination: {destination['name']}")
    print("\nGenerated Email:")
    print("-" * 20)
    print(trip_plan['summary']['email_content'])
    print("\nEstimated Total Price:")
    print(f"${trip_plan['summary']['total_price']}")</span></pre> <p><span class="koboSpan" id="kobo.1076.1">Ensure that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1077.1">main()</span></strong><span class="koboSpan" id="kobo.1078.1">  script runs only if the script is </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">executed directly:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1080.1">
if __name__ == "__main__":
    main()</span></pre> <p><span class="koboSpan" id="kobo.1081.1">At this point, we just have to </span><a id="_idIndexMarker1199"/><span class="koboSpan" id="kobo.1082.1">test it. </span><span class="koboSpan" id="kobo.1082.2">Once you have run the script, this should be </span><span class="No-Break"><span class="koboSpan" id="kobo.1083.1">the result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer297">
<span class="koboSpan" id="kobo.1084.1"><img alt="Figure 9.39 – Screenshots showing the execution" src="image/B21257_09_39.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1085.1">Figure 9.39 – Screenshots showing the execution</span></p>
<p><span class="koboSpan" id="kobo.1086.1">This </span><em class="italic"><span class="koboSpan" id="kobo.1087.1">Travel Planning System</span></em><span class="koboSpan" id="kobo.1088.1"> is a</span><a id="_idIndexMarker1200"/><span class="koboSpan" id="kobo.1089.1"> prototype demonstrating how AI agents can collaborate to automate a </span><span class="No-Break"><span class="koboSpan" id="kobo.1090.1">real-world problem.</span></span></p>
<p><span class="koboSpan" id="kobo.1091.1">Of course, a whole series of improvements can be made to make the system </span><span class="No-Break"><span class="koboSpan" id="kobo.1092.1">more useful:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1093.1">The data used is static (it is a toy example). </span><span class="koboSpan" id="kobo.1093.2">You could connect with a number of APIs to obtain real-time data for the weather (OpenWeatherMap or AccuWeather), hotels (Booking.com or Expedia API), and destinations (Google Places API or Yelp). </span><span class="koboSpan" id="kobo.1093.3">Extensions such as flights and transportation could also be added (Google Flights API </span><span class="No-Break"><span class="koboSpan" id="kobo.1094.1">or Rome2Rio).</span></span></li>
<li><span class="koboSpan" id="kobo.1095.1">GPT-2 is outdated (we used it because it is much smaller than other models) and not fine-tuned for travel. </span><span class="koboSpan" id="kobo.1095.2">You can replace GPT-2 with a larger or travel-optimized model. </span><span class="koboSpan" id="kobo.1095.3">For example, you could use larger models such as GPT-4 or Claude, or open source alternatives such as LLaMA. </span><span class="koboSpan" id="kobo.1095.4">Also, open source models can be fine-tuned on real travel itineraries from Tripadvisor, Lonely Planet, </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">or Reddit.</span></span></li>
<li><span class="koboSpan" id="kobo.1097.1">The itinerary is</span><a id="_idIndexMarker1201"/><span class="koboSpan" id="kobo.1098.1"> generic and not adaptable to different types of travelers. </span><span class="koboSpan" id="kobo.1098.2">You could ask for different information from the traveler, such as budget preferences, what kinds of activities they prefer (cultural, adventure, food, family-friendly, and so on), or whether they need special accommodations (wheelchair, traveling with elderly, or pet-friendly). </span><span class="koboSpan" id="kobo.1098.3">This requires a larger model, and you can also test recommendation models. </span><span class="koboSpan" id="kobo.1098.4">In addition, there are methods and models that implement </span><strong class="bold"><span class="koboSpan" id="kobo.1099.1">Multi-Criteria Decision-Making</span></strong><span class="koboSpan" id="kobo.1100.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1101.1">MCDM</span></strong><span class="koboSpan" id="kobo.1102.1">) to </span><a id="_idIndexMarker1202"/><span class="koboSpan" id="kobo.1103.1">conduct more </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">sophisticated rankings.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1105.1">In any case, this system, though simple, allows us to see several </span><span class="No-Break"><span class="koboSpan" id="kobo.1106.1">interesting elements:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1107.1">Instead of using one large monolithic AI model, the system is broken down into specialized agents. </span><span class="koboSpan" id="kobo.1107.2">This idea can come in handy for modern </span><span class="No-Break"><span class="koboSpan" id="kobo.1108.1">software design.</span></span></li>
<li><span class="koboSpan" id="kobo.1109.1">This simple example mimics how multi-agent AI platforms work in autonomous vehicles, finance, healthcare, and robotics. </span><span class="koboSpan" id="kobo.1109.2">In fact, multi-agent collaboration is a system designed with scalability, modularity, and efficiency in mind, which are necessary for </span><span class="No-Break"><span class="koboSpan" id="kobo.1110.1">real-world applications.</span></span></li>
<li><span class="koboSpan" id="kobo.1111.1">The system can dynamically generate personalized recommendations (although in our case, it is hardcoded, we are mimicking what happens when a user enters </span><span class="No-Break"><span class="koboSpan" id="kobo.1112.1">their preferences).</span></span></li>
<li><span class="koboSpan" id="kobo.1113.1">The system also analyzes multiple factors (weather, hotels, and attractions) and optimizes travel plans. </span><span class="koboSpan" id="kobo.1113.2">Modern systems that do something similar use precise ML models (we used random forest in our example), have vast databases (in our case, we are mimicking a database of hotels), take user preferences into account, and use automated systems to respond to the customer (</span><span class="No-Break"><span class="koboSpan" id="kobo.1114.1">our email).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1115.1">Although this is a very simple system, we can think about how a similar system could be used in various </span><span class="No-Break"><span class="koboSpan" id="kobo.1116.1">other industries:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1117.1">AI medical assistants that recommend treatments, optimize hospital schedules, and predict </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">disease risks</span></span></li>
<li><span class="koboSpan" id="kobo.1119.1">AI shopping assistants that recommend products based on user preferences and </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">purchase history</span></span></li>
<li><span class="koboSpan" id="kobo.1121.1">Multi-agent AI systems for self-driving cars (navigation, pedestrian detection, or </span><span class="No-Break"><span class="koboSpan" id="kobo.1122.1">traffic optimization)</span></span></li>
<li><span class="koboSpan" id="kobo.1123.1">AI-driven advisors that help with investment strategies, risk management, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">fraud detection</span></span></li>
<li><span class="koboSpan" id="kobo.1125.1">An AI-powered urban </span><a id="_idIndexMarker1203"/><span class="koboSpan" id="kobo.1126.1">planner that optimizes traffic, energy use, and public </span><span class="No-Break"><span class="koboSpan" id="kobo.1127.1">transport routes</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1128.1">In this section, we looked at how to create a multi-agent system. </span><span class="koboSpan" id="kobo.1128.2">In the next section, we will discuss how multi-agent systems fit into the various business models that exist today or are under greater development. </span><span class="koboSpan" id="kobo.1128.3">This will provide an important perspective, as it will allow you to understand how to adapt your multi-agent platform to the needs </span><span class="No-Break"><span class="koboSpan" id="kobo.1129.1">of businesses.</span></span></p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.1130.1">SaaS, MaaS, DaaS, and RaaS</span></h1>
<p><span class="koboSpan" id="kobo.1131.1">In this section, we will explore various business models influenced by recent advancements in AI. </span><span class="koboSpan" id="kobo.1131.2">While multi-agent LLMs represent cutting-edge technology, their value lies in being adaptable to meet business needs, enabling them to be effectively packaged, marketed, and delivered to businesses and consumers. </span><span class="koboSpan" id="kobo.1131.3">Considering that these systems are extremely expensive to develop and maintain, it is important for the reader to understand what the revenue models are so that they can think about, design, and develop products that align with the company’s strategy. </span><span class="koboSpan" id="kobo.1131.4">Understanding these models allows us to grasp that a multi-agent system is not a standalone item but should be considered a product and that this product can be marketed in various ways. </span><span class="koboSpan" id="kobo.1131.5">In addition, LLMs are extremely expensive products, and each of these business models has advantages and disadvantages in terms of continuous updates, scalability, and flexibility in AI deployment. </span><span class="koboSpan" id="kobo.1131.6">At the same time, these business models regulate access to technology whether you are interested in developing AI models or are a customer. </span><span class="koboSpan" id="kobo.1131.7">These choices (about the platform, business models, and so on) must be made before the product is developed, and they determine its development, since the costs do not allow for trial and error. </span><span class="koboSpan" id="kobo.1131.8">The choice of business model is defined by the structure of the product and the multi-agent system, as well as the economic viability of </span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">the company.</span></span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.1133.1">Software as a Service (SaaS)</span></h2>
<p><span class="koboSpan" id="kobo.1134.1">SaaS is a service model in which </span><a id="_idIndexMarker1204"/><span class="koboSpan" id="kobo.1135.1">software is hosted in the cloud by a provider and is made available to users over the internet. </span><span class="koboSpan" id="kobo.1135.2">In the traditional model, software is provided to the user to be installed and used locally (on the user’s device). </span><span class="koboSpan" id="kobo.1135.3">SaaS, on the other hand, allows access over the internet, usually on the web browser or with a mobile app. </span><span class="koboSpan" id="kobo.1135.4">Often, SaaS is provided via subscription rather than through a one-time purchase. </span><span class="koboSpan" id="kobo.1135.5">The </span><a id="_idIndexMarker1205"/><span class="koboSpan" id="kobo.1136.1">SaaS paradigm began in 1999 when Salesforce launched its </span><strong class="bold"><span class="koboSpan" id="kobo.1137.1">Customer Relationship Management</span></strong><span class="koboSpan" id="kobo.1138.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1139.1">CRM</span></strong><span class="koboSpan" id="kobo.1140.1">) as a cloud-hosted service. </span><span class="koboSpan" id="kobo.1140.2">SaaS is now the most widely used sales paradigm by</span><a id="_idIndexMarker1206"/><span class="koboSpan" id="kobo.1141.1"> different companies, especially for </span><strong class="bold"><span class="koboSpan" id="kobo.1142.1">Business-to-Business</span></strong><span class="koboSpan" id="kobo.1143.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1144.1">B2B</span></strong><span class="koboSpan" id="kobo.1145.1">) applications. </span><span class="koboSpan" id="kobo.1145.2">Its popularity is growing, and it is expected that SaaS software revenue will grow more and more in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1146.1">coming years.</span></span></p>
<p><span class="koboSpan" id="kobo.1147.1">SaaS applications are typically built to be hosted in the cloud (they are called cloud-native). </span><span class="koboSpan" id="kobo.1147.2">The company developing these apps can decide whether to host on its own infrastructure or leverage that of cloud service </span><a id="_idIndexMarker1207"/><span class="koboSpan" id="kobo.1148.1">providers (examples are Google Cloud, IBM Cloud, OVH, Aruba, </span><strong class="bold"><span class="koboSpan" id="kobo.1149.1">Amazon Web Services</span></strong><span class="koboSpan" id="kobo.1150.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1151.1">AWS</span></strong><span class="koboSpan" id="kobo.1152.1">), and Microsoft Azure). </span><span class="koboSpan" id="kobo.1152.2">Given the demand for app providers, some providers create focused infrastructure for</span><a id="_idIndexMarker1208"/><span class="koboSpan" id="kobo.1153.1"> hosting these apps, and so we also talk about </span><strong class="bold"><span class="koboSpan" id="kobo.1154.1">Platform as a </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1155.1">Service</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1156.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1157.1">PaaS</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.1159.1">In PaaS solutions, a provider conducts hosting of both hardware and software through dedicated infrastructure that is made available to product developers. </span><span class="koboSpan" id="kobo.1159.2">This allows developers to focus on coding without having to worry about maintaining or managing the infrastructure behind it. </span><span class="koboSpan" id="kobo.1159.3">The platform allows the hosting of both the application and the data, or even the training of a model, leaving only the coding to the developer. </span><span class="koboSpan" id="kobo.1159.4">This has enabled accelerated product development by many businesses,  who have managed to avoid investing in expensive infrastructure (although extensive use of these platforms can have a high cost, especially when the applications are generative AI). </span><span class="koboSpan" id="kobo.1159.5">Although PaaS allows a simplification of the process, developers are forced to conform their applications to the requirements of the platforms and environment. </span><span class="koboSpan" id="kobo.1159.6">This is not always possible, resulting in difficulties in deployment or other issues. </span><span class="koboSpan" id="kobo.1159.7">Therefore, an alternative paradigm has emerged that allows the user greater flexibility, control, and adaptability, especially when the</span><a id="_idIndexMarker1209"/><span class="koboSpan" id="kobo.1160.1"> application or business requires it. </span><span class="koboSpan" id="kobo.1160.2">This paradigm is called </span><strong class="bold"><span class="koboSpan" id="kobo.1161.1">Infrastructure as a Service</span></strong><span class="koboSpan" id="kobo.1162.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1163.1">IaaS</span></strong><span class="koboSpan" id="kobo.1164.1">) and </span><a id="_idIndexMarker1210"/><span class="koboSpan" id="kobo.1165.1">emerged around 2010. </span><span class="koboSpan" id="kobo.1165.2">In IaaS, a user can access computing resources through web services, thus being able to rent infrastructure (servers, networking, and storage) as needed. </span><span class="koboSpan" id="kobo.1165.3">The user retains more control over the infrastructure, while the provider focuses on the hardware (examples include Google Compute Engine, DigitalOcean, and Amazon Elastic Compute Cloud). </span><span class="koboSpan" id="kobo.1165.4">PaaS and IaaS can thus be seen as extensions of SaaS or as services for businesses that need a </span><span class="No-Break"><span class="koboSpan" id="kobo.1166.1">supporting ecosystem.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer298">
<span class="koboSpan" id="kobo.1167.1"><img alt="Figure 9.40 – Comparison between different paradigms (https://arxiv.org/pdf/2311.05804)" src="image/B21257_09_40.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1168.1">Figure 9.40 – Comparison between different paradigms (</span><a href="https://arxiv.org/pdf/2311.05804"><span class="koboSpan" id="kobo.1169.1">https://arxiv.org/pdf/2311.05804</span></a><span class="koboSpan" id="kobo.1170.1">)</span></p>
<p><span class="koboSpan" id="kobo.1171.1">SaaS </span><a id="_idIndexMarker1211"/><span class="koboSpan" id="kobo.1172.1">applications are therefore designed to be accessible via an internet connection from a device that must be connected to the internet in order to access the application (a device that is not connected cannot access the application and it is not a requirement to allow access locally). </span><span class="koboSpan" id="kobo.1172.2">Software is developed to be used through a web browser or with a specific app (mobile software). </span><span class="koboSpan" id="kobo.1172.3">Some SaaS applications (as in the case of Adobe Acrobat) may require the user to download and install a dedicated client (a light program, which is not the full application, that has to be installed on a local PC) on their computers (but this is generally a minority of cases). </span><span class="koboSpan" id="kobo.1172.4">A SaaS application is generally a </span><strong class="bold"><span class="koboSpan" id="kobo.1173.1">multi-tenant software architecture</span></strong><span class="koboSpan" id="kobo.1174.1">, where a </span><a id="_idIndexMarker1212"/><span class="koboSpan" id="kobo.1175.1">single instance of a software application (along with its database and hardware) serves different user accounts (or multiple tenants). </span><span class="koboSpan" id="kobo.1175.2">A tenant is what is called a user of the software, and it is a user or group of users within </span><span class="No-Break"><span class="koboSpan" id="kobo.1176.1">an organization.</span></span></p>
<p><span class="koboSpan" id="kobo.1177.1">In SaaS, it is crucial to have an architecture that ensures each tenant’s data is isolated and inaccessible to other tenants. </span><span class="koboSpan" id="kobo.1177.2">This approach offers the advantage of cost reduction by enabling the software to be optimized for a single piece of hardware and infrastructure, which is then shared among all users. </span><span class="koboSpan" id="kobo.1177.3">It also allows for greater scalability, easier customization, and maintenance (providers can conduct the update on their own infrastructure and on a </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">single architecture).</span></span></p>
<p><span class="koboSpan" id="kobo.1179.1">SaaS is therefore one of the </span><a id="_idIndexMarker1213"/><span class="koboSpan" id="kobo.1180.1">most widely used paradigms because it has a number </span><span class="No-Break"><span class="koboSpan" id="kobo.1181.1">of advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1182.1">Cost efficiency</span></strong><span class="koboSpan" id="kobo.1183.1">: There are no upfront costs to the customer, such as expenses for hardware or a software license. </span><span class="koboSpan" id="kobo.1183.2">In SaaS, the customer either pays by subscription or on a </span><span class="No-Break"><span class="koboSpan" id="kobo.1184.1">pay-as-you-go basis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1185.1">Scalability</span></strong><span class="koboSpan" id="kobo.1186.1">: SaaS scales easily for the customer and does not require additional hardware. </span><span class="koboSpan" id="kobo.1186.2">Similarly, software is structured to make it easy to scale up customers. </span><span class="koboSpan" id="kobo.1186.3">In the case of AI models, the customer does not need large hardware but can directly leverage that provided by </span><span class="No-Break"><span class="koboSpan" id="kobo.1187.1">the provider.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1188.1">Accessibility</span></strong><span class="koboSpan" id="kobo.1189.1">: The customer can access the application from anywhere in the world via an internet connection. </span><span class="koboSpan" id="kobo.1189.2">Also, using the web browser, the software is optimized for whatever hardware the client has. </span><span class="koboSpan" id="kobo.1189.3">SaaS also reduces the barrier of access to AI for clients (fewer resources and less need for expertise) through the use of templates, APIs, </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">and frameworks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1191.1">Ease of integration and customization</span></strong><span class="koboSpan" id="kobo.1192.1">: It is much easier for the developer to provide updates, security patches, and maintenance, in terms of both resources and time. </span><span class="koboSpan" id="kobo.1192.2">The ability to manage customization for the client is usually provided in an easier way, while at the same time maintaining control. </span><span class="koboSpan" id="kobo.1192.3">Equally, for an AI system, updated templates can </span><span class="No-Break"><span class="koboSpan" id="kobo.1193.1">be provided.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1194.1">Fast deployment</span></strong><span class="koboSpan" id="kobo.1195.1">: SaaS reduces deployment and market access time by being immediately available in </span><span class="No-Break"><span class="koboSpan" id="kobo.1196.1">the marketplace.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1197.1">Data and model sharing</span></strong><span class="koboSpan" id="kobo.1198.1">: Model and data access can be easily allowed to users from different teams or </span><a id="_idIndexMarker1214"/><span class="koboSpan" id="kobo.1199.1">in various locations simultaneously and effectively </span><span class="No-Break"><span class="koboSpan" id="kobo.1200.1">and efficiently.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1201.1">There are, of course, also some </span><a id="_idIndexMarker1215"/><span class="koboSpan" id="kobo.1202.1">limitations and disadvantages </span><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">to SaaS:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1204.1">Dependency on internet connectivity</span></strong><span class="koboSpan" id="kobo.1205.1">: SaaS requires a stable connection, and connection disruptions can stop critical processes and errors. </span><span class="koboSpan" id="kobo.1205.2">Rural areas and countries with little infrastructure may not </span><span class="No-Break"><span class="koboSpan" id="kobo.1206.1">be covered.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1207.1">Limited customization</span></strong><span class="koboSpan" id="kobo.1208.1">: SaaS solutions are developed with the idea of covering as much business as possible with one product. </span><span class="koboSpan" id="kobo.1208.2">Typically, they provide a limited number of customization possibilities that may not cover all the needs of a particular business. </span><span class="koboSpan" id="kobo.1208.3">This is also true in the case of an AI system; the client has little control over the models and the models may not be able to meet </span><span class="No-Break"><span class="koboSpan" id="kobo.1209.1">client requirements.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1210.1">Data security and privacy concerns</span></strong><span class="koboSpan" id="kobo.1211.1">: Hosting on third-party servers brings the risk of data breaches or unauthorized access. </span><span class="koboSpan" id="kobo.1211.2">In addition, there may be compliance issues with regulations in countries such as the European Union (e.g., data must be maintained on servers in certain countries). </span><span class="koboSpan" id="kobo.1211.3">Training or using AI models may require having to share sensitive data, and this may be against GDPR or other regulations (as well as an additional </span><span class="No-Break"><span class="koboSpan" id="kobo.1212.1">privacy risk).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1213.1">Vendor lock-in</span></strong><span class="koboSpan" id="kobo.1214.1">: Businesses may remain anchored to a particular SaaS provider and then be unable to migrate to other platforms due to cost and complexity. </span><span class="koboSpan" id="kobo.1214.2">In addition, different providers may terminate the service (or be acquired), increase costs abruptly, or eliminate features considered essential. </span><span class="koboSpan" id="kobo.1214.3">SaaS can become expensive over a period of time, especially when subscription-based (some providers charge more as </span><span class="No-Break"><span class="koboSpan" id="kobo.1215.1">users increase).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1216.1">Performance issues</span></strong><span class="koboSpan" id="kobo.1217.1">: Shared resources in multi-tenant architectures can lead to slower performance during peak usage. </span><span class="koboSpan" id="kobo.1217.2">In addition, there may be unexpected server downtime or maintenance schedules that hurt the business (for example, if maintenance is conducted at night on Pacific Time, it disrupts business hours in Europe) and over which the customer has no control. </span><span class="koboSpan" id="kobo.1217.3">AI systems that must run in real time may have latency or performance problems (both in training and inference). </span><span class="koboSpan" id="kobo.1217.4">In addition, the provider may not provide cutting-edge AI or may not have implemented it yet (or they may use models that do not fit the </span><span class="No-Break"><span class="koboSpan" id="kobo.1218.1">customer’s needs).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1219.1">High computational costs</span></strong><span class="koboSpan" id="kobo.1220.1">: SaaS has</span><a id="_idIndexMarker1216"/><span class="koboSpan" id="kobo.1221.1"> an infrastructure cost for the developer, and in the case of AI, this cost can be higher (use of GPUs or large storage costs). </span><span class="koboSpan" id="kobo.1221.2">Some of these services are particularly expensive </span><span class="No-Break"><span class="koboSpan" id="kobo.1222.1">for users.</span></span></li>
</ul>
<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.1223.1">Model as a Service (MaaS)</span></h2>
<p><span class="koboSpan" id="kobo.1224.1">MaaS is a new paradigm that was </span><a id="_idIndexMarker1217"/><span class="koboSpan" id="kobo.1225.1">born with the development of big data, AI, and Web 3.0. </span><span class="koboSpan" id="kobo.1225.2">MaaS is a cloud computing-based service paradigm that offers AI and ML models and related IaaS to developers </span><span class="No-Break"><span class="koboSpan" id="kobo.1226.1">and enterprises.</span></span></p>
<p><span class="koboSpan" id="kobo.1227.1">MaaS seeks to simplify access to AI for businesses that have neither the expertise nor the infrastructure to train generative AI or broad models in general. </span><span class="koboSpan" id="kobo.1227.2">MaaS enables the use of pre-trained ML models and algorithms through the use of simple interfaces, APIs, or the browser. </span><span class="koboSpan" id="kobo.1227.3">Just like with SaaS, access to models is through the internet (and requires the business to have an internet connection). </span><span class="koboSpan" id="kobo.1227.4">The provider must then conduct the hosting of the models and allow developers access to the models that have been trained. </span><span class="koboSpan" id="kobo.1227.5">Developers can then use these models to add AI functions to their systems and apps. </span><span class="koboSpan" id="kobo.1227.6">MaaS is often a platform where models that have been trained on a large amount of data or optimized for a possible task are hosted. </span><span class="koboSpan" id="kobo.1227.7">MaaS reduces the complexity of managing these models (especially training and deployment) and allows developers to focus on using the models or how to integrate them for specific applications. </span><span class="koboSpan" id="kobo.1227.8">Developers save time and resources since they do not have to train these models from scratch. </span><span class="koboSpan" id="kobo.1227.9">MaaS thus has certain similarities to PaaS and IaaS but conducts an additional level of abstraction and focuses on AI solutions. </span><span class="koboSpan" id="kobo.1227.10">In a sense, MaaS can be viewed as an intermediate solution between SaaS and PaaS or IaaS. </span><span class="koboSpan" id="kobo.1227.11">It not only provides a service but also offers an infrastructure that enables the development of </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">custom products.</span></span></p>
<p><span class="koboSpan" id="kobo.1229.1">Another difference between SaaS and MaaS is in the underlying architecture of the two paradigms. </span><span class="koboSpan" id="kobo.1229.2">SaaS focuses on applications (application layer) that depend on an operating system (whether mobile or desktop application) that allows them to run, as well as on a layer that allows the app to be hosted. </span><span class="koboSpan" id="kobo.1229.3">In the case of MaaS, the architecture focuses on the model that needs a specific</span><a id="_idIndexMarker1218"/><span class="koboSpan" id="kobo.1230.1"> framework to </span><span class="No-Break"><span class="koboSpan" id="kobo.1231.1">be hosted.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer299">
<span class="koboSpan" id="kobo.1232.1"><img alt="Figure 9.41 – Comparison between traditional and model-based technology stacks (https://arxiv.org/pdf/2311.05804)" src="image/B21257_09_41.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1233.1">Figure 9.41 – Comparison between traditional and model-based technology stacks (</span><a href="https://arxiv.org/pdf/2311.05804"><span class="koboSpan" id="kobo.1234.1">https://arxiv.org/pdf/2311.05804</span></a><span class="koboSpan" id="kobo.1235.1">)</span></p>
<p><span class="koboSpan" id="kobo.1236.1">In MaaS, the following elements are </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">often present:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1238.1">Cloud computing</span></strong><span class="koboSpan" id="kobo.1239.1">: MaaS is</span><a id="_idIndexMarker1219"/><span class="koboSpan" id="kobo.1240.1"> based on an infrastructure on the cloud where various models are maintained and deployed. </span><span class="koboSpan" id="kobo.1240.2">This allows easy access to the models and enables </span><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">greater scalability.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1242.1">Model training and optimization</span></strong><span class="koboSpan" id="kobo.1243.1">: MaaS providers take care of the training of large models on large datasets. </span><span class="koboSpan" id="kobo.1243.2">MaaS providers also take care of the entire ecosystem to enable more effective exploitation of models. </span><span class="koboSpan" id="kobo.1243.3">For example, they can provide models of different sizes, including quantized or fine-tuned versions for </span><span class="No-Break"><span class="koboSpan" id="kobo.1244.1">specific applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1245.1">API and development tools</span></strong><span class="koboSpan" id="kobo.1246.1">: MaaS providers also provide APIs and tools that allow the developer to use the models for their applications easily. </span><span class="koboSpan" id="kobo.1246.2">The purpose is to allow easy integration of models into other applications and infrastructures. </span><span class="koboSpan" id="kobo.1246.3">So, the API acts as an endpoint, takes data, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1247.1">returns predictions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1248.1">Monitoring and analytics</span></strong><span class="koboSpan" id="kobo.1249.1">: To date, there is increasing focus on how to monitor models once they are in production. </span><span class="koboSpan" id="kobo.1249.2">MaaS providers typically provide a number of tools to monitor model performance, identify the presence of issues, integrate feedback, or improve </span><span class="No-Break"><span class="koboSpan" id="kobo.1250.1">resource allocation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1251.1">Scalability, security, and privacy</span></strong><span class="koboSpan" id="kobo.1252.1">: MaaS providers focus on the scalability of their systems by allowing customers to be able to manage multiple users at the same time (thus allocating different bandwidth, computing power, or storage as needed). </span><span class="koboSpan" id="kobo.1252.2">At the same time, today there is more attention to privacy and security (especially as there is much more regulation). </span><span class="koboSpan" id="kobo.1252.3">Platforms often have a number of tools to be able to increase the privacy and security of applications that integrate </span><span class="No-Break"><span class="koboSpan" id="kobo.1253.1">their models.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1254.1">Hugging Face is an example of a MaaS provider. </span><span class="koboSpan" id="kobo.1254.2">Hugging Face provides access to thousands of pre-trained models (from the company itself, other companies, or users) for computer vision, NLP, audio, video, and more. </span><span class="koboSpan" id="kobo.1254.3">These models are hosted on their Model Hub and can be either used via an API or installed locally. </span><span class="koboSpan" id="kobo.1254.4">So, a user who doesn’t want to download models can use an inference API without owning the infrastructure needed to manage the model (this API uses the pay-as-you-go system). </span><span class="koboSpan" id="kobo.1254.5">Developers who do not have the expertise or resources can directly use the endpoint API to directly integrate AI models within their applications. </span><span class="koboSpan" id="kobo.1254.6">In addition, Hugging Face also offers a platform for hosting and deploying both the model and application, extending MaaS capabilities and providing flexibility to customers who want to use their custom models. </span><span class="koboSpan" id="kobo.1254.7">Hugging Face also provides tools to improve the scalability of models and open source libraries to facilitate model development or integration (e.g., Transformers, Datasets, Diffusers, sentence embedding, and so on), as well as offering a forum to enable user exchange, educational resources for users, and other services. </span><span class="koboSpan" id="kobo.1254.8">There are other MaaS providers, such as Google AI (pre-trained models for NLP (Natural Language API), vision (Vision API), speech to text, translation, or custom model training with Vertex AI) and AWS (which offers pre-trained models for language, image, and text (e.g., AWS Comprehend, Rekognition, and Translate) or infrastructure for </span><span class="No-Break"><span class="koboSpan" id="kobo.1255.1">custom models).</span></span></p>
<p><span class="koboSpan" id="kobo.1256.1">MaaS has the</span><a id="_idIndexMarker1220"/><span class="koboSpan" id="kobo.1257.1"> following advantages, especially regarding the </span><span class="No-Break"><span class="koboSpan" id="kobo.1258.1">AI domain:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1259.1">Simplified model development and deployment</span></strong><span class="koboSpan" id="kobo.1260.1">: MaaS lowers the technical barrier to using generative AI. </span><span class="koboSpan" id="kobo.1260.2">Companies do not need developers who are experts in the technology or different algorithms because most models are delivered via endpoints. </span><span class="koboSpan" id="kobo.1260.3">This allows companies to focus on applications and model integration for their products. </span><span class="koboSpan" id="kobo.1260.4">If needed, MaaS also simplifies the approach to fine-tuning models for their applications. </span><span class="koboSpan" id="kobo.1260.5">MaaS, as opposed to SaaS, is tailored to the entire AI workflow and offers tools for deploying, training, managing, and scaling models, thus enabling better support for companies interested in </span><span class="No-Break"><span class="koboSpan" id="kobo.1261.1">using AI.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1262.1">High performance and scalability</span></strong><span class="koboSpan" id="kobo.1263.1">: The use of cloud computing facilitates system scaling. </span><span class="koboSpan" id="kobo.1263.2">In fact, the use of AI can require high costs and large resources (especially when it comes to using LLMs), and MaaS allows for better resource management by facilitating access to large models without initial entry costs for different businesses. </span><span class="koboSpan" id="kobo.1263.3">Typically, users pay for their consumption and receive computing according to their needs, thus enabling better performance and scalability. </span><span class="koboSpan" id="kobo.1263.4">Since MaaS is optimized for AI workloads, it can scale easily when there are fluctuating computational demands (SaaS typically focuses on allocating a variable number of users, but users may have a different need for computing depending on the different usage </span><span class="No-Break"><span class="koboSpan" id="kobo.1264.1">of models).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1265.1">Shared knowledge and collaboration</span></strong><span class="koboSpan" id="kobo.1266.1">: MaaS is built on collecting large datasets and training large models. </span><span class="koboSpan" id="kobo.1266.2">These pre-trained models can then be fine-tuned by developers interested in adapting the models to particular applications. </span><span class="koboSpan" id="kobo.1266.3">This means that developers need to collect much less data and do not have to train large models from scratch. </span><span class="koboSpan" id="kobo.1266.4">This saves both resources and costs (fine-tuning is much less computationally expensive than pre-training). </span><span class="koboSpan" id="kobo.1266.5">In addition, MaaS allows standardization that reduces the technical knowledge required to be able to use these models and allows information and tutorials to be obtained easily. </span><span class="koboSpan" id="kobo.1266.6">Models can then also be shared by the community on platforms on which both information and experiences are also exchanged (this promotes a collaborative environment and accelerates the development of </span><span class="No-Break"><span class="koboSpan" id="kobo.1267.1">new models).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1268.1">Business support</span></strong><span class="koboSpan" id="kobo.1269.1">: MaaS uses a flexible payment model, such as subscription based, where you pay only for current consumption. </span><span class="koboSpan" id="kobo.1269.2">Generally, this solution is cost effective and affordable for many small businesses. </span><span class="koboSpan" id="kobo.1269.3">It is convenient for providers because once they choose a technology and integrate it into their products, users remain loyal. </span><span class="koboSpan" id="kobo.1269.4">Model integration allows businesses to gain insights in an easy and inexpensive way (models for forecasts or other predictions, report writing, </span><span class="No-Break"><span class="koboSpan" id="kobo.1270.1">and visualizations).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1271.1">Flexibility</span></strong><span class="koboSpan" id="kobo.1272.1">: MaaS provides</span><a id="_idIndexMarker1221"/><span class="koboSpan" id="kobo.1273.1"> models for a large number of applications and allows businesses to integrate a large number of potential models, providing wide flexibility (e.g., NLP, computer vision, time series, and so many other applications). </span><span class="koboSpan" id="kobo.1273.2">In addition, developers can test many pre-trained models quickly without changing setups (e.g., Hugging Face offers thousands of models that can be used with just a few pipelines). </span><span class="koboSpan" id="kobo.1273.3">Similarly, MaaS providers often offer many tools to simplify the AI life cycle (data labeling, data format integration, monitoring tools, and so on) from training </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">to deployment.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1275.1">MaaS is a new paradigm, and the field</span><a id="_idIndexMarker1222"/><span class="koboSpan" id="kobo.1276.1"> of generative AI is also in active development, so there are challenges and possible drawbacks that need to </span><span class="No-Break"><span class="koboSpan" id="kobo.1277.1">be addressed:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1278.1">Security and privacy</span></strong><span class="koboSpan" id="kobo.1279.1">: Often, a large amount of data is transferred, especially for model training, which can be intercepted. </span><span class="koboSpan" id="kobo.1279.2">In addition, models trained on sensitive data can end up outputting sensitive data. </span><span class="koboSpan" id="kobo.1279.3">These models could also be trained on copyrighted data, and the legislation on training with such data is not entirely clear. </span><span class="koboSpan" id="kobo.1279.4">So, organizations that adhere to particularly regulated industries may not adopt MaaS. </span><span class="koboSpan" id="kobo.1279.5">Data is the basis of these models, but the models could be trained on, or become biased due to, low-quality data. </span><span class="koboSpan" id="kobo.1279.6">Often, there is no information on what data these models were trained on. </span><span class="koboSpan" id="kobo.1279.7">In these cases, both the platform and the businesses using these models may be subject to fines or </span><span class="No-Break"><span class="koboSpan" id="kobo.1280.1">other regulations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1281.1">Vendor lock-in</span></strong><span class="koboSpan" id="kobo.1282.1">: MaaS providers use proprietary tools and APIs, which does not make it easy to change from one provider to another (e.g., changing providers complicates model integration or exporting models that have been fine-tuned). </span><span class="koboSpan" id="kobo.1282.2">This difficulty can reduce flexibility and innovation and can make a business dependent on a single provider. </span><span class="koboSpan" id="kobo.1282.3">There may be downtime or service disruption that impacts built applications. </span><span class="koboSpan" id="kobo.1282.4">It also makes it more difficult to </span><span class="No-Break"><span class="koboSpan" id="kobo.1283.1">experiment locally.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1284.1">Limited customization</span></strong><span class="koboSpan" id="kobo.1285.1">: Not all MaaS providers allow the fine-tuning or modification of pre-trained models. </span><span class="koboSpan" id="kobo.1285.2">Pre-trained models may not be suitable for some particular operations, or a business may need to have control over hyperparameters and infrastructure. </span><span class="koboSpan" id="kobo.1285.3">In addition, MaaS providers may make changes or plan updates that impact the business or no longer allow some core features of </span><span class="No-Break"><span class="koboSpan" id="kobo.1286.1">their applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1287.1">Interpretability of model and results</span></strong><span class="koboSpan" id="kobo.1288.1">: A model is often a black box, and a user cannot access the decision-making process. </span><span class="koboSpan" id="kobo.1288.2">Especially for GenAI models, it is difficult to understand how the model processes the input and gets the output. </span><span class="koboSpan" id="kobo.1288.3">For sensitive applications, this could cause problems, especially when the model produces hallucinations or incorrect outputs. </span><span class="koboSpan" id="kobo.1288.4">In addition, the lack of transparency of the platforms may affect the ability to diagnose errors or know how to </span><span class="No-Break"><span class="koboSpan" id="kobo.1289.1">correct them.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1290.1">Performance and cost</span></strong><span class="koboSpan" id="kobo.1291.1">: Latency refers to the time elapsed between a request and its corresponding response. </span><span class="koboSpan" id="kobo.1291.2">The latency of models depends on the underlying infrastructure, which can experience strain during periods of peak usage. </span><span class="koboSpan" id="kobo.1291.3">Shared multi-tenant environments in MaaS platforms can lead to resource bottlenecks during peak usage times. </span><span class="koboSpan" id="kobo.1291.4">Businesses may encounter a considerable increase in latency that makes their applications unusable. </span><span class="koboSpan" id="kobo.1291.5">MaaS allows pay as you go, but</span><a id="_idIndexMarker1223"/><span class="koboSpan" id="kobo.1292.1"> large-scale training or inference can quickly </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">become expensive.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1294.1">MaaS remains an expanding paradigm for several businesses. </span><span class="koboSpan" id="kobo.1294.2">For example, MaaS could have a big impact in healthcare where there are large amounts of data, and many models have already been developed. </span><span class="koboSpan" id="kobo.1294.3">The models could be available on a platform and be used when needed by practitioners or pharmaceutical companies. </span><span class="koboSpan" id="kobo.1294.4">Obviously, in healthcare, data security and output consistency are critical (especially if these applications are used for hospitals or other health providers). </span><span class="koboSpan" id="kobo.1294.5">MaaS is also growing in other domains, such as finance, blockchain, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1295.1">Web 3.0.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer300">
<span class="koboSpan" id="kobo.1296.1"><img alt="Figure 9.42 – The applications of various industries within MaaS (https://arxiv.org/pdf/2311.05804)" src="image/B21257_09_42.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1297.1">Figure 9.42 – The applications of various industries within MaaS (</span><a href="https://arxiv.org/pdf/2311.05804"><span class="koboSpan" id="kobo.1298.1">https://arxiv.org/pdf/2311.05804</span></a><span class="koboSpan" id="kobo.1299.1">)</span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.1300.1">Data as a Service (DaaS)</span></h2>
<p><span class="koboSpan" id="kobo.1301.1">DaaS is a business model </span><a id="_idIndexMarker1224"/><span class="koboSpan" id="kobo.1302.1">where data is delivered on demand to users regardless of their geographical location or organizational boundaries. </span><span class="koboSpan" id="kobo.1302.2">In DaaS, data is stored in the cloud and a client can access it (with or without additional tools) by paying a subscription to a provider. </span><span class="koboSpan" id="kobo.1302.3">DaaS, therefore, is built around the concept that data is an asset and can be provided to users on demand. </span><span class="koboSpan" id="kobo.1302.4">This access can then be conducted through a platform, the use of APIs, or additional means. </span><span class="koboSpan" id="kobo.1302.5">In addition, the provider can provide either raw data or data that has been normalized to be machine-readable </span><span class="No-Break"><span class="koboSpan" id="kobo.1303.1">or machine-ready.</span></span></p>
<p><span class="koboSpan" id="kobo.1304.1">AI models are notoriously data hungry, and retrieving quality data may not be easy. </span><span class="koboSpan" id="kobo.1304.2">So, there are players who focus on collecting hard-to-access data and then selling it to other players. </span><span class="koboSpan" id="kobo.1304.3">For example, patient data can be difficult to collect, and a company may collect and process the data and then sell it to pharmaceutical companies. </span><span class="koboSpan" id="kobo.1304.4">Alternatively, DaaS allows companies to create a new business model, using data collected during their normal operations as an asset they can sell. </span><span class="koboSpan" id="kobo.1304.5">For example, a telecom company that has collected data from its users can sell the anonymized data to retailers. </span><span class="koboSpan" id="kobo.1304.6">This data is sold through a secure portal and can be charged for on a per-access basis or through a subscription. </span><span class="koboSpan" id="kobo.1304.7">Subscription is usually the most popular method and can be divided into three subcategories: time model, quantity-based pricing model, and pay-per-call or data type </span><span class="No-Break"><span class="koboSpan" id="kobo.1305.1">base model.</span></span></p>
<p><span class="koboSpan" id="kobo.1306.1">A DaaS provider may just sell the raw data it has collected, but more often it also processes it and makes it analyzable by models. </span><span class="koboSpan" id="kobo.1306.2">Some DaaS providers aggregate different sources, process them, and thus simplify the analysis process for a client. </span><span class="koboSpan" id="kobo.1306.3">In fact, the purpose of this data is to improve business processes and decision-making for customers, or to allow customers to train their </span><span class="No-Break"><span class="koboSpan" id="kobo.1307.1">AI models.</span></span></p>
<p><span class="koboSpan" id="kobo.1308.1">There can also be bidirectionality, in which the provider collects the data and harmonizes it to integrate it with its own, before making it accessible to the client again. </span><span class="koboSpan" id="kobo.1308.2">In this way, by relating it to other data, the client can extract additional value from its </span><span class="No-Break"><span class="koboSpan" id="kobo.1309.1">own data.</span></span></p>
<p><span class="koboSpan" id="kobo.1310.1">DaaS has </span><span class="No-Break"><span class="koboSpan" id="kobo.1311.1">some advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1312.1">Cost efficiency</span></strong><span class="koboSpan" id="kobo.1313.1">: DaaS </span><a id="_idIndexMarker1225"/><span class="koboSpan" id="kobo.1314.1">reduces customers’ need to build and maintain data infrastructure and teams. </span><span class="koboSpan" id="kobo.1314.2">It also reduces the cost of data access because of its flexibility. </span><span class="koboSpan" id="kobo.1314.3">Customers do not need to store data; they can directly access the data stream when they </span><span class="No-Break"><span class="koboSpan" id="kobo.1315.1">need it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1316.1">Ease of access</span></strong><span class="koboSpan" id="kobo.1317.1">: Providing data on demand allows real-time access and saves time and expertise to obtain data information. </span><span class="koboSpan" id="kobo.1317.2">Users do not need to know the data and the structure behind it, but they can easily learn how to use it. </span><span class="koboSpan" id="kobo.1317.3">Also, as long as there is an internet connection, the client can always access </span><span class="No-Break"><span class="koboSpan" id="kobo.1318.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1319.1">Scalability</span></strong><span class="koboSpan" id="kobo.1320.1">: It easily scales to accommodate increasing data needs without requiring additional infrastructure investment. </span><span class="koboSpan" id="kobo.1320.2">Customers can easily choose the data workload they need or </span><span class="No-Break"><span class="koboSpan" id="kobo.1321.1">can handle.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1322.1">Centralized data management</span></strong><span class="koboSpan" id="kobo.1323.1">: DaaS enables consistent and centralized data storage, reducing both inconsistencies and redundancies in data. </span><span class="koboSpan" id="kobo.1323.2">This enables simplified data governance and compliance </span><span class="No-Break"><span class="koboSpan" id="kobo.1324.1">with regulations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1325.1">Focus on core activities</span></strong><span class="koboSpan" id="kobo.1326.1">: DaaS saves resources and time, allowing businesses to focus on extracting value from data rather than managing it. </span><span class="koboSpan" id="kobo.1326.2">In addition, it enables better collaboration among the various team members and collaborators, which can then access the same data (in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1327.1">same format).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1328.1">Integration with other services</span></strong><span class="koboSpan" id="kobo.1329.1">: DaaS makes it easy to integrate data with other services in the business, especially when it comes to analytics platforms, visualization tools, and other cloud services. </span><span class="koboSpan" id="kobo.1329.2">Likewise, it facilitates the regular updating of datasets and allows users to have access to the most accurate and </span><span class="No-Break"><span class="koboSpan" id="kobo.1330.1">current data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1331.1">Data quality</span></strong><span class="koboSpan" id="kobo.1332.1">: As data is centralized, data quality tends to improve. </span><span class="koboSpan" id="kobo.1332.2">Once this data is tested, if </span><a id="_idIndexMarker1226"/><span class="koboSpan" id="kobo.1333.1">there are no updates, there is no need for </span><span class="No-Break"><span class="koboSpan" id="kobo.1334.1">further testing.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1335.1">The disadvantages of </span><a id="_idIndexMarker1227"/><span class="koboSpan" id="kobo.1336.1">DaaS are similar to the other models associated with </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">cloud computing:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1338.1">Data security and privacy risks</span></strong><span class="koboSpan" id="kobo.1339.1">: Obviously, the</span><a id="_idIndexMarker1228"/><span class="koboSpan" id="kobo.1340.1"> location of data on the cloud can mean that sensitive and proprietary data can be accessed by third parties or be at risk of breach. </span><span class="koboSpan" id="kobo.1340.2">Providers must comply with regulations, which are increasingly stringent today. </span><span class="koboSpan" id="kobo.1340.3">The costs of securing infrastructure are growing, and data piracy attacks are on the rise. </span><span class="koboSpan" id="kobo.1340.4">In addition, although data is sold anonymized, in some cases, it is possible to reconstruct </span><span class="No-Break"><span class="koboSpan" id="kobo.1341.1">the information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1342.1">Dependency on providers</span></strong><span class="koboSpan" id="kobo.1343.1">: DaaS creates a reliance on external providers for critical data. </span><span class="koboSpan" id="kobo.1343.2">Service outages or disruptions on the provider’s end impact the client and all services that are related to accessing this data. </span><span class="koboSpan" id="kobo.1343.3">The client normally has access to the data stream but does not download the data, so it can be cut off from data that is necessary to </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">its business.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1345.1">Limited customization</span></strong><span class="koboSpan" id="kobo.1346.1">: DaaS may not provide data in the format needed or have the right granularity. </span><span class="koboSpan" id="kobo.1346.2">Providers have an interest in providing data in a format that is useful to as many clients as possible, but specific clients may have different requirements. </span><span class="koboSpan" id="kobo.1346.3">An inadequate format makes it more complicated to integrate into existing systems or their own workflows, requiring costs to be incurred in order to adapt either the systems or </span><span class="No-Break"><span class="koboSpan" id="kobo.1347.1">the data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1348.1">Quality assurance</span></strong><span class="koboSpan" id="kobo.1349.1">: In DaaS, quality in terms of accuracy of data is key, and poor-quality data can lead to flawed decision-making or errors in related services. </span><span class="koboSpan" id="kobo.1349.2">The quality, accuracy, and reliability of the data depend on the provider. </span><span class="koboSpan" id="kobo.1349.3">Therefore, the provider must ensure that the data is relevant, updated, and of </span><span class="No-Break"><span class="koboSpan" id="kobo.1350.1">good quality.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1351.1">Latency and performance issues</span></strong><span class="koboSpan" id="kobo.1352.1">: Accessing data over the internet can lead to introducing latency (especially when the connection is not good or the datasets are very large). </span><span class="koboSpan" id="kobo.1352.2">In addition, this latency can reduce performance if the data stream is embedded in</span><a id="_idIndexMarker1229"/> <span class="No-Break"><span class="koboSpan" id="kobo.1353.1">additional services.</span></span></li>
</ul>
<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.1354.1">Results as a Service (RaaS)</span></h2>
<p><span class="koboSpan" id="kobo.1355.1">RaaS, or OaaS, is a new paradigm</span><a id="_idIndexMarker1230"/><span class="koboSpan" id="kobo.1356.1"> that has developed in recent years. </span><span class="koboSpan" id="kobo.1356.2">RaaS is a business model where a service provider delivers specific results or outcomes instead of providing tools, platforms, or raw data. </span><span class="koboSpan" id="kobo.1356.3">This model has attracted attention in fields such as data analytics, AI, and automation. </span><span class="koboSpan" id="kobo.1356.4">In RaaS, AI (including LLMs and agents) is used by the provider to provide personalized insights for customers. </span><span class="koboSpan" id="kobo.1356.5">While the provider conducts the entire analysis, the client can focus on business insights without the need for specialized technology staff. </span><span class="koboSpan" id="kobo.1356.6">In general, instead of paying a lump sum for a service, the client pays through a subscription to receive analytics at </span><span class="No-Break"><span class="koboSpan" id="kobo.1357.1">constant intervals.</span></span></p>
<p><span class="koboSpan" id="kobo.1358.1">Since customers increasingly demand value from models (businesses are more interested in the value obtained from models than from an additional tool), RaaS focuses on providing an outcome rather than a model (or data). </span><span class="koboSpan" id="kobo.1358.2">In addition, customers are looking for ways to reduce the costs of adopting a technology but preserving its value, and RaaS thus seeks to reduce the initial cost to a business. </span><span class="koboSpan" id="kobo.1358.3">The provider focuses on identifying the technology or what tool is needed to achieve the outcome, while the customer explains what their needs and </span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">requirements are.</span></span></p>
<p><span class="koboSpan" id="kobo.1360.1">The purpose of RaaS is to build customer loyalty, and so a provider has every interest in automating the analysis process. </span><span class="koboSpan" id="kobo.1360.2">Therefore, AI agents can be envisioned as a new core component of this business model. </span><span class="koboSpan" id="kobo.1360.3">By itself, an LLM is capable of almost instantaneously producing a possible report and thus generating insights for a customer. </span><span class="koboSpan" id="kobo.1360.4">These reports can be personalized using LLMs and provide insights tailored to the clients. </span><span class="koboSpan" id="kobo.1360.5">The addition of tools and databases allows for both adding a quantitative component and extending the capabilities of an LLM. </span><span class="koboSpan" id="kobo.1360.6">Agents then allow tasks to be completed automatically and routinely. </span><span class="koboSpan" id="kobo.1360.7">In fact, agents can analyze large amounts of data and can be complemented with additional models. </span><span class="koboSpan" id="kobo.1360.8">The reports (or even presentations) generated can be used to make </span><span class="No-Break"><span class="koboSpan" id="kobo.1361.1">informed decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.1362.1">RaaS thus has</span><a id="_idIndexMarker1231"/> <span class="No-Break"><span class="koboSpan" id="kobo.1363.1">several advantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1364.1">Outcome-focused approach</span></strong><span class="koboSpan" id="kobo.1365.1">: The business pays only for results (and thus for the value that is delivered) and not for tools, infrastructure, and expertise. </span><span class="koboSpan" id="kobo.1365.2">This reduces risk for a business, since it has no responsibility for either using software or </span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">conducting analysis.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1367.1">Cost efficiency</span></strong><span class="koboSpan" id="kobo.1368.1">: For the customer, there is no need to spend money to build infrastructure and expertise. </span><span class="koboSpan" id="kobo.1368.2">Instead, the service provider can automate the process and reduce costs (it can be rather expensive for a small business). </span><span class="koboSpan" id="kobo.1368.3">Also, the client can adopt a subscription plan at an agreed price (with the added benefit that outcome-based pricing models align costs directly with results achieved), and the provider instead gets a stable </span><span class="No-Break"><span class="koboSpan" id="kobo.1369.1">monthly income.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1370.1">Focus on core competencies</span></strong><span class="koboSpan" id="kobo.1371.1">: Since a company does not have to invest resources in building and maintaining systems or managing processes, RaaS provides a large time advantage. </span><span class="koboSpan" id="kobo.1371.2">This also allows the business to implement new capabilities, demanding execution only from the provider. </span><span class="koboSpan" id="kobo.1371.3">The customer can then focus on its core competencies and incorporate the results directly into </span><span class="No-Break"><span class="koboSpan" id="kobo.1372.1">its pipeline.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1373.1">Scalability, accuracy, and flexibility</span></strong><span class="koboSpan" id="kobo.1374.1">: The system is scalable and flexible, as the provider can reuse the technology for different clients. </span><span class="koboSpan" id="kobo.1374.2">Providers are incentivized to deliver high-quality outcomes since their payment or reputation depends on the</span><a id="_idIndexMarker1232"/><span class="koboSpan" id="kobo.1375.1"> success of </span><span class="No-Break"><span class="koboSpan" id="kobo.1376.1">the service.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1377.1">RaaS can also have </span><span class="No-Break"><span class="koboSpan" id="kobo.1378.1">some </span></span><span class="No-Break"><a id="_idIndexMarker1233"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1379.1">disadvantages:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1380.1">Loss of control</span></strong><span class="koboSpan" id="kobo.1381.1">: Clients have limited control over how these outcomes are achieved. </span><span class="koboSpan" id="kobo.1381.2">They can’t track the process or diagnose potential problems that arise during the process. </span><span class="koboSpan" id="kobo.1381.3">In addition, there could be potential concerns over compliance, quality, or ethical practices on the part of the provider that the client might not notice. </span><span class="koboSpan" id="kobo.1381.4">In general, RaaS does not promote transparency, and it relies on the client’s trust in </span><span class="No-Break"><span class="koboSpan" id="kobo.1382.1">the provider.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1383.1">Dependency on providers</span></strong><span class="koboSpan" id="kobo.1384.1">: For customers, RaaS means heavy reliance on a service provider, which can lead to vendor lock-in, difficulty in changing providers, or high costs in changing a provider. </span><span class="koboSpan" id="kobo.1384.2">Any failure or inefficiency on the provider’s part has a direct impact on customer operations. </span><span class="koboSpan" id="kobo.1384.3">In these cases, the customer has </span><span class="No-Break"><span class="koboSpan" id="kobo.1385.1">limited options.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1386.1">Data security and privacy risks</span></strong><span class="koboSpan" id="kobo.1387.1">: Sensitive data may need to be shared with the service provider, creating privacy and security concerns. </span><span class="koboSpan" id="kobo.1387.2">Businesses may not be able to share this data due to regulation, risking potential breaches and hefty fines. </span><span class="koboSpan" id="kobo.1387.3">At the same time, if sensitive data were intercepted, businesses could face serious reputational damage or fines. </span><span class="koboSpan" id="kobo.1387.4">RaaS service providers, therefore, come with large costs to maintain system security, data storage, </span><span class="No-Break"><span class="koboSpan" id="kobo.1388.1">and connections.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1389.1">Complexity in measuring results</span></strong><span class="koboSpan" id="kobo.1390.1">: Defining clear, measurable outcomes can be challenging, especially when the goal or analysis is complex. </span><span class="koboSpan" id="kobo.1390.2">Misaligned expectations between the client and the provider may lead to disputes about whether outcomes have been achieved. </span><span class="koboSpan" id="kobo.1390.3">These disputes can become costly lawsuits and impact the </span><span class="No-Break"><span class="koboSpan" id="kobo.1391.1">provider’s reputation.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1392.1">Potential for higher costs</span></strong><span class="koboSpan" id="kobo.1393.1">: On the one hand, RaaS reduces upfront costs, but in the long run, the service can become expensive for a business. </span><span class="koboSpan" id="kobo.1393.2">Also, there may be added costs for further analysis, or if there is misalignment in performance </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">and goals.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1395.1">Limited customization</span></strong><span class="koboSpan" id="kobo.1396.1">: RaaS solutions may be defined by broad application, and may not meet specific, niche requirements of a business. </span><span class="koboSpan" id="kobo.1396.2">A service provider has every interest in automating tasks and creating solutions that are useful to the greatest number of customers. </span><span class="koboSpan" id="kobo.1396.3">This means specific customer needs may have additional costs, not be addressed, or not be fully understood by </span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">the provider.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1398.1">Quality assurance challenges</span></strong><span class="koboSpan" id="kobo.1399.1">: The provider has an interest in reducing costs; this is done through automation and trying to achieve a solution that fits all clients. </span><span class="koboSpan" id="kobo.1399.2">A provider may cut corners to achieve outcomes quickly, potentially compromising</span><a id="_idIndexMarker1234"/> <span class="No-Break"><span class="koboSpan" id="kobo.1400.1">long-term value.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1401.1">RaaS, in any case, is a growing business model, especially with the growing interest in AI and generative AI (many businesses want to integrate AI services but have neither the expertise nor the infrastructure to do so). </span><span class="koboSpan" id="kobo.1401.2">Many companies are only interested in the outcome of the model (such as predictions for maintenance or a patient’s outcome) rather than the model itself. </span><span class="koboSpan" id="kobo.1401.3">Many businesses would be interested in tailoring the outcome to their specific needs, without needing to develop the entire process. </span><span class="koboSpan" id="kobo.1401.4">Therefore, as competition increases, different providers are beginning to specialize in highly specific offerings for different types of industries. </span><span class="koboSpan" id="kobo.1401.5">This drives innovation as companies strive to cover needs that are currently unmet. </span><span class="koboSpan" id="kobo.1401.6">With more offerings, customers’ needs will also evolve,  allowing companies to focus on improving crucial elements of </span><span class="No-Break"><span class="koboSpan" id="kobo.1402.1">their business.</span></span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.1403.1">A comparison of the different paradigms</span></h2>
<p><span class="koboSpan" id="kobo.1404.1">We can summarize the choice of paradigm </span><span class="No-Break"><span class="koboSpan" id="kobo.1405.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1406.1">SaaS</span></strong><span class="koboSpan" id="kobo.1407.1">: A provider </span><a id="_idIndexMarker1235"/><span class="koboSpan" id="kobo.1408.1">should choose SaaS when they want to offer a steady and predictable revenue stream through subscriptions, their product is scalable to a large number of customers (thus reducing the cost of their solution), it is easy to support updates and maintenance, they have capabilities to leverage cloud infrastructure to minimize hardware costs, they can guarantee frequent software improvements, and ensure customer loyalty. </span><span class="koboSpan" id="kobo.1408.2">A customer should choose SaaS when they need quick access to software without having to invest in hardware or maintenance, software flexibility and scalability are critical, or they prefer paying for software on a subscription basis rather than making large upfront investments. </span><span class="koboSpan" id="kobo.1408.3">SaaS is also a good choice when customers prefer that updates, maintenance, and security are handled by an external provider or they are interested in applications that are remotely accessible (e.g., they have teams that are spread across various countries or various locations). </span><span class="koboSpan" id="kobo.1408.4">Examples of companies using SaaS are Salesforce (a cloud-based CRM system widely used across industries), Microsoft 365 (offers productivity tools such as Word, Excel, and Teams via cloud subscription), Adobe Creative Cloud (provides access to creative tools such as Photoshop and Illustrator with continuous cloud updates), and Slack (a communication platform used by distributed teams for messaging </span><span class="No-Break"><span class="koboSpan" id="kobo.1409.1">and collaboration).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1410.1">MaaS</span></strong><span class="koboSpan" id="kobo.1411.1">: A provider</span><a id="_idIndexMarker1236"/><span class="koboSpan" id="kobo.1412.1"> should look to MaaS when they can reduce the cost of model delivery with other partners (or have a solid infrastructure), have developed high-performing AI/ML models that can serve various industries (e.g., healthcare, finance, or retail), want to monetize the developed models or expertise without sharing the algorithms, and can securely and reliably guarantee the model access. </span><span class="koboSpan" id="kobo.1412.2">Users should consider these solutions when they require advanced AI/ML models but lack the resources to build or train them in-house, or prefer outsourcing model maintenance, retraining, and optimization rather than managing it internally. </span><span class="koboSpan" id="kobo.1412.3">These models should also be considered when cost efficiency and flexibility are priorities, especially for start-ups and businesses experimenting with AI/ML, as well as when time to market for AI/ML-driven applications is critical. </span><span class="koboSpan" id="kobo.1412.4">Examples of companies using MaaS are OpenAI (provides access to GPT models through APIs for tasks such as text generation or summarization), Google Cloud AI Platform (offers models for translation, vision, speech recognition, and more), AWS SageMaker JumpStart (lets businesses quickly deploy pre-trained models for tasks such as fraud detection), and</span><a id="_idIndexMarker1237"/><span class="koboSpan" id="kobo.1413.1"> Hugging Face (through its Inference API, offers hosted access to thousands of open </span><span class="No-Break"><span class="koboSpan" id="kobo.1414.1">source models).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1415.1">DaaS</span></strong><span class="koboSpan" id="kobo.1416.1">: A provider should </span><a id="_idIndexMarker1238"/><span class="koboSpan" id="kobo.1417.1">choose DaaS if they have access to high-value, unique datasets that can benefit multiple industries, they want to capitalize on the growing reliance on data for decision-making and analytics, they want to create an additional business opportunity for their company (e.g., selling data that has been acquired over time), they can ensure compliance with data protection regulations (e.g., GDPR or CCPA), they have the infrastructure to be able to conduct data sharing, or they provide (or intend to) added value beyond raw data, such as insights, visualizations, or integration with tools. </span><span class="koboSpan" id="kobo.1417.2">A client should consider DaaS if they need large volumes of data but do not want to invest in storage and processing infrastructure, their business relies on external or specialized datasets (e.g., market data, weather data, geolocation data, financial data, healthcare data, and so on), they prefer flexibility in accessing different datasets and scaling, or they do not want to deal with data compliance, maintenance, and security. </span><span class="koboSpan" id="kobo.1417.3">Examples include Snowflake (a cloud data platform that enables secure data sharing across organizations), Quandl by Nasdaq (offers financial, economic, and alternative data to analysts and institutions), Clearbit (provides B2B data for sales and marketing enrichment), and the Climate Data Store from Copernicus (offers environmental and climate datasets for </span><a id="_idIndexMarker1239"/><span class="koboSpan" id="kobo.1418.1">scientific and </span><span class="No-Break"><span class="koboSpan" id="kobo.1419.1">commercial use).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1420.1">RaaS</span></strong><span class="koboSpan" id="kobo.1421.1">: A provider</span><a id="_idIndexMarker1240"/><span class="koboSpan" id="kobo.1422.1"> may consider RaaS if they have the appropriate infrastructure to guarantee reliable and measurable outcomes to customers, prefer to differentiate themselves by focusing on delivering value and results rather than selling products or services, can measure performance and guaranteed outcomes to the customer, and have expertise in mitigating risks and guaranteeing performance. </span><span class="koboSpan" id="kobo.1422.2">Customers should choose RaaS when they want to achieve specific outcomes without managing the underlying processes, infrastructure, or technology; when their focus is on outcomes (e.g., performance improvement or operational efficiency) rather than tools or inputs; when they want to minimize risks by paying only for successful outcomes or results; when they lack expertise in achieving some complex and specialized outcomes; or when they want to reduce costs and spread them out over time. </span><span class="koboSpan" id="kobo.1422.3">Examples of companies that use RaaS are Pymetrics (delivers hiring recommendations based on neuroscience and AI without exposing internal mechanisms), Afiniti (uses AI to optimize call center pairings and charges based on improved performance), Uptake (provides predictive maintenance in industrial contexts tied to uptime or efficiency gains), and ZS Associates (offers analytics-driven solutions in healthcare and pharma, charging based on KPIs and </span><span class="No-Break"><span class="koboSpan" id="kobo.1423.1">performance improvements).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1424.1">The following table provides a summary of the advantages and disadvantages of each paradigm for providers </span><span class="No-Break"><span class="koboSpan" id="kobo.1425.1">and users:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1426.1">Category</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1427.1">SaaS</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1428.1">MaaS</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1429.1">DaaS</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1430.1">RaaS</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1431.1">Advantages (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1432.1">provider)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1433.1">- Recurring </span><a id="_idIndexMarker1241"/><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">revenue model.</span></span></p>
<p><span class="koboSpan" id="kobo.1435.1">- Scalable </span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">infrastructure</span></span></p>
<p><span class="koboSpan" id="kobo.1437.1">- Easier </span><span class="No-Break"><span class="koboSpan" id="kobo.1438.1">software updates.</span></span></p>
<p><span class="koboSpan" id="kobo.1439.1">- Cost-efficient </span><a id="_idIndexMarker1242"/><span class="koboSpan" id="kobo.1440.1">development </span><span class="No-Break"><span class="koboSpan" id="kobo.1441.1">life cycle.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1442.1">- Enables </span><a id="_idIndexMarker1243"/><span class="koboSpan" id="kobo.1443.1">monetization of </span><span class="No-Break"><span class="koboSpan" id="kobo.1444.1">AI/ML models.</span></span></p>
<p><span class="koboSpan" id="kobo.1445.1">- Scalable distribution of computational </span><span class="No-Break"><span class="koboSpan" id="kobo.1446.1">resources.</span></span></p>
<p><span class="koboSpan" id="kobo.1447.1">- Supports various industries such as healthcare </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">and finance.</span></span></p>
<p><span class="koboSpan" id="kobo.1449.1">- Reduced infrastructure needs (e.g., cloud-hosted </span><span class="No-Break"><span class="koboSpan" id="kobo.1450.1">ML models).</span></span></p>
<p><span class="koboSpan" id="kobo.1451.1">- Opportunity to expand into niche </span><span class="No-Break"><span class="koboSpan" id="kobo.1452.1">AI/ML applications.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1453.1">- Data </span><a id="_idIndexMarker1244"/><span class="koboSpan" id="kobo.1454.1">monetization </span><span class="No-Break"><span class="koboSpan" id="kobo.1455.1">opportunities.</span></span></p>
<p><span class="koboSpan" id="kobo.1456.1">- Centralized management </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">of data.</span></span></p>
<p><span class="koboSpan" id="kobo.1458.1">- </span><span class="No-Break"><span class="koboSpan" id="kobo.1459.1">Predictable revenue.</span></span></p>
<p><span class="koboSpan" id="kobo.1460.1">- Ability to leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.1461.1">existing datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.1462.1">- Flexibility</span><a id="_idIndexMarker1245"/><span class="koboSpan" id="kobo.1463.1"> in serving </span><span class="No-Break"><span class="koboSpan" id="kobo.1464.1">different industries.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1465.1">- Steady</span><a id="_idIndexMarker1246"/><span class="koboSpan" id="kobo.1466.1"> and predictable </span><span class="No-Break"><span class="koboSpan" id="kobo.1467.1">revenue streams.</span></span></p>
<p><span class="koboSpan" id="kobo.1468.1">- Encourages value-based pricing </span><span class="No-Break"><span class="koboSpan" id="kobo.1469.1">for outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.1470.1">- Differentiates offering in competitive </span><span class="No-Break"><span class="koboSpan" id="kobo.1471.1">markets.</span></span></p>
<p><span class="koboSpan" id="kobo.1472.1">- Enables providers to focus on delivering outcomes rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.1473.1">selling products.</span></span></p>
<p><span class="koboSpan" id="kobo.1474.1">- Improved </span><a id="_idIndexMarker1247"/><span class="No-Break"><span class="koboSpan" id="kobo.1475.1">customer retentio</span></span><span class="No-Break"><span class="koboSpan" id="kobo.1476.1">n.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1477.1">Advantages (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1478.1">User)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1479.1">- Low </span><a id="_idIndexMarker1248"/><span class="No-Break"><span class="koboSpan" id="kobo.1480.1">upfront cost.</span></span></p>
<p><span class="koboSpan" id="kobo.1481.1">- Easy access to the latest software </span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">versions.</span></span></p>
<p><span class="koboSpan" id="kobo.1483.1">- Accessibility </span><span class="No-Break"><span class="koboSpan" id="kobo.1484.1">from anywhere.</span></span></p>
<p><span class="koboSpan" id="kobo.1485.1">- Flexibility in subscriptions to </span><span class="No-Break"><span class="koboSpan" id="kobo.1486.1">match business</span></span>
<a id="_idIndexMarker1249"/><span class="No-Break"><span class="koboSpan" id="kobo.1487.1">needs.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1488.1">- Access</span><a id="_idIndexMarker1250"/><span class="koboSpan" id="kobo.1489.1"> to advanced models without the need to build or </span><span class="No-Break"><span class="koboSpan" id="kobo.1490.1">train them.</span></span></p>
<p><span class="koboSpan" id="kobo.1491.1">- Scalable computing power to process </span><span class="No-Break"><span class="koboSpan" id="kobo.1492.1">models efficiently.</span></span></p>
<p><span class="koboSpan" id="kobo.1493.1">- Flexibility in using models for predictions </span><span class="No-Break"><span class="koboSpan" id="kobo.1494.1">or automation.</span></span></p>
<p><span class="koboSpan" id="kobo.1495.1">- Cost </span><a id="_idIndexMarker1251"/><span class="koboSpan" id="kobo.1496.1">savings by avoiding the need to build in-house </span><span class="No-Break"><span class="koboSpan" id="kobo.1497.1">AI/ML infrastructure</span></span></p>
<p><span class="koboSpan" id="kobo.1498.1">- Enables faster time to market for </span><span class="No-Break"><span class="koboSpan" id="kobo.1499.1">AI-powered applications.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1500.1">- Easy and quick access </span><a id="_idIndexMarker1252"/><span class="koboSpan" id="kobo.1501.1">
to curated, </span><span class="No-Break"><span class="koboSpan" id="kobo.1502.1">usable data.</span></span></p>
<p><span class="koboSpan" id="kobo.1503.1">- Lower cost of ownership for </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">data systems.</span></span></p>
<p><span class="koboSpan" id="kobo.1505.1">- Eliminates the need for large data storage/processing</span><a id="_idIndexMarker1253"/> <span class="No-Break"><span class="koboSpan" id="kobo.1506.1">infrastructure.</span></span></p>
<p><span class="koboSpan" id="kobo.1507.1">- </span><span class="No-Break"><span class="koboSpan" id="kobo.1508.1">Flexible scaling.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1509.1">- Reduced risk with </span><a id="_idIndexMarker1254"/><span class="koboSpan" id="kobo.1510.1">outcome-based </span><span class="No-Break"><span class="koboSpan" id="kobo.1511.1">payments.</span></span></p>
<p><span class="koboSpan" id="kobo.1512.1">- Focus </span><span class="No-Break"><span class="koboSpan" id="kobo.1513.1">on results</span></span><span class="koboSpan" id="kobo.1514.1">
without worrying about underlying </span><span class="No-Break"><span class="koboSpan" id="kobo.1515.1">infrastructure.</span></span></p>
<p><span class="koboSpan" id="kobo.1516.1">- Predictable performance </span><span class="No-Break"><span class="koboSpan" id="kobo.1517.1">and value.</span></span></p>
<p><span class="koboSpan" id="kobo.1518.1">- No need for large </span><span class="No-Break"><span class="koboSpan" id="kobo.1519.1">initial investments.</span></span></p>
<p><span class="koboSpan" id="kobo.1520.1">- Simplifies achieving desired results with </span><span class="No-Break"><span class="koboSpan" id="kobo.1521.1">expert support.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1522.1">Disadvantages (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1523.1">provider)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1524.1">- High competition and</span><a id="_idIndexMarker1255"/> <span class="No-Break"><span class="koboSpan" id="kobo.1525.1">customer churn.</span></span></p>
<p><span class="koboSpan" id="kobo.1526.1">- Ongoing costs for infrastructure </span><span class="No-Break"><span class="koboSpan" id="kobo.1527.1">and updates.</span></span></p>
<p><span class="koboSpan" id="kobo.1528.1">- Challenges with regional regulations </span><span class="No-Break"><span class="koboSpan" id="kobo.1529.1">and compliance.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1530.1">- High </span><a id="_idIndexMarker1256"/><span class="koboSpan" id="kobo.1531.1">initial development cost </span><span class="No-Break"><span class="koboSpan" id="kobo.1532.1">for models.</span></span></p>
<p><span class="koboSpan" id="kobo.1533.1">- Ensuring fairness, reliability, and compliance in AI/ML models </span><span class="No-Break"><span class="koboSpan" id="kobo.1534.1">is challenging.</span></span></p>
<p><span class="koboSpan" id="kobo.1535.1">- Managing performance expectations of models across diverse </span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.1537.1">- Resource-intensive </span><a id="_idIndexMarker1257"/><span class="koboSpan" id="kobo.1538.1">model updates </span><span class="No-Break"><span class="koboSpan" id="kobo.1539.1">and retraining.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1540.1">-Privacy/security concerns with </span><span class="No-Break"><span class="koboSpan" id="kobo.1541.1">data </span></span><span class="No-Break"><a id="_idIndexMarker1258"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">usage.</span></span></p>
<p><span class="koboSpan" id="kobo.1543.1">- Infrastructure for real-time </span><span class="No-Break"><span class="koboSpan" id="kobo.1544.1">data delivery.</span></span></p>
<p><span class="koboSpan" id="kobo.1545.1">- Need for compliance with complex</span><a id="_idIndexMarker1259"/><span class="koboSpan" id="kobo.1546.1"> data regulations (</span><span class="No-Break"><span class="koboSpan" id="kobo.1547.1">e.g., GDPR</span></span><span class="No-Break"><span class="koboSpan" id="kobo.1548.1">).</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1549.1">- Revenue depends </span><a id="_idIndexMarker1260"/><span class="koboSpan" id="kobo.1550.1">on the successful delivery </span><span class="No-Break"><span class="koboSpan" id="kobo.1551.1">of outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.1552.1">- High upfront costs for performance </span><span class="No-Break"><span class="koboSpan" id="kobo.1553.1">guarantees.</span></span></p>
<p><span class="koboSpan" id="kobo.1554.1">- Complex measurement and accountability </span><span class="No-Break"><span class="koboSpan" id="kobo.1555.1">metrics.</span></span></p>
<p><span class="koboSpan" id="kobo.1556.1">- Risk of lower margins if outcomes are hard to deliver or expectations </span><span class="No-Break"><span class="koboSpan" id="kobo.1557.1">are misaligned.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.1558.1">Disadvantages (</span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1559.1">user)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1560.1">- Dependence on </span><a id="_idIndexMarker1261"/><span class="koboSpan" id="kobo.1561.1">internet </span><span class="No-Break"><span class="koboSpan" id="kobo.1562.1">connectivity.</span></span></p>
<p><span class="koboSpan" id="kobo.1563.1">- Data security and </span><span class="No-Break"><span class="koboSpan" id="kobo.1564.1">privacy risks.</span></span></p>
<p><span class="koboSpan" id="kobo.1565.1">- Long-term costs may exceed owning </span><span class="No-Break"><span class="koboSpan" id="kobo.1566.1">software outright.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1567.1">- Dependence on</span><a id="_idIndexMarker1262"/> <span class="No-Break"><span class="koboSpan" id="kobo.1568.1">third-party models.</span></span></p>
<p><span class="koboSpan" id="kobo.1569.1">- Potential for bias or errors in </span><span class="No-Break"><span class="koboSpan" id="kobo.1570.1">AI/ML models.</span></span></p>
<p><span class="koboSpan" id="kobo.1571.1">- May incur long-term costs if </span><span class="No-Break"><span class="koboSpan" id="kobo.1572.1">frequently needed.</span></span></p>
<p><span class="koboSpan" id="kobo.1573.1">- Limited ability to customize models for highly </span><span class="No-Break"><span class="koboSpan" id="kobo.1574.1">specific needs.</span></span></p>
<p><span class="koboSpan" id="kobo.1575.1">- Privacy concerns </span><a id="_idIndexMarker1263"/><span class="koboSpan" id="kobo.1576.1">in certain </span><span class="No-Break"><span class="koboSpan" id="kobo.1577.1">AI/ML applications.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1578.1">- Concerns </span><a id="_idIndexMarker1264"/><span class="koboSpan" id="kobo.1579.1">about data ownership and </span><span class="No-Break"><span class="koboSpan" id="kobo.1580.1">vendor lock-in.</span></span></p>
<p><span class="koboSpan" id="kobo.1581.1">- Potential for high </span><span class="No-Break"><span class="koboSpan" id="kobo.1582.1">long-term costs.</span></span></p>
<p><span class="koboSpan" id="kobo.1583.1">- Possible over-reliance on </span><span class="No-Break"><span class="koboSpan" id="kobo.1584.1">third-party data.</span></span></p>
<p><span class="koboSpan" id="kobo.1585.1">- Security</span><a id="_idIndexMarker1265"/><span class="koboSpan" id="kobo.1586.1"> risks with </span><span class="No-Break"><span class="koboSpan" id="kobo.1587.1">sensitive data.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.1588.1">- Dependence on </span><a id="_idIndexMarker1266"/><span class="koboSpan" id="kobo.1589.1">
vendor for </span><span class="No-Break"><span class="koboSpan" id="kobo.1590.1">outcome success.</span></span></p>
<p><span class="koboSpan" id="kobo.1591.1">- Lack of transparency in how the processes </span><span class="No-Break"><span class="koboSpan" id="kobo.1592.1">achieve results.</span></span></p>
<p><span class="koboSpan" id="kobo.1593.1">- Limited flexibility to modify outcomes </span><span class="No-Break"><span class="koboSpan" id="kobo.1594.1">during contracts.</span></span></p>
<p><span class="koboSpan" id="kobo.1595.1">- May not suit users with highly </span><span class="No-Break"><span class="koboSpan" id="kobo.1596.1">specific, non-</span></span>
<span class="No-Break"><span class="koboSpan" id="kobo.1597.1">standardized needs.</span></span></p>
<p><span class="koboSpan" id="kobo.1598.1">- Costs can escalate if outcomes are not </span><span class="No-Break"><span class="koboSpan" id="kobo.1599.1">well defined.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1600.1">Table 9.1 – Advantages and disadvantages for providers and users</span></p>
<p><span class="koboSpan" id="kobo.1601.1">The choice of business paradigm is an important one. </span><span class="koboSpan" id="kobo.1601.2">Each paradigm has an impact on both a user and a business. </span><span class="koboSpan" id="kobo.1601.3">Finding the right paradigm saves resources and increases revenue. </span><span class="koboSpan" id="kobo.1601.4">The choice of paradigm impacts the technical choices for developing a </span><span class="No-Break"><span class="koboSpan" id="kobo.1602.1">multi-agent system.</span></span></p>
<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.1603.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1604.1">In this chapter, we have seen how the tools we looked at in previous chapters can be added to an LLM. </span><span class="koboSpan" id="kobo.1604.2">We saw that an LLM is capable of planning and reasoning, but it produces weaker results when it comes to execution. </span><span class="koboSpan" id="kobo.1604.3">An LLM is capable of generating text, but at the same time, the enormous amount of information learned allows it to develop skills beyond text generation. </span><span class="koboSpan" id="kobo.1604.4">While it is a computational waste to ask an LLM to classify an image, an LLM can use a specialized model to solve the task. </span><span class="koboSpan" id="kobo.1604.5">As we saw with HuggingGPT, a model can invoke other models to identify a pizza in an image. </span><span class="koboSpan" id="kobo.1604.6">In that case, we saw an LLM invoke more than one model, collect their outputs, and conduct reasoning about the results (observe that the models agreed on the type of pizza in the image). </span><span class="koboSpan" id="kobo.1604.7">The LLM can then conduct reasoning and choose which models need to run to complete the task, collect the outputs, and observe whether the task </span><span class="No-Break"><span class="koboSpan" id="kobo.1605.1">is completed.</span></span></p>
<p><span class="koboSpan" id="kobo.1606.1">This concept makes it possible to revolutionize various industrial applications. </span><span class="koboSpan" id="kobo.1606.2">For example, a customer can request by email to exchange an item because the size they purchased was too small. </span><span class="koboSpan" id="kobo.1606.3">An LLM understands the complaint, devises a plan, and executes it. </span><span class="koboSpan" id="kobo.1606.4">The model can use tools to verify the purchase, another tool to see whether the size up is in stock, software to order the shipment, and, once the order is complete, respond to the customer that their request has been fulfilled. </span><span class="koboSpan" id="kobo.1606.5">Agents therefore enable the automation of various tasks, as they allow an LLM to use other tools necessary for task completion. </span><span class="koboSpan" id="kobo.1606.6">As we have seen, this approach extends to many other applications: agents in the law field, agents for research in chemistry and biology, and so on. </span><span class="koboSpan" id="kobo.1606.7">For example, AI agents could be legal assistants to help write papers, assist professors in creating lectures, or help researchers define </span><span class="No-Break"><span class="koboSpan" id="kobo.1607.1">scientific hypotheses.</span></span></p>
<p><span class="koboSpan" id="kobo.1608.1">Although these seem like advanced scenarios, it must be understood that LLMs have limitations in reasoning, and at present, they can automate simple tasks but not yet complex business needs. </span><span class="koboSpan" id="kobo.1608.2">For this, there needs to be human oversight, and developers need to be aware of what the limitations of the system are. </span><span class="koboSpan" id="kobo.1608.3">In addition, LLMs consume resources, and these systems can be computationally expensive. </span><span class="koboSpan" id="kobo.1608.4">Scalability is one of the main issues for a business that wants to adopt agents. </span><span class="koboSpan" id="kobo.1608.5">Therefore, in the last section of this chapter, we discussed the various business paradigms that open up with the arrival of LLMs. </span><span class="koboSpan" id="kobo.1608.6">SaaS is the classic paradigm that has dominated the last three decades; it was conceived during the internet revolution but before the arrival of AI as a mass product. </span><span class="koboSpan" id="kobo.1608.7">DaaS focuses on AI and businesses’ need for quality data to make informed decisions. </span><span class="koboSpan" id="kobo.1608.8">MaaS is dedicated to companies that want to provide ML and AI models, while RaaS focuses only on the output of these models. </span><span class="koboSpan" id="kobo.1608.9">There are clear similarities between SaaS and these paradigms, but they take into consideration two factors: AI models require infrastructure and resources to train and use, and developing and maintaining these models requires considerable expertise. </span><span class="koboSpan" id="kobo.1608.10">MaaS and RaaS thus allow a business to reduce the initial investment into infrastructure, training, and expertise. </span><span class="koboSpan" id="kobo.1608.11">The choice of provider or client is different depending on their needs and resources, so we have provided a comparative table and </span><span class="No-Break"><span class="koboSpan" id="kobo.1609.1">some guidelines.</span></span></p>
<p><span class="koboSpan" id="kobo.1610.1">In this chapter, therefore, we have defined what an agent is in practice (or a group of agents in the case of a multi-agent platform) and discussed how these agents can be integrated into the business. </span><span class="koboSpan" id="kobo.1610.2">In other words, we have defined an agent-based system. </span><span class="koboSpan" id="kobo.1610.3">This system is not an isolated entity; in the next chapter, we will focus on the ecosystem around an agent and how an agent integrates </span><span class="No-Break"><span class="koboSpan" id="kobo.1611.1">into it.</span></span></p>
<h1 id="_idParaDest-179"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.1612.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.1613.1">Shen, </span><em class="italic"><span class="koboSpan" id="kobo.1614.1">HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</span></em><span class="koboSpan" id="kobo.1615.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1616.1">2023, </span></span><a href="https://arxiv.org/abs/2303.17580"><span class="No-Break"><span class="koboSpan" id="kobo.1617.1">https://arxiv.org/abs/2303.17580</span></span></a></li>
<li><span class="koboSpan" id="kobo.1618.1">Wang, </span><em class="italic"><span class="koboSpan" id="kobo.1619.1">A Survey on Large Language Model based Autonomous Agents</span></em><span class="koboSpan" id="kobo.1620.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1621.1">2023, </span></span><a href="https://arxiv.org/abs/2308.11432"><span class="No-Break"><span class="koboSpan" id="kobo.1622.1">https://arxiv.org/abs/2308.11432</span></span></a></li>
<li><span class="koboSpan" id="kobo.1623.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.1624.1">HuggingGPT: Give Your Chatbot an AI </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1625.1">Army</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1626.1">, </span></span><a href="https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98"><span class="No-Break"><span class="koboSpan" id="kobo.1627.1">https://levelup.gitconnected.com/hugginggpt-give-your-chatbot-an-ai-army-cfadf5647f98</span></span></a></li>
<li><span class="koboSpan" id="kobo.1628.1">Schick, </span><em class="italic"><span class="koboSpan" id="kobo.1629.1">Toolformer: Language Models Can Teach Themselves to Use Tools</span></em><span class="koboSpan" id="kobo.1630.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1631.1">2023, </span></span><a href="https://arxiv.org/abs/2302.04761"><span class="No-Break"><span class="koboSpan" id="kobo.1632.1">https://arxiv.org/abs/2302.04761</span></span></a></li>
<li><span class="koboSpan" id="kobo.1633.1">Bran, </span><em class="italic"><span class="koboSpan" id="kobo.1634.1">ChemCrow: Augmenting </span></em><em class="italic"><span class="koboSpan" id="kobo.1635.1">Large Language Models with Chemistry Tools</span></em><span class="koboSpan" id="kobo.1636.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1637.1">2023, </span></span><a href="https://arxiv.org/abs/2304.05376"><span class="No-Break"><span class="koboSpan" id="kobo.1638.1">https://arxiv.org/abs/2304.05376</span></span></a></li>
<li><span class="koboSpan" id="kobo.1639.1">Cui, </span><em class="italic"><span class="koboSpan" id="kobo.1640.1">Chatlaw: A Multi-Agent Collaborative Legal Assistant with Knowledge Graph Enhanced Mixture-of-Experts Large Language Model</span></em><span class="koboSpan" id="kobo.1641.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1642.1">2023, </span></span><a href="https://arxiv.org/abs/2306.16092v2"><span class="No-Break"><span class="koboSpan" id="kobo.1643.1">https://arxiv.org/abs/2306.16092v2</span></span></a></li>
<li><span class="koboSpan" id="kobo.1644.1">Hamilton, </span><em class="italic"><span class="koboSpan" id="kobo.1645.1">Blind Judgement: Agent-Based Supreme Court Modelling With GPT</span></em><span class="koboSpan" id="kobo.1646.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1647.1">2023, </span></span><a href="https://arxiv.org/abs/2301.05327"><span class="No-Break"><span class="koboSpan" id="kobo.1648.1">https://arxiv.org/abs/2301.05327</span></span></a></li>
<li><span class="koboSpan" id="kobo.1649.1">Cheng, </span><em class="italic"><span class="koboSpan" id="kobo.1650.1">Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects</span></em><span class="koboSpan" id="kobo.1651.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1652.1">2024, </span></span><a href="https://arxiv.org/pdf/2401.03428"><span class="No-Break"><span class="koboSpan" id="kobo.1653.1">https://arxiv.org/pdf/2401.03428</span></span></a></li>
<li><span class="koboSpan" id="kobo.1654.1">Swanson, </span><em class="italic"><span class="koboSpan" id="kobo.1655.1">The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation</span></em><span class="koboSpan" id="kobo.1656.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1657.1">2024, </span></span><a href="https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full"><span class="No-Break"><span class="koboSpan" id="kobo.1658.1">https://www.biorxiv.org/content/10.1101/2024.11.11.623004v1.full</span></span></a></li>
<li><span class="koboSpan" id="kobo.1659.1">Lu, </span><em class="italic"><span class="koboSpan" id="kobo.1660.1">The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery</span></em><span class="koboSpan" id="kobo.1661.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1662.1">2024, </span></span><a href="https://arxiv.org/abs/2408.06292"><span class="No-Break"><span class="koboSpan" id="kobo.1663.1">https://arxiv.org/abs/2408.06292</span></span></a></li>
<li><span class="koboSpan" id="kobo.1664.1">Fossi, </span><em class="italic"><span class="koboSpan" id="kobo.1665.1">SwiftDossier: Tailored Automatic Dossier for Drug Discovery with LLMs and Agents</span></em><span class="koboSpan" id="kobo.1666.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1667.1">2024, </span></span><a href="https://arxiv.org/abs/2409.15817"><span class="No-Break"><span class="koboSpan" id="kobo.1668.1">https://arxiv.org/abs/2409.15817</span></span></a></li>
<li><span class="koboSpan" id="kobo.1669.1">Si, </span><em class="italic"><span class="koboSpan" id="kobo.1670.1">Can LLMs Generate Novel Research Ideas? </span><span class="koboSpan" id="kobo.1670.2">A Large-Scale Human Study with 100+ NLP Researchers</span></em><span class="koboSpan" id="kobo.1671.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1672.1">2024, </span></span><a href="https://arxiv.org/abs/2409.04109"><span class="No-Break"><span class="koboSpan" id="kobo.1673.1">https://arxiv.org/abs/2409.04109</span></span></a></li>
<li><span class="koboSpan" id="kobo.1674.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.1675.1">AI Planning or Serendipity? </span><span class="koboSpan" id="kobo.1675.2">Where Do the Best Research Ideas Come </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1676.1">From?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1677.1">, </span></span><a href="https://ai.gopubby.com/ai-planning-or-serendipity-where-do-the-best-research-ideas-come-from-f8e5e6692964"><span class="No-Break"><span class="koboSpan" id="kobo.1678.1">https://ai.gopubby.com/ai-planning-or-serendipity-where-do-the-best-research-ideas-come-from-f8e5e6692964</span></span></a></li>
<li><span class="koboSpan" id="kobo.1679.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.1680.1">A Brave New World for Scientific Discovery: Are AI Research Ideas </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1681.1">Better?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1682.1">, </span></span><a href="https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182"><span class="No-Break"><span class="koboSpan" id="kobo.1683.1">https://levelup.gitconnected.com/a-brave-new-world-for-scientific-discovery-are-ai-research-ideas-better-5692c5aa8182</span></span></a></li>
<li><span class="koboSpan" id="kobo.1684.1">Schmidgall, </span><em class="italic"><span class="koboSpan" id="kobo.1685.1">Agent Laboratory: Using LLM Agents as Research Assistants</span></em><span class="koboSpan" id="kobo.1686.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1687.1">2024, </span></span><a href="https://arxiv.org/abs/2501.04227"><span class="No-Break"><span class="koboSpan" id="kobo.1688.1">https://arxiv.org/abs/2501.04227</span></span></a></li>
<li><span class="koboSpan" id="kobo.1689.1">Tang, </span><em class="italic"><span class="koboSpan" id="kobo.1690.1">ChemAgent: Self-updating Library in Large Language Models Improves Chemical Reasoning</span></em><span class="koboSpan" id="kobo.1691.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1692.1">2025, </span></span><a href="https://arxiv.org/abs/2501.06590"><span class="No-Break"><span class="koboSpan" id="kobo.1693.1">https://arxiv.org/abs/2501.06590</span></span></a></li>
<li><span class="koboSpan" id="kobo.1694.1">Raieli, </span><em class="italic"><span class="koboSpan" id="kobo.1695.1">Can AI Replace Human </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1696.1">Researchers</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1697.1">, </span></span><a href="https://levelup.gitconnected.com/can-ai-replace-human-researchers-50fcc43ea587"><span class="No-Break"><span class="koboSpan" id="kobo.1698.1">https://levelup.gitconnected.com/can-ai-replace-human-researchers-50fcc43ea587</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.1699.1">European </span></em><em class="italic"><span class="koboSpan" id="kobo.1700.1">Cloud Computing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1701.1">Platforms</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1702.1">, </span></span><a href="https://european-alternatives.eu/category/cloud-computing-platforms"><span class="No-Break"><span class="koboSpan" id="kobo.1703.1">https://european-alternatives.eu/category/cloud-computing-platforms</span></span></a></li>
<li><span class="koboSpan" id="kobo.1704.1">IBM, </span><em class="italic"><span class="koboSpan" id="kobo.1705.1">What is </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1706.1">Multi</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1707.1">-tenant?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1708.1">, </span></span><a href="https://www.ibm.com/topics/multi-tenant"><span class="No-Break"><span class="koboSpan" id="kobo.1709.1">https://www.ibm.com/topics/multi-tenant</span></span></a></li>
<li><span class="koboSpan" id="kobo.1710.1">Gan, 2023, </span><em class="italic"><span class="koboSpan" id="kobo.1711.1">Model-as-a-Service (MaaS): A </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1712.1">Survey</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1713.1">, </span></span><a href="https://arxiv.org/pdf/2311.05804"><span class="No-Break"><span class="koboSpan" id="kobo.1714.1">https://arxiv.org/pdf/2311.05804</span></span></a></li>
<li><span class="koboSpan" id="kobo.1715.1">Abe, </span><em class="italic"><span class="koboSpan" id="kobo.1716.1">A Data as a Service (DaaS) Model for GPU-based Data Analytics</span></em><span class="koboSpan" id="kobo.1717.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1718.1">2018, </span></span><a href="https://arxiv.org/abs/1802.01639"><span class="No-Break"><span class="koboSpan" id="kobo.1719.1">https://arxiv.org/abs/1802.01639</span></span></a></li>
<li><span class="koboSpan" id="kobo.1720.1">Forbes, </span><em class="italic"><span class="koboSpan" id="kobo.1721.1">AI Agents: The Next Frontier In Intelligent </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1722.1">Automation</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1723.1">, </span></span><a href="https://www.forbes.com/councils/forbestechcouncil/2025/01/02/ai-agents-the-next-frontier-in-intelligent-automation/"><span class="No-Break"><span class="koboSpan" id="kobo.1724.1">https://www.forbes.com/councils/forbestechcouncil/2025/01/02/ai-agents-the-next-frontier-in-intelligent-automation/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1725.1">World Economic Forum, </span><em class="italic"><span class="koboSpan" id="kobo.1726.1">Why </span></em><em class="italic"><span class="koboSpan" id="kobo.1727.1">Should Manufacturers Embrace AI's Next Frontier</span></em><em class="italic"><span class="koboSpan" id="kobo.1728.1"> – AI agents – </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1729.1">Now</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1730.1">?</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1731.1">, </span></span><a href="https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/"><span class="No-Break"><span class="koboSpan" id="kobo.1732.1">https://www.weforum.org/stories/2025/01/why-manufacturers-should-embrace-next-frontier-ai-agents/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1733.1">Deng, 2023, </span><em class="italic"><span class="koboSpan" id="kobo.1734.1">Mind2Web: Towards a Generalist Agent for the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1735.1">Web</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1736.1">, </span></span><a href="https://arxiv.org/abs/2306.06070"><span class="No-Break"><span class="koboSpan" id="kobo.1737.1">https://arxiv.org/abs/2306.06070</span></span></a></li>
</ul>
</div>
</body></html>