<html><head></head><body>
<div><h1 class="chapterNumber">1</h1>
<h1 class="chapterTitle" id="_idParaDest-14">Introduction to Generative AI</h1>
<p class="normal">Hello! Welcome to <em class="italic">Practical Generative AI with ChatGPT</em>! In this book, we will explore the fascinating <a id="_idIndexMarker000"/>world of generative <strong class="keyWord">artificial intelligence</strong> (<strong class="keyWord">AI</strong>) and its groundbreaking applications, with a particular focus on ChatGPT.</p>
<p class="normal">Generative AI has transformed the way we interact with machines, enabling computers to create, predict, and learn without explicit human instruction. Since the launch of OpenAI’s ChatGPT in November 2022, we have witnessed unprecedented advances in natural language processing, image and video synthesis, and many other fields. Whether you are a curious beginner or an experienced practitioner, this guide will equip you with the knowledge and skills to effectively navigate the exciting landscape of generative AI. So, let’s dive in and start the book with some definitions of the context we are moving in.</p>
<p class="normal">In this chapter, we focus on the applications of generative AI to various fields, such as image synthesis, text generation, and music composition, highlighting the potential of generative AI to revolutionize various industries with concrete examples and recent developments. Being aware of the research journey toward the current state of the art of generative AI will give you an understanding of the foundations of recent developments and state-of-the-art models.</p>
<p class="normal">All this, we will cover through the following topics:</p>
<ul>
<li class="bulletList">Introducing generative AI</li>
<li class="bulletList">Exploring the domains of generative AI</li>
<li class="bulletList">Main trends and innovation after 2 years of ChatGPT</li>
<li class="bulletList">Legal and ethical landscape of generative AI</li>
</ul>
<p class="normal">By the end of this chapter, you will be familiar with the exciting world of generative AI, its applications, the research history behind it, and the current developments that could have – and are currently having – a disruptive impact on businesses.</p>
<h1 class="heading-1" id="_idParaDest-15">Introducing generative AI</h1>
<p class="normal">Generative AI is an <a id="_idIndexMarker001"/>exciting branch of AI that focuses on creating new content, such as text, images, music, or even videos, that is often indistinguishable from something made by humans.</p>
<p class="normal">To understand where it fits, let’s break it down:</p>
<ul>
<li class="bulletList"><strong class="keyWord">AI</strong>: AI is the <a id="_idIndexMarker002"/>broad field that enables <a id="_idIndexMarker003"/>machines to mimic human-like tasks, such as decision-making or problem-solving.</li>
<li class="bulletList"><strong class="keyWord">Machine learning</strong> (<strong class="keyWord">ML</strong>): Within AI, ML refers to techniques where machines learn <a id="_idIndexMarker004"/>patterns from data to make predictions <a id="_idIndexMarker005"/>or decisions without being explicitly programmed. The process of learning is made possible by sophisticated mathematical models called algorithms.</li>
<li class="bulletList"><strong class="keyWord">Deep learning</strong> (<strong class="keyWord">DL</strong>): A subset of ML, DL uses complex algorithms inspired by the human <a id="_idIndexMarker006"/>brain to process large amounts of data and <a id="_idIndexMarker007"/>recognize intricate patterns. Because of their architecture – inspired by our brains and neural connections – these algorithms are called artificial neural networks.<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">An artificial neural network is a type of computer program designed to learn patterns by processing <a id="_idIndexMarker008"/>information in a way that’s inspired by the human brain. Instead of following strict, step-by-step rules, it uses interconnected “nodes” (like virtual brain cells) that work together and adjust their connections over time. By repeatedly reviewing examples, it gradually improves at tasks like recognizing images, understanding speech, or predicting outcomes—all without needing explicit instructions for each step.</p>
</div>
</li>
</ul>
<p class="normal">Generative AI emerges from DL and uses specialized algorithms to generate something entirely new based on what it has learned from existing data. For example, a generative AI model trained on thousands of paintings could create brand-new art that blends different styles or themes.</p>
<p class="normal">The following figure shows how these areas of research are related to each other:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_01_01.png"/></figure>
<p class="packt_figref">Figure 1.1: Relationship between AI, ML, DL, and generative AI</p>
<p class="normal">Generative AI <a id="_idIndexMarker009"/>models are trained on vast amounts of data and then they can generate new examples based on user’s requests. And the game-changer element here is that these requests are made in the easiest way possible – using our natural language. These <a id="_idIndexMarker010"/>models are called <strong class="keyWord">large language models</strong> (<strong class="keyWord">LLMs</strong>).</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">LLMs are a type of artificial neural network featured by a particular architectural framework called “Transformer.” They are characterized by a huge number of parameters (in the order of billions) and have been trained on billions of words. Given the training set, LLMs are capable of inferring language patterns and intents in user queries and generating natural language responses.</p>
</div>
<p class="normal">The possibility of interacting in natural language with LLMs is disruptive, and a whole new science has been born around that activity. This science is called “prompt engineering,” named after the term “prompt,” which we are going to cover in <em class="italic">Chapter 3</em>.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">A prompt is the specific text, question, or description you provide to a generative AI model to guide it toward producing the kind of output you want—whether that’s a helpful explanation, a creative story, or a detailed solution. How you phrase the prompt can greatly affect the AI’s response. This practice of carefully designing and refining prompts, often called “prompt engineering,” involves experimenting with different word choices, instructions, and formats to improve both the quality and accuracy of the AI’s output. By learning how to craft effective prompts, you help ensure the AI more consistently gives you results that are useful, engaging, and aligned with your goals.</p>
</div>
<p class="normal">Even though text understanding and generation is probably one of the most outstanding features of Generative AI, this field covers many domains, which we will cover next.</p>
<h1 class="heading-1" id="_idParaDest-16">Domains of generative AI</h1>
<p class="normal">In recent years, generative AI has made significant advancements and has expanded its applications <a id="_idIndexMarker011"/>to a wide range of domains, such as art, music, fashion, and architecture. In some of them, it is indeed transforming the way we create, design, and understand the world around us. In others, it is improving and making existing processes and operations more efficient.</p>
<p class="normal">For example, in the context of the pharmaceutical industry, generative AI is revolutionizing drug discovery by enabling the rapid design of novel therapeutic molecules, thereby significantly reducing development timelines and costs. By analyzing extensive datasets of chemical and biological information, generative AI models can identify promising drug candidates and predict their interactions within the human body. For instance, Insilico Medicine utilized generative AI to develop ISM001-055, a drug candidate for idiopathic pulmonary fibrosis, which progressed to Phase II clinical trials in 2023 (https://insilico.com/blog/first_phase2).</p>
<p class="normal">Another example is the way generative AI is revolutionizing game development by enabling the creation of dynamic and adaptive environments that respond to player actions, thereby enhancing immersion and replayability. By leveraging generative AI, developers can procedurally generate vast, ever-changing game worlds, ensuring that each playthrough offers a <a id="_idIndexMarker012"/>unique experience. This technology facilitates the creation of realistic <strong class="keyWord">non-playable characters</strong> (<strong class="keyWord">NPCs</strong>) with behaviors that adapt to player interactions, making game narratives more engaging. Additionally, generative AI streamlines the development process by automating asset creation, which reduces production time and costs. </p>
<p class="normal">As a result, developers can focus more on crafting innovative gameplay mechanics and rich storytelling, ultimately delivering more personalized and captivating gaming experiences (https://www.xcubelabs.com/blog/generative-ai-in-game-development-creating-dynamic-and-adaptive-environments/).</p>
<p class="normal">Lastly, generative AI can have a great impact on advertising and visual asset generation. For example, in March 2023, Coca-Cola launched the “Create Real Magic” platform (https://www.coca-colacompany.com/media-center/coca-cola-invites-digital-artists-to-create-real-magic-using-new-ai-platform), inviting digital artists worldwide to craft original artwork using iconic brand assets from its archives. Developed in collaboration with OpenAI and Bain &amp; Company, this innovative platform combines the capabilities of GPT-4 and DALL-E, enabling users to generate unique pieces that blend Coca-Cola’s heritage with modern AI technology. Participants had the opportunity to submit their creations for <a id="_idIndexMarker013"/>a chance to be featured on Coca-Cola’s digital billboards in New York’s Times Square and London’s Piccadilly Circus, exemplifying the brand’s commitment to fostering creativity through cutting-edge technology. These are just a few examples of how generative AI can reshape business processes.</p>
<p class="normal">Now, the fact that generative AI is used in many domains also implies that its models can deal with different kinds of data, from natural language to audio or images. In the next section, we’ll explore how generative AI models address different types of data and domains.</p>
<h2 class="heading-2" id="_idParaDest-17">Text generation</h2>
<p class="normal">The evolution of text generation within AI has been a journey from early theoretical concepts to <a id="_idIndexMarker014"/>today’s sophisticated language models. The 1950s marked <a id="_idIndexMarker015"/>the formal inception of AI as a field, with pioneers like Alan Turing exploring machine intelligence. Early efforts in <strong class="keyWord">natural language processing</strong> (<strong class="keyWord">NLP</strong>) during <a id="_idIndexMarker016"/>the 1960s and 1970s led to programs such as ELIZA, which simulated conversation through pattern matching. The 1980s and 1990s saw the development of statistical models that improved language modeling by probabilistically predicting word sequences. The advent of ML algorithms during this period further advanced text generation capabilities.</p>
<p class="normal">A significant breakthrough occurred in 2017 with the introduction of the Transformer architecture which, as aforementioned, is the framework that features today’s LLMs.</p>
<p class="normal">The unique element of this new series of models featuring the landscape of generative AI is that – once they are trained – they can be consumed, queried, and instructed in the easiest way possible. The introduction of LLMs marked a paradigm shift in the context of AI since no advanced skills were needed to benefit from them.</p>
<p class="normal">Today, one of the greatest applications of generative AI—and the one we are going to cover the most throughout this book—is its ability to produce new content in natural language. Indeed, generative AI models can be used to generate new coherent and grammatically correct text in different languages, such as articles, poetry, and product descriptions. They can also extract relevant features from text such as keywords, topics, or full summaries.</p>
<p class="normal">Here is an example of working with GPT-4o, one of the latest models released by OpenAI and available through ChatGPT:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B31559_01_02.png"/></figure>
<p class="packt_figref">Figure 1.2: Example of ChatGPT responding to a user’s query in natural language</p>
<p class="normal">As you <a id="_idIndexMarker017"/>can see, the model was not only able to answer my question <a id="_idIndexMarker018"/>with an explanation of what a proton is; it also adapted its style and jargon to a specific target audience – in my case, a 5-year-old child. This is remarkable since it paves the way for many scenarios of hyper-personalization that were not possible before. In the next chapters, we will cover many examples of that.</p>
<p class="normal">ChatGPT is the main focus of this book, and in the upcoming chapters, you will see examples that showcase this powerful application.</p>
<p class="normal">Now, we will move on to image generation.</p>
<h2 class="heading-2" id="_idParaDest-18">Image generation</h2>
<p class="normal">One of the <a id="_idIndexMarker019"/>earliest and most well-known examples of generative AI in image <a id="_idIndexMarker020"/>synthesis is the <strong class="keyWord">generative adversarial network (GAN)</strong> architecture <a id="_idIndexMarker021"/>introduced in the 2014 paper by I. Goodfellow et al., <em class="italic">Generative Adversarial Networks</em>. The purpose of GANs is to generate realistic images that are indistinguishable from real images. This ability has several interesting business applications, such as generating synthetic datasets for training computer vision models, generating realistic product images, and generating realistic images for virtual reality and augmented reality applications.</p>
<p class="normal">Then, in 2021, a new generative AI model was introduced in this field by OpenAI, <strong class="screenText">DALL-E</strong>. Different from GANs, the DALL-E model is designed to generate images from descriptions in natural language and can generate a wide range of images. The main difference here is that while GANs are often used to create or improve realistic images, models like DALL-E are ideal for <a id="_idIndexMarker022"/>visual creativity, turning any description in natural language into an illustration.</p>
<p class="normal">DALL-E has <a id="_idIndexMarker023"/>great potential in creative industries such as advertising, product design, and fashion to create unique and creative images.</p>
<p class="normal">Since its first release to the time of writing (December 2024), DALL-E has improved dramatically, as you can see in the following examples. Below is an artistic creation by DALL-E at the dawn of its life:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_01_03.png"/></figure>
<p class="packt_figref">Figure 1.3: Images generated by DALL-E with a natural language prompt as input</p>
<p class="normal">Let’s now see what <strong class="screenText">DALL-E3</strong>, the most recent version of the model at the time of writing this book, can produce (here, we will use Microsoft Image Creator, powered by DALL-E3. You can try it at https://copilot.microsoft.com/images/create):</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B31559_01_04.png"/></figure>
<p class="packt_figref">Figure 1.4: Images generated by DALL-E3 with a natural language prompt as input</p>
<p class="normal">It’s impressive <a id="_idIndexMarker024"/>to see the level of improvement of this model in less than 2 years. We are <a id="_idIndexMarker025"/>just scraping the surface of the massive improvements occurring at a fast pace.</p>
<h2 class="heading-2" id="_idParaDest-19">Music generation</h2>
<p class="normal">The first approaches to generative AI for music generation trace back to the 1950s, with research in <a id="_idIndexMarker026"/>the field of algorithmic composition, a technique <a id="_idIndexMarker027"/>that uses algorithms to generate musical compositions. In 1957, Lejaren Hiller and Leonard Isaacson created the <em class="italic">Illiac Suite</em> for <em class="italic">String Quartet </em>(https://www.youtube.com/watch?v=n0njBFLQSk8), the first piece of music entirely composed by AI. Since then, the field of generative AI for music has been the subject of ongoing research. </p>
<p class="normal">Among recent years’ developments, new architectures and frameworks have become widespread among the general public, such as the WaveNet architecture introduced by Google in 2016, which has been able to generate high-quality audio samples, and the Magenta project, also developed by Google, which uses <strong class="keyWord">recurrent neural networks</strong> (<strong class="keyWord">RNNs</strong>) and other ML techniques to generate music and other forms of art.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal"><strong class="keyWord">RNNs</strong> are a type of neural network designed to process sequential data by <a id="_idIndexMarker028"/>retaining information from previous inputs through a loop-like structure. This allows them to recognize patterns and dependencies over time, making them ideal for tasks like language modeling, time-series prediction, and speech recognition.</p>
</div>
<p class="normal">In 2020, OpenAI also announced Jukebox, a neural network that generates music when provided with genre, artist, and lyrics as input.</p>
<p class="normal">These and <a id="_idIndexMarker029"/>other frameworks became the foundations <a id="_idIndexMarker030"/>of many AI composer assistants for music generation. An example is Flow Machines, developed by Sony CSL Research. This generative AI system was trained on a large database of musical pieces to create new music in a variety of styles. It was used by French composer Benoît Carré to compose an album called <em class="italic">Hello World </em>(https:// www.helloworldalbum.net/), which features collaborations with several human musicians.</p>
<p class="normal">Here, you can see an example of a track generated entirely by Music Transformer, one of the models within the Magenta project:</p>
<figure class="mediaobject"><img alt="" src="img/B31559_01_05.png"/></figure>
<p class="packt_figref">Figure 1.5: Music Transformer allows users to listen to musical performances generated by AI (https://magenta.tensorflow.org/music-transformer)</p>
<p class="normal">Another incredible application of generative AI within the music domain is speech synthesis. This refers to AI tools that can create audio based on text inputs in the voices of well-known singers.</p>
<p class="normal">For example, if you <a id="_idIndexMarker031"/>have always wondered how your songs would sound if Lady <a id="_idIndexMarker032"/>Gaga performed them, well, you can now fulfill your dreams with tools such as FakeYou <em class="italic">Text to Speech</em> (https://fakeyou.com/tts) or UberDuck.ai (https://uberduck.ai/)!</p>
<figure class="mediaobject"><img alt="" src="img/B31559_01_06.png"/></figure>
<p class="packt_figref">Figure 1.6: Text-to-speech synthesis with fakeyou.com</p>
<p class="normal">The results are really impressive! If you want to have fun, you can also try voices from your favorite cartoons, such as <em class="italic">Winnie the Pooh</em>. The only thing you need to do is input the text of the song you want your favorite voice to sing aloud.</p>
<p class="normal">Let’s go even further. What if we could generate a song from scratch, just asking the generative AI to do that for us in natural language? Well, we can do that seamlessly today and without any knowledge about music. Among the generative AI products that are rising in the music market today is Suno, whose mission is <em class="italic">“[...]building a future where anyone can make great music. Whether you’re a shower singer or a charting artist, we break barriers between you and the song you dream of making. No instrument needed, just imagination. From your mind to music.”</em> (source: https://suno.com/about).</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B31559_01_07.png"/></figure>
<p class="packt_figref">Figure 1.7: Example of an entire song generated by Suno.com from a description in natural language</p>
<p class="normal">As you can see, on the left-hand side of the picture, I provided a very brief song description in natural <a id="_idIndexMarker033"/>language – this was my prompt. From that, the model <a id="_idIndexMarker034"/>was able to generate not only the title and lyrics of a song (on the right-hand side) but also the music!</p>
<p class="normal">Can you believe that it became my summer 2024 hit? If you want to create your summer hit too, you can try it for free at https://suno.com/create.</p>
<h2 class="heading-2" id="_idParaDest-20">Video generation</h2>
<p class="normal">Generative AI for video generation shares a similar timeline of development with image generation. One <a id="_idIndexMarker035"/>of the key developments in the field of video generation <a id="_idIndexMarker036"/>has been the development of GANs. Thanks to their accuracy in producing realistic images, researchers have started to apply this technique to video generation as well. One of the most notable examples of GAN-based video generation is DeepMind’s Veo, which generates high-quality videos from a single image and a sequence of motions. Another great example is NVIDIA’s <strong class="keyWord">video-to-video synthesis</strong> (<strong class="keyWord">Vid2Vid</strong>) DL-based <a id="_idIndexMarker037"/>framework, which uses GANs to synthesize high-quality videos from input videos.</p>
<p class="normal">The Vid2Vid system can generate temporally consistent videos, meaning that they maintain smooth and realistic motion over time. The technology can be used to perform a variety of video <a id="_idIndexMarker038"/>synthesis tasks, such as the following:</p>
<ul>
<li class="bulletList">Converting videos from one domain into another (for example, converting a daytime video into a nighttime video or a sketch into a realistic image)</li>
<li class="bulletList">Modifying existing videos (for example, changing the style or appearance of objects in a video)</li>
<li class="bulletList">Creating new videos from static images (for example, animating a sequence of still images)</li>
</ul>
<p class="normal">In September 2022, Meta’s <a id="_idIndexMarker039"/>researchers announced the general availability of <strong class="keyWord">Make-A-Video</strong> (https://makeavideo.studio/), a new AI system that allows users to convert their natural language prompts into video clips. Behind this technology, you can <a id="_idIndexMarker040"/>recognize many of the models that we <a id="_idIndexMarker041"/>mentioned in other domains – language understanding for the prompt, image and motion generation with image generation, and background music made by AI composers.</p>
<p class="normal">Now, everything we’ve mentioned above pales in comparison to the latest text-to-video models. To name one, OpenAI <a id="_idIndexMarker042"/>announced a text-to-video model called <strong class="keyWord">SORA</strong> in February 2024 and released some early experiments:</p>
<figure class="mediaobject"><img alt="A person in a black jacket and red dress standing on a wet street  Description automatically generated" src="img/B31559_01_08.png"/><img alt="A group of mammoths in the snow  Description automatically generated" src="img/B31559_01_09.png"/></figure>
<figure class="mediaobject"><img alt="A person in a space suit  Description automatically generated" src="img/B31559_01_10.png"/> <img alt="A cartoon animal looking at a candle  Description automatically generated" src="img/B31559_01_11.png"/></figure>
<p class="packt_figref">Figure 1.8: Videos generated by SORA from prompts in natural language. Source: https://openai.com/index/sora/</p>
<p class="normal">I do encourage <a id="_idIndexMarker043"/>you to visit the SORA webpage to have a look at the amazing <a id="_idIndexMarker044"/>videos it created. At the time of writing, SORA is not publicly available, as it is going through several tests by the OpenAI Red Team.</p>
<p class="normal">Overall, generative AI has impacted many domains for years, and some AI tools already consistently support artists, organizations, and general users. Despite the fact we’ve been experimenting and building applications with generative AI for only two years, there are already some consolidated trends and future innovations to keep in mind. Let’s explore them in the next section.</p>
<h1 class="heading-1" id="_idParaDest-21">Main trends and innovations</h1>
<p class="normal">From November 2022 to today, we have witnessed a huge amount of innovation in the field of generative AI. Many <a id="_idIndexMarker045"/>of these innovations are linked to the brand-new models developed and released to the public, like OpenAI’s GPT-4o and DALL-E3, but also Google Gemini, Meta Llama 3, Microsoft Phi3, and many others.</p>
<p class="normal">However, the most remarkable achievements probably lie in the way we interact with and build applications around those models. In this section, we are going to explore three main advancements that have marked the most popular reference architectures for generative-AI-powered applications.</p>
<h2 class="heading-2" id="_idParaDest-22">Retrieval augmented generation</h2>
<p class="normal">One of the <a id="_idIndexMarker046"/>first limitations of ChatGPT and, generally speaking, of LLMs was the knowledge base cutoff. The knowledge of LLMs is limited to the training set they have been trained on and, although this can be exhaustive, it’s not up to date (in fact, once the model is trained, any new data or information that emerges afterward won’t be part of its knowledge, since it wasn’t included in the original training set). Plus, the data is likely missing the proprietary knowledge base that might be relevant for us or our organization. For example, if you ask ChatGPT, “What is my company’s policy for employee health insurance?”, the model won’t be able to answer since it has no access to this information.</p>
<p class="normal">To bypass this limitation, a new framework was designed to allow LLMs to navigate through customized <a id="_idIndexMarker047"/>documentation <a id="_idIndexMarker048"/>that we can provide. This framework is called <strong class="keyWord">retrieval augmented generation</strong> (<strong class="keyWord">RAG</strong>).</p>
<p class="normal">The idea behind RAG is to augment the LLM’s knowledge by adding external sources of information, yet without modifying the structure of the model at all.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">An embedding is <a id="_idIndexMarker049"/>a way to turn complex information—like words, sentences, or images—into a list of numbers (a vector). This makes it easier for a computer to understand what those words or sentences mean. If two pieces of text have similar meanings, their vectors will be close together in the numerical space. In other words, embeddings let computers measure how alike different inputs are based on their content, not just their exact wording.</p>
</div>
<p class="normal">For example, if two concepts are similar, then their vector representations should also be similar.</p>
<figure class="mediaobject"><img alt="" src="img/B31559_01_12.png"/></figure>
<p class="packt_figref">Figure 1.9: Example of vector representation of four different words</p>
<p class="normal">In this example, we can see that the mathematical distance between the two vectors corresponding to “Queen” and “King” is more or less the same as the difference between “Woman” and “Man.” Semantically speaking, this makes sense, as we are talking about a similar relationship. A similar example might be applied to the relationship between countries and capital cities: once embedded in a vector space, the distance between “Italy” and “Rome” should <a id="_idIndexMarker050"/>be similar to the distance between “France” and “Paris” as they are mapping the same relationships.</p>
<p class="normal">RAG is <a id="_idIndexMarker051"/>made of three phases:</p>
<ol>
<li class="numberedList" value="1"><strong class="screenText">Retrieval</strong>: Given a user’s query and its corresponding numerical representation, the most similar pieces of documents (those corresponding to the vectors that are closest to the user query’s vector) are retrieved and used as the base context for the LLM.</li>
</ol>
<figure class="mediaobject"><img alt="A close-up of a text box  Description automatically generated" src="img/B31559_01_13.png"/></figure>
<p class="packt_figref">Figure 1.10: Example of retrieving three different chunks from different documents, since they are represented by the closest vectors to the user query</p>
<ol>
<li class="numberedList" value="2"><strong class="screenText">Augmentation</strong>: The retrieved context is enriched through additional instructions, rules, safety guardrails, and similar practices that are typical of prompt engineering techniques (we will cover the topic of prompt engineering in <em class="italic">Chapter 3</em>).</li>
</ol>
<figure class="mediaobject"><img alt="" src="img/B31559_01_14.png"/></figure>
<p class="packt_figref">Figure 1.11: Example of adding more context to the retrieved chunks of documents</p>
<ol>
<li class="numberedList" value="3"><strong class="screenText">Generation</strong>: Based on <a id="_idIndexMarker052"/>the augmented <a id="_idIndexMarker053"/>context, the LLM generates the response to the user’s query.</li>
</ol>
<figure class="mediaobject"><img alt="A screenshot of a computer screen  Description automatically generated" src="img/B31559_01_15.png"/></figure>
<p class="packt_figref">Figure 1.12: Example of using the augmented context as the system message for the model to generate the final answer</p>
<p class="normal">RAG combines the strengths of generative models and information retrieval systems to enhance the quality and relevance of generated content. Traditional generative models rely solely on <a id="_idIndexMarker054"/>their training data to produce responses, which <a id="_idIndexMarker055"/>can sometimes result in outdated or irrelevant information. RAG addresses this limitation by integrating external knowledge bases during the generation process.</p>
<h2 class="heading-2" id="_idParaDest-23">Multimodality</h2>
<p class="normal">Earlier in this chapter, we looked at the various domains of generative AI, ranging from text to images, from <a id="_idIndexMarker056"/>videos to music. Typically, large foundation models tend to be domain-specific, as we saw for LLMs in the case of language <a id="_idIndexMarker057"/>understanding and generation, or DALL-E3 in the case of image generation.</p>
<p class="normal">However, the recent <a id="_idIndexMarker058"/>advances in generative AI have enabled the development of <strong class="keyWord">large multimodal models</strong> (<strong class="keyWord">LMMs</strong>) that can process and generate different types of data, such as text, images, audio, and video.</p>
<p class="normal">LMMs share with <em class="italic">standard</em> LLMs the ability to generalize and adapt typical large foundation models. However, LMMs are capable of processing diverse data with the idea of mirroring the way humans interact with the surrounding ecosystem – that is, with all our senses.</p>
<p class="normal">A great example of a multimodal model is OpenAI’s GPT-4o, which is able to interact with users via text, images, and audio. Take the following example:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B31559_01_16.png"/></figure>
<p class="packt_figref">Figure 1.13: Example of providing ChatGPT-4o with a picture and asking it to name the building</p>
<p class="normal">As you can see, the model was able to analyze the image and reason over it.</p>
<p class="normal">Let’s now go ahead and ask the model to generate an illustration:</p>
<figure class="mediaobject"><img alt="A black and white drawing of a tall building  Description automatically generated" src="img/B31559_01_17.png"/></figure>
<p class="packt_figref">Figure 1.14: Example of ChatGPT-4o generating an illustration based on a previously provided picture</p>
<p class="normal">What sets LLMs apart is their ability to retain advanced reasoning capabilities, making them uniquely <a id="_idIndexMarker059"/>suited for tackling complex reasoning tasks across <a id="_idIndexMarker060"/>diverse data contexts, unlike traditional AI models. Let’s consider, for example, traditional computer vision models, which are task-specific, and they do not <em class="italic">reason</em> over an image, but rather perform tasks like detecting objects or extracting text from images. On the other hand, LMMs can use the same reasoning capabilities as LLMs, yet they can apply these capabilities to data other than text.</p>
<p class="normal">Let’s consider this last example (showing only the first lines of the response):</p>
<figure class="mediaobject"><img alt="A crossword puzzle with text  Description automatically generated" src="img/B31559_01_18.png"/></figure>
<p class="packt_figref">Figure 1.15: Example of ChatGPT 4o solving a crossword game</p>
<p class="normal">As you <a id="_idIndexMarker061"/>can see, the model was able to:</p>
<ul>
<li class="bulletList">Read and <a id="_idIndexMarker062"/>understand the scenario the image is posing</li>
<li class="bulletList">Reason about it and solve the complex task that it is offering, which is solving a puzzle</li>
</ul>
<p class="normal">As you may imagine, this opens a landscape of applications in various industries, and we are going to see some concrete examples in the upcoming chapters.</p>
<h2 class="heading-2" id="_idParaDest-24">AI agents</h2>
<p class="normal">In previous sections, we uncovered how LLMs are great when it comes to generating content. However, they lack one ability, which is taking action and interacting with the surrounding <a id="_idIndexMarker063"/>ecosystem that goes beyond the single user. For example, what if we want our LLM to be able not only to generate an amazing LinkedIn <a id="_idIndexMarker064"/>post but also publish it on our page?</p>
<p class="normal">AI agents emerge as key players in overcoming this limitation. But what exactly are they? Agents can be seen as AI systems powered by LLMs that, given a user’s query, are able to interact with the surrounding ecosystem to the extent to which we allow them. The perimeter of the ecosystem is delimited by the tools (or plugins) we provide the agents with (in our previous example, we might provide the agent with a LinkedIn plugin so that it is able to post the generated content).</p>
<p class="normal">Agents are made of the following ingredients:</p>
<ul>
<li class="bulletList">An LLM, which acts as the reasoning engine of the AI system.</li>
<li class="bulletList">A system message, which instructs the agent to behave and think in a given way. For example, you can design an agent as a teaching assistant for students with the following system message: “You are a teaching assistant. Given a student’s query, NEVER provide the final answer, but rather provide some hints to get there.”</li>
<li class="bulletList">A set of tools the agent can leverage to interact with the surrounding ecosystem.</li>
</ul>
<p class="normal">AI agents are a perfect representation of the meaning of “LLM as reasoning engine of an application.” In fact, the beauty of agents is that they can pick the best tool to use to accomplish <a id="_idIndexMarker065"/>a user’s request. For example, let’s say we have an AI agent to produce <a id="_idIndexMarker066"/>LinkedIn content, and we provide it with two tools: a LinkedIn plugin and a web search plugin (each one with a correct description of its functionality). Let’s explore the behavior of the agent in three different scenarios:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Generate a story about a little dog walking around the mountains</strong>: The agent will generate the story without using a plugin.</li>
<li class="bulletList"><strong class="keyWord">Generate a story about the current weather in Milan</strong>: The agent will invoke the web search plugin to get the current weather in Milan.</li>
<li class="bulletList"><strong class="keyWord">Generate a LinkedIn post about the current weather in Milan and publish it on my profile</strong>: The agent will invoke the web search plugin to get the current weather in Milan and the LinkedIn plugin to post it on my profile.</li>
</ul>
<p class="normal">The combination of instructions and a set of plugins makes AI agents extremely versatile, and you can create highly specialized entities to address specific scenarios.</p>
<p class="normal">And that’s not all.</p>
<p class="normal">Why have only one agent if you can create your own crew of agents talking to and cooperating with each other? Imagine multiple agents, each one with a specific expertise and goal, communicating and interacting to accomplish a task. This is what <strong class="keyWord">multi-agent applications</strong> look like, and in <a id="_idIndexMarker067"/>the last few months, this pattern started showing very interesting results.</p>
<p class="normal">Let’s consider the following example. We want to generate an elevator pitch about climate change. We need up-to-date information to do so (latest trends and research, future perspectives, and so on), as well as solid research grounded by academic papers. Plus, we need to be concise yet sharp and effective, delivering all the key information in a very short pitch.</p>
<p class="normal">Now, we could ask a single agent to do all of that, providing it with all the required tools and long instructions to accomplish the task. However, if the task gets very complex, a single agent might not be the best approach as it might lead to inaccurate results. Instead, let’s use a multi-agent approach, creating a team with the following AI professionals:</p>
<ul>
<li class="bulletList">A market analyst who can search the web for the latest news about climate change: This will be an agent with a web search plugin and specific instructions to search for news.</li>
<li class="bulletList">An expert researcher who can easily navigate through academic research papers about climate change: This will be an agent with an Arxiv (a curated research-sharing platform) plugin and specific instructions on how to retrieve relevant information.</li>
<li class="bulletList">An expert in public speaking who can easily consolidate all the information in one elevator pitch: This will be an agent with instructions on how to deliver perfect pitches.</li>
<li class="bulletList">A critic who will review the pitch and propose some changes to the expert in public speaking, if needed: This will be an agent with instructions on how to review and improve a pitch by identifying pitfalls and areas of improvement.</li>
</ul>
<p class="normal">So, when the user asks the agents to generate an elevator pitch about the current issue of climate change, all the agents can start working on the project.</p>
<p class="normal">There are <a id="_idIndexMarker068"/>many frameworks that can help developers with multi-agent applications (including AutoGen, LangGraph, and CrewAI), especially when it comes to the <em class="italic">flow</em> that we <a id="_idIndexMarker069"/>want our agents to follow. For example, we might want to enforce a specific number of iterations; or that all agents are invoked at least once; or even to involve us, as users, in every iteration to provide further feedback to be incorporated in the upcoming iteration.</p>
<p class="normal">At the time of writing, the multi-agent framework is showing promising advancements, and it is a glimpse of the outstanding reasoning capabilities behind LLMs and how they can unlock new ways of problem-solving.</p>
<h2 class="heading-2" id="_idParaDest-25">Small language models</h2>
<p class="normal">LLMs are, unsurprisingly, large. This means that the architecture of an <strong class="keyWord">ANN</strong> featuring <a id="_idIndexMarker070"/>LLMs is made of a huge number <a id="_idIndexMarker071"/>of parameters, in the order of billions. Typically, a large number <a id="_idIndexMarker072"/>of parameters is associated with a better-performing model, since it is able to deal with more information and examples and henceforth is able to recognize and infer more patterns the moment users ask their questions. However, with large numbers of parameters typically comes a high cost of training and hosting, since a powerful AI infrastructure is needed. Plus, the energy consumption of these models raises serious questions about the environmental impact of LLM training and their overall sustainability in the long run.</p>
<p class="normal">These smaller models are called <strong class="keyWord">small language models</strong> (<strong class="keyWord">SLMs</strong>) and, besides being lighter and less <a id="_idIndexMarker073"/>demanding in terms of infrastructure, they are also showing surprisingly high performance.</p>
<p class="normal">Now, we might think that GPT-3.5-turbo is deprecated; however, we have to remember that it used to be <a id="_idIndexMarker074"/>the most powerful model on the market just one year ago, and it is <a id="_idIndexMarker075"/>remarkable to see that a 7B model is capable of better results.</p>
<p class="normal">SLMs are definitely a research stream to keep an eye on, especially when it comes to scenarios where we might want to deploy a model locally or even customize it with fine-tuning (we will cover fine-tuning in the next chapter).</p>
<h1 class="heading-1" id="_idParaDest-26">Legal and ethical landscape of generative AI</h1>
<p class="normal">When developing and deploying generative AI systems, a broad range of legal and ethical considerations <a id="_idIndexMarker076"/>must be carefully addressed to ensure responsible and sustainable use. These considerations extend beyond mere compliance and enter a domain where moral responsibility, public trust, and technological accountability intersect.</p>
<h2 class="heading-2" id="_idParaDest-27">Copyright and intellectual property issues</h2>
<p class="normal">LLMs are often trained on vast corpora scraped from the internet, including content that may be <a id="_idIndexMarker077"/>copyrighted. As a result, there is a real risk of embedding copyrighted text, music, images, or video segments directly into AI output, inadvertently producing infringements when <a id="_idIndexMarker078"/>these outputs are shared or commercialized.</p>
<p class="normal">This concrete risk also escalated in November 2024, when major Canadian news organizations (https://www.reuters.com/sustainability/boards-policy-regulation/major-canadian-news-media-companies-launch-legal-action-against-openai-2024-11-29/), including The Globe and Mail and CBC/Radio-Canada, filed a lawsuit against OpenAI. They alleged that OpenAI used their copyrighted content without authorization to train its AI models, seeking damages and an injunction to prevent further unauthorized use.</p>
<h2 class="heading-2" id="_idParaDest-28">Misinformation, hallucinations, and the risk of fake news</h2>
<p class="normal">One of <a id="_idIndexMarker079"/>the known limitations of current <a id="_idIndexMarker080"/>generative AI models <a id="_idIndexMarker081"/>is their tendency to <strong class="keyWord">hallucinate</strong> – to produce entirely plausible-sounding but factually incorrect statements. This can result in the inadvertent spread of misinformation, especially when AI-generated content is taken at face value by consumers, journalists, or public officials.</p>
<p class="normal">As an example, in December 2024, misinformation researcher Jeff Hancock (https://www.theverge.com/2024/12/4/24313132/jeff-hancock-minnesota-deepfake-law-ai-hallucinations-citation) admitted that ChatGPT fabricated details in a court filing he prepared, leading to the submission of non-existent citations. This incident emphasizes <a id="_idIndexMarker082"/>the risk of AI-generated <a id="_idIndexMarker083"/>content introducing <a id="_idIndexMarker084"/>inaccuracies in critical documents.</p>
<p class="normal">Continual exposure to unreliable AI output may lead to widespread skepticism regarding all digital content, undermining the credibility of legitimate sources and diminishing trust in expert commentary and reputable journalism. Organizations must therefore invest in factual verification processes, human-in-the-loop validation, and transparent model evaluation methods.</p>
<h2 class="heading-2" id="_idParaDest-29">Deepfakes and deceptive manipulation</h2>
<p class="normal">Deepfake <a id="_idIndexMarker085"/>technology, an advanced subset of generative AI that synthesizes highly realistic images, videos, and voice recordings, can be weaponized to impersonate public figures, fabricate scandalous events, or produce manipulative political propaganda.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">A deepfake is <a id="_idIndexMarker086"/>a type of artificial media created using DL algorithms, where a person’s likeness, voice, or movements are digitally manipulated to create realistic but fake content. Typically, deepfakes involve altering videos or images to make it appear as if someone said or did something they never actually did.</p>
</div>
<p class="normal">A recent <a id="_idIndexMarker087"/>example occurred back in 2023, when a finance clerk at a Hong Kong branch of a multinational corporation was deceived into transferring over $25 million after scammers used deepfake audio to impersonate senior executives, directing unauthorized fund transfers (https://www.secureworld.io/industry-news/hong-kong-deepfake-cybercrime).</p>
<p class="normal">Companies, governments, and individuals targeted by deepfakes may suffer severe reputational harm, leading to public embarrassment, financial losses, or diminished trust. Building detection tools, implementing digital watermarking techniques, and establishing legal frameworks that penalize malicious deepfake creators are crucial steps in mitigating these risks.</p>
<h2 class="heading-2" id="_idParaDest-30">Bias, discrimination, and social harm</h2>
<p class="normal">Generative AI models can unintentionally reproduce and magnify existing societal prejudices present in <a id="_idIndexMarker088"/>their training data. For example, models might consistently portray certain professions as male-dominated or depict particular cultural groups in stereotypical roles.</p>
<p class="normal">These biased outputs can influence hiring decisions, product recommendations, and policy-making processes, ultimately disadvantaging underrepresented groups.</p>
<p class="normal">In this regard, a 2023 study, <em class="italic">Demographic Stereotypes in Text-to-Image Generation </em>(https://hai.stanford.edu/sites/default/files/2023-11/Demographic-Stereotypes.pdf), highlighted that text-to-image generative AI models tend to encode substantial <a id="_idIndexMarker089"/>bias and stereotypes. For example, prompts requesting images of professionals often resulted in depictions aligning with traditional gender roles, such as male doctors and female nurses, thereby reinforcing outdated and discriminatory views.</p>
<p class="normal">Another study, <em class="italic">Social Dangers of Generative Artificial Intelligence: Review and Guidelines</em> (https://dl.acm.org/doi/fullHtml/10.1145/3657054.3664243), investigates the extent to which <a id="_idIndexMarker090"/>these technologies can exacerbate existing inequality. For instance, AI-generated content may marginalize certain communities by underrepresenting them or portraying them negatively, leading to social harm and reinforcing systemic discrimination.</p>
<p class="normal">Organizations must commit to comprehensive bias audits, regularly updating training datasets, implementing fairness constraints, and involving diverse stakeholders in model development and evaluation.</p>
<p class="normal">These are just some examples of the potential risks and issues associated with generative AI. Furthermore, it is important to acknowledge that similar legal and ethical implications are not limited to generative AI, but rather they apply to the broader landscape of AI, whose applications have always been raising some concerns (for example, privacy considerations when it comes to face recognition).</p>
<p class="normal">However, the extremely rapid evolvement and – most importantly – adoption of generative AI tools has highlighted the pressing need for organizations, policymakers, and developers to collaborate to craft robust governance frameworks that address the unique challenges posed by generative AI. This involves adopting standards for transparent data sourcing, obtaining explicit permissions for copyrighted content, implementing strict verification procedures to counter misinformation, and working closely with regulators to establish legal guardrails. It also demands that AI practitioners remain continuously vigilant in updating models, refining algorithms, and engaging with interdisciplinary experts to ensure that generative AI serves as a force for innovation and positive societal impact, rather than a source of harm or ethical compromise.</p>
<h1 class="heading-1" id="_idParaDest-31">Summary</h1>
<p class="normal">In this chapter, we have explored the exciting world of generative AI and its various domains of application, including image generation, text generation, music generation, and video generation. We learned how generative AI models such as ChatGPT and DALL-E, trained by OpenAI, use DL techniques to learn patterns in large datasets and generate new content that is both novel and coherent. We also discussed the history of generative AI, its origins, and the current status of research on it.</p>
<p class="normal">The goal of this chapter was to provide a solid foundation in the basics of generative AI and to inspire you to explore this fascinating field further.</p>
<p class="normal">In the next chapter, we will focus on one of the most promising technologies available on the market today, ChatGPT. We will go through the research behind it and its development by OpenAI, the architecture of its model, and the main use cases it can address as of today.</p>
<h1 class="heading-1" id="_idParaDest-32">References</h1>
<ul>
<li class="bulletList">Generative adversarial networks: https://arxiv.org/abs/1406.2661</li>
<li class="bulletList">Analyzing and improving the image quality of StyleGAN: https://arxiv.org/abs/1912.04958</li>
<li class="bulletList">Video-to-video synthesis: https://arxiv.org/abs/1808.06601</li>
<li class="bulletList">A deep generative model trifecta: Three advances that work towards harnessing large-scale power: https://www.microsoft.com/en-us/research/blog/a-deep-generative- model-trifecta-three-advances-that-work-towards-harnessing- large-scale-power/</li>
<li class="bulletList">Vid2Vid: https://tcwang0509.github.io/vid2vid/</li>
<li class="bulletList">LLaMA: Open and Efficient Foundation Language Models: https://arxiv.org/pdf/2302.13971</li>
<li class="bulletList">Introducing Phi-3: https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/</li>
</ul>
<h1 class="heading-1">Join our communities on Discord and Reddit</h1>
<p class="normal">Have questions about the book or want to contribute to discussions on Generative AI and LLMs? Join our Discord server at <a href="Chapter_1.xhtml">https://packt.link/I1tSU</a> and our Reddit channel at <a href="Chapter_1.xhtml">https://packt.link/jwAmA</a> to connect, share, and collaborate with like-minded enthusiasts.</p>
<p class="normal"><img alt="" src="img/Discord.png"/> <img alt="" src="img/QR_Code757615820155951000.png"/></p>
</div>
</body></html>