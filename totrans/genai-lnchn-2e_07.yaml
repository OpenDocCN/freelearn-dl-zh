- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Advanced Applications and Multi-Agent Systems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级应用和多智能体系统
- en: In the previous chapter, we defined what an agent is. But how do we design and
    build a high-performing agent? Unlike the prompt engineering techniques we’ve
    previously explored, developing effective agents involves several distinct design
    patterns every developer should be familiar with. In this chapter, we’re going
    to discuss key architectural patterns behind agentic AI. We’ll look into multi-agentic
    architectures and the ways to organize communication between agents. We will develop
    an advanced agent with self-reflection that uses tools to answer complex exam
    questions. We will learn about additional LangChain and LangGraph APIs that are
    useful when implementing agentic architectures, such as details about LangGraph
    streaming and ways to implement handoff as part of advanced control flows.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们定义了什么是智能体。但我们是如何设计和构建一个高性能的智能体的呢？与之前探索的提示工程技巧不同，开发有效的智能体涉及到几个独特的模式，每个开发者都应该熟悉。在本章中，我们将讨论智能体AI背后的关键架构模式。我们将探讨多智能体架构以及组织智能体之间通信的方式。我们将开发一个具有自我反思能力的先进智能体，使用工具来回答复杂的考试问题。我们还将了解LangChain和LangGraph
    API的附加功能，这些功能在实现智能体架构时很有用，例如关于LangGraph流式传输的细节以及实现高级控制流中交接的方式。
- en: Then, we’ll briefly touch on the LangGraph platform and discuss how to develop
    adaptive systems, by including humans in the loop, and what kind of prebuilt building
    blocks LangGraph offers for this. We will also look into the **Tree-of-Thoughts**
    (**ToT**) pattern and develop a ToT agent ourselves, discussing further ways to
    improve it by implementing advanced trimming mechanisms. Finally, we’ll learn
    about advanced long-term memory mechanisms on LangChain and LangGraph, such as
    caches and stores.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将简要介绍LangGraph平台，并讨论如何通过将人类纳入循环来开发自适应系统，以及LangGraph为此提供的预建构建块。我们还将探讨**思维树**（**ToT**）模式，并开发自己的ToT智能体，进一步讨论通过实现高级修剪机制来改进它的方法。最后，我们将了解LangChain和LangGraph上的高级长期记忆机制，例如缓存和存储。
- en: 'In all, we’ll touch on the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在本章中，我们将涉及以下主题：
- en: Agentic architectures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体架构
- en: Multi-agent architectures
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多智能体架构
- en: Building adaptive systems
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建自适应系统
- en: Exploring reasoning paths
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索推理路径
- en: Agent memory
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体内存
- en: Agentic architectures
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体架构
- en: As we learned in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), agents help
    humans solve tasks. Building an agent involves balancing two elements. On one
    side, it’s very similar to application development in the sense that you’re combining
    APIs (including calling foundational models) with production-ready quality. On
    the other side, you’re helping LLMs think and solve a task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中学到的，智能体帮助人类解决问题。构建智能体涉及到平衡两个要素。一方面，它与应用程序开发非常相似，因为您正在结合API（包括调用基础模型）以生产就绪的质量。另一方面，您正在帮助LLM思考和解决问题。
- en: 'As we discussed in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), agents
    don’t have a specific algorithm to follow. We give an LLM partial control over
    the execution flow, but to guide it, we use various tricks that help us as humans
    to reason, solve tasks, and think clearly. We should not assume that an LLM can
    magically figure everything out itself; at the current stage, we should guide
    it by creating reasoning workflows. Let’s recall the ReACT agent we learned about
    in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), an example of a tool-calling
    pattern:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中讨论的那样，智能体没有特定的算法要遵循。我们给予LLM部分控制执行流程的权限，但为了引导它，我们使用各种技巧，这些技巧有助于我们人类进行推理、解决问题和清晰思考。我们不应假设LLM可以神奇地自己解决所有问题；在当前阶段，我们应该通过创建推理工作流程来引导它。让我们回顾一下我们在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中学到的ReACT智能体，这是一个工具调用模式的例子：
- en: '![Figure 6.1: A prebuilt REACT workflow on LangGraph](img/B32363_06_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1：LangGraph上的预建REACT工作流程](img/B32363_06_01.png)'
- en: 'Figure 6.1: A prebuilt REACT workflow on LangGraph'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：LangGraph上的预建REACT工作流程
- en: 'Let’s look at a few relatively simple design patterns that help with building
    well-performing agents. You will see these patterns in various combinations across
    different domains and agentic architectures:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些相对简单的设计模式，这些模式有助于构建性能良好的智能体。您将在不同领域和智能体架构中看到这些模式的各种组合：
- en: '**Tool calling**: LLMs are trained to do controlled generation via tool calling.
    Hence, wrap your problem as a tool-calling problem when appropriate instead of
    creating complex prompts. Keep in mind that tools should have clear descriptions
    and property names, and experimenting with them is part of the prompt engineering
    exercise. We discussed this pattern in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231).'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具调用**：LLM被训练通过工具调用进行受控生成。因此，在适当的时候，将问题包装为工具调用问题，而不是创建复杂的提示。请记住，工具应该有清晰的描述和属性名称，并且对它们的实验是提示工程练习的一部分。我们已在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中讨论了这种模式。'
- en: '**Task decomposition**: Keep your prompts relatively simple. Provide specific
    instructions with few-shot examples and split complex tasks into smaller steps.
    You can give an LLM partial control over the task decomposition and planning process,
    managing the flow by an external orchestrator. We used this pattern in [*Chapter
    5*](E_Chapter_5.xhtml#_idTextAnchor231) when we built a plan-and-solve agent.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务分解**：保持您的提示相对简单。提供具有少量示例的具体指令，并将复杂任务分解成更小的步骤。您可以给予LLM对任务分解和规划过程的有限控制，通过外部协调器管理流程。我们在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中使用了这种模式，当时我们构建了一个计划并解决代理。'
- en: '**Cooperation and diversity**: Final outputs on complex tasks can be improved
    if you introduce cooperation between multiple instances of LLM-enabled agents.
    Communicating, debating, and sharing different perspectives helps, and you can
    also benefit from various skill sets by initiating your agents with different
    system prompts, available toolsets, etc. Natural language is a native way for
    such agents to communicate since LLMs were trained on natural language tasks.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合作与多样性**：在多个启用LLM的代理实例之间引入合作可以改善复杂任务的最终输出。沟通、辩论和分享不同的观点有助于提高效率，通过为您的代理启动不同的系统提示、可用的工具集等，您还可以从各种技能集中受益。自然语言是此类代理进行沟通的本地方式，因为LLM是在自然语言任务上训练的。'
- en: '**Reflection and adaptation**: Adding implicit loops of reflection generally
    improves the quality of end-to-end reasoning on complex tasks. LLMs get feedback
    from the external environment by calling the tools (and these calls might fail
    or produce unexpected results), but at the same time, LLMs can continue iterating
    and self-recover from their mistakes. As an exaggeration, remember that we often
    use the same LLM-as-a-judge, so adding a loop when we ask an LLM to evaluate its
    own reasoning and find errors often helps it to recover. We will learn how to
    build adaptive systems later in this chapter.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反思与适应**：添加隐式循环的反思通常可以提高复杂任务端到端推理的质量。LLM通过调用工具（这些调用可能会失败或产生意外结果）从外部环境获取反馈，但与此同时，LLM可以继续迭代并从错误中自我恢复。作为夸张的说法，请记住我们经常使用同一个LLM作为评判者，所以当我们要求LLM评估其自身的推理并找出错误时，添加循环通常有助于其恢复。我们将在本章后面学习如何构建自适应系统。'
- en: '**Models are nondeterministic and can generate multiple candidates**: Do not
    focus on a single output; explore different reasoning paths by expanding the dimension
    of potential options to try out when an LLM interacts with the external environment
    when looking for the solution. We will investigate this pattern in more detail
    in the section below when we discuss ToT and **Language Agent Tree Search** (**LATS**)
    examples.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型是非确定性的，可以生成多个候选方案**：不要专注于单个输出；当LLM与外部环境互动寻找解决方案时，通过扩展潜在选项的维度来探索不同的推理路径。我们将在下面的章节中更详细地研究这种模式，当我们讨论ToT和**语言代理树搜索**（**LATS**）示例时。'
- en: '**Code-centric problem framing**: Writing code is very natural for an LLM,
    so try to frame the problem as a code-writing problem if possible. This might
    become a very powerful way of solving the task, especially if you wrap it with
    a code-executing sandbox, a loop for improvement based on the output, access to
    various powerful libraries for data analysis or visualization, and a generation
    step afterward. We will go into more detail in [*Chapter 7*](E_Chapter_7.xhtml#_idTextAnchor327).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以代码为中心的问题框架**：编写代码对于LLM来说非常自然，因此如果可能的话，尽量将问题框架为代码编写问题。这可能成为一种非常强大的任务解决方式，特别是如果您将其包装在代码执行沙盒中，基于输出的改进循环，访问各种强大的数据分析或可视化库，以及之后的生成步骤。我们将在[*第七章*](E_Chapter_7.xhtml#_idTextAnchor327)中更详细地介绍这一点。'
- en: 'Two important comments: first, develop your agents aligned with the best software
    development practices, and make them agile, modular, and easily configurable.
    That would allow you to put multiple specialized agents together, and give users
    the opportunity to easily tune each agent based on their specific task.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 两个重要的评论：首先，开发与最佳软件开发实践一致的代理，并使它们敏捷、模块化且易于配置。这将允许您将多个专业代理组合在一起，并使用户能够根据其特定任务轻松调整每个代理。
- en: Second, we want to emphasize (once again!) the importance of evaluation and
    experimentation. We will talk about evaluation in more detail in [*Chapter 9*](E_Chapter_9.xhtml#_idTextAnchor448).
    But it’s important to keep in mind that there is no clear path to success. Different
    patterns work better on different types of tasks. Try things, experiment, iterate,
    and don’t forget to evaluate the results of your work. Data, such as tasks and
    expected outputs, and simulators, a safe way for LLMs to interact with tools,
    are key to building really complex and effective agents.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们想再次强调（再次！）评估和实验的重要性。我们将在[*第9章*](E_Chapter_9.xhtml#_idTextAnchor448)中更详细地讨论评估。但重要的是要记住，没有一条明确的成功之路。不同的模式在不同的任务类型上效果更好。尝试新事物，进行实验，迭代，并不要忘记评估您工作的结果。数据，如任务和预期输出，以及模拟器，这是LLMs与工具安全交互的一种方式，是构建真正复杂和有效的代理的关键。
- en: Now that we have created a mental map of various design patterns, we’ll look
    deeper into these principles by discussing various agentic architectures and looking
    at examples. We will start by enhancing the RAG architecture we discussed in [*Chapter
    4*](E_Chapter_4.xhtml#_idTextAnchor152) with an agentic approach.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个各种设计模式的思维导图，我们将通过讨论各种代理架构和查看示例来更深入地探讨这些原则。我们将首先通过代理方法增强我们在[*第4章*](E_Chapter_4.xhtml#_idTextAnchor152)中讨论的RAG架构。
- en: Agentic RAG
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理RAG
- en: LLMs enable the development of intelligent agents capable of tackling complex,
    non-repetitive tasks that defy description as deterministic workflows. By splitting
    reasoning into steps in different ways and orchestrating them in a relatively
    simple way, agents can demonstrate a significantly higher task completion rate
    on complex open tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs使开发能够处理复杂、非重复性任务，这些任务难以描述为确定性工作流程的智能代理成为可能。通过以不同的方式将推理分解为步骤并在相对简单的方式下编排它们，代理可以在复杂开放任务上展示出显著更高的任务完成率。
- en: This agent-based approach can be applied across numerous domains, including
    RAG systems, which we discussed in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152).
    As a reminder, what exactly is *agentic RAG*? Remember, a classic pattern for
    a RAG system is to retrieve chunks given the query, combine them into the context,
    and ask an LLM to generate an answer given a system prompt, combined context,
    and the question.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于代理的方法可以应用于众多领域，包括我们在[*第4章*](E_Chapter_4.xhtml#_idTextAnchor152)中讨论的RAG系统。作为提醒，*代理RAG*究竟是什么？记住，RAG系统的经典模式是根据查询检索块，将它们组合到上下文中，然后根据系统提示、组合上下文和问题让LLM生成答案。
- en: 'We can improve each of these steps using the principles discussed above (decomposition,
    tool calling, and adaptation):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用上面讨论的原则（分解、工具调用和适应）来改进这些步骤：
- en: '**Dynamic retrieval** hands over the retrieval query generation to the LLM.
    It can decide itself whether to use sparse embeddings, hybrid methods, keyword
    search, or web search. You can wrap retrievals as tools and orchestrate them as
    a LangGraph graph.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态检索**将检索查询生成的工作交给LLM。它可以自己决定是否使用稀疏嵌入、混合方法、关键字搜索或网络搜索。您可以将检索作为工具封装，并以LangGraph图的形式编排它们。'
- en: '**Query expansion** tasks an LLM to generate multiple queries based on initial
    ones, and then you combine search outputs based on reciprocal fusion or another
    technique.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询扩展**任务要求LLM根据初始查询生成多个查询，然后您根据互惠融合或其他技术结合搜索输出。'
- en: '**Decomposition of reasoning on retrieved chunks** allows you to ask an LLM
    to evaluate each individual chunk given the question (and filter it out if it’s
    irrelevant) to compensate for retrieval inaccuracies. Or you can ask an LLM to
    summarize each chunk by keeping only information given for the input question.
    Anyway, instead of throwing a huge piece of context in front of an LLM, you perform
    many smaller reasoning steps in parallel first.This can not only improve the RAG
    quality by itself but also increase the amount of initially retrieved chunks (by
    decreasing the relevance threshold) or expand each individual chunk with its neighbors.
    In other words, you can overcome some retrieval challenges with LLM reasoning.
    It might increase the overall performance of your application, but of course,
    it comes with latency and potential cost implications.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对检索块推理分解**允许你要求一个LLM评估给定问题（如果它不相关则过滤掉）的每个单独块，以补偿检索的不准确性。或者你可以要求LLM通过仅保留为输入问题提供的信息来总结每个块。无论如何，你首先并行执行许多较小的推理步骤，而不是将一大块上下文抛给LLM。这不仅可以通过自身提高RAG质量，还可以通过降低相关性阈值来增加最初检索到的块的数量，或者通过扩展每个单独的块及其邻居来扩展每个单独的块。换句话说，你可以通过LLM推理克服一些检索挑战。这可能会提高你应用程序的整体性能，但当然，这也伴随着延迟和潜在的成本影响。'
- en: '**Reflection steps and iterations** task LLMs to dynamically iterate on retrieval
    and query expansion by evaluating the outputs after each iteration. You can also
    use additional grounding and attribution tools as a separate step in your workflow
    and, based on that, reason whether you need to continue working on the answer
    or the answer can be returned to the user.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反思步骤和迭代**要求LLM在每次迭代后评估输出，以动态地对检索和查询扩展进行迭代。你还可以将额外的接地和归因工具作为工作流程中的单独步骤使用，并根据这些工具判断你是否需要继续工作在答案上，或者答案可以返回给用户。'
- en: Based on our definition from the previous chapters, RAG becomes agentic RAG
    when you have shared partial control with the LLM over the execution flow. For
    example, if the LLM decides how to retrieve, reflects on retrieved chunks, and
    adapts based on the first version of the answer, it becomes agentic RAG. From
    our perspective, at this point, it starts making sense to migrate to LangGraph
    since it’s designed specifically for building such applications, but of course,
    you can stay with LangChain or any other framework you prefer (compare how we
    implemented map-reduce video summarization with LangChain and LangGraph separately
    in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前几章的定义，当你与LLM共享对执行流程的部分控制时，RAG变为代理RAG。例如，如果LLM决定如何检索、反思检索到的块并根据答案的第一个版本进行调整，它就变成了代理RAG。从我们的角度来看，此时开始迁移到LangGraph是有意义的，因为它专门设计用于构建此类应用程序，但当然，你可以继续使用LangChain或任何你喜欢的其他框架（比较我们在[*第三章*](E_Chapter_3.xhtml#_idTextAnchor107)中分别使用LangChain和LangGraph实现map-reduce视频摘要的方式）。
- en: Multi-agent architectures
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多代理架构
- en: In [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231), we learned that decomposing
    a complex task into simpler subtasks typically increases LLM performance. We built
    a plan-and-solve agent that goes a step further than CoT and encourages the LLM
    to generate a plan and follow it. To a certain extent, this architecture was a
    multi-agent one since the research agent (which was responsible for generating
    and following the plan) invoked another agent that focused on a different type
    of task – solving very specific tasks with provided tools. Multi-agentic workflows
    orchestrate multiple agents, allowing them to enhance each other and at the same
    time keep agents modular (which makes it easier to test and reuse them).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中，我们了解到将复杂任务分解成更简单的子任务通常会增加LLM的性能。我们构建了一个计划并解决代理，它比CoT更进一步，鼓励LLM生成一个计划并遵循它。在某种程度上，这种架构是多代理的，因为研究代理（负责生成和遵循计划）调用了另一个专注于不同类型任务的代理——使用提供的工具解决非常具体的任务。多代理工作流程协调多个代理，使它们能够相互增强，同时保持代理模块化（这使得测试和重用它们更容易）。
- en: We will look into a few core agentic architectures in the remainder of this
    chapter, and introduce some important LangGraph interfaces (such as streaming
    details and handoffs) that are useful to develop agents. If you’re interested,
    you can find more examples and tutorials on the LangChain documentation page at
    [https://langchain-ai.github.io/langgraph/tutorials/#agent-architectures](https://langchain-ai.github.io/langgraph/tutorials/#agent-architectures).
    We’ll begin with discussing the importance of specialization in multi-agentic
    systems, including what the consensus mechanism is and the different consensus
    mechanisms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将探讨几个核心的代理架构，并介绍一些重要的LangGraph接口（如流细节和交接），这些接口对于开发代理非常有用。如果你感兴趣，你可以在LangChain文档页面上找到更多示例和教程，网址为[https://langchain-ai.github.io/langgraph/tutorials/#agent-architectures](https://langchain-ai.github.io/langgraph/tutorials/#agent-architectures)。我们将从讨论多代理系统中专业化的重要性开始，包括共识机制是什么以及不同的共识机制。
- en: Agent roles and specialization
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代理角色和专业化
- en: When working on a complex task, we as humans know that usually, it’s beneficial
    to have a team with diverse skills and backgrounds. There is much evidence from
    research and experiments that suggests this is also true for generative AI agents.
    In fact, developing specialized agents offers several advantages for complex AI
    systems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理复杂任务时，我们知道，通常，拥有一个技能和背景多样化的团队是有益的。研究和实验的大量证据表明，这一点也适用于生成式AI代理。事实上，开发专门的代理为复杂AI系统提供了几个优点。
- en: 'First, specialization improves performance on specific tasks. This allows you
    to:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，专业化提高了特定任务上的性能。这允许你：
- en: Select the optimal set of tools for each task type.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为每种任务类型选择最佳的工具集。
- en: Craft tailored prompts and workflows.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计定制的提示和工作流程。
- en: Fine-tune hyperparameters such as temperature for specific contexts.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调特定上下文中的超参数，例如温度。
- en: Second, specialized agents help manage complexity. Current LLMs struggle when
    handling too many tools at once. As a best practice, limit each agent to 5-15
    different tools, rather than overloading a single agent with all available tools.
    How to group tools is still an open question; typically, grouping them into toolkits
    to create coherent specialized agents helps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，专门的代理有助于管理复杂性。当前的LLM在同时处理太多工具时会有所挣扎。作为最佳实践，限制每个代理使用5-15种不同的工具，而不是将所有可用工具都加载到一个代理上。如何分组工具仍然是一个开放性问题；通常，将它们分组到工具集中以创建连贯的专门代理是有帮助的。
- en: '![Figure 6.2: A supervisor pattern](img/B32363_06_02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2：监督模式](img/B32363_06_02.png)'
- en: 'Figure 6.2: A supervisor pattern'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：监督模式
- en: Besides becoming *specialized*, keep your agents *modular*. It becomes easier
    to maintain and improve such agents. Also, by working on enterprise assistant
    use cases, you will eventually end up with many different agents available for
    users and developers within your organization that can be composed together. Hence,
    keep in mind that you should make such specialized agents configurable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 除了成为*专门化*的，还要保持你的代理*模块化*。这样，维护和改进这些代理会变得更容易。此外，通过处理企业助手用例，你最终会在你的组织内部拥有许多不同的代理，这些代理可供用户和开发者使用，并可以组合在一起。因此，请记住，你应该使这些专门化的代理可配置。
- en: 'LangGraph allows you to easily compose graphs by including them as a subgraph
    in a larger graph. There are two ways of doing this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph允许你通过将它们包含为较大图中的子图来轻松组合图。有两种方法可以做到这一点：
- en: 'Compile an agent as a graph and pass it as a callable when defining a node
    of another agent:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将代理作为图编译，并在定义另一个代理的节点时传递它作为可调用对象：
- en: '[PRE0]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Wrap the child agent’s invocation with a Python function and use it within
    the definition of the parent’s node:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python函数包装子代理的调用，并在父节点定义中使用它：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note, that your agents might have different schemas (since they perform different
    tasks). In the first case, the parent agent would pass the same keys in schemas
    with the child agent when invoking it. In turn, when the child agent finishes,
    it would update the parent’s state and send back the values corresponding to matching
    keys in both schemas. At the same time, the second option gives you full control
    over how you construct a state that is passed to the child agent, and how the
    state of the parent agent should be updated as a result. For more information,
    take a look at the documentation at [https://langchain-ai.github.io/langgraph/how-tos/subgraph/](https://langchain-ai.github.io/langgraph/how-tos/subgraph/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你的代理可能有不同的模式（因为它们执行不同的任务）。在第一种情况下，父代理在调用它时会在模式中传递给子代理相同的键。反过来，当子代理完成时，它会更新父代理的状态，并发送回匹配键的值。同时，第二种选项让你完全控制如何构建传递给子代理的状态，以及如何更新父代理的状态。有关更多信息，请参阅[https://langchain-ai.github.io/langgraph/how-tos/subgraph/](https://langchain-ai.github.io/langgraph/how-tos/subgraph/)上的文档。
- en: Consensus mechanism
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共识机制
- en: We can let multiple agents work on the same tasks in parallel as well. These
    agents might have a different “personality” (introduced by their system prompts;
    for example, some of them might be more curious and explorative, and others might
    be more strict and heavily grounded) or even varying architectures. Each of them
    independently works on getting a solution for the problem, and then you use a
    consensus mechanism to choose the best solution from a few drafts.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以让多个代理并行处理相同的任务。这些代理可能具有不同的“个性”（由它们的系统提示引入；例如，其中一些可能更好奇和探索，而另一些可能更严格和重实际）或甚至不同的架构。每个代理都会独立工作，为问题找到解决方案，然后你使用共识机制从几个草案中选择最佳解决方案。
- en: '![Figure 6.3: A parallel execution of the task with a final consensus step](img/B32363_06_03.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3：具有最终共识步骤的任务并行执行](img/B32363_06_03.png)'
- en: 'Figure 6.3: A parallel execution of the task with a final consensus step'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：具有最终共识步骤的任务并行执行
- en: 'We saw an example of implementing a consensus mechanism based on majority voting
    in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107). You can wrap it as a separate
    LangGraph node, and there are alternative ways of coming to a consensus across
    multiple agents:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第3章*](E_Chapter_3.xhtml#_idTextAnchor107)中看到了一个基于多数投票实现共识机制的例子。你可以将其封装为一个单独的LangGraph节点，并且有其他方式在多个代理之间达成共识：
- en: Let each agent see other solutions and score each of them on a scale of 0 to
    1, and then take the solution with the maximum score.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让每个代理看到其他解决方案，并按0到1的比例对每个解决方案进行评分，然后选择得分最高的解决方案。
- en: Use an alternative voting mechanism.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用替代投票机制。
- en: Use majority voting. It typically works for classification or similar tasks,
    but it might be difficult to implement majority voting if you have a free-text
    output. This is the fastest and the cheapest (in terms of token consumption) mechanism
    since you don’t need to run any additional prompts.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多数投票。它通常适用于分类或类似任务，但如果你有自由文本输出，实施多数投票可能很困难。这是最快且最便宜（在token消耗方面）的机制，因为你不需要运行任何额外的提示。
- en: Use an external oracle if it exists. For instance, when solving a mathematical
    equation, you can easily verify if the solution is feasible. Computational costs
    depend on the problem but typically are low.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在，请使用外部预言机。例如，在解决数学方程时，你可以轻松验证解决方案是否可行。计算成本取决于问题，但通常较低。
- en: Use another (maybe more powerful) LLM as a judge to pick the best solution.
    You can ask an LLM to come up with a score for each solution, or you can task
    it with a multi-class classification problem by presenting all of them and asking
    it to pick the best one.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用另一个（可能更强大）的LLM作为裁判来挑选最佳解决方案。你可以要求LLM为每个解决方案给出一个分数，或者你可以通过展示所有解决方案并要求它选择最佳方案来将其分配给一个多类分类问题。
- en: Develop another agent that excels at the task of selecting the best solution
    for a general task from a set of solutions.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发另一个擅长从一组解决方案中选择最佳解决方案的通用任务的代理。
- en: It’s worth mentioning that a consensus mechanism has certain latency and cost
    implications, but typically they’re negligible relative to the costs of solving
    a task itself. If you task N agents with the same task, your token consumption
    increases N times, and the consensus mechanism adds a relatively small overhead
    on top of that difference.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，共识机制有一定的延迟和成本影响，但通常与解决任务本身的成本相比可以忽略不计。如果您将相同的任务分配给N个代理，您的令牌消耗量会增加N倍，而共识机制在上述差异之上增加了一个相对较小的开销。
- en: 'You can also implement your own consensus mechanism. When you do this, consider
    the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以实现自己的共识机制。当您这样做时，请考虑以下因素：
- en: Use few-shot prompting when using an LLM as a judge.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用LLM作为评判者时，使用少量样本提示。
- en: Add examples demonstrating how to score different input-output pairs.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加示例，展示如何评分不同的输入-输出对。
- en: Consider including scoring rubrics for different types of responses.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑包含针对不同类型响应的评分标准。
- en: Test the mechanism on diverse outputs to ensure consistency.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不同的输出上测试该机制以确保一致性。
- en: One important note on parallelization – when you let LangGraph execute nodes
    in parallel, updates are applied to the main state in the same order as you’ve
    added nodes to your graph.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 关于并行化的重要注意事项——当您让LangGraph并行执行节点时，更新将按照您向图中添加节点的顺序应用到主状态。
- en: Communication protocols
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通信协议
- en: The third architecture option is to let agents communicate and work collaboratively
    on a task. For example, the agents might benefit from various personalities configured
    through system prompts. Decomposition of a complex task into smaller subtasks
    also helps you retain control over your application and how your agents communicate.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种架构选项是让代理在任务上相互通信并协作。例如，代理可能从通过系统提示配置的各种个性中受益。将复杂任务分解成更小的子任务也有助于您保持对应用程序和代理通信的控制。
- en: '![Figure 6.4: Reflection pattern](img/B32363_06_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4：反射模式](img/B32363_06_04.png)'
- en: 'Figure 6.4: Reflection pattern'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：反射模式
- en: Agents can work collaboratively on a task by providing critique and reflection.
    There are multiple reflection patterns starting from self-reflection, when the
    agent analyzes its own steps and identifies areas for improvements (but as mentioned
    above, you might initiate the reflecting agent with a slightly different system
    prompt); cross-reflection, when you use another agent (for example, using another
    foundational model); or even reflection, which includes **Human-in-the-Loop**
    (**HIL**) on critical checkpoints (we’ll see in the next section how to build
    adaptive systems of this kind).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可以通过提供批评和反思来在任务上协作工作。从自我反思开始，代理分析自己的步骤并确定改进领域（但如上所述，您可能需要使用略有不同的系统提示来启动反思代理）；交叉反思，当您使用另一个代理（例如，使用另一个基础模型）时；甚至反思，这包括**人机交互**（**HIL**）在关键检查点上（我们将在下一节中看到如何构建此类自适应系统）。
- en: You can keep one agent as a supervisor, allow agents to communicate in a network
    (allowing them to decide which agent to send a message or a task), introduce a
    certain hierarchy, or develop more complex flows (for inspiration, take a look
    at some diagrams on the LangGraph documentation page at [https://langchain-ai.github.io/langgraph/concepts/multi_agent/](https://langchain-ai.github.io/langgraph/concepts/multi_agent/)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以保留一个代理作为监督者，允许代理在网络中进行通信（允许它们决定向哪个代理发送消息或任务），引入一定的层次结构，或开发更复杂的工作流程（为了获得灵感，请查看LangGraph文档页面上的某些图表[https://langchain-ai.github.io/langgraph/concepts/multi_agent/](https://langchain-ai.github.io/langgraph/concepts/multi_agent/)）。
- en: 'Designing multi-agent workflows is still an open area of research and experimentation,
    and you need to answer a lot of questions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 设计多代理工作流程仍然是研究和实验的开放领域，您需要回答很多问题：
- en: What and how many agents should we include in our system?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该在系统中包含哪些代理以及多少个？
- en: What roles should we assign to these agents?
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该将这些代理分配哪些角色？
- en: What tools should each agent have access to?
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个代理应该有权访问哪些工具？
- en: How should agents interact with each other and through which mechanism?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理应该如何相互交互以及通过哪种机制？
- en: What specific parts of the workflow should we automate?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该自动化工作流程的哪些具体部分？
- en: How do we evaluate our automation and how can we collect data for this evaluation?
    Additionally, what are our success criteria?
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何评估我们的自动化，以及我们如何收集用于此评估的数据？此外，我们的成功标准是什么？
- en: 'Now that we’ve examined some core considerations and open questions around
    multi-agent communication, let’s explore two practical mechanisms to structure
    and facilitate agent interactions: *semantic routing*, which directs tasks intelligently
    based on their content, and *organizing interaction*, detailing the specific formats
    and structures that agents can use to effectively exchange information.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了关于多代理通信的一些核心考虑因素和开放性问题，让我们来探讨两种实用的机制来结构和促进代理交互：*语义路由*，根据任务的内容智能地指导任务，以及*组织交互*，详细说明了代理可以用来有效交换信息的特定格式和结构。
- en: Semantic router
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义路由器
- en: Among many different ways to organize communication between agents in a true
    multi-agent setup, an important one is a semantic router. Imagine developing an
    enterprise assistant. Typically it becomes more and more complex because it starts
    dealing with various types of questions – general questions (requiring public
    data and general knowledge), questions about the company (requiring access to
    the proprietary company-wide data sources), and questions specific to the user
    (requiring access to the data provided by the user itself). Maintaining such an
    application as a single agent becomes very difficult very soon. Again, we can
    apply our design patterns – decomposition and collaboration!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在真正的多代理设置中，组织代理之间通信的许多不同方式中，一个重要的是语义路由器。想象一下开发一个企业助手。通常它变得越来越复杂，因为它开始处理各种类型的问题——通用问题（需要公共数据和一般知识）、关于公司的问题（需要访问专有的公司数据源），以及特定于用户的问题（需要访问用户本身提供的数据）。很快，将这样的应用程序作为一个单一代理来维护就变得非常困难。再次强调，我们可以应用我们的设计模式——分解和协作！
- en: Imagine we have implemented three types of agents – one answering general questions
    grounded on public data, another one grounded on a company-wide dataset and knowing
    about company specifics, and the third one specialized on working with a small
    source of user-provided documents. Such specialization helps us to use patterns
    such as few-shot prompting and controlled generation. Now we can add a semantic
    router – the first layer that asks an LLM to classify the question and routes
    it to the corresponding agent based on classification results. Each agent (or
    some of them) might even use a self-consistency approach, as we learned in [*Chapter
    3*](E_Chapter_3.xhtml#_idTextAnchor107), to increase the LLM classification accuracy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经实现了三种类型的代理——一种基于公共数据的通用问题回答者，另一种基于公司范围内的数据集并了解公司具体情况，第三种专注于处理用户提供的少量文档。这种专业化有助于我们使用诸如少样本提示和控制生成等模式。现在我们可以添加一个语义路由器——第一个要求LLM对问题进行分类，并根据分类结果将其路由到相应的代理。每个代理（或其中一些）甚至可能使用我们学到的自我一致性方法，如[第3章](E_Chapter_3.xhtml#_idTextAnchor107)中所述，以提高LLM分类的准确性。
- en: '![Figure 6.5: Semantic router pattern](img/B32363_06_05.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5：语义路由器模式](img/B32363_06_05.png)'
- en: 'Figure 6.5: Semantic router pattern'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：语义路由器模式
- en: It’s worth mentioning that a task might fall into two or more categories – for
    example, I can ask, “*What is X and how can I do Y?* “ This might not be such
    a common use case in an assistant setting, and you can decide what to do in that
    case. First of all, you might just educate the user by replying with an explanation
    that they should task your application with a single problem per turn. Sometimes
    developers tend to be too focused on trying to solve everything programmatically.
    But some product features are relatively easy to solve via the UI, and users (especially
    in the enterprise setup) are ready to provide their input. Maybe, instead of solving
    a classification problem on the prompt, just add a simple checkbox in the UI,
    or let the system double-check if the level of confidence is low.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，一个任务可能属于两个或更多类别——例如，我可以问，“*X是什么，我该如何做Y？*”这种情况在助手环境中可能并不常见，你可以决定在这种情况下如何处理。首先，你可能只是通过回复一个解释来教育用户，告诉他们每次只让应用程序处理一个问题。有时开发者可能会过于专注于尝试通过编程来解决所有问题。但是，一些产品特性相对容易通过用户界面来解决，并且用户（尤其是在企业环境中）愿意提供他们的输入。也许，与其在提示中解决分类问题，不如在用户界面中添加一个简单的复选框，或者让系统在置信度低时进行双重检查。
- en: You can also use tool calling or other controlled generation techniques we’ve
    learned about to extract both goals and route the execution to two specialized
    agents with different tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用工具调用或其他我们了解的控制生成技术来提取目标和将执行路由到两个具有不同任务的专用代理。
- en: Another important aspect of semantic routing is that the performance of your
    application depends a lot on classification accuracy. You can use all the techniques
    we have discussed in the book to improve it – few-shot prompting (including dynamic
    one), incorporating user feedback, sampling, and others.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 语义路由的另一个重要方面是，您应用程序的性能在很大程度上取决于分类的准确性。您可以使用我们在书中讨论的所有技术来提高它——包括动态的少样本提示（few-shot
    prompting），结合用户反馈，采样等。
- en: Organizing interactions
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组织交互
- en: 'There are two ways to organize communication in multi-agent systems:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在多代理系统中组织通信有两种方式：
- en: Agents communicate via specific structures that force them to put their thoughts
    and reasoning traces in a specific form, as we saw in the *plan-and-solve* example
    in the previous chapter. We saw how our planning node communicated with the ReACT
    agent via a Pydantic model with a well-structured plan (which, in turn, was a
    result of an LLM’s controlled generation).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理通过特定的结构进行通信，这些结构迫使他们将思想和推理痕迹以特定形式表达出来，正如我们在上一章的*计划-解决*示例中所看到的。我们看到了我们的规划节点是如何通过一个结构良好的计划（这反过来又是LLM受控生成的结果）的Pydantic模型与ReACT代理进行通信的。
- en: On the other hand, LLMs were trained to take natural language as input and produce
    an output in the same format. Hence, it’s a very natural way for them to communicate
    via messages, and you can implement a communication mechanism by applying messages
    from different agents to the shared list of messages!.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，LLMs被训练以接受自然语言作为输入，并以相同格式产生输出。因此，通过消息进行通信对他们来说是非常自然的方式，您可以通过将不同代理的消息应用于共享的消息列表来实现通信机制！
- en: When communicating with messages, you can share all messages via a so-called
    *scratchpad* – a shared list of messages. In that case, your context can grow
    relatively quickly and you might need to use some of the mechanisms to trim the
    chat memory (like preparing running summaries) that we discussed in [*Chapter
    3*](E_Chapter_3.xhtml#_idTextAnchor107). But as general advice, if you need to
    filter or prioritize messages in the history of communication between multiple
    agents, go with the first approach and let them communicate through a controlled
    output. It would give you more control of the state of your workflow at any given
    point in time. Also, you might end up with a situation where you have a complicated
    sequence of messages, for example, *[SystemMessage, HumanMessage, AIMessage, ToolMessage,
    AIMessage, AIMessage, SystemMessage, …]*. Depending on the foundational model
    you’re using, double-check that the model’s provider supports such sequences,
    since previously, many providers supported only relatively simple sequences –
    SystemMessages followed by alternating HumanMessage and AIMessage (maybe with
    a ToolMessage instead of a human one if a tool invocation was decided).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用消息进行通信时，您可以通过所谓的*草稿板*——一个共享的消息列表来共享所有消息。在这种情况下，您的上下文可能会相对快速地增长，您可能需要使用我们在[*第三章*](E_Chapter_3.xhtml#_idTextAnchor107)中讨论的一些机制来修剪聊天内存（如准备运行摘要）。但作为一般建议，如果您需要在多个代理之间的通信历史中过滤或优先处理消息，请采用第一种方法，让他们通过受控输出进行通信。这将使您在任何给定时间点对工作流程的状态有更多的控制。此外，您可能会遇到一个复杂的消息序列，例如，*[系统消息，人类消息，AI消息，工具消息，AI消息，AI消息，系统消息，…]*。根据您使用的底层模型，请务必检查模型提供商是否支持此类序列，因为之前，许多提供商只支持相对简单的序列——系统消息后跟交替的人类消息和AI消息（如果决定调用工具，则可能用工具消息代替人类消息）。
- en: Another alternative is to share only the final results of each execution. This
    keeps the list of messages relatively short.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是只共享每个执行的最终结果。这使消息列表相对较短。
- en: 'Now it’s time to look at a practical example. Let’s develop a research agent
    that uses tools to answer complex multiple-choice questions based on the public
    MMLU dataset (we’ll use high school geography questions). First, we need to grab
    a dataset from Hugging Face:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看一个实际例子了。让我们开发一个研究代理，该代理使用工具来回答基于公共MMLU数据集（我们将使用高中地理问题）的复杂多项选择题。首先，我们需要从Hugging
    Face获取一个数据集：
- en: '[PRE2]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These are our answer options:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们的答案选项：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s start with a ReACT agent, but let’s deviate from a default system prompt
    and write our own prompt. Let’s focus this agent on being creative and working
    on an evidence-based solution (please note that we used elements of CoT prompting,
    which we discussed in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107)):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从ReACT代理开始，但让我们偏离默认的系统提示，并编写我们自己的提示。让我们将这个代理专注于创造性和基于证据的解决方案（请注意，我们使用了CoT提示的元素，我们在[*第三章*](E_Chapter_3.xhtml#_idTextAnchor107)中讨论过）：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, let’s create the agent itself. Since we have a custom prompt for the agent,
    we need a prompt template that includes a system message, a template that formats
    the first user message based on a question and answers provided, and a placeholder
    for further messages to be added to the graph’s state. We also redefine the default
    agent’s state by inheriting from `AgentState` and adding additional keys to it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建代理本身。由于我们为代理提供了一个自定义提示，我们需要一个包含系统消息、格式化第一个用户消息的模板（基于提供的问题和答案）以及用于添加到图状态的进一步消息占位符的提示模板。我们还通过从`AgentState`继承并添加额外的键来重新定义默认代理的状态：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We could have stopped here, but let’s go further. We used a specialized research
    agent based on the ReACT pattern (and we slightly adjusted its default configuration).
    Now let’s add a reflection step to it, and use another role profile for an agent
    who will actionably criticize our “student’s” work:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以在这里停止，但让我们更进一步。我们使用了一个基于ReACT模式的专业研究代理（并对其默认配置进行了轻微调整）。现在让我们向其中添加一个反思步骤，并为将实际批评我们“学生”工作的代理使用另一个角色配置文件：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we need another research agent that takes not only question and answer options
    but also the previous answer and the feedback. The research agent is tasked with
    using tools to improve the answer and address the critique. We created a simplistic
    and illustrative example. You can always improve it by adding error handling,
    Pydantic validation (for example, checking that either an answer or critique is
    provided), or handling conflicting or ambiguous feedback (for example, structure
    prompts that help the agent prioritize feedback points when there are multiple
    criticisms).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要另一个研究代理，它不仅接受问题和答案选项，还包括之前的答案和反馈。研究代理的任务是使用工具来改进答案并回应批评。我们创建了一个简单且具有说明性的例子。你可以通过添加错误处理、Pydantic验证（例如，检查是否提供了答案或批评）或处理冲突或模糊的反馈（例如，结构化提示以帮助代理在存在多个批评时优先考虑反馈点）来不断改进它。
- en: 'Note that we use a less capable LLM for our ReACT agents, just to demonstrate
    the power of the reflection approach (otherwise the graph might finish in a single
    iteration since the agent would figure out the correct answer with the first attempt):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们为我们的ReACT代理使用了一个能力较弱的LLM，只是为了展示反思方法的力量（否则图可能在单次迭代中就完成了，因为代理可能会在第一次尝试中就找到正确答案）：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When defining the state of our graph, we need to keep track of the question
    and answer options, the current answer, and the critique. Also note that we track
    the amount of interaction between a *student* and a *professor* (to avoid infinite
    cycles between them) and we use a custom reducer for that (which summarizes old
    steps and new steps on each run). Let’s define the full state, nodes, and conditional
    edges:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义我们图的状态时，我们需要跟踪问题和答案选项、当前答案和批评。此外，请注意，我们跟踪学生和教授之间的交互次数（以避免他们之间的无限循环）并为此使用自定义的reducer（它总结每次运行中的旧步骤和新步骤）。让我们定义完整的状态、节点和条件边：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s put it all together and create our graph:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有这些都放在一起，创建我们的图：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Figure 6.6: A research agent with reflection](img/B32363_06_06.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6：具有反思功能的研究代理](img/B32363_06_06.png)'
- en: 'Figure 6.6: A research agent with reflection'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：具有反思功能的研究代理
- en: 'Let’s run it and inspect what’s happening:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行它并检查发生了什么：
- en: '[PRE12]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have omitted the full output here (you’re welcome to take the code from
    our GitHub repository and experiment with it yourself), but the first answer was
    wrong:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里省略了完整的输出（欢迎您从我们的GitHub仓库中获取代码并自行实验），但第一个答案是错误的：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After five iterations, the weaker LLM was able to figure out the correct answer
    (keep in mind that the “professor” only evaluated the reasoning itself and it
    didn’t use external tools or its own knowledge). Note that, technically speaking,
    we implemented cross-reflection and not self-reflection (since we’ve used a different
    LLM for reflection than the one we used for the reasoning). Here’s an example
    of the feedback provided during the first round:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 经过五次迭代后，能力较弱的LLM能够找到正确答案（请记住，“教授”只评估推理本身，并没有使用外部工具或自己的知识）。请注意，从技术角度讲，我们实现了交叉反思而不是自我反思（因为我们用于反思的LLM与用于推理的不同）。以下是第一轮提供的反馈示例：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Next, let’s discuss an alternative communication style for a multi-agent setup,
    via a shared list of messages. But before that, we should discuss the LangGraph
    handoff mechanism and dive into some details of streaming with LangGraph.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论一种适用于多代理设置的替代通信风格，即通过共享消息列表。但在那之前，我们应该讨论 LangGraph 交接机制，并深入了解 LangGraph
    的流式传输细节。
- en: LangGraph streaming
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangGraph 流式传输
- en: LangGraph streaming might sometimes be a source of confusion. Each graph has
    not only a `stream` and a corresponding asynchronous `astream` method, but also
    an `astream_events`. Let’s dive into the difference.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph 流式传输有时可能引起混淆。每个图不仅有一个 `stream` 和相应的异步 `astream` 方法，还有一个 `astream_events`。让我们深入了解它们之间的区别。
- en: The `Stream` method allows you to stream changes to the graph’s state after
    each super-step. Remember, we discussed what a super-step is in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107),
    but to keep it short, it’s a single iteration over the graph where parallel nodes
    belong to a single super-step while sequential nodes belong to different super-steps.
    If you need actual streaming behavior (like in a chatbot, so that users feel like
    something is happening and the model is actually thinking), you should use `astream`
    with `messages` mode.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`Stream` 方法允许你在每个超级步骤之后流式传输图状态的更改。记住，我们在 [*第 3 章*](E_Chapter_3.xhtml#_idTextAnchor107)
    中讨论了超级步骤是什么，但为了简洁起见，它是对图的单次迭代，其中并行节点属于单个超级步骤，而顺序节点属于不同的超级步骤。如果你需要实际的流式传输行为（例如在聊天机器人中，以便用户感觉有事情发生，模型实际上在思考），你应该使用
    `astream` 与 `messages` 模式。'
- en: 'You have five modes with `stream/astream` methods (of course, you can combine
    multiple modes):'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你有五种 `stream/astream` 方法模式（当然，你可以组合多个模式）：
- en: '| **Mode** | **Description** | **Output** |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| **Mode** | **描述** | **输出** |'
- en: '| updates | Streams only updates to the graph produced by the node | A dictionary
    where each node name maps to its corresponding state update) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| updates | 仅流式传输由节点产生的图更新 | 一个字典，其中每个节点名称映射到其对应的状态更新 |'
- en: '| values | Streams the full state of the graph after each super-step | A dictionary
    with the entire graph’s state |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| values | 在每个超级步骤之后流式传输图的完整状态 | 包含整个图状态的字典 |'
- en: '| debug | Attempts to stream as much information as possible in the debug mode
    | A dictionary with a timestamp, task_type, and all the corresponding information
    for every event |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| debug | 在调试模式下尝试尽可能多地流式传输信息 | 包含时间戳、任务类型以及每个事件所有对应信息的字典 |'
- en: '| custom | Streams events emitted by the node using a StreamWriter | A dictionary
    that was written from the node to a custom writer |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| custom | 使用 StreamWriter 流式传输节点发出的事件 | 从节点写入到自定义编写器的字典 |'
- en: '| messages | Streams full events (for example, ToolMessages) or its chunks
    in a streaming node if possible (e.g., AI Messages) | A tuple with token or message
    segment and a dictionary containing metadata from the node |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| messages | 如果可能的话，在流式节点中流式传输完整事件（例如，ToolMessages）或其块（例如，AI Messages） | 包含标记或消息段以及包含节点元数据的字典
    |'
- en: 'Table 6.1: Different streaming modes for LangGraph'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1：LangGraph 的不同流式传输模式
- en: 'Let’s look at an example. If we take the ReACT agent we used in the section
    above and stream with the `values` mode, we’ll get the full state returned after
    every super-step (you can see that the total number of messages is always increasing):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。如果我们使用上面章节中使用的 ReACT 代理并以 `values` 模式进行流式传输，我们将得到每个超级步骤之后返回的完整状态（你可以看到消息总数总是在增加）：
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we switch to the `update` mode, we’ll get a dictionary where the key is
    the node’s name (remember that parallel nodes can be called within a single super-step)
    and a corresponding update to the state sent by this node:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们切换到 `update` 模式，我们将得到一个字典，其键是节点的名称（记住，并行节点可以在单个超级步骤内被调用）以及由该节点发送的相应状态更新：
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: LangGraph `stream` always emits a tuple where the first value is a stream mode
    (since you can pass multiple modes by adding them to the list).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph 的 `stream` 总是输出一个元组，其中第一个值是流模式（因为你可以通过将它们添加到列表中来传递多个模式）。
- en: 'Then you need an `astream_events` method that streams back events happening
    within the nodes – not just tokens generated by the LLM but any event available
    for a callback:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你需要一个 `astream_events` 方法，该方法流式传输节点内发生的事件——不仅仅是 LLM 生成的标记，而是任何可用于回调的事件：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: "You can find a full list of the events at [https://python.langchain.com/docs/concepts/callbacks/#callback-events](https://python.langchain.com/docs/concepts/callbacks/#callback-even\uFEFF\
    ts)."
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: "你可以在 [https://python.langchain.com/docs/concepts/callbacks/#callback-events](https://python.langchain.com/docs/concepts/callbacks/#callback-even\uFEFF\
    ts) 找到事件的完整列表。"
- en: Handoffs
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 交接
- en: So far, we have learned that a node in LangGraph does a chunk of work and sends
    updates to a common state, and an edge controls the flow – it decides which node
    to invoke next (in a deterministic manner or based on the current state). When
    implementing multi-agent architectures, your nodes can be not only functions but
    other agents, or subgraphs (with their own state). You might need to combine state
    updates and flow controls.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解到 LangGraph 中的节点执行一部分工作并向公共状态发送更新，而边控制着流程——它决定下一个要调用的节点（以确定性的方式或基于当前状态）。在实现多代理架构时，您的节点不仅可以是函数，还可以是其他代理或子图（具有它们自己的状态）。您可能需要结合状态更新和流程控制。
- en: 'LangGraph allows you to do that with a `Command` – you can update your graph’s
    state and at the same time invoke another agent by passing a custom state to it.
    This is called a *handoff* – since an agent hands off control to another one.
    You need to pass an `update` – a dictionary with an update of the current state
    to be sent to your graph – and `goto` – a name (or list of names) of the nodes
    to hand off control to:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph 允许您使用 `命令` 来实现这一点——您可以通过传递一个自定义状态来更新您图的状态，并同时调用另一个代理。这被称为 *移交* ——因为一个代理将控制权移交给另一个代理。您需要传递一个
    `update` ——一个包含当前状态更新的字典，以便发送到您的图——以及 `goto` ——要移交控制权的节点名称（或名称列表）：
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A destination agent can be a node from the current or a parent (`Command.PARENT`)
    graph. In other words, you can change the control flow only within the current
    graph, or you can pass it back to the workflow that initiated this one (for example,
    you can’t pass control to any random workflow by ID). You can also invoke a `Command`
    from a tool, or wrap a `Command` as a tool, and then an LLM can decide to hand
    off control to a specific agent. In [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107),
    we discussed the map-reduce pattern and the `Send` class, which allowed us to
    invoke a node in the graph by passing a specific input state to it. We can use
    `Command` together with `Send` (in this example, the destination agent belongs
    to the parent graph):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 目标代理可以是当前图或父图（`Command.PARENT`）中的一个节点。换句话说，您只能在当前图中更改控制流，或者将其传递回启动此图的流程（例如，您不能通过
    ID 将控制权传递给任何随机的流程）。您还可以从工具中调用 `命令`，或将 `命令` 包装为工具，然后 LLM 可以决定将控制权移交给特定的代理。在 [*第
    3 章*](E_Chapter_3.xhtml#_idTextAnchor107) 中，我们讨论了 map-reduce 模式和 `Send` 类，它们允许我们通过传递特定的输入状态来调用图中的节点。我们可以将
    `Command` 与 `Send` 一起使用（在这个例子中，目标代理属于父图）：
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Communication via a shared messages list
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过共享消息列表进行通信
- en: 'A few chapters earlier, we discussed how two agents can communicate via controlled
    output (by sending each other special Pydantic instances). Now let’s go back to
    the communication topic and illustrate how agents can communicate with native
    LangChain messages. Let’s take the research agent with a cross-reflection and
    make it work with a shared list of messages. First, the research agent itself
    looks simpler – it has a default state since it gets a user’s question as a HumanMessage:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在几章之前，我们讨论了两个代理如何通过受控输出（通过发送特殊的 Pydantic 实例给对方）进行通信。现在让我们回到通信主题，并说明代理如何使用本地的
    LangChain 消息进行通信。让我们以具有交叉反射的研究代理为例，使其与共享的消息列表一起工作。首先，研究代理本身看起来更简单——它有一个默认状态，因为它接收一个用户的问题作为
    HumanMessage：
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We also need to slightly modify the reflection prompt:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要稍微修改一下反思提示：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The nodes themselves also look simpler, but we add `Command` after the reflection
    node since we decide what to call next with the node itself. Also, we don’t wrap
    a ReACT research agent as a node anymore:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 节点本身看起来更简单，但我们会在反思节点后添加 `Command`，因为我们决定使用节点本身来调用什么。此外，我们不再将 ReACT 研究代理作为节点进行包装：
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The graph itself also looks very simple:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图本身看起来也非常简单：
- en: '[PRE23]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If we run it, we will see that at every stage, the graph operates on the same
    (and growing) list of messages.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行它，我们会看到在每一个阶段，图都在操作同一个（并且不断增长的）消息列表。
- en: LangGraph platform
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LangGraph 平台
- en: LangGraph and LangChain, as you know, are open-source frameworks, but LangChain
    as a company offers the LangGraph platform – a commercial solution that helps
    you develop, manage, and deploy agentic applications. One component of the LangGraph
    platform is LangGraph Studio – an IDE that helps you visualize and debug your
    agents – and another is LangGraph Server.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，LangGraph 和 LangChain 是开源框架，但 LangChain 作为一家公司提供了 LangGraph 平台——一个商业解决方案，可以帮助您开发、管理和部署代理应用程序。LangGraph
    平台的一个组件是 LangGraph Studio ——一个 IDE，可以帮助您可视化并调试您的代理——另一个是 LangGraph Server。
- en: You can read more about the LangGraph platform at the official website ([https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform](https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform)),
    but let’s discuss a few key concepts for a better understanding of what it means
    to develop an agent.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在官方网站（[https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform](https://langchain-ai.github.io/langgraph/concepts/#langgraph-platform)）上了解更多关于LangGraph平台的信息，但让我们讨论几个关键概念，以便更好地理解开发代理的含义。
- en: After you’ve developed an agent, you can wrap it as an HTTP API (using Flask,
    FastAPI, or any other web framework). The LangGraph platform offers you a native
    way to deploy agents, and it wraps them with a unified API (which makes it easier
    for your applications to use these agents). When you’ve built your agent as a
    LangGraph graph object, you deploy an *assistant* – a specific deployment that
    includes an instance of your graph coupled together with a configuration. You
    can easily version and configure assistants in the UI, but it’s important to keep
    parameters configurable (and pass them as `RunnableConfig` to your nodes and tools).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开发代理之后，你可以将其包装成HTTP API（使用Flask、FastAPI或其他任何Web框架）。LangGraph平台为你提供了一种原生的方式来部署代理，并且它将它们包装在一个统一的API中（这使得你的应用程序使用这些代理变得更容易）。当你将你的代理构建为LangGraph图对象时，你部署的是一个*助手*——一个特定的部署，包括你的图实例与配置一起。你可以在UI中轻松地版本化和配置助手，但保持参数可配置（并将它们作为`RunnableConfig`传递给你的节点和工具）是很重要的。
- en: Another important concept is a *thread*. Don’t be confused, a LangGraph thread
    is a different concept from a Python thread (and when you pass a `thread_id` in
    your `RunnableConfig`, you’re passing a LangGraph thread ID). When you think about
    LangGraph threads, think about conversation or Reddit threads. A thread represents
    a session between your assistant (a graph with a specific configuration) and a
    user. You can add per-thread persistence using the checkpointing mechanism we
    discussed in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的概念是*线程*。不要混淆，LangGraph线程与Python线程是不同的概念（并且当你你在`RunnableConfig`中传递`thread_id`时，你传递的是LangGraph线程ID）。当你思考LangGraph线程时，想想对话或Reddit线程。线程代表你的助手（具有特定配置的图）与用户之间的会话。你可以使用我们在[*第三章*](E_Chapter_3.xhtml#_idTextAnchor107)中讨论的检查点机制来为每个线程添加持久化。
- en: A *run* is an invocation of an assistant. In most cases, runs are executed on
    a thread (for persistence). LangGraph Server also allows you to schedule stateless
    runs – they are not assigned to any thread, and because of that, the history of
    interactions is not persisted. LangGraph Server allows you to schedule long-running
    runs, scheduled runs (a.k.a. crons), etc., and it also offers a rich mechanism
    for webhooks attached to runs and polling results back to the user.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*运行*是对助手的调用。在大多数情况下，运行是在一个线程上执行的（用于持久化）。LangGraph服务器还允许你安排无状态的运行——它们不会被分配到任何线程，因此交互历史不会被持久化。LangGraph服务器允许你安排长时间运行的运行、计划中的运行（也称为cron）等，并且它还提供了一套丰富的机制来处理运行附加的webhooks和将结果轮询回用户。'
- en: We’re not going to discuss the LangGraph Server API in this book. Please take
    a look at the documentation instead.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在本书中讨论LangGraph服务器API。请查看文档。
- en: Building adaptive systems
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自适应系统
- en: 'Adaptability is a great attribute of agents. They should adapt to external
    and user feedback and correct their actions accordingly. As we discussed in [*Chapter
    5*](E_Chapter_5.xhtml#_idTextAnchor231), generative AI agents are adaptive through:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 适应性是代理的一个优秀属性。它们应该适应外部和用户反馈，并据此纠正其行为。正如我们在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中讨论的那样，生成式AI代理通过以下方式实现适应性：
- en: '**Tool interaction**: They incorporate feedback from previous tool calls and
    their outputs (by including `ToolMessages` that represent tool-calling results)
    when planning the next steps (like our ReACT agent adjusting based on search results).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具交互**：它们在规划下一步（如我们的ReACT代理根据搜索结果进行调整）时，会整合来自先前工具调用及其输出的反馈（通过包含表示工具调用结果的`ToolMessages`）。'
- en: '**Explicit reflection**: They can be instructed to analyze current results
    and deliberately adjust their behavior.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**显式反思**：它们可以被指示分析当前结果并故意调整其行为。'
- en: '**Human feedback**: They can incorporate user input at critical decision points.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人类反馈**：它们可以在关键决策点整合用户输入。'
- en: Dynamic behavior adjustment
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态行为调整
- en: We saw how to add a reflection step to our plan-and-solve agent. Given the initial
    plan, and the output of the steps performed so far, we’ll ask the LLM to reflect
    on the plan and adjust it. Again, we continue reiterating the key idea – such
    reflection might not happen naturally; you might add it as a separate task (decomposition),
    and you keep partial control over the execution flow by designing its generic
    components.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何向我们的计划-解决代理添加一个反思步骤。给定初始计划，以及到目前为止执行步骤的输出，我们将要求 LLM 反思计划并调整它。再次强调，我们继续重复关键思想——这种反思可能不会自然发生；您可能将其作为单独的任务（分解）添加，并通过设计其通用组件来保持对执行流程的部分控制。
- en: Human-in-the-loop
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类在回路中
- en: Additionally, when developing agents with complex reasoning trajectories, it
    might be beneficial to incorporate human feedback at a certain point. An agent
    can ask a human to approve or reject certain actions (for example, when it’s invoking
    a tool that is irreversible, like a tool that makes a payment), provide additional
    context to the agent, or give a specific input by modifying the graph’s state.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在开发具有复杂推理轨迹的代理时，在某个点上引入人类反馈可能是有益的。代理可以要求人类批准或拒绝某些操作（例如，当它调用不可逆的工具时，如支付工具），向代理提供额外的上下文，或者通过修改图的状态来给出特定的输入。
- en: Imagine we’re developing an agent that searches for job postings, generates
    an application, and sends this application. We might want to ask the user before
    submitting an application, or the logic might be more complex – the agent might
    be collecting data about the user, and for some job postings, it might be missing
    relevant context about past job experience. It should ask the user and persist
    this knowledge in long-term memory for better long-term adaptation.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们正在开发一个代理，该代理搜索工作职位，生成申请，并发送这个申请。我们可能在提交申请之前要求用户，或者逻辑可能更复杂——代理可能正在收集有关用户的数据，并且对于某些工作职位，它可能缺少关于过去工作经验的相关上下文。它应该询问用户，并将此知识持久化到长期记忆中，以实现更好的长期适应。
- en: LangGraph has a special `interrupt` function to implement **HIL**-type interactions.
    You should include this function in the node, and by the first execution, it would
    throw a `GraphInterrupt` exception (the value of which would be presented to the
    user). To resume the execution of the graph, a client should use the `Command`
    class, which we discussed earlier in this chapter. LangGraph would start from
    the same node, re-execute it, and return corresponding values as a result of the
    node invoking the `interrupt` function (if there are multiple `interrupts` in
    your node, LangGraph would keep an ordering). You can also use `Command` to route
    to different nodes based on the user’s input. Of course, you can use `interrupt`
    only when a checkpointer is provided to the graph since its state should be persisted.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph 具有特殊的 `interrupt` 函数以实现 **HIL**-type 交互。您应该在节点中包含此函数，并在第一次执行时，它会抛出
    `GraphInterrupt` 异常（其值将展示给用户）。为了恢复图的执行，客户端应使用我们在此章中之前讨论过的 `Command` 类。LangGraph
    将从相同的节点开始，重新执行它，并返回节点调用 `interrupt` 函数的结果（如果您的节点中有多个 `interrupt`，LangGraph 会保持顺序）。您还可以使用
    `Command` 根据用户的输入路由到不同的节点。当然，只有当向图提供检查点器时，您才能使用 `interrupt`，因为其状态应该被持久化。
- en: 'Let’s construct a very simple graph with only the node that asks a user for
    their home address:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个非常简单的图，其中只有一个节点要求用户输入他们的家庭地址：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The graph returns us a special `__interrupt__` state and stops. Now our application
    (the client) should ask the user this question, and then we can resume. Please
    note that we’re providing the same `thread_id` to restore from the checkpoint:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图返回一个特殊的 `__interrupt__` 状态并停止。现在我们的应用程序（客户端）应该询问用户这个问题，然后我们可以继续。请注意，我们正在提供相同的
    `thread_id` 从检查点恢复：
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Note that the graph continued to execute the human_input node, but this time
    the `interrupt` function returned the result, and the graph’s state was updated.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图继续执行 human_input 节点，但这次 `interrupt` 函数返回了结果，并且图的状态已更新。
- en: So far, we’ve discussed a few architectural patterns on how to develop an agent.
    Now let’s take a look at another interesting one that allows LLMs to run multiple
    simulations while they’re looking for a solution.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了几种架构模式，说明了如何开发代理。现在让我们看看另一个有趣的模式，它允许 LLM 在寻找解决方案的同时运行多个模拟。
- en: Exploring reasoning paths
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索推理路径
- en: In [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107), we discussed CoT prompting.
    But with CoT prompting, the LLM creates a reasoning path within a single turn.
    What if we combine the decomposition pattern and the adaptation pattern by splitting
    this reasoning into pieces?
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第三章*](E_Chapter_3.xhtml#_idTextAnchor107)中，我们讨论了CoT提示。但是，使用CoT提示时，LLM在单次回复中创建一个推理路径。如果我们通过将这个推理拆分成片段来结合分解模式和适应性模式会怎样呢？
- en: Tree of Thoughts
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维树
- en: Researchers from Google DeepMind and Princeton University introduced **the ToT**
    technique in December 2023\. They generalize the CoT pattern and use thoughts
    as intermediate steps in the exploration process toward the global solution.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Google DeepMind和普林斯顿大学的研究人员在2023年12月介绍了**思维树（ToT）**技术。他们推广了CoT模式，并使用思维作为探索全局解决方案过程中的中间步骤。
- en: Let’s return to the plan-and-solve agent we built in the previous chapter. Let’s
    use the non-deterministic nature of LLMs to improve it. We can generate multiple
    candidates for the next action in the plan on every step (we might need to increase
    the temperature of the underlying LLM). That would help the agent to be more adaptive
    since the next plan generated will take into account the outputs of the previous
    step.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到上一章中构建的计划-解决代理。让我们利用LLM的非确定性来改进它。我们可以在计划的每一步生成多个候选动作（我们可能需要增加底层LLM的温度）。这将帮助代理更具适应性，因为生成的下一个计划将考虑前一步的输出。
- en: Now we can build a tree of various options and explore this tree with the depth-for-search
    or breadth-for-search method. At the end, we’ll get multiple solutions, and we’ll
    use some of the consensus mechanisms discussed above to pick the best one (for
    example, LLM-as-a-judge).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以构建一个各种选项的树，并使用深度优先搜索或广度优先搜索方法来探索这棵树。最后，我们将得到多个解决方案，并使用上面讨论的一些共识机制来选择最好的一个（例如，LLM作为裁判）。
- en: '![Figure 6.7: Solution path exploration with ToT](img/B32363_06_07.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7：使用ToT的解决方案路径探索](img/B32363_06_07.png)'
- en: 'Figure 6.7: Solution path exploration with ToT'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：使用ToT的解决方案路径探索
- en: Please note that the model’s provider should support the generation of multiple
    candidates in the response (not all providers support this feature).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，模型的提供者应支持在响应中生成多个候选方案（并非所有提供者都支持此功能）。
- en: We would like to highlight (and we’re not tired of doing this repeatedly in
    this chapter) that there’s nothing entirely new in the ToT pattern. You take what
    algorithms and patterns have been used already in other areas, and you use them
    to build capable agents.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调（并且我们在这个章节中反复这样做并不感到疲倦）的是，ToT模式中没有任何全新的东西。你只是将已经在其他领域使用过的算法和模式拿过来，并用它们来构建有能力的代理。
- en: 'Now it’s time to do some coding. We’ll take the same components of the plan-and-solve
    agents we developed in [*Chapter 5*](E_Chapter_5.xhtml#_idTextAnchor231) – a planner
    that creates an initial plan and `execution_agent`, which is a research agent
    with access to tools and works on a specific step in the plan. We can make our
    execution agent simpler since we don’t need a custom state:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行一些编码了。我们将采用我们在[*第五章*](E_Chapter_5.xhtml#_idTextAnchor231)中开发的计划-解决代理的相同组件——一个创建初始计划的规划器和`execution_agent`，这是一个可以访问工具并在计划中的特定步骤上工作的研究代理。由于我们不需要自定义状态，我们可以使我们的执行代理更简单：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We also need a `replanner`component, which will take care of adjusting the
    plan based on previous observations and generating multiple candidates for the
    next action:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要一个`replanner`组件，它将负责根据之前的观察调整计划，并为下一步生成多个候选方案：
- en: '[PRE28]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This `replanner` component is crucial for our ToT approach. It takes the current
    plan state and generates multiple potential next steps, encouraging exploration
    of different solution paths rather than following a single linear sequence.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`replanner`组件对我们ToT方法至关重要。它接受当前的计划状态并生成多个潜在的下一步，鼓励探索不同的解决方案路径，而不是遵循单一的线性序列。
- en: 'To track our exploration path, we need a tree data structure. The `TreeNode`
    class below helps us maintain it:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟踪我们的探索路径，我们需要一个树形数据结构。下面的`TreeNode`类帮助我们维护它：
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Each `TreeNode` tracks its identity, current step, output, parent relationship,
    and children. We also created a method to get a formatted full plan (we’ll substitute
    it in place of the prompt’s template), and just to make debugging more convenient,
    we overrode a `__repr__` method that returns a readable description of the node.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`TreeNode`跟踪其标识、当前步骤、输出、父节点关系和子节点。我们还创建了一个方法来获取格式化的完整计划（我们将用它替换提示模板），为了使调试更加方便，我们重写了`__repr__`方法，该方法返回节点的可读描述。
- en: 'Now we need to implement the core logic of our agent. We will explore our tree
    of actions in a depth-for-search mode. This is where the real power of the ToT
    pattern comes into play:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要实现代理的核心逻辑。我们将以深度优先搜索模式探索我们的动作树。这正是ToT模式真正发挥威力的地方：
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The `_run_node` function executes the current step, while `_plan_next` generates
    new candidate steps and adds them to our exploration queue. When we reach a final
    node (one where no further steps are needed), `_get_final_response` generates
    a final solution by picking the best one from multiple candidates (originating
    from different solution paths explored). Hence, in our agent’s state, we should
    keep track of the root node, the next node, the queue of nodes to be explored,
    and the nodes we’ve already explored:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`_run_node`函数执行当前步骤，而`_plan_next`生成新的候选步骤并将它们添加到我们的探索队列中。当我们达到最终节点（不需要进一步步骤的节点）时，`_get_final_response`通过从多个候选方案（来自不同探索的解决方案路径）中选择最佳方案来生成最终解决方案。因此，在我们的代理状态中，我们应该跟踪根节点、下一个节点、要探索的节点队列以及我们已经探索的节点：'
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This state structure keeps track of everything we need: the original task,
    our tree structure, exploration queue, path metadata, and candidate solutions.
    Note the special `Annotated` types that use custom reducers (like `operator.add`)
    to handle merging state values properly.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这种状态结构跟踪了我们所需的一切：原始任务、我们的树结构、探索队列、路径元数据和候选解决方案。注意使用自定义reducer（如`operator.add`）来正确合并状态值的特殊`Annotated`类型。
- en: 'One important thing to keep in mind is that LangGraph doesn’t allow you to
    modify `state` directly. In other words, if we execute something like the following
    within a node, it won’t have an effect on the actual queue in the agent’s state:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一个重要事情是，LangGraph不允许你直接修改`state`。换句话说，如果我们在一个节点内执行如下操作，它不会对代理状态中的实际队列产生任何影响：
- en: '[PRE34]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If we want to modify the queue that belongs to the state itself, we should either
    use a custom reducer (as we discussed in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107))
    or return the queue object to be replaced (since under the hood, LangGraph always
    created deep copies of the state before passing it to the node).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要修改属于状态本身的队列，我们应该要么使用自定义的reducer（如我们在[*第3章*](E_Chapter_3.xhtml#_idTextAnchor107)中讨论的）或者返回要替换的队列对象（因为底层，LangGraph在传递给节点之前总是创建了状态的深拷贝）。
- en: 'We need to define the final step now – the consensus mechanism to choose the
    final answer based on multiple generated candidates:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要定义最终步骤——基于多个生成的候选方案选择最终答案的共识机制：
- en: '[PRE35]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This voting mechanism presents all candidate solutions to the model and asks
    it to select the best one, leveraging the model’s ability to evaluate and compare
    options.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种投票机制向模型展示所有候选解决方案，并要求它选择最佳方案，利用模型评估和比较选项的能力。
- en: 'Now let’s add the remaining nodes and edges of the agent. We need two nodes
    – the one that creates an initial plan and another that evaluates the final output.
    Alongside these, we define two corresponding edges that evaluate whether the agent
    should continue on its exploration and whether it’s ready to provide a final response
    to the user:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加代理的剩余节点和边。我们需要两个节点——一个用于创建初始计划，另一个用于评估最终输出。在这些节点旁边，我们定义了两个相应的边，用于评估代理是否应该继续探索以及是否准备好向用户提供最终响应：
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These functions round out our implementation by defining the initial plan creation,
    final response generation, and flow control logic. The `_should_create_final_response`
    and `_should_continue` functions determine when to generate a final response and
    when to continue exploration. With all the components in place, we construct the
    final state graph:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数通过定义初始计划创建、最终响应生成和流程控制逻辑来完善我们的实现。`_should_create_final_response`和`_should_continue`函数确定何时生成最终响应以及何时继续探索。所有组件就绪后，我们构建最终的状态图：
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This creates our finished agent with a complete execution flow. The graph begins
    with initial planning, proceeds through execution and replanning steps, generates
    responses for completed paths, and finally selects the best solution through voting.
    We can visualize the flow using the Mermaid diagram generator, giving us a clear
    picture of our agent’s decision-making process:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这就创建了我们完成的代理，具有完整的执行流程。图从初始规划开始，经过执行和重新规划步骤，为完成的路径生成响应，并通过投票选择最佳解决方案。我们可以使用Mermaid图表生成器来可视化流程，从而清楚地了解我们的代理的决策过程：
- en: '![Figure 6.8: LATS agent](img/B32363_06_08.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8：LATS代理](img/B32363_06_08.png)'
- en: 'Figure 6.8: LATS agent'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8：LATS代理
- en: 'We can control the maximum number of super-steps, the maximum number of paths
    in the tree to be explored (in particular, the maximum number of candidates for
    the final solution to be generated), and the number of candidates per step. Potentially,
    we could extend our config and control the maximum depth of the tree. Let’s run
    our graph:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以控制超级步骤的最大数量，探索树中路径的最大数量（特别是生成最终解决方案候选人的最大数量），以及每一步的候选人数量。潜在地，我们可以扩展我们的配置并控制树的最大深度。让我们运行我们的图：
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can also visualize the explored tree:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以可视化已探索的树：
- en: '![Figure 6.9: Example of an explored execution tree](img/B32363_06_09-01.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9：已探索执行树的示例](img/B32363_06_09-01.png)'
- en: 'Figure 6.9: Example of an explored execution tree'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9：已探索执行树的示例
- en: We limited the number of candidates, but we can potentially increase it and
    add additional pruning logic (which will prune the leaves that are not promising).
    We can use the same LLM-as-a-judge approach, or use some other heuristic for pruning.
    We can also explore more advanced pruning strategies; we’ll talk about one of
    them in the next section.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们限制了候选人的数量，但我们可以潜在地增加它并添加额外的修剪逻辑（这将修剪没有希望的叶子）。我们可以使用相同的LLM作为裁判的方法，或者使用其他启发式方法进行修剪。我们还可以探索更高级的修剪策略；我们将在下一节中讨论其中之一。
- en: Trimming ToT with MCTS
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MCTS修剪ToT
- en: Some of you might remember AlphaGo – the first computer program that defeated
    humans in a game of Go. Google DeepMind developed it back in 2015, and it used
    **Monte Carlo Tree Search** (**MCTS**) as the core decision-making algorithm.
    Here’s a simple idea of how it works. Before taking the next move in a game, the
    algorithm builds a decision tree with potential future moves, with nodes representing
    your moves and your opponent’s potential responses (this tree expands quickly,
    as you can imagine). To keep the tree from expanding too fast, they used MCTS
    to search only through the most promising paths that lead to a better state in
    the game.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 一些你可能还记得AlphaGo——这是第一个在围棋游戏中击败人类的计算机程序。谷歌DeepMind在2015年开发了它，并使用**蒙特卡洛树搜索**（**MCTS**）作为核心决策算法。这里有一个简单的工作原理。在游戏中进行下一步之前，算法构建一个决策树，其中包含潜在的未来移动，节点代表你的移动和对手可能的回应（这个树会迅速扩展，就像你可以想象的那样）。为了防止树扩展得太快，他们使用了MCTS来搜索通向游戏更好状态的最有希望的路径。
- en: 'Now, coming back to the ToT pattern we learned about in the previous chapter.
    Think about the fact that the dimensionality of the ToT we’ve been building in
    the previous section might grow really fast. If, on every step, we’re generating
    3 candidates and there are only 5 steps in the workflow, we’ll end up with 3⁵=243
    steps to evaluate. That incurs a lot of cost and time. We can trim the dimensionality
    in different ways, for example, by using MCTS. It includes selection and simulation
    components:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们在上一章中学到的ToT模式。考虑这样一个事实，我们在上一节中构建的ToT的维度可能会增长得非常快。如果在每个步骤中都生成3个候选人，并且工作流程中只有5个步骤，我们将最终有3⁵=243个步骤需要评估。这会带来很多成本和时间。我们可以以不同的方式修剪维度，例如，使用MCTS。它包括选择和模拟组件：
- en: '**Selection** helps you pick the next node when analyzing the tree. You do
    that by balancing exploration and exploitation (you estimate the most promising
    node but add some randomness to this process).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择**帮助你在分析树时选择下一个节点。你通过平衡探索和利用来实现这一点（你估计最有希望的节点，但在这个过程中添加一些随机性）。'
- en: After you **expand** the tree by adding a new child to it, if it’s not a terminal
    node, you need to simulate the consequences of it. This might be done just by
    randomly playing all the next moves until the end, or using more sophisticated
    simulation approaches. After evaluating the child, you backpropagate the results
    to all the parent nodes by adjusting their probability scores for the next round
    of selection.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在通过添加一个新的子节点来扩展树之后，如果它不是一个终端节点，你需要模拟它的后果。这可能只是随机地玩所有后续移动直到结束，或者使用更复杂的模拟方法。在评估子节点后，你需要通过调整它们在下一轮选择中的概率分数，将结果回传给所有父节点。
- en: We’re not aiming to go into the details and teach you MCTS. We only want to
    demonstrate how you apply already-existing algorithms to agentic workflows to
    increase their performance. One such example is a **LATS** approach suggested
    by Andy Zhou and colleagues in June 2024 in their paper *Language Agent Tree Search
    Unifies Reasoning, Acting, and Planning in Language Models*. Without going into
    too much detail (you’re welcome to look at the original paper or the corresponding
    tutorials), the authors added MCTS on top of ToT, and they demonstrated an increased
    performance on complex tasks by getting number 1 on the HumanEval benchmark.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标不是深入细节并教你MCTS。我们只想展示如何将现有的算法应用到智能体工作流程中以提高其性能。一个例子是Andy Zhou及其同事在2024年6月在其论文《Language
    Agent Tree Search Unifies Reasoning, Acting, and Planning in Language Models》中提出的**LATS**方法。我们不打算过多深入细节（欢迎你查看原始论文或相应的教程），作者在ToT之上添加了MCTS，并通过在HumanEval基准测试中获得第一名，展示了在复杂任务上的性能提升。
- en: The key idea was that instead of exploring the whole tree, they use an LLM to
    evaluate the quality of the solution you get at every step (by looking at the
    sequence of all the steps on these specific reasoning steps and the outputs you’ve
    got so far).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是，他们不是探索整个树，而是使用一个LLM来评估你在每一步得到的解决方案的质量（通过查看这些特定推理步骤上所有步骤的序列以及你迄今为止得到的输出）。
- en: Now, as we’ve discussed some more advanced architectures that allow us to build
    better agents, there’s one last component to briefly touch on – memory. Helping
    agents to retain and retrieve relevant information from long-term interactions
    helps us to develop more advanced and helpful agents.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着我们讨论了一些更高级的架构，这些架构允许我们构建更好的智能体，还有一个最后的组件需要简要提及——记忆。帮助智能体保留和检索长期交互中的相关信息，有助于我们开发更高级、更有帮助的智能体。
- en: Agent memory
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能体记忆
- en: We discussed memory mechanisms in [*Chapter 3*](E_Chapter_3.xhtml#_idTextAnchor107).
    To recap, LangGraph has the notion of short-term memory via the `Checkpointer`
    mechanism, which saves checkpoints to persistent storage. This is the so-called
    per-thread persistence (remember, we discussed earlier in this chapter that the
    notion of a thread in LangGraph is similar to a conversation). In other words,
    the agent remembers our interactions within a given session, but it starts from
    scratch each time.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[*第3章*](E_Chapter_3.xhtml#_idTextAnchor107)中讨论了记忆机制。为了回顾，LangGraph通过`Checkpointer`机制具有短期记忆的概念，它将检查点保存到持久存储中。这就是所谓的线程持久化（记住，我们在这章中之前讨论过，LangGraph中的线程概念类似于对话）。换句话说，智能体记得我们在给定会话中的互动，但每次都是从零开始。
- en: As you can imagine, for complex agents, this memory mechanism might be inefficient
    for two reasons. First, you might lose important information about the user. Second,
    during the exploration phase when looking for a solution, an agent might learn
    something important about the environment that it forgets each time – and it doesn’t
    look efficient. That’s why there’s the concept of **long-term memory**, which
    helps an agent to accumulate knowledge and gain from historical experiences, and
    enables its continuous improvement on the long horizon.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，对于复杂的智能体，这种记忆机制可能由于两个原因而效率低下。首先，你可能会丢失关于用户的重要信息。其次，在探索阶段寻找解决方案时，智能体可能会学到关于环境的重要信息，但每次都会忘记——这看起来并不高效。这就是为什么有**长期记忆**的概念，它帮助智能体积累知识，从历史经验中获益，并使其在长期内持续改进。
- en: How to design and use long-term memory in practice is still an open question.
    First, you need to extract useful information (keeping in mind privacy requirements
    too; more about that in [*Chapter 9*](E_Chapter_9.xhtml#_idTextAnchor448)) that
    you want to store during the runtime and then you need to extract it during the
    next execution. Extraction is close to the retrieval problem we discussed while
    talking about RAG since we need to extract only knowledge relevant to the given
    context. The last component is the compaction of memory – you probably want to
    periodically self-reflect on what you have learned, optimize it, and forget irrelevant
    facts.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中如何设计和使用长期记忆仍然是一个未解之谜。首先，你需要提取出有用的信息（同时也要考虑到隐私要求；更多关于这一点的内容可以在[*第9章*](E_Chapter_9.xhtml#_idTextAnchor448)中找到），这些信息是你希望在运行时存储的，然后你需要在下一次执行中提取它。提取过程与我们在讨论RAG时提到的检索问题相似，因为我们只需要提取与给定上下文相关的知识。最后一个组件是内存压缩——你可能希望定期自我反思你所学到的内容，优化它，并忘记无关紧要的事实。
- en: 'These are key considerations to take into account, but we haven’t seen any
    great practical implementations of long-term memory for agentic workflows yet.
    In practice, these days people typically use two components – a built-in **cache**
    (a mechanism to cache LLMs responses), a built-in **store** (a persistent key-value
    store), and a custom cache or database. Use the custom option when:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是需要考虑的关键因素，但我们还没有看到任何针对代理工作流程的长期记忆的出色实际实现。在实践中，如今人们通常使用两个组件 – 内置的 **缓存**（一种缓存
    LLM 响应的机制）、内置的 **存储**（一个持久化的键值存储）以及自定义缓存或数据库。当使用以下自定义选项时：
- en: You need additional flexibility for how you organize memory – for example, you
    would like to keep track of all memory states.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要额外的灵活性来组织内存 – 例如，您可能希望跟踪所有内存状态。
- en: You need advanced read or write access patterns when working with this memory.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当与这种内存一起工作时，您需要高级的读写访问模式。
- en: You need to keep the memory distributed and across multiple workers, and you’d
    like to use a database other than PostgreSQL.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要保持内存分布式并在多个工作者之间，并且您希望使用除 PostgreSQL 之外的数据库。
- en: Cache
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: Caching allows you to save and retrieve key values. Imagine you’re working on
    an enterprise question-answering assistance application, and in the UI, you ask
    a user whether they like the answer. If the answer is positive, or if you have
    a curated dataset of question-answer pairs for the most important topics, you
    can store these in a cache. When the same (or a similar) question is asked later,
    the system can quickly return the cached response instead of regenerating it from
    scratch.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存允许您保存和检索键值。想象一下，您正在开发一个企业级问答辅助应用程序，在 UI 中，您询问用户是否喜欢这个答案。如果答案是肯定的，或者如果您有一个针对最重要主题的问答对精选数据集，您可以将这些存储在缓存中。当稍后再次（或类似地）提出相同（或类似）的问题时，系统可以快速返回缓存的响应，而不是从头开始重新生成。
- en: 'LangChain allows you to set a global cache for LLM responses in the following
    way (after you have initialized the cache, the LLM’s response will be added to
    the cache, as we’ll see below):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 允许您以以下方式设置 LLM 响应的全局缓存（在您初始化缓存之后，LLM 的响应将被添加到缓存中，如下所示）：
- en: '[PRE42]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Caching with LangChain works as follows: Each vendor’s implementation of a
    `ChatModel` inherits from the base class, and the base class first tries to look
    up a value in the cache during generation. cache is a global variable that we
    can expect (of course, only after it has been initialized). It caches responses
    based on the key that consists of a string representation of the prompt and the
    string representation of the LLM instance (produced by the `llm._get_llm_string`
    method).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 中的缓存工作方式如下：每个供应商的 `ChatModel` 实现都继承自基类，基类在生成过程中首先尝试在缓存中查找值。cache 是一个全局变量，我们预期（当然，只有在其初始化之后）。它根据由提示的字符串表示和
    LLM 实例的字符串表示（由 `llm._get_llm_string` 方法产生）组成的键来缓存响应。
- en: 'This means the LLM’s generation parameters (such as `stop_words` or `temperature`)
    are included in the cache key:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 LLM 的生成参数（例如 `stop_words` 或 `temperature`）包含在缓存键中：
- en: '[PRE43]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'LangChain supports in-memory and SQLite caches out of the box (they form part
    of `langchain_core.caches`), and there are also many vendor integrations – available
    through the `langchain_community.cache` subpackage at [https://python.langchain.com/api_reference/community/cache.html](https://python.langchain.com/api_reference/community/cache.html)
    or through specific vendor integrations (for example, `langchain-mongodb` offers
    cache integration for MongoDB: [https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/api_docs.html](https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/api_docs.html)).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain 支持内存和 SQLite 缓存（它们是 `langchain_core.caches` 的一部分），并且还有许多供应商集成 – 通过
    [https://python.langchain.com/api_reference/community/cache.html](https://python.langchain.com/api_reference/community/cache.html)
    中的 `langchain_community.cache` 子包或通过特定的供应商集成（例如，`langchain-mongodb` 为 MongoDB
    提供缓存集成：[https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/api_docs.html](https://langchain-mongodb.readthedocs.io/en/latest/langchain_mongodb/api_docs.html))。
- en: We recommend introducing a separate LangGraph node instead that hits an actual
    cache (based on Redis or another database), since it allows you to control whether
    you’d like to search for similar questions using the embedding mechanism we discussed
    in [*Chapter 4*](E_Chapter_4.xhtml#_idTextAnchor152) when we were talking about
    RAG.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议引入一个单独的 LangGraph 节点，该节点击中实际的缓存（基于 Redis 或其他数据库），因为它允许您控制是否希望使用我们在讨论 RAG
    时提到的嵌入机制（[*第 4 章*](E_Chapter_4.xhtml#_idTextAnchor152)）来搜索类似的问题。
- en: Store
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储
- en: As we have learned before, a `Checkpointer` mechanism allows you to enhance
    your workflows with a thread-level persistent memory; by thread-level, we mean
    a conversation-level persistence. Each conversation can be started where it stops,
    and the workflow executes the previously collected context.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所学的，`Checkpointer`机制允许您通过线程级持久内存来增强您的流程；通过线程级，我们指的是对话级持久。每个对话都可以从停止的地方开始，并且工作流程执行之前收集的上下文。
- en: A `BaseStore` is a persistent key-value storage system that organizes your values
    by namespace (hierarchical tuples of string paths, similar to folders. It supports
    standard operations such as `put`, `delete` and `get` operations, as well as a
    `search` method that implements different semantic search capabilities (typically,
    based on the embedding mechanism) and accounts for a hierarchical nature of namespaces.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`BaseStore`是一个持久键值存储系统，它通过命名空间（类似于文件夹的字符串路径的分层元组）组织您的值。它支持标准操作，如`put`、`delete`和`get`操作，以及一个实现不同语义搜索能力的`search`方法（通常基于嵌入机制），并考虑了命名空间的分层性质。'
- en: 'Let’s initialize a store and add some values to it:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们初始化一个存储并添加一些值到它中：
- en: '[PRE44]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can easily query the value:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松查询值：
- en: '[PRE45]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If we query it by a partial path of the namespace, we won’t get any results
    (we need a full matching namespace). The following would return no results:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过命名空间的局部路径查询它，我们将不会得到任何结果（我们需要一个完全匹配的命名空间）。以下将不会返回任何结果：
- en: '[PRE46]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'On the other side, when using `search`, we can use a partial namespace path:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一方面，当使用`search`时，我们可以使用部分命名空间路径：
- en: '[PRE47]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As you can see, we were able to retrieve all relevant facts stored in memory
    by using a partial search.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们通过使用部分搜索，能够检索存储在内存中的所有相关事实。
- en: LangGraph has built-in `InMemoryStore` and `PostgresStore` implementations.
    Agentic memory mechanisms are still evolving. You can build your own implementation
    from available components, but we should see a lot of progress in the coming years
    or even months.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph内置了`InMemoryStore`和`PostgresStore`实现。代理内存机制仍在不断发展。您可以从可用组件中构建自己的实现，但我们应该在接下来的几年甚至几个月内看到大量的进展。
- en: Summary
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we dived deep into advanced applications of LLMs and the architectural
    patterns that enable them, leveraging LangChain and LangGraph. The key takeaway
    is that effectively building complex AI systems goes beyond simply prompting an
    LLM; it requires careful architectural design of the workflow itself, tool usage,
    and giving an LLM partial control over the workflow. We also discussed different
    agentic AI design patterns and how to develop agents that leverage LLMs’ tool-calling
    abilities to solve complex tasks.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了LLM的高级应用以及使它们成为可能的架构模式，利用LangChain和LangGraph。关键要点是，有效地构建复杂的AI系统不仅需要简单地提示LLM；它还需要对工作流程本身进行仔细的架构设计，工具使用，并给予LLM对工作流程的部分控制。我们还讨论了不同的代理AI设计模式以及如何开发利用LLM的工具调用能力来解决复杂任务的代理。
- en: We explored how LangGraph streaming works and how to control what information
    is streamed back during execution. We discussed the difference between streaming
    state updates and partial streaming answer tokens, learned about the Command interface
    as a way to hand off execution to a specific node within or outside the current
    LangGraph workflow, looked at the LangGraph platform and its main capabilities,
    and discussed how to implement HIL with LangGraph. We discussed how a thread on
    LangGraph differs from a traditional Pythonic definition (a thread is somewhat
    similar to a conversation instance), and we learned how to add memory to our workflow
    per-thread and with cross-thread persistence. Finally, we learned how to expand
    beyond basic LLM applications and build robust, adaptive, and intelligent systems
    by leveraging the advanced capabilities of LangChain and LangGraph.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了LangGraph流的工作原理以及如何在执行期间控制要流回的信息。我们讨论了流状态更新和部分流答案标记之间的区别，了解了命令接口作为将执行传递给当前LangGraph工作流程内或外的特定节点的方式，查看了LangGraph平台及其主要功能，并讨论了如何使用LangGraph实现HIL。我们还讨论了LangGraph上的线程与传统Python定义（线程在某种程度上类似于对话实例）之间的区别，并学习了如何通过跨线程持久性为我们的工作流程按线程添加内存。最后，我们学习了如何利用LangChain和LangGraph的高级功能，扩展基本LLM应用，并构建强大、自适应和智能的系统。
- en: In the next chapter, we’ll take a look at how generative AI transforms the software
    engineering industry by assisting in code development and data analysis.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨生成式AI如何通过协助代码开发和数据分析来改变软件工程行业。
- en: Questions
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Name at least three design patterns to consider when building generative AI
    agents.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建生成式 AI 代理时，至少列出三种需要考虑的设计模式。
- en: Explain the concept of “dynamic retrieval” in the context of agentic RAG.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理 RAG 的背景下解释“动态检索”的概念。
- en: How can cooperation between agents improve the outputs of complex tasks? How
    can you increase the diversity of cooperating agents, and what impact on performance
    might it have?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理之间的合作如何提高复杂任务的输出？如何增加合作代理的多样性，这可能会对性能产生什么影响？
- en: Describe examples of reaching consensus across multiple agents’ outputs.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 描述在多个代理输出之间达成共识的例子。
- en: What are the two main ways to organize communication in a multi-agent system
    with LangGraph?
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LangGraph 在多代理系统中组织通信的主要两种方式是什么？
- en: Explain the differences between stream, astream, and astream_events in LangGraph.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释 LangGraph 中 stream、astream 和 astream_events 之间的区别。
- en: What is a command in LangGraph, and how is it related to handoffs?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LangGraph 中的命令是什么，它与 handoffs 有何关联？
- en: Explain the concept of a thread in the LangGraph platform. How is it different
    from Pythonic threads?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释 LangGraph 平台中线程的概念。它与 Pythonic 线程有何不同？
- en: Explain the core idea behind the Tree of Thoughts (ToT) technique. How is ToT
    related to the decomposition pattern?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释 Tree of Thoughts (ToT) 技术背后的核心思想。ToT 与分解模式有何关联？
- en: Describe the difference between short-term and long-term memory in the context
    of agentic systems.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代理系统的背景下描述短期记忆和长期记忆的区别。
- en: Subscribe to our weekly newsletter
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 订阅我们的每周通讯。
- en: Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers,
    and innovators, at [https://packt.link/Q5UyU](E_Chapter_6.xhtml).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 订阅 AI_Distilled，这是 AI 专业人士、研究人员和革新者的首选通讯，请访问 [https://packt.link/Q5UyU](E_Chapter_6.xhtml)。
- en: '![](img/Newsletter_QRcode1.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![Newsletter_QRcode1.jpg](img/Newsletter_QRcode1.jpg)'
