<html><head></head><body>
<div class="calibre1" id="_idContainer173">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">9</span></h1>
<h1 class="chaptertitle" id="_idParaDest-125"><span class="kobospan" id="kobo.2.1">Working with Code</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">In this chapter, we are going to cover another great capability of Large Language Models, that is, working with programming languages. </span><span class="kobospan" id="kobo.3.2">In the previous chapter, we’ve already seen a glimpse of this capability, namely, SQL query generation in a SQL database. </span><span class="kobospan" id="kobo.3.3">In this chapter, we are going to examine the other ways in which LLMs can be used with code, from “simple” code generation to interaction with code repositories and, finally, to the possibility of letting an application behave as if it were an algorithm. </span><span class="kobospan" id="kobo.3.4">By the end of this chapter, you will be able to leverage LLMs to code-related projects, as well as build LLM-powered applications with natural language interfaces to work with code.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.4.1">Throughout this chapter, we will cover the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.5.1">Analysis of the main LLMs with top-performing code capabilities</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.6.1">Using LLMs for code understanding and generation</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.7.1">Building LLM-powered agents to “act as” algorithms</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.8.1">Leveraging Code Interpreter</span></li>
</ul>
<h1 class="heading" id="_idParaDest-126"><span class="kobospan" id="kobo.9.1">Technical requirements</span></h1>
<p class="normal"><span class="kobospan" id="kobo.10.1">To complete the tasks in this chapter, you will need the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.11.1">A Hugging Face account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.12.1">An OpenAI account and user access token.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.13.1">Python 3.7.1 or a later version.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.14.1">Python packages. </span><span class="kobospan" id="kobo.14.2">Make sure you have the following Python packages installed: </span><code class="inlinecode"><span class="kobospan" id="kobo.15.1">langchain</span></code><span class="kobospan" id="kobo.16.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.17.1">python-dotenv</span></code><span class="kobospan" id="kobo.18.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.19.1">huggingface_hub</span></code><span class="kobospan" id="kobo.20.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.21.1">streamlit</span></code><span class="kobospan" id="kobo.22.1">, </span><code class="inlinecode"><span class="kobospan" id="kobo.23.1">codeinterpreterapi</span></code><span class="kobospan" id="kobo.24.1">, and </span><code class="inlinecode"><span class="kobospan" id="kobo.25.1">jupyter_kernel_gateway</span></code><span class="kobospan" id="kobo.26.1">. </span><span class="kobospan" id="kobo.26.2">Those can be easily installed via </span><code class="inlinecode"><span class="kobospan" id="kobo.27.1">pip</span></code> <code class="inlinecode"><span class="kobospan" id="kobo.28.1">install</span></code><span class="kobospan" id="kobo.29.1"> in your terminal.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.30.1">You can find all the code and examples in the book’s GitHub repository at </span><a href="Chapter_09.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.31.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.32.1">.</span></p>
<h1 class="heading" id="_idParaDest-127"><span class="kobospan" id="kobo.33.1">Choosing the right LLM for code</span></h1>
<p class="normal"><span class="kobospan" id="kobo.34.1">In </span><em class="italic"><span class="kobospan" id="kobo.35.1">Chapter 3</span></em><span class="kobospan" id="kobo.36.1">, we described a decision</span><a id="_idIndexMarker651" class="calibre3"/><span class="kobospan" id="kobo.37.1"> framework to use in order</span><a id="_idIndexMarker652" class="calibre3"/><span class="kobospan" id="kobo.38.1"> to decide the proper LLM for a given application. </span><span class="kobospan" id="kobo.38.2">Generally speaking, all LLMs are endowed with knowledge of code understanding and generation; however, some of them are particularly specialized in doing so. </span><span class="kobospan" id="kobo.38.3">More specifically, there are some evaluation benchmarks – such as the HumanEval – that are specifically tailored to assessing LLMs’ capabilities of working with code. </span><span class="kobospan" id="kobo.38.4">The leaderboard of HumanEval One is a good source for determining the top-performing models, available at </span><a href="https://paperswithcode.com/sota/code-generation-on-humaneval" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.39.1">https://paperswithcode.com/sota/code-generation-on-humaneval</span></span></a><span class="kobospan" id="kobo.40.1">. </span><span class="kobospan" id="kobo.40.2">HumanEval is a benchmark introduced by OpenAI to assess the code generation capabilities of LLMs, where the model completes Python functions based on their signature and docstring. </span><span class="kobospan" id="kobo.40.3">It has been used to evaluate models like Codex, demonstrating its effectiveness in measuring functional correctness.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.41.1">In the following screenshot, you can see the situation of the leaderboard as of January 2024:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.42.1"><img alt="" role="presentation" src="../Images/B21714_09_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.43.1">Figure 9.1: HumanEval benchmark in January 2024</span></p>
<p class="normal1"><span class="kobospan" id="kobo.44.1">As you can see, the majority of the models are fine-tuned versions of the GPT-4 (as well as the GPT-4 itself), as it is the state-of-the-art LLM in basically all the domains. </span><span class="kobospan" id="kobo.44.2">Nevertheless, there are many open-source models that reached stunning results in the field of code understanding and generation, some of which will be covered in the next sections. </span><span class="kobospan" id="kobo.44.3">Another benchmark is </span><strong class="screentext"><span class="kobospan" id="kobo.45.1">Mostly Basic Programming Problems </span></strong><span class="kobospan" id="kobo.46.1">(</span><strong class="screentext"><span class="kobospan" id="kobo.47.1">MBPP</span></strong><span class="kobospan" id="kobo.48.1">), a dataset of 974 programming tasks in Python, designed</span><a id="_idIndexMarker653" class="calibre3"/><span class="kobospan" id="kobo.49.1"> to be solvable by entry-level programmers. </span><span class="kobospan" id="kobo.49.2">Henceforth, when choosing your model for a code-specific task, it might be useful to have a look at these benchmarks as well as other similar code metrics (we will see throughout the chapter some further benchmarks for code-specific LLMs). </span></p>
<p class="normal1"><span class="kobospan" id="kobo.50.1">Staying within the scope of coding, below you can find three additional benchmarks often used in the market:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.51.1">MultiPL-E</span></strong><span class="kobospan" id="kobo.52.1">: An extension of HumanEval</span><a id="_idIndexMarker654" class="calibre3"/><span class="kobospan" id="kobo.53.1"> to many other languages, such as Java, C#, Ruby, and SQL.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.54.1">DS-1000</span></strong><span class="kobospan" id="kobo.55.1">: A data science benchmark</span><a id="_idIndexMarker655" class="calibre3"/><span class="kobospan" id="kobo.56.1"> that tests if the model can write code for common data analysis tasks in Python.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.57.1">Tech Assistant Prompt</span></strong><span class="kobospan" id="kobo.58.1">: A prompt that tests if the model</span><a id="_idIndexMarker656" class="calibre3"/><span class="kobospan" id="kobo.59.1"> can act as a technical assistant and answer programming-related requests.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.60.1">In this chapter, we are going</span><a id="_idIndexMarker657" class="calibre3"/><span class="kobospan" id="kobo.61.1"> to test </span><a id="_idIndexMarker658" class="calibre3"/><span class="kobospan" id="kobo.62.1">different LLMs: two code-specific (CodeLlama and StarCoder) and one general-purpose, yet also with emerging capabilities in the field of code generation (Falcon LLM).</span></p>
<h1 class="heading" id="_idParaDest-128"><span class="kobospan" id="kobo.63.1">Code understanding and generation</span></h1>
<p class="normal"><span class="kobospan" id="kobo.64.1">The first experiment we are going</span><a id="_idIndexMarker659" class="calibre3"/><span class="kobospan" id="kobo.65.1"> to run will be code understanding</span><a id="_idIndexMarker660" class="calibre3"/><span class="kobospan" id="kobo.66.1"> and generation leveraging LLMs. </span><span class="kobospan" id="kobo.66.2">This simple use case is at the base of the many AI code assistants that were developed since the launch of ChatGPT, first among all the GitHub Copilot.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.67.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.68.1">GitHub Copilot is an AI-powered tool</span><a id="_idIndexMarker661" class="calibre3"/><span class="kobospan" id="kobo.69.1"> that assists developers in writing code more efficiently. </span><span class="kobospan" id="kobo.69.2">It analyzes code and comments to provide suggestions for individual lines and entire functions. </span><span class="kobospan" id="kobo.69.3">The tool is developed by GitHub, OpenAI, and Microsoft and supports multiple programming languages. </span><span class="kobospan" id="kobo.69.4">It can perform various tasks such as code completion, modification, explanation, and technical </span><a id="_idIndexMarker662" class="calibre3"/><span class="kobospan" id="kobo.70.1">assistance.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.71.1">In this experiment, we are going to try three different models: Falcon LLM, which we already explored in </span><em class="italic"><span class="kobospan" id="kobo.72.1">Chapter 3</span></em><span class="kobospan" id="kobo.73.1">; CodeLlama, a fine-tuned version of Meta AI’s Llama; and StarCoder, a code-specific model that we are going to investigate in the upcoming sections.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.74.1">Since those models</span><a id="_idIndexMarker663" class="calibre3"/><span class="kobospan" id="kobo.75.1"> are pretty heavy to run on a local machine, for this purpose I’m going</span><a id="_idIndexMarker664" class="calibre3"/><span class="kobospan" id="kobo.76.1"> to use a Hugging Face Hub Inference Endpoint, with a GPU-powered virtual machine. </span><span class="kobospan" id="kobo.76.2">You can link one model per Inference Endpoint and then embed it in your code, or use the convenient library </span><code class="inlinecode"><span class="kobospan" id="kobo.77.1">HuggingFaceEndpoint</span></code><span class="kobospan" id="kobo.78.1">, available in LangChain.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.79.1">To start using your Inference Endpoint, you can use the following code:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.80.1">llm = HuggingFaceEndpoint(endpoint_url = </span><span class="hljs-string"><span class="kobospan" id="kobo.81.1">"your_endpoint_url"</span></span><span class="kobospan" id="kobo.82.1">, task = </span><span class="hljs-string"><span class="kobospan" id="kobo.83.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.84.1">text-generation'</span></span><span class="kobospan" id="kobo.85.1">,
        model_kwargs = {</span><span class="hljs-string"><span class="kobospan" id="kobo.86.1">"max_new_tokens"</span></span><span class="kobospan" id="kobo.87.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.88.1">1100</span></span><span class="kobospan" id="kobo.89.1">})
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.90.1">Alternatively, you can copy and paste the Python code provided on your endpoint’s webpage at </span><a href="https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.91.1">https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name</span></span></a><span class="kobospan" id="kobo.92.1">:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.93.1"><img alt="" role="presentation" src="../Images/B21714_09_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.94.1">Figure 9.2: User interface of the Hugging Face Inference Endpoint</span></p>
<p class="normal1"><span class="kobospan" id="kobo.95.1">To create your Hugging Face Inference Endpoint, you</span><a id="_idIndexMarker665" class="calibre3"/><span class="kobospan" id="kobo.96.1"> can follow the instructions at </span><a href="https://huggingface.co/docs/inference-endpoints/index" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.97.1">https://huggingface.co/docs/inference-endpoints/index</span></span></a><span class="kobospan" id="kobo.98.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.99.1">You can always leverage</span><a id="_idIndexMarker666" class="calibre3"/><span class="kobospan" id="kobo.100.1"> the free Hugging Face API</span><a id="_idIndexMarker667" class="calibre3"/><span class="kobospan" id="kobo.101.1"> as described in </span><em class="italic"><span class="kobospan" id="kobo.102.1">Chapter 4</span></em><span class="kobospan" id="kobo.103.1">, but you have to expect some latency when running the models.</span></p>
<h2 class="heading1" id="_idParaDest-129"><span class="kobospan" id="kobo.104.1">Falcon LLM</span></h2>
<p class="normal"><span class="kobospan" id="kobo.105.1">Falcon LLM is an open-source model</span><a id="_idIndexMarker668" class="calibre3"/><span class="kobospan" id="kobo.106.1"> developed by Abu Dhabi’s </span><strong class="screentext"><span class="kobospan" id="kobo.107.1">Technology Innovation Institute</span></strong><span class="kobospan" id="kobo.108.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.109.1">TII</span></strong><span class="kobospan" id="kobo.110.1">) and launched on the market</span><a id="_idIndexMarker669" class="calibre3"/><span class="kobospan" id="kobo.111.1"> in May 2023. </span><span class="kobospan" id="kobo.111.2">It is an autoregressive, decoder-only transformer, trained on 1 trillion tokens, and has 40 billion parameters (although it has also been released as a lighter version with 7 billion parameters). </span><span class="kobospan" id="kobo.111.3">As discussed in </span><em class="italic"><span class="kobospan" id="kobo.112.1">Chapter 3</span></em><span class="kobospan" id="kobo.113.1">, “small” language models are a representation of a new trend of LLMs, consisting of building lighter models (with fewer parameters) that focus instead on the quality of the training dataset.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.114.1">To start using Falcon LLM, we can</span><a id="_idIndexMarker670" class="calibre3"/><span class="kobospan" id="kobo.115.1"> follow these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.116.1">We can leverage the HuggingFaceHub wrapper available in LangChain (remember to set the Hugging Face API in the </span><code class="inlinecode"><span class="kobospan" id="kobo.117.1">.env</span></code><span class="kobospan" id="kobo.118.1"> file, passing your secrets as </span><code class="inlinecode"><span class="kobospan" id="kobo.119.1">os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN</span></code><span class="kobospan" id="kobo.120.1">):
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.121.1">from</span></span><span class="kobospan" id="kobo.122.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.123.1">import</span></span><span class="kobospan" id="kobo.124.1"> HuggingFaceHub
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.125.1">from</span></span><span class="kobospan" id="kobo.126.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.127.1">import</span></span><span class="kobospan" id="kobo.128.1"> PromptTemplate, LLMChain
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.129.1">import</span></span><span class="kobospan" id="kobo.130.1"> os
load_dotenv()hugging_face_api = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.131.1">"HUGGINGFACEHUB_API_TOKEN"</span></span><span class="kobospan" id="kobo.132.1">]
repo_id = </span><span class="hljs-string"><span class="kobospan" id="kobo.133.1">"tiiuae/falcon-7b-instruct"</span></span><span class="kobospan" id="kobo.134.1">
llm = HuggingFaceHub(
    repo_id=repo_id,  model_kwargs={</span><span class="hljs-string"><span class="kobospan" id="kobo.135.1">"temperature"</span></span><span class="kobospan" id="kobo.136.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.137.1">0.2</span></span><span class="kobospan" id="kobo.138.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.139.1">"max_new_tokens"</span></span><span class="kobospan" id="kobo.140.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.141.1">1000</span></span><span class="kobospan" id="kobo.142.1">}
)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.143.1">Now that we’ve initialized the model, let’s ask it to generate the code for a simple webpage:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.144.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.145.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.146.1">Generate a short html code to a simple webpage with a header, a subheader, and a text body.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.147.1">&lt;!DOCTYPE html&gt;</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.148.1">&lt;html&gt;</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.149.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.150.1">print</span></span><span class="kobospan" id="kobo.151.1">(llm(prompt))
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.152.1">The following is the corresponding output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.153.1">&lt;head&gt;
    &lt;title&gt;My Webpage&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;My Webpage&lt;/h1&gt;
    &lt;h2&gt;Subheader&lt;/h2&gt;
    &lt;p&gt;This is the text body.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.154.1">If you save it as an HTML file and execute it, the result will look like the following:</span></li>
</ol>
<figure class="mediaobject"><span class="kobospan" id="kobo.155.1"><img alt="A close up of a text  Description automatically generated" src="../Images/B21714_09_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.156.1">Figure 9.3: Sample webpage generated by FalconLLM</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="4"><span class="kobospan" id="kobo.157.1">We can also try to generate</span><a id="_idIndexMarker671" class="calibre3"/><span class="kobospan" id="kobo.158.1"> a Python function to generate random passwords:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.159.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.160.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.161.1">Generate a python program that create random password with lenght of 12 characters, 3 numbers, one capital letter.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.162.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.163.1">print</span></span><span class="kobospan" id="kobo.164.1">(llm(prompt))
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.165.1">Here is our output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.166.1">import random
def generate_password():
    chars = "abcdefghijklmnopqrstuvwxyz0123456789"
    lenght = 12
    num = random.randint(1, 9)
    cap = random.randint(1, 9)
    password = ""
    for i in range(lenght):
        password += chars[random.randint(0, 9)]
    password += num
    password += cap
    return password
print(generate_password())
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.167.1">We now have a function named </span><code class="inlinecode"><span class="kobospan" id="kobo.168.1">generate_password()</span></code><span class="kobospan" id="kobo.169.1">, which uses random functions to generate a password as per our prompt.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="5"><span class="kobospan" id="kobo.170.1">Finally, let’s do the opposite, asking the model to explain to us the above code:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.171.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.172.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.173.1">Explain to me the following code:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.174.1">def generate_password():</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.175.1">    chars = "abcdefghijklmnopqrstuvwxyz0123456789"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.176.1">    lenght = 12</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.177.1">    num = random.randint(1, 9)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.178.1">    cap = random.randint(1, 9)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.179.1">    password = ""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.180.1">    for i in range(lenght):</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.181.1">        password += chars[random.randint(0, 9)]</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.182.1">    password += num</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.183.1">    password += cap</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.184.1">    return password</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.185.1">print(generate_password())</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.186.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.187.1">print</span></span><span class="kobospan" id="kobo.188.1">(llm(prompt))
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.189.1">Here is the obtained output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.190.1">&lt;p&gt;The code generates a random password of length 12 characters that contains a mix of letters, numbers, and special characters. </span><span class="kobospan" id="kobo.190.2">The password is then printed to the console.&lt;/p&gt;
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.191.1">Overall, even if not code-specific, the model was able to correctly perform all the tasks. </span><span class="kobospan" id="kobo.191.2">Note also that this is the “light” version</span><a id="_idIndexMarker672" class="calibre3"/><span class="kobospan" id="kobo.192.1"> of the model (7 billion parameters), yet its performance is great.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.193.1">Let’s now investigate the capabilities of CodeLlama.</span></p>
<h2 class="heading1" id="_idParaDest-130"><span class="kobospan" id="kobo.194.1">CodeLlama</span></h2>
<p class="normal"><span class="kobospan" id="kobo.195.1">CodeLlama is a family of LLMs</span><a id="_idIndexMarker673" class="calibre3"/><span class="kobospan" id="kobo.196.1"> for code based on Llama 2, which is a general-purpose language model developed by Meta AI (as discussed in </span><em class="italic"><span class="kobospan" id="kobo.197.1">Chapter 3</span></em><span class="kobospan" id="kobo.198.1">). </span><span class="kobospan" id="kobo.198.2">CodeLlama can generate and discuss code in various programming languages, such as Python, C++, Java, PHP, and more. </span><span class="kobospan" id="kobo.198.3">CodeLlama can also perform infilling, which is the ability to fill in missing parts of code based on the surrounding context, as well as follow instructions given in natural language and produce code that matches the desired functionality.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.199.1">The model comes in three sizes (7B, 13B, and 34B parameters) and three flavors (base model, Python fine-tuned, and instruction-tuned) to cover a wide range of applications. </span><span class="kobospan" id="kobo.199.2">CodeLlama is trained on sequences of 16k tokens and can handle inputs with up to 100k tokens.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.200.1">In the model paper “Code Llama: Open Foundation Models for Code” by Rozière Baptiste et al, released in August 2023, the authors describe how the various models were tested against some of the most popular evaluation benchmarks in the domain of code understanding and generation, including HumanEval and MBPP, according to which CodeLlama models achieved a score up to 53% and 55%, respectively. </span><span class="kobospan" id="kobo.200.2">On top of those remarkable results, it is stunning that the Python fine-tuned CodeLlama’s smallest size (7 billion parameters) outperformed the largest version of Llama 2 (70 billion parameters) on HumanEval and MBPP.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.201.1">Now, let’s run some tests</span><a id="_idIndexMarker674" class="calibre3"/><span class="kobospan" id="kobo.202.1"> with this model. </span><span class="kobospan" id="kobo.202.2">As per the previous section, we can initialize the model leveraging either the Hugging Face Inference API (pay per use) or the free Hugging Face API (with the constraint of higher latency). </span><span class="kobospan" id="kobo.202.3">You can consume it as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.203.1">repo_id = </span><span class="hljs-string"><span class="kobospan" id="kobo.204.1">"codellama/CodeLlama-7b-Instruct-hf"</span></span><span class="kobospan" id="kobo.205.1">
llm = HuggingFaceHub(    repo_id=repo_id,  model_kwargs={</span><span class="hljs-string"><span class="kobospan" id="kobo.206.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.207.1">temperature"</span></span><span class="kobospan" id="kobo.208.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.209.1">0.2</span></span><span class="kobospan" id="kobo.210.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.211.1">"max_new_tokens"</span></span><span class="kobospan" id="kobo.212.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.213.1">1000</span></span><span class="kobospan" id="kobo.214.1">}
)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.215.1">Let’s now test it with some code tasks. </span><span class="kobospan" id="kobo.215.2">The first task will be that of optimizing Python code so that it runs more efficiently. </span><span class="kobospan" id="kobo.215.3">Let’s see how our model performs in this task. </span><span class="kobospan" id="kobo.215.4">In the following code snippet, we simply prompt the model to regenerate the provided code in a more efficient way:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.216.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.217.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.218.1">Regenerate the below code in a more efficient way.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.219.1">```</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.220.1">def factorial(n):</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.221.1">    result = 1</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.222.1">    for i in range(1, n + 1):</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.223.1">        result *= i</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.224.1">    return result</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.225.1"># Example usage:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.226.1">n = 5</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.227.1">print("Factorial of", n, "is", factorial(n))</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.228.1">```</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.229.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.230.1">print</span></span><span class="kobospan" id="kobo.231.1">(llm(prompt))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.232.1">Here is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.233.1">### Solution
```
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
```
### Explanation
The solution is a recursive function that calls itself until the base case is reached.
</span><span class="kobospan" id="kobo.233.2">### References
- [Wikipedia](https://en.wikipedia.org/wiki/Factorial)
- [Math is Fun](https://www.mathsisfun.com/numbers/factorial.html)
- [Mathematics Stack Exchange](https://math.stackexchange.com/questions/117042/what-is-the-difference-between-factorial-and-permutation)
- [Mathematics Stack Exchange](https://math.stackexchange.com/questions/117042/what-is-the-difference-between-factorial-and-permutation)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.234.1">As you can see, the model</span><a id="_idIndexMarker675" class="calibre3"/><span class="kobospan" id="kobo.235.1"> was able to use a recursive approach, which is more efficient and “Pythonic.” </span><span class="kobospan" id="kobo.235.2">It also provides a reference for the user to dive deeper into the mathematical theory behind the function.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.236.1">Next, let’s leverage the model’s completion capabilities by initializing a function to remove non-ASCII characters.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.237.1">Definition</span></strong></p>
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.238.1">American Standard Code for Information Interchange</span></strong><span class="kobospan" id="kobo.239.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.240.1">ASCII</span></strong><span class="kobospan" id="kobo.241.1">) is a character</span><a id="_idIndexMarker676" class="calibre3"/><span class="kobospan" id="kobo.242.1"> encoding standard that uses 7 bits to represent 128 characters, such as letters, digits, punctuation marks, and control codes.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.243.1">Non-ASCII characters are those that are not part of the ASCII standard and use more than 7 bits to encode. </span><span class="kobospan" id="kobo.243.2">They include special characters such as letters with accents, glyphs, ideograms, and mathematical symbols. </span><span class="kobospan" id="kobo.243.3">Non-ASCII characters can be encoded</span><a id="_idIndexMarker677" class="calibre3"/><span class="kobospan" id="kobo.244.1"> using different standards, such as Unicode, ISO 8859-1, Windows-1252, etc.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.245.1">For example, the letter é is a non-ASCII character that can be encoded using Unicode as U+00E9 or using Windows-1252 as 0xE9.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.246.1">Accordingly, here is the code</span><a id="_idIndexMarker678" class="calibre3"/><span class="kobospan" id="kobo.247.1"> to generate the function:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.248.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.249.1">'''</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.250.1">Define a python function that remove non-ascii character from a string.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.251.1">'''</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.252.1">print</span></span><span class="kobospan" id="kobo.253.1">(llm(prompt))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.254.1">The following is the function that we receive as the output:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.255.1">def</span></span> <span class="hljs-title"><span class="kobospan" id="kobo.256.1">remove_non_ascii</span></span><span class="kobospan" id="kobo.257.1">(</span><span><span class="kobospan" id="kobo.258.1">string</span></span><span class="kobospan" id="kobo.259.1">):
    </span><span class="hljs-keyword"><span class="kobospan" id="kobo.260.1">return</span></span><span class="kobospan" id="kobo.261.1"> string.encode(</span><span class="hljs-string"><span class="kobospan" id="kobo.262.1">'ascii'</span></span><span class="kobospan" id="kobo.263.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.264.1">'ignore'</span></span><span class="kobospan" id="kobo.265.1">).decode(</span><span class="hljs-string"><span class="kobospan" id="kobo.266.1">'utf-8'</span></span><span class="kobospan" id="kobo.267.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.268.1">Let’s now leverage the model as a bug fixer, prompting it with the wrong function and also asking it to provide an explanation of why it is wrong and how it can be fixed:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.269.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.270.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.271.1">Identify the bug in the following code:.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.272.1">```</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.273.1">#wrong function</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.274.1">import random</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.275.1">a = random.randint(1, 12)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.276.1">b = random.randint(1, 12)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.277.1">for i in range(10):</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.278.1">    question = "What is " + a + " x " + b + "? </span><span class="kobospan" id="kobo.278.2">"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.279.1">    answer = input(question)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.280.1">    if answer = a * b:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.281.1">        print (Well done!)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.282.1">    else:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.283.1">        print("No.")</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.284.1">```</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.285.1">Explanation:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.286.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.287.1">print</span></span><span class="kobospan" id="kobo.288.1">(llm(prompt))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.289.1">Here is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.290.1">- The function is using the wrong syntax for the `randint` function. </span><span class="kobospan" id="kobo.290.2">The correct syntax is `randint(start, end)`.
</span><span class="kobospan" id="kobo.290.3">- The `for` loop is using the wrong syntax. </span><span class="kobospan" id="kobo.290.4">The correct syntax is `for i in range(start, end)`.
</span><span class="kobospan" id="kobo.290.5">- The `question` and `answer` variables are not being used in the loop.
</span><span class="kobospan" id="kobo.290.6">- The `if` statement is not being used correctly. </span><span class="kobospan" id="kobo.290.7">It should be `if answer == a * b:`.
</span><span class="kobospan" id="kobo.290.8">- The `print` statement is not being used correctly. </span><span class="kobospan" id="kobo.290.9">It should be `print(Well done!)`.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.291.1">Finally, let’s ask the model in natural language</span><a id="_idIndexMarker679" class="calibre3"/><span class="kobospan" id="kobo.292.1"> to generate specific Python code for a given task that, in our example, will be that of writing a function that finds the longest substring of a given string containing only unique characters:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.293.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.294.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.295.1">Write a Python function that finds the longest substring of a given string containing only unique characters.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.296.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.297.1">print</span></span><span class="kobospan" id="kobo.298.1">(llm(prompt))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.299.1">We then get the following function as our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.300.1">def longest_unique_substring(s):
    unique_count = 0
    longest_substring = ""
    for i in range(1, len(s)):
        if s[i] not in unique_count:
            unique_count += 1
            longest_substring = s[i]
    return longest_substring
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.301.1">As per the Falcon LLM, in this case we used the light version of the model (7 billion parameters), still obtaining great results. </span><span class="kobospan" id="kobo.301.2">This is a perfect example of how the task you want to address with your application must be a factor in deciding what LLM to use: if you are only interested in code generation, completion, infilling, debugging, or any other code-related tasks, a light and open-source model could be more than enough, rather than 70 billion parameters</span><a id="_idIndexMarker680" class="calibre3"/><span class="kobospan" id="kobo.302.1"> of a state-of-the-art GPT-4.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.303.1">In the next section, we are going to cover the third and last LLM in the context of code generation and understanding.</span></p>
<h2 class="heading1" id="_idParaDest-131"><span class="kobospan" id="kobo.304.1">StarCoder</span></h2>
<p class="normal"><span class="kobospan" id="kobo.305.1">The StarCoder model is an LLM for code</span><a id="_idIndexMarker681" class="calibre3"/><span class="kobospan" id="kobo.306.1"> that can perform various tasks, such as code completion, code modification, code explanation, and technical assistance. </span><span class="kobospan" id="kobo.306.2">It was trained on permissively licensed data from GitHub, including from 80+ programming languages, Git commits, GitHub issues, and Jupyter notebooks. </span><span class="kobospan" id="kobo.306.3">It has a context length of over 8,000 tokens, which enables it to process more input than any other open-source language model. </span><span class="kobospan" id="kobo.306.4">It also has an improved license that simplifies the process for companies to integrate the model into their products.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.307.1">The StarCoder model was evaluated on several benchmarks that test its ability to write and understand code in different languages and domains, including the aforementioned HumanEval and MBPP, where the model scored, respectively, 33.6% and 52.7%. </span><span class="kobospan" id="kobo.307.2">Additionally, it was tested against MultiPL-E (where the model matched or outperformed the code-cushman-001 model from OpenAI on many languages), the DS-1000 (where the model clearly beat the code-cushman-001 model as well as all other open-access models), and the Tech Assistant Prompt (where the model was able to respond to various queries with relevant and accurate information).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.308.1">According to a survey published on May 4 2023 by Hugging Face, StarCoder demonstrated great capabilities compared to other models, using HumanEval and MBPP as benchmarks. </span><span class="kobospan" id="kobo.308.2">You can see an illustration of this study below:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.309.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_09_04.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.310.1">Figure 9.4: Results of evaluation benchmarks for various LLMs. </span><span class="kobospan" id="kobo.310.2">Source: </span><a href="https://huggingface.co/blog/starcoder" class="calibre3"><span class="kobospan" id="kobo.311.1">https://huggingface.co/blog/starcoder</span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.312.1">To start using StarCoder, we can follow</span><a id="_idIndexMarker682" class="calibre3"/><span class="kobospan" id="kobo.313.1"> these steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.314.1">We can leverage the HuggingFaceHub wrapper available in LangChain (remember to set the Hugging Face API in the </span><code class="inlinecode"><span class="kobospan" id="kobo.315.1">.env</span></code><span class="kobospan" id="kobo.316.1"> file):
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.317.1">import</span></span><span class="kobospan" id="kobo.318.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.319.1">from</span></span><span class="kobospan" id="kobo.320.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.321.1">import</span></span><span class="kobospan" id="kobo.322.1"> load_dotenv
load_dotenv()
hugging_face_api = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.323.1">"HUGGINGFACEHUB_API_TOKEN"</span></span><span class="kobospan" id="kobo.324.1">]
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.325.1">Let’s set the </span><code class="inlinecode"><span class="kobospan" id="kobo.326.1">repo_id</span></code><span class="kobospan" id="kobo.327.1"> for the StarCoder model and initialize it:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.328.1">from</span></span><span class="kobospan" id="kobo.329.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.330.1">import</span></span><span class="kobospan" id="kobo.331.1"> HuggingFaceHub
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.332.1">from</span></span><span class="kobospan" id="kobo.333.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.334.1">import</span></span><span class="kobospan" id="kobo.335.1"> PromptTemplate, LLMChain
repo_id = </span><span class="hljs-string"><span class="kobospan" id="kobo.336.1">"bigcode/starcoderplus"</span></span><span class="kobospan" id="kobo.337.1">
llm = HuggingFaceHub(
    repo_id=repo_id,  model_kwargs={</span><span class="hljs-string"><span class="kobospan" id="kobo.338.1">"temperature"</span></span><span class="kobospan" id="kobo.339.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.340.1">0.2</span></span><span class="kobospan" id="kobo.341.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.342.1">"max_new_tokens"</span></span><span class="kobospan" id="kobo.343.1">: </span><span class="hljs-number"><span class="kobospan" id="kobo.344.1">500</span></span><span class="kobospan" id="kobo.345.1">}
)
</span></code></pre>
</li>
</ol>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.346.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.347.1">StarCoder is a gated model on the Hugging Face Hub, meaning that you will need to request access directly from the bigcode/starcoderplus repo before being able to connect to it.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.348.1">Now that we’re set up, let’s start asking</span><a id="_idIndexMarker683" class="calibre3"/><span class="kobospan" id="kobo.349.1"> our model to compile some code. </span><span class="kobospan" id="kobo.349.2">To start with, we will ask it to generate a Python function to generate the nth Fibonacci number:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.350.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.351.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.352.1">How can I write a Python function to generate the nth Fibonacci number?</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.353.1">"""</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.354.1">print</span></span><span class="kobospan" id="kobo.355.1">(llm(prompt))
</span></code></pre>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.356.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.357.1">The Fibonacci sequence is a mathematical</span><a id="_idIndexMarker684" class="calibre3"/><span class="kobospan" id="kobo.358.1"> series that begins with 0 and 1, and each subsequent number is the sum of the two preceding numbers. </span><span class="kobospan" id="kobo.358.2">For instance, the first 10 numbers of the Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.359.1">There are different ways to compute the nth Fibonacci number, which is denoted by F(n). </span><span class="kobospan" id="kobo.359.2">One way is to use a recursive formula:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.360.1"><img alt="" role="presentation" src="../Images/B21714_09_001.png" class="calibre4"/></span></figure>
<p class="normal1"><span class="kobospan" id="kobo.361.1">This means that to find F(n), we need to find F(n-1) and F(n-2) first, and then add them together. </span><span class="kobospan" id="kobo.361.2">This works for any n greater than or equal to 2. </span><span class="kobospan" id="kobo.361.3">For n equal to 0 or 1, we simply return n as the answer.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.362.1">We then see the following output:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.363.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_09_05.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.364.1">Figure 9.5: Example of Fibonacci functions generated by StarCode</span></p>
<p class="normal1"><span class="kobospan" id="kobo.365.1">As you can see, it also proposed</span><a id="_idIndexMarker685" class="calibre3"/><span class="kobospan" id="kobo.366.1"> different approaches to solve the problem, alongside the explanation.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.367.1">Let’s now ask the model to generate a webpage to play tic tac toe against the computer:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.368.1">prompt = </span><span class="hljs-string"><span class="kobospan" id="kobo.369.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.370.1">Generate the html code for a single page website that let the user play tic tac toe.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.371.1"> """</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.372.1">print</span></span><span class="kobospan" id="kobo.373.1">(llm(prompt))
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.374.1">Here is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.375.1"> ## How to use
```
git clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git
cd tic-tac-toe
python3 -m http.server
```
## License
 [MIT](https://choosealicense.com/licenses/mit/)
&lt;|endoftext|&gt;
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.376.1">Interestingly enough, the model</span><a id="_idIndexMarker686" class="calibre3"/><span class="kobospan" id="kobo.377.1"> in this case didn’t generate the whole code; rather, it gave the instructions to clone and run a git repository that can achieve this result.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.378.1">Finally, StarCoder is also available as an extension in VS Code</span><a id="_idIndexMarker687" class="calibre3"/><span class="kobospan" id="kobo.379.1"> to act as your code copilot. </span><span class="kobospan" id="kobo.379.2">You can find it as </span><strong class="screentext"><span class="kobospan" id="kobo.380.1">HF Code Autocomplete</span></strong><span class="kobospan" id="kobo.381.1">, as shown in the following screenshot:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.382.1"><img alt="A screenshot of a computer  Description automatically generated" src="../Images/B21714_09_06.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.383.1">Figure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder</span></p>
<p class="normal1"><span class="kobospan" id="kobo.384.1">Once enabled, you can see that, while compiling your code, StarCoder will provide suggestions to complete the code. </span><span class="kobospan" id="kobo.384.2">For example:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.385.1"><img alt="" role="presentation" src="../Images/B21714_09_07.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.386.1">Figure 9.7: Screenshot of a suggested completion, given a function description</span></p>
<p class="normal1"><span class="kobospan" id="kobo.387.1">As you can see, I commented my code, describing a function to generate the nth Fibonacci number, and then started defining the function. </span><span class="kobospan" id="kobo.387.2">Automatically, I’ve been provided with the StarCoder auto-completion suggestion.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.388.1">Code understanding and generation are great capabilities of LLMs. </span><span class="kobospan" id="kobo.388.2">On top of those capabilities, there are further applications that we can think about, going beyond code generation. </span><span class="kobospan" id="kobo.388.3">In fact, the code can be seen also as a backend reasoning tool to propose solutions to complex problems, such as an energy optimization problem rather than an algorithm task. </span><span class="kobospan" id="kobo.388.4">To do this, we can leverage LangChain</span><a id="_idIndexMarker688" class="calibre3"/><span class="kobospan" id="kobo.389.1"> to create powerful agents that can </span><em class="italic"><span class="kobospan" id="kobo.390.1">act as if they were algorithms</span></em><span class="kobospan" id="kobo.391.1">. </span><span class="kobospan" id="kobo.391.2">In the upcoming section, we will see how to do so.</span></p>
<h1 class="heading" id="_idParaDest-132"><span class="kobospan" id="kobo.392.1">Act as an algorithm</span></h1>
<p class="normal"><span class="kobospan" id="kobo.393.1">Some problems are complex</span><a id="_idIndexMarker689" class="calibre3"/><span class="kobospan" id="kobo.394.1"> by definition and difficult to solve leveraging “only” LLMs’ analytical reasoning skills. </span><span class="kobospan" id="kobo.394.2">However, LLMs are still intelligent enough to understand the problems overall and leverage their coding capabilities to solve them.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.395.1">In this context, LangChain</span><a id="_idIndexMarker690" class="calibre3"/><span class="kobospan" id="kobo.396.1"> provides a tool that empowers the LLM to reason “in Python,” meaning that the LLM-powered agent will leverage Python to solve complex problems. </span><span class="kobospan" id="kobo.396.2">This tool is the Python REPL, which is a simple Python shell that can execute Python commands. </span><span class="kobospan" id="kobo.396.3">The Python REPL is important because it allows users to perform complex calculations, generate code, and interact with language models using Python syntax. </span><span class="kobospan" id="kobo.396.4">In this section, we will cover some examples of the tool’s capabilities.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.397.1">Let’s first initialize our agent</span><a id="_idIndexMarker691" class="calibre3"/><span class="kobospan" id="kobo.398.1"> using the </span><code class="inlinecode"><span class="kobospan" id="kobo.399.1">create_python_agent</span></code><span class="kobospan" id="kobo.400.1"> class in LangChain. </span><span class="kobospan" id="kobo.400.2">To do so, we</span><a id="_idIndexMarker692" class="calibre3"/><span class="kobospan" id="kobo.401.1"> will need to provide this class with an LLM and a tool, which, in our example, will be the Python REPL:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.402.1">import</span></span><span class="kobospan" id="kobo.403.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.404.1">from</span></span><span class="kobospan" id="kobo.405.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.406.1">import</span></span><span class="kobospan" id="kobo.407.1"> load_dotenv
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.408.1">from</span></span><span class="kobospan" id="kobo.409.1"> langchain.agents.agent_types </span><span class="hljs-keyword"><span class="kobospan" id="kobo.410.1">import</span></span><span class="kobospan" id="kobo.411.1"> AgentType
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.412.1">from</span></span><span class="kobospan" id="kobo.413.1"> langchain.chat_models </span><span class="hljs-keyword"><span class="kobospan" id="kobo.414.1">import</span></span><span class="kobospan" id="kobo.415.1"> ChatOpenAI
 </span><span class="hljs-keyword"><span class="kobospan" id="kobo.416.1">from</span></span><span class="kobospan" id="kobo.417.1"> langchain_experimental.agents.agent_toolkits.python.base </span><span class="hljs-keyword"><span class="kobospan" id="kobo.418.1">import</span></span><span class="kobospan" id="kobo.419.1"> create_python_agent
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.420.1">from</span></span><span class="kobospan" id="kobo.421.1"> langchain_experimental.tools </span><span class="hljs-keyword"><span class="kobospan" id="kobo.422.1">import</span></span><span class="kobospan" id="kobo.423.1"> PythonREPLTool
load_dotenv()
openai_api_key = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.424.1">'OPENAI_API_KEY'</span></span><span class="kobospan" id="kobo.425.1">]
model = ChatOpenAI(temperature=</span><span class="hljs-number"><span class="kobospan" id="kobo.426.1">0</span></span><span class="kobospan" id="kobo.427.1">, model=</span><span class="hljs-string"><span class="kobospan" id="kobo.428.1">"gpt-3.5-turbo-0613"</span></span><span class="kobospan" id="kobo.429.1">)
agent_executor = create_python_agent(
    llm=model,
    tool=PythonREPLTool(),
    verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.430.1">True</span></span><span class="kobospan" id="kobo.431.1">,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.432.1">As always, before starting to work with the agent, let’s first inspect the default prompt:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.433.1">print</span></span><span class="kobospan" id="kobo.434.1">(agent_executor.agent.llm_chain.prompt.template)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.435.1">Here is our output:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.436.1"><img alt="A screenshot of a computer program  Description automatically generated" src="../Images/B21714_09_08.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.437.1">Figure 9.8: Default prompt of the Python agent</span></p>
<p class="normal1"><span class="kobospan" id="kobo.438.1">Now, let’s start with an</span><a id="_idIndexMarker693" class="calibre3"/><span class="kobospan" id="kobo.439.1"> easy query, asking the model to generate a scatter plot based on sample attributes of basketball players:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.440.1">query = </span><span class="hljs-string"><span class="kobospan" id="kobo.441.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.442.1">In a different basketball game, we have the following player stats:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.443.1">- Player A: 38 points, 10 rebounds, 7 assists</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.444.1">- Player B: 28 points, 9 rebounds, 6 assists</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.445.1">- Player C: 19 points, 6 rebounds, 3 assists</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.446.1">- Player D: 12 points, 4 rebounds, 2 assists</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.447.1">- Player E: 7 points, 2 rebounds, 1 assist</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.448.1">Could you create a scatter plot graph in Seaborn talk mode for each player, where the y-axis represents the number of points, the x-axis represents the number of rebounds, and use 'o' as the marker? </span><span class="kobospan" id="kobo.448.2">Additionally, please label each point with the player's name and set the title as "Team Players."</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.449.1">"""</span></span><span class="kobospan" id="kobo.450.1">
agent_executor.run(query)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.451.1">We then get the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.452.1">Invoking: `Python_REPL` with `import seaborn as sns
import matplotlib.pyplot as plt
# Player stats
players = ['Player A', 'Player B', 'Player C', 'Player D', 'Player E']
points = [38, 28, 19, 12, 7]
rebounds = [10, 9, 6, 4, 2]
# Create scatter plot
sns.scatterplot(x=rebounds, y=points, marker='o')
# Label each point with player's name
for i, player in enumerate(players):
    plt.text(rebounds[i], points[i], player, ha='center', va='bottom')
# Set title
plt.title('Team Players')
# Show the plot
plt.show()`
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.453.1">This output is accompanied</span><a id="_idIndexMarker694" class="calibre3"/><span class="kobospan" id="kobo.454.1"> by the following graph based on the players’ statistics:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.455.1"><img alt="A screenshot of a computer program  Description automatically generated" src="../Images/B21714_09_09.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.456.1">Figure 9.9: Sample plot generated by the Python agent</span></p>
<p class="normal1"><span class="kobospan" id="kobo.457.1">Let’s look at another example. </span><span class="kobospan" id="kobo.457.2">Say we want to predict the price of a house based on some features, such as the number of bedrooms or the size of the house. </span><span class="kobospan" id="kobo.457.3">To do so, we can ask our agent to design and train a model to give us the result of a given house. </span><span class="kobospan" id="kobo.457.4">For example, let’s consider the following prompt:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.458.1">query = </span><span class="hljs-string"><span class="kobospan" id="kobo.459.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.460.1">I want to predict the price of a house given the following information:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.461.1">- the number of rooms</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.462.1">- the number of bathrooms</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.463.1">- the size of the house in square meters</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.464.1">Design and train a regression model to predict the price of a house. </span><span class="kobospan" id="kobo.464.2">Generate and use synthetic data to train the model.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.465.1">Once the model is trained, tell me the price of a house with the following features:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.466.1">- 2 rooms</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.467.1">- 1 bathroom</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.468.1">- 100 square meters</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.469.1">"""</span></span><span class="kobospan" id="kobo.470.1">
agent_executor.run(query)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.471.1">Here, we ask the agent to train a regression</span><a id="_idIndexMarker695" class="calibre3"/><span class="kobospan" id="kobo.472.1"> model on synthetic data (representative of houses with various configurations of rooms, bathrooms, and area, each with an associated price as a dependent variable) to give us the estimated price of a house with the above features. </span><span class="kobospan" id="kobo.472.2">Let’s see the output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.473.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.473.2">Invoking: `Python_REPL` with `import numpy as np
from sklearn.linear_model import LinearRegression
# Generate synthetic data
np.random.seed(0)
X = np.random.rand(100, 3)  # 100 houses with 3 features: rooms, bathrooms, size
y = 100000 * X[:, 0] + 200000 * X[:, 1] + 300000 * X[:, 2] + 50000  # Price = 100k * rooms + 200k * bathrooms + 300k * size + 50k
# Train the regression model
model = LinearRegression()
model.fit(X, y)
# Predict the price of a house with the given features
features = np.array([[2, 1, 100]])
predicted_price = model.predict(features)
predicted_price`
responded: {content}
The predicted price of a house with 2 rooms, 1 bathroom, and 100 square meters is approximately $550,000.
</span><span class="kobospan" id="kobo.473.3">&gt; Finished chain.
</span><span class="kobospan" id="kobo.473.4">'The predicted price of a house with 2 rooms, 1 bathroom, and 100 square meters is approximately $550,000.'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.474.1">As you can see, the agent was able to generate synthetic training data, train a proper regression model using the </span><code class="inlinecode"><span class="kobospan" id="kobo.475.1">sklearn</span></code><span class="kobospan" id="kobo.476.1"> libraries, and predict with the model the price of the house we provided.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.477.1">With this approach, we can program an agent to act as an algorithm in real-time scenarios. </span><span class="kobospan" id="kobo.477.2">Imagine, for example, that we want to design an agent that is capable of solving optimization problems in a smart building environment. </span><span class="kobospan" id="kobo.477.3">The goal</span><a id="_idIndexMarker696" class="calibre3"/><span class="kobospan" id="kobo.478.1"> is to optimize the </span><strong class="screentext"><span class="kobospan" id="kobo.479.1">Heating, Ventilation and Air Conditioning</span></strong><span class="kobospan" id="kobo.480.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.481.1">HVAC</span></strong><span class="kobospan" id="kobo.482.1">) setpoints in the building to minimize energy costs while ensuring occupant comfort. </span><span class="kobospan" id="kobo.482.2">Let’s define the variables and constraints of the problem: the objective is to adjust</span><a id="_idIndexMarker697" class="calibre3"/><span class="kobospan" id="kobo.483.1"> the temperature setpoints within the specified comfort ranges for each of the three zones while considering the varying energy costs per degree, per hour. </span></p>
<p class="normal1"><span class="kobospan" id="kobo.484.1">The goal is to strike a balance between energy efficiency and occupant comfort. </span><span class="kobospan" id="kobo.484.2">Below, you can find a description of the problem and also the initialization of our variables and constraints (energy cost per zone, initial temperature per zone, and comfort range per zone):</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.485.1">query = </span><span class="hljs-string"><span class="kobospan" id="kobo.486.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.487.1">**Problem**:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.488.1">You are tasked with optimizing the HVAC setpoints in a smart building to minimize energy costs while ensuring occupant comfort. </span><span class="kobospan" id="kobo.488.2">The building has three zones, and you can adjust the temperature setpoints for each zone. </span><span class="kobospan" id="kobo.488.3">The cost function for energy consumption is defined as:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.489.1">- Zone 1: Energy cost = $0.05 per degree per hour</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.490.1">- Zone 2: Energy cost = $0.07 per degree per hour</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.491.1">- Zone 3: Energy cost = $0.06 per degree per hour</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.492.1">You need to find the optimal set of temperature setpoints for the three zones to minimize the energy cost while maintaining a comfortable temperature. </span><span class="kobospan" id="kobo.492.2">The initial temperatures in each zone are as follows:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.493.1">- Zone 1: 72°F</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.494.1">- Zone 2: 75°F</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.495.1">- Zone 3: 70°F</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.496.1">The comfort range for each zone is as follows:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.497.1">- Zone 1: 70°F to 74°F</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.498.1">- Zone 2: 73°F to 77°F</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.499.1">- Zone 3: 68°F to 72°F</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.500.1">**Question**:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.501.1">What is the minimum total energy cost (in dollars per hour) you can achieve by adjusting the temperature setpoints for the three zones within their respective comfort ranges?</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.502.1">"""</span></span><span class="kobospan" id="kobo.503.1">
agent_executor.run(query)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.504.1">We then get the following </span><a id="_idIndexMarker698" class="calibre3"/><span class="kobospan" id="kobo.505.1">output (you can find the whole reasoning chain in the book’s GitHub repository):</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.506.1">&gt; Entering new AgentExecutor chain...
</span><span class="kobospan" id="kobo.506.2">Invoking: `Python_REPL` with `import scipy.optimize as opt
# Define the cost function
def cost_function(x):
    zone1_temp = x[0]
    zone2_temp = x[1]
    zone3_temp = x[2]
   
    # Calculate the energy cost for each zone
    zone1_cost = 0.05 * abs(zone1_temp - 72)
    zone2_cost = 0.07 * abs(zone2_temp - 75)
    zone3_cost = 0.06 * abs(zone3_temp - 70)
[…]
&gt; Finished chain.
</span><span class="kobospan" id="kobo.506.3">'The minimum total energy cost that can be achieved by adjusting the temperature setpoints for the three zones within their respective comfort ranges is $0.15 per hour.'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.507.1">The agent was able to solve</span><a id="_idIndexMarker699" class="calibre3"/><span class="kobospan" id="kobo.508.1"> the smart building optimization problem, finding the minimum total energy cost, given some constraints. </span><span class="kobospan" id="kobo.508.2">Staying in the scope of optimization problems, there are further use cases that these models could address with a similar approach, including:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.509.1">Supply chain optimization</span></strong><span class="kobospan" id="kobo.510.1">: Optimize the logistics</span><a id="_idIndexMarker700" class="calibre3"/><span class="kobospan" id="kobo.511.1"> and distribution of goods to minimize transportation costs, reduce inventory, and ensure timely deliveries.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.512.1">Portfolio optimization</span></strong><span class="kobospan" id="kobo.513.1">: In finance, use algorithms to construct investment portfolios that maximize returns while managing risk.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.514.1">Route planning</span></strong><span class="kobospan" id="kobo.515.1">: Plan optimal routes for delivery trucks, emergency services, or ride-sharing platforms to minimize travel time and fuel consumption.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.516.1">Manufacturing process optimization</span></strong><span class="kobospan" id="kobo.517.1">: Optimize manufacturing processes to minimize waste, energy consumption, and production costs while maintaining product quality.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.518.1">Healthcare resource allocation</span></strong><span class="kobospan" id="kobo.519.1">: Allocate healthcare resources like hospital beds, medical staff, and equipment efficiently during a pandemic or other healthcare crisis.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.520.1">Network routing</span></strong><span class="kobospan" id="kobo.521.1">: Optimize data routing in computer networks to reduce latency, congestion, and energy consumption.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.522.1">Fleet management</span></strong><span class="kobospan" id="kobo.523.1">: Optimize the use of a fleet of vehicles, such as taxis or delivery vans, to reduce operating costs and improve service quality.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.524.1">Inventory management</span></strong><span class="kobospan" id="kobo.525.1">: Determine optimal inventory levels and reorder points to minimize storage costs while preventing stockouts.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.526.1">Agricultural planning</span></strong><span class="kobospan" id="kobo.527.1">: Optimize crop planting and harvesting schedules based on weather patterns and market demand to maximize yield and profits.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.528.1">Telecommunications network design</span></strong><span class="kobospan" id="kobo.529.1">: Design the layout of telecommunications networks to provide coverage while minimizing infrastructure costs.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.530.1">Waste management</span></strong><span class="kobospan" id="kobo.531.1">: Optimize routes for garbage collection trucks to reduce fuel consumption and emissions.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.532.1">Airline crew scheduling</span></strong><span class="kobospan" id="kobo.533.1">: Create efficient flight crew schedules that adhere to labor regulations</span><a id="_idIndexMarker701" class="calibre3"/><span class="kobospan" id="kobo.534.1"> and minimize costs for airlines.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.535.1">The Python REPL agent</span><a id="_idIndexMarker702" class="calibre3"/><span class="kobospan" id="kobo.536.1"> is amazing; however, it comes with some caveats:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.537.1">It does not allow for FileIO, meaning that it cannot read and write with your local file system.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.538.1">It forgets the variables after every run, meaning that you cannot keep trace of your initialized variables after the model’s response.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.539.1">To bypass these caveats, in the next section, we are going to cover an open-source project built on top of the LangChain agent: the Code Interpreter API.</span></p>
<h1 class="heading" id="_idParaDest-133"><span class="kobospan" id="kobo.540.1">Leveraging Code Interpreter</span></h1>
<p class="normal"><span class="kobospan" id="kobo.541.1">The name “Code Interpreter” was coined</span><a id="_idIndexMarker703" class="calibre3"/><span class="kobospan" id="kobo.542.1"> by OpenAI, referring to the recently developed plugin for ChatGPT. </span><span class="kobospan" id="kobo.542.2">The Code Interpreter plugin allows ChatGPT to write and execute computer code in various programming languages. </span><span class="kobospan" id="kobo.542.3">This enables ChatGPT to perform tasks such as calculations, data analysis, and generating visualizations.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.543.1">The Code Interpreter plugin is one of the tools designed specifically for language models with safety as a core principle. </span><span class="kobospan" id="kobo.543.2">It helps ChatGPT access up-to-date information, run computations, or use third-party services. </span><span class="kobospan" id="kobo.543.3">The plugin is currently in private beta and is available for selected developers and ChatGPT Plus users.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.544.1">While OpenAI’s Code Interpreter still doesn’t offer an API, there are some open-source projects that adapted the concept of this plugin in an open-source Python library. </span><span class="kobospan" id="kobo.544.2">In this section, we are going to leverage the work of Shroominic, available at </span><a href="https://github.com/shroominic/codeinterpreter-api" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.545.1">https://github.com/shroominic/codeinterpreter-api</span></span></a><span class="kobospan" id="kobo.546.1">. </span><span class="kobospan" id="kobo.546.2">You can install it via </span><code class="inlinecode"><span class="kobospan" id="kobo.547.1">pip install codeinterpreterapi</span></code><span class="kobospan" id="kobo.548.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.549.1">According to the blog post published by Shroominic, the author of the Code Interpreter API (which you can read at </span><a href="https://blog.langchain.dev/code-interpreter-api/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.550.1">https://blog.langchain.dev/code-interpreter-api/</span></span></a><span class="kobospan" id="kobo.551.1">), it is based on the LangChain agent </span><code class="inlinecode"><span class="kobospan" id="kobo.552.1">OpenAIFunctionsAgent</span></code><span class="kobospan" id="kobo.553.1">.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.554.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.555.1">OpenAIFunctionsAgent is a type of agent</span><a id="_idIndexMarker704" class="calibre3"/><span class="kobospan" id="kobo.556.1"> that can use the OpenAI functions’ ability to respond to the user’s prompts using an LLM. </span><span class="kobospan" id="kobo.556.2">The agent is driven by a model that supports using OpenAI functions, and it has access to a set of tools that it can use to interact with the user.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.557.1">The OpenAIFunctionsAgent can also integrate custom functions. </span><span class="kobospan" id="kobo.557.2">For example, you can define custom functions to get the current stock price or stock performance using Yahoo Finance. </span><span class="kobospan" id="kobo.557.3">The OpenAIFunctionsAgent can use the ReAct framework to decide which tool to use, and it can use memory to remember the previous conversation interactions.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.558.1">The API comes already with some tools, such as the possibility to navigate the web to get up-to-date information.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.559.1">Yet the greatest difference</span><a id="_idIndexMarker705" class="calibre3"/><span class="kobospan" id="kobo.560.1"> from the Python REPL tool that we covered in the previous section is that the Code Interpreter API can actually execute the code it generates. </span><span class="kobospan" id="kobo.560.2">In fact, when a Code Interpreter session starts, a miniature of a Jupyter Kernel is launched on your device, thanks to the underlying</span><a id="_idIndexMarker706" class="calibre3"/><span class="kobospan" id="kobo.561.1"> Python execution environment called CodeBox.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.562.1">To start using the code interpreter in your notebook, you can install all the dependencies as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.563.1">!pip install </span><span class="hljs-string"><span class="kobospan" id="kobo.564.1">"codeinterpreterapi[all]"</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.565.1">In this case, I will ask it to generate a plot of COVID-19 cases in a specific time range:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.566.1">from</span></span><span class="kobospan" id="kobo.567.1"> codeinterpreterapi </span><span class="hljs-keyword"><span class="kobospan" id="kobo.568.1">import</span></span><span class="kobospan" id="kobo.569.1"> CodeInterpreterSession
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.570.1">import</span></span><span class="kobospan" id="kobo.571.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.572.1">from</span></span><span class="kobospan" id="kobo.573.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.574.1">import</span></span><span class="kobospan" id="kobo.575.1"> load_dotenv
load_dotenv()
api_key = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.576.1">'OPENAI_API_KEY'</span></span><span class="kobospan" id="kobo.577.1">]
</span><span class="hljs-comment"><span class="kobospan" id="kobo.578.1"># create a session</span></span>
<span class="hljs-keyword"><span class="kobospan" id="kobo.579.1">async</span></span> <span class="hljs-keyword"><span class="kobospan" id="kobo.580.1">with</span></span><span class="kobospan" id="kobo.581.1"> CodeInterpreterSession() </span><span class="hljs-keyword"><span class="kobospan" id="kobo.582.1">as</span></span><span class="kobospan" id="kobo.583.1"> session:
    </span><span class="hljs-comment"><span class="kobospan" id="kobo.584.1"># generate a response based on user input</span></span><span class="kobospan" id="kobo.585.1">
    response = </span><span class="hljs-keyword"><span class="kobospan" id="kobo.586.1">await</span></span><span class="kobospan" id="kobo.587.1"> session.generate_response(
        </span><span class="hljs-string"><span class="kobospan" id="kobo.588.1">"Generate a plot of the evolution of Covid-19 from March to June 2020, taking data from web."</span></span><span class="kobospan" id="kobo.589.1">
    )
    </span><span class="hljs-comment"><span class="kobospan" id="kobo.590.1"># output the response</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.591.1">print</span></span><span class="kobospan" id="kobo.592.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.593.1">"AI: "</span></span><span class="kobospan" id="kobo.594.1">, response.content)
    </span><span class="hljs-keyword"><span class="kobospan" id="kobo.595.1">for</span></span><span class="kobospan" id="kobo.596.1"> file </span><span class="hljs-keyword"><span class="kobospan" id="kobo.597.1">in</span></span><span class="kobospan" id="kobo.598.1"> response.files:
        file.show_image()
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.599.1">Here is the generated</span><a id="_idIndexMarker707" class="calibre3"/><span class="kobospan" id="kobo.600.1"> output, including a graph that shows the number of global confirmed cases in the specified time period:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.601.1">AI:  Here is the plot showing the evolution of global daily confirmed COVID-19 cases from March to June 2020. </span><span class="kobospan" id="kobo.601.2">As you can see, the number of cases has been increasing over time during this period. </span><span class="kobospan" id="kobo.601.3">Please note that these numbers are cumulative. </span><span class="kobospan" id="kobo.601.4">Each point on the graph represents the total number of confirmed cases up to that date, not just the new cases on that day.
</span></code></pre>
<figure class="mediaobject"><span class="kobospan" id="kobo.602.1"><img alt="A graph with a line going up  Description automatically generated" src="../Images/B21714_09_10.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.603.1">Figure 9.10: Line chart generated by the Code Intepreter API</span></p>
<p class="normal1"><span class="kobospan" id="kobo.604.1">As you can see, the Code Interpreter</span><a id="_idIndexMarker708" class="calibre3"/><span class="kobospan" id="kobo.605.1"> answered the question with an explanation as well as a plot.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.606.1">Let’s try another one, this time also leveraging its real-time capabilities of searching for up-to-date information. </span><span class="kobospan" id="kobo.606.2">In the following snippet, we ask the model to plot the price of the S&amp;P 500 index over the last 5 days:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.607.1">async</span></span> <span class="hljs-keyword"><span class="kobospan" id="kobo.608.1">with</span></span><span class="kobospan" id="kobo.609.1"> CodeInterpreterSession() </span><span class="hljs-keyword"><span class="kobospan" id="kobo.610.1">as</span></span><span class="kobospan" id="kobo.611.1"> session:
    </span><span class="hljs-comment"><span class="kobospan" id="kobo.612.1"># generate a response based on user input</span></span><span class="kobospan" id="kobo.613.1">
    response = </span><span class="hljs-keyword"><span class="kobospan" id="kobo.614.1">await</span></span><span class="kobospan" id="kobo.615.1"> session.generate_response(
        </span><span class="hljs-string"><span class="kobospan" id="kobo.616.1">"Generate a plot of the price of S&amp;P500 index in the last 5 days."</span></span><span class="kobospan" id="kobo.617.1">
    )
    </span><span class="hljs-comment"><span class="kobospan" id="kobo.618.1"># output the response</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.619.1">print</span></span><span class="kobospan" id="kobo.620.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.621.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.622.1">AI: "</span></span><span class="kobospan" id="kobo.623.1">, response.content)
    </span><span class="hljs-keyword"><span class="kobospan" id="kobo.624.1">for</span></span><span class="kobospan" id="kobo.625.1"> file </span><span class="hljs-keyword"><span class="kobospan" id="kobo.626.1">in</span></span><span class="kobospan" id="kobo.627.1"> response.files:
        file.show_image()
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.628.1">We then get the following</span><a id="_idIndexMarker709" class="calibre3"/><span class="kobospan" id="kobo.629.1"> output, together with a line graph showing the price of the S&amp;P 500 index over the last 5 days:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.630.1">AI:  Here is the plot of the S&amp;P 500 index for the last 5 days. </span><span class="kobospan" id="kobo.630.2">The y-axis represents the closing price of the index, and the x-axis represents the date.
</span></code></pre>
<figure class="mediaobject"><span class="kobospan" id="kobo.631.1"><img alt="A graph with numbers and a line  Description automatically generated" src="../Images/B21714_09_11.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.632.1">Figure 9.11: S&amp;P 500 index price plotted by the Code Interpreter API</span></p>
<p class="normal1"><span class="kobospan" id="kobo.633.1">Finally, we can provide local files to the Code Interpreter so that it can perform some analyses on that specific data. </span><span class="kobospan" id="kobo.633.2">For example, I’ve downloaded the Titanic dataset from Kaggle at </span><a href="https://www.kaggle.com/datasets/brendan45774/test-file" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.634.1">https://www.kaggle.com/datasets/brendan45774/test-file</span></span></a><span class="kobospan" id="kobo.635.1">. </span><span class="kobospan" id="kobo.635.2">The Titanic dataset is a popular dataset for machine learning that describes the survival status of individual passengers on the Titanic. </span><span class="kobospan" id="kobo.635.3">It contains information such as age, sex, class, fare, and whether they survived or not.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.636.1">Once the dataset had downloaded, I passed</span><a id="_idIndexMarker710" class="calibre3"/><span class="kobospan" id="kobo.637.1"> it as a parameter to the model as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.638.1">from</span></span><span class="kobospan" id="kobo.639.1"> codeinterpreterapi </span><span class="hljs-keyword"><span class="kobospan" id="kobo.640.1">import</span></span><span class="kobospan" id="kobo.641.1"> CodeInterpreterSession, File
</span><span class="hljs-comment"><span class="kobospan" id="kobo.642.1">#os.environ["HUGGINGFACEHUB_API_TOKEN"]</span></span><span class="kobospan" id="kobo.643.1">
os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.644.1">'OPENAI_API_KEY'</span></span><span class="kobospan" id="kobo.645.1">] = </span><span class="hljs-string"><span class="kobospan" id="kobo.646.1">"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"</span></span><span class="kobospan" id="kobo.647.1">
os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.648.1">'VERBOSE'</span></span><span class="kobospan" id="kobo.649.1">] = </span><span class="hljs-string"><span class="kobospan" id="kobo.650.1">"True"</span></span>
<span class="hljs-keyword"><span class="kobospan" id="kobo.651.1">async</span></span> <span class="hljs-keyword"><span class="kobospan" id="kobo.652.1">with</span></span><span class="kobospan" id="kobo.653.1"> CodeInterpreterSession() </span><span class="hljs-keyword"><span class="kobospan" id="kobo.654.1">as</span></span><span class="kobospan" id="kobo.655.1"> session:
        </span><span class="hljs-comment"><span class="kobospan" id="kobo.656.1"># define the user request</span></span><span class="kobospan" id="kobo.657.1">
        user_request = </span><span class="hljs-string"><span class="kobospan" id="kobo.658.1">"Analyze this dataset and plot something interesting about it."</span></span><span class="kobospan" id="kobo.659.1">
        files = [
            File.from_path(</span><span class="hljs-string"><span class="kobospan" id="kobo.660.1">"drive/MyDrive/titanic.csv"</span></span><span class="kobospan" id="kobo.661.1">),
        ]
        </span><span class="hljs-comment"><span class="kobospan" id="kobo.662.1"># generate the response</span></span><span class="kobospan" id="kobo.663.1">
        response = </span><span class="hljs-keyword"><span class="kobospan" id="kobo.664.1">await</span></span><span class="kobospan" id="kobo.665.1"> session.generate_response(
            user_request, files=files
        )
        </span><span class="hljs-comment"><span class="kobospan" id="kobo.666.1"># output to the user</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.667.1">print</span></span><span class="kobospan" id="kobo.668.1">(</span><span class="hljs-string"><span class="kobospan" id="kobo.669.1">"AI: "</span></span><span class="kobospan" id="kobo.670.1">, response.content)
        </span><span class="hljs-keyword"><span class="kobospan" id="kobo.671.1">for</span></span><span class="kobospan" id="kobo.672.1"> file </span><span class="hljs-keyword"><span class="kobospan" id="kobo.673.1">in</span></span><span class="kobospan" id="kobo.674.1"> response.files:
            file.show_image()
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.675.1">We then get the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.676.1">AI:  The plot shows the survival count based on the passenger class. </span><span class="kobospan" id="kobo.676.2">It appears that passengers in the 3rd class had a significantly lower survival rate compared to those in the 1st and 2nd classes. </span><span class="kobospan" id="kobo.676.3">This could suggest that the class of the passengers might have influenced their survival, possibly due to factors such as the location of their cabins and access to lifeboats.
</span><span class="kobospan" id="kobo.676.4">These are just a few examples of the kind of insights we can extract from this dataset. </span><span class="kobospan" id="kobo.676.5">Depending on the specific questions you're interested in, we could perform further analysis. </span><span class="kobospan" id="kobo.676.6">For example, we could look at the survival rate based on age, or investigate whether the fare passengers paid had any influence on their survival.
</span></code></pre>
<figure class="mediaobject"><span class="kobospan" id="kobo.677.1"><img alt="A screenshot of a graph  Description automatically generated" src="../Images/B21714_09_12.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.678.1">Figure 9.12: Sample plots generated by the Code Interpreter API</span></p>
<p class="normal1"><span class="kobospan" id="kobo.679.1">As you can see, the model was able to generate to bar charts showing the survival status grouped by sex (in the first plot) and then by class (in the second plot).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.680.1">The Code Interpreter plugin, together</span><a id="_idIndexMarker711" class="calibre3"/><span class="kobospan" id="kobo.681.1"> with code-specific LLMs and the Python agent, are great examples of how LLMs are having a huge impact on the world of software development. </span><span class="kobospan" id="kobo.681.2">This can be summarized in two main capabilities:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.682.1">LLMs can understand and generate code, since they have been trained on a huge amount of programming languages, GitHub repos, StackOverflow conversations, and so on. </span><span class="kobospan" id="kobo.682.2">Henceforth, along with natural language, programming languages are part of their parametric knowledge.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.683.1">LLMs can understand a user’s intent and act as a reasoning engine to activate tools like Python REPL or Code Interpreter, which are then able to provide a response by working with code.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.684.1">Overall, LLMs are going well beyond the elimination of the gap between natural language and machine language: rather, they are integrating the two so that they can leverage each other to respond to a user’s query.</span></p>
<h1 class="heading" id="_idParaDest-134"><span class="kobospan" id="kobo.685.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.686.1">In this chapter, we explored multiple ways in which LLMs can be leveraged to work with code. </span><span class="kobospan" id="kobo.686.2">Armed with a refresher of how to evaluate LLMs and the specific evaluation benchmarks to take into account when choosing an LLM for code-related tasks, we delved into practical experimentations.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.687.1">We started from the “plain vanilla” application that we have all tried at least once using ChatGPT, which is code understanding and generation. </span><span class="kobospan" id="kobo.687.2">For this purpose, we leveraged three different models – Falcon LLM, CodeLlama, and StarCoder – each resulting in very good results.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.688.1">We then moved forward with the additional applications that LLMs’ coding capabilities can have in the real world. </span><span class="kobospan" id="kobo.688.2">In fact, we saw how code-specific knowledge can be used as a booster to solve complex problems, such as algorithmic or optimization tasks. </span><span class="kobospan" id="kobo.688.3">Furthermore, we covered how code knowledge can not only be used in the backend reasoning of an LLM but also actually executed in a working notebook, leveraging the open-source version of the Code Interpreter API.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.689.1">With this chapter, we are getting closer to the end of Part 2. </span><span class="kobospan" id="kobo.689.2">So far, we have covered the multiple capabilities of LLMs, while always handling language data (natural or code). </span><span class="kobospan" id="kobo.689.3">In the next chapter, we will see how to go a step further toward multi-modality and build powerful multi-modal agents that can handle data in multiple formats.</span></p>
<h1 class="heading" id="_idParaDest-135"><span class="kobospan" id="kobo.690.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.691.1">The open-source version of the Code Interpreter API: </span><a href="https://github.com/shroominic/codeinterpreter-api" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.692.1">https://github.com/shroominic/codeinterpreter-api</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.693.1">StarCoder: </span><a href="https://huggingface.co/blog/starcoder" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.694.1">https://huggingface.co/blog/starcoder</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.695.1">The LangChain agent for the Python REPL: </span><a href="https://python.langchain.com/docs/integrations/toolkits/python" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.696.1">https://python.langchain.com/docs/integrations/toolkits/python</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.697.1">A LangChain blog about the Code Interpreter API: </span><a href="https://blog.langchain.dev/code-interpreter-api/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.698.1">https://blog.langchain.dev/code-interpreter-api/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.699.1">The Titanic dataset: </span><a href="https://www.kaggle.com/datasets/brendan45774/test-file" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.700.1">https://www.kaggle.com/datasets/brendan45774/test-file</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.701.1">The HF Inference Endpoint: </span><a href="https://huggingface.co/docs/inference-endpoints/index" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.702.1">https://huggingface.co/docs/inference-endpoints/index</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.703.1">The CodeLlama model card: </span><a href="https://huggingface.co/codellama/CodeLlama-7b-hf" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.704.1">https://huggingface.co/codellama/CodeLlama-7b-hf</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.705.1">Code Llama: Open Foundation Models for Code, </span><em class="italic"><span class="kobospan" id="kobo.706.1">Rozière. </span><span class="kobospan" id="kobo.706.2">B., et al</span></em><span class="kobospan" id="kobo.707.1"> (2023): </span><a href="https://arxiv.org/abs/2308.12950" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.708.1">https://arxiv.org/abs/2308.12950</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.709.1">The Falcon LLM model card: </span><a href="https://huggingface.co/tiiuae/falcon-7b-instruct" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.710.1">https://huggingface.co/tiiuae/falcon-7b-instruct</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.711.1">The StarCoder model card: </span><a href="https://huggingface.co/bigcode/starcoder" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.712.1">https://huggingface.co/bigcode/starcoder</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.713.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.714.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.715.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.716.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>