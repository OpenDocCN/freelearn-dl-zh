["```py\nfrom wrappers import *\n```", "```py\nenv_id = 'PongNoFrameskip-v4'\nenv = make_atari(env_id)\nenv = wrap_deepmind(env)\nenv = wrap_pytorch(env)\n```", "```py\nepsilon_start = 1.0\nepsilon_final = 0.01\nepsilon_decay = 30000\n\nepsilon_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1\\. * episode / epsilon_decay)\n\nplt.plot([epsilon_by_episode(i) for i in range(1000000)])\nplt.show()\n```", "```py\nmodel = CnnDQN(env.observation_space.shape, env.action_space.n)\noptimizer = optim.Adam(model.parameters(), lr=0.00001)\n\nreplay_start = 10000\nreplay_buffer = ReplayBuffer(100000)\n```", "```py\nepisodes = 1400000\n```", "```py\nif episode % 200000 == 0:\n  plot(episode, all_rewards, losses) \n```", "```py\nclass CnnDQN(nn.Module):\n def __init__(self, input_shape, num_actions):\n   super(CnnDQN, self).__init__()\n\n   self.input_shape = input_shape\n   self.num_actions = num_actions\n\n   self.features = nn.Sequential(\n     nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n     nn.ReLU(),\n     nn.Conv2d(32, 64, kernel_size=4, stride=2),\n    nn.ReLU(),\n     nn.Conv2d(64, 64, kernel_size=3, stride=1),\n     nn.ReLU())\n   self.fc = nn.Sequential(\n     nn.Linear(self.feature_size(), 512),\n     nn.ReLU(),\n     nn.Linear(512, self.num_actions))\n\n  def forward(self, x):\n    x = self.features(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n    return x\n\n def feature_size(self): \n   return self.features(autograd.Variable(torch.zeros(1,\n     *self.input_shape))).view(1, -1).size(1)\n\n  def act(self, state, epsilon):\n    if random.random() > epsilon:\n      state = autograd.Variable(torch.FloatTensor(\n        np.float32(state)).unsqueeze(0), volatile=True)\n      q_value = self.forward(state)\n      action = q_value.max(1)[1].data[0]\n    else:\n      action = random.randrange(env.action_space.n)\n    return action\n```", "```py\nself.features = nn.Sequential(\n     nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n```", "```py\nself.fc = nn.Sequential(\n     nn.Linear(self.feature_size(), 512),\n```", "```py\ndef forward(self, x):\n  x = self.features(x)\n  x = x.view(x.size(0), -1)\n  x = self.fc(x)\n  return x\n```", "```py\ncurrent_model = DQN(env.observation_space.shape[0], env.action_space.n)\ntarget_model = DQN(env.observation_space.shape[0], env.action_space.n)\n\noptimizer = optim.Adam(current_model.parameters())\n```", "```py\ndef update_target(current_model, target_model):\n  target_model.load_state_dict(current_model.state_dict())\n\nupdate_target(current_model, target_model)\n```", "```py\ndef compute_td_loss(batch_size):\n state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n\n state = autograd.Variable(torch.FloatTensor(np.float32(state)))\n next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)),\n   volatile=True)\n action = autograd.Variable(torch.LongTensor(action))\n reward = autograd.Variable(torch.FloatTensor(reward))\n done = autograd.Variable(torch.FloatTensor(done))\n\n q_values = current_model(state)\n next_q_values = current_model(next_state)\n next_q_state_values = target_model(next_state)\n\n q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1) \n next_q_value = next_q_state_values.gather(1,\n torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n expected_q_value = reward + gamma * next_q_value * (1 - done)\n\n loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n\n optimizer.zero_grad()\n loss.backward()\n optimizer.step()\n\n return loss\n```", "```py\naction = current_model.act(state, epsilon)\n```", "```py\nif episode % 500 == 0:\n update_target(current_model, target_model)\n```", "```py\ndef play_game():\n done = False\n state = env.reset()\n while(not done):\n   action = current_model.act(state, epsilon_final)\n   next_state, reward, done, _ = env.step(action)\n   env.render()\n   state = next_state\n```", "```py\nclass DDQN(nn.Module):\n def __init__(self, num_inputs, num_outputs):\n   super(DDQN, self).__init__() \n\n   self.feature = nn.Sequential(\n     nn.Linear(num_inputs, 128),\n     nn.ReLU())\n\n   self.advantage = nn.Sequential(\n     nn.Linear(128, 128),\n     nn.ReLU(),\n     nn.Linear(128, num_outputs))\n\n   self.value = nn.Sequential(\n     nn.Linear(128, 128),\n     nn.ReLU(),\n     nn.Linear(128, 1))\n\n def forward(self, x):\n   x = self.feature(x)\n   advantage = self.advantage(x)\n   value = self.value(x)\n   return value + advantage - advantage.mean()\n\n def act(self, state, epsilon):\n   if random.random() > epsilon:\n     state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0),\n       volatile=True)\n     q_value = self.forward(state)\n     action = q_value.max(1)[1].item() \n  else:\n    action = random.randrange(env.action_space.n)\n  return action\n```", "```py\nreturn value + advantage - advantage.mean()\n```", "```py\ncurrent_model = DDQN(env.observation_space.shape[0], env.action_space.n)\ntarget_model = DDQN(env.observation_space.shape[0], env.action_space.n)\n```", "```py\nq_values = current_model(state)\nnext_q_values = target_model(next_state)\n\nq_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\nnext_q_value = next_q_values.max(1)[0]\nexpected_q_value = reward + gamma * next_q_value * (1 - done)\n\nloss = (q_value - expected_q_value.detach()).pow(2).mean()\n```", "```py\nclass NaivePrioritizedBuffer(object):\n def __init__(self, capacity, prob_alpha=0.6):\n   self.prob_alpha = prob_alpha\n   self.capacity = capacity\n   self.buffer = []\n   self.pos = 0\n   self.priorities = np.zeros((capacity,), dtype=np.float32)\n\n def push(self, state, action, reward, next_state, done):\n   assert state.ndim == next_state.ndim\n   state = np.expand_dims(state, 0)\n   next_state = np.expand_dims(next_state, 0)\n   max_prio = self.priorities.max() if self.buffer else 1.0\n\n   if len(self.buffer) < self.capacity:\n     self.buffer.append((state, action, reward, next_state, done))\n   else:\n     self.buffer[self.pos] = (state, action, reward, next_state, done)\n\n   self.priorities[self.pos] = max_prio\n   self.pos = (self.pos + 1) % self.capacity\n\n def sample(self, batch_size, beta=0.4):\n   if len(self.buffer) == self.capacity:\n     prios = self.priorities\n   else:\n     prios = self.priorities[:self.pos]\n\n   probs = prios ** self.prob_alpha\n   probs /= probs.sum()\n\n   indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n   samples = [self.buffer[idx] for idx in indices]\n\n   total = len(self.buffer)\n   weights = (total * probs[indices]) ** (-beta)\n   weights /= weights.max()\n   weights = np.array(weights, dtype=np.float32)\n\n   batch = list(zip(*samples))\n   states = np.concatenate(batch[0])\n   actions = batch[1]\n   rewards = batch[2]\n   next_states = np.concatenate(batch[3])\n   dones = batch[4]\n\n   return states, actions, rewards, next_states, dones, indices, weights\n\n  def update_priorities(self, batch_indices, batch_priorities):\n    for idx, prio in zip(list(batch_indices), [batch_priorities]):\n      self.priorities[idx] = prio\n\n  def __len__(self):\n    return len(self.buffer)\n```", "```py\nbeta_start = 0.4\nbeta_episodes = episodes / 10 \nbeta_by_episode = lambda episode: min(1.0,\n  beta_start + episode * (1.0 - beta_start) / beta_episodes)\n\nplt.plot([beta_by_episode(i) for i in range(episodes)])\n```", "```py\ndef compute_td_loss(batch_size, beta):\n  state, action, reward, next_state, done, indices, \n weights = replay_buffer.sample(batch_size, beta)\n\n  state = autograd.Variable(torch.FloatTensor(np.float32(state)))\n  next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)))\n  action = autograd.Variable(torch.LongTensor(action))\n  reward = autograd.Variable(torch.FloatTensor(reward))\n  done = autograd.Variable(torch.FloatTensor(done))\n  weights = autograd.Variable(torch.FloatTensor(weights))\n\n  q_values = current_model(state)\n  next_q_values = target_model(next_state)\n\n  q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n  next_q_value = next_q_values.max(1)[0]\n  expected_q_value = reward + gamma * next_q_value * (1 - done)\n\n  loss = (q_value - expected_q_value.detach()).pow(2).mean()\n  prios = loss + 1e-5\n  loss = loss.mean()\n\n  optimizer.zero_grad()\n  loss.backward()\n  replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n  optimizer.step()\n\n  return loss\n```", "```py\nif done:\n  if episode > buffer_size and avg_reward > min_play_reward:\n    play_game() \n  state = env.reset()\n  all_rewards.append(episode_reward)\n  episode_reward = 0\n```", "```py\nif len(replay_buffer) > batch_size:\n  beta = beta_by_episode(episode)\n  loss = compute_td_loss(batch_size, beta)\n  losses.append(loss.item())\n```"]