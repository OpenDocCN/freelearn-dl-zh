- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Integrating Dynamic RAG into the GenAISys
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A business-ready **generative AI system** (**GenAISys**) needs to be flexible
    and ready to face the rapidly evolving landscape of the AI market. The AI controller
    acts as an adaptive orchestrator for e-marketing, production, storage, distribution,
    and support, but to satisfy such a range of tasks, we need a **retrieval-augmented
    generation** (**RAG**) framework. In the previous chapter, we built a conversational
    AI agent and a function for similarity search for instruction scenarios (AI orchestrator)
    for a generative AI model. In this chapter, we will enhance that foundation and
    build a scalable RAG in a Pinecone index to integrate both instruction scenarios
    and classical data, which the generative AI model will connect to.
  prefs: []
  type: TYPE_NORMAL
- en: We make a clear distinction in this chapter between **instruction scenarios**—expert-crafted
    prompt fragments (or *task tags*, as explained in the previous chapter) that tell
    the model *how* to reason or act—and **classical data**—the reference material
    the RAG system retrieves to ground its answers.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need this dynamic and adaptive RAG framework with vectorized scenarios
    of instructions on top of classical data? Because the global market affects entities
    internally and externally. For example, a hurricane can cause electricity shortages,
    putting the supply chain of businesses in peril. Businesses might have to relocate
    supply routes, production, or distribution. General-purpose AI cloud platforms
    might do some of the job. But more often than not, we will need to provide custom,
    domain-specific functionality. For that reason, we need a dynamic set of instructions
    in a vector store repository as we do for RAG data.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by defining the architecture scenario-driven task executions for
    a generative AI model, in this case, GPT-4o, through a Pinecone index. We will
    carefully go through the cost-benefits of investing in intelligent scenarios for
    the generative model through similarity search and retrieval. We will introduce
    a dynamic framework to produce ChatGPT-like capabilities that we will progressively
    introduce in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Once the architecture is defined, we will first build a Pinecone index to chunk,
    embed, and upsert instruction scenarios. We will make sure the GenAISys vector
    store can embed a query and find a relevant instruction scenario. This capability
    will be a key component in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110), *Building
    the AI Controller Orchestration Interface*, when we design the conversational
    agent’s interface and orchestrator. Finally, we will write a program to upsert
    classical data in a RAG environment to the same Pinecone index alongside the instruction
    scenarios. Differentiation between scenarios and classical data will be maintained
    using distinct namespaces. By the end of this chapter, we will have built the
    main components to link instructions to a generative AI model. We will be ready
    to design a user interface and AI controller orchestrator in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110).
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Architecting RAG for the dynamic retrieval of instructions and data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The law of diminishing returns when developing similarity searches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining the architecture of a hybrid GenAISys CoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Pinecone index by chunking, embedding, and upserting instruction
    scenarios
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing a Pinecone index with classical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying the Pinecone index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our first task is to architect a RAG framework for dynamic retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Architecting RAG for dynamic retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will define a Pinecone index that stores both instruction
    scenarios and classical data. This structure gives GenAISys dynamic, cost-effective
    retrieval: the instruction scenarios steer the generative AI model (GPT-4o in
    our example), while the classical data supplies the factual context used by the
    RAG pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will go through the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario-driven task execution**: Designing optimized instructional prompts
    (“scenarios”) that we will upsert to the Pinecone index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost-benefit strategies**: Considering the law of diminishing returns to
    avoid overinvesting in automation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partitioning Pinecone with namespaces**: Using Pinecone index namespaces
    to clearly differentiate instruction scenarios from classical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hybrid retrieval framework**: Implementing implicit vector similarity searches
    but also triggering explicit instructions for the generative AI model (more on
    this in the *Scenario-driven task execution* section).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoT loops**: Explaining how the flexibility of the scenario selection process
    will lead to loops of generative AI functions before finally producing an output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GenAISys framework**: Laying the groundwork for the advanced GenAISys framework
    we are building throughout the book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s first dive deeper into scenario-driven task execution.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario-driven task execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we saw two complementary ways the AI controller can
    pick what to do next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Implicit selection**: The controller embeds the user’s prompt, runs a semantic
    similarity search across its scenario library, and chooses the closest match without
    any task tag. This gave us flexible, code-free orchestration (e.g., it automatically
    chose a sentiment-analysis scenario for the *Gladiator II* review).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explicit selection**: The desired task is spelled out, either as a task tag
    in the prompt or as a user interface action, such as “Run web search.” Here, the
    controller skips the similarity search and jumps straight to the requested tool
    or workflow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'That same pattern continues in this chapter, but at a larger scale. Instead
    of a few hand-picked prompts, we manage hundreds or even thousands of expert-authored
    instruction scenarios stored in a vector database; instead of single-user experiments,
    we support many concurrent users and workflows. This scenario-driven (implicit)
    approach has three advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Professional experts typically create these advanced prompts/instruction scenarios,
    often surpassing the expertise level of mainstream users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scenarios can be co-designed by AI specialists and subject-matter experts,
    covering a wide range of activities in a corporation, from sales to delivery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The order of execution of the scenarios is prompt-driven, flexible, and unordered.
    This dynamic approach avoids hardcoding the order of the tasks, increasing adaptability
    as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, while implicit task planning maximizes flexibility, as we move toward
    building business-ready systems, we must balance flexibility with cost-efficiency.
    In some cases, therefore, *explicit* instructions, such as triggering a web search
    by selecting the option in the user interface, can significantly reduce the potential
    costs, as shown in *Figure 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Diminishing returns as costs increase](img/B32304_03_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Diminishing returns as costs increase'
  prefs: []
  type: TYPE_NORMAL
- en: 'The more we automate implicit scenarios that the generative AI model will select
    with vector similarity searches in the Pinecone index, the higher the cost. To
    manage this, we must carefully consider the law of diminishing returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B32304_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, as illustrated in *Figure 3.1*, in theoretical units, we
    have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B32304_03_002.png) represents the overall benefit, which is represented
    by roughly 15 when the cost reaches 50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B32304_03_003.png) represents the initial benefit of storing instruction
    scenarios in the Pinecone index and asking the generative AI model to select one
    through vector similarity with the user input. In this case, it is nearly 1 benefit
    unit for 1 cost unit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B32304_03_004.png) is the rate at which the benefit begins to increase
    as we increase the cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B32304_03_005.png) represents the cost measured in theoretical units
    (currency, human resources, or computational resources).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B32304_03_006.png) denotes the rate at which returns diminish as cost
    increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, when the cost reaches 7 theoretical units, the benefit reaches
    7 theoretical units. This 1 unit of cost generating 1 unit of benefit is reasonable.
    However, when the benefit reaches 10 units, the cost could double to 14 units,
    which signals that something is going wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 'The diminishing ![](img/B32304_03_006.png) factor has a strong negative impact
    ![](img/B32304_03_008.png) on the benefits through squared costs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B32304_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: We will carefully monitor the factor ![](img/B32304_03_010.png) as we move through
    the use cases in this book. We will have to make choices between running implicit
    automated scenario selections through the Pinecone index and explicitly triggering
    actions through predefined instructions in the prompt itself.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore how we identify instruction scenarios within a Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid retrieval and CoT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our first step is teaching the GenAISys framework to distinguish clearly between
    classical data and instruction scenarios. To achieve this, we will separate the
    instruction scenarios and the data with two namespaces within the same Pinecone
    index, named `genai-v1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`genaisys` will contain instruction vectors of information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data01` will contain data vectors of information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will implement `genai-v1` in code with additional explanations in the *Creating
    the* *Pinecone index* section of this chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Once the Pinecone index has been partitioned into scenarios and data, we can
    take the GenAISys to another level with hybrid retrieval, as shown in *Figure
    3.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: AI controller orchestrating GenAISys](img/B32304_03_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: AI controller orchestrating GenAISys'
  prefs: []
  type: TYPE_NORMAL
- en: 'The hybrid retrieval framework depicted in the preceding figure will enable
    GenAISys to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the generative AI model using processed, chunked, and embedded data memory
    (see **1**–**3** in *Figure 3.2*) directly without going through the Pinecone
    index (see **3B**). This will reduce costs for ephemeral data, for example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the generative AI model after the chunked and embedded data is upserted
    to the Pinecone index either as a set of instructions in a scenario or as classical
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a CoT loop between the Pinecone index (see **3B**) and the generative
    AI model controller as an orchestrator. For example, the output of the model can
    serve as input for another CoT cycle that will retrieve scenarios or data from
    the Pinecone index. ChatGPT-like copilots often present their output and then
    finish by asking whether you’d like to explore further—sometimes even suggesting
    ready-made follow-up prompts you can click on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoT loops can operate implicitly via vector similarity search or explicitly
    via direct instruction triggers or task tags (such as “Run a web search”). For
    example, ChatGPT-like copilots can trigger web searches directly through the user
    interface or rules in the AI controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll begin building our GenAISys in this chapter and continue refining it
    over the next few chapters. Starting from [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110),
    *Building the AI Controller Orchestration Interface*, we’ll use the RAG foundations
    introduced here to develop the hybrid retrieval framework shown in *Figure 3.2*.
    The GenAISys we’re building will include dynamic process management—a requisite
    for keeping pace with the shifting market conditions. Specifically, our GenAISys
    will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Leverage Pinecone’s vector database or in-memory chunked and embedded information
    with similarity searches to optimize retrieval (instructions or data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explicitly trigger direct instructions, such as a web search, and include them
    in a CoT loop for summarization, sentiment analysis, or semantic analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Break down complex sets of instructions and data retrieval into manageable steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteratively refine solutions in a human-like thought process before producing
    an output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the best out of generative AI models, including OpenAI’s reasoning models
    such as o3, by providing them with an optimized instruction scenario
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our initial step in this chapter is building the `genai-v1` Pinecone index,
    which the AI controller will use to manage instruction scenarios within the `genaisys`
    namespace. Then, we’ll demonstrate how to chunk, embed, and upsert classical data
    into the `data01` namespace. Let’s get moving!
  prefs: []
  type: TYPE_NORMAL
- en: Building a dynamic Pinecone index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll focus on creating a Pinecone index designed to manage both instruction
    scenarios and classical data. In the upcoming sections, we’ll begin upserting
    the instruction scenarios as well as classical data. The workflow breaks down
    into three straightforward stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment for OpenAI and Pinecone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing the data, chunking it, and then embedding it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing the Pinecone index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open the `Pinecone_instruction_scenarios.ipynb` notebook within the Chapter03
    directory on GitHub ([https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main](https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main)).
    Our first task is to set up the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we move through the book, we will continually reuse functions and features
    implemented in *Chapters 1* and *2*, add new ones for Pinecone, and organize the
    installations into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenAI using the same process as in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021).
    Refer back to that chapter if needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Pinecone for this and following chapters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To begin, download the files we need by retrieving `grequests.py` from the
    GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To install OpenAI, follow the same steps as in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021).
    We’ll move on to install Pinecone now, which we will refer to in upcoming chapters
    throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Pinecone
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download the Pinecone requirements file that contains the instructions for
    the Pinecone version we want to use throughout the book. If another version is
    required, this will be the only file that needs to be updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3-PPMUMLAP0325.png)**Quick tip**: Enhance your coding experience with
    the **AI Code Explainer** and **Quick Copy** features. Open this book in the next-gen
    Packt Reader. Click the **Copy** button'
  prefs: []
  type: TYPE_NORMAL
- en: (**1**) to quickly copy code into your coding environment, or click the **Explain**
    button
  prefs: []
  type: TYPE_NORMAL
- en: (**2**) to get the AI assistant to explain a block of code to you.
  prefs: []
  type: TYPE_NORMAL
- en: '![A white background with a black text  AI-generated content may be incorrect.](img/image_%282%29.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4.png)**The next-gen Packt Reader** is included for free with the purchase
    of this book. Scan the QR code OR visit [packtpub.com/unlock](http://packtpub.com/unlock),
    then use the search bar to find this book by name. Double-check the edition shown
    to make sure you get the right one.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A qr code on a white background  AI-generated content may be incorrect.](img/Unlock_Code1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The file contains the installation function, which we will call with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The script is the same as the one for OpenAI described in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021),
    but adapted to Pinecone. We first uninstall Pinecone and then install the version
    we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we verify the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows we have successfully installed the client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let’s go ahead and initialize the Pinecone API key.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the Pinecone API key
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The program now downloads `pinecone_setup.py`, which we will use to initialize
    the Pinecone API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This setup mirrors the Google Colab secrets-based approach we used for OpenAI
    in [*Chapter 1*](Chapter_1.xhtml#_idTextAnchor021), but it’s adapted here for
    initializing the Pinecone API.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If Google secrets was set to `True` for OpenAI in the OpenAI section of this
    notebook, then the Pinecone setup function will be called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If Google secrets was set to `False`, then you can implement a custom function
    by uncommenting the code and entering the Pinecone API key with any method you
    wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The program is now ready to process the data we will upsert to the Pinecone
    index.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our goal now is to prepare the scenarios for storage and retrieval so that
    we can then query the Pinecone index. The main steps of the process are represented
    in *Figure 3.2*, which is only one layer of the roadmap for the following chapters.
    We will process the data in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data loading and preparation**, in which the data will be broken into smaller
    parts. In this case, each scenario will be stored in one line of a scenario list,
    which will prepare the chunking process. We will not always break text into lines,
    however, as we will see in the *Upserting classical data into the index* section
    later.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Chunking functionality** to store each line of scenarios into chunks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Embedding** the chunks of text obtained.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Verification** to ensure that we embedded the corresponding number of chunks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s now cover the first two steps: loading and preparing the data, followed
    by chunking.'
  prefs: []
  type: TYPE_NORMAL
- en: Data loading and chunking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use the scenarios implemented in [*Chapter 2*](Chapter_2.xhtml#_idTextAnchor055).
    They are stored in a file that we will now download:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will add more scenarios throughout our journey in this book to create a
    GenAISys. For the moment, our main objective is to get our Pinecone index to work.
    The program first initializes `start_time` for time measurement. Then we load
    the lines of scenario instructions directly into `chunks` line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the code displays the number of chunks and the time it took to create
    the chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The program now verifies the first three chunks of scenario instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the three scenarios we will be working on in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The chunks of data are now ready for embedding. Let’s proceed with embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To embed the dataset, we will first initialize the embedding model and then
    embed the chunks. The program first initializes the embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the embedding model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We will be using an OpenAI embedding model to embed the data. To embed our
    data with an OpenAI model, we can choose one of three main models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text-embedding-3-small`, which is fast and has a lower resource usage. This
    is sufficient for real-time usage. It is a smaller model and is thus cost-effective.
    However, as the vector store will increase in size with complex scenarios, it
    might be less accurate for nuanced tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-embedding-3-large`, which provides high accuracy and nuanced embeddings
    and will prove effective for complex semantic similarity searches. It requires
    more resources and costs more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-embedding-ada-002`, which is cost-effective for good-quality embeddings.
    However, it’s slightly slower than models such as `text-embedding-3-small` and
    `text-embedding-3-large`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can consult the OpenAI documentation at [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)
    for more info.
  prefs: []
  type: TYPE_NORMAL
- en: 'To import a limited number of scenarios in this chapter, we will use `text-embedding-3-small`
    to optimize speed and cost. The program initializes the model while the others
    are commented for further use if needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We initialize the OpenAI client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'An embedding function is then created that will convert the text sent to it
    into embeddings. The function is designed to produce embeddings for a batch of
    input texts (`texts`) with the embeddings model of our choice, in this case, `text-embedding-3-small`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The function first cleans the text by replacing newline characters in each
    text with spaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the function makes the API embedding call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The embeddings are extracted from the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the embeddings are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The program is now ready to embed the chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding the chunks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The program first defines a function to embed the chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the function are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chunks`: The parts of text to embed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding_model`: Defines the model to use, such as `text-embedding-3-small`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: The number of chunks the function can process in a single batch,
    such as `batch_size=1000`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pause_time`: A pause time in seconds, which can be useful for rate limits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then initialize the timing function, `embeddings` variable, and counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The code is now ready to process the chunks in batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Each batch is then sent to the embedding function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The embedded batch is appended to the embeddings list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of batches is monitored and displayed and the pause is activated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Once all the batches are processed, the total time is displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The embedding function is ready to be called with the chunks list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the scenario data has been embedded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The first embedding is displayed for verification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the embeddings have been generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The final verification is to check that the number of embeddings matches the
    number of chunks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that the chunking and embedding process is most probably
    successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The chunks and embeddings are now ready to be upserted into the Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Pinecone index
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `genai-v1` Pinecone index we will create will contain two namespaces, as
    shown in *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`genaisys`: A repository of instruction scenarios. These prompts drive generative
    AI behavior and can also trigger traditional functions such as web search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Data01`: The embedded classical data that the RAG pipeline queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.3: Partitioning the Pinecone index into namespaces](img/B32304_03_3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Partitioning the Pinecone index into namespaces'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing two classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The `Pinecone` class is the primary interface to interact with the Pinecone
    index. We will use this class to configure Pinecone’s serverless services.
  prefs: []
  type: TYPE_NORMAL
- en: Before going further, you will need to set up a Pinecone account and obtain
    an API key. Make sure to verify the cost of these services at [https://www.pinecone.io/](https://www.pinecone.io/).
    This chapter is self-contained, so you can begin by reading the content, comments,
    and code before deciding on creating a Pinecone account.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once our account is set up, we need to retrieve and initialize our API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We now import the specification class, define the name of our index (`genai-v1`),
    and initialize our first namespace (`genaisys`) for our scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We now have a project management decision to make—use the Pinecone cloud to
    host our index or **Amazon Web Services** (**AWS**)?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The code first checks whether an environment variable (`PINECONE_CLOUD`) is
    set to use the Pinecone cloud. If there is no predefined environment variable
    check set, the variable defaults to AWS with `'aws'` and `'``us-east-1'` as the
    default region.
  prefs: []
  type: TYPE_NORMAL
- en: For more information, refer to the Pinecone Python SDK documentation at [https://docs.pinecone.io/reference/python-sdk](https://docs.pinecone.io/reference/python-sdk).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, AWS was chosen for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Market leadership and reliability**: AWS has a market share of over 30% of
    the global infrastructure market. As such, it is deemed reliable by a large number
    of organizations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compliance and security standards**: AWS has over 140 security standards
    for data security and privacy, including PCI-DSS and HIPAA/HITECH, FedRAMP, GDPR,
    FIPS 140-2, and NIST 800-171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: AWS has a global network of data centers, making scalability
    seamless.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, you can create an index manually in your Pinecone console to
    select the embedding model and the host, such as AWS or **Google Cloud Platform**
    (**GCP**). You can also select your pod size from x1 to more, which will determine
    the maximum size of your index. Each choice depends on your project and resource
    optimization strategy.
  prefs: []
  type: TYPE_NORMAL
- en: In any case, we need metrics to monitor usage and cost. Pinecone provides detailed
    usage metrics accessible via your account, allowing you to manage indexes efficiently.
    For example, we might want to delete information you don’t need anymore, add targeted
    data, or optimize the usage per user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pinecone provides three key metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serverless storage usage**: Measured in **gigabyte-hours** (**GB-hours**).
    The cost is calculated at 1 GB of storage per hour. Carefully monitoring the amount
    of data we store is an important factor in any AI project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless write operations units**: Measures the resources consumed by write
    operations to the Pinecone database that contains our index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serverless read operations** **units**: Measures the resources consumed by
    read operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can download detailed information on your consumption by going to your
    Pinecone account, selecting **Usage**, and then clicking on the **Download** button,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Downloading Pinecone usage data](img/B32304_03_4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Downloading Pinecone usage data'
  prefs: []
  type: TYPE_NORMAL
- en: The download file is in CSV format and contains a detailed account of our Pinecone
    usage, such as `BillingAccountId` (account identifier), `BillingAccountName` (account
    name), `OrganizationName` (organization name), `OrganizationId` (organization
    ID), `ProjectId` (project identifier), `ProjectName` (project name), `ResourceId`
    (resource identifier), `ResourceName` (resource name), `ChargePeriodStart` (charge
    start date), `ChargePeriodEnd` (charge end date), `BillingPeriodStart` (billing
    start date), `BillingPeriodEnd` (billing end date), `SkuId` (SKU identifier),
    `SkuPriceId` (SKU price ID), `ServiceName` (service name), `ChargeDescription`
    (charge details), `CloudId` (cloud provider), `RegionId` (region), `Currency`
    (currency type), `PricingQuantity` (usage quantity), `PricingUnit` (usage unit),
    `ListCost` (listed cost), `EffectiveCost` (calculated cost), `BilledCost` (final
    cost), and `Metadata` (additional data).
  prefs: []
  type: TYPE_NORMAL
- en: As AI slowly enters its industrial age, straying away from the initial excitement
    of the early 2020s, continuous monitoring of these metrics becomes increasingly
    critical.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now check whether the index we selected exists or not. The program
    imports the `pinecone` and `time` classes to insert a sleep time before checking
    whether the index exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'If the index exists, the following code will be skipped to avoid creating duplicate
    indexes. If not, an index is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`index_name`, which is the name of our Pinecone index, `genai-v1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dimension=1536`, the dimensionality of the embedding vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metric=''cosine''`, which sets the distance metric for similarity searches
    to cosine similarity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec=spec`, which defines the region and the serverless specification we defined
    previously for the cloud services'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`time.sleep(1)`, which makes the program wait to make sure the index is fully
    created before continuing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the index has just been created, the output shows its details with `total_vector_count`
    set to `0` (if you see a number other than `0`, the notebook has likely already
    been run):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If the index already exists, the statistics will be displayed, including `index_fullness`
    to monitor the space used in your index pod from 0 to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, we haven’t populated the index yet. We can connect to the index
    we just created and display its statistics before populating it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the information, confirming that we are connected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The selected embedding model must match Pinecone’s index dimension (`1536`).
    We will create the parameters of a Pinecone index interactively when we begin
    working on use cases in [*Chapter 5*](Chapter_5.xhtml#_idTextAnchor140). Here,
    we are using `embedding_model="text-embedding-3-small` with its 1,536 dimensions,
    which matches the dimension of the Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that the `'genaisys'` namespace we initialized is taken into account.
    This ensures that when we upsert the scenarios we designed, they will not be confused
    with the classical data that is in another namespace of the same index. We are
    now ready to upsert the data to our Pinecone index.
  prefs: []
  type: TYPE_NORMAL
- en: Upserting instruction scenarios into the index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Upserting embedded chunks into a Pinecone index comes with a cost, as explained
    at the beginning of this section. We must carefully decide which data to upsert.
    If we upsert all the data, we might do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Overload the index and make retrieval challenging, be it instruction scenarios
    or classical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drive up the cost of write and read operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add more noise than is manageable and confuse the retrieval functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we choose not to upsert the data, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Querying in real time in memory**: Loading chunked, embedded data into memory
    and querying the information in real time could alleviate the data store and be
    a pragmatic way to deal with ephemeral information we don’t need to store, such
    as the daily weather forecast. However, we must also weigh the cost/benefit of
    this approach versus upserting at each step for the use cases we’ll be working
    on starting from [*Chapter 5*](Chapter_5.xhtml#_idTextAnchor140).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning data**: This comes with the cost of building training datasets,
    which requires human and computing resources. In the case of fast-moving markets,
    we might have to fine-tune regularly, which entails high investments. The cost/benefit
    will be up to the project management team to consider. A cost-benefit analysis
    of fine-tuning versus RAG will be explored in [*Chapter 5*](Chapter_5.xhtml#_idTextAnchor140).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We first initialize the libraries and start a timer to measure how long it
    takes to run the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The program must then calculate the maximum size of the batch we send to Pinecone.
    It is set to 400,000 bytes, or 4 MB, to play it safe. If the limit is reached,
    the batch size is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need an `upsert` function that takes the batch size into account when
    called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In production, we would typically exit on error, but for this educational notebook,
    printing helps us observe without stopping execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we will upsert the instruction scenarios into the namespace, `genaisys`,
    within the Pinecone index. We can now define the main batch upsert function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The function begins by determining the total length of the data and then prepares
    batches that match the batch size that it will calculate with the `get_batch_size`
    function. Then, it creates a batch and sends it to the `upsert_to_pinecone` function
    we defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'When the upsert is completed, the output will display a success message, signaling
    that we are ready to prepare the upsert process. A Pinecone index requires an
    ID that we will now create:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Once each embedded chunk has an ID, we need to format the data to fit Pinecone’s
    index structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is now formatted with an ID, values (embeddings), and metadata (the
    chunks). Let’s call the `batch_upsert` function that will call the related functions
    we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'When the upserting process is finished, the number of vectors upserted to the
    namespace and the time it took are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also display the statistics of the Pinecone index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Note that you might have to wait a few seconds to give Pinecone time to update
    the index information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output displays the information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The information displayed is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`''dimension'': 1536`: Dimension of the embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''index_fullness'': 0.0`: A value between 0 and 1 that shows how full the
    Pinecone index is. We must monitor this value to optimize the data we are upserting
    to avoid having to increase the size of the storage capacity we are using. For
    more information, consult the Pinecone documentation at [https://docs.pinecone.io/guides/get-started/overview](https://docs.pinecone.io/guides/get-started/overview).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''namespaces'': {''genaisys'': {''vector_count'': 3}}`: Displays the namespace
    and vector count.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''total_vector_count'': 3}`: Displays the total vector count in the Pinecone
    index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are now ready to upload the classical data into its namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Upserting classical data into the index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a GenAISys involves teams. So that each team can work in parallel to
    optimize production times, we will upsert the classical data in a separate program/notebook.
    One team can work on instruction scenarios while another team works on gathering
    and processing data.
  prefs: []
  type: TYPE_NORMAL
- en: Open `Pinecone_RAG.ipynb`. We will be reusing several components of the `Pinecone_instruction_scenarios.ipynb`
    notebook built in the *Building a dynamic Pinecone index* section of this chapter.
    Setting up the environment is identical to the previous notebook. The Pinecone
    index is the same, `genai-v1`. The namespace for source-data upserting is `data01`,
    as we’ve already established in earlier sections, to make sure the data is separated
    from the instruction scenarios. So, the only real difference is the data we load
    and the chunking method. Let’s get into it!
  prefs: []
  type: TYPE_NORMAL
- en: Data loading and chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section embeds chunks using the same process as for instruction scenarios
    in `Pinecone_instruction_scenarios.ipynb`. However, this time, GPT-4o does the
    chunking. When importing lines of instruction scenarios, we wanted to keep the
    integrity of the scenario in one chunk to be able to provide a complete set of
    instructions to the generative AI model. In this case, we will leverage the power
    of generative AI and chunk raw text with GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by downloading data, not scenarios, and setting the path of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the text file is loaded as one big chunk in a variable and displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: While a production application would typically exit on a critical `FileNotFoundError`,
    for this educational notebook, printing the error allows us to observe the outcome
    without interrupting the learning flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can comment `print(text)` or only print a few lines. In this case, let’s
    verify that we have correctly imported the file. The output shows that we did:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The text contains a message from the CTO of the company whose data we are uploading
    to our custom RAG database. A company might have thousands of such internal messages—far
    too many (and too volatile) to justify model fine-tuning. Storing only the key
    chunks in Pinecone gives us searchable context without flooding the index with
    noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `text` variable is not ready yet to be chunked by GPT-4o. The first step
    is to create an OpenAI instance and give the GPT-4o model instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to keep an eye on the `max_tokens=1024` setting: GPT-4o will stop generating
    once it hits that limit. For very large documents, you can stream the text in
    smaller slices—then let GPT-4o refine each slice. We can also use ready-made chunking
    functions that will break the text down into optimized *chunks* to obtain more
    nuanced and precise results when retrieving the data. However, in this case, let’s
    maximize the usage of GPT-4o; we send the entire file in one call with a low temperature
    so we can watch the model partition a real-world document from end to end.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can retrieve the chunks from the response, clean them, store them in
    a list of chunks, and return the `chunks` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can call the chunking function. We don’t have to display the chunks
    and can comment the code in production. However, in this case, let’s verify that
    everything is working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the chunks were successfully created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The remaining embedding and upsert steps are identical to those in `Pinecone_instruction_scenarios.ipynb`—just
    remember to use `namespace="data01"` when writing the vectors. After that, we’re
    ready to query the index and verify retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the Pinecone index
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As you know, our vector store now has two logical areas—`genaisys` for instruction
    scenarios and `data01` for classical data. In this section, we’ll query each area
    interactively to prove the retrieval code works before we wire it into the multi-user
    interface in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110). We will query these
    two namespaces in the Pinecone index, as shown in *Figure 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Generative AI model querying either the instruction scenarios
    or the data](img/B32304_03_5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Generative AI model querying either the instruction scenarios or
    the data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `Query_Pinecone.ipynb` to run the verification queries. The next steps
    are the same as those in the *Setting up the environment* and *Creating the Pinecone
    index* sections, except for two minor differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The namespace is not provided when we connect to the Pinecone index, only its
    name: `index_name = ''genai-v1''`. This is because the querying function will
    manage the choice of a namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Upserting` section of the notebook has been removed because we are not
    upserting but querying the Pinecone index.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Query` section of the notebook is divided into two subsections. The first
    subsection contains the querying functions and the second one the querying requests.
    Let’s begin with the querying functions.
  prefs: []
  type: TYPE_NORMAL
- en: Querying functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are four querying functions, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'QF1: `query_vector_store(query_text, namespace)`, which receives the query,
    sends the request to QF2, and returns the response. It will use QF4 to display
    the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QF2: `get_query_results(query_text, namespace)`, which receives the query from
    QF1, sends it to QF3 to be embedded, makes the actual query, and returns a response
    to QF1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QF3: `get_embedding(text, model=embedding_model)`, which receives text to embed
    from QF2 and sends the embedded text back to QF2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QF4: `display_results(query_results)`, which receives the results from QF1,
    processes them, and returns them to QF1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can simplify the representation as shown in *Figure 3.6* by creating two
    groups of functions:'
  prefs: []
  type: TYPE_NORMAL
- en: A group with QF1, `query_vector_store`, and QF4, `display_results`, in which
    QF1 queries the vector store through QF2 and returns the results to display.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A group with QF2, `get_query_results`, queries the vector store after embedding
    the query with QF3, `get_embedding`, and returns the results to QF1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.6: Querying the vector store with two groups of functions](img/B32304_03_6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Querying the vector store with two groups of functions'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin with the first group of functions.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the vector store and returning results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first function, QF1, receives the user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the function calls QF2, `query_results`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'QF2 then returns the results in `query_results`, which, in turn, is sent to
    `display_results` to obtain the text and target ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '`display_results` processes the query results it receives and returns the result
    along with metadata to find the text obtained in the metadata of the Pinecone
    index. When it is found, the function retrieves the ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The text and ID are returned to QF1, `query_vector_store`, which, in turn, returns
    the results when the function is called. Note that for educational purposes, this
    function assumes `query_results` will always contain at least one match with `'metadata'`
    and `'text'` fields. Let’s now see how the query is processed.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The program queries the Pinecone index with `get_query_results` with the input
    text and namespace provided. But first, the input text must be embedded to enable
    a vector similarity search in the vector store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the input is embedded, a vector search is requested with the vectorized
    input within the namespace specified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `k` is set to `1` in this example to retrieve a single top result
    for precision, and also, the metadata is set to `True` to include the corresponding
    text. The results are returned to QF2,`query_results`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The embedding function is the same as what we used to upsert the data in the
    Pinecone index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to use the same model to embed queries as you did to embed the data
    you upserted so that the embedded input is in the same vector format as the embedded
    data stored. This is critical for similarity search to make accurate similarity
    calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re now ready to run two tests: an instruction scenario query (namespace
    `genaisys`) and a source data query (namespace `data01`).'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval queries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To retrieve an instruction scenario, we will enter a user input and the namespace
    to let the system find the closest instruction to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'The system should detect the task briefly asked for and return a comprehensive
    instruction scenario. For that, we’ll call the entry point of the functions, `query_vector_store`,
    and display the output returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory and is ready to be used in [*Chapter 4*](Chapter_4.xhtml#_idTextAnchor110)
    in a conversational loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The program now retrieves data from the Pinecone index. The query functions
    are identical since the namespace is a variable. Let’s just look at the query
    and output. The query is directed to the data namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: We have thus populated a Pinecone vector store and queried it. Let’s summarize
    the implementation of the Pinecone index before we move on to adding more layers
    to our GenAISys.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we pushed our GenAISys project another step forward by moving
    beyond ordinary RAG. First, we layered expert-written instruction scenarios on
    top of the source data corpus, turning a static RAG pipeline into a dynamic framework
    that can fetch not only facts but also the exact reasoning pattern the model should
    follow. The global market is accelerating so quickly that users now expect ChatGPT-level
    assistance the moment a need arises; if we hope to keep pace, our architecture
    must be flexible, cost-aware, and capable of near-real-time delivery.
  prefs: []
  type: TYPE_NORMAL
- en: 'We began by laying out that architecture, then introduced the law of diminishing
    returns to determine when an implicit similarity search is worth its compute bill
    and when a direct, explicit call—such as a simple web search—will do the job more
    cheaply. With the theory in place, we wrote a program to download, chunk, embed,
    and upsert the instruction scenarios into a dedicated namespace inside a Pinecone
    index. Next, we enlisted GPT-4o to perform the same chunk-and-embed routine on
    the source documents, storing those vectors in a second namespace. Once both partitions
    were in place, we verified the retrieval layer: a single query function now routes
    any prompt to the correct namespace and returns the best match along with its
    metadata.'
  prefs: []
  type: TYPE_NORMAL
- en: With scenarios and data cleanly separated yet instantly searchable, the GenAISys
    has the retrieval backbone it needs. In the next chapter, we will plug these components
    into the conversational loop and let the system demonstrate its full, business-ready
    agility.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no limit to automating all tasks in a generative AI system. (True or
    False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The law of diminishing returns is of no use in AI. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunking is the process of breaking data into smaller parts to retrieve more
    nuanced information. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is only one embedding model you should use. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upserting data to a Pinecone index means uploading data to a database. (True
    or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A namespace is the name of a database. (True or False).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A namespace can be used to access different types of data. (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Querying the Pinecone index requires the user input to be embedded. (True or
    False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Querying the Pinecone index is based on a metric such as cosine similarity.
    (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Pinecone index and the query functions are the only components of a GenAISys.
    (True or False)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI embeddings documentation: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinecone Python SDK documentation: [https://docs.pinecone.io/reference/python-sdk](https://docs.pinecone.io/reference/python-sdk)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pinecone documentation: [https://docs.pinecone.io/guides/get-started/overview](https://docs.pinecone.io/guides/get-started/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*AI Development Cost: Learn What Makes Developing an AI Solution*: [https://www.spaceo.ai/blog/ai-development-cost/](https://www.spaceo.ai/blog/ai-development-cost/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlock this book’s exclusive benefits now
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Scan this QR code or go to [packtpub.com/unlock](http://packtpub.com/unlock),
    then search for this book by name. | ![A qr code on a white background  AI-generated
    content may be incorrect.](img/Unlock.png) |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| *Note: Keep your purchase invoice ready before you start.* |'
  prefs:
  - PREF_IND
  type: TYPE_TB
