- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Streamlining Text Preprocessing Techniques for Optimal NLP Performance
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Streamlining Text Preprocessing Techniques for Optimal NLP Performance
- en: Text preprocessing stands as a vital initial step in the realm of **natural
    language processing** (**NLP**). It encompasses converting raw, unrefined text
    data into a format that machine learning algorithms can readily comprehend. To
    extract meaningful insights from textual data, it is essential to clean, normalize,
    and transform the data into a more structured form. This chapter provides an overview
    of the most commonly used text preprocessing techniques, including tokenization,
    stemming, lemmatization, stop word removal, and **part-of-speech** (**POS**) tagging,
    along with their advantages and limitations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预处理在自然语言处理（NLP）领域是一个至关重要的初始步骤。它包括将原始、未加工的文本数据转换为机器学习算法可以轻松理解的格式。为了从文本数据中提取有意义的见解，必须清理、规范化和将数据转换成更结构化的形式。本章概述了最常用的文本预处理技术，包括分词、词干提取、词形还原、停用词移除和**词性标注**（**POS**），以及它们的优缺点。
- en: Effective text preprocessing is essential for various NLP tasks, including sentiment
    analysis, language translation, and information retrieval. By applying these techniques,
    raw text data can be transformed into a structured and normalized format that
    can be easily analyzed using statistical and machine learning methods. However,
    selecting the appropriate preprocessing techniques can be challenging since the
    optimal methods depend on the specific task and dataset at hand. Therefore, it
    is important to carefully evaluate and compare different text preprocessing techniques
    to determine the most effective approach for a given application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的文本预处理对于各种NLP任务至关重要，包括情感分析、语言翻译和信息检索。通过应用这些技术，原始文本数据可以被转换成一种结构化和规范化的格式，便于使用统计和机器学习方法进行分析。然而，选择合适的预处理技术可能具有挑战性，因为最佳方法取决于具体任务和数据集。因此，仔细评估和比较不同的文本预处理技术，以确定针对特定应用的最高效方法，这一点非常重要。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Lowercasing in NLP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP中的小写转换
- en: Removing special characters and punctuations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除特殊字符和标点符号
- en: Removing stop words
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 移除停用词
- en: Named entity recognition (NER)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）
- en: POS tagging
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词性标注
- en: Explaining the preprocessing pipeline
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释预处理流程
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To follow along with the examples and exercises in this chapter on text preprocessing,
    you will need a working knowledge of a programming language such as Python, as
    well as some familiarity with NLP concepts. You will also need to have certain
    libraries installed, such as **Natural Language Toolkit** (**NLTK**), **spaCy**,
    and **scikit-learn**. These libraries provide powerful tools for text preprocessing
    and feature extraction. It is recommended that you have access to a **Jupyter
    Notebook** environment or another interactive coding environment to facilitate
    experimentation and exploration. Additionally, having a sample dataset to work
    with can help you understand the various techniques and their effects on text
    data.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本章关于文本预处理的示例和练习，你需要对一种编程语言（如Python）有实际操作的知识，以及一些对NLP概念的了解。你还需要安装某些库，例如**自然语言工具包**（**NLTK**）、**spaCy**和**scikit-learn**。这些库提供了强大的文本预处理和特征提取工具。建议你能够访问**Jupyter
    Notebook**环境或另一个交互式编码环境，以促进实验和探索。此外，拥有一个用于工作的样本数据集可以帮助你理解各种技术及其对文本数据的影响。
- en: Text normalization is the process of transforming text into a standard form
    to ensure consistency and reduce variations. Different techniques are used for
    normalizing text, including lowercasing, removing special characters, spell checking,
    and stemming or lemmatization. We will explain these steps in detail, and how
    to use them, with code examples.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 文本规范化是将文本转换为标准形式的过程，以确保一致性并减少变化。用于规范化文本的技术包括小写转换、移除特殊字符、拼写检查以及词干提取或词形还原。我们将详细解释这些步骤，以及如何使用它们，并附上代码示例。
- en: Lowercasing in NLP
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP中的小写转换
- en: Lowercasing is a common text preprocessing technique that’s used in NLP to standardize
    text and reduce the complexity of vocabulary. In this technique, all the text
    is converted into lowercase characters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为小写是一种常见的文本预处理技术，在自然语言处理（NLP）中用于标准化文本并降低词汇的复杂性。在这种技术中，所有文本都被转换为小写字符。
- en: The main purpose of lowercasing is to make the text uniform and avoid any discrepancies
    that may arise from capitalization. By converting all the text into lowercase,
    the machine learning algorithms can treat the same words that are capitalized
    and non-capitalized as the same, reducing the overall vocabulary size and making
    the text easier to process.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为小写的主要目的是使文本统一，并避免因大写字母引起的任何差异。通过将所有文本转换为小写，机器学习算法可以将大写和非大写的相同单词视为相同，从而减少整体词汇量，使文本更容易处理。
- en: Lowercasing is particularly useful for tasks such as text classification, sentiment
    analysis, and language modeling, where the meaning of the text is not affected
    by the capitalization of the words. However, it may not be suitable for certain
    tasks, such as NER, where capitalization can be an important feature.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本转换为小写对于文本分类、情感分析和语言建模等任务特别有用，在这些任务中，文本的意义不受单词大小写的影响。然而，它可能不适合某些任务，例如命名实体识别（NER），在这些任务中，大小写可能是一个重要的特征。
- en: Removing special characters and punctuation
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 移除特殊字符和标点符号
- en: 'Removing special characters and punctuation is an important step in text preprocessing.
    Special characters and punctuation marks do not add much meaning to the text and
    can cause issues for machine learning models if they are not removed. One way
    to perform this task is by using regular expressions, such as the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 移除特殊字符和标点符号是文本预处理的重要步骤。特殊字符和标点符号对文本的意义贡献不大，如果它们没有被移除，可能会给机器学习模型带来问题。执行此任务的一种方法是通过使用正则表达式，如下所示：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will remove non-characters and numbers from our input string. Sometimes,
    there may be special characters that we would want to replace with a whitespace.
    Take a look at the following examples:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这将移除输入字符串中的非字符和数字。有时，我们可能希望将特殊字符替换为空格。请看以下示例：
- en: president-elect
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞选总统
- en: body-type
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正文类型
- en: 'In these two examples, we would want to replace the “-” with whitespace, as
    follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个示例中，我们希望将“-”替换为空格，如下所示：
- en: President elect
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 竞选总统
- en: Body type
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正文类型
- en: Next, we’ll cover stop word removal.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍停用词移除。
- en: Stop word removal
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 停用词移除
- en: Stop words are words that do not contribute much to the meaning of a sentence
    or piece of text, and therefore can be safely removed without us losing much information.
    Examples of stop words include “a,” “an,” “the,” “and,” “in,” “at,” “on,” “to,”
    “for,” “is,” “are,” and so on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是不太影响句子或文本意义的单词，因此可以安全地移除，而不会丢失太多信息。停用词的例子包括“a”、“an”、“the”、“and”、“in”、“at”、“on”、“to”、“for”、“is”、“are”等等。
- en: Stop word removal is a common text preprocessing step that is performed before
    any text analysis tasks, such as **sentiment analysis**, **topic modeling**, or
    **information retrieval**. The goal is to reduce the size of the vocabulary and
    the dimensionality of the feature space, which can improve the efficiency and
    effectiveness of subsequent analysis steps.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词移除是文本预处理中常见的步骤，在执行任何文本分析任务之前进行，如**情感分析**、**主题建模**或**信息检索**。目标是减少词汇量和特征空间的维度，这可以提高后续分析步骤的效率和效果。
- en: The process of stop word removal involves identifying a list of stop words (usually
    predefined or learned from a corpus), tokenizing the input text into words or
    tokens, and then removing any words that match the stop word list. The resulting
    text consists of only the important words that carry the meaning of the text.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词移除的过程包括识别一组停用词（通常是预定义的或从语料库中学习得到的），将输入文本分词成单词或标记，然后移除任何与停用词列表匹配的单词。结果文本仅包含携带文本意义的单词。
- en: Stop word removal can be performed using various programming languages, tools,
    and libraries. For example, NLTK, which is a popular Python library for NLP, provides
    a list of stop words for various languages, as well as a method for removing stop
    words from text.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词移除可以使用各种编程语言、工具和库来完成。例如，NLTK是一个流行的Python NLP库，它为各种语言提供停用词列表，以及从文本中移除停用词的方法。
- en: 'Here’s an example of stop word removal:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个停用词移除的示例：
- en: '*This is a sample sentence demonstrating stop* *word filtration.*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*这是一个展示停用词过滤的示例句子。*'
- en: 'After performing stop word removal, we get the following output:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行停用词移除后，我们得到以下输出：
- en: '*Sample sentence demonstrating stop* *word filtration*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例句子展示停用词过滤*'
- en: This chapter contains Python code dedicated to this. You can refer to it for
    each of the actions that are described in this chapter.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含专门为此编写的Python代码。您可以参考本章中描述的每个操作。
- en: As we can see, the stop words “This,” “is,” and “a,” have been removed from
    the original sentence, leaving only the important words.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，原始句子中的停用词“这是”、“是”和“一个”已被移除，只留下了重要的单词。
- en: Spell checking and correction
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拼写检查和纠错
- en: 'Spell checking and correction involves correcting misspelled words in the text.
    This is important because misspelled words can cause inconsistencies in the data
    and affect the accuracy of algorithms. For example, take a look at the following
    sentence:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写检查和纠错涉及纠正文本中的拼写错误。这很重要，因为拼写错误可能会导致数据不一致并影响算法的准确性。例如，看看以下句子：
- en: '*I am going to* *the bakkery*'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*我要去* *面包店*'
- en: 'This would be transformed into the following:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这将转换为以下内容：
- en: '*I am going to* *the bakery*'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*我要去* *面包店*'
- en: Let’s move on to lemmatization.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续讨论词形还原。
- en: Lemmatization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词形还原
- en: '**Lemmatization** is a text normalization approach that aims to simplify a
    word to its base or dictionary form, referred to as a lemma. The primary objective
    of lemmatization is to aggregate various forms of the same word, facilitating
    their analysis as a unified term.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**词形还原**是一种文本归一化方法，旨在将一个单词简化为其基本或词典形式，称为词元。词形还原的主要目标是聚合同一单词的各种形式，以便将它们作为一个统一术语进行分析。'
- en: 'For example, consider the following sentence:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下句子：
- en: '*Three cats were chasing the mice in the fields, while one cat watched* *one
    mouse.*'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*三只猫在田野里追逐老鼠，同时一只猫在观察* *一只老鼠*。'
- en: 'In the context of this sentence, “cat” and “cats” are two different forms of
    the same word, and “mouse” and “mice” are also two different forms of the same
    word. Lemmatization would reduce these words to their base forms:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个句子的上下文中，“cat”和“cats”是同一单词的两种不同形式，“mouse”和“mice”也是同一单词的两种不同形式。词形还原会将这些单词还原到其基本形式：
- en: '*the cat be chasing the mouse in the field, while one cat watched* *one mouse.*'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*这只猫正在田野里追逐老鼠，同时一只猫在观察* *一只老鼠*。'
- en: In this case, “cat” and “cats” have both been reduced to their base form of
    “cat,” and “mouse” and “mice” have both been reduced to their base form of “mouse.”
    This allows for better analysis of the text since the occurrences of “cat” and
    “mouse” are now treated as the same term, regardless of their inflectional variations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“cat”和“cats”都已还原到其基本形式“cat”，“mouse”和“mice”也都已还原到其基本形式“mouse”。这允许更好地分析文本，因为“cat”和“mouse”的出现现在被视为同一术语，无论其屈折变化如何。
- en: Lemmatization is different from stemming, which involves reducing a word to
    a common stem that may not necessarily be a word in its own right. For example,
    the stem of “cats” and “cat” would both be “cat.” The lemma of “cats” and “cat”
    would be “cat” as well.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原与词干提取不同，后者涉及将一个单词还原到一个可能本身不是单词的通用词干。例如，“cats”和“cat”的词干都是“cat”。而“cats”和“cat”的词元也是“cat”。
- en: Lemmatization can be performed using various NLP libraries and tools, such as
    NLTK, spaCy, and Stanford CoreNLP.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用各种NLP库和工具执行词形还原，例如NLTK、spaCy和Stanford CoreNLP。
- en: Stemming
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词干提取
- en: Stemming involves reducing words to their fundamental or root form, referred
    to as the “stem.” This process is commonly used in NLP to prepare text for analysis,
    retrieval, or storage. Stemming algorithms work by cutting off the ends or suffixes
    of words, leaving only the stem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取涉及将单词还原到其基本或根形式，称为“词干”。这个过程在NLP中通常用于准备文本以进行分析、检索或存储。词干提取算法通过截断单词的末尾或后缀来实现，只留下词干。
- en: The goal of stemming is to convert all inflected or derived forms of a word
    into a common base form. For example, the stem of the word “running” is “run,”
    and the stem of the word “runs” is also “run.”
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取的目标是将一个单词的所有屈折或派生形式转换为一种通用词干。例如，“running”这个词的词干是“run”，“runs”这个词的词干也是“run”。
- en: One commonly used stemming algorithm is the Porter stemming algorithm. This
    algorithm is based on a series of rules that identify suffixes and remove them
    from words to obtain the stem. For example, the Porter algorithm would convert
    the word “leaping” into “leap” by removing the “ing” suffix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的词干提取算法之一是波特词干提取算法。该算法基于一系列规则，用于识别词尾并将其从单词中移除，以获得词干。例如，波特算法会将单词“leaping”转换为“leap”，通过移除“ing”后缀。
- en: 'Let’s look at an example sentence to see stemming in action:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个示例句子来看看词干提取的实际应用：
- en: '*They are running and leaping across* *the walls*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们正在跑跳着穿过* *墙壁*'
- en: 'Here’s the stemmed text (using the Porter algorithm):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用Porter算法提取的词干文本：
- en: '*They are run and leap across* *the wall*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们跑着跳着穿过* *墙壁*'
- en: As you can see, the words “running” and “leaping” have been converted into their
    base forms of “run” and “leap,” respectively, and the suffix “s” has been removed
    from “walls.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，单词“running”和“leaping”已被转换为它们的词干形式“run”和“leap”，并且从“walls”中移除了后缀“s”。
- en: Stemming can be useful for text analysis tasks such as information retrieval
    or sentiment analysis as it reduces the number of unique words in a document or
    corpus and can help to group similar words. However, stemming can also introduce
    errors as it can sometimes produce stems that are not actual words or produce
    stems that are not the intended base form of the word. For example, the stemmer
    might produce “walk” as the stem for both “walked” and “walking,” even though
    “walk” and “walked” have different meanings. Therefore, it’s important to evaluate
    the results of stemming to ensure that it is producing accurate and useful results.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取对于文本分析任务，如信息检索或情感分析是有用的，因为它减少了文档或语料库中独特单词的数量，并有助于将相似单词分组。然而，词干提取也可能引入错误，因为它有时会产生不是实际单词的词干，或者产生不是单词预期基本形式的词干。例如，词干提取器可能会将“walk”作为“walked”和“walking”的词干，尽管“walk”和“walked”有不同的含义。因此，评估词干提取的结果以确保其产生准确和有用的结果是重要的。
- en: NER
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NER
- en: NER is an NLP technique that’s designed to detect and categorize named entities
    within text, including but not limited to person’s names, organization’s names,
    locations, and more. NER’s primary objective is to autonomously identify and extract
    information about these named entities from unstructured text data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是一种自然语言处理（NLP）技术，旨在检测和分类文本中的命名实体，包括但不限于人名、组织名、地点等。NER的主要目标是自主地从无结构化文本数据中识别和提取有关这些命名实体的信息。
- en: NER typically involves using machine learning models, such as **conditional
    random fields** (**CRFs**) or **recurrent neural networks** (**RNNs**), to tag
    words in a given sentence with their corresponding entity types. The models are
    trained on large annotated datasets that contain text with labeled entities. These
    models then use context-based rules to identify named entities in new text.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别通常涉及使用机器学习模型，如**条件随机字段**（CRFs）或**循环神经网络**（RNNs），为给定句子中的单词标记相应的实体类型。这些模型在包含带有标签实体的文本的大型标注数据集上训练。然后，这些模型使用基于上下文的规则来识别新文本中的命名实体。
- en: 'There are several categories of named entities that can be identified by NER,
    including the following:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）可以识别的命名实体类别有几个，包括以下内容：
- en: '**Person**: A named individual, such as “Barack Obama”'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人物**：一个命名个人，例如“巴拉克·奥巴马”（Barack Obama）'
- en: '**Organization**: A named company, institution, or organization, such as “Google”'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**组织**：一个命名公司、机构或组织，例如“谷歌”（Google）'
- en: '**Location**: A named place, such as “New York City”'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**地点**：一个命名地点，例如“纽约市”（New York City）'
- en: '**Date**: A named date or time, such as “January 1, 2023”'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**日期**：一个命名的日期或时间，例如“2023年1月1日”（January 1, 2023）'
- en: '**Product**: A named product or brand, such as “iPhone”'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品**：一个命名的产品或品牌，例如“iPhone”'
- en: 'Here’s an example of how NER works. Take a look at the following sentence:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个命名实体识别（NER）的工作示例。看看以下句子：
- en: '*Apple Inc. is a technology company headquartered in* *Cupertino, California.*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*苹果公司（Apple Inc.）是一家总部位于加利福尼亚州库比蒂诺（Cupertino, California）的技术公司。*'
- en: 'Here, NER would identify “Apple Inc.” as an organization and “Cupertino, California”
    as a location. The output of an NER system could be a structured representation
    of the sentence, as shown here:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，NER会将“Apple Inc.”识别为组织，将“Cupertino, California”识别为地点。命名实体识别系统的输出可以是句子的结构化表示，如下所示：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: NER has many applications in various fields, including **information retrieval**,
    **question-answering**, **sentiment analysis**, and more. It can be used to automatically
    extract structured information from unstructured text data, which can be further
    analyzed or used for downstream tasks.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）在各个领域都有许多应用，包括**信息检索**、**问答**、**情感分析**等。它可以自动从无结构化文本数据中提取结构化信息，这些信息可以进一步分析或用于下游任务。
- en: 'There are different approaches and tools to perform NER, but the general steps
    when performing NER are as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 执行命名实体识别（NER）有不同的方法和工具，但执行NER的一般步骤如下：
- en: '**Data collection**: The first step is to collect the data that will be used
    for NER. This data can be in the form of unstructured text, such as articles,
    social media posts, or web pages.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据收集**：第一步是收集用于命名实体识别（NER）的数据。这些数据可以是无结构化文本的形式，例如文章、社交媒体帖子或网页。'
- en: '**Preprocessing**: The next step is to preprocess the data, which involves
    various steps such as tokenization, stop word removal, stemming or lemmatization,
    and normalization.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理**：下一步是预处理数据，这涉及各种步骤，如分词、停用词去除、词干提取或词形还原以及归一化。'
- en: '**Labeling**: After preprocessing, the next step is to label the data with
    named entity tags. There are different tagging schemes, but one of the most commonly
    used is the **Inside-Outside-Beginning** (**IOB**) tagging scheme. In this scheme,
    each word in the text is labeled as either **B** (**beginning of a named entity**),
    **I** (**inside of a named entity**), or **O** (**outside of a** **named entity**).'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标记**：预处理后，下一步是将数据用命名实体标签进行标记。有不同的标记方案，但最常用的一种是**内部-外部-开始**（**IOB**）标记方案。在这个方案中，文本中的每个单词都被标记为**B**（命名实体的开始）、**I**（命名实体内部）或**O**（命名实体外部）。'
- en: '**Training**: Once the data has been labeled, the next step is to train a machine
    learning model to recognize named entities in new, unseen text. Different types
    of models can be used for NER, such as rule-based systems, statistical models,
    and deep learning models.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练**：一旦数据被标记，下一步就是训练一个机器学习模型以识别新、未见过的文本中的命名实体。NER可以使用不同类型的模型，例如基于规则的系统、统计模型和深度学习模型。'
- en: '**Evaluation**: After training the model, it is important to evaluate its performance
    on a test dataset. This can help identify any issues with the model, such as overfitting,
    underfitting, or bias.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估**：在训练模型后，评估其在测试数据集上的性能非常重要。这有助于识别模型中可能存在的问题，例如过拟合、欠拟合或偏差。'
- en: '**Deployment**: Finally, the trained model can be deployed to perform NER on
    new, unseen text. This can be done in real time or in batch mode, depending on
    the application’s requirements.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**部署**：最后，训练好的模型可以被部署到对新、未见过的文本执行命名实体识别（NER）。这可以实时进行，也可以批量进行，具体取决于应用程序的需求。'
- en: 'Here’s an example of how NER can be performed:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个如何执行NER的示例：
- en: 'Original text:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文本：
- en: '*Apple is negotiating to buy a Chinese start-up* *this year.*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apple is negotiating to buy a Chinese start-up* *this year.*'
- en: 'Preprocessed text:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理文本：
- en: '*apple negotiating buy Chinese* *start-up year*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*apple negotiating buy Chinese* *start-up year*'
- en: 'Tagged text:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 标记文本：
- en: '*B-ORG O O B-LOC O O*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*B-ORG O O B-LOC O O*'
- en: In this example, the named entities “Apple” and “Chinese” are identified as
    an organization (B-ORG) and a location (B-LOC), respectively. “this year” is not
    recognized as a named entity in this example, but it would be if a more complex
    tagging scheme is used or if the model is trained on data that would promote that.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，命名实体“Apple”和“Chinese”分别被识别为组织（B-ORG）和地点（B-LOC）。在这个示例中，“this year”没有被识别为命名实体，但如果使用更复杂的标记方案或模型在促进这种识别的数据上训练，它将被识别。
- en: 'Several libraries can be used for NER, depending on the programming language
    and specific needs of the project. Let’s take a look at some commonly used libraries:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 根据编程语言和项目的具体需求，可以使用几个库进行NER。让我们看看一些常用的库：
- en: '**spaCy**: **spaCy** is a widely used open source library designed for various
    NLP tasks, including NER. Offering pre-trained models across multiple languages,
    the library additionally empowers users to undertake model training for distinct
    domains tailored to their specific needs.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**spaCy**：**spaCy**是一个广泛使用的开源库，旨在用于各种NLP任务，包括NER。它提供跨多种语言的预训练模型，并且还允许用户针对特定需求进行模型训练。'
- en: '**NLTK**: This is another widely used library for NLP tasks, including NER.
    It provides several pre-trained models and also allows users to train their models.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NLTK**：这是另一个广泛使用的NLP任务库，包括NER。它提供几个预训练模型，并允许用户训练自己的模型。'
- en: '**Stanford Named Entity Recognizer** (**NER**): This is a Java-based NER tool
    that provides pre-trained models for several languages, including English, German,
    and Chinese.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**斯坦福命名实体识别器**（**NER**）：这是一个基于Java的NER工具，为包括英语、德语和中文在内的多种语言提供预训练模型。'
- en: '**AllenNLP**: AllenNLP is a popular open source library for building and evaluating
    NLP models, including NER. It provides pre-trained models for several tasks, including
    NER, and also allows users to train their own models.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AllenNLP**：AllenNLP是一个流行的开源库，用于构建和评估NLP模型，包括NER。它为包括NER在内的几个任务提供预训练模型，并允许用户训练自己的模型。'
- en: '**Flair**: Flair is a Python library for state-of-the-art NLP, including NER.
    It provides pre-trained models for several languages and also allows users to
    train their own models.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Flair**：Flair是一个用于最先进自然语言处理（NLP）的Python库，包括NER。它为多种语言提供预训练模型，并允许用户训练自己的模型。'
- en: '**General Architecture for Text Engineering** (**GATE**): This is a suite of
    tools for NLP, including NER. It provides a graphical interface for creating and
    evaluating NLP models and also allows users to develop custom plugins for specific
    tasks.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用文本工程架构**（**GATE**）：这是一个用于NLP的工具套件，包括NER。它提供了一个图形界面来创建和评估NLP模型，并允许用户为特定任务开发自定义插件。'
- en: There are many other libraries available for NER, and the choice of library
    will depend on factors such as the programming language, available models, and
    specific requirements of the project. In the next section, we will explain POS
    tagging and different methods to perform this task.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于命名实体识别（NER），有许多其他库可供选择，库的选择将取决于编程语言、可用模型和项目的具体要求。在下一节中，我们将解释词性标注（POS tagging）以及执行此任务的不同方法。
- en: POS tagging
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: POS tagging is the practice of attributing grammatical labels, such as nouns,
    verbs, adjectives, and others, to individual words within a sentence. This tagging
    process holds significance as a foundational step in various NLP tasks, including
    text classification, sentiment analysis, and machine translation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注（POS tagging）是将诸如名词、动词、形容词等语法标签分配给句子中单个单词的实践。这一标注过程在包括文本分类、情感分析和机器翻译在内的各种NLP任务中是一个基础步骤。
- en: POS tagging can be performed using different approaches such as rule-based methods,
    statistical methods, and deep learning-based methods. In this section, we’ll provide
    a brief overview of each approach.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注可以使用不同的方法进行，例如基于规则的方法、统计方法和基于深度学习的方法。在本节中，我们将简要概述每种方法。
- en: Rule-based methods
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于规则的方法
- en: Rule-based methods for POS tagging involve defining a set of rules or patterns
    that can be used to automatically tag words in a text with their corresponding
    parts of speech, such as nouns, verbs, adjectives, and so on.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法进行词性标注涉及定义一组规则或模式，这些规则或模式可以用于自动将文本中的单词标注为相应的词性，如名词、动词、形容词等。
- en: The process involves defining a set of rules or patterns for identifying the
    different parts of speech in a sentence. For example, a rule may state that any
    word ending in “-ing” is a gerund (a verb acting as a noun), while another rule
    may state that any word preceded by an article such as “a” or “an” is likely a
    noun.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程包括定义一组规则或模式，用于识别句子中的不同词性。例如，一条规则可能声明以“-ing”结尾的任何单词都是动名词（作为名词使用的动词），而另一条规则可能声明以“a”或“an”等冠词开头的任何单词很可能是名词。
- en: These rules are typically based on linguistic knowledge, such as knowledge of
    grammar and syntax, and are often specific to a particular language. They can
    also be supplemented with lexicons or dictionaries that provide additional information
    about the meanings and usage of words.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这些规则通常基于语言学知识，例如语法和句法知识，并且通常针对特定语言。它们还可以补充以词典或词汇表，这些词典或词汇表提供了关于单词意义和用法的额外信息。
- en: The process of rule-based tagging involves applying these rules to a given text
    and identifying the parts of speech for each word. This can be done manually but
    is typically automated using software tools and programming languages that support
    regular expressions and pattern matching.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的标签化过程涉及将这些规则应用于给定的文本，并为每个单词识别词性。这可以手动完成，但通常使用支持正则表达式和模式匹配的软件工具和编程语言进行自动化。
- en: One advantage of rule-based methods is that they can be highly accurate when
    the rules are well-designed and cover a wide range of linguistic phenomena. They
    can also be customized to specific domains or genres of text, such as scientific
    literature or legal documents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法的一个优点是，当规则设计得很好并且覆盖了广泛的语言学现象时，它们可以非常准确。它们还可以根据特定领域或文本类型进行定制，例如科学文献或法律文件。
- en: However, one limitation of rule-based methods is that they may not be able to
    capture the full complexity and variability of natural language, and may require
    significant effort to develop and maintain the rules as language evolves and changes
    over time. They may also struggle with ambiguity, such as in cases where a word
    can have multiple possible parts of speech depending on the context.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于规则的方法的局限性在于，它们可能无法捕捉自然语言的全部复杂性和可变性，并且随着语言随时间演变和变化，可能需要大量努力来开发和维护规则。它们还可能难以处理歧义，例如在单词可以根据上下文具有多种可能的词性时。
- en: Despite these limitations, rule-based methods for POS tagging remain an important
    approach in NLP, especially for applications that require high accuracy and precision.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些局限性，基于规则的 POS 标注方法在 NLP 中仍然是一个重要的方法，特别是对于需要高精度和精确度的应用。
- en: Statistical methods
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计方法
- en: Statistical methods for POS tagging are based on using probabilistic models
    to automatically assign the most likely POS tag to each word in a sentence. These
    methods rely on a training corpus of tagged text, where the POS tags have already
    been assigned to the words, to learn the probabilities of a particular word being
    associated with each tag.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: POS 标注的统计方法基于使用概率模型自动为句子中的每个单词分配最可能的 POS 标签。这些方法依赖于一个已标记的语料库，其中 POS 标签已经分配给单词，以学习特定单词与每个标签关联的概率。
- en: 'Two main types of statistical methods are used for POS tagging: **Hidden Markov
    Models** (**HMMs**) and CRFs.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 POS 标注的两种主要统计方法是：**隐马尔可夫模型**（**HMMs**）和 CRFs。
- en: HMMs serve as a category of probabilistic models that are extensively applied
    in handling sequential data, including text. In the context of POS tagging, HMMs
    represent the probability distribution of a sequence of POS tags concerning a
    sequence of words. HMMs assume that the likelihood of a POS tag at a specific
    position within a sentence is contingent solely upon the preceding tag in the
    sequence. Furthermore, they presume that the likelihood of a particular word,
    given its tag, remains independent of other words within the sentence. To identify
    the most probable sequence of POS tags for a given sentence, HMMs employ the Viterbi
    algorithm.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: HMMs 是一类广泛应用于处理序列数据（包括文本）的概率模型。在 POS 标注的背景下，HMMs 表示一系列单词的 POS 标签的概率分布。HMMs 假设一个句子中特定位置的
    POS 标签的可能性仅取决于序列中的前一个标签。此外，它们还假设给定其标签的特定单词的可能性与其他句子中的单词无关。为了确定给定句子的最可能的 POS 标签序列，HMMs
    使用 Viterbi 算法。
- en: CRFs are another type of probabilistic model that is commonly used for sequence
    labeling tasks, including POS tagging. CRFs differ from HMMs in that they model
    the conditional probability of the output sequence (that is, the POS tags) given
    the input sequence (that is, the words), rather than the joint probability of
    the output and input sequences. This allows CRFs to capture more complex dependencies
    between the input and output sequences than HMMs. CRFs use an iterative algorithm,
    such as gradient descent or L-BFGS, to learn the optimal set of weights for the
    model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: CRFs 是另一种常用于序列标注任务（包括 POS 标注）的概率模型。CRFs 与 HMMs 的不同之处在于，它们模型化的是给定输入序列（即单词）的输出序列（即
    POS 标签）的条件概率，而不是输出和输入序列的联合概率。这使得 CRFs 能够比 HMMs 捕获输入和输出序列之间更复杂的依赖关系。CRFs 使用迭代算法，如梯度下降或
    L-BFGS，来学习模型的最佳权重集。
- en: 'Let’s look at the advantages of statistical methods:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看统计方法的优点：
- en: Statistical methods can capture the context of a word and the relationships
    between words in a sentence, leading to more accurate tagging results
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计方法可以捕捉到单词的上下文以及句子中单词之间的关系，从而产生更准确的标注结果。
- en: These methods can handle unseen words and sentences that are not present in
    the training data
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法可以处理训练数据中未出现的单词和句子。
- en: Statistical methods can be trained on large datasets, allowing them to capture
    more variations and patterns in the language
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计方法可以在大型数据集上训练，这使得它们能够捕捉到语言中的更多变化和模式。
- en: 'Now, let’s look at the disadvantages:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看缺点：
- en: These methods require a large amount of annotated data for training, which can
    be time-consuming and expensive to create
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法需要大量的标注数据用于训练，这可能既耗时又昂贵。
- en: Statistical methods can be sensitive to the quality of the training data and
    may perform poorly if the data is noisy or biased
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计方法可能对训练数据的质量敏感，如果数据是嘈杂或偏颇的，可能会表现不佳。
- en: Statistical models are typically black boxes, making it difficult to interpret
    the decisions made by the model
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型通常是黑盒，这使得很难解释模型所做的决策。
- en: Deep learning-based methods
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于深度学习的方法
- en: Deep learning-based methods for POS tagging involve training a neural network
    model to predict the POS tags for each word in a given sentence. These methods
    can learn complex patterns and relationships in the text data to accurately tag
    words with their appropriate parts of speech.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的 POS 标注方法涉及训练一个神经网络模型来预测给定句子中每个单词的 POS 标签。这些方法可以在文本数据中学习复杂的模式和关系，以准确地为单词分配其适当的词性。
- en: One of the most popular deep learning-based methods for POS tagging is using
    an RNN with LSTM cells. LSTM-based models can process sequences of words and capture
    dependencies between them. The input to the model is a sequence of word embeddings,
    which are vector representations of words in a high-dimensional space. These embeddings
    are learned during the training process.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的基于深度学习的词性标注方法之一是使用带有LSTM单元的RNN。基于LSTM的模型可以处理单词序列并捕捉它们之间的依赖关系。模型输入是一个单词嵌入序列，这些嵌入是高维空间中单词的向量表示。这些嵌入是在训练过程中学习的。
- en: 'The LSTM-based model is comprised of three main layers: an input layer, an
    LSTM layer, and an output layer. The structure involves taking word embeddings
    as input into the input layer. Subsequently, the LSTM layer processes the sequence
    of these embeddings, aiming to grasp the interdependencies inherent within them.
    Ultimately, the output layer is responsible for predicting the POS tag for each
    word within the input sequence. Another popular deep learning-based method for
    POS tagging is using a transformer-based model, such as **Bidirectional Encoder
    Representations from Transformers** (**BERT**). BERT is a language model that
    comes pre-trained and employs a transformer-based architecture to acquire a profound
    understanding of contextual relationships among words within a sentence. It undergoes
    training with vast quantities of text data and can be fine-tuned to excel in diverse
    NLP tasks, one of which is POS tagging.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LSTM的模型由三个主要层组成：输入层、LSTM层和输出层。结构涉及将单词嵌入作为输入传递到输入层。随后，LSTM层处理这些嵌入的序列，旨在掌握它们内在的相互依赖关系。最终，输出层负责预测输入序列中每个单词的词性标签。另一种流行的基于深度学习的词性标注方法是使用基于transformer的模型，例如**双向编码器表示的Transformer**（**BERT**）。BERT是一个预训练的语言模型，它使用基于transformer的架构来深入理解句子中单词之间的上下文关系。它使用大量文本数据进行训练，并且可以微调以在各种自然语言处理任务中表现出色，其中之一就是词性标注。
- en: To use BERT for POS tagging, the input sentence must be tokenized, and each
    token must be assigned an initial POS tag. The token embeddings are then fed into
    the pre-trained BERT model, which outputs contextualized embeddings for each token.
    These embeddings are passed through a feedforward neural network to predict the
    final POS tag for each token.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用BERT进行词性标注，输入句子必须进行分词，并且每个标记词必须分配一个初始的词性标签。然后，标记词嵌入被输入到预训练的BERT模型中，该模型为每个标记词输出上下文化的嵌入。这些嵌入随后通过前馈神经网络传递，以预测每个标记词的最终词性标签。
- en: Deep learning approaches for POS tagging have demonstrated leading-edge performance
    across numerous benchmark datasets. Nonetheless, their effectiveness demands substantial
    training data and computational resources, and the training process can be time-consuming.
    Moreover, they may suffer from a lack of interpretability, which makes it difficult
    to understand how the model is making its predictions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的词性标注方法在多个基准数据集上展示了前沿的性能。然而，它们的有效性需要大量的训练数据和计算资源，并且训练过程可能耗时。此外，它们可能缺乏可解释性，这使得理解模型如何进行预测变得困难。
- en: Several libraries are available for performing POS tagging in various programming
    languages, including Python, Java, and C++. Some popular NLP libraries that provide
    POS tagging functionality include NLTK, spaCy, Stanford CoreNLP, and Apache OpenNLP.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种编程语言中，如Python、Java和C++，都有可用于执行词性标注的库。一些提供词性标注功能的流行自然语言处理库包括NLTK、spaCy、Stanford
    CoreNLP和Apache OpenNLP。
- en: 'Here is an example of POS tagging using the NLTK library in Python:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用Python中的NLTK库进行词性标注的示例：
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In this example, the `nltk.pos_tag()` function is used to tag the words in the
    sentence. The function returns a list of tuples where each tuple contains a word
    and its POS tag. The POS tags that have been used here are based on the **Penn**
    **Treebank tagset**.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，使用了`nltk.pos_tag()`函数来标注句子中的单词。该函数返回一个元组列表，其中每个元组包含一个单词及其词性标签。这里使用的词性标签基于**宾州树库标签集**。
- en: Regular expressions
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正则表达式
- en: A regular expression is a type of text pattern that has various applications
    in modern programming languages and software. They are useful for validating whether
    an input conforms to a particular text pattern, locating text within a larger
    text body that matches the pattern, replacing text that matches the pattern with
    alternative text or rearranging parts of the matched text, and dividing a block
    of text into a list of subtexts, but can cause unintended consequences if used
    incorrectly.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式是一种文本模式，在现代编程语言和软件中具有多种应用。它们用于验证输入是否符合特定的文本模式，定位与模式匹配的大文本体内的文本，将匹配模式的文本替换为其他文本或重新排列匹配文本的部分，以及将文本块划分为子文本列表，但如果不正确使用，可能会导致意外的后果。
- en: In computer science and mathematics, the term **regular expression** is derived
    from the concept of “regularity” in mathematical expressions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机科学和数学中，术语**正则表达式**来源于数学表达式中的“规律性”概念。
- en: A regular expression, often referred to as regex or regexp, is a series of characters
    that constitutes a search pattern. Regular expressions are used to match and manipulate
    text, typically in the context of text processing, search algorithms, and NLP.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式，通常称为regex或regexp，是一系列构成搜索模式的字符。正则表达式用于匹配和操作文本，通常在文本处理、搜索算法和NLP的上下文中使用。
- en: A regular expression comprises a mix of characters and metacharacters, which
    collectively establish a pattern to search for within a text string. The simplest
    form of a regular expression is a mere sequence of characters that must be matched
    precisely. For example, the regular expression “hello” would match any string
    that contains the characters “hello” in sequence.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式由字符和元字符的混合组成，共同建立用于在文本字符串中搜索的模式。正则表达式的最简单形式是精确匹配字符序列。例如，正则表达式“hello”会匹配包含“hello”字符序列的任何字符串。
- en: Metacharacters are unique characters within regular expressions that possess
    pre-defined meanings. For instance, the “.” (dot) metacharacter is employed to
    match any individual character, whereas the “*” (asterisk) metacharacter is used
    to match zero or more instances of the preceding characters or group. Regular
    expressions can be used for a wide range of text-processing tasks. Let’s take
    a closer look.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 元字符是正则表达式中的独特字符，具有预定义的含义。例如，“.”（点）元字符用于匹配任何单个字符，而“*”（星号）元字符用于匹配前一个字符或组的零个或多个实例。正则表达式可用于广泛的文本处理任务。让我们更深入地了解一下。
- en: Validating input
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证输入
- en: Regular expressions can be used to validate input by matching it against a pattern.
    For example, you can use a regular expression to validate an email address or
    a phone number.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用正则表达式通过匹配模式来验证输入。例如，您可以使用正则表达式验证电子邮件地址或电话号码。
- en: Text manipulation
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本操作
- en: Text manipulation using regular expressions involves using pattern-matching
    techniques to find and manipulate text strings in a document or dataset. Regular
    expressions are powerful tools for working with text data, allowing for complex
    search and replace operations, text extraction, and formatting.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式进行文本操作涉及使用模式匹配技术来查找和操作文档或数据集中的文本字符串。正则表达式是处理文本数据的强大工具，允许进行复杂的搜索和替换操作、文本提取和格式化。
- en: 'Some common text manipulation tasks that can be accomplished with regular expressions
    are as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用正则表达式完成的一些常见文本操作任务如下：
- en: '**Search and replace**: Using regular expressions to search for specific patterns
    or character sequences in a document and replace them with other text or formatting'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索和替换**：使用正则表达式在文档中搜索特定的模式或字符序列，并用其他文本或格式替换它们'
- en: '**Data extraction**: Regular expressions can be used for data extraction from
    text by defining patterns that match specific data formats'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据提取**：可以通过定义匹配特定数据格式的模式来使用正则表达式从文本中提取数据'
- en: 'Here are the general steps for using regular expressions for data extraction:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用正则表达式进行数据提取的一般步骤：
- en: '**Define a regular expression pattern**: The first step is to define a regular
    expression pattern that matches the data you want to extract. For example, if
    you want to extract all phone numbers from a text document, you can define a pattern
    that matches the format of a phone number.'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义正则表达式模式**：第一步是定义一个匹配您想要提取的数据的正则表达式模式。例如，如果您想从文本文档中提取所有电话号码，您可以定义一个匹配电话号码格式的模式。'
- en: '**Compile the regular expression pattern**: After establishing the regular
    expression pattern, the next step involves compiling it into a regular expression
    object, which can then be utilized for matching purposes.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**编译正则表达式模式**：在建立正则表达式模式后，下一步是将它编译成一个正则表达式对象，然后可用于匹配目的。'
- en: '**Search for the pattern in the text**: Once you have compiled the regular
    expression object, you can use it to search for the pattern in the text. You can
    search for the pattern in a single string or a larger block of text.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在文本中搜索模式**：一旦编译了正则表达式对象，您就可以使用它来在文本中搜索该模式。您可以在单个字符串或更大的文本块中搜索该模式。'
- en: '**Extract the matched data**: After you have searched for the pattern in the
    text, you can extract the data that matches that pattern. You can extract all
    occurrences of the matched data or only the first occurrence.'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**提取匹配的数据**：在您在文本中搜索到模式后，您可以提取与该模式匹配的数据。您可以提取所有匹配数据的所有出现，或者只提取第一次出现。'
- en: 'Here’s an example of how to extract all email addresses from a string using
    regular expressions in Python:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用Python中的正则表达式从字符串中提取所有电子邮件地址的示例：
- en: '[PRE4]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here’s the output:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE5]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, we’ll cover text cleaning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍文本清理。
- en: Text cleaning
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本清理
- en: Text cleaning means using regular expressions to clean and standardize text
    data, thereby removing unwanted characters, whitespace, or other formatting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 文本清理意味着使用正则表达式清理和标准化文本数据，从而移除不需要的字符、空白或其他格式。
- en: 'Here are some common text-cleaning techniques that use regular expressions:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些常见的使用正则表达式的文本清理技术：
- en: '**Removing special characters**: Regular expressions can be used to match and
    remove specific characters such as punctuation marks, brackets, and other special
    symbols. For example, the **[^a-zA-Z0-9]** regular expression will match any non-alphanumeric
    character.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除特殊字符**：正则表达式可用于匹配并移除特定字符，例如标点符号、括号和其他特殊符号。例如，**[^a-zA-Z0-9]** 正则表达式将匹配任何非字母数字字符。'
- en: '**Removing stop words**: Stop words are common words such as “the,” “and,”
    and “but” that are often removed from text to focus on the most meaningful words.
    Regular expressions can be used to match and remove these words from text.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除停用词**：停用词是诸如“the”、“and”和“but”等常见单词，通常从文本中移除以关注最有意义的单词。正则表达式可用于匹配并从文本中移除这些单词。'
- en: '**Removing HTML tags**: If you’re working with text that has been scraped from
    a website, you may need to remove HTML tags before analyzing the text. Regular
    expressions can be used to match and remove HTML tags.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除HTML标签**：如果您正在处理从网站抓取的文本，您可能需要在分析文本之前移除HTML标签。正则表达式可用于匹配并移除HTML标签。'
- en: '**Converting text into lowercase**: Regular expressions can be used to convert
    all text into lowercase or uppercase, which can make it easier to compare and
    analyze.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**将文本转换为小写**：正则表达式可用于将所有文本转换为小写或大写，这可以使比较和分析更容易。'
- en: '**Normalizing text**: Normalization involves transforming text into a standard
    format. Regular expressions can be used to perform tasks such as stemming and
    lemmatization, which involves reducing words to their root form.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本归一化**：归一化涉及将文本转换成标准格式。正则表达式可用于执行诸如词干提取和词形还原等任务，这涉及到将单词还原到其词根形式。'
- en: By using regular expressions for text cleaning, you can remove noise and irrelevant
    information from text, making it easier to analyze and extract meaningful insights.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用正则表达式进行文本清理，您可以从文本中移除噪声和不相关信息，使其更容易分析和提取有意义的见解。
- en: Parsing
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解析
- en: '**Parsing** involves analyzing a text string to discern its grammatical structure
    according to a specified grammar. Regular expressions serve as potent instruments
    for text parsing, especially when dealing with uncomplicated and regular grammatical
    patterns.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**解析**涉及分析文本字符串以根据指定的语法识别其语法结构。正则表达式是文本解析的有力工具，尤其是在处理简单和规则的语法模式时。'
- en: To parse text using regular expressions, you need to define a grammar for the
    language you want to parse. The grammar should specify the possible components
    of a sentence, such as nouns, verbs, adjectives, and so on, as well as the rules
    that dictate how these components can be combined to form valid sentences.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用正则表达式解析文本，您需要为要解析的语言定义一个语法。该语法应指定句子的可能组成部分，例如名词、动词、形容词等，以及规定这些组成部分如何组合成有效句子的规则。
- en: Once you have defined the grammar, you can use regular expressions to identify
    the individual components of a sentence and the relationships between them. For
    example, you can use regular expressions to match all the nouns in a sentence
    or to identify the subject and object of a verb.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您定义了语法，您就可以使用正则表达式来识别句子的各个组成部分及其之间的关系。例如，您可以使用正则表达式匹配句子中的所有名词或识别动词的主语和宾语。
- en: One common approach to parsing with regular expressions is to define a set of
    patterns that correspond to the different parts of speech and sentence structures
    in your grammar. For example, you might define a pattern for matching nouns, a
    pattern for matching verbs, and a pattern for matching sentences that consist
    of a subject followed by a verb and an object.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正则表达式进行解析的一个常见方法是为语法中的不同词性和句子结构定义一组模式。例如，您可能为匹配名词定义一个模式，为匹配动词定义一个模式，以及为匹配由主语、动词和宾语组成的句子定义一个模式。
- en: To use these patterns for parsing, you would apply them to a text string using
    a regular expression engine, which would match the patterns to the appropriate
    parts of the string. The output of the parsing process would be a parse tree or
    other data structure that represents the grammatical structure of the sentence.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这些模式进行解析，您需要使用正则表达式引擎将它们应用于文本字符串，该引擎会将模式与字符串的适当部分匹配。解析过程的输出将是一个解析树或其他表示句子语法结构的数据库结构。
- en: One limitation of regular expression parsing is that it is generally not suitable
    for handling more complex or ambiguous grammar. For example, it can be difficult
    to handle cases where a word could be either a noun or a verb depending on the
    context, or where the structure of a sentence is ambiguous.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式解析的一个局限性是它通常不适用于处理更复杂或模糊的语法。例如，处理一个词可能是名词或动词取决于上下文的情况，或者句子结构模糊的情况可能会很困难。
- en: We can also use regular expressions to break a larger text document into smaller
    chunks or tokens based on specific patterns or delimiters.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用正则表达式根据特定的模式或分隔符将较大的文本文档拆分为较小的块或词元。
- en: To use regular expressions for text manipulation, you typically need to define
    a pattern that matches the text you want to find or manipulate. This pattern can
    include special characters and syntax to define the specific sequence of characters,
    numbers, or other elements that make up the text string.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用正则表达式进行文本操作，您通常需要定义一个匹配您想要查找或操作的文本的模式。此模式可以包括特殊字符和语法来定义构成文本字符串的特定字符序列、数字或其他元素。
- en: For example, the regular expression pattern *\d{3}-\d{2}-\d{4}* might be used
    to search for and extract Social Security numbers in a larger text document. This
    pattern matches a sequence of three digits, followed by a dash, then two more
    digits, another dash, and four final digits followed by a non-digit, which together
    represent the standard format for a Social Security number in the USA.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，正则表达式模式 `\d{3}-\d{2}-\d{4}` 可能用于在更大的文本文档中搜索和提取社会保险号码。此模式匹配三个数字序列，后面跟着一个连字符，然后是两个更多数字，另一个连字符，最后是四个数字，后面跟着一个非数字字符，这些字符一起代表了美国社会保险号码的标准格式。
- en: Once you have defined your regular expression pattern, you can use it with various
    text manipulation tools and programming languages, such as grep, sed, awk, Perl,
    Python, and many others, to perform complex text manipulation tasks.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您定义了正则表达式模式，您就可以使用它和各种文本操作工具和编程语言，如grep、sed、awk、Perl、Python等，来执行复杂的文本操作任务。
- en: Some programming languages, such as Perl and Python, have built-in support for
    regular expressions. Other programming languages, such as Java and C++, require
    you to use a library or API to work with regular expressions.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一些编程语言，如Perl和Python，内置了对正则表达式的支持。其他编程语言，如Java和C++，则需要您使用库或API来处理正则表达式。
- en: While regular expressions are powerful tools for text processing, they can also
    be complex and difficult to understand. It’s important to be familiar with the
    syntax and behavior of regular expressions to use them effectively in your code.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然正则表达式是文本处理中的强大工具，但它们也可能很复杂，难以理解。熟悉正则表达式的语法和行为对于在代码中有效地使用它们至关重要。
- en: Tokenization
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词元化
- en: Tokenization is a process in NLP that involves breaking down a piece of text
    or a sentence into individual words or terms, known as tokens. The tokenization
    process can be applied to various forms of data, such as textual documents, social
    media posts, web pages, and more.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是NLP中的一个过程，涉及将文本片段或句子分解成单个单词或术语，称为标记。分词过程可以应用于各种形式的数据，如文本文档、社交媒体帖子、网页等。
- en: The tokenization process is an important initial step in many NLP tasks as it
    transforms unstructured text data into a structured format that can be analyzed
    using machine learning algorithms or other techniques. These tokens can be used
    to perform various operations in the text, such as counting word frequencies,
    identifying the most common phrases, and so on.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分词过程是许多NLP任务的重要初始步骤，因为它将非结构化文本数据转换成可以用于机器学习算法或其他技术分析的格式化格式。这些标记可以用于在文本中执行各种操作，例如统计单词频率、识别最常见的短语等。
- en: 'There are different methods of tokenization:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 分词有不同的方法：
- en: '**Word tokenization**: This method splits a piece of text into individual words
    or tokens using whitespace, punctuation, and other characters as delimiters. For
    example, take a look at the following sentence:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单词分词**：这种方法使用空格、标点符号和其他字符作为分隔符将文本片段分解成单个单词或标记。例如，看看以下句子：'
- en: '*The nimble white cat jumps over the* *sleepy dog*'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*敏捷的白色猫跳过了* *睡着的狗*'
- en: 'This can be tokenized into the following list of words:'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以分词成以下单词列表：
- en: '*[“The”, “nimble”, “white”, “cat”, “jumps”, “over”, “the”, “**sleepy”, “dog”]*'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*[“The”， “nimble”， “white”， “cat”， “jumps”， “over”， “the”， “**sleepy”， “dog”]*'
- en: '**Sentence tokenization**: This method splits a piece of text into individual
    sentences by using punctuation marks such as periods, exclamation marks, and question
    marks as delimiters. For example, take a look at the following paragraph:'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子分词**：这种方法使用句号、感叹号和问号等标点符号作为分隔符将文本片段分解成单个句子。例如，看看以下段落：'
- en: '*This is the* *first sentence.*'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*这是* *第一句话。*'
- en: '*This is the* *second sentence.*'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*这是* *第二句话。*'
- en: '*This is the* *third sentence*.'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*这是* *第三句话*。'
- en: 'This can be tokenized into the following list of sentences:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这可以分词成以下句子列表：
- en: '*[“This is the* *first sentence.”,*'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*[“This is the* *first sentence.”,*'
- en: '*“This is the* *second sentence.”,*'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*“这是* *第二句话。”,*'
- en: '*“This is the* *third sentence.”]*'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*“这是* *第三句话。”]*'
- en: '**Regular expression tokenization**: This method uses regular expressions to
    define the tokenization rules. Regular expressions can be used to match patterns
    in the text, such as email addresses, URLs, or phone numbers, and extract them
    as individual tokens.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则表达式分词**：这种方法使用正则表达式来定义分词规则。正则表达式可以用于匹配文本中的模式，如电子邮件地址、URL或电话号码，并将它们作为单独的标记提取出来。'
- en: Tokenization is an important step in NLP and is used in many applications, such
    as sentiment analysis, document classification, machine translation, and more.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 分词是NLP中的重要步骤，并在许多应用中使用，如情感分析、文档分类、机器翻译等。
- en: Tokenization is also an important step in language models. For example, in BERT,
    which is a well-known language model, a tokenizer is a sub-word tokenizer, meaning
    it breaks down words into smaller sub-word units called tokens. It uses **WordPiece**
    tokenization, which is a data-driven approach that builds a large vocabulary of
    sub-words based on the corpus of text being trained on.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 分词也是语言模型中的重要步骤。例如，在BERT这个著名的语言模型中，分词器是一个子词分词器，意味着它将单词分解成更小的子词单元，称为标记。它使用**WordPiece**分词，这是一种基于训练文本语料库的数据驱动方法，它构建了一个包含大量子词的大词汇表。
- en: Using a tokenizer is an important step in language models as well. For example,
    BERT utilizes a WordPiece tokenizer, which employs the technique of dividing words
    into either their full forms or smaller components known as word pieces. This
    means that a single word can be represented by several tokens. It employs a data-driven
    approach that builds a large vocabulary of sub-words based on the corpus of text
    being trained on. These sub-word units are represented as embeddings that are
    used as input to the BERT model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分词器也是语言模型中的重要步骤。例如，BERT利用WordPiece分词器，该分词器采用将单词分为完整形式或更小的称为词元的组件的技术。这意味着单个单词可以由多个标记表示。它采用数据驱动的方法，根据训练文本语料库构建一个包含大量子词的大词汇表。这些子词单元以嵌入的形式表示，用作BERT模型的输入。
- en: One of the key features of the BERT tokenizer is that it can handle **out-of-vocabulary**
    (**OOV**) words. If the tokenizer encounters a word that is not in its vocabulary,
    it will break the word down into sub-words and represent the word as a combination
    of its sub-word embeddings. We will explain BERT and its tokenizer in more detail
    later in this book. The benefit of using a tokenizer in language models is that
    we can limit the number of inputs to the size of our dictionary rather than all
    possible inputs. For example, BERT has a 30,000-word vocabulary size, which helps
    us limit the size of the deep learning language model. Using a bigger tokenizer
    will increase the size of the model. In the next section, we will explain how
    to use the methods that were covered in this chapter in a complete preprocessing
    pipeline.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: BERT分词器的一个关键特性是它可以处理**词汇表外的**（**OOV**）单词。如果分词器遇到不在其词汇表中的单词，它将单词分解成子词，并将单词表示为其子词嵌入的组合。我们将在本书的后面部分更详细地解释BERT及其分词器。在语言模型中使用分词器的优点是我们可以将输入的大小限制为我们字典的大小，而不是所有可能的输入。例如，BERT有30,000个单词的词汇量，这有助于我们限制深度学习语言模型的大小。使用更大的分词器会增加模型的大小。在下一节中，我们将解释如何使用本章中介绍的方法在完整的预处理流程中使用。
- en: Explaining the preprocessing pipeline
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释预处理流程
- en: We will explain a complete preprocessing pipeline that has been provided by
    the authors to you, the reader.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解释作者提供的完整的预处理流程，供读者了解。
- en: 'As shown in the following code, the input is a formatted text with encoded
    tags, similar to what we can extract from HTML web pages:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下代码所示，输入是一个带有编码标签的格式化文本，类似于我们可以从HTML网页中提取的内容：
- en: '[PRE6]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s take a look at the effect of applying each step to the text:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看将每个步骤应用到文本上的效果：
- en: 'Decode/remove encoding:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码/移除编码：
- en: '*Employees details. Attached are 2 files, 1st one is pairoll, 2nd* *is healtcare!*'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险！*'
- en: 'Lowercasing:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 小写化：
- en: '*employees details. attached are 2 files, 1st one is pairoll, 2nd* *is healtcare!*'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险！*'
- en: 'Digits to words:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数字转文字：
- en: '*employees details. attached are two files, first one is pairoll, second* *is
    healtcare!*'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险！*'
- en: 'Remove punctuation and other special characters:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除标点符号和其他特殊字符：
- en: '*employees details attached are two files first one is pairoll second* *is
    healtcare*'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险*'
- en: 'Spelling corrections:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拼写纠正：
- en: '*employees details attached are two files first one is payroll second* *is
    healthcare*'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险*'
- en: 'Remove stop words:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除停用词：
- en: '*employees details attached two files first one payroll* *second healthcare*'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险*'
- en: 'Stemming:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词干提取：
- en: '*employe detail attach two file first one payrol* *second healthcar*'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单，第二个* *是医疗保险*'
- en: 'Lemmatizing:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 词形还原：
- en: '*employe detail attach two file first one payrol* *second healthcar*'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*员工详情附上两个文件，第一个是工资单* *第二个是医疗保险*'
- en: With that, we’ve learned about different preprocessing methods. Next, we’ll
    review a piece of code for performing NER and POS.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些，我们了解了不同的预处理方法。接下来，我们将回顾一段执行NER和POS的代码。
- en: Code for NER and POS
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NER和POS的代码
- en: 'For this example, we used the spaCy library for Python to perform these tasks.
    Here our input is:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们使用了Python的spaCy库来执行这些任务。我们的输入如下：
- en: '[PRE7]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here’s the output for NER:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是NER的输出：
- en: '*The companies that would be releasing their quarterly DATE reports tomorrow
    DATE are Microsoft ORG , 4pm TIME , Google ORG , 4pm TIME , and AT&T ORG , 6pm*
    *TIME .*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*明天将发布季度报告的公司有微软 ORG ，下午4点 TIME ，谷歌 ORG ，下午4点 TIME ，以及AT&T ORG ，晚上6点* *TIME
    。*'
- en: As you can see, using NER, we were able to detect parts of the sentence that
    are related to company names (ORG) or dates.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，使用命名实体识别（NER），我们能够检测到与公司名称（ORG）或日期相关的句子部分。
- en: '*Figure 4**.1* shows an example of performing POS tagging:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4**.1* 展示了使用spaCy进行词性标注的示例：'
- en: '![Figure 4.1 – POS tagging using spaCy](img/B18949_04_1.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 使用spaCy进行词性标注](img/B18949_04_1.jpg)'
- en: Figure 4.1 – POS tagging using spaCy
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 使用spaCy进行词性标注
- en: 'Here’s the output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '[PRE8]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The preceding code examples exemplify the various aspects of preprocessing,
    which processes raw text and transforms it into a form that suits the downstream
    model so that it suits the purpose of the overall design.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例展示了预处理的不同方面，预处理将原始文本转换为适合下游模型的形式，以便符合整体设计的用途。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered a range of techniques and methods for text preprocessing,
    including normalization, tokenization, stop word removal, POS tagging, and more.
    We explored different approaches to these techniques, such as rule-based methods,
    statistical methods, and deep learning-based methods. We also discussed the advantages
    and disadvantages of each method and provided examples and code snippets to illustrate
    their use.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一系列文本预处理的技术和方法，包括规范化、分词、停用词去除、词性标注等。我们探讨了这些技术的不同方法，例如基于规则的方法、统计方法以及基于深度学习的方法。我们还讨论了每种方法的优缺点，并提供了示例和代码片段来说明它们的使用。
- en: At this point, you should have a solid understanding of the importance of text
    preprocessing and the various techniques and methods available for cleaning and
    preparing text data for analysis. You should be able to implement these techniques
    using popular libraries and frameworks in Python and understand the trade-offs
    between different approaches. Furthermore, you should have a better understanding
    of how to process text data to achieve better results in NLP tasks such as sentiment
    analysis, topic modeling, and text classification.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对文本预处理的重要性以及用于清理和准备文本数据以供分析的各种技术和方法有了扎实的理解。你应该能够使用Python中流行的库和框架实现这些技术，并理解不同方法之间的权衡。此外，你应该对如何处理文本数据以在自然语言处理任务（如情感分析、主题建模和文本分类）中取得更好的结果有更深入的了解。
- en: In the next chapter, we will explain text classification, and different methods
    for performing this task.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将解释文本分类，以及执行此任务的不同方法。
