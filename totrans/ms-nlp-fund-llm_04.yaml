- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streamlining Text Preprocessing Techniques for Optimal NLP Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text preprocessing stands as a vital initial step in the realm of **natural
    language processing** (**NLP**). It encompasses converting raw, unrefined text
    data into a format that machine learning algorithms can readily comprehend. To
    extract meaningful insights from textual data, it is essential to clean, normalize,
    and transform the data into a more structured form. This chapter provides an overview
    of the most commonly used text preprocessing techniques, including tokenization,
    stemming, lemmatization, stop word removal, and **part-of-speech** (**POS**) tagging,
    along with their advantages and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Effective text preprocessing is essential for various NLP tasks, including sentiment
    analysis, language translation, and information retrieval. By applying these techniques,
    raw text data can be transformed into a structured and normalized format that
    can be easily analyzed using statistical and machine learning methods. However,
    selecting the appropriate preprocessing techniques can be challenging since the
    optimal methods depend on the specific task and dataset at hand. Therefore, it
    is important to carefully evaluate and compare different text preprocessing techniques
    to determine the most effective approach for a given application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercasing in NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing special characters and punctuations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition (NER)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: POS tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining the preprocessing pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To follow along with the examples and exercises in this chapter on text preprocessing,
    you will need a working knowledge of a programming language such as Python, as
    well as some familiarity with NLP concepts. You will also need to have certain
    libraries installed, such as **Natural Language Toolkit** (**NLTK**), **spaCy**,
    and **scikit-learn**. These libraries provide powerful tools for text preprocessing
    and feature extraction. It is recommended that you have access to a **Jupyter
    Notebook** environment or another interactive coding environment to facilitate
    experimentation and exploration. Additionally, having a sample dataset to work
    with can help you understand the various techniques and their effects on text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Text normalization is the process of transforming text into a standard form
    to ensure consistency and reduce variations. Different techniques are used for
    normalizing text, including lowercasing, removing special characters, spell checking,
    and stemming or lemmatization. We will explain these steps in detail, and how
    to use them, with code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Lowercasing in NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lowercasing is a common text preprocessing technique that’s used in NLP to standardize
    text and reduce the complexity of vocabulary. In this technique, all the text
    is converted into lowercase characters.
  prefs: []
  type: TYPE_NORMAL
- en: The main purpose of lowercasing is to make the text uniform and avoid any discrepancies
    that may arise from capitalization. By converting all the text into lowercase,
    the machine learning algorithms can treat the same words that are capitalized
    and non-capitalized as the same, reducing the overall vocabulary size and making
    the text easier to process.
  prefs: []
  type: TYPE_NORMAL
- en: Lowercasing is particularly useful for tasks such as text classification, sentiment
    analysis, and language modeling, where the meaning of the text is not affected
    by the capitalization of the words. However, it may not be suitable for certain
    tasks, such as NER, where capitalization can be an important feature.
  prefs: []
  type: TYPE_NORMAL
- en: Removing special characters and punctuation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Removing special characters and punctuation is an important step in text preprocessing.
    Special characters and punctuation marks do not add much meaning to the text and
    can cause issues for machine learning models if they are not removed. One way
    to perform this task is by using regular expressions, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will remove non-characters and numbers from our input string. Sometimes,
    there may be special characters that we would want to replace with a whitespace.
    Take a look at the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: president-elect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: body-type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In these two examples, we would want to replace the “-” with whitespace, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: President elect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Body type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we’ll cover stop word removal.
  prefs: []
  type: TYPE_NORMAL
- en: Stop word removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stop words are words that do not contribute much to the meaning of a sentence
    or piece of text, and therefore can be safely removed without us losing much information.
    Examples of stop words include “a,” “an,” “the,” “and,” “in,” “at,” “on,” “to,”
    “for,” “is,” “are,” and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Stop word removal is a common text preprocessing step that is performed before
    any text analysis tasks, such as **sentiment analysis**, **topic modeling**, or
    **information retrieval**. The goal is to reduce the size of the vocabulary and
    the dimensionality of the feature space, which can improve the efficiency and
    effectiveness of subsequent analysis steps.
  prefs: []
  type: TYPE_NORMAL
- en: The process of stop word removal involves identifying a list of stop words (usually
    predefined or learned from a corpus), tokenizing the input text into words or
    tokens, and then removing any words that match the stop word list. The resulting
    text consists of only the important words that carry the meaning of the text.
  prefs: []
  type: TYPE_NORMAL
- en: Stop word removal can be performed using various programming languages, tools,
    and libraries. For example, NLTK, which is a popular Python library for NLP, provides
    a list of stop words for various languages, as well as a method for removing stop
    words from text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of stop word removal:'
  prefs: []
  type: TYPE_NORMAL
- en: '*This is a sample sentence demonstrating stop* *word filtration.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'After performing stop word removal, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sample sentence demonstrating stop* *word filtration*'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter contains Python code dedicated to this. You can refer to it for
    each of the actions that are described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the stop words “This,” “is,” and “a,” have been removed from
    the original sentence, leaving only the important words.
  prefs: []
  type: TYPE_NORMAL
- en: Spell checking and correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spell checking and correction involves correcting misspelled words in the text.
    This is important because misspelled words can cause inconsistencies in the data
    and affect the accuracy of algorithms. For example, take a look at the following
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I am going to* *the bakkery*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This would be transformed into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I am going to* *the bakery*'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s move on to lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Lemmatization** is a text normalization approach that aims to simplify a
    word to its base or dictionary form, referred to as a lemma. The primary objective
    of lemmatization is to aggregate various forms of the same word, facilitating
    their analysis as a unified term.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Three cats were chasing the mice in the fields, while one cat watched* *one
    mouse.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of this sentence, “cat” and “cats” are two different forms of
    the same word, and “mouse” and “mice” are also two different forms of the same
    word. Lemmatization would reduce these words to their base forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*the cat be chasing the mouse in the field, while one cat watched* *one mouse.*'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, “cat” and “cats” have both been reduced to their base form of
    “cat,” and “mouse” and “mice” have both been reduced to their base form of “mouse.”
    This allows for better analysis of the text since the occurrences of “cat” and
    “mouse” are now treated as the same term, regardless of their inflectional variations.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization is different from stemming, which involves reducing a word to
    a common stem that may not necessarily be a word in its own right. For example,
    the stem of “cats” and “cat” would both be “cat.” The lemma of “cats” and “cat”
    would be “cat” as well.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization can be performed using various NLP libraries and tools, such as
    NLTK, spaCy, and Stanford CoreNLP.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stemming involves reducing words to their fundamental or root form, referred
    to as the “stem.” This process is commonly used in NLP to prepare text for analysis,
    retrieval, or storage. Stemming algorithms work by cutting off the ends or suffixes
    of words, leaving only the stem.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of stemming is to convert all inflected or derived forms of a word
    into a common base form. For example, the stem of the word “running” is “run,”
    and the stem of the word “runs” is also “run.”
  prefs: []
  type: TYPE_NORMAL
- en: One commonly used stemming algorithm is the Porter stemming algorithm. This
    algorithm is based on a series of rules that identify suffixes and remove them
    from words to obtain the stem. For example, the Porter algorithm would convert
    the word “leaping” into “leap” by removing the “ing” suffix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example sentence to see stemming in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '*They are running and leaping across* *the walls*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the stemmed text (using the Porter algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '*They are run and leap across* *the wall*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the words “running” and “leaping” have been converted into their
    base forms of “run” and “leap,” respectively, and the suffix “s” has been removed
    from “walls.”
  prefs: []
  type: TYPE_NORMAL
- en: Stemming can be useful for text analysis tasks such as information retrieval
    or sentiment analysis as it reduces the number of unique words in a document or
    corpus and can help to group similar words. However, stemming can also introduce
    errors as it can sometimes produce stems that are not actual words or produce
    stems that are not the intended base form of the word. For example, the stemmer
    might produce “walk” as the stem for both “walked” and “walking,” even though
    “walk” and “walked” have different meanings. Therefore, it’s important to evaluate
    the results of stemming to ensure that it is producing accurate and useful results.
  prefs: []
  type: TYPE_NORMAL
- en: NER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NER is an NLP technique that’s designed to detect and categorize named entities
    within text, including but not limited to person’s names, organization’s names,
    locations, and more. NER’s primary objective is to autonomously identify and extract
    information about these named entities from unstructured text data.
  prefs: []
  type: TYPE_NORMAL
- en: NER typically involves using machine learning models, such as **conditional
    random fields** (**CRFs**) or **recurrent neural networks** (**RNNs**), to tag
    words in a given sentence with their corresponding entity types. The models are
    trained on large annotated datasets that contain text with labeled entities. These
    models then use context-based rules to identify named entities in new text.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several categories of named entities that can be identified by NER,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Person**: A named individual, such as “Barack Obama”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Organization**: A named company, institution, or organization, such as “Google”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Location**: A named place, such as “New York City”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Date**: A named date or time, such as “January 1, 2023”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product**: A named product or brand, such as “iPhone”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of how NER works. Take a look at the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Apple Inc. is a technology company headquartered in* *Cupertino, California.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, NER would identify “Apple Inc.” as an organization and “Cupertino, California”
    as a location. The output of an NER system could be a structured representation
    of the sentence, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: NER has many applications in various fields, including **information retrieval**,
    **question-answering**, **sentiment analysis**, and more. It can be used to automatically
    extract structured information from unstructured text data, which can be further
    analyzed or used for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different approaches and tools to perform NER, but the general steps
    when performing NER are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data collection**: The first step is to collect the data that will be used
    for NER. This data can be in the form of unstructured text, such as articles,
    social media posts, or web pages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Preprocessing**: The next step is to preprocess the data, which involves
    various steps such as tokenization, stop word removal, stemming or lemmatization,
    and normalization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Labeling**: After preprocessing, the next step is to label the data with
    named entity tags. There are different tagging schemes, but one of the most commonly
    used is the **Inside-Outside-Beginning** (**IOB**) tagging scheme. In this scheme,
    each word in the text is labeled as either **B** (**beginning of a named entity**),
    **I** (**inside of a named entity**), or **O** (**outside of a** **named entity**).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training**: Once the data has been labeled, the next step is to train a machine
    learning model to recognize named entities in new, unseen text. Different types
    of models can be used for NER, such as rule-based systems, statistical models,
    and deep learning models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluation**: After training the model, it is important to evaluate its performance
    on a test dataset. This can help identify any issues with the model, such as overfitting,
    underfitting, or bias.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deployment**: Finally, the trained model can be deployed to perform NER on
    new, unseen text. This can be done in real time or in batch mode, depending on
    the application’s requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s an example of how NER can be performed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Original text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Apple is negotiating to buy a Chinese start-up* *this year.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preprocessed text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*apple negotiating buy Chinese* *start-up year*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tagged text:'
  prefs: []
  type: TYPE_NORMAL
- en: '*B-ORG O O B-LOC O O*'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the named entities “Apple” and “Chinese” are identified as
    an organization (B-ORG) and a location (B-LOC), respectively. “this year” is not
    recognized as a named entity in this example, but it would be if a more complex
    tagging scheme is used or if the model is trained on data that would promote that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Several libraries can be used for NER, depending on the programming language
    and specific needs of the project. Let’s take a look at some commonly used libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**spaCy**: **spaCy** is a widely used open source library designed for various
    NLP tasks, including NER. Offering pre-trained models across multiple languages,
    the library additionally empowers users to undertake model training for distinct
    domains tailored to their specific needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NLTK**: This is another widely used library for NLP tasks, including NER.
    It provides several pre-trained models and also allows users to train their models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stanford Named Entity Recognizer** (**NER**): This is a Java-based NER tool
    that provides pre-trained models for several languages, including English, German,
    and Chinese.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AllenNLP**: AllenNLP is a popular open source library for building and evaluating
    NLP models, including NER. It provides pre-trained models for several tasks, including
    NER, and also allows users to train their own models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flair**: Flair is a Python library for state-of-the-art NLP, including NER.
    It provides pre-trained models for several languages and also allows users to
    train their own models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**General Architecture for Text Engineering** (**GATE**): This is a suite of
    tools for NLP, including NER. It provides a graphical interface for creating and
    evaluating NLP models and also allows users to develop custom plugins for specific
    tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many other libraries available for NER, and the choice of library
    will depend on factors such as the programming language, available models, and
    specific requirements of the project. In the next section, we will explain POS
    tagging and different methods to perform this task.
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: POS tagging is the practice of attributing grammatical labels, such as nouns,
    verbs, adjectives, and others, to individual words within a sentence. This tagging
    process holds significance as a foundational step in various NLP tasks, including
    text classification, sentiment analysis, and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging can be performed using different approaches such as rule-based methods,
    statistical methods, and deep learning-based methods. In this section, we’ll provide
    a brief overview of each approach.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rule-based methods for POS tagging involve defining a set of rules or patterns
    that can be used to automatically tag words in a text with their corresponding
    parts of speech, such as nouns, verbs, adjectives, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The process involves defining a set of rules or patterns for identifying the
    different parts of speech in a sentence. For example, a rule may state that any
    word ending in “-ing” is a gerund (a verb acting as a noun), while another rule
    may state that any word preceded by an article such as “a” or “an” is likely a
    noun.
  prefs: []
  type: TYPE_NORMAL
- en: These rules are typically based on linguistic knowledge, such as knowledge of
    grammar and syntax, and are often specific to a particular language. They can
    also be supplemented with lexicons or dictionaries that provide additional information
    about the meanings and usage of words.
  prefs: []
  type: TYPE_NORMAL
- en: The process of rule-based tagging involves applying these rules to a given text
    and identifying the parts of speech for each word. This can be done manually but
    is typically automated using software tools and programming languages that support
    regular expressions and pattern matching.
  prefs: []
  type: TYPE_NORMAL
- en: One advantage of rule-based methods is that they can be highly accurate when
    the rules are well-designed and cover a wide range of linguistic phenomena. They
    can also be customized to specific domains or genres of text, such as scientific
    literature or legal documents.
  prefs: []
  type: TYPE_NORMAL
- en: However, one limitation of rule-based methods is that they may not be able to
    capture the full complexity and variability of natural language, and may require
    significant effort to develop and maintain the rules as language evolves and changes
    over time. They may also struggle with ambiguity, such as in cases where a word
    can have multiple possible parts of speech depending on the context.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these limitations, rule-based methods for POS tagging remain an important
    approach in NLP, especially for applications that require high accuracy and precision.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statistical methods for POS tagging are based on using probabilistic models
    to automatically assign the most likely POS tag to each word in a sentence. These
    methods rely on a training corpus of tagged text, where the POS tags have already
    been assigned to the words, to learn the probabilities of a particular word being
    associated with each tag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two main types of statistical methods are used for POS tagging: **Hidden Markov
    Models** (**HMMs**) and CRFs.'
  prefs: []
  type: TYPE_NORMAL
- en: HMMs serve as a category of probabilistic models that are extensively applied
    in handling sequential data, including text. In the context of POS tagging, HMMs
    represent the probability distribution of a sequence of POS tags concerning a
    sequence of words. HMMs assume that the likelihood of a POS tag at a specific
    position within a sentence is contingent solely upon the preceding tag in the
    sequence. Furthermore, they presume that the likelihood of a particular word,
    given its tag, remains independent of other words within the sentence. To identify
    the most probable sequence of POS tags for a given sentence, HMMs employ the Viterbi
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: CRFs are another type of probabilistic model that is commonly used for sequence
    labeling tasks, including POS tagging. CRFs differ from HMMs in that they model
    the conditional probability of the output sequence (that is, the POS tags) given
    the input sequence (that is, the words), rather than the joint probability of
    the output and input sequences. This allows CRFs to capture more complex dependencies
    between the input and output sequences than HMMs. CRFs use an iterative algorithm,
    such as gradient descent or L-BFGS, to learn the optimal set of weights for the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the advantages of statistical methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods can capture the context of a word and the relationships
    between words in a sentence, leading to more accurate tagging results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These methods can handle unseen words and sentences that are not present in
    the training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical methods can be trained on large datasets, allowing them to capture
    more variations and patterns in the language
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s look at the disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: These methods require a large amount of annotated data for training, which can
    be time-consuming and expensive to create
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical methods can be sensitive to the quality of the training data and
    may perform poorly if the data is noisy or biased
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical models are typically black boxes, making it difficult to interpret
    the decisions made by the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning-based methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning-based methods for POS tagging involve training a neural network
    model to predict the POS tags for each word in a given sentence. These methods
    can learn complex patterns and relationships in the text data to accurately tag
    words with their appropriate parts of speech.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most popular deep learning-based methods for POS tagging is using
    an RNN with LSTM cells. LSTM-based models can process sequences of words and capture
    dependencies between them. The input to the model is a sequence of word embeddings,
    which are vector representations of words in a high-dimensional space. These embeddings
    are learned during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The LSTM-based model is comprised of three main layers: an input layer, an
    LSTM layer, and an output layer. The structure involves taking word embeddings
    as input into the input layer. Subsequently, the LSTM layer processes the sequence
    of these embeddings, aiming to grasp the interdependencies inherent within them.
    Ultimately, the output layer is responsible for predicting the POS tag for each
    word within the input sequence. Another popular deep learning-based method for
    POS tagging is using a transformer-based model, such as **Bidirectional Encoder
    Representations from Transformers** (**BERT**). BERT is a language model that
    comes pre-trained and employs a transformer-based architecture to acquire a profound
    understanding of contextual relationships among words within a sentence. It undergoes
    training with vast quantities of text data and can be fine-tuned to excel in diverse
    NLP tasks, one of which is POS tagging.'
  prefs: []
  type: TYPE_NORMAL
- en: To use BERT for POS tagging, the input sentence must be tokenized, and each
    token must be assigned an initial POS tag. The token embeddings are then fed into
    the pre-trained BERT model, which outputs contextualized embeddings for each token.
    These embeddings are passed through a feedforward neural network to predict the
    final POS tag for each token.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning approaches for POS tagging have demonstrated leading-edge performance
    across numerous benchmark datasets. Nonetheless, their effectiveness demands substantial
    training data and computational resources, and the training process can be time-consuming.
    Moreover, they may suffer from a lack of interpretability, which makes it difficult
    to understand how the model is making its predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Several libraries are available for performing POS tagging in various programming
    languages, including Python, Java, and C++. Some popular NLP libraries that provide
    POS tagging functionality include NLTK, spaCy, Stanford CoreNLP, and Apache OpenNLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of POS tagging using the NLTK library in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `nltk.pos_tag()` function is used to tag the words in the
    sentence. The function returns a list of tuples where each tuple contains a word
    and its POS tag. The POS tags that have been used here are based on the **Penn**
    **Treebank tagset**.
  prefs: []
  type: TYPE_NORMAL
- en: Regular expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A regular expression is a type of text pattern that has various applications
    in modern programming languages and software. They are useful for validating whether
    an input conforms to a particular text pattern, locating text within a larger
    text body that matches the pattern, replacing text that matches the pattern with
    alternative text or rearranging parts of the matched text, and dividing a block
    of text into a list of subtexts, but can cause unintended consequences if used
    incorrectly.
  prefs: []
  type: TYPE_NORMAL
- en: In computer science and mathematics, the term **regular expression** is derived
    from the concept of “regularity” in mathematical expressions.
  prefs: []
  type: TYPE_NORMAL
- en: A regular expression, often referred to as regex or regexp, is a series of characters
    that constitutes a search pattern. Regular expressions are used to match and manipulate
    text, typically in the context of text processing, search algorithms, and NLP.
  prefs: []
  type: TYPE_NORMAL
- en: A regular expression comprises a mix of characters and metacharacters, which
    collectively establish a pattern to search for within a text string. The simplest
    form of a regular expression is a mere sequence of characters that must be matched
    precisely. For example, the regular expression “hello” would match any string
    that contains the characters “hello” in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Metacharacters are unique characters within regular expressions that possess
    pre-defined meanings. For instance, the “.” (dot) metacharacter is employed to
    match any individual character, whereas the “*” (asterisk) metacharacter is used
    to match zero or more instances of the preceding characters or group. Regular
    expressions can be used for a wide range of text-processing tasks. Let’s take
    a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Validating input
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Regular expressions can be used to validate input by matching it against a pattern.
    For example, you can use a regular expression to validate an email address or
    a phone number.
  prefs: []
  type: TYPE_NORMAL
- en: Text manipulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text manipulation using regular expressions involves using pattern-matching
    techniques to find and manipulate text strings in a document or dataset. Regular
    expressions are powerful tools for working with text data, allowing for complex
    search and replace operations, text extraction, and formatting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some common text manipulation tasks that can be accomplished with regular expressions
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search and replace**: Using regular expressions to search for specific patterns
    or character sequences in a document and replace them with other text or formatting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data extraction**: Regular expressions can be used for data extraction from
    text by defining patterns that match specific data formats'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the general steps for using regular expressions for data extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define a regular expression pattern**: The first step is to define a regular
    expression pattern that matches the data you want to extract. For example, if
    you want to extract all phone numbers from a text document, you can define a pattern
    that matches the format of a phone number.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compile the regular expression pattern**: After establishing the regular
    expression pattern, the next step involves compiling it into a regular expression
    object, which can then be utilized for matching purposes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Search for the pattern in the text**: Once you have compiled the regular
    expression object, you can use it to search for the pattern in the text. You can
    search for the pattern in a single string or a larger block of text.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Extract the matched data**: After you have searched for the pattern in the
    text, you can extract the data that matches that pattern. You can extract all
    occurrences of the matched data or only the first occurrence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here’s an example of how to extract all email addresses from a string using
    regular expressions in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Next, we’ll cover text cleaning.
  prefs: []
  type: TYPE_NORMAL
- en: Text cleaning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Text cleaning means using regular expressions to clean and standardize text
    data, thereby removing unwanted characters, whitespace, or other formatting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some common text-cleaning techniques that use regular expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Removing special characters**: Regular expressions can be used to match and
    remove specific characters such as punctuation marks, brackets, and other special
    symbols. For example, the **[^a-zA-Z0-9]** regular expression will match any non-alphanumeric
    character.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Removing stop words**: Stop words are common words such as “the,” “and,”
    and “but” that are often removed from text to focus on the most meaningful words.
    Regular expressions can be used to match and remove these words from text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Removing HTML tags**: If you’re working with text that has been scraped from
    a website, you may need to remove HTML tags before analyzing the text. Regular
    expressions can be used to match and remove HTML tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Converting text into lowercase**: Regular expressions can be used to convert
    all text into lowercase or uppercase, which can make it easier to compare and
    analyze.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Normalizing text**: Normalization involves transforming text into a standard
    format. Regular expressions can be used to perform tasks such as stemming and
    lemmatization, which involves reducing words to their root form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using regular expressions for text cleaning, you can remove noise and irrelevant
    information from text, making it easier to analyze and extract meaningful insights.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Parsing** involves analyzing a text string to discern its grammatical structure
    according to a specified grammar. Regular expressions serve as potent instruments
    for text parsing, especially when dealing with uncomplicated and regular grammatical
    patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: To parse text using regular expressions, you need to define a grammar for the
    language you want to parse. The grammar should specify the possible components
    of a sentence, such as nouns, verbs, adjectives, and so on, as well as the rules
    that dictate how these components can be combined to form valid sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have defined the grammar, you can use regular expressions to identify
    the individual components of a sentence and the relationships between them. For
    example, you can use regular expressions to match all the nouns in a sentence
    or to identify the subject and object of a verb.
  prefs: []
  type: TYPE_NORMAL
- en: One common approach to parsing with regular expressions is to define a set of
    patterns that correspond to the different parts of speech and sentence structures
    in your grammar. For example, you might define a pattern for matching nouns, a
    pattern for matching verbs, and a pattern for matching sentences that consist
    of a subject followed by a verb and an object.
  prefs: []
  type: TYPE_NORMAL
- en: To use these patterns for parsing, you would apply them to a text string using
    a regular expression engine, which would match the patterns to the appropriate
    parts of the string. The output of the parsing process would be a parse tree or
    other data structure that represents the grammatical structure of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of regular expression parsing is that it is generally not suitable
    for handling more complex or ambiguous grammar. For example, it can be difficult
    to handle cases where a word could be either a noun or a verb depending on the
    context, or where the structure of a sentence is ambiguous.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use regular expressions to break a larger text document into smaller
    chunks or tokens based on specific patterns or delimiters.
  prefs: []
  type: TYPE_NORMAL
- en: To use regular expressions for text manipulation, you typically need to define
    a pattern that matches the text you want to find or manipulate. This pattern can
    include special characters and syntax to define the specific sequence of characters,
    numbers, or other elements that make up the text string.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the regular expression pattern *\d{3}-\d{2}-\d{4}* might be used
    to search for and extract Social Security numbers in a larger text document. This
    pattern matches a sequence of three digits, followed by a dash, then two more
    digits, another dash, and four final digits followed by a non-digit, which together
    represent the standard format for a Social Security number in the USA.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have defined your regular expression pattern, you can use it with various
    text manipulation tools and programming languages, such as grep, sed, awk, Perl,
    Python, and many others, to perform complex text manipulation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Some programming languages, such as Perl and Python, have built-in support for
    regular expressions. Other programming languages, such as Java and C++, require
    you to use a library or API to work with regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: While regular expressions are powerful tools for text processing, they can also
    be complex and difficult to understand. It’s important to be familiar with the
    syntax and behavior of regular expressions to use them effectively in your code.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tokenization is a process in NLP that involves breaking down a piece of text
    or a sentence into individual words or terms, known as tokens. The tokenization
    process can be applied to various forms of data, such as textual documents, social
    media posts, web pages, and more.
  prefs: []
  type: TYPE_NORMAL
- en: The tokenization process is an important initial step in many NLP tasks as it
    transforms unstructured text data into a structured format that can be analyzed
    using machine learning algorithms or other techniques. These tokens can be used
    to perform various operations in the text, such as counting word frequencies,
    identifying the most common phrases, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different methods of tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word tokenization**: This method splits a piece of text into individual words
    or tokens using whitespace, punctuation, and other characters as delimiters. For
    example, take a look at the following sentence:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The nimble white cat jumps over the* *sleepy dog*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This can be tokenized into the following list of words:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*[“The”, “nimble”, “white”, “cat”, “jumps”, “over”, “the”, “**sleepy”, “dog”]*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Sentence tokenization**: This method splits a piece of text into individual
    sentences by using punctuation marks such as periods, exclamation marks, and question
    marks as delimiters. For example, take a look at the following paragraph:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*This is the* *first sentence.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*This is the* *second sentence.*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*This is the* *third sentence*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This can be tokenized into the following list of sentences:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*[“This is the* *first sentence.”,*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*“This is the* *second sentence.”,*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*“This is the* *third sentence.”]*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Regular expression tokenization**: This method uses regular expressions to
    define the tokenization rules. Regular expressions can be used to match patterns
    in the text, such as email addresses, URLs, or phone numbers, and extract them
    as individual tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenization is an important step in NLP and is used in many applications, such
    as sentiment analysis, document classification, machine translation, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization is also an important step in language models. For example, in BERT,
    which is a well-known language model, a tokenizer is a sub-word tokenizer, meaning
    it breaks down words into smaller sub-word units called tokens. It uses **WordPiece**
    tokenization, which is a data-driven approach that builds a large vocabulary of
    sub-words based on the corpus of text being trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Using a tokenizer is an important step in language models as well. For example,
    BERT utilizes a WordPiece tokenizer, which employs the technique of dividing words
    into either their full forms or smaller components known as word pieces. This
    means that a single word can be represented by several tokens. It employs a data-driven
    approach that builds a large vocabulary of sub-words based on the corpus of text
    being trained on. These sub-word units are represented as embeddings that are
    used as input to the BERT model.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features of the BERT tokenizer is that it can handle **out-of-vocabulary**
    (**OOV**) words. If the tokenizer encounters a word that is not in its vocabulary,
    it will break the word down into sub-words and represent the word as a combination
    of its sub-word embeddings. We will explain BERT and its tokenizer in more detail
    later in this book. The benefit of using a tokenizer in language models is that
    we can limit the number of inputs to the size of our dictionary rather than all
    possible inputs. For example, BERT has a 30,000-word vocabulary size, which helps
    us limit the size of the deep learning language model. Using a bigger tokenizer
    will increase the size of the model. In the next section, we will explain how
    to use the methods that were covered in this chapter in a complete preprocessing
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining the preprocessing pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will explain a complete preprocessing pipeline that has been provided by
    the authors to you, the reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code, the input is a formatted text with encoded
    tags, similar to what we can extract from HTML web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a look at the effect of applying each step to the text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Decode/remove encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Employees details. Attached are 2 files, 1st one is pairoll, 2nd* *is healtcare!*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lowercasing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employees details. attached are 2 files, 1st one is pairoll, 2nd* *is healtcare!*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Digits to words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employees details. attached are two files, first one is pairoll, second* *is
    healtcare!*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove punctuation and other special characters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employees details attached are two files first one is pairoll second* *is
    healtcare*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Spelling corrections:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employees details attached are two files first one is payroll second* *is
    healthcare*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Remove stop words:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employees details attached two files first one payroll* *second healthcare*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Stemming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employe detail attach two file first one payrol* *second healthcar*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lemmatizing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*employe detail attach two file first one payrol* *second healthcar*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With that, we’ve learned about different preprocessing methods. Next, we’ll
    review a piece of code for performing NER and POS.
  prefs: []
  type: TYPE_NORMAL
- en: Code for NER and POS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this example, we used the spaCy library for Python to perform these tasks.
    Here our input is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output for NER:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The companies that would be releasing their quarterly DATE reports tomorrow
    DATE are Microsoft ORG , 4pm TIME , Google ORG , 4pm TIME , and AT&T ORG , 6pm*
    *TIME .*'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, using NER, we were able to detect parts of the sentence that
    are related to company names (ORG) or dates.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.1* shows an example of performing POS tagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – POS tagging using spaCy](img/B18949_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – POS tagging using spaCy
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code examples exemplify the various aspects of preprocessing,
    which processes raw text and transforms it into a form that suits the downstream
    model so that it suits the purpose of the overall design.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a range of techniques and methods for text preprocessing,
    including normalization, tokenization, stop word removal, POS tagging, and more.
    We explored different approaches to these techniques, such as rule-based methods,
    statistical methods, and deep learning-based methods. We also discussed the advantages
    and disadvantages of each method and provided examples and code snippets to illustrate
    their use.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a solid understanding of the importance of text
    preprocessing and the various techniques and methods available for cleaning and
    preparing text data for analysis. You should be able to implement these techniques
    using popular libraries and frameworks in Python and understand the trade-offs
    between different approaches. Furthermore, you should have a better understanding
    of how to process text data to achieve better results in NLP tasks such as sentiment
    analysis, topic modeling, and text classification.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explain text classification, and different methods
    for performing this task.
  prefs: []
  type: TYPE_NORMAL
