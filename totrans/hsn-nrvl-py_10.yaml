- en: Hypercube-Based NEAT for Visual Discrimination
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the main concepts behind a hypercube-based
    NEAT algorithm and about the main challenges it was designed to solve. We take
    a look at the problems that arise when attempting to use direct genome encoding
    with large-scale **artificial neural networks** (**ANN**) and how they can be
    solved with the introduction of an indirect genome encoding scheme. You will learn
    how a **Compositional Pattern Producing** **Network** (**CPPN**) can be used to
    store genome encoding information with an extra-high compression rate and how
    CPPNs are employed by the HyperNEAT algorithm. Finally, you will work with practical
    examples that demonstrate the power of the HyperNEAT algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The problem with the direct encoding of large-scale natural networks using NEAT,
    and how HyperNEAT can help by introducing the indirect encoding method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The evolution of CPPNs with NEAT to explore geometric regularities within the
    hypercube, which allows us to efficiently encode connectivity patterns within
    the target ANN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the HyperNEAT method to detect and recognize objects in a visual
    field
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The definition of the objective function for a visual discrimination experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A discussion of the visual discrimination experiment results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following technical requirements should be met in order to execute the
    experiments described in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Windows 8/10, macOS 10.13 or newer, modern Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda Distribution version 2019.03 or newer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/​Hands-
    on-​Neuroevolution-​with-​Python/​tree/​master/​Chapter7](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter7)
  prefs: []
  type: TYPE_NORMAL
- en: Indirect encoding of ANNs with CPPNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you learned about the direct encoding of ANNs using
    the nature-inspired conception of a genotype that is mapped to the phenotype in
    a `1:1` ratio to represent the ANN topology. This mapping allows us to use advanced
    NEAT algorithm features such as an innovation number, which allows us to track
    when a particular mutation was introduced during the evolution. Each gene in the
    genome has a specific value of the innovation number, allowing fast and accurate
    crossover of parent genomes to produce offspring. While this feature introduces
    immense benefits and also reduces the computational costs needed to match the
    parent genomes during the recombination, the direct encoding used to encode the
    ANN topology of the phenotype has a significant drawback as it limits the size
    of the encoded ANN. The bigger the encoded ANN, the bigger the genome that is
    evaluated during the evolution, and this involves tremendous computational costs.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tasks, primarily related to pattern recognition in images or
    other high-dimensional data sources, that require employing ANNs that have advanced
    topologies with many layers and nodes within them. Such topological configurations
    cannot be effectively processed by the classic NEAT algorithm due to the inefficiencies
    of direct encoding discussed previously.
  prefs: []
  type: TYPE_NORMAL
- en: The new method of encoding the phenotype ANN was proposed to address this drawback
    while still having all the benefits provided by the NEAT algorithm. We'll discuss
    it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: CPPN encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The proposed encoding scheme employs a method of representing the connectivity
    patterns within the phenotype ANN by querying another specialized neural network
    about the weights of the connections between the nodes. This specialized neural
    network is called a **CPPN**. Its main task is to represent the connectivity patterns
    of the phenotype ANN as a function of its geometry. The resulting connectivity
    pattern is represented as a four-dimensional hypercube. Each point of the hypercube
    encodes the connection between two related nodes within the phenotype ANN and
    is described by four numbers: the coordinates of the source node and the coordinates
    of the target node. The connective CPPN takes as input each point of the hypercube
    and calculates the weights of the connections between every node in the phenotype
    ANN. Also, a connection between two nodes is not expressed if the magnitude of
    the connection weight returned by the CPPN is less than a minimal threshold (![](img/ade9b185-b4ae-44a0-b628-082618ce0fea.png)).
    Thus, we can define the connective CPPN as a four-dimensional function returning
    the connection weight, as given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5dd4ef9f-20b1-401a-95e5-35807388b252.png)'
  prefs: []
  type: TYPE_IMG
- en: The source node of the phenotype ANN is at ![](img/c248e489-d2cf-4726-93e6-1ba5a39b6518.png), and
    the target node is at ![](img/830c9acc-1001-4637-a7d8-e01d615adf9a.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another essential feature of CPPNs is that unlike conventional ANNs, which
    employ only one type of activation function for each node (usually from the sigmoid
    family of functions), CPPNs can use multiple geometric functions as node activators.
    Due to this, CPPNs can express a rich set of geometric motifs in the produced
    connectivity patterns:'
  prefs: []
  type: TYPE_NORMAL
- en: Symmetry (Gaussian function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imperfect symmetry (Gaussian combined with an asymmetric coordinate frame)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repetition (sine function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repetition with variations (sine combined with a coordinate frame that doesn't
    repeat)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering the features of the CPPN that we've discussed, we can assume that
    the connectivity pattern produced by it can represent any network topology for
    the phenotype ANN. Also, the connectivity pattern can be used to encode large-scale
    topologies by discovering the regularities in the training data and reusing the
    same set of genes within the CPPN to encode repetitions in the phenotype ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Hypercube-based NeuroEvolution of Augmenting Topologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The methodology described in the previous section was invented by Kenneth O.
    Stanley and was called **Hypercube-based** **NeuroEvolution of Augmenting Topologies**
    (**HyperNEAT**). As its name suggests, it is an extension of the NEAT algorithm
    that we have already used in this book. The main difference between these two
    methods is that the HyperNEAT method uses an indirect encoding scheme based on
    the CPPN. During the evolution, the HyperNEAT method employs a NEAT algorithm
    to evolve a population of genomes that encode a topology of the connective CPPN.
    After that, each created CPPN can be used to establish the connectivity patterns
    within a specific phenotype ANN. Finally, the phenotype ANN can be evaluated against
    the problem space.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have discussed how connectivity patterns can be evolved using NEAT
    with a CPPN and can be applied to the nodes of the phenotype ANN. However, we
    have not mentioned how the geometric layout of the nodes is determined in the
    first place. The responsibility of defining the nodes and their positions (layout)
    is assigned to the human architect. The architect analyzes the problem space and
    utilizes the most appropriate layout.
  prefs: []
  type: TYPE_NORMAL
- en: 'By convention, the initial layout of the nodes of the phenotype ANN has a name:
    substrate. There are several types of substrate configuration (layout), and they
    have proven their efficiency for particular tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-dimensional grid**: A regular grid of network nodes in a two-dimensional
    Cartesian space centered at (0,0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Three-dimensional grid**: A regular grid of network nodes in a three-dimensional
    Cartesian space centered at (0,0,0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State-space sandwich**: Two two-dimensional planar grids with corresponding source
    and target nodes in which one layer can only send connections in the direction
    of the other one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circular**: A regular radial structure suited to define regularities in radial
    geometry based on polar coordinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By arranging the ANN nodes in an appropriate layout on the substrate, it is
    possible to exploit regularities in the geometry of the problem space. That significantly
    increases the efficiency of the encoding by using the connective CPPN to draw
    connectivity patterns between the substrate nodes. Let's now look at the basics
    of a visual discrimination experiment.
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the HyperNEAT method, please refer to [Chapter 1](f59c6396-55e5-4495-95c0-7af9a42c2f20.xhtml),
    *Overview of Neuroevolution Methods.*
  prefs: []
  type: TYPE_NORMAL
- en: Visual discrimination experiment basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have already mentioned, the main advantage of the indirect encoding employed
    by the HyperNEAT algorithm is the ability to encode the topology of the large-scale
    ANN. In this section, we will describe an experiment that can be used to test
    the capacity of the HyperNEAT method to train a large-scale ANN. Visual pattern
    recognition tasks typically require large ANNs as detectors due to the high dimensionality
    of the input data (the image height multiplied by the image width). In this chapter,
    we consider a variation of this family of computer science problems called visual
    discrimination tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task of visual discrimination is to distinguish a large object from a small
    object in a two-dimensional visual space, regardless of their positions in the
    visual space and their positions relative to each other. The visual discrimination
    task is performed by a specialized discriminator ANN, which is built on a substrate
    configured as a state-space sandwich with two sheets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visual field is a two-dimensional array of sensors that can be in two states:
    on or off (black and white).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The target field is a two-dimensional array of outputs with activation values
    in the `[0,1]` range.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The scheme of the visual discrimination task is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c50c3901-45c5-40e5-a73d-774997ad8083.png)'
  prefs: []
  type: TYPE_IMG
- en: The visual discrimination task
  prefs: []
  type: TYPE_NORMAL
- en: You can see in the diagram that the objects to be detected are represented as
    two squares separated by an empty space. The larger object is precisely three
    times bigger than the other one. The algorithm we are trying to build needs to
    detect the center of the larger object. The detection is based on measuring the
    activation values of the ANN nodes in the target field. The position of the node
    with the highest activation value marks the center of the detected object. Our
    goal is to discover the right connectivity patterns between visual and target
    fields that align the output node with the highest activation and the center of
    the big object in the visual field. Also, the discovered connectivity pattern
    should be invariant to the relative positions of both objects.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm for the task of visual discrimination needs to evaluate a large
    number of inputs - the values representing the cells in the visual field. Also,
    the successful algorithm needs to discover the strategy that can process inputs
    from multiple cells simultaneously. Such a strategy should be based on the general
    principle that allows the detection of the relative sizes of the objects in the
    visual field. The visual field in our experiment is represented as a two-dimensional
    grid. Thus, the general geometric principle to be discovered is the concept of locality.
  prefs: []
  type: TYPE_NORMAL
- en: We can exploit the locality principle in the substrate configuration of the
    discriminator ANN that we have chosen by discovering a particular pattern in the
    scheme of the links that connect the nodes of the visual and the target fields.
    In this connection scheme, separate nodes of the visual field are connected to
    the multiple adjacent output nodes around a specific location in the target field.
    As a result, the more activations the output node collects, the more signals are
    supplied into it through connections with individual input nodes.
  prefs: []
  type: TYPE_NORMAL
- en: To effectively exploit the locality principle mentioned previously, the representation
    of connections should take into account the geometry of the discriminator ANN
    substrate and the fact that the correct connectivity pattern repeats across it.
    The best candidate for such a representation is a CPPN, which can discover the
    local connectivity pattern once and repeat it across the substrate grid at any
    resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Objective function definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main task of the visual discriminator is to correctly determine the position
    of a larger object regardless of the relative positions of both objects. Thus,
    we can define the objective function to guide the neuroevolution process. The
    objective function should be based on the Euclidean distance between the exact
    position of the larger object in the visual field and its predicted position in
    the target field.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function can be directly represented as the Euclidean distance between
    the actual and predicted positions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/699dc1b0-b237-4196-8156-039e0ab72b66.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/510315d6-bb2f-4e07-bad5-0f1feeb1da7d.png) is a loss function, ![](img/91dfcca9-9a9a-4677-b363-98a6577866fc.png) is
    the ground truth coordinates of the big object, and ![](img/f77190a1-381b-4b78-843f-2a578f739538.png) is
    predicted by the discriminator ANN coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the loss function as defined previously, we can write the objective function
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ecb5686-c207-44ca-bce5-3c9103ebe605.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a5fa600f-b85b-497a-a646-2f398d51b634.png) is the maximal possible distance
    between the two points within the target field space. The objective function formula
    guarantees that the calculated fitness score (![](img/96b5cab2-09d7-45ab-8cad-e9443b8e07c6.png)
    ) always falls within the `[0,1] `range. Now that we know the basics of the visual
    discrimination experiment, let''s start with setting it up.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual discrimination experiment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our experiment, during the training of the discriminator ANN, we use the
    resolution of the visual and target fields fixed at 11 x 11\. Thus, the connective
    CPPN must learn the correct connectivity pattern between the 121 inputs of the
    visual field and the 121 outputs of the target fields, which results in a total
    of 14,641 potential connection weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the scheme of the substrate for the discriminator
    ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d14d3979-7aaf-4756-803f-23d01cde0226.png)'
  prefs: []
  type: TYPE_IMG
- en: The state-space sandwich substrate of the discriminator ANN
  prefs: []
  type: TYPE_NORMAL
- en: The discriminator ANN shown in the diagram has two layers with nodes forming
    one two-dimensional planar grid per layer. The connective CPPN draws the connectivity
    patterns by connecting nodes from one layer to another.
  prefs: []
  type: TYPE_NORMAL
- en: At each generation of the evolution, each individual in the population (the
    genome encoding the CPPN) is evaluated for its ability to create connectivity
    patterns of the discriminator ANN. The discriminator ANN is then tested to see
    whether it can find the center of the large object within the visual field. There
    are a total of 75 evaluation trials for a particular ANN, in which two objects
    are placed at different locations in each trial. At each trial, we put a small
    object in one of the 25 positions uniformly distributed in the visual field. The
    center of a large object is five steps from the small object to the right, down,
    or diagonally. If a large object doesn't fit into the visual field completely,
    then it wraps around to the other side. Thus, considering the logic of the placement
    of objects relative to each other and the grid, we should be able to evaluate
    all possible configurations in 75 trials.
  prefs: []
  type: TYPE_NORMAL
- en: Our experiment setup has two major parts, which we will discuss in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Visual discriminator test environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First we need to define the test environment and provide access to the dataset,
    which contains all the possible visual field configurations as described in the
    previous section. The dataset used in this experiment is created during the test
    environment initialization. We will discuss dataset creation later in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test environment has two major components:'
  prefs: []
  type: TYPE_NORMAL
- en: The data structure to maintain the visual field definition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The test environment manager, which stores the dataset and provides a means
    to evaluate discriminator ANNs against it
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we provide a detailed description of these components.
  prefs: []
  type: TYPE_NORMAL
- en: Visual field definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We store the configuration of the visual field for each of the 75 trials discussed
    previously in the `VisualField` Python class. It has the following constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The constructor of `VisualField` accepts as parameters the tuple with coordinates
    (*x*, *y*) of the large and small object, as well as the size of the visual field.
    We consider the square visual field, so the size of the visual field along each
    axis is equal. The visual field is internally represented as a two-dimensional
    binary array where ones represent positions occupied by objects, and zeros are
    empty spaces. It is stored in the `self.data` field, which is a NumPy array with
    the shape (2, 2).
  prefs: []
  type: TYPE_NORMAL
- en: 'The small object has a size 1 x 1, and the large object is three times bigger.
    The following snippet from the constructor''s source code creates the representation
    of the big object in the data array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The constructor of the `VisualField` class receives the coordinates of the center
    of the big object as a tuple, (`x`, `y`). The preceding code draws the big object
    starting from the top-left corner (`x-1`, `y-1`) and ending at the bottom-right
    corner (`x+1`, `y+1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_set_point(self, x, y)` function referred to in the preceding code sets
    the `1.0` value at the specific position in the `self.data` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The `_set_point(self, x, y)` function performs coordinate wrapping when the
    coordinate value exceeds the allowed number of dimensions per axis. For example,
    for the *x* axis, the source code for the coordinate value wrapping is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The source code for coordinate wrapping along the *y* axis is similar.
  prefs: []
  type: TYPE_NORMAL
- en: After wrapping the coordinates specified as parameters of the function (if needed),
    we set the corresponding positions in the `self.data` field to have a value of
    `1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: NumPy indexes as `[row, column]`. Thus, we need to use *y* in the first position
    and *x* in the second position of the index.
  prefs: []
  type: TYPE_NORMAL
- en: Visual discriminator environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The visual discriminator environment holds the generated dataset with the visual
    field definitions. Also, it provides methods to create the dataset and to evaluate
    the discriminator ANN against the dataset. The `VDEnvironment` Python class holds
    the definitions of all mentioned methods, as well as related data structures.
    Next, we''ll look at all the significant parts of the `VDEnvironment` class definition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The class constructor is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The first parameter of the `VDEnvironment` constructor is an array with the
    definitions of all the possible small object positions defined as a sequence of
    coordinate values for each axis. The second parameter defines the offset of the
    coordinates of the center of the big object from the small object coordinates.
    We use `5` as the value of this parameter in our experiment. Finally, the third
    parameter is the visual field size along with both dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the received parameters are saved into object fields, we calculate
    the maximum possible distance between two points in the visual field as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The Euclidean distance between the top-left and the bottom-right corner of the
    visual field is then stored in the `self.max_dist` field. This value will be used
    later to normalize the distances between points in the visual field by keeping
    them in the `[0, 1]` range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_create_data_set()` function creates all possible datasets given the specified
    environment parameters. The source code of this function is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The function iterates over the small object positions along two axes and tries
    to create the big object at coordinates that are to the right, below, or on a
    diagonal from the small object coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_create_visual_field` function creates the appropriate visual field configuration
    using the coordinates of the small object (`sx`, `sy`) and an offset of the big
    object''s center (`x_off`, `y_off`). The following source code shows how this
    is implemented:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If the coordinates of the big object calculated by the preceding function are
    outside the visual field space, we apply the wrapping as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding snippet shows the wrapping along the *x* axis. The wrapping along
    the *y* axis is similar. Finally, the `VisualField` object is created and returned
    to be appended to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the most exciting part of the `VDEnvironment` definition is related
    to the evaluation of the discriminator ANN, which is defined in the `evaluate_net(self,
    net)` function as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function receives the discriminator ANN as a parameter, and returns
    the evaluated fitness score and the mean distance between the detected coordinates
    of the big object and the ground truth values calculated for all evaluated visual
    fields. The average distance is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding source code iterates over all `VisualField` objects in the dataset,
    and uses the discriminator ANN to determine the coordinates of the big object.
    After that, we calculate the distance (detection error) between the ground truth
    and the predicted position of the big object. Finally, we find the mean of the
    detection errors and normalize it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum possible error value is `1.0`, according to the preceding code.
    The value of the fitness score is a complement to the `1.0` of the error value
    since it increases as the error decreases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `evaluate_net` function returns the calculated fitness score along with
    the unnormalized detection error.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `evaluate_net_vf(self, net, vf)` function provides a means to evaluate
    the discriminator ANN against a specific `VisualField` object. It is defined as
    follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function receives the discriminator ANN as the first parameter
    and the `VisualField` object as the second parameter. After that, it obtains the
    flattened input array from the `VisualField` object and uses it as input to the
    discriminator ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: After we set the inputs of the discriminator ANN, it must be activated to propagate
    input values through all network nodes. Our discriminator ANN has only two layers,
    as determined by the space-sandwich substrate configuration. Thus we need to activate
    it twice—once per each layer. After propagation of the activation signal through
    both layers of the discriminator ANN, we can determine the position of the big
    object in the target field as an index of the maximal value in the output array.
    Using the `_big_object_coordinates(self, outputs)` function, we can extract the
    Cartesian coordinates (*x*, *y*) of the big object within the target field.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the `evaluate_net_vf` function returns the raw output array along with
    the extracted Cartesian coordinates (*x*, *y*) of the big object in the target
    field space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `_big_object_coordinates(self, outputs)` function extracts the Cartesian
    coordinates of the big object within the target field space from the raw outputs
    obtained from the discriminator ANN. The function''s source code is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, the function enumerates through the output array and finds the index
    of the maximal value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, it uses the index it finds to estimate the Cartesian coordinates,
    taking into account the size of the target field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the function returns the tuple (*x*, *y*) with the Cartesian coordinates
    of the big object within the target field.
  prefs: []
  type: TYPE_NORMAL
- en: For complete implementation details, please look at `vd_environment.py` at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_environment.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_environment.py).
  prefs: []
  type: TYPE_NORMAL
- en: Experiment runner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we described earlier, the solution for the visual discrimination task can
    be found using the HyperNEAT method. Thus, we need to use a library that provides
    an implementation of the HyperNEAT algorithm. The MultiNEAT Python library is
    the right candidate for this experiment. As such, we are implementing our experiment
    using this library.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discuss the most critical components of the experiment runner implementation.
  prefs: []
  type: TYPE_NORMAL
- en: For complete implementation details, please refer to `vd_experiment_multineat.py`
    at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_experiment_multineat.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_experiment_multineat.py).
  prefs: []
  type: TYPE_NORMAL
- en: The experiment runner function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `run_experiment` function allows us to run the experiment using the provided
    hyperparameters and the initialized visual discriminator test environment. The
    function implementation has the following parts.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the first CPPN genome population
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code, at first, we initialize the random number generator
    seed with the current system time. After that, we create the appropriate substrate
    configuration for the discriminator ANN that is able to operate over the dimensionality
    of the experiment''s visual field. Next, we create the CPPN genome using the created
    substrate configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The CPPN genome created in the preceding code has the appropriate number of
    input and output nodes provided by the substrate. Initially, it uses the unsigned
    sigmoid as the node activation function. Later, during the evolution, the activation
    function type at each node of the CPPN will be changed, following the HyperNEAT
    algorithm routines. Finally, the initial population is created using the initialized
    CPPN genome and the HyperNEAT hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Running the neuroevolution over the specified number of generations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the beginning of this part, we create the intermediate variables to hold
    the execution results and create the statistics collector (`Statistics`). After
    that, we execute the evolution loop for the number of generations specified in
    the `n_generations` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the evolution loop, we obtain the list of genomes belonging to the population
    at the current generation and evaluate all genomes from the list against the test
    environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We save the values returned by the `eval_genomes(genomes, substrate, vd_environment,
    generation)` function for the current generation into the statistics collector.
    Also, we use the fitness score returned by `eval_genomes` to estimate whether
    a successful solution has been found or not. If the fitness score exceeds the
    `FITNESS_THRESHOLD` value, we consider that a successful solution has been found.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a successful solution was found or the current fitness score is the maximum
    fitness score ever achieved, we save the CPPN genome and the current fitness score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, if a successful solution is found, we break the evolution loop and move
    to the reporting steps, which we will discuss later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If a successful solution was not found, we print the statistics for the current
    generation and advance to the next generation with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: After the main evolution loop, the results of the experiment are reported, which
    uses the statistics collected in the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the results of the experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The experiment results reported and saved in textual and graphical representations
    (SVG files). We start by printing general performance statistics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The first three lines of the preceding code print the best ever fitness score
    obtained among all the generations of evolution to the console. After that, we
    print the experiment's elapsed time and the random seed value used.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we requested to save or show visualizations, the corresponding functions
    are invoked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code draws the network graph of the CPPN and prints the statistics
    of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we move to the visualization of the output of the discriminator ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we create the discriminator ANN using the best CPPN genome
    found during the evolution. After that, we draw the activation outputs obtained
    by running the evaluation of the discriminator ANN against the test environment.
    We use the visual field that is randomly selected from the dataset of the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we render the general statistics collected during the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The statistics plot includes the best fitness scores and the average error distances
    drawn over the generations of evolution.
  prefs: []
  type: TYPE_NORMAL
- en: For implementation details of the visualization functions mentioned in this
    section, please refer to `visualize.py` at [https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/visualize.py](https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/visualize.py).
  prefs: []
  type: TYPE_NORMAL
- en: The substrate builder function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The HyperNEAT method is built around the notion of the substrate that defines
    the structure of the discriminator ANN. Therefore, it is crucial to create an
    appropriate substrate configuration to be used during the experiment execution.
    The substrate creation routines are defined in the following two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The substrate builder function `create_substrate` creates the substrate object
    as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function first creates the two grid-based Cartesian sheets that
    represent inputs (the visual field) and outputs (the target field) of the substrate
    configuration. Remember that for this experiment we selected a state-space sandwich
    substrate configuration. After that, the substrate instance was initialized using
    the created field configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Please note that the substrate doesn't use any hidden nodes; we provide an empty
    list instead of hidden nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we configure the substrate to only allow connections from input to output
    nodes and to use a signed sigmoid activation function at the output nodes. Finally,
    we set the maximum values for the bias and the connection weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `create_sheet_space` function invoked by the substrate builder function
    is defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `create_sheet_space` function receives the start and end coordinates of
    the grid within one dimension along with the number of grid dimensions. Also,
    the *z* coordinate of the sheet is provided. Using the specified parameters, the
    preceding code creates the uniform linear space with coordinates starting in the `[start,
    stop]` range with a step of `dim`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we use this linear space to populate the two-dimensional array
    with the coordinates of the grid nodes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `create_sheet_space` function returns the grid configuration in the form
    of a two-dimensional array.
  prefs: []
  type: TYPE_NORMAL
- en: Fitness evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The genome''s fitness evaluation is a significant part of any neuroevolution
    algorithm, including the HyperNEAT method. As you''ve seen, the main experiment
    loop invokes the `eval_genomes` function to evaluate the fitness of all genomes
    within a population for each generation. Here, we consider the implementation
    details of the fitness evaluation routines, which consists of two main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `eval_genomes` function evaluates all genomes in the population:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The `eval_genomes` function takes a list of genomes, the discriminator ANN
    substrate configuration, the initialized test environment, and the ID of the current
    generation as parameters. The first lines of the function create intermediate
    variables, which are used to store the evaluation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we iterate over all the genomes in the population and collect appropriate
    statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the `eval_genomes` function returns the collected statistics as a tuple, `(best_genome,
    max_fitness, distances)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `eval_individual` function allows us to evaluate the fitness of the individual
    genome as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the beginning, the preceding source code creates the discriminator ANN phenotype
    using the CPPN genome provided as a parameter. After that, the discriminator ANN
    phenotype evaluated against the test environment.
  prefs: []
  type: TYPE_NORMAL
- en: The `eval_individual` function returns the fitness score and error distance
    obtained from the test environment during the phenotype evaluation. Now that we
    have completed the setup, let us start with the visual discrimination experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Visual discrimination experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having done all of the necessary setup steps, we are ready to start the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the visual discrimination experiment, we use the following configuration
    of the visual field:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| Size of the visual field | 11 x 11 |'
  prefs: []
  type: TYPE_TB
- en: '| Positions of the small objects in the visual field along each axis | [1,
    3, 5, 7, 9] |'
  prefs: []
  type: TYPE_TB
- en: '| Size of the small object | 1 x 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Size of the big object | 3 x 3 |'
  prefs: []
  type: TYPE_TB
- en: '| Offset of the center of the big object from the small object | 5 |'
  prefs: []
  type: TYPE_TB
- en: Next, we need to select the appropriate values of the HyperNEAT hyperparameters,
    allowing us to find a successful solution to the visual discrimination problem.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the hyperparameter that we describe next determines how to evolve
    the connective CPPN using the neuroevolution process. The discriminator ANN is
    created by applying the connective CPPN to the substrate.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MultiNEAT library uses the `Parameters` Python class to hold all the required
    hyperparameters. To set the appropriate values of the hyperparameters, we define
    the `create_hyperparameters` function in the experiment runner Python script.
    Here, we describe the essential hyperparameters that have a significant impact
    on the HyperNEAT algorithm performance in this experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `create_hyperparameters` function begins by creating a `Parameters` object to
    hold the HyperNEAT parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We decided to start with a medium-sized population of genomes to keep the computations
    fast. At the same time, we want to maintain a sufficient number of organisms in
    the population for evolutionary diversity. The population size is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We are interested in producing compact CPPN genomes that have as few nodes
    as possible to increase the effectiveness of indirect encoding. Thus, we set a
    tiny probability of adding a new node during evolution, and also keep the probability
    of creating a new connection quite low:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The HyperNEAT method produces CPPN genomes with different types of activation
    functions in the hidden and output nodes. Thus, we define the probability of mutation
    that changes the type of the node activation. Also, in this experiment, we are
    interested in using only four types of activation function: signed Gaussian, signed
    sigmoid, signed sine, and linear. We set the likelihood of choosing any activation
    type among the four we just mentioned to `1.0`, which effectively makes the probability
    of choosing each type equal. We define this in the hyperparameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define the number of species within the population to be kept in
    the `[5,10]` range and set the value of the species stagnation parameter to `100`
    generations. This configuration maintains moderate species diversity, but keeps
    species for long enough to allow them to evolve and produce useful CPPN genome
    configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The selection of hyperparameters presented here demonstrates the high efficiency
    of producing successful CPPN genomes during the evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Working environment setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this experiment, we use the MultiNEAT Python library, which provides the
    implementation of the HyperNEAT algorithm. Thus, we need to create an appropriate
    Python environment, which includes the MultiNEAT Python library and all the necessary
    dependencies. This can be done using Anaconda by executing the following commands
    in the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: These commands create and activate a `vd_multineat` virtual environment with
    Python 3.5\. After that, they install the latest version of the MultiNEAT Python
    library, along with the dependencies that are used by our code for the result
    visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Running the visual discrimination experiment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start the experiment, you need to enter the local directory that contains
    the `vd_experiment_multineat.py` script, and execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Do not forget to activate the appropriate virtual environment with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '**`$ conda activate vd_multineat`**'
  prefs: []
  type: TYPE_NORMAL
- en: 'After a particular number of generations, the successful solution will be found,
    and you will see lines similar to the following in the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The console output says that the solution was found at generation `17`. The
    ID of the successful CPPN genome is `2565`, and this genome has 10 nodes and 16
    connections among them. Also, you can see the results of the evaluation of the
    discriminator ANN produced by the best CPPN genome against the randomly selected
    visual field.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the detected Cartesian coordinates of the big object in the target
    field and the actual coordinates in the visual field are the same (5, 1), which
    means that the solution found is capable of visual discrimination with exact precision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, it is interesting to take a look at the visualization of the activation
    outputs of the discriminator ANN obtained during test evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c11a26be-bf38-494e-84a5-16ccf7802e91.png)'
  prefs: []
  type: TYPE_IMG
- en: The target field activations of the discriminator ANN
  prefs: []
  type: TYPE_NORMAL
- en: The right part of the preceding plot renders the activation values of the target
    field (the output layer) of the discriminator ANN, which we obtained during evaluation
    against a random visual field. Also, in the left part of the plot, you can see
    the actual visual field configuration. As you can see, the maximum target field
    activation value (the darkest cell) is precisely at the same position as the center
    of the big object in the visual field, having the coordinates (`5`, `1`).
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from the preceding graph, the scale of ANN activation values
    is extremely low: the minimum activation is `~1e-13`, and the maximum is only
    `~9e-13`. A human-designed ANN would probably be normalized so that the output
    is on `[0,1]`, having a minimum close to zero and a maximum near one. However,
    we only require the activation to have a maximum in the right place, and the network
    is free to choose an output activation scheme that most folks would view as unusual.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another plot allows you to study how the evolution process performed over the
    generations of evolution and how good the produced connective CPPNs were in creating
    successful discriminator ANNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81b24e28-f535-46a1-ac00-c0303ae401a0.png)'
  prefs: []
  type: TYPE_IMG
- en: The best fitness scores and average error distances of discriminator ANNs
  prefs: []
  type: TYPE_NORMAL
- en: The preceding plot renders the change in the fitness scores (the ascending line)
    and the average error distances (the descending line) for each generation of the
    evolution. You can see that the fitness scores almost reached the maximum value
    in the third generation of the evolution and needed seven more generations to
    elaborate over the CPPN genome configurations to finally find the winner. Also,
    you can see that the average error distance between the detected and the ground
    truth position of the big object gradually decreases during the evolution process.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the most exciting part of this experiment is shown in the following
    diagram of the CPPN phenotype graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e49c120b-23b9-443f-80ea-a456227ddb41.png)'
  prefs: []
  type: TYPE_IMG
- en: The CPPN phenotype graph of the best genome
  prefs: []
  type: TYPE_NORMAL
- en: The plot demonstrates the network topology of the CPPN phenotype that was used
    to draw connections over the discriminator ANN producing the successful visual
    discriminator. In the CPPN phenotype plot, input nodes are marked with squares,
    output nodes are filled circles, and the bias node is a diamond.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two output nodes of the CPPN have the following meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: The first node (8) provides the weight of the connection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second node (9) determines whether the connection is expressed or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CPPN input nodes are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first two nodes (0 and 1) set the point coordinates (*x*, *y*) in the input
    layer of the substrate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next two nodes (2 and 3) set the point coordinates (*x*, *y*) in the hidden
    layer of the substrate (not used in our experiment).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next two nodes (4 and 5) set the point coordinates (*x*, y) in the output
    layer of the substrate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last node (6) sets the Euclidean distance from the point in the input layer
    from the origin of the coordinates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, you can see that the CPPN phenotype doesn't include any hidden nodes.
    For the visual discrimination task, the neuroevolutionary process was able to
    find the appropriate activation function types for the output nodes of the CPPN.
    This finding allows the connective CPPN to draw the correct connectivity patterns
    within the substrate of the discriminator ANN.
  prefs: []
  type: TYPE_NORMAL
- en: By counting the number of nodes and connections between them as presented in
    the preceding plot, you can feel the power of the indirect encoding method introduced
    by the HyperNEAT algorithm. With only 16 connections between 10 nodes, the CPPN
    phenotype was able to expose the connectivity pattern of the substrate, which
    for the visual field at 11 x 11 resolution can have up to 14,641 connections between
    the nodes in the visual field and the target field. So, we achieved an information
    compression ratio of about 0.11%, which is pretty impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Such a high compression rate is possible because of the discovery of the geometric
    regularities within the connectivity motifs of the substrate by the connective
    CPPN. Using the regularities of the discovered patterns, the CPPN can store only
    a few patterns (local connectivity motifs) for the whole connectivity space of
    the substrate. After that, the CPPN can apply these local patterns multiple times
    at different substrate positions to draw the full connectivity scheme between
    the substrate layers, in our case, to draw connections between the input layer
    (visual field) and the output layer (target field).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Try to decrease the value of the `params.PopulationSize` hyperparameter and
    see what happens. How did this affect the algorithm's performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to set zero probabilities for the values of the following hyperparameters: `params.ActivationFunction_SignedGauss_Prob`,
    `params.ActivationFunction_SignedSigmoid_Prob`, and `params.ActivationFunction_SignedSine_Prob`.
    Was a successful solution found with these changes? How did this affect the configuration
    of the substrate connections?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print out the winning genome, try to come up with a visualization, then see
    how your intuition from looking at the genome matches with the visualized CPPN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the method of indirect encoding of the ANN
    topology using CPPNs. You learned about the HyperNEAT extension of the NEAT algorithm,
    which uses a connective CPPN to draw connectivity patterns within the substrate
    of the phenotype of the discriminator ANN. Also, we demonstrated how the indirect
    encoding scheme allows the HyperNEAT algorithm to work with large-scale ANN topologies,
    which is common in pattern recognition and visual discrimination tasks.
  prefs: []
  type: TYPE_NORMAL
- en: With the theoretical background we provided, you have had the chance to improve
    your coding skills by implementing the solution for a visual discrimination task
    using Python and the MultiNEAT library. Also, you learned about a new visualization
    method that renders the activation values of the nodes in the output layer of
    the discriminator ANN and how this visualization can be used to verify the solution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how the HyperNEAT method can be further
    improved by introducing an automatic way to generate the appropriate substrate
    configuration. We will consider the **Evolvable Substrate HyperNEAT** (**ES-HyperNEAT**)
    extension of the NEAT algorithm and see how it can be applied to solve practical
    tasks that require the modular topologies of the solver ANN.
  prefs: []
  type: TYPE_NORMAL
