<html><head></head><body>
<div epub:type="chapter" id="_idContainer040">
<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor014"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/><span class="koboSpan" id="kobo.2.1">Analyzing Text Data with Deep Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Language is one of the most amazing abilities of human beings; it evolves during the individual’s lifetime and is capable of conveying a message with complex meaning. </span><span class="koboSpan" id="kobo.3.2">Language in its natural form is not understandable to machines, and it is extremely challenging to develop an algorithm that can pick up the different nuances. </span><span class="koboSpan" id="kobo.3.3">Therefore, in this chapter, we will discuss how to represent text in a form that is digestible </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">by machines.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In natural form, text cannot be directly fed to a </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">deep learning</span></strong><span class="koboSpan" id="kobo.7.1"> model. </span><span class="koboSpan" id="kobo.7.2">In this chapter, we will discuss how text can be represented in a form that can be used by </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">machine learning</span></strong><span class="koboSpan" id="kobo.9.1"> models. </span><span class="koboSpan" id="kobo.9.2">Starting with natural text, we will transform the text into numerical vectors that are increasingly sophisticated (one-hot encoding, </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">bag of words</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">BoW</span></strong><span class="koboSpan" id="kobo.13.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">term frequency-inverse document frequency</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.16.1">TF-IDF</span></strong><span class="koboSpan" id="kobo.17.1">)) until we create vectors of real numbers that represent the meaning of a word (or document) and allow us to conduct operations (word2vec). </span><span class="koboSpan" id="kobo.17.2">In this chapter, we introduce deep learning models, such as </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">recurrent neural networks</span></strong><span class="koboSpan" id="kobo.19.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.20.1">RNNs</span></strong><span class="koboSpan" id="kobo.21.1">), l</span><strong class="bold"><span class="koboSpan" id="kobo.22.1">ong short-term memory</span></strong><span class="koboSpan" id="kobo.23.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.24.1">LSTM</span></strong><span class="koboSpan" id="kobo.25.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.26.1">gated recurrent units </span></strong><span class="koboSpan" id="kobo.27.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.28.1">GRUs</span></strong><span class="koboSpan" id="kobo.29.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.30.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.31.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.32.1">CNNs</span></strong><span class="koboSpan" id="kobo.33.1">), to analyze sequences and discuss their strengths as well as the problems associated with them. </span><span class="koboSpan" id="kobo.33.2">Finally, we will assemble these models all together to conduct text classification, showing the power of the learned approaches. </span></p>
<p><span class="koboSpan" id="kobo.34.1">By the end of this chapter, we will be able to take a corpus of text and use deep learning to analyze it. </span><span class="koboSpan" id="kobo.34.2">These are the bases that will help us understand how a </span><strong class="bold"><span class="koboSpan" id="kobo.35.1">large language model</span></strong><span class="koboSpan" id="kobo.36.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.37.1">LLM</span></strong><span class="koboSpan" id="kobo.38.1">) (such as ChatGPT) </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">works internally.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">In this chapter, we'll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.41.1">following topics</span></span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.43.1">Representing text </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">for AI</span></span></li>
<li><span class="koboSpan" id="kobo.45.1">Embedding, application, </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">and representation</span></span></li>
<li><span class="koboSpan" id="kobo.47.1">RNNs, LSTMs, GRUs, and CNNs </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">for text</span></span></li>
<li><span class="koboSpan" id="kobo.49.1">Performing sentiment analysis with embedding and </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">deep learning</span></span></li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.51.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.52.1">In this chapter, we will use standard libraries for Python. </span><span class="koboSpan" id="kobo.52.2">The necessary libraries can be found within each of the Jupyter notebooks that are in the GitHub repository for this chapter: </span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1"><span class="koboSpan" id="kobo.53.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1</span></a><span class="koboSpan" id="kobo.54.1">. </span><span class="koboSpan" id="kobo.54.2">The code can be executed on a CPU, but a GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">is advised.</span></span></p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.56.1">Representing text for AI</span></h1>
<p><span class="koboSpan" id="kobo.57.1">Compared </span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.58.1">to other types of data (such as images or tables), it is much more challenging to represent text in a digestible representation</span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.59.1"> for computers, especially because there is no unique relationship between the meaning of a word (signified) and the symbol that represents it (signifier). </span><span class="koboSpan" id="kobo.59.2">In fact, the meaning of a word changes from the context and the author’s intentions in using it in a sentence. </span><span class="koboSpan" id="kobo.59.3">In addition, native text has to be transformed into a numerical representation to be ingested by an algorithm, which is not a trivial task. </span><span class="koboSpan" id="kobo.59.4">Nevertheless, several approaches were initially developed to be able to find a vector representation of a text. </span><span class="koboSpan" id="kobo.59.5">These vector representations have the advantage that they can then be used as input to </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">a computer.</span></span></p>
<p><span class="koboSpan" id="kobo.61.1">First, a collection </span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.62.1">of texts (</span><strong class="bold"><span class="koboSpan" id="kobo.63.1">corpus</span></strong><span class="koboSpan" id="kobo.64.1">) should be divided into fundamental units (words). </span><span class="koboSpan" id="kobo.64.2">This process requires making certain decisions and process operations that collectively are </span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.65.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">text normalization</span></strong><span class="koboSpan" id="kobo.67.1">. </span><span class="koboSpan" id="kobo.67.2">A sentence, therefore, is divided into words by exploiting the natural division of spaces (</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">text segmentation</span></strong><span class="koboSpan" id="kobo.69.1">); each punctuation mark</span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.70.1"> is also considered a single word. </span><span class="koboSpan" id="kobo.70.2">In fact, punctuation marks are considered to be the boundaries of sentences and convey important information (change of topic, </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">questions, exclamations).</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">The second step is the definition of what a word is and whether some terms in the corpus should be directly joined under the same vocabulary instance. </span><span class="koboSpan" id="kobo.72.2">For example, “He” and “he” represent the same instance; the former is only capitalized. </span><span class="koboSpan" id="kobo.72.3">Since an algorithm does not include such nuances, one must normalize the text in lowercase. </span><span class="koboSpan" id="kobo.72.4">In some cases, we want to conduct more sophisticated normalizations</span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.73.1"> such as </span><strong class="bold"><span class="koboSpan" id="kobo.74.1">lemmatization</span></strong><span class="koboSpan" id="kobo.75.1"> (joining words with the same root: “came” and “comes” are two forms of the verb) or </span><strong class="bold"><span class="koboSpan" id="kobo.76.1">stemming</span></strong><span class="koboSpan" id="kobo.77.1"> (stripping</span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.78.1"> all suffixes </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">of words).</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.80.1">Tokenization</span></strong><span class="koboSpan" id="kobo.81.1"> is the </span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.82.1">task of transforming a text into fundamental units. </span><span class="koboSpan" id="kobo.82.2">This is because, in addition to words, a text may also include percentages, numbers, websites, and other components. </span><span class="koboSpan" id="kobo.82.3">We will return to this later, but in the meantime, we will look at some simpler forms </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">of tokenization.</span></span></p>
<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.84.1">One-hot encoding</span></h2>
<p><span class="koboSpan" id="kobo.85.1">In</span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.86.1"> traditional </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">natural language processing</span></strong><span class="koboSpan" id="kobo.88.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.89.1">NLP</span></strong><span class="koboSpan" id="kobo.90.1">), text representation is conducted using</span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.91.1"> discrete symbols. </span><span class="koboSpan" id="kobo.91.2">The simplest example is one-hot encoding. </span><span class="koboSpan" id="kobo.91.3">From a</span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.92.1"> sequence of text in a corpus (consisting of </span><em class="italic"><span class="koboSpan" id="kobo.93.1">n</span></em><span class="koboSpan" id="kobo.94.1"> different words), we obtain an </span><em class="italic"><span class="koboSpan" id="kobo.95.1">n</span></em><span class="koboSpan" id="kobo.96.1">-dimensional vector. </span><span class="koboSpan" id="kobo.96.2">In fact, the first step is to compute the set of different words present in the whole text corpus called vocabulary. </span><span class="koboSpan" id="kobo.96.3">For each word, we obtain a vector as long as the size of the vocabulary. </span><span class="koboSpan" id="kobo.96.4">Then for each word, we will have a long vector composed mainly of zeros and ones to represent the word (one-hot vectors). </span><span class="koboSpan" id="kobo.96.5">This system is mainly used when we want a matrix of features and then train a model. </span><span class="koboSpan" id="kobo.96.6">This process is also </span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.97.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">vectorization</span></strong><span class="koboSpan" id="kobo.99.1">; here's a sparse vector for the following </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">two words</span></span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>p</mi><mi>i</mi><mi>z</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>a</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.102.1">There are different problems associated with this representation. </span><span class="koboSpan" id="kobo.102.2">First, it captures only the presence (or the absence) of a word in a document. </span><span class="koboSpan" id="kobo.102.3">Thus, we are losing all the semantic relationships between the words. </span><span class="koboSpan" id="kobo.102.4">Second, an average language has about 200,000 words, so for each word, we would have a vector of length 200,000. </span><span class="koboSpan" id="kobo.102.5">This leads to very sparse and high-dimensional vectors. </span><span class="koboSpan" id="kobo.102.6">For large corpora, we need high memory to store the vectors and high computational capacity to handle them. </span><span class="koboSpan" id="kobo.102.7">In addition, there is no notion of similarity. </span><span class="koboSpan" id="kobo.102.8">The two words in the preceding example are two places that sell food, and we would like the vectors representing these words to encode this similarity. </span><span class="koboSpan" id="kobo.102.9">If the vectors had a notion of similarity, we could conduct clustering, and the synonyms would be in the </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">same cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.104.1">In order to obtain such a matrix, we must do </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.106.1">Standardize the text before tokenization. </span><span class="koboSpan" id="kobo.106.2">In this case, we simply transform everything </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">into lowercase.</span></span></li>
<li><span class="koboSpan" id="kobo.108.1">We construct a vocabulary constituted of unique words and save the vocabulary so that in a case from a vector, we can get the </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">corresponding word.</span></span></li>
<li><span class="koboSpan" id="kobo.110.1">We create an array and then populate it with </span><strong class="source-inline"><span class="koboSpan" id="kobo.111.1">1</span></strong><span class="koboSpan" id="kobo.112.1"> at the index of the word in the vocabulary; </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">s elsewhere.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.115.1">Let’s </span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.116.1">take a look at how this works </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">in</span></span><span class="No-Break"><a id="_idIndexMarker013"/></span><span class="No-Break"><span class="koboSpan" id="kobo.118.1"> code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.119.1">
import numpy as np
def one_hot_encoding(sentence):
    words = sentence.lower().split()
    vocabulary = sorted(set(words))
    word_to_index = {word: i for i,
        word in enumerate(vocabulary)}
    one_hot_matrix = np.zeros((
        len(words), len(vocabulary)), dtype=int)
    for i, word in enumerate(words):
        one_hot_matrix[i, word_to_index[word]] = 1
    return one_hot_matrix, vocabulary</span></pre> <p><span class="koboSpan" id="kobo.120.1">Let’s look at a </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">specific example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.122.1">
sentence = "Should we go to a pizzeria or do you prefer a restaurant?"
</span><span class="koboSpan" id="kobo.122.2">one_hot_matrix, vocabulary = one_hot_encoding(sentence)
print("Vocabulary:", vocabulary)
print("One-Hot Encoding Matrix:\n", one_hot_matrix)</span></pre> <p><span class="koboSpan" id="kobo.123.1">We get the </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<span class="koboSpan" id="kobo.125.1"><img alt="" src="image/B21257_01_Figure_01.jpg"/></span>
</div>
</div>
<p class="callout-heading"><span class="koboSpan" id="kobo.126.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.127.1">Observe how choosing another sentence will result in a different matrix and how, by increasing the length of the sentence, the matrix grows proportionally to the number of different words. </span><span class="koboSpan" id="kobo.127.2">Also, note that for repeated words, we get equal vectors. </span><span class="koboSpan" id="kobo.127.3">Check the </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">preceding output.</span></span></p>
<p><span class="koboSpan" id="kobo.129.1">Even</span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.130.1"> if it </span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.131.1">is a simple method, we have obtained a first representation of text in a </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">vectorial form.</span></span></p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.133.1">Bag-of-words</span></h2>
<p><span class="koboSpan" id="kobo.134.1">In the </span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.135.1">previous section, we discussed one-hot encoding and some of the problems associated with this form of text representation. </span><span class="koboSpan" id="kobo.135.2">In</span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.136.1"> the previous example, we worked with a single sentence, but a corpus is made up of thousands if not millions of documents; each of these documents contains several words with a different frequency. </span><span class="koboSpan" id="kobo.136.2">We want a system that preserves this frequency information, as it is important for the classification of text. </span><span class="koboSpan" id="kobo.136.3">In fact, documents that have similar content are similar, and their meaning will also </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">be similar.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.138.1">BoW</span></strong><span class="koboSpan" id="kobo.139.1"> is an algorithm for extracting features from text that preserves this frequency property. </span><span class="koboSpan" id="kobo.139.2">BoW is a very simple algorithm that ignores the position of words in the text and only considers this frequency property. </span><span class="koboSpan" id="kobo.139.3">The name “bag” comes precisely from the fact that any information concerning sentence order and structure is not preserved by the algorithm. </span><span class="koboSpan" id="kobo.139.4">For</span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.140.1"> BoW, we only need a vocabulary and a way to be able to count words. </span><span class="koboSpan" id="kobo.140.2">In this case, the idea is to create document vectors: a single vector represents a document and the frequency of words contained in the vocabulary. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.141.1">Figure 1</span></em></span><em class="italic"><span class="koboSpan" id="kobo.142.1">.1</span></em><span class="koboSpan" id="kobo.143.1"> visualizes this concept with a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">from</span></span><span class="No-Break"> </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.145.1">Hamlet</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<span class="koboSpan" id="kobo.147.1"><img alt="Figure 1.1 – Representation of the BoW algorithm" src="image/B21257_01_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.148.1">Figure 1.1 – Representation of the BoW algorithm</span></p>
<p><span class="koboSpan" id="kobo.149.1">Even this </span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.150.1">representation is not without problems. </span><span class="koboSpan" id="kobo.150.2">Again, as the vocabulary grows, so will the size of the vectors (the size of each vector is equal to the length of the vocabulary). </span><span class="koboSpan" id="kobo.150.3">In addition, these vectors tend to be scattered, especially when the documents are very different from each other. </span><span class="koboSpan" id="kobo.150.4">High-dimensional or sparse vectors are not only problematic for memory and computational costs but for algorithms as well (the longer the vectors, the more weight you need in the algorithm, leading to a risk of overfitting). </span><span class="koboSpan" id="kobo.150.5">This is called the </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">curse of dimensionality</span></strong><span class="koboSpan" id="kobo.152.1">; the </span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.153.1">greater the number of features, the less meaningful the distances between examples. </span><span class="koboSpan" id="kobo.153.2">For large corpora, some solutions have been proposed, such as ignoring punctuation, correcting misspelled words, stemming algorithms, or ignoring words with high frequency that don’t add information (articles, prepositions, and </span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.155.1">In order to get a BoW matrix for a list of documents, we need to do </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.157.1">Tokenize each document to get a list </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">of words.</span></span></li>
<li><span class="koboSpan" id="kobo.159.1">Create our vocabulary of unique words and map each word to the corresponding index in </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">the vocabulary.</span></span></li>
<li><span class="koboSpan" id="kobo.161.1">Create a matrix where each row represents a document and each column, instead, a word in the vocabulary (the documents are the examples, and the words are the </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">associated features).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.163.1">Let’s</span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.164.1"> look at</span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.165.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">code again:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.167.1">
import numpy as np
def bag_of_words(sentences):
    """
    Creates a bag-of-words representation of a list of documents.
</span><span class="koboSpan" id="kobo.167.2">    """
    tokenized_sentences = [
        sentence.lower().split() for sentence in sentences
    ]
    flat_words = [
        word for sublist in tokenized_sentences for word in sublist
    ]
    vocabulary = sorted(set(flat_words))
    word_to_index = {word: i for i, word in enumerate(vocabulary)}
    bow_matrix = np.zeros((
        len(sentences), len(vocabulary)), dtype=int)
    for i, sentence in enumerate(tokenized_sentences):
        for word in sentence:
            if word in word_to_index:
                bow_matrix[i, word_to_index[word]] += 1
    return vocabulary, bow_matrix</span></pre> <p><span class="koboSpan" id="kobo.168.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">an example:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.170.1">
corpus = ["This movie is awesome awesome",
          "I do not say is good, but neither awesome",
          "Awesome? </span><span class="koboSpan" id="kobo.170.2">Only a fool can say that"]
vocabulary, bow_matrix = bag_of_words(corpus)
print("Vocabulary:", vocabulary)
print("Bag of Words Matrix:\n", bow_matrix)</span></pre> <p><span class="koboSpan" id="kobo.171.1">This </span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.172.1">prints the </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<span class="koboSpan" id="kobo.174.1"><img alt="" src="image/B21257_01_Figure_02.jpg"/></span>
</div>
</div>
<p class="callout-heading"><span class="koboSpan" id="kobo.175.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.176.1">Note how in the example, the word “</span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">awesome</span></strong><span class="koboSpan" id="kobo.178.1">” is associated with a review with a positive, neutral, or negative meaning. </span><span class="koboSpan" id="kobo.178.2">Without context, the frequency of the word “awesome” alone does not tell us the sentiment of </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">the review.</span></span></p>
<p><span class="koboSpan" id="kobo.180.1">Here, we</span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.181.1"> have learned how to transform text in a vectorial form while keeping the notion of frequency for </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">each word.</span></span></p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.183.1">TF-IDF</span></h2>
<p><span class="koboSpan" id="kobo.184.1">In the</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.185.1"> previous</span><a id="_idIndexMarker026"/><span class="koboSpan" id="kobo.186.1"> section, we obtained a document-term matrix. </span><span class="koboSpan" id="kobo.186.2">However, the raw frequency is very skewed and does not always allow us to discriminate between two documents. </span><span class="koboSpan" id="kobo.186.3">The document-term matrix was born in information retrieval to find documents, though words such as “good” or “bad” are not very discriminative since they are often used in text with a generic meaning. </span><span class="koboSpan" id="kobo.186.4">In contrast, words with low frequency are much more informative, so we are interested</span><a id="_idIndexMarker027"/><span class="koboSpan" id="kobo.187.1"> more in relative than </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">absolute frequency:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<span class="koboSpan" id="kobo.189.1"><img alt="Figure 1.2 – Intuition of the components of TF-IDF" src="image/B21257_01_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.190.1">Figure 1.2 – Intuition of the components of TF-IDF</span></p>
<p><span class="koboSpan" id="kobo.191.1">Instead of using raw frequency, we can use the logarithm in base 10, because a word that </span><a id="_idIndexMarker028"/><span class="koboSpan" id="kobo.192.1">occurs 100 times in a document is not 100 times more relevant to its meaning in the document. </span><span class="koboSpan" id="kobo.192.2">Of course, since vectors can be very sparse, we assign 0 if the frequency is 0. </span><span class="koboSpan" id="kobo.192.3">Second, we want to pay more attention to words that are present only in some documents. </span><span class="koboSpan" id="kobo.192.4">These words will be more relevant to the meaning of the document, and we want to preserve this information. </span><span class="koboSpan" id="kobo.192.5">To do this, we normalize by IDF. </span><span class="koboSpan" id="kobo.192.6">IDF is defined as the ratio of the total number of documents in the corpus to how many documents a term is present in. </span><span class="koboSpan" id="kobo.192.7">To summarize, to obtain the TF-IDF, we multiply TF by the logarithm </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">of IDF.</span></span></p>
<p><span class="koboSpan" id="kobo.194.1">This is demonstrated in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">code block:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.196.1">
import numpy as np
def compute_tf(sentences):
    """Compute the term frequency matrix for a list of sentences."""
</span><span class="koboSpan" id="kobo.196.2">    vocabulary = sorted(set(
        word for sentence in sentences
        for word in sentence.lower().split()))
    word_index = {word: i for i, word in enumerate(vocabulary)}
    tf = np.zeros((
        len(sentences), len(vocabulary)), dtype=np.float32)
    for i, sentence in enumerate(sentences):
        words = sentence.lower().split()
        word_count = len(words)
        for word in words:
            if word in word_index:
                tf[i, word_index[word]] += 1 / word_count
    return tf, vocabulary
def compute_idf(sentences, vocabulary):
    """Compute the inverse document frequency for a list of sentences."""
</span><span class="koboSpan" id="kobo.196.3">    num_documents = len(sentences)
    idf = np.zeros(len(vocabulary), dtype=np.float32)
    word_index = {word: i for i, word in enumerate(vocabulary)}
    for word in vocabulary:
        df = sum(
            1 for sentence in sentences
            if word in sentence.lower().split()
        )
        idf[word_index[word]] = np.log(
            num_documents / (1 + df)) + 1  # Smoothing
    return idf
def tf_idf(sentences):
    """Generate a TF-IDF matrix for a list of sentences."""
</span><span class="koboSpan" id="kobo.196.4">    tf, vocabulary = compute_tf(sentences)
    idf = compute_idf(sentences, vocabulary)
    tf_idf_matrix = tf * idf
    return vocabulary, tf_idf_matrix
vocabulary, tf_idf_matrix = tf_idf(corpus)
print("Vocabulary:", vocabulary)
print("TF-IDF Matrix:\n", tf_idf_matrix)</span></pre> <p><span class="koboSpan" id="kobo.197.1">This </span><a id="_idIndexMarker029"/><span class="koboSpan" id="kobo.198.1">generates</span><a id="_idIndexMarker030"/><span class="koboSpan" id="kobo.199.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">following output:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<span class="koboSpan" id="kobo.201.1"><img alt="" src="image/B21257_01_Figure_03.jpg"/></span>
</div>
</div>
<p class="callout-heading"><span class="koboSpan" id="kobo.202.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.203.1">In this example, we used the same corpus as in the previous section. </span><span class="koboSpan" id="kobo.203.2">Note how word frequencies changed after </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">this normalization.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">In this section, we </span><a id="_idIndexMarker031"/><span class="koboSpan" id="kobo.206.1">learned how we can normalize text to decrease the impact of the most frequent</span><a id="_idIndexMarker032"/><span class="koboSpan" id="kobo.207.1"> words and give relevance to words that are specific to a subset of documents. </span><span class="koboSpan" id="kobo.207.2">Next, we’ll </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">discuss embedding.</span></span></p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.209.1">Embedding, application, and representation</span></h1>
<p><span class="koboSpan" id="kobo.210.1">In the previous section, we discussed how to use vectors to represent text. </span><span class="koboSpan" id="kobo.210.2">These vectors are digestible for a computer, but they still suffer from some problems (sparsity, high dimensionality, etc.). </span><span class="koboSpan" id="kobo.210.3">According to the distributional hypothesis, words with a similar meaning frequently appear close together (or words that appear often in the same context have the same meaning). </span><span class="koboSpan" id="kobo.210.4">Similarly, a word can have a different meaning depending on its context: “I went to deposit money in the </span><em class="italic"><span class="koboSpan" id="kobo.211.1">bank</span></em><span class="koboSpan" id="kobo.212.1">” or “We went to do a picnic on the river </span><em class="italic"><span class="koboSpan" id="kobo.213.1">bank</span></em><span class="koboSpan" id="kobo.214.1">.” </span><span class="koboSpan" id="kobo.214.2">In the following diagram, we have a high-level representation of the embedding process. </span><span class="koboSpan" id="kobo.214.3">So, we want a process that allows us to start from text to obtain an array of vectors, where each vector corresponds to the representation of a word. </span><span class="koboSpan" id="kobo.214.4">In this </span><a id="_idIndexMarker033"/><span class="koboSpan" id="kobo.215.1">case, we want a model that will then allow us to map each word to a vector representation. </span><span class="koboSpan" id="kobo.215.2">In the next section, we will describe the process in detail and discuss the theory </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">behind it.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<span class="koboSpan" id="kobo.217.1"><img alt="Figure 1.3 – High-level representation of the embedding process" src="image/B21257_01_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.218.1">Figure 1.3 – High-level representation of the embedding process</span></p>
<p><span class="koboSpan" id="kobo.219.1">We would, therefore, like to generate vectors that are small in size, composed of real (dense) numbers, and that preserve this contextual information. </span><span class="koboSpan" id="kobo.219.2">Thus, the purpose is to have vectors of limited size that can represent the meaning of a word. </span><span class="koboSpan" id="kobo.219.3">The scattered vectors we obtained earlier cannot be used efficiently for mathematical operations or downstream tasks. </span><span class="koboSpan" id="kobo.219.4">Also, the more words there are in the vocabulary, the larger the size of the vectors we get. </span><span class="koboSpan" id="kobo.219.5">Therefore, we want dense vectors (with real numbers) that are small in size and whose size does not increase as the number of words in the </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">vocabulary increases.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">In addition, these </span><a id="_idIndexMarker034"/><span class="koboSpan" id="kobo.222.1">vectors have a distributed representation of the meaning of the word (whereas in sparse vectors, it was local or where the </span><strong class="source-inline"><span class="koboSpan" id="kobo.223.1">1</span></strong><span class="koboSpan" id="kobo.224.1"> was located). </span><span class="koboSpan" id="kobo.224.2">As we will see a little later, these dense vectors can be used for different operations because they better represent the concept of</span><a id="_idIndexMarker035"/><span class="koboSpan" id="kobo.225.1"> similarity between words. </span><span class="koboSpan" id="kobo.225.2">These dense vectors are called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.226.1">word embeddings</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">This concept was introduced in 2013 by Mikolov with a framework called </span><strong class="bold"><span class="koboSpan" id="kobo.229.1">word2vec</span></strong><span class="koboSpan" id="kobo.230.1">, which will be described in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">next section.</span></span></p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.232.1">Word2vec</span></h2>
<p><span class="koboSpan" id="kobo.233.1">The</span><a id="_idIndexMarker036"/><span class="koboSpan" id="kobo.234.1"> intuition behind word2vec is simple: predict a word </span><em class="italic"><span class="koboSpan" id="kobo.235.1">w</span></em><span class="koboSpan" id="kobo.236.1"> from its context. </span><span class="koboSpan" id="kobo.236.2">To do this, we</span><a id="_idIndexMarker037"/><span class="koboSpan" id="kobo.237.1"> need a </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">neural network</span></strong><span class="koboSpan" id="kobo.239.1"> and a large corpus. </span><span class="koboSpan" id="kobo.239.2">The revolutionary idea is that by training this neural network to predict which words </span><em class="italic"><span class="koboSpan" id="kobo.240.1">c</span></em><span class="koboSpan" id="kobo.241.1"> are needed near the target word </span><em class="italic"><span class="koboSpan" id="kobo.242.1">w</span></em><span class="koboSpan" id="kobo.243.1">, the weights of the neural network will be the embedding vectors. </span><span class="koboSpan" id="kobo.243.2">This model is self-supervised; the labels in this case are implicit, and we do not </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">provide them.</span></span></p>
<p><span class="koboSpan" id="kobo.245.1">Word2vec simplifies this idea by making the system extremely fast and effective in two ways: by turning the task into binary classification (Is the word </span><em class="italic"><span class="koboSpan" id="kobo.246.1">c</span></em><span class="koboSpan" id="kobo.247.1"> needed in the context of the word </span><em class="italic"><span class="koboSpan" id="kobo.248.1">w</span></em><span class="koboSpan" id="kobo.249.1">? </span><span class="koboSpan" id="kobo.249.2">Yes or no?) and using a logistic </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">regression classifier:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<span class="koboSpan" id="kobo.251.1"><img alt="Figure 1.4 – In word2vec, we slide a context window (here represented as a  three-word context window), and then we randomly sample some negative words" src="image/B21257_01_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.252.1">Figure 1.4 – In word2vec, we slide a context window (here represented as a three-word context window), and then we randomly sample some negative words</span></p>
<p><span class="koboSpan" id="kobo.253.1">Given a text </span><em class="italic"><span class="koboSpan" id="kobo.254.1">t</span></em><span class="koboSpan" id="kobo.255.1">, we scroll a window </span><em class="italic"><span class="koboSpan" id="kobo.256.1">c</span></em><span class="koboSpan" id="kobo.257.1"> (our context) for a word </span><em class="italic"><span class="koboSpan" id="kobo.258.1">w</span></em><span class="koboSpan" id="kobo.259.1"> in the center of our window; the words around it are examples of the positive class. </span><span class="koboSpan" id="kobo.259.2">After that, we select other random words as negative examples. </span><span class="koboSpan" id="kobo.259.3">Finally, we train a model to classify the positive and negative examples; the weights of the model are </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">our embeddings.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">Given a word </span><em class="italic"><span class="koboSpan" id="kobo.262.1">w</span></em><span class="koboSpan" id="kobo.263.1"> and a word </span><em class="italic"><span class="koboSpan" id="kobo.264.1">c</span></em><span class="koboSpan" id="kobo.265.1">, we want the probability that the word </span><em class="italic"><span class="koboSpan" id="kobo.266.1">c</span></em><span class="koboSpan" id="kobo.267.1"> is in the context of </span><em class="italic"><span class="koboSpan" id="kobo.268.1">w</span></em><span class="koboSpan" id="kobo.269.1"> to be similar to its </span><a id="_idIndexMarker038"/><span class="koboSpan" id="kobo.270.1">embedding similarity. </span><span class="koboSpan" id="kobo.270.2">In other words, if the vector representing </span><em class="italic"><span class="koboSpan" id="kobo.271.1">w</span></em><span class="koboSpan" id="kobo.272.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.273.1">c</span></em><span class="koboSpan" id="kobo.274.1"> are similar, </span><em class="italic"><span class="koboSpan" id="kobo.275.1">c</span></em><span class="koboSpan" id="kobo.276.1"> must often be in the context of </span><em class="italic"><span class="koboSpan" id="kobo.277.1">w</span></em><span class="koboSpan" id="kobo.278.1"> (word2vec is based on the notion of context similarity). </span><span class="koboSpan" id="kobo.278.2">We define this embedding similarity by the dot product between the two embedding vectors for </span><em class="italic"><span class="koboSpan" id="kobo.279.1">w</span></em><span class="koboSpan" id="kobo.280.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.281.1">c</span></em><span class="koboSpan" id="kobo.282.1"> (we use the sigmoid function to transform this dot product into a probability and thus allow comparison). </span><span class="koboSpan" id="kobo.282.2">So, the probability that </span><em class="italic"><span class="koboSpan" id="kobo.283.1">c</span></em><span class="koboSpan" id="kobo.284.1"> is in the context of </span><em class="italic"><span class="koboSpan" id="kobo.285.1">w</span></em><span class="koboSpan" id="kobo.286.1"> is equal to the probability that their embeddings </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">are similar:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mi>σ</mi><mfenced close=")" open="("><mrow><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><mo>−</mo><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.288.1">This is done for all words in context </span><em class="italic"><span class="koboSpan" id="kobo.289.1">L</span></em><span class="koboSpan" id="kobo.290.1">. </span><span class="koboSpan" id="kobo.290.2">To simplify, we assume that all words in the context window are independent, so we can multiply the probabilities of the various words </span><em class="italic"><span class="koboSpan" id="kobo.291.1">c</span></em><span class="koboSpan" id="kobo.292.1">. </span><span class="koboSpan" id="kobo.292.2">Similarly, we want to ensure that this dot product is minimal for words that are not in the context of word </span><em class="italic"><span class="koboSpan" id="kobo.293.1">w</span></em><span class="koboSpan" id="kobo.294.1">. </span><span class="koboSpan" id="kobo.294.2">So, on the one hand, we maximize the probability for words in the context, and on the other hand, we minimize the probability for words that are not in the context. </span><span class="koboSpan" id="kobo.294.3">In fact, words that are not in the context of </span><em class="italic"><span class="koboSpan" id="kobo.295.1">w</span></em><span class="koboSpan" id="kobo.296.1"> are randomly extracted during training, and the process is </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">the same:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.298.1">For simplicity, we take the logarithm </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">of probability:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.300.1">The matrix of weights </span><em class="italic"><span class="koboSpan" id="kobo.301.1">w</span></em><span class="koboSpan" id="kobo.302.1"> is our embedding; it is what we will use from now on. </span><span class="koboSpan" id="kobo.302.2">Actually, the model learns two matrices of vectors (one for </span><em class="italic"><span class="koboSpan" id="kobo.303.1">w</span></em><span class="koboSpan" id="kobo.304.1"> and one for </span><em class="italic"><span class="koboSpan" id="kobo.305.1">c</span></em><span class="koboSpan" id="kobo.306.1">), but the two matrices are very similar, so we take just one. </span><span class="koboSpan" id="kobo.306.2">We then use cross-entropy to train the models and learn the weights for </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">each vector:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></span></p>
<p><span class="koboSpan" id="kobo.308.1">This is represented visually in the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<span class="koboSpan" id="kobo.310.1"><img alt="Figure 1.5 – Word and context embedding" src="image/B21257_01_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.311.1">Figure 1.5 – Word and context embedding</span></p>
<p><span class="koboSpan" id="kobo.312.1">The following choices</span><a id="_idIndexMarker039"/><span class="koboSpan" id="kobo.313.1"> affect the quality </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">of embedding:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.315.1">Data quality is critical</span></em><span class="koboSpan" id="kobo.316.1">. </span><span class="koboSpan" id="kobo.316.2">For example, leveraging Wikipedia allows better embedding for semantic tasks, while using news improves performance for syntactic tasks (a mixture of the two is recommended). </span><span class="koboSpan" id="kobo.316.3">Using Twitter or other social networks can </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">insert bias.</span></span></li>
<li><span class="koboSpan" id="kobo.318.1">At the same time, </span><em class="italic"><span class="koboSpan" id="kobo.319.1">a larger amount of text improves embedding performance</span></em><span class="koboSpan" id="kobo.320.1">. </span><span class="koboSpan" id="kobo.320.2">A large amount of text can partially compensate for poor quality but at the cost of much longer training (for example, Common Crawl is a huge dataset downloaded from the internet that is pretty </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">dirty, though).</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.322.1">The number of dimensions is another important factor</span></em><span class="koboSpan" id="kobo.323.1">. </span><span class="koboSpan" id="kobo.323.2">The larger the size of the embedding, the better its performance. </span><span class="koboSpan" id="kobo.323.3">300 is considered a sweet spot because, beyond this size, number performance does not </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">increase significantly.</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.325.1">The size of the context window also has an impact</span></em><span class="koboSpan" id="kobo.326.1">. </span><span class="koboSpan" id="kobo.326.2">Generally, a context window of 4 is used, but a context window of 2 allows for vectors that better identify parts of speech. </span><span class="koboSpan" id="kobo.326.3">In contrast, long context windows are more useful if we are interested in similarity </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">more broadly.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.328.1">In Python, we </span><a id="_idIndexMarker040"/><span class="koboSpan" id="kobo.329.1">can easily get an embedding from lists of tokens using the </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.331.1">
from gensim.models import Word2Vec
model = Word2Vec(sentences=list_of_tokens,
                 sg=1,
                 vector_size=100,
                 window=5,
                 workers=4)</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.332.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.333.1">The complete code is present in the GitHub repository. </span><span class="koboSpan" id="kobo.333.2">We used an embedding of 100 dimensions and a window of </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">5 words.</span></span></p>
<p><span class="koboSpan" id="kobo.335.1">Once we have our embedding, we</span><a id="_idIndexMarker041"/><span class="koboSpan" id="kobo.336.1"> can visualize it. </span><span class="koboSpan" id="kobo.336.2">For example, if we try </span><strong class="bold"><span class="koboSpan" id="kobo.337.1">clustering</span></strong><span class="koboSpan" id="kobo.338.1"> the vectors of some words, words that have similar meanings should be </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">closer together:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer024">
<span class="koboSpan" id="kobo.340.1"><img alt="Figure 1.6 – Clustering of some of the vectors obtained from the embedding" src="image/B21257_01_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.341.1">Figure 1.6 – Clustering of some of the vectors obtained from the embedding</span></p>
<p><span class="koboSpan" id="kobo.342.1">Another</span><a id="_idIndexMarker042"/><span class="koboSpan" id="kobo.343.1"> way to visualize vectors is to use dimensionality reduction techniques. </span><span class="koboSpan" id="kobo.343.2">Vectors are multidimensional (100-1,024), so it is more convenient to reduce them to two or three dimensions so that they can be visualized more easily. </span><span class="koboSpan" id="kobo.343.3">Some of the most </span><a id="_idIndexMarker043"/><span class="koboSpan" id="kobo.344.1">commonly used techniques</span><a id="_idIndexMarker044"/><span class="koboSpan" id="kobo.345.1"> are </span><strong class="bold"><span class="koboSpan" id="kobo.346.1">principal component analysis</span></strong><span class="koboSpan" id="kobo.347.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.348.1">PCA</span></strong><span class="koboSpan" id="kobo.349.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.350.1">t-distributed stochastic neighbor embedding</span></strong><span class="koboSpan" id="kobo.351.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.352.1">t-SNE)</span></strong><span class="koboSpan" id="kobo.353.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.354.1">Uniform Manifold Approximation and Projection</span></strong><span class="koboSpan" id="kobo.355.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.356.1">UMAP</span></strong><span class="koboSpan" id="kobo.357.1">), on the other hand, is a technique that has</span><a id="_idIndexMarker045"/><span class="koboSpan" id="kobo.358.1"> become the first choice for visualizing multidimensional data in </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">recent years:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer025">
<span class="koboSpan" id="kobo.360.1"><img alt="Figure 1.7 – 2D projection of word2vec embedding highlighting some examples" src="image/B21257_01_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.361.1">Figure 1.7 – 2D projection of word2vec embedding highlighting some examples</span></p>
<p><span class="koboSpan" id="kobo.362.1">UMAP has emerged because it produces visualizations that better preserve semantic meaning and relationships between examples and also better represent local and global structures. </span><span class="koboSpan" id="kobo.362.2">This makes for better clusters, and UMAP can also be used in preprocessing steps before a classification task </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">on vectors.</span></span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.364.1">A notion of similarity for text</span></h2>
<p><span class="koboSpan" id="kobo.365.1">Once we have obtained vector</span><a id="_idIndexMarker046"/><span class="koboSpan" id="kobo.366.1"> representations, we need a method to calculate the similarity between them. </span><span class="koboSpan" id="kobo.366.2">This is crucial in many applications—for instance, to find words in an embedding space that are most similar to a given word, we compute the similarity between its vector and those of other words. </span><span class="koboSpan" id="kobo.366.3">Similarly, given a query sentence, we can retrieve the most relevant documents by comparing its vector with document embeddings and selecting those with the </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">highest similarity.</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">Most similarity measures </span><a id="_idIndexMarker047"/><span class="koboSpan" id="kobo.369.1">are based on the </span><strong class="bold"><span class="koboSpan" id="kobo.370.1">dot product</span></strong><span class="koboSpan" id="kobo.371.1">. </span><span class="koboSpan" id="kobo.371.2">This is because the dot product is high when the two vectors have values in the same dimension. </span><span class="koboSpan" id="kobo.371.3">In contrast, vectors that have zero alternately will have a dot product of zero, thus orthogonal or dissimilar. </span><span class="koboSpan" id="kobo.371.4">This is why the dot product was used as a similarity measure for word co-occurrence matrices or with vectors derived from document </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">TF matrices:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">u</mi><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">t</mi><mo>:</mo><mi mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><msub><mi>a</mi><mn>1</mn></msub><mo>×</mo><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><mo>×</mo><msub><mi>b</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msub><mi>a</mi><mi>n</mi></msub><mo>×</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>m</mi><mi>a</mi><mi>g</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>u</mi><mi>d</mi><mi>e</mi><mo>=</mo><mfenced close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mo>=</mo><mroot><mrow><munderover><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.373.1">The dot product has several </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">problems, though:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.375.1">It tends to favor vectors with </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">long dimensions</span></span></li>
<li><span class="koboSpan" id="kobo.377.1">It favors vectors with high values (which, in general, are those of very frequent and, therefore, </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">useless words)</span></span></li>
<li><span class="koboSpan" id="kobo.379.1">The value of the dot product has </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">no limits</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.381.1">Therefore, alternatives have been sought, such as a </span><a id="_idIndexMarker048"/><span class="koboSpan" id="kobo.382.1">normalized version of the dot product. </span><span class="koboSpan" id="kobo.382.2">The normalized </span><a id="_idIndexMarker049"/><span class="koboSpan" id="kobo.383.1">dot product is equivalent to the cosine of the angle between the two vectors, hence </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.384.1">cosine similarity</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">Θ</mi><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi></mrow><mrow><mfenced close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mfenced close="|" open="|"><mi mathvariant="bold-italic">b</mi></mfenced></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub></mrow></mrow><mrow><mroot><mrow><msubsup><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot><mroot><mrow><msubsup><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.386.1">Cosine similarity </span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.387.1">has some </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">interesting properties:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.389.1">It is between -1 and 1. </span><span class="koboSpan" id="kobo.389.2">Opposite or totally dissimilar vectors will have a value of -1, 0 for orthogonal vectors (or totally dissimilar for scattered vectors), and 1 for perfectly similar vectors. </span><span class="koboSpan" id="kobo.389.3">Since it measures the angle between two vectors, the interpretation is easier and is within a specific range, so it allows one intuitively to understand the similarity </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">or dissimilarity.</span></span></li>
<li><span class="koboSpan" id="kobo.391.1">It is fast and cheap </span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">to compute.</span></span></li>
<li><span class="koboSpan" id="kobo.393.1">It is less sensitive to word frequency and, thus, more robust </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">to outliers.</span></span></li>
<li><span class="koboSpan" id="kobo.395.1">It is scale-invariant, meaning that it is not influenced by the magnitude of </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">the vectors.</span></span></li>
<li><span class="koboSpan" id="kobo.397.1">Being normalized, it can also be used with </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">high-dimensional data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.399.1">For 2D vectors, we can plot to observe </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">these properties:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<span class="koboSpan" id="kobo.401.1"><img alt="Figure 1.8 – Example of cosine similarity between two vectors" src="image/B21257_01_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.402.1">Figure 1.8 – Example of cosine similarity between two vectors</span></p>
<p><span class="koboSpan" id="kobo.403.1">In the next section, we can </span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.404.1">define the properties of our trained embedding using this notion </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">of similarity.</span></span></p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/><span class="koboSpan" id="kobo.406.1">Properties of embeddings</span></h2>
<p><span class="koboSpan" id="kobo.407.1">Embeddings</span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.408.1"> are a surprisingly flexible method and manage to encode different syntactic and semantic properties that can both be visualized and exploited for different operations. </span><span class="koboSpan" id="kobo.408.2">Once</span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.409.1"> we have a notion of similarity, we can search for the words that are most similar to a word </span><em class="italic"><span class="koboSpan" id="kobo.410.1">w</span></em><span class="koboSpan" id="kobo.411.1">. </span><span class="koboSpan" id="kobo.411.2">Note that similarity is defined as appearing in the same context window; the model cannot differentiate synonyms </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">and antonyms.</span></span></p>
<p><span class="koboSpan" id="kobo.413.1">In addition, the model is also capable of representing grammatical relations such as superlatives or </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">verb forms.</span></span></p>
<p><span class="koboSpan" id="kobo.415.1">Another interesting relationship we can study is analogies. </span><span class="koboSpan" id="kobo.415.2">The parallelogram model is a system for representing analogies in a cognitive space. </span><span class="koboSpan" id="kobo.415.3">The classic example is </span><em class="italic"><span class="koboSpan" id="kobo.416.1">king:queen::man:?</span></em><span class="koboSpan" id="kobo.417.1"> (which in a formula would be </span><em class="italic"><span class="koboSpan" id="kobo.418.1">a:b::a*:?</span></em><span class="koboSpan" id="kobo.419.1">). </span><span class="koboSpan" id="kobo.419.2">Given that we have vectors, we can turn this into an </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.420.1">a-a*+b</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.421.1"> operation.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">We can test this in Python using the embedding model we </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">have trained:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.424.1">We can check the most </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">similar words</span></span></li>
<li><span class="koboSpan" id="kobo.426.1">We can test </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">the analogy</span></span></li>
<li><span class="koboSpan" id="kobo.428.1">We can then test the capacity to identify synonyms </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">and antonyms</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.430.1">The code for</span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.431.1"> this process is </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.433.1">
word_1 = "good"
syn = "great"
ant = "bad"
most_sim =model.wv.most_similar("good")
print("Top 3 most similar words to {} are :{}".format(
    word_1, most_sim[:3]))
synonyms_dist = model.wv.distance(word_1, syn)
antonyms_dist = model.wv.distance(word_1, ant)
print("Synonyms {}, {} have cosine distance: {}".format(
    word_1, syn, synonyms_dist))
print("Antonyms {}, {} have cosine distance: {}".format(
    word_1, ant, antonyms_dist))
a = 'king'
a_star = 'man'
b = 'woman'
b_star= model.wv.most_similar(positive=[a, b], negative=[a_star])
print("{} is to {} as {} is to: {} ".format(
    a, a_star, b, b_star[0][0]))</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.434.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.435.1">This is done with the embedding we have trained before. </span><span class="koboSpan" id="kobo.435.2">Notice how the model is not handling </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">antonyms well.</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">The</span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.438.1"> method is not exactly perfect, so sometimes the right answer is not the first result, but it could be among the first three inputs. </span><span class="koboSpan" id="kobo.438.2">Also, this system works for entities that are frequent within the text (city names, common words) but much less with </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">rarer entities.</span></span></p>
<p><span class="koboSpan" id="kobo.440.1">Embeddings can also be used as a tool to study how the meaning of a word changes over time, especially if you have text corpora that span several decades. </span><span class="koboSpan" id="kobo.440.2">This is demonstrated in the following diagram, which shows how the meanings of the words </span><em class="italic"><span class="koboSpan" id="kobo.441.1">gay</span></em><span class="koboSpan" id="kobo.442.1">, </span><em class="italic"><span class="koboSpan" id="kobo.443.1">broadcast</span></em><span class="koboSpan" id="kobo.444.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.445.1">awful</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.446.1">have changed:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<span class="koboSpan" id="kobo.447.1"><img alt="Figure 1.9 – 2D visualization of how the semantic meaning of a word changes over the years; these projections are obtained using text from different decades and embeddings (https://arxiv.org/abs/1605.09096)" src="image/B21257_01_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.448.1">Figure 1.9 – 2D visualization of how the semantic meaning of a word changes over the years; these projections are obtained using text from different decades and embeddings (</span><a href="https://arxiv.org/abs/1605.09096"><span class="koboSpan" id="kobo.449.1">https://arxiv.org/abs/1605.09096</span></a><span class="koboSpan" id="kobo.450.1">)</span></p>
<p><span class="koboSpan" id="kobo.451.1">Finally, a word can still have multiple meanings! </span><span class="koboSpan" id="kobo.451.2">For example, common words such as “good” have more than one meaning depending on the context. </span><span class="koboSpan" id="kobo.451.3">One may wonder whether a vector for a word in an embedding represents only one meaning or whether it represents the set of meanings of a word. </span><span class="koboSpan" id="kobo.451.4">Fortunately, embedding vectors represent a weighted sum of the various meanings of a word (linear superposition). </span><span class="koboSpan" id="kobo.451.5">The weights of each meaning are proportional to the frequency of that meaning in the text. </span><span class="koboSpan" id="kobo.451.6">Although these</span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.452.1"> meanings reside in the same vector, when we add or subtract during the calculation of analogies, we are working with these components. </span><span class="koboSpan" id="kobo.452.2">For example, “apple” is both a fruit and the name of a company; if we conduct the operation </span><em class="italic"><span class="koboSpan" id="kobo.453.1">apple:red::banana:?</span></em><span class="koboSpan" id="kobo.454.1">, we are subtracting only a very specific semantic component from the apple vector (the component that is similar to red). </span><span class="koboSpan" id="kobo.454.2">This flexibility can be useful when we want to disambiguate meanings. </span><span class="koboSpan" id="kobo.454.3">Also, since the vector space is sparse, by exploiting sparse coding, we can separate the </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">various meanings:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<span class="koboSpan" id="kobo.456.1"><img alt="Figure 1.10 – Table showing how a vector in word2vec is encoding for different meanings at the same time (https://aclanthology.org/Q18-1034/)" src="image/B21257_01_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.457.1">Figure 1.10 – Table showing how a vector in word2vec is encoding for different meanings at the same time (</span><a href="https://aclanthology.org/Q18-1034/"><span class="koboSpan" id="kobo.458.1">https://aclanthology.org/Q18-1034/</span></a><span class="koboSpan" id="kobo.459.1">)</span></p>
<p><span class="koboSpan" id="kobo.460.1">These vectors are now providing contextual and semantic meaning for each word in the text. </span><span class="koboSpan" id="kobo.460.2">We can use this rich source of information for tasks such as text classification. </span><span class="koboSpan" id="kobo.460.3">What we need now are models that can handle the sequential nature of text, which we will learn about in the </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">next section.</span></span></p>
<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/><span class="koboSpan" id="kobo.462.1">RNNs, LSTMs, GRUs, and CNNs for text</span></h1>
<p><span class="koboSpan" id="kobo.463.1">So far, we have discussed how to represent text in a way that is digestible for the model; in this section, we will discuss how to analyze the text once a representation has been obtained. </span><span class="koboSpan" id="kobo.463.2">Traditionally, once we obtained a representation of the text, it was fed to models such as naïve Bayes or even algorithms such as logistic regression. </span><span class="koboSpan" id="kobo.463.3">The success of neural networks has made these machine learning algorithms outdated. </span><span class="koboSpan" id="kobo.463.4">In this section, we will discuss deep learning models that can be used for </span><span class="No-Break"><span class="koboSpan" id="kobo.464.1">various tasks.</span></span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/><span class="koboSpan" id="kobo.465.1">RNNs</span></h2>
<p><span class="koboSpan" id="kobo.466.1">The </span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.467.1">problem with classical neural networks is that they have no memory. </span><span class="koboSpan" id="kobo.467.2">This is especially problematic for time series and text inputs. </span><span class="koboSpan" id="kobo.467.3">In a sequence of words </span><em class="italic"><span class="koboSpan" id="kobo.468.1">t</span></em><span class="koboSpan" id="kobo.469.1">, the word </span><em class="italic"><span class="koboSpan" id="kobo.470.1">w</span></em><span class="koboSpan" id="kobo.471.1"> at time </span><em class="italic"><span class="koboSpan" id="kobo.472.1">t</span></em><span class="koboSpan" id="kobo.473.1"> depends on the </span><em class="italic"><span class="koboSpan" id="kobo.474.1">w</span></em><span class="koboSpan" id="kobo.475.1"> at time </span><em class="italic"><span class="koboSpan" id="kobo.476.1">t-1</span></em><span class="koboSpan" id="kobo.477.1">. </span><span class="koboSpan" id="kobo.477.2">In fact, in a sentence, the last word is often dependent on several words in the sentence. </span><span class="koboSpan" id="kobo.477.3">Therefore, we want an NN model that maintains a memory of previous inputs. </span><span class="koboSpan" id="kobo.477.4">An </span><strong class="bold"><span class="koboSpan" id="kobo.478.1">RNN</span></strong><span class="koboSpan" id="kobo.479.1"> maintains an internal state that maintains this memory; that is, it stores information about previous inputs, and the outputs it produces are affected by previous inputs. </span><span class="koboSpan" id="kobo.479.2">These networks perform the same operation on all elements of the sequence (hence recurrent) and maintain the memory of </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">this operation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.481.1"><img alt="Figure 1.11 – Simple example of an RNN (https://arxiv.org/pdf/1506.00019)" src="image/B21257_01_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.482.1">Figure 1.11 – Simple example of an RNN (</span><a href="https://arxiv.org/pdf/1506.00019"><span class="koboSpan" id="kobo.483.1">https://arxiv.org/pdf/1506.00019</span></a><span class="koboSpan" id="kobo.484.1">)</span></p>
<p><span class="koboSpan" id="kobo.485.1">A </span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.486.1">classical neural network (</span><strong class="bold"><span class="koboSpan" id="kobo.487.1">feedforward neural network</span></strong><span class="koboSpan" id="kobo.488.1">) considers inputs to be independent, and one layer of a neural network performs the following operation for a vector representing the element at </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">time </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.490.1">t</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.492.1">However, in a simple RNN, the following operations </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">are conducted:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><msup><mi mathvariant="bold-italic">a</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="bold-italic">b</mi><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mi mathvariant="bold-italic">W</mi><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.494.1">These operations may seem complicated, but in fact, we are simply maintaining a hidden state that considers the previous iterations. </span><span class="koboSpan" id="kobo.494.2">The first equation is a normal feedforward layer modified, in which we multiply the previously hidden state </span><em class="italic"><span class="koboSpan" id="kobo.495.1">h</span></em><span class="koboSpan" id="kobo.496.1"> by a set of weights </span><em class="italic"><span class="koboSpan" id="kobo.497.1">U</span></em><span class="koboSpan" id="kobo.498.1">. </span><span class="koboSpan" id="kobo.498.2">This matrix </span><em class="italic"><span class="koboSpan" id="kobo.499.1">U</span></em><span class="koboSpan" id="kobo.500.1"> allows us to control how the neural network uses the previous context to bind input and past inputs (how the past influences the output for input at time </span><em class="italic"><span class="koboSpan" id="kobo.501.1">t</span></em><span class="koboSpan" id="kobo.502.1">). </span><span class="koboSpan" id="kobo.502.2">In the second equation, we create a new hidden state that will then be used for </span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.503.1">subsequent computations but also for the next input. </span><span class="koboSpan" id="kobo.503.2">In the third equation, we are creating the output; we use a bias vector and a matrix to compute the output. </span><span class="koboSpan" id="kobo.503.3">In the last equation, it is simply passed as a </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">nonlinearity function.</span></span></p>
<p><span class="koboSpan" id="kobo.505.1">These RNNs can be seen as unrolled entities throughout time, in which we can represent the network and its computations throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">the sequence:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.507.1"><img alt="Figure 1.12 – Simple example of an RNN unrolled through the sequence (https://arxiv.org/pdf/1506.00019)" src="image/B21257_01_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.508.1">Figure 1.12 – Simple example of an RNN unrolled through the sequence (</span><a href="https://arxiv.org/pdf/1506.00019"><span class="koboSpan" id="kobo.509.1">https://arxiv.org/pdf/1506.00019</span></a><span class="koboSpan" id="kobo.510.1">)</span></p>
<p><span class="koboSpan" id="kobo.511.1">We can test in Python with a PyTorch RNN to see how it is transforming </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.513.1">
array = np.random.random((10, 5, 3))
# Convert the numpy array to a PyTorch tensor
data_tensor = torch.tensor(array, dtype=torch.float32)
RNN = nn.RNN(input_size=3, hidden_size=10,
             num_layers=1, batch_first=True)
output, hidden = RNN(data_tensor)
output.shape</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.514.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.515.1">Notice how the model is transforming the data; we can also access the </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">hidden state.</span></span></p>
<p><span class="koboSpan" id="kobo.517.1">We can see </span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.518.1">several </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">interesting things:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.520.1">The RNN is not limited by the size of the input; it is a cyclic operation that is conducted over the entire sequence. </span><span class="koboSpan" id="kobo.520.2">RNNs basically process one word at a time. </span><span class="koboSpan" id="kobo.520.3">This cyclicality also means backpropagation is conducted for each time step. </span><span class="koboSpan" id="kobo.520.4">Although this model works well for series analysis, its sequential nature does not allow it to </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">be parallelized.</span></span></li>
<li><span class="koboSpan" id="kobo.522.1">Theoretically, this model could be trained with infinite sequences of words; theoretically, after a few time steps, it begins to forget the </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">initial inputs.</span></span></li>
<li><span class="koboSpan" id="kobo.524.1">Training can become inefficient due to the vanishing gradient problem, where gradients must propagate from the final cell back to the initial one. </span><span class="koboSpan" id="kobo.524.2">During this process, they can shrink exponentially and approach zero, making it difficult for the model to learn long-range dependencies. </span><span class="koboSpan" id="kobo.524.3">Conversely, the exploding gradient problem can also occur, where gradients grow uncontrollably large, leading to </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">unstable training.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.526.1">RNNs are not the only form of deep learning models that are relevant to </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">this topic.</span></span></p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/><span class="koboSpan" id="kobo.528.1">LSTMs</span></h2>
<p><span class="koboSpan" id="kobo.529.1">In theory, RNNs should </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.530.1">be able to process long sequences and remember the initial input. </span><span class="koboSpan" id="kobo.530.2">However, in reality, the information inside the hidden state is local rather than global, and for a time </span><em class="italic"><span class="koboSpan" id="kobo.531.1">t</span></em><span class="koboSpan" id="kobo.532.1">, it considers only the previous time steps and not the entire sequence. </span><span class="koboSpan" id="kobo.532.2">The main problem with such a simple model is that the hidden state must simultaneously fulfill two roles: provide information relevant to the output at time </span><em class="italic"><span class="koboSpan" id="kobo.533.1">t</span></em><span class="koboSpan" id="kobo.534.1"> and store memory for </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">future decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.536.1">An </span><strong class="bold"><span class="koboSpan" id="kobo.537.1">LSTM</span></strong><span class="koboSpan" id="kobo.538.1"> is an</span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.539.1"> extension of RNNs, designed with the idea that the model can forget information that is not important and keep only the important context. </span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.540.1"><img alt="Figure 1.13 – Internal structure of an LSTM cell (https://arxiv.org/pdf/2304.11461)" src="image/B21257_01_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.541.1">Figure 1.13 – Internal structure of an LSTM cell (</span><a href="https://arxiv.org/pdf/2304.11461"><span class="koboSpan" id="kobo.542.1">https://arxiv.org/pdf/2304.11461</span></a><span class="koboSpan" id="kobo.543.1">)</span></p>
<p><span class="koboSpan" id="kobo.544.1">An LSTM has internal mechanisms to control the information (gates) within the layer; additionally, it has a dedicated context layer. </span><span class="koboSpan" id="kobo.544.2">So, we have two hidden states in which the first, </span><em class="italic"><span class="koboSpan" id="kobo.545.1">h</span></em><span class="koboSpan" id="kobo.546.1">, serves for information at time </span><em class="italic"><span class="koboSpan" id="kobo.547.1">t</span></em><span class="koboSpan" id="kobo.548.1"> (short memory) and the other, </span><em class="italic"><span class="koboSpan" id="kobo.549.1">c</span></em><span class="koboSpan" id="kobo.550.1">, for information at long term. </span><span class="koboSpan" id="kobo.550.2">The gates can be open (1) or closed (0); this is achieved by a feed-forward layer with sigmoid activation to squeeze values between zero and one. </span><span class="koboSpan" id="kobo.550.3">After that, we </span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.551.1">use the </span><strong class="bold"><span class="koboSpan" id="kobo.552.1">Hadamard product</span></strong><span class="koboSpan" id="kobo.553.1"> (or pointwise multiplication) for the gating mechanism of a layer. </span><span class="koboSpan" id="kobo.553.2">This multiplication acts as a binary gate, allowing information to pass if the value is close to 1 or blocking it when the value is close to 0. </span><span class="koboSpan" id="kobo.553.3">These gates allow a dynamic system in which during a time step, we decide how much information we preserve and how much </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">we forget.</span></span></p>
<p><span class="koboSpan" id="kobo.555.1">The first gate is </span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.556.1">called the </span><strong class="bold"><span class="koboSpan" id="kobo.557.1">forget gate</span></strong><span class="koboSpan" id="kobo.558.1"> because it is used to forget information that is no longer needed from the context and, therefore, will no longer be needed in the next time step. </span><span class="koboSpan" id="kobo.558.2">So, we will use the output of the forget gate to multiply the context. </span><span class="koboSpan" id="kobo.558.3">At this time, we extract both information from the input and the previous time step’s hidden state. </span><span class="koboSpan" id="kobo.558.4">Each gate has a set of gate-specific </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.559.1">U</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.560.1"> weights:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">f</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">f</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></span></p>
<p><span class="koboSpan" id="kobo.561.1">The next step is to extract information from the input and decide which of that information will be added to the context. </span><span class="koboSpan" id="kobo.561.2">This is </span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.562.1">controlled by an </span><strong class="bold"><span class="koboSpan" id="kobo.563.1">input gate</span></strong> <em class="italic"><span class="koboSpan" id="kobo.564.1">i</span></em><span class="koboSpan" id="kobo.565.1"> that controls how much information will then be added. </span><span class="koboSpan" id="kobo.565.2">The context is then obtained by the sum of what we add and what </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">we forget:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">g</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><msub><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">g</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">i</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">i</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></span></p>
<p><span class="koboSpan" id="kobo.567.1">The</span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.568.1"> final step is the calculation of the output; this is achieved with a final gate. </span><span class="koboSpan" id="kobo.568.2">The output or final layer decision is also used to update the </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">hidden state:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">o</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">o</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></span></p>
<p><span class="koboSpan" id="kobo.570.1">These gates are independent of each other so that efficient implementations of the LSTM can parallelize them. </span><span class="koboSpan" id="kobo.570.2">We can test in Python with a PyTorch RNN to see how it is transforming </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.572.1">
data_tensor = torch.tensor(np.random.random((10, 5, 3)),
                           dtype=torch.float32)
LSTM =nn.LSTM(input_size=3, hidden_size=10,
              num_layers=1, batch_first=True)
output, (hidden, cell) = LSTM(data_tensor)
output.shape</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.573.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.574.1">Notice how the model is transforming the data; we can access the hidden state as well as the </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">cell state.</span></span></p>
<p><span class="koboSpan" id="kobo.576.1">We can also note some other </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">interesting properties:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.578.1">Computation augmentation is internal to layers, which means we can easily substitute LSTMs </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">for RNNs.</span></span></li>
<li><span class="koboSpan" id="kobo.580.1">The LSTM </span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.581.1">manages to preserve information for a long time because it retains only the relevant part of the information and forgets what is </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">not needed.</span></span></li>
<li><span class="koboSpan" id="kobo.583.1">Standard practice is to initialize an LSTM with vector 1 (preserves everything), after which it learns by itself what to forget and what </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">to add.</span></span></li>
<li><span class="koboSpan" id="kobo.585.1">An LSTM, as opposed to an RNN, can remember up to 100 time steps (the RNN after 7 time steps starts forgetting). </span><span class="koboSpan" id="kobo.585.2">The plus operation makes vanishing or exploding gradients </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">less likely</span></span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.588.1">Let’s look at another model option that is computationally lighter but still has this notion of </span><span class="No-Break"><span class="koboSpan" id="kobo.589.1">context vector.</span></span></p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/><span class="koboSpan" id="kobo.590.1">GRUs</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.591.1">GRUs</span></strong><span class="koboSpan" id="kobo.592.1"> are </span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.593.1">another variant of RNNs to solve the vanishing gradient problem, thus making them more effective in remembering information. </span><span class="koboSpan" id="kobo.593.2">They are very similar to LSTMs since they have internal gates, but they are much simpler and lighter. </span><span class="koboSpan" id="kobo.593.3">Despite having fewer parameters, GRUs can converge faster than LSTMs and still achieve comparable performance. </span><span class="koboSpan" id="kobo.593.4">GRUs exploit some of the elements that have made LSTMs so effective: the plus operation, the Hadamard product, the presence of a context, and the control of information within </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">the layer:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.595.1"><img alt="Figure 1.14 – Internal structure of a GRU cell (https://arxiv.org/pdf/2304.11461)" src="image/B21257_01_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.596.1">Figure 1.14 – Internal structure of a GRU cell (</span><a href="https://arxiv.org/pdf/2304.11461"><span class="koboSpan" id="kobo.597.1">https://arxiv.org/pdf/2304.11461</span></a><span class="koboSpan" id="kobo.598.1">)</span></p>
<p><span class="koboSpan" id="kobo.599.1">In a GRU, the </span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.600.1">forget gate is </span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.601.1">called the </span><strong class="bold"><span class="koboSpan" id="kobo.602.1">update gate</span></strong><span class="koboSpan" id="kobo.603.1">, but it has the same purpose: important information is retained (values near 1) and unimportant information is rewritten during the update (values near 0). </span><span class="koboSpan" id="kobo.603.2">In a GRU, the input gate is called</span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.604.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.605.1">reset gate</span></strong><span class="koboSpan" id="kobo.606.1"> and is not independent as in an LSTM, but connected to the </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">update gate.</span></span></p>
<p><span class="koboSpan" id="kobo.608.1">The first step is the update gate </span><em class="italic"><span class="koboSpan" id="kobo.609.1">z</span></em><span class="koboSpan" id="kobo.610.1">, which is practically the same as the forget gate in an LSTM. </span><span class="koboSpan" id="kobo.610.2">At the same time, we calculate the reset </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">gate </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.612.1">r</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">z</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">r</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.614.1">The next step is to update the hidden state; this depends on the reset gate. </span><span class="koboSpan" id="kobo.614.2">In this way, we decide what new information is put into the hidden state and what relevant information from the past is saved. </span><span class="koboSpan" id="kobo.614.3">This </span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.615.1">is called the </span><strong class="bold"><span class="koboSpan" id="kobo.616.1">current </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.617.1">memory gate</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Base"><math display="block"><mrow><mrow><mrow><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">h</mi><mo>(</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">h</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>+</mo><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>)</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.619.1">At this point, we have the final update of the hidden state in which we also use the </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">update gate:</span></span></p>
<p class="IMG---Figure"><span class="_-----MathTools-_Math_Variable_v-bold-italic"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>)</mo><mo>⊙</mo><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")" open="("><mi>t</mi></mfenced></msup></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.621.1">We can</span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.622.1"> test in Python with a PyTorch RNN to see how it is transforming </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.624.1">
data_tensor = torch.tensor(np.random.random((10, 5, 3)),
                           dtype=torch.float32)
GRU =nn.GRU(input_size=3, hidden_size=10,
            num_layers=1, batch_first=True)
output, hidden = GRU(data_tensor)
output.shape</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.625.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.626.1">Notice how the model is transforming the data; we can also access the </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">hidden state.</span></span></p>
<p><span class="koboSpan" id="kobo.628.1">We can see some interesting elements here </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">as well:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.630.1">GRU networks are similar to LSTM networks, but they have the advantage of fewer parameters and are computationally more efficient. </span><span class="koboSpan" id="kobo.630.2">This means, though, they are more prone </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">to overfitting.</span></span></li>
<li><span class="koboSpan" id="kobo.632.1">They can handle long sequences of data without forgetting previous inputs. </span><span class="koboSpan" id="kobo.632.2">For many textual tasks (but also speech recognition and music generation) they perform quite well, though they are less efficient than LSTMs when it comes to modeling long-term dependencies or </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">complex patterns.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.634.1">Next, we’ll look </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">at CNNs.</span></span></p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/><span class="koboSpan" id="kobo.636.1">CNNs for text</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.637.1">CNNs</span></strong><span class="koboSpan" id="kobo.638.1"> are</span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.639.1"> designed to find patterns in images (or other 2D matrixes) by running a filter (a matrix or kernel) along them. </span><span class="koboSpan" id="kobo.639.2">The convolution is conducted pixel by pixel, and the filter values are multiplied by the pixels in the image and then summed. </span><span class="koboSpan" id="kobo.639.3">During training, a weight is learned for each of the filter entries. </span><span class="koboSpan" id="kobo.639.4">For each filter, we get a different scan of the image that can be visualized; this is called a </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">feature map.</span></span></p>
<p><span class="koboSpan" id="kobo.641.1">Convolutional networks have been successful </span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.642.1">in </span><strong class="bold"><span class="koboSpan" id="kobo.643.1">computer vision</span></strong><span class="koboSpan" id="kobo.644.1"> because of their ability to extract local information and recognize complex patterns. </span><span class="koboSpan" id="kobo.644.2">For </span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.645.1">this reason, convolutional networks have been proposed for sequences. </span><span class="koboSpan" id="kobo.645.2">In this case, 1-dimensional convolutional networks are exploited, but the idea is the same. </span><span class="koboSpan" id="kobo.645.3">In fact, on a sequence, 1D convolution is used to extract a feature map (instead of being a 2-dimensional filter or matrix, we have a uni-dimensional filter that can be seen as the context window </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">word2vec</span></span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.649.1"><img alt="Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence" src="image/B21257_01_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.650.1">Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence</span></p>
<p><span class="koboSpan" id="kobo.651.1">In the preceding figure, we scroll a uni-dimensional filter over the sequence; the process is very fast, and the filter can have an arbitrary size (three to seven words or even more). </span><span class="koboSpan" id="kobo.651.2">The model tries to learn patterns among the various words found within this kernel. </span><span class="koboSpan" id="kobo.651.3">It can also be used on vectors previously obtained from an embedding, and we can also use multiple kernels (so as to learn different patterns for each sequence). </span><span class="koboSpan" id="kobo.651.4">As with image CNNs, we can add operations such as max pooling to extract the most </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">important features.</span></span></p>
<p><span class="koboSpan" id="kobo.653.1">We can</span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.654.1"> test in Python with a PyTorch RNN to see how it is transforming </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">the data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.656.1">
data_tensor = torch.tensor(np.random.random((10, 5, 3)),
                           dtype=torch.float32)
Conv1d = nn.Conv1d(in_channels=5, out_channels=16,
                   kernel_size=3, stride=1, padding=1)
output = Conv1d(data_tensor)
output.shape</span></pre> <p class="callout-heading"><span class="koboSpan" id="kobo.657.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.658.1">Notice how the model is transforming the data and how this is different from what we have </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">seen before.</span></span></p>
<p><span class="koboSpan" id="kobo.660.1">Now that we have a method to transform text into numerical representation (while preserving the contextual information) and models that can handle this representation, we can combine them to obtain an </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">end-to-end system.</span></span></p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/><span class="koboSpan" id="kobo.662.1">Performing sentiment analysis with embedding and deep learning</span></h1>
<p><span class="koboSpan" id="kobo.663.1">In this section, we</span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.664.1"> will train a model for conducting sentiment analysis on movie reviews. </span><span class="koboSpan" id="kobo.664.2">The model we will train will be able to classify reviews as positive or negative. </span><span class="koboSpan" id="kobo.664.3">To build and train the model, we will exploit the elements we have encountered so far. </span><span class="koboSpan" id="kobo.664.4">In brief, we’re doing </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.666.1">We are preprocessing the dataset, transforming in numerical vectors, and harmonizing </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">the vectors</span></span></li>
<li><span class="koboSpan" id="kobo.668.1">We are defining a neural network with an embedding and </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">training it</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.670.1">The dataset consists of 50,000 positive and negative reviews. </span><span class="koboSpan" id="kobo.670.2">We can see that it contains a heterogeneous length for reviews and that on average, there are </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">230 words:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.672.1"><img alt="Figure 1.16 – Graphs showing the distribution of the length of the review in the text; the left plot is for positive reviews, while the right plot is for negative reviews" src="image/B21257_01_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.673.1">Figure 1.16 – Graphs showing the distribution of the length of the review in the text; the left plot is for positive reviews, while the right plot is for negative reviews</span></p>
<p><span class="koboSpan" id="kobo.674.1">In </span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.675.1">addition, the most prevalent words are, obviously, “</span><em class="italic"><span class="koboSpan" id="kobo.676.1">movie</span></em><span class="koboSpan" id="kobo.677.1">” </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">and “</span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.679.1">film</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">”:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.681.1"><img alt="Figure 1.17 – Word cloud for the most frequent words in positive (left plot) and negative (right plot) reviews" src="image/B21257_01_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.682.1">Figure 1.17 – Word cloud for the most frequent words in positive (left plot) and negative (right plot) reviews</span></p>
<p><span class="koboSpan" id="kobo.683.1">The text is messy and must be cleaned before the model can be trained. </span><span class="koboSpan" id="kobo.683.2">The first step is binary encoding of the label (“positive” equals 0, “negative” equals 1). </span><span class="koboSpan" id="kobo.683.3">After that, we divide the features and the</span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.684.1"> labels (for a dataset in </span><strong class="bold"><span class="koboSpan" id="kobo.685.1">supervised learning</span></strong><span class="koboSpan" id="kobo.686.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.687.1">X</span></strong><span class="koboSpan" id="kobo.688.1"> are the features and </span><strong class="source-inline"><span class="koboSpan" id="kobo.689.1">y</span></strong><span class="koboSpan" id="kobo.690.1"> are the labels). </span><span class="koboSpan" id="kobo.690.2">Next, we create three balanced datasets for training, validation, </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">and testing</span></span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.693.1">
df['sentiment_encoded'] = np.where(
    df['sentiment']=='positive',0,1)
X,y = df['review'].values, df['sentiment_encoded'].values
x_train,x_test,y_train,y_test = train_test_split(
    X,y,stratify=y, test_size=.2)
x_train,x_val,y_train,y_val = train_test_split(
    x_train,y_train,stratify=y_train, test_size=.1)
y_train, y_val, y_test = np.array(y_train), np.array(y_val), \
    np.array(y_test)</span></pre> <p><span class="koboSpan" id="kobo.694.1">A few </span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.695.1">steps are necessary before proceeding with </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">the training:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.697.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.698.1">preprocessing</span></strong><span class="koboSpan" id="kobo.699.1"> step </span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.700.1">in which we remove excessive spaces, special characters, </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">and punctuation.</span></span></li>
<li><span class="koboSpan" id="kobo.702.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.703.1">tokenization</span></strong><span class="koboSpan" id="kobo.704.1"> step </span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.705.1">in which we convert the various reviews into tokens. </span><span class="koboSpan" id="kobo.705.2">In this step, we also remove stopwords and single-character words. </span><span class="koboSpan" id="kobo.705.3">We extract for each review only the 1,000 most popular words (this step is only to reduce computation time </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">during training).</span></span></li>
<li><span class="koboSpan" id="kobo.707.1">Transformation of the the words into </span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.708.1">indices (</span><strong class="bold"><span class="koboSpan" id="kobo.709.1">vectorization</span></strong><span class="koboSpan" id="kobo.710.1">) according to our vocabulary to make the model work with </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">numerical values.</span></span></li>
<li><span class="koboSpan" id="kobo.712.1">Since the reviews have different lengths, we apply padding to harmonize the length of the review to a fixed number (we need this for </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">the training).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.714.1">These preprocessing steps depend on the dataset. </span><span class="koboSpan" id="kobo.714.2">The code is in the GitHub repository. </span><span class="koboSpan" id="kobo.714.3">Note, however, that the tokenization and preprocessing choices alter the properties of the reviews – in this case, the </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">summary statistics.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.716.1"><img alt="Figure 1.18 – Graph showing the distribution of review length after tokenization" src="image/B21257_01_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.717.1">Figure 1.18 – Graph showing the distribution of review length after tokenization</span></p>
<p><span class="koboSpan" id="kobo.718.1">We </span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.719.1">are defining the model with its hyperparameters. </span><span class="koboSpan" id="kobo.719.2">In this case, we are training a neural network to predict sentiment data composed of embeddings and GRUs. </span><span class="koboSpan" id="kobo.719.3">To make the training more stable, we add regularization (dropout). </span><span class="koboSpan" id="kobo.719.4">The linear layer is to map these features that we extracted to a single representation. </span><span class="koboSpan" id="kobo.719.5">We use this representation to calculate the probability that the review is positive </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">or negative:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.721.1">
# Hyperparameters
no_layers = 3
vocab_size = len(vocab) + 1  # extra 1 for padding
embedding_dim = 300
output_dim = 1
hidden_dim = 256
# Initialize the model
model = SentimentRNN(no_layers, vocab_size, hidden_dim,
                     embedding_dim, drop_prob=0.5)</span></pre> <p><span class="koboSpan" id="kobo.722.1">Note that in this case, we use binary cross-entropy loss because we have only two categories (positive and negative). </span><span class="koboSpan" id="kobo.722.2">Also, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.723.1">Adam</span></strong><span class="koboSpan" id="kobo.724.1"> as an optimizer, but one can test others. </span><span class="koboSpan" id="kobo.724.2">In </span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.725.1">this case, we conduct batch training since we have thousands </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">of reviews:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.727.1">
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epoch_tr_loss, epoch_vl_loss = [], []
epoch_tr_acc, epoch_vl_acc = [], []
for epoch in range(epochs):
    train_losses = []
    train_acc = 0.0
    model.train()
    h = model.init_hidden(50)
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        h = h.data
        model.zero_grad()
        output, h = model(inputs, h)
        loss = criterion(output.squeeze(), labels.float())
        loss.backward()
        train_losses.append(loss.item())
        accuracy = acc(output, labels)
        train_acc += accuracy
        optimizer.step()</span></pre> <p><span class="koboSpan" id="kobo.728.1">The following graph displays the accuracy and loss for the training and </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">validation sets:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.730.1"><img alt="Figure 1.19 – Training curves for training and validation set, for accuracy and loss" src="image/B21257_01_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.731.1">Figure 1.19 – Training curves for training and validation set, for accuracy and loss</span></p>
<p><span class="koboSpan" id="kobo.732.1">The </span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.733.1">model achieves good accuracy, as we can see from the following </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">confusion matrix:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.735.1"><img alt="Figure 1.20 – Confusion matrix for the test set" src="image/B21257_01_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.736.1">Figure 1.20 – Confusion matrix for the test set</span></p>
<p><span class="koboSpan" id="kobo.737.1">In addition, if we look at the projection of reviews before and after the training, we can see that the model has learned how to separate positive and </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">negative reviews:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.739.1"><img alt="Figure 1.21 – Embedding projection obtained from the model before (left plot) and after (right plot) training" src="image/B21257_01_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.740.1">Figure 1.21 – Embedding projection obtained from the model before (left plot) and after (right plot) training</span></p>
<p><span class="koboSpan" id="kobo.741.1">We have</span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.742.1"> now trained a model that can take a review in plain text and classify it as positive or negative. </span><span class="koboSpan" id="kobo.742.2">We did that by combining the elements we saw previously in the chapter. </span><span class="koboSpan" id="kobo.742.3">The same approach can be followed with any other dataset; that is the power of </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">deep learning.</span></span></p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/><span class="koboSpan" id="kobo.744.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.745.1">In this chapter, we saw how to transform text to an increasingly complex vector representation. </span><span class="koboSpan" id="kobo.745.2">This numerical representation of text allowed us to be able to use machine learning models. </span><span class="koboSpan" id="kobo.745.3">We saw how to preserve the contextual information (word embedding) of a text and how this can then be used for later analysis (for example, searching synonyms or clustering words). </span><span class="koboSpan" id="kobo.745.4">In addition, we saw how neural networks (RNNs, LSTM, GRUs) can be used to analyze text and perform tasks (for example, </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">sentiment analysis).</span></span></p>
<p><span class="koboSpan" id="kobo.747.1">In the next chapter, we will see how to solve some of the remaining unsolved challenges and see how this will lead to the natural evolution of the models </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">seen here.</span></span></p>
</div>
</body></html>