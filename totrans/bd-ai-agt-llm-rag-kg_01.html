<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor014"/>1</h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Analyzing Text Data with Deep Learning</h1>
<p>Language is one of the most amazing abilities of human beings; it evolves during the individual’s lifetime and is capable of conveying a message with complex meaning. Language in its natural form is not understandable to machines, and it is extremely challenging to develop an algorithm that can pick up the different nuances. Therefore, in this chapter, we will discuss how to represent text in a form that is digestible by machines.</p>
<p>In natural form, text cannot be directly fed to a <strong class="bold">deep learning</strong> model. In this chapter, we will discuss how text can be represented in a form that can be used by <strong class="bold">machine learning</strong> models. Starting with natural text, we will transform the text into numerical vectors that are increasingly sophisticated (one-hot encoding, <strong class="bold">bag of words</strong> (<strong class="bold">BoW</strong>), <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">TF-IDF</strong>)) until we create vectors of real numbers that represent the meaning of a word (or document) and allow us to conduct operations (word2vec). In this chapter, we introduce deep learning models, such as <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>), l<strong class="bold">ong short-term memory</strong> (<strong class="bold">LSTM</strong>), <strong class="bold">gated recurrent units </strong>(<strong class="bold">GRUs</strong>), and <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNNs</strong>), to analyze sequences and discuss their strengths as well as the problems associated with them. Finally, we will assemble these models all together to conduct text classification, showing the power of the learned approaches. </p>
<p>By the end of this chapter, we will be able to take a corpus of text and use deep learning to analyze it. These are the bases that will help us understand how a <strong class="bold">large language model</strong> (<strong class="bold">LLM</strong>) (such as ChatGPT) works internally.</p>
<p>In this chapter, we'll be covering the following topics:</p>
<ul>
<li>Representing text for AI</li>
<li>Embedding, application, and representation</li>
<li>RNNs, LSTMs, GRUs, and CNNs for text</li>
<li>Performing sentiment analysis with embedding and deep learning</li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Technical requirements</h1>
<p>In this chapter, we will use standard libraries for Python. The necessary libraries can be found within each of the Jupyter notebooks that are in the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr1</a>. The code can be executed on a CPU, but a GPU is advised.</p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Representing text for AI</h1>
<p>Compared <a id="_idIndexMarker000"/>to other types of data (such as images or tables), it is much more challenging to represent text in a digestible representation<a id="_idIndexMarker001"/> for computers, especially because there is no unique relationship between the meaning of a word (signified) and the symbol that represents it (signifier). In fact, the meaning of a word changes from the context and the author’s intentions in using it in a sentence. In addition, native text has to be transformed into a numerical representation to be ingested by an algorithm, which is not a trivial task. Nevertheless, several approaches were initially developed to be able to find a vector representation of a text. These vector representations have the advantage that they can then be used as input to a computer.</p>
<p>First, a collection <a id="_idIndexMarker002"/>of texts (<strong class="bold">corpus</strong>) should be divided into fundamental units (words). This process requires making certain decisions and process operations that collectively are <a id="_idIndexMarker003"/>called <strong class="bold">text normalization</strong>. A sentence, therefore, is divided into words by exploiting the natural division of spaces (<strong class="bold">text segmentation</strong>); each punctuation mark<a id="_idIndexMarker004"/> is also considered a single word. In fact, punctuation marks are considered to be the boundaries of sentences and convey important information (change of topic, questions, exclamations).</p>
<p>The second step is the definition of what a word is and whether some terms in the corpus should be directly joined under the same vocabulary instance. For example, “He” and “he” represent the same instance; the former is only capitalized. Since an algorithm does not include such nuances, one must normalize the text in lowercase. In some cases, we want to conduct more sophisticated normalizations<a id="_idIndexMarker005"/> such as <strong class="bold">lemmatization</strong> (joining words with the same root: “came” and “comes” are two forms of the verb) or <strong class="bold">stemming</strong> (stripping<a id="_idIndexMarker006"/> all suffixes of words).</p>
<p><strong class="bold">Tokenization</strong> is the <a id="_idIndexMarker007"/>task of transforming a text into fundamental units. This is because, in addition to words, a text may also include percentages, numbers, websites, and other components. We will return to this later, but in the meantime, we will look at some simpler forms of tokenization.</p>
<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>One-hot encoding</h2>
<p>In<a id="_idIndexMarker008"/> traditional <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>), text representation is conducted using<a id="_idIndexMarker009"/> discrete symbols. The simplest example is one-hot encoding. From a<a id="_idIndexMarker010"/> sequence of text in a corpus (consisting of <em class="italic">n</em> different words), we obtain an <em class="italic">n</em>-dimensional vector. In fact, the first step is to compute the set of different words present in the whole text corpus called vocabulary. For each word, we obtain a vector as long as the size of the vocabulary. Then for each word, we will have a long vector composed mainly of zeros and ones to represent the word (one-hot vectors). This system is mainly used when we want a matrix of features and then train a model. This process is also <a id="_idIndexMarker011"/>called <strong class="bold">vectorization</strong>; here's a sparse vector for the following two words:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>r</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>t</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow></math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>p</mi><mi>i</mi><mi>z</mi><mi>z</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>a</mi><mo>=</mo><mfenced close="]" open="["><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>1</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd><mtd><mtable columnalign="center center center" columnspacing="0.8000em 0.8000em" columnwidth="auto auto auto" rowalign="baseline"><mtr><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd><mtd><mn>0</mn></mtd></mtr></mtable></mtd></mtr></mtable></mfenced></mrow></mrow></math></p>
<p>There are different problems associated with this representation. First, it captures only the presence (or the absence) of a word in a document. Thus, we are losing all the semantic relationships between the words. Second, an average language has about 200,000 words, so for each word, we would have a vector of length 200,000. This leads to very sparse and high-dimensional vectors. For large corpora, we need high memory to store the vectors and high computational capacity to handle them. In addition, there is no notion of similarity. The two words in the preceding example are two places that sell food, and we would like the vectors representing these words to encode this similarity. If the vectors had a notion of similarity, we could conduct clustering, and the synonyms would be in the same cluster.</p>
<p>In order to obtain such a matrix, we must do the following:</p>
<ul>
<li>Standardize the text before tokenization. In this case, we simply transform everything into lowercase.</li>
<li>We construct a vocabulary constituted of unique words and save the vocabulary so that in a case from a vector, we can get the corresponding word.</li>
<li>We create an array and then populate it with <code>1</code> at the index of the word in the vocabulary; <code>0</code>s elsewhere.</li>
</ul>
<p>Let’s <a id="_idIndexMarker012"/>take a look at how this works in<a id="_idIndexMarker013"/> code:</p>
<pre class="source-code">
import numpy as np
def one_hot_encoding(sentence):
    words = sentence.lower().split()
    vocabulary = sorted(set(words))
    word_to_index = {word: i for i,
        word in enumerate(vocabulary)}
    one_hot_matrix = np.zeros((
        len(words), len(vocabulary)), dtype=int)
    for i, word in enumerate(words):
        one_hot_matrix[i, word_to_index[word]] = 1
    return one_hot_matrix, vocabulary</pre> <p>Let’s look at a specific example:</p>
<pre class="source-code">
sentence = "Should we go to a pizzeria or do you prefer a restaurant?"
one_hot_matrix, vocabulary = one_hot_encoding(sentence)
print("Vocabulary:", vocabulary)
print("One-Hot Encoding Matrix:\n", one_hot_matrix)</pre> <p>We get the following output:</p>
<div><div><img alt="" src="img/B21257_01_Figure_01.jpg"/>
</div>
</div>
<p class="callout-heading">Important note</p>
<p class="callout">Observe how choosing another sentence will result in a different matrix and how, by increasing the length of the sentence, the matrix grows proportionally to the number of different words. Also, note that for repeated words, we get equal vectors. Check the preceding output.</p>
<p>Even<a id="_idIndexMarker014"/> if it <a id="_idIndexMarker015"/>is a simple method, we have obtained a first representation of text in a vectorial form.</p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Bag-of-words</h2>
<p>In the <a id="_idIndexMarker016"/>previous section, we discussed one-hot encoding and some of the problems associated with this form of text representation. In<a id="_idIndexMarker017"/> the previous example, we worked with a single sentence, but a corpus is made up of thousands if not millions of documents; each of these documents contains several words with a different frequency. We want a system that preserves this frequency information, as it is important for the classification of text. In fact, documents that have similar content are similar, and their meaning will also be similar.</p>
<p><strong class="bold">BoW</strong> is an algorithm for extracting features from text that preserves this frequency property. BoW is a very simple algorithm that ignores the position of words in the text and only considers this frequency property. The name “bag” comes precisely from the fact that any information concerning sentence order and structure is not preserved by the algorithm. For<a id="_idIndexMarker018"/> BoW, we only need a vocabulary and a way to be able to count words. In this case, the idea is to create document vectors: a single vector represents a document and the frequency of words contained in the vocabulary. <em class="italic">Figure 1</em><em class="italic">.1</em> visualizes this concept with a few lines from <em class="italic">Hamlet</em>:</p>
<div><div><img alt="Figure 1.1 – Representation of the BoW algorithm" src="img/B21257_01_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Representation of the BoW algorithm</p>
<p>Even this <a id="_idIndexMarker019"/>representation is not without problems. Again, as the vocabulary grows, so will the size of the vectors (the size of each vector is equal to the length of the vocabulary). In addition, these vectors tend to be scattered, especially when the documents are very different from each other. High-dimensional or sparse vectors are not only problematic for memory and computational costs but for algorithms as well (the longer the vectors, the more weight you need in the algorithm, leading to a risk of overfitting). This is called the <strong class="bold">curse of dimensionality</strong>; the <a id="_idIndexMarker020"/>greater the number of features, the less meaningful the distances between examples. For large corpora, some solutions have been proposed, such as ignoring punctuation, correcting misspelled words, stemming algorithms, or ignoring words with high frequency that don’t add information (articles, prepositions, and so on).</p>
<p>In order to get a BoW matrix for a list of documents, we need to do the following:</p>
<ul>
<li>Tokenize each document to get a list of words.</li>
<li>Create our vocabulary of unique words and map each word to the corresponding index in the vocabulary.</li>
<li>Create a matrix where each row represents a document and each column, instead, a word in the vocabulary (the documents are the examples, and the words are the associated features).</li>
</ul>
<p>Let’s<a id="_idIndexMarker021"/> look at<a id="_idIndexMarker022"/> the code again:</p>
<pre class="source-code">
import numpy as np
def bag_of_words(sentences):
    """
    Creates a bag-of-words representation of a list of documents.
    """
    tokenized_sentences = [
        sentence.lower().split() for sentence in sentences
    ]
    flat_words = [
        word for sublist in tokenized_sentences for word in sublist
    ]
    vocabulary = sorted(set(flat_words))
    word_to_index = {word: i for i, word in enumerate(vocabulary)}
    bow_matrix = np.zeros((
        len(sentences), len(vocabulary)), dtype=int)
    for i, sentence in enumerate(tokenized_sentences):
        for word in sentence:
            if word in word_to_index:
                bow_matrix[i, word_to_index[word]] += 1
    return vocabulary, bow_matrix</pre> <p>Here’s an example:</p>
<pre class="source-code">
corpus = ["This movie is awesome awesome",
          "I do not say is good, but neither awesome",
          "Awesome? Only a fool can say that"]
vocabulary, bow_matrix = bag_of_words(corpus)
print("Vocabulary:", vocabulary)
print("Bag of Words Matrix:\n", bow_matrix)</pre> <p>This <a id="_idIndexMarker023"/>prints the following output:</p>
<div><div><img alt="" src="img/B21257_01_Figure_02.jpg"/>
</div>
</div>
<p class="callout-heading">Important note</p>
<p class="callout">Note how in the example, the word “<code>awesome</code>” is associated with a review with a positive, neutral, or negative meaning. Without context, the frequency of the word “awesome” alone does not tell us the sentiment of the review.</p>
<p>Here, we<a id="_idIndexMarker024"/> have learned how to transform text in a vectorial form while keeping the notion of frequency for each word.</p>
<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>TF-IDF</h2>
<p>In the<a id="_idIndexMarker025"/> previous<a id="_idIndexMarker026"/> section, we obtained a document-term matrix. However, the raw frequency is very skewed and does not always allow us to discriminate between two documents. The document-term matrix was born in information retrieval to find documents, though words such as “good” or “bad” are not very discriminative since they are often used in text with a generic meaning. In contrast, words with low frequency are much more informative, so we are interested<a id="_idIndexMarker027"/> more in relative than absolute frequency:</p>
<div><div><img alt="Figure 1.2 – Intuition of the components of TF-IDF" src="img/B21257_01_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Intuition of the components of TF-IDF</p>
<p>Instead of using raw frequency, we can use the logarithm in base 10, because a word that <a id="_idIndexMarker028"/>occurs 100 times in a document is not 100 times more relevant to its meaning in the document. Of course, since vectors can be very sparse, we assign 0 if the frequency is 0. Second, we want to pay more attention to words that are present only in some documents. These words will be more relevant to the meaning of the document, and we want to preserve this information. To do this, we normalize by IDF. IDF is defined as the ratio of the total number of documents in the corpus to how many documents a term is present in. To summarize, to obtain the TF-IDF, we multiply TF by the logarithm of IDF.</p>
<p>This is demonstrated in the following code block:</p>
<pre class="source-code">
import numpy as np
def compute_tf(sentences):
    """Compute the term frequency matrix for a list of sentences."""
    vocabulary = sorted(set(
        word for sentence in sentences
        for word in sentence.lower().split()))
    word_index = {word: i for i, word in enumerate(vocabulary)}
    tf = np.zeros((
        len(sentences), len(vocabulary)), dtype=np.float32)
    for i, sentence in enumerate(sentences):
        words = sentence.lower().split()
        word_count = len(words)
        for word in words:
            if word in word_index:
                tf[i, word_index[word]] += 1 / word_count
    return tf, vocabulary
def compute_idf(sentences, vocabulary):
    """Compute the inverse document frequency for a list of sentences."""
    num_documents = len(sentences)
    idf = np.zeros(len(vocabulary), dtype=np.float32)
    word_index = {word: i for i, word in enumerate(vocabulary)}
    for word in vocabulary:
        df = sum(
            1 for sentence in sentences
            if word in sentence.lower().split()
        )
        idf[word_index[word]] = np.log(
            num_documents / (1 + df)) + 1  # Smoothing
    return idf
def tf_idf(sentences):
    """Generate a TF-IDF matrix for a list of sentences."""
    tf, vocabulary = compute_tf(sentences)
    idf = compute_idf(sentences, vocabulary)
    tf_idf_matrix = tf * idf
    return vocabulary, tf_idf_matrix
vocabulary, tf_idf_matrix = tf_idf(corpus)
print("Vocabulary:", vocabulary)
print("TF-IDF Matrix:\n", tf_idf_matrix)</pre> <p>This <a id="_idIndexMarker029"/>generates<a id="_idIndexMarker030"/> the following output:</p>
<div><div><img alt="" src="img/B21257_01_Figure_03.jpg"/>
</div>
</div>
<p class="callout-heading">Important note</p>
<p class="callout">In this example, we used the same corpus as in the previous section. Note how word frequencies changed after this normalization.</p>
<p>In this section, we <a id="_idIndexMarker031"/>learned how we can normalize text to decrease the impact of the most frequent<a id="_idIndexMarker032"/> words and give relevance to words that are specific to a subset of documents. Next, we’ll discuss embedding.</p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Embedding, application, and representation</h1>
<p>In the previous section, we discussed how to use vectors to represent text. These vectors are digestible for a computer, but they still suffer from some problems (sparsity, high dimensionality, etc.). According to the distributional hypothesis, words with a similar meaning frequently appear close together (or words that appear often in the same context have the same meaning). Similarly, a word can have a different meaning depending on its context: “I went to deposit money in the <em class="italic">bank</em>” or “We went to do a picnic on the river <em class="italic">bank</em>.” In the following diagram, we have a high-level representation of the embedding process. So, we want a process that allows us to start from text to obtain an array of vectors, where each vector corresponds to the representation of a word. In this <a id="_idIndexMarker033"/>case, we want a model that will then allow us to map each word to a vector representation. In the next section, we will describe the process in detail and discuss the theory behind it.</p>
<div><div><img alt="Figure 1.3 – High-level representation of the embedding process" src="img/B21257_01_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – High-level representation of the embedding process</p>
<p>We would, therefore, like to generate vectors that are small in size, composed of real (dense) numbers, and that preserve this contextual information. Thus, the purpose is to have vectors of limited size that can represent the meaning of a word. The scattered vectors we obtained earlier cannot be used efficiently for mathematical operations or downstream tasks. Also, the more words there are in the vocabulary, the larger the size of the vectors we get. Therefore, we want dense vectors (with real numbers) that are small in size and whose size does not increase as the number of words in the vocabulary increases.</p>
<p>In addition, these <a id="_idIndexMarker034"/>vectors have a distributed representation of the meaning of the word (whereas in sparse vectors, it was local or where the <code>1</code> was located). As we will see a little later, these dense vectors can be used for different operations because they better represent the concept of<a id="_idIndexMarker035"/> similarity between words. These dense vectors are called <strong class="bold">word embeddings</strong>.</p>
<p>This concept was introduced in 2013 by Mikolov with a framework called <strong class="bold">word2vec</strong>, which will be described in detail in the next section.</p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Word2vec</h2>
<p>The<a id="_idIndexMarker036"/> intuition behind word2vec is simple: predict a word <em class="italic">w</em> from its context. To do this, we<a id="_idIndexMarker037"/> need a <strong class="bold">neural network</strong> and a large corpus. The revolutionary idea is that by training this neural network to predict which words <em class="italic">c</em> are needed near the target word <em class="italic">w</em>, the weights of the neural network will be the embedding vectors. This model is self-supervised; the labels in this case are implicit, and we do not provide them.</p>
<p>Word2vec simplifies this idea by making the system extremely fast and effective in two ways: by turning the task into binary classification (Is the word <em class="italic">c</em> needed in the context of the word <em class="italic">w</em>? Yes or no?) and using a logistic regression classifier:</p>
<div><div><img alt="Figure 1.4 – In word2vec, we slide a context window (here represented as a  three-word context window), and then we randomly sample some negative words" src="img/B21257_01_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – In word2vec, we slide a context window (here represented as a three-word context window), and then we randomly sample some negative words</p>
<p>Given a text <em class="italic">t</em>, we scroll a window <em class="italic">c</em> (our context) for a word <em class="italic">w</em> in the center of our window; the words around it are examples of the positive class. After that, we select other random words as negative examples. Finally, we train a model to classify the positive and negative examples; the weights of the model are our embeddings.</p>
<p>Given a word <em class="italic">w</em> and a word <em class="italic">c</em>, we want the probability that the word <em class="italic">c</em> is in the context of <em class="italic">w</em> to be similar to its <a id="_idIndexMarker038"/>embedding similarity. In other words, if the vector representing <em class="italic">w</em> and <em class="italic">c</em> are similar, <em class="italic">c</em> must often be in the context of <em class="italic">w</em> (word2vec is based on the notion of context similarity). We define this embedding similarity by the dot product between the two embedding vectors for <em class="italic">w</em> and <em class="italic">c</em> (we use the sigmoid function to transform this dot product into a probability and thus allow comparison). So, the probability that <em class="italic">c</em> is in the context of <em class="italic">w</em> is equal to the probability that their embeddings are similar:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mi>σ</mi><mfenced close=")" open="("><mrow><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><mo>−</mo><mi mathvariant="bold-italic">c</mi><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></math></p>
<p>This is done for all words in context <em class="italic">L</em>. To simplify, we assume that all words in the context window are independent, so we can multiply the probabilities of the various words <em class="italic">c</em>. Similarly, we want to ensure that this dot product is minimal for words that are not in the context of word <em class="italic">w</em>. So, on the one hand, we maximize the probability for words in the context, and on the other hand, we minimize the probability for words that are not in the context. In fact, words that are not in the context of <em class="italic">w</em> are randomly extracted during training, and the process is the same:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></math></p>
<p>For simplicity, we take the logarithm of probability:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>+</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><mo>−</mo><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mi>P</mi><mfenced close=")" open="("><mrow><mo>−</mo><mo>|</mo><mi>w</mi><mo>,</mo><mi>c</mi></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>L</mi></munderover><mrow><mi mathvariant="normal">l</mi><mi mathvariant="normal">o</mi><mi mathvariant="normal">g</mi><mo>(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">i</mi></msub><mo>∙</mo><mi mathvariant="bold-italic">w</mi></mrow></mfenced></mrow></mfrac><mo>)</mo></mrow></mrow></mrow></mrow></math></p>
<p>The matrix of weights <em class="italic">w</em> is our embedding; it is what we will use from now on. Actually, the model learns two matrices of vectors (one for <em class="italic">w</em> and one for <em class="italic">c</em>), but the two matrices are very similar, so we take just one. We then use cross-entropy to train the models and learn the weights for each vector:</p>
<p class="IMG---Figure"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mi>P</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mo>-</mml:mo></mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></p>
<p>This is represented visually in the following diagram:</p>
<div><div><img alt="Figure 1.5 – Word and context embedding" src="img/B21257_01_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Word and context embedding</p>
<p>The following choices<a id="_idIndexMarker039"/> affect the quality of embedding:</p>
<ul>
<li><em class="italic">Data quality is critical</em>. For example, leveraging Wikipedia allows better embedding for semantic tasks, while using news improves performance for syntactic tasks (a mixture of the two is recommended). Using Twitter or other social networks can insert bias.</li>
<li>At the same time, <em class="italic">a larger amount of text improves embedding performance</em>. A large amount of text can partially compensate for poor quality but at the cost of much longer training (for example, Common Crawl is a huge dataset downloaded from the internet that is pretty dirty, though).</li>
<li><em class="italic">The number of dimensions is another important factor</em>. The larger the size of the embedding, the better its performance. 300 is considered a sweet spot because, beyond this size, number performance does not increase significantly.</li>
<li><em class="italic">The size of the context window also has an impact</em>. Generally, a context window of 4 is used, but a context window of 2 allows for vectors that better identify parts of speech. In contrast, long context windows are more useful if we are interested in similarity more broadly.</li>
</ul>
<p>In Python, we <a id="_idIndexMarker040"/>can easily get an embedding from lists of tokens using the following code:</p>
<pre class="source-code">
from gensim.models import Word2Vec
model = Word2Vec(sentences=list_of_tokens,
                 sg=1,
                 vector_size=100,
                 window=5,
                 workers=4)</pre> <p class="callout-heading">Important note</p>
<p class="callout">The complete code is present in the GitHub repository. We used an embedding of 100 dimensions and a window of 5 words.</p>
<p>Once we have our embedding, we<a id="_idIndexMarker041"/> can visualize it. For example, if we try <strong class="bold">clustering</strong> the vectors of some words, words that have similar meanings should be closer together:</p>
<div><div><img alt="Figure 1.6 – Clustering of some of the vectors obtained from the embedding" src="img/B21257_01_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Clustering of some of the vectors obtained from the embedding</p>
<p>Another<a id="_idIndexMarker042"/> way to visualize vectors is to use dimensionality reduction techniques. Vectors are multidimensional (100-1,024), so it is more convenient to reduce them to two or three dimensions so that they can be visualized more easily. Some of the most <a id="_idIndexMarker043"/>commonly used techniques<a id="_idIndexMarker044"/> are <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>) and <strong class="bold">t-distributed stochastic neighbor embedding</strong> (<strong class="bold">t-SNE)</strong>. <strong class="bold">Uniform Manifold Approximation and Projection</strong> (<strong class="bold">UMAP</strong>), on the other hand, is a technique that has<a id="_idIndexMarker045"/> become the first choice for visualizing multidimensional data in recent years:</p>
<div><div><img alt="Figure 1.7 – 2D projection of word2vec embedding highlighting some examples" src="img/B21257_01_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – 2D projection of word2vec embedding highlighting some examples</p>
<p>UMAP has emerged because it produces visualizations that better preserve semantic meaning and relationships between examples and also better represent local and global structures. This makes for better clusters, and UMAP can also be used in preprocessing steps before a classification task on vectors.</p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>A notion of similarity for text</h2>
<p>Once we have obtained vector<a id="_idIndexMarker046"/> representations, we need a method to calculate the similarity between them. This is crucial in many applications—for instance, to find words in an embedding space that are most similar to a given word, we compute the similarity between its vector and those of other words. Similarly, given a query sentence, we can retrieve the most relevant documents by comparing its vector with document embeddings and selecting those with the highest similarity.</p>
<p>Most similarity measures <a id="_idIndexMarker047"/>are based on the <strong class="bold">dot product</strong>. This is because the dot product is high when the two vectors have values in the same dimension. In contrast, vectors that have zero alternately will have a dot product of zero, thus orthogonal or dissimilar. This is why the dot product was used as a similarity measure for word co-occurrence matrices or with vectors derived from document TF matrices:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">t</mi><mi mathvariant="bold-italic">p</mi><mi mathvariant="bold-italic">r</mi><mi mathvariant="bold-italic">o</mi><mi mathvariant="bold-italic">d</mi><mi mathvariant="bold-italic">u</mi><mi mathvariant="bold-italic">c</mi><mi mathvariant="bold-italic">t</mi><mo>:</mo><mi mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub><mo>=</mo><msub><mi>a</mi><mn>1</mn></msub><mo>×</mo><msub><mi>b</mi><mn>1</mn></msub><mo>+</mo><msub><mi>a</mi><mn>2</mn></msub><mo>×</mo><msub><mi>b</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><msub><mi>a</mi><mi>n</mi></msub><mo>×</mo><msub><mi>b</mi><mi>n</mi></msub></mrow></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>m</mi><mi>a</mi><mi>g</mi><mi>n</mi><mi>i</mi><mi>t</mi><mi>u</mi><mi>d</mi><mi>e</mi><mo>=</mo><mfenced close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mo>=</mo><mroot><mrow><munderover><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mrow></math></p>
<p>The dot product has several problems, though:</p>
<ul>
<li>It tends to favor vectors with long dimensions</li>
<li>It favors vectors with high values (which, in general, are those of very frequent and, therefore, useless words)</li>
<li>The value of the dot product has no limits</li>
</ul>
<p>Therefore, alternatives have been sought, such as a <a id="_idIndexMarker048"/>normalized version of the dot product. The normalized <a id="_idIndexMarker049"/>dot product is equivalent to the cosine of the angle between the two vectors, hence <strong class="bold">cosine similarity</strong>:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mi>c</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">Θ</mi><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">a</mi><mo>∙</mo><mi mathvariant="bold-italic">b</mi></mrow><mrow><mfenced close="|" open="|"><mi mathvariant="bold-italic">a</mi></mfenced><mfenced close="|" open="|"><mi mathvariant="bold-italic">b</mi></mfenced></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mrow><msub><mi>a</mi><mi>i</mi></msub><mo>×</mo><msub><mi>b</mi><mi>i</mi></msub></mrow></mrow><mrow><mroot><mrow><msubsup><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>a</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot><mroot><mrow><msubsup><mo>∑</mo><mrow><mi mathvariant="bold-italic">i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><msubsup><mi>b</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mn>2</mn></mroot></mrow></mfrac></mrow></mrow></math></p>
<p>Cosine similarity <a id="_idIndexMarker050"/>has some interesting properties:</p>
<ul>
<li>It is between -1 and 1. Opposite or totally dissimilar vectors will have a value of -1, 0 for orthogonal vectors (or totally dissimilar for scattered vectors), and 1 for perfectly similar vectors. Since it measures the angle between two vectors, the interpretation is easier and is within a specific range, so it allows one intuitively to understand the similarity or dissimilarity.</li>
<li>It is fast and cheap to compute.</li>
<li>It is less sensitive to word frequency and, thus, more robust to outliers.</li>
<li>It is scale-invariant, meaning that it is not influenced by the magnitude of the vectors.</li>
<li>Being normalized, it can also be used with high-dimensional data.</li>
</ul>
<p>For 2D vectors, we can plot to observe these properties:</p>
<div><div><img alt="Figure 1.8 – Example of cosine similarity between two vectors" src="img/B21257_01_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Example of cosine similarity between two vectors</p>
<p>In the next section, we can <a id="_idIndexMarker051"/>define the properties of our trained embedding using this notion of similarity.</p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Properties of embeddings</h2>
<p>Embeddings<a id="_idIndexMarker052"/> are a surprisingly flexible method and manage to encode different syntactic and semantic properties that can both be visualized and exploited for different operations. Once<a id="_idIndexMarker053"/> we have a notion of similarity, we can search for the words that are most similar to a word <em class="italic">w</em>. Note that similarity is defined as appearing in the same context window; the model cannot differentiate synonyms and antonyms.</p>
<p>In addition, the model is also capable of representing grammatical relations such as superlatives or verb forms.</p>
<p>Another interesting relationship we can study is analogies. The parallelogram model is a system for representing analogies in a cognitive space. The classic example is <em class="italic">king:queen::man:?</em> (which in a formula would be <em class="italic">a:b::a*:?</em>). Given that we have vectors, we can turn this into an <em class="italic">a-a*+b</em> operation.</p>
<p>We can test this in Python using the embedding model we have trained:</p>
<ul>
<li>We can check the most similar words</li>
<li>We can test the analogy</li>
<li>We can then test the capacity to identify synonyms and antonyms</li>
</ul>
<p>The code for<a id="_idIndexMarker054"/> this process is as follows:</p>
<pre class="source-code">
word_1 = "good"
syn = "great"
ant = "bad"
most_sim =model.wv.most_similar("good")
print("Top 3 most similar words to {} are :{}".format(
    word_1, most_sim[:3]))
synonyms_dist = model.wv.distance(word_1, syn)
antonyms_dist = model.wv.distance(word_1, ant)
print("Synonyms {}, {} have cosine distance: {}".format(
    word_1, syn, synonyms_dist))
print("Antonyms {}, {} have cosine distance: {}".format(
    word_1, ant, antonyms_dist))
a = 'king'
a_star = 'man'
b = 'woman'
b_star= model.wv.most_similar(positive=[a, b], negative=[a_star])
print("{} is to {} as {} is to: {} ".format(
    a, a_star, b, b_star[0][0]))</pre> <p class="callout-heading">Important note</p>
<p class="callout">This is done with the embedding we have trained before. Notice how the model is not handling antonyms well.</p>
<p>The<a id="_idIndexMarker055"/> method is not exactly perfect, so sometimes the right answer is not the first result, but it could be among the first three inputs. Also, this system works for entities that are frequent within the text (city names, common words) but much less with rarer entities.</p>
<p>Embeddings can also be used as a tool to study how the meaning of a word changes over time, especially if you have text corpora that span several decades. This is demonstrated in the following diagram, which shows how the meanings of the words <em class="italic">gay</em>, <em class="italic">broadcast</em>, and <em class="italic">awful</em> have changed:</p>
<div><div><img alt="Figure 1.9 – 2D visualization of how the semantic meaning of a word changes over the years; these projections are obtained using text from different decades and embeddings (https://arxiv.org/abs/1605.09096)" src="img/B21257_01_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – 2D visualization of how the semantic meaning of a word changes over the years; these projections are obtained using text from different decades and embeddings (<a href="https://arxiv.org/abs/1605.09096">https://arxiv.org/abs/1605.09096</a>)</p>
<p>Finally, a word can still have multiple meanings! For example, common words such as “good” have more than one meaning depending on the context. One may wonder whether a vector for a word in an embedding represents only one meaning or whether it represents the set of meanings of a word. Fortunately, embedding vectors represent a weighted sum of the various meanings of a word (linear superposition). The weights of each meaning are proportional to the frequency of that meaning in the text. Although these<a id="_idIndexMarker056"/> meanings reside in the same vector, when we add or subtract during the calculation of analogies, we are working with these components. For example, “apple” is both a fruit and the name of a company; if we conduct the operation <em class="italic">apple:red::banana:?</em>, we are subtracting only a very specific semantic component from the apple vector (the component that is similar to red). This flexibility can be useful when we want to disambiguate meanings. Also, since the vector space is sparse, by exploiting sparse coding, we can separate the various meanings:</p>
<div><div><img alt="Figure 1.10 – Table showing how a vector in word2vec is encoding for different meanings at the same time (https://aclanthology.org/Q18-1034/)" src="img/B21257_01_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Table showing how a vector in word2vec is encoding for different meanings at the same time (<a href="https://aclanthology.org/Q18-1034/">https://aclanthology.org/Q18-1034/</a>)</p>
<p>These vectors are now providing contextual and semantic meaning for each word in the text. We can use this rich source of information for tasks such as text classification. What we need now are models that can handle the sequential nature of text, which we will learn about in the next section.</p>
<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>RNNs, LSTMs, GRUs, and CNNs for text</h1>
<p>So far, we have discussed how to represent text in a way that is digestible for the model; in this section, we will discuss how to analyze the text once a representation has been obtained. Traditionally, once we obtained a representation of the text, it was fed to models such as naïve Bayes or even algorithms such as logistic regression. The success of neural networks has made these machine learning algorithms outdated. In this section, we will discuss deep learning models that can be used for various tasks.</p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>RNNs</h2>
<p>The <a id="_idIndexMarker057"/>problem with classical neural networks is that they have no memory. This is especially problematic for time series and text inputs. In a sequence of words <em class="italic">t</em>, the word <em class="italic">w</em> at time <em class="italic">t</em> depends on the <em class="italic">w</em> at time <em class="italic">t-1</em>. In fact, in a sentence, the last word is often dependent on several words in the sentence. Therefore, we want an NN model that maintains a memory of previous inputs. An <strong class="bold">RNN</strong> maintains an internal state that maintains this memory; that is, it stores information about previous inputs, and the outputs it produces are affected by previous inputs. These networks perform the same operation on all elements of the sequence (hence recurrent) and maintain the memory of this operation:</p>
<div><div><img alt="Figure 1.11 – Simple example of an RNN (https://arxiv.org/pdf/1506.00019)" src="img/B21257_01_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – Simple example of an RNN (<a href="https://arxiv.org/pdf/1506.00019">https://arxiv.org/pdf/1506.00019</a>)</p>
<p>A <a id="_idIndexMarker058"/>classical neural network (<strong class="bold">feedforward neural network</strong>) considers inputs to be independent, and one layer of a neural network performs the following operation for a vector representing the element at time <em class="italic">t</em>:</p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">W</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mo>)</mml:mo></mml:math></p>
<p>However, in a simple RNN, the following operations are conducted:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><msup><mi mathvariant="bold-italic">a</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="bold-italic">b</mi><mo>+</mo><mi mathvariant="bold-italic">U</mi><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mi mathvariant="bold-italic">W</mi><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup></mrow></mrow></math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">V</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math></p>
<p>These operations may seem complicated, but in fact, we are simply maintaining a hidden state that considers the previous iterations. The first equation is a normal feedforward layer modified, in which we multiply the previously hidden state <em class="italic">h</em> by a set of weights <em class="italic">U</em>. This matrix <em class="italic">U</em> allows us to control how the neural network uses the previous context to bind input and past inputs (how the past influences the output for input at time <em class="italic">t</em>). In the second equation, we create a new hidden state that will then be used for <a id="_idIndexMarker059"/>subsequent computations but also for the next input. In the third equation, we are creating the output; we use a bias vector and a matrix to compute the output. In the last equation, it is simply passed as a nonlinearity function.</p>
<p>These RNNs can be seen as unrolled entities throughout time, in which we can represent the network and its computations throughout the sequence:</p>
<div><div><img alt="Figure 1.12 – Simple example of an RNN unrolled through the sequence (https://arxiv.org/pdf/1506.00019)" src="img/B21257_01_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – Simple example of an RNN unrolled through the sequence (<a href="https://arxiv.org/pdf/1506.00019">https://arxiv.org/pdf/1506.00019</a>)</p>
<p>We can test in Python with a PyTorch RNN to see how it is transforming the data:</p>
<pre class="source-code">
array = np.random.random((10, 5, 3))
# Convert the numpy array to a PyTorch tensor
data_tensor = torch.tensor(array, dtype=torch.float32)
RNN = nn.RNN(input_size=3, hidden_size=10,
             num_layers=1, batch_first=True)
output, hidden = RNN(data_tensor)
output.shape</pre> <p class="callout-heading">Important note</p>
<p class="callout">Notice how the model is transforming the data; we can also access the hidden state.</p>
<p>We can see <a id="_idIndexMarker060"/>several interesting things:</p>
<ul>
<li>The RNN is not limited by the size of the input; it is a cyclic operation that is conducted over the entire sequence. RNNs basically process one word at a time. This cyclicality also means backpropagation is conducted for each time step. Although this model works well for series analysis, its sequential nature does not allow it to be parallelized.</li>
<li>Theoretically, this model could be trained with infinite sequences of words; theoretically, after a few time steps, it begins to forget the initial inputs.</li>
<li>Training can become inefficient due to the vanishing gradient problem, where gradients must propagate from the final cell back to the initial one. During this process, they can shrink exponentially and approach zero, making it difficult for the model to learn long-range dependencies. Conversely, the exploding gradient problem can also occur, where gradients grow uncontrollably large, leading to unstable training.</li>
</ul>
<p>RNNs are not the only form of deep learning models that are relevant to this topic.</p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>LSTMs</h2>
<p>In theory, RNNs should <a id="_idIndexMarker061"/>be able to process long sequences and remember the initial input. However, in reality, the information inside the hidden state is local rather than global, and for a time <em class="italic">t</em>, it considers only the previous time steps and not the entire sequence. The main problem with such a simple model is that the hidden state must simultaneously fulfill two roles: provide information relevant to the output at time <em class="italic">t</em> and store memory for future decisions.</p>
<p>An <strong class="bold">LSTM</strong> is an<a id="_idIndexMarker062"/> extension of RNNs, designed with the idea that the model can forget information that is not important and keep only the important context. </p>
<div><div><img alt="Figure 1.13 – Internal structure of an LSTM cell (https://arxiv.org/pdf/2304.11461)" src="img/B21257_01_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – Internal structure of an LSTM cell (<a href="https://arxiv.org/pdf/2304.11461">https://arxiv.org/pdf/2304.11461</a>)</p>
<p>An LSTM has internal mechanisms to control the information (gates) within the layer; additionally, it has a dedicated context layer. So, we have two hidden states in which the first, <em class="italic">h</em>, serves for information at time <em class="italic">t</em> (short memory) and the other, <em class="italic">c</em>, for information at long term. The gates can be open (1) or closed (0); this is achieved by a feed-forward layer with sigmoid activation to squeeze values between zero and one. After that, we <a id="_idIndexMarker063"/>use the <strong class="bold">Hadamard product</strong> (or pointwise multiplication) for the gating mechanism of a layer. This multiplication acts as a binary gate, allowing information to pass if the value is close to 1 or blocking it when the value is close to 0. These gates allow a dynamic system in which during a time step, we decide how much information we preserve and how much we forget.</p>
<p>The first gate is <a id="_idIndexMarker064"/>called the <strong class="bold">forget gate</strong> because it is used to forget information that is no longer needed from the context and, therefore, will no longer be needed in the next time step. So, we will use the output of the forget gate to multiply the context. At this time, we extract both information from the input and the previous time step’s hidden state. Each gate has a set of gate-specific <em class="italic">U</em> weights:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">f</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">f</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">f</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold-italic">f</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></p>
<p>The next step is to extract information from the input and decide which of that information will be added to the context. This is <a id="_idIndexMarker065"/>controlled by an <strong class="bold">input gate</strong> <em class="italic">i</em> that controls how much information will then be added. The context is then obtained by the sum of what we add and what we forget:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">g</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><msub><mrow><mi>t</mi><mi>a</mi><mi>n</mi><mi>h</mi><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">g</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">g</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">i</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">i</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">i</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">j</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></p>
<p>The<a id="_idIndexMarker066"/> final step is the calculation of the output; this is achieved with a final gate. The output or final layer decision is also used to update the hidden state:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">o</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">o</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">o</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><mml:math display="block"><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold-italic">o</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>⁡</mml:mo><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">c</mml:mi></mml:mrow><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msup></mml:math></p>
<p>These gates are independent of each other so that efficient implementations of the LSTM can parallelize them. We can test in Python with a PyTorch RNN to see how it is transforming the data:</p>
<pre class="source-code">
data_tensor = torch.tensor(np.random.random((10, 5, 3)),
                           dtype=torch.float32)
LSTM =nn.LSTM(input_size=3, hidden_size=10,
              num_layers=1, batch_first=True)
output, (hidden, cell) = LSTM(data_tensor)
output.shape</pre> <p class="callout-heading">Important note</p>
<p class="callout">Notice how the model is transforming the data; we can access the hidden state as well as the cell state.</p>
<p>We can also note some other interesting properties:</p>
<ul>
<li>Computation augmentation is internal to layers, which means we can easily substitute LSTMs for RNNs.</li>
<li>The LSTM <a id="_idIndexMarker067"/>manages to preserve information for a long time because it retains only the relevant part of the information and forgets what is not needed.</li>
<li>Standard practice is to initialize an LSTM with vector 1 (preserves everything), after which it learns by itself what to forget and what to add.</li>
<li>An LSTM, as opposed to an RNN, can remember up to 100 time steps (the RNN after 7 time steps starts forgetting). The plus operation makes vanishing or exploding gradients less likely.</li>
</ul>
<p>Let’s look at another model option that is computationally lighter but still has this notion of context vector.</p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>GRUs</h2>
<p><strong class="bold">GRUs</strong> are <a id="_idIndexMarker068"/>another variant of RNNs to solve the vanishing gradient problem, thus making them more effective in remembering information. They are very similar to LSTMs since they have internal gates, but they are much simpler and lighter. Despite having fewer parameters, GRUs can converge faster than LSTMs and still achieve comparable performance. GRUs exploit some of the elements that have made LSTMs so effective: the plus operation, the Hadamard product, the presence of a context, and the control of information within the layer:</p>
<div><div><img alt="Figure 1.14 – Internal structure of a GRU cell (https://arxiv.org/pdf/2304.11461)" src="img/B21257_01_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – Internal structure of a GRU cell (<a href="https://arxiv.org/pdf/2304.11461">https://arxiv.org/pdf/2304.11461</a>)</p>
<p>In a GRU, the <a id="_idIndexMarker069"/>forget gate is <a id="_idIndexMarker070"/>called the <strong class="bold">update gate</strong>, but it has the same purpose: important information is retained (values near 1) and unimportant information is rewritten during the update (values near 0). In a GRU, the input gate is called<a id="_idIndexMarker071"/> the <strong class="bold">reset gate</strong> and is not independent as in an LSTM, but connected to the update gate.</p>
<p>The first step is the update gate <em class="italic">z</em>, which is practically the same as the forget gate in an LSTM. At the same time, we calculate the reset gate <em class="italic">r</em>:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">z</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi>σ</mi><msub><mrow><mo>(</mo><mi mathvariant="bold-italic">b</mi></mrow><mi mathvariant="bold-italic">r</mi></msub><mo>+</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">r</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p>The next step is to update the hidden state; this depends on the reset gate. In this way, we decide what new information is put into the hidden state and what relevant information from the past is saved. This <a id="_idIndexMarker072"/>is called the <strong class="bold">current </strong><strong class="bold">memory gate</strong>:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><mi mathvariant="normal">t</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">n</mi><mi mathvariant="normal">h</mi><mo>(</mo><msub><mi mathvariant="bold-italic">W</mi><mi mathvariant="bold-italic">h</mi></msub><msup><mi mathvariant="bold-italic">x</mi><mi>t</mi></msup><mo>+</mo><msup><mi mathvariant="bold-italic">r</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msub><mi mathvariant="bold-italic">U</mi><mi mathvariant="bold-italic">z</mi></msub><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>)</mo></mrow></mrow></mrow></math></p>
<p>At this point, we have the final update of the hidden state in which we also use the update gate:</p>
<p class="IMG---Figure"><math display="block"><mrow><mrow><mrow><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>=</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>⊙</mo><msup><mi mathvariant="bold-italic">h</mi><mfenced close=")" open="("><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></mfenced></msup><mo>+</mo><mo>(</mo><mn>1</mn><mo>−</mo><msup><mi mathvariant="bold-italic">z</mi><mfenced close=")" open="("><mi>t</mi></mfenced></msup><mo>)</mo><mo>⊙</mo><msup><mover><mi mathvariant="bold-italic">h</mi><mo stretchy="true">‾</mo></mover><mfenced close=")" open="("><mi>t</mi></mfenced></msup></mrow></mrow></mrow></math></p>
<p>We can<a id="_idIndexMarker073"/> test in Python with a PyTorch RNN to see how it is transforming the data:</p>
<pre class="source-code">
data_tensor = torch.tensor(np.random.random((10, 5, 3)),
                           dtype=torch.float32)
GRU =nn.GRU(input_size=3, hidden_size=10,
            num_layers=1, batch_first=True)
output, hidden = GRU(data_tensor)
output.shape</pre> <p class="callout-heading">Important note</p>
<p class="callout">Notice how the model is transforming the data; we can also access the hidden state.</p>
<p>We can see some interesting elements here as well:</p>
<ul>
<li>GRU networks are similar to LSTM networks, but they have the advantage of fewer parameters and are computationally more efficient. This means, though, they are more prone to overfitting.</li>
<li>They can handle long sequences of data without forgetting previous inputs. For many textual tasks (but also speech recognition and music generation) they perform quite well, though they are less efficient than LSTMs when it comes to modeling long-term dependencies or complex patterns.</li>
</ul>
<p>Next, we’ll look at CNNs.</p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>CNNs for text</h2>
<p><strong class="bold">CNNs</strong> are<a id="_idIndexMarker074"/> designed to find patterns in images (or other 2D matrixes) by running a filter (a matrix or kernel) along them. The convolution is conducted pixel by pixel, and the filter values are multiplied by the pixels in the image and then summed. During training, a weight is learned for each of the filter entries. For each filter, we get a different scan of the image that can be visualized; this is called a feature map.</p>
<p>Convolutional networks have been successful <a id="_idIndexMarker075"/>in <strong class="bold">computer vision</strong> because of their ability to extract local information and recognize complex patterns. For <a id="_idIndexMarker076"/>this reason, convolutional networks have been proposed for sequences. In this case, 1-dimensional convolutional networks are exploited, but the idea is the same. In fact, on a sequence, 1D convolution is used to extract a feature map (instead of being a 2-dimensional filter or matrix, we have a uni-dimensional filter that can be seen as the context window of word2vec):</p>
<div><div><img alt="Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence" src="img/B21257_01_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – In 1D convolution, we are sliding a 1D filter over the sequence</p>
<p>In the preceding figure, we scroll a uni-dimensional filter over the sequence; the process is very fast, and the filter can have an arbitrary size (three to seven words or even more). The model tries to learn patterns among the various words found within this kernel. It can also be used on vectors previously obtained from an embedding, and we can also use multiple kernels (so as to learn different patterns for each sequence). As with image CNNs, we can add operations such as max pooling to extract the most important features.</p>
<p>We can<a id="_idIndexMarker077"/> test in Python with a PyTorch RNN to see how it is transforming the data:</p>
<pre class="source-code">
data_tensor = torch.tensor(np.random.random((10, 5, 3)),
                           dtype=torch.float32)
Conv1d = nn.Conv1d(in_channels=5, out_channels=16,
                   kernel_size=3, stride=1, padding=1)
output = Conv1d(data_tensor)
output.shape</pre> <p class="callout-heading">Important note</p>
<p class="callout">Notice how the model is transforming the data and how this is different from what we have seen before.</p>
<p>Now that we have a method to transform text into numerical representation (while preserving the contextual information) and models that can handle this representation, we can combine them to obtain an end-to-end system.</p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Performing sentiment analysis with embedding and deep learning</h1>
<p>In this section, we<a id="_idIndexMarker078"/> will train a model for conducting sentiment analysis on movie reviews. The model we will train will be able to classify reviews as positive or negative. To build and train the model, we will exploit the elements we have encountered so far. In brief, we’re doing the following:</p>
<ul>
<li>We are preprocessing the dataset, transforming in numerical vectors, and harmonizing the vectors</li>
<li>We are defining a neural network with an embedding and training it</li>
</ul>
<p>The dataset consists of 50,000 positive and negative reviews. We can see that it contains a heterogeneous length for reviews and that on average, there are 230 words:</p>
<div><div><img alt="Figure 1.16 – Graphs showing the distribution of the length of the review in the text; the left plot is for positive reviews, while the right plot is for negative reviews" src="img/B21257_01_16.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – Graphs showing the distribution of the length of the review in the text; the left plot is for positive reviews, while the right plot is for negative reviews</p>
<p>In <a id="_idIndexMarker079"/>addition, the most prevalent words are, obviously, “<em class="italic">movie</em>” and “<em class="italic">film</em>”:</p>
<div><div><img alt="Figure 1.17 – Word cloud for the most frequent words in positive (left plot) and negative (right plot) reviews" src="img/B21257_01_17.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – Word cloud for the most frequent words in positive (left plot) and negative (right plot) reviews</p>
<p>The text is messy and must be cleaned before the model can be trained. The first step is binary encoding of the label (“positive” equals 0, “negative” equals 1). After that, we divide the features and the<a id="_idIndexMarker080"/> labels (for a dataset in <code>X</code> are the features and <code>y</code> are the labels). Next, we create three balanced datasets for training, validation, and testing:</p>
<pre class="source-code">
df['sentiment_encoded'] = np.where(
    df['sentiment']=='positive',0,1)
X,y = df['review'].values, df['sentiment_encoded'].values
x_train,x_test,y_train,y_test = train_test_split(
    X,y,stratify=y, test_size=.2)
x_train,x_val,y_train,y_val = train_test_split(
    x_train,y_train,stratify=y_train, test_size=.1)
y_train, y_val, y_test = np.array(y_train), np.array(y_val), \
    np.array(y_test)</pre> <p>A few <a id="_idIndexMarker081"/>steps are necessary before proceeding with the training:</p>
<ul>
<li>A <strong class="bold">preprocessing</strong> step <a id="_idIndexMarker082"/>in which we remove excessive spaces, special characters, and punctuation.</li>
<li>A <strong class="bold">tokenization</strong> step <a id="_idIndexMarker083"/>in which we convert the various reviews into tokens. In this step, we also remove stopwords and single-character words. We extract for each review only the 1,000 most popular words (this step is only to reduce computation time during training).</li>
<li>Transformation of the the words into <a id="_idIndexMarker084"/>indices (<strong class="bold">vectorization</strong>) according to our vocabulary to make the model work with numerical values.</li>
<li>Since the reviews have different lengths, we apply padding to harmonize the length of the review to a fixed number (we need this for the training).</li>
</ul>
<p>These preprocessing steps depend on the dataset. The code is in the GitHub repository. Note, however, that the tokenization and preprocessing choices alter the properties of the reviews – in this case, the summary statistics.</p>
<div><div><img alt="Figure 1.18 – Graph showing the distribution of review length after tokenization" src="img/B21257_01_18.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – Graph showing the distribution of review length after tokenization</p>
<p>We <a id="_idIndexMarker085"/>are defining the model with its hyperparameters. In this case, we are training a neural network to predict sentiment data composed of embeddings and GRUs. To make the training more stable, we add regularization (dropout). The linear layer is to map these features that we extracted to a single representation. We use this representation to calculate the probability that the review is positive or negative:</p>
<pre class="source-code">
# Hyperparameters
no_layers = 3
vocab_size = len(vocab) + 1  # extra 1 for padding
embedding_dim = 300
output_dim = 1
hidden_dim = 256
# Initialize the model
model = SentimentRNN(no_layers, vocab_size, hidden_dim,
                     embedding_dim, drop_prob=0.5)</pre> <p>Note that in this case, we use binary cross-entropy loss because we have only two categories (positive and negative). Also, we use <code>Adam</code> as an optimizer, but one can test others. In <a id="_idIndexMarker086"/>this case, we conduct batch training since we have thousands of reviews:</p>
<pre class="source-code">
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
epoch_tr_loss, epoch_vl_loss = [], []
epoch_tr_acc, epoch_vl_acc = [], []
for epoch in range(epochs):
    train_losses = []
    train_acc = 0.0
    model.train()
    h = model.init_hidden(50)
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        h = h.data
        model.zero_grad()
        output, h = model(inputs, h)
        loss = criterion(output.squeeze(), labels.float())
        loss.backward()
        train_losses.append(loss.item())
        accuracy = acc(output, labels)
        train_acc += accuracy
        optimizer.step()</pre> <p>The following graph displays the accuracy and loss for the training and validation sets:</p>
<div><div><img alt="Figure 1.19 – Training curves for training and validation set, for accuracy and loss" src="img/B21257_01_19.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Training curves for training and validation set, for accuracy and loss</p>
<p>The <a id="_idIndexMarker087"/>model achieves good accuracy, as we can see from the following confusion matrix:</p>
<div><div><img alt="Figure 1.20 – Confusion matrix for the test set" src="img/B21257_01_20.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – Confusion matrix for the test set</p>
<p>In addition, if we look at the projection of reviews before and after the training, we can see that the model has learned how to separate positive and negative reviews:</p>
<div><div><img alt="Figure 1.21 – Embedding projection obtained from the model before (left plot) and after (right plot) training" src="img/B21257_01_21.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – Embedding projection obtained from the model before (left plot) and after (right plot) training</p>
<p>We have<a id="_idIndexMarker088"/> now trained a model that can take a review in plain text and classify it as positive or negative. We did that by combining the elements we saw previously in the chapter. The same approach can be followed with any other dataset; that is the power of deep learning.</p>
<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Summary</h1>
<p>In this chapter, we saw how to transform text to an increasingly complex vector representation. This numerical representation of text allowed us to be able to use machine learning models. We saw how to preserve the contextual information (word embedding) of a text and how this can then be used for later analysis (for example, searching synonyms or clustering words). In addition, we saw how neural networks (RNNs, LSTM, GRUs) can be used to analyze text and perform tasks (for example, sentiment analysis).</p>
<p>In the next chapter, we will see how to solve some of the remaining unsolved challenges and see how this will lead to the natural evolution of the models seen here.</p>
</div>
</body></html>