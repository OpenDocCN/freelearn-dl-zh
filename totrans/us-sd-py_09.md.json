["```py\n    # initialize model\n    ```", "```py\n    from diffusers import StableDiffusionPipeline\n    ```", "```py\n    import torch\n    ```", "```py\n    model_id = \"stablediffusionapi/deliberate-v2\"\n    ```", "```py\n    pipe = StableDiffusionPipeline.from_pretrained(\n    ```", "```py\n        model_id,\n    ```", "```py\n        torch_dtype=torch.float16\n    ```", "```py\n    ).to(\"cuda\")\n    ```", "```py\n    # without using TI\n    ```", "```py\n    prompt = \"a high quality photo of a futuristic city in deep \\ \n    ```", "```py\n    space, midjourney-style\"\n    ```", "```py\n    image = pipe(\n    ```", "```py\n        prompt,\n    ```", "```py\n        num_inference_steps = 50,\n    ```", "```py\n        generator = torch.Generator(\"cuda\").manual_seed(1)\n    ```", "```py\n    ).images[0]\n    ```", "```py\n    image\n    ```", "```py\n    pipe.load_textual_inversion(\n    ```", "```py\n        \"sd-concepts-library/midjourney-style\",\n    ```", "```py\n        token = \"midjourney-style\"\n    ```", "```py\n    )\n    ```", "```py\npipe.load_textual_inversion(\n    \"sd-concepts-library/midjourney-style\",\n    token = \"colorful-magic-style\"\n)\n```", "```py\n# load a pt TI\nimport torch\nloaded_learned_embeds = torch.load(\"badhandsv5-neg.pt\",\n    map_location=\"cpu\")\nkeys = list(loaded_learned_embeds.keys())\nfor key in keys:\n    print(key,\":\",loaded_learned_embeds[key])\n```", "```py\nstring_to_token : {'*': 265}\nstring_to_param : {'*': tensor([[ 0.0399, -0.2473,  0.1252,  ...,  0.0455,  0.0845, -0.1463],\n        [-0.1385, -0.0922, -0.0481,  ...,  0.1766, -0.1868,  0.3851]],\n       requires_grad=True)}\nname : bad-hands-5\nstep : 1364\nsd_checkpoint : 7ab762a7\nsd_checkpoint_name : blossom-extract\n```", "```py\nstring_to_token = loaded_learned_embeds['string_to_token']\nstring_to_param = loaded_learned_embeds['string_to_param']\n# separate token and the embeds\ntrained_token = list(string_to_token.keys())[0]\nembeds = string_to_param[trained_token]\n```", "```py\nimport torch\nloaded_learned_embeds = torch.load(\"midjourney_style.bin\",\n    map_location=\"cpu\")\nkeys = list(loaded_learned_embeds.keys())\nfor key in keys:\n    print(key,\":\",loaded_learned_embeds[key])\n```", "```py\n<midjourney-style> : tensor([-5.9785e-02, -3.8523e-02,  5.1913e-02,  8.0925e-03, -6.2018e-02,\n         1.3361e-01,  1.3679e-01,  8.2224e-02, -2.0598e-01,  1.8543e-02,\n         1.9180e-01, -1.5537e-01, -1.5216e-01, -1.2607e-01, -1.9420e-01,\n         1.0445e-01,  1.6942e-01,  4.2150e-02, -2.7406e-01,  1.8115e-01,\n...\n])\n```", "```py\nkeys = list(loaded_learned_embeds.keys())\nembeds =  loaded_learned_embeds[keys[0]] * weight\n```", "```py\n    def load_textual_inversion(\n    ```", "```py\n        learned_embeds_path,\n    ```", "```py\n        token,\n    ```", "```py\n        text_encoder,\n    ```", "```py\n        tokenizer,\n    ```", "```py\n        weight = 0.5,\n    ```", "```py\n        device = \"cpu\"\n    ```", "```py\n    ):\n    ```", "```py\n        loaded_learned_embeds = \\\n    ```", "```py\n            torch.load(learned_embeds_path, map_location=device)\n    ```", "```py\n        if \"string_to_token\" in loaded_learned_embeds:\n    ```", "```py\n            string_to_token = \\\n    ```", "```py\n                loaded_learned_embeds['string_to_token']\n    ```", "```py\n            string_to_param = \\\n    ```", "```py\n                loaded_learned_embeds['string_to_param']\n    ```", "```py\n            # separate token and the embeds\n    ```", "```py\n            trained_token = list(string_to_token.keys())[0]\n    ```", "```py\n            embeds = string_to_param[trained_token]\n    ```", "```py\n            embeds = embeds[0] * weight\n    ```", "```py\n        elif \"emb_params\" in loaded_learned_embeds:\n    ```", "```py\n            embeds = loaded_learned_embeds[\"emb_params\"][0] * weight\n    ```", "```py\n        else:\n    ```", "```py\n            keys = list(loaded_learned_embeds.keys())\n    ```", "```py\n            embeds =  loaded_learned_embeds[keys[0]] * weight\n    ```", "```py\n        # ...\n    ```", "```py\n    dtype = text_encoder.get_input_embeddings().weight.dtype\n    ```", "```py\n    embeds.to(dtype)\n    ```", "```py\n    token = token if token is not None else trained_token\n    ```", "```py\n    num_added_tokens = tokenizer.add_tokens(token)\n    ```", "```py\n    if num_added_tokens == 0:\n    ```", "```py\n        raise ValueError(\n    ```", "```py\n            f\"\"\"The tokenizer already contains the token {token}.\n    ```", "```py\n            Please pass a different `token` that is not already in \n    ```", "```py\n            the tokenizer.\"\"\"\n    ```", "```py\n        )\n    ```", "```py\n# resize the token embeddings\ntext_encoder.resize_token_embeddings(len(tokenizer))\n# get the id for the token and assign the embeds\ntoken_id = tokenizer.convert_tokens_to_ids(token)\ntext_encoder.get_input_embeddings().weight.data[token_id] = embeds\n```", "```py\ndef load_textual_inversion(\n    learned_embeds_path,\n    token,\n    text_encoder,\n    tokenizer,\n    weight = 0.5,\n    device = \"cpu\"\n):\n    loaded_learned_embeds = torch.load(learned_embeds_path, \n        map_location=device)\n    if \"string_to_token\" in loaded_learned_embeds:\n        string_to_token = loaded_learned_embeds['string_to_token']\n        string_to_param = loaded_learned_embeds['string_to_param']\n        # separate token and the embeds\n        trained_token = list(string_to_token.keys())[0]\n        embeds = string_to_param[trained_token]\n        embeds = embeds[0] * weight\n    elif \"emb_params\" in loaded_learned_embeds:\n        embeds = loaded_learned_embeds[\"emb_params\"][0] * weight\n    else:\n        keys = list(loaded_learned_embeds.keys())\n        embeds =  loaded_learned_embeds[keys[0]] * weight\n    # cast to dtype of text_encoder\n    dtype = text_encoder.get_input_embeddings().weight.dtype\n    embeds.to(dtype)\n    # add the token in tokenizer\n    token = token if token is not None else trained_token\n    num_added_tokens = tokenizer.add_tokens(token)\n    if num_added_tokens == 0:\n        raise ValueError(\n            f\"\"\"The tokenizer already contains the token {token}.\n            Please pass a different `token` that is not already in the \n            tokenizer.\"\"\"\n        )\n    # resize the token embeddings\n    text_encoder.resize_token_embeddings(len(tokenizer))\n    # get the id for the token and assign the embeds\n    token_id = tokenizer.convert_tokens_to_ids(token)\n    text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n    return (tokenizer,text_encoder)\n```", "```py\ntext_encoder = pipe.text_encoder\ntokenizer = pipe.tokenizer\n```", "```py\nload_textual_inversion(\n    learned_embeds_path = \"learned_embeds.bin\",\n    token = \"colorful-magic-style\",\n    text_encoder = text_encoder,\n    tokenizer = tokenizer,\n    weight = 0.5,\n    device = \"cuda\"\n)\n```", "```py\nprompt = \"a high quality photo of a futuristic city in deep space, colorful-magic-style\"\nimage = pipe(\n    prompt,\n    num_inference_steps = 50,\n    generator = torch.Generator(\"cuda\").manual_seed(1)\n).images[0]\nimage\n```"]