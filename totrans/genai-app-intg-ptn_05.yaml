- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: 'Integration Pattern: Batch Metadata Extraction'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成模式：批量元数据提取
- en: In this chapter, we will explore a metadata extraction use case, which serves
    as an excellent entry point to understand the capabilities of **Generative Artificial
    Intelligence** (**GenAI**). This topic is particularly relevant across industries
    and thought-provoking.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一个元数据提取用例，这作为一个理解**生成式人工智能**（**GenAI**）能力的绝佳切入点。这个主题在各个行业中都特别相关，并且富有启发性。
- en: To illustrate this use case, let us consider a scenario where we work with a
    financial services company that requires the extraction of data from a 10-K report.
    These reports, filed annually with the **Securities and Exchange Commission**
    (**SEC**) by publicly traded companies, provide a comprehensive overview of their
    financial performance, operations, and significant events. They are extensive
    documents that are over 100 pages long and contain a wealth of information, structured
    across different sections across different data modalities (tables, text, etc.).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个用例，让我们考虑一个场景，其中我们与一家金融服务公司合作，需要从10-K报告中提取数据。这些报告由上市公司每年提交给**证券交易委员会**（**SEC**），提供了他们财务表现、运营和重大事件的全面概述。这些是超过100页的广泛文件，包含大量信息，结构化地分布在不同的数据模式（表格、文本等）中。
- en: In this chapter, our objective is to identify the specific dataset and critical
    data points that need to be extracted from this vast repository of information.
    This process requires a systematic approach to pinpoint the desired data points
    with precision.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们的目标是确定需要从这庞大的信息库中提取的具体数据集和关键数据点。这个过程需要一种系统性的方法，以精确地定位所需的数据点。
- en: Once the relevant data points have been identified, we will determine the appropriate
    storage location for this extracted data. This could involve a centralized database
    accessible to authorized individuals within the organization, or a cloud-based
    repository that facilitates seamless access and collaboration across teams and
    locations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了相关数据点，我们将确定这些提取数据的适当存储位置。这可能涉及一个组织内部授权人员可访问的集中式数据库，或者一个基于云的存储库，它促进了团队和地点之间的无缝访问和协作。
- en: Regardless of the chosen storage solution, GenAI will play a pivotal role throughout
    this endeavor. With its ability to understand and process natural language, GenAI
    will prove invaluable in navigating the intricate structure of financial reports,
    extracting the essential data points with remarkable efficiency and accuracy.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 不论选择哪种存储解决方案，GenAI将在整个过程中发挥关键作用。凭借其理解和处理自然语言的能力，GenAI将在导航金融报告复杂结构、以显著效率和准确性提取关键数据点方面证明其价值。
- en: 'We will cover the following main topics in this chapter:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要主题：
- en: '**Use case definition**: We will describe an example scenario of extracting
    metadata from a 10-K report for a financial services company, explaining the structure
    and importance of these reports.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用例定义**：我们将描述一个从金融服务公司的10-K报告中提取元数据的示例场景，解释这些报告的结构和重要性。'
- en: '**Architecture**: We will outline a cloud-based, serverless architecture using
    Google Cloud services to process 10-K reports, including storage, messaging, processing,
    and database components.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：我们将概述一个基于云的无服务器架构，使用Google Cloud服务处理10-K报告，包括存储、消息传递、处理和数据库组件。'
- en: '**Entry point**: We will explain how the batch processing pipeline is triggered,
    using Google Cloud Storage and Cloud Functions to initiate the content extraction
    process.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切入点**：我们将解释如何通过Google Cloud Storage和Cloud Functions触发批量处理管道，以启动内容提取过程。'
- en: '**Prompt pre-processing**: We will detail the creation of an initial prompt
    for the AI model, leveraging SEC guidance to identify key data points for extraction
    from 10-K reports.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提示预处理**：我们将详细介绍为AI模型创建初始提示的过程，利用SEC指南来识别从10-K报告中提取的关键数据点。'
- en: '**Inference**: We will discuss the submission of the prompt to Gemini Pro 1.5
    via Vertex AI, showcasing how the model processes and extracts information from
    the 10-K report.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理**：我们将讨论通过Vertex AI将提示提交给Gemini Pro 1.5，展示模型如何处理和从10-K报告中提取信息。'
- en: '**Result post processing**: We will cover parsing the JSON output from the
    **large language model** (**LLM**) and the strategies to ingest the extracted
    data into databases, considering both relational and document database options.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果后处理**：我们将介绍解析来自**大型语言模型**（**LLM**）的JSON输出以及将提取的数据导入数据库的策略，考虑到关系型数据库和文档数据库选项。'
- en: '**Result presentation**: We will consider how to present the extracted data,
    including the use of business intelligence tools, data visualization platforms,
    and custom applications.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果展示**：我们将考虑如何展示提取的数据，包括使用商业智能工具、数据可视化平台和定制应用程序。'
- en: '**Code sample**: We will provide a practical implementation of the metadata
    extraction process, including setup, prompt creation, inference, and result handling.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码示例**：我们将提供一个元数据提取过程的实际实现，包括设置、提示创建、推理和结果处理。'
- en: Use case definition
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用例定义
- en: Extracting metadata from 10-K reports holds significant value for financial
    services companies and other stakeholders. These reports, mandated by the SEC,
    are a treasure trove of information that can provide valuable insights into a
    company’s financial health, operational performance, and strategic direction.
    However, the sheer volume, complexity of these documents, and lack of consistency
    across the way companies build their reports can make it challenging to manually
    extract and analyze the relevant data points.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从10-K报告中提取元数据对金融服务公司和其他利益相关者具有重大价值。这些由SEC规定的报告是信息宝库，可以提供有关公司财务状况、运营绩效和战略方向的宝贵见解。然而，这些文件的庞大体积、复杂性和公司在构建报告方式上的不一致性，使得手动提取和分析相关数据点变得具有挑战性。
- en: Typical 10-K reports follow a standardized structure, comprising multiple sections
    that cover various aspects of a company’s operations. These sections may include
    a business overview, risk factors, management’s discussion and analysis, financial
    statements, and disclosures about corporate governance, among others. While the
    structure is consistent across companies, the specific data points and their presentation
    can vary, making it difficult to establish a one-size-fits-all approach to data
    extraction.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的10-K报告遵循标准化的结构，包含多个部分，涵盖公司运营的各个方面。这些部分可能包括业务概述、风险因素、管理层讨论与分析、财务报表以及关于公司治理的披露等。虽然结构在公司之间是一致的，但具体的数据点和它们的呈现方式可能有所不同，这使得建立一种适用于所有情况的数据提取方法变得困难。
- en: 'The 10-K includes five distinct sections:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 10-K报告包括五个不同的部分：
- en: '**Business**: This section provides an overview of the company’s main operations,
    including its products and services, key markets, competitive landscape, and other
    relevant details about its business model and operations.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**业务**：本节概述了公司的主要运营，包括其产品和服务、关键市场、竞争格局以及与其商业模式和运营相关的其他细节。'
- en: '**Risk factors**: In this section, the company outlines and discusses any,
    and all, risks it faces. This includes operational, financial, legal, regulatory,
    and industry-specific risks that could potentially impact its performance or future
    prospects.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**风险因素**：在这一节中，公司概述并讨论了它面临的所有风险。这包括可能影响其业绩或未来前景的运营、财务、法律、监管和行业特定风险。'
- en: '**Selected financial data**: This section presents specific financial information
    about the company over the last five years, typically including key metrics such
    as revenue, net income, earnings per share, and other relevant financial data.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选定的财务数据**：本节展示了公司过去五年的具体财务信息，通常包括关键指标，如收入、净收入、每股收益和其他相关财务数据。'
- en: '**Management’s Discussion and Analysis** (**MD&A**): The fourth section offers
    senior management’s explanation and analysis of the company’s financial results,
    including a detailed discussion of the factors that influenced its performance,
    future strategies, and potential opportunities and challenges.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理层讨论与分析**（**MD&A**）：第四部分提供了高级管理层对公司财务结果的解释和分析，包括对影响其业绩的因素、未来战略、潜在的机会和挑战的详细讨论。'
- en: '**Financial statements and supplementary data**: The final section furnishes
    the audited financial statements, including the income statement, balance sheets,
    statement of cash flows, and accompanying notes and disclosures. This section
    provides a comprehensive and detailed picture of the company’s financial position
    and performance during the reporting period.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**财务报表和补充数据**：最后一部分提供了经审计的财务报表，包括利润表、资产负债表、现金流量表以及相关的注释和披露。本部分提供了公司报告期内财务状况和业绩的全面和详细情况。'
- en: The biggest challenge to these documents is that specific data points and their
    presentation can vary, making it very difficult to establish a one-size-fits-all
    approach to data extraction. However, by leveraging GenAI’s natural language processing
    capabilities, financial services companies can efficiently navigate this structured
    format and extract relevant data points from each section, tailoring their approach
    to the unique characteristics of each company’s report.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文档最大的挑战在于具体数据点和其呈现方式可能各不相同，这使得建立一个适用于所有情况的数据提取方法变得非常困难。然而，通过利用通用人工智能的自然语言处理能力，金融服务公司可以高效地导航这种结构化格式，并从每个部分中提取相关数据点，根据每家公司报告的独特特征调整他们的方法。
- en: The availability of structured metadata extracted from these documents opens
    up opportunities for financial services companies. For example, they can conduct
    an in-depth analysis of a company’s financial performance, benchmarking it against
    industry peers or historical trends. This analysis can inform investment decisions,
    risk assessments, and strategic planning efforts.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些文档中提取的结构化元数据的可用性为金融服务公司开辟了机会。例如，他们可以对公司的财务绩效进行深入分析，将其与行业同行或历史趋势进行比较。这种分析可以指导投资决策、风险评估和战略规划工作。
- en: Another opportunity would be to leverage the extracted metadata to develop predictive
    models and identify patterns that may not be immediately apparent from manual
    analysis. These models can help anticipate potential risks, identify emerging
    trends, and uncover new investment opportunities.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个机会是利用提取的元数据来开发预测模型，并识别出从手动分析中可能不会立即显现的模式。这些模型可以帮助预测潜在风险，识别新兴趋势，并揭示新的投资机会。
- en: Thinking further, the extracted metadata can be integrated into existing data
    repositories or business intelligence platforms, enabling seamless access and
    collaboration among various teams within the organization. This integration can
    foster cross-functional collaboration, enabling different departments, such as
    investment banking, asset management, and risk management, to leverage the same
    data for their respective analyses and decision-making processes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步思考，提取的元数据可以集成到现有的数据存储库或商业智能平台中，使组织内部各个团队之间的访问和协作无缝进行。这种集成可以促进跨职能协作，使不同的部门，如投资银行、资产管理、风险管理，能够利用相同的数据进行各自的分析和决策过程。
- en: In addition to financial services companies, the extracted metadata can also
    be valuable for regulatory bodies, academic researchers, and other stakeholders
    interested in studying corporate performance, industry trends, and the overall
    health of the financial markets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 除了金融服务公司，提取的元数据对于监管机构、学术研究人员以及其他对研究企业绩效、行业趋势和金融市场整体健康状况感兴趣的利益相关者来说也极具价值。
- en: By leveraging the power of GenAI to extract metadata from 10-K reports, financial
    services companies can unlock a wealth of insights, streamline their analysis
    processes, and make more informed decisions that drive business growth and mitigate
    risks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用通用人工智能（GenAI）从10-K报告中提取元数据，金融服务公司可以解锁丰富的见解，简化他们的分析流程，并做出更明智的决策，这些决策可以推动业务增长并降低风险。
- en: Architecture
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Though the scope of this book is not to provide a deep dive into the intricacies
    of a LLM processing architecture, we will briefly discuss what a cloud-based architecture
    for our metadata extraction use case might look like. For this example, we will
    leverage the capabilities of Google Cloud, as it offers a native AI platform called
    Vertex AI that allows us to seamlessly integrate leading models, including Google’s
    Gemini and third-party models such as Anthropic’s Claude, in an enterprise-compliant
    manner.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本书的范围不是深入探讨大型语言模型处理架构的复杂性，但我们将简要讨论我们的元数据提取用例可能看起来像的基于云的架构。在这个例子中，我们将利用 Google
    Cloud 的能力，因为它提供了一个名为 Vertex AI 的本地 AI 平台，允许我们以企业合规的方式无缝集成领先的模型，包括 Google 的 Gemini
    和第三方模型，如 Anthropic 的 Claude。
- en: The approach we’ll adopt for this use case is to leverage a batch-optimized
    architecture, which is suitable for processing large volumes of data in an efficient
    and scalable manner. This kind of architecture aligns with cloud-native principles
    and is a serverless architecture that leverages various Google Cloud services.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用适用于处理大量数据的高效和可扩展的批优化架构来处理此用例。此类架构符合云原生原则，是一种无服务器架构，利用了各种 Google Cloud 服务。
- en: This architecture will consist of an object store (Google Cloud Storage) to
    store the 10-K reports, a messaging queue (Google Cloud Pub/Sub) to coordinate
    the data flow, a processing component (Google Cloud Functions) to execute the
    LLM-based metadata extraction tasks, an LLM model (such as Google Gemini hosted
    on Vertex-AI) to perform the actual extraction, and a database (Google Big-Query)
    to store the extracted metadata.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构将包括一个对象存储（Google Cloud Storage）来存储 10-K 报告，一个消息队列（Google Cloud Pub/Sub）来协调数据流，一个处理组件（Google
    Cloud Functions）来执行基于 LLM 的元数据提取任务，一个 LLM 模型（例如托管在 Vertex-AI 上的 Google Gemini）来执行实际的提取，以及一个数据库（Google
    Big-Query）来存储提取的元数据。
- en: 'Here’s a more detailed breakdown of how this architecture will function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对该架构如何运作的更详细分解：
- en: The 10-K reports are stored in Google Cloud Storage, a highly scalable and durable
    object store.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 10-K 报告存储在高度可扩展和耐用的对象存储 Google Cloud Storage 中。
- en: A Cloud Function is triggered periodically (for example, daily or weekly) to
    initiate the metadata extraction process.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 云函数会定期触发（例如，每日或每周）以启动元数据提取过程。
- en: A Cloud Function will read a list of 10-K reports from Cloud Storage and publish
    messages to a Pub/Sub topic, effectively creating a queue of reports to be processed.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 云函数将读取来自云存储的 10-K 报告列表，并将消息发布到 Pub/Sub 主题，从而有效地创建一个待处理报告队列。
- en: Another Cloud Function, subscribed to the Pub/Sub topic, is triggered for each
    message tied to a given report in the queue.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一个云函数，订阅了 Pub/Sub 主题，会在队列中与给定报告相关的每条消息触发。
- en: This second Cloud Function invokes the LLM model (for example, Google Gemini)
    hosted on Vertex-AI, passing the 10-K report content as input.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个云函数调用托管在 Vertex-AI 上的 LLM 模型（例如，Google Gemini），并将 10-K 报告内容作为输入。
- en: The LLM model processes the report, leveraging its natural language understanding
    capabilities to extract the relevant metadata.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM 模型处理报告，利用其自然语言理解能力提取相关元数据。
- en: The extracted metadata is then stored in a structured format (for example, BigQuery)
    for further analysis and consumption.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取的元数据随后以结构化格式（例如，BigQuery）存储，以便进行进一步分析和消费。
- en: This serverless architecture provides several benefits, including automatic
    scaling, cost-efficiency (pay-per-use pricing), and seamless integration with
    other Google Cloud services.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种无服务器架构提供了几个好处，包括自动扩展、成本效益（按使用付费定价）以及与其他 Google Cloud 服务的无缝集成。
- en: '![](img/B22175_05_01.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_05_01.png)'
- en: 'Figure 5.1: GenAI document data extraction pipeline'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1：GenAI 文档数据提取管道
- en: 'The following diagram showcases the architecture that will be leveraged in
    this example, following our GenAI integration framework discussed in *Chapter
    3*, *Designing Patterns for Interacting with Generative AI*:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了我们将利用的架构，遵循我们在第 3 章“与生成式 AI 交互的设计模式”中讨论的 GenAI 集成框架：
- en: '![](img/B22175_05_02.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B22175_05_02.png)'
- en: 'Figure 5.2: The Application Integration Framework'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：应用程序集成框架
- en: Entry point
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 入口点
- en: The entry point for our batch processing pipeline will be an object created
    in **Google Cloud Storage** (**GCS**), which will then trigger a Google Cloud
    Function to start the processing pipeline. This setup allows us to seamlessly
    integrate with existing workflows, where the 10-K reports are uploaded to a designated
    GCS bucket. By leveraging the event-driven nature of Cloud Functions, our system
    can automatically get into action as soon as a new report lands in the storage
    bucket.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们批量处理管道的入口点将是一个在**Google Cloud Storage**（**GCS**）中创建的对象，然后触发一个Google Cloud
    Function以启动处理管道。这种设置使我们能够无缝集成到现有的工作流程中，其中10-K报告被上传到指定的GCS存储桶。通过利用云函数的事件驱动特性，我们的系统可以在新报告到达存储桶时自动采取行动。
- en: Once triggered, the Cloud Function will initiate the content extraction process.
    For this step, we’ve decided to employ the powerful capabilities of Google’s Gemini
    Pro 1.5, a state-of-the-art GenAI multimodal model that supports processing PDF
    documents directly. Gemini Pro 1.5 will analyze the uploaded 10-K report, intelligently
    extracting not only the textual content but also the relevant data points we’re
    interested in, such as financial figures, company overviews, and key performance
    indicators.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦触发，云函数将启动内容提取过程。为此步骤，我们决定采用Google的Gemini Pro 1.5的强大功能，这是一个支持直接处理PDF文档的尖端GenAI多模态模型。Gemini
    Pro 1.5将分析上传的10-K报告，智能地提取不仅包括文本内容，还包括我们感兴趣的相关的数据点，例如财务数据、公司概述和关键绩效指标。
- en: By leveraging Gemini Pro 1.5’s advanced natural language processing and document
    understanding capabilities, we can obtain a comprehensive transcript of the report’s
    content. This transcript will serve as the foundation for further analysis and
    processing steps in our pipeline. Additionally, the extracted data points will
    be structured and organized in a format of our choosing (JSON, Markup, etc.),
    defined in the prompt, allowing us to seamlessly integrate them into our downstream
    systems to generate insightful summaries, visualizations, and other valuable outputs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用Gemini Pro 1.5的高级自然语言处理和文档理解能力，我们可以获得报告内容的全面记录。这份记录将成为我们管道中进一步分析和处理步骤的基础。此外，提取的数据点将以我们在提示中定义的格式（JSON、标记等）进行结构化和组织，使我们能够无缝地将它们集成到我们的下游系统中，生成有洞察力的摘要、可视化和其他有价值的输出。
- en: Prompt pre-processing
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示预处理
- en: As a first step, we will start elaborating a prompt to extract the essential
    data points required to comprehensively understand a 10-K document. To guide our
    efforts, we will leverage a valuable resource provided by the SEC itself – a document
    titled *How to Read a 10k*, which is available on the SEC website ([https://www.sec.gov/files/reada10k.pdf](https://www.sec.gov/files/reada10k.pdf)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将开始详细阐述一个提示，以提取全面理解10-K文件所需的必要数据点。为了指导我们的工作，我们将利用美国证券交易委员会（SEC）本身提供的一项宝贵资源——一份名为《如何阅读10-K》（*How
    to Read a 10k*）的文件，该文件可在SEC网站上找到（[https://www.sec.gov/files/reada10k.pdf](https://www.sec.gov/files/reada10k.pdf)）。
- en: This SEC-provided document serves as a very useful roadmap, outlining the critical
    sections and information that investors and analysts should focus on when delving
    into a company’s 10-K filing. By carefully studying this resource, we can identify
    the key data points that are most relevant and insightful, ensuring that our GenAI
    system extracts the information that truly matters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这份由美国证券交易委员会（SEC）提供的文件是一个非常实用的路线图，概述了投资者和分析师在深入研究公司10-K文件时应关注的重点章节和信息。通过仔细研究这份资源，我们可以识别出最相关和有洞察力的关键数据点，确保我们的通用人工智能（GenAI）系统提取出真正重要的信息。
- en: To kickstart our data extraction process, we will create a simple yet effective
    prompt for our GenAI model. This initial prompt will serve as a starting point,
    instructing the model to identify and extract the specific data points outlined
    in the *How to Read a 10k* document. While this initial prompt may be concise,
    it will lay the foundation for more sophisticated prompts and fine-tuning techniques
    as our project progresses. The goal is to iteratively refine our prompts, leveraging
    the power of GenAI to extract the most crucial information accurately and efficiently
    from these complex financial documents.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了启动我们的数据提取过程，我们将为我们的GenAI模型创建一个简单而有效的提示。这个初始提示将作为起点，指导模型识别和提取《如何阅读10-K文件》（*How
    to Read a 10k*）文档中概述的具体数据点。虽然这个初始提示可能很简洁，但它将为我们的项目进展中的更复杂提示和微调技术奠定基础。目标是迭代优化我们的提示，利用GenAI的力量准确高效地从这些复杂的财务文件中提取最关键的信息。
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following is example output from the previous prompt:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从上一个提示生成的示例输出：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we have a template of what to look for in the actual 10-K, we can
    create a prompt to extract those data points:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了实际 10-K 中要查找内容的模板，我们可以创建一个提示来提取这些数据点：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that the template has very specific instructions on what to do as well
    as what *not* to do. This is a best practice for prompting LLMs, as these models
    require such specific instructions in order to effectively give you precise information.
    A good analogy is to guide them as if they are a first-year student, offering
    clear instructions and providing as much context as possible.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模板对要做什么以及不要做什么有非常具体的说明。这是提示 LLM 的最佳实践，因为这些模型需要这样的具体说明才能有效地提供精确信息。一个好的类比是像指导一年级学生一样引导他们，提供清晰的指示，并尽可能提供多的背景信息。
- en: Inference
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理
- en: For the inference, we are going to submit our prompt to Gemini Pro 1.5 available
    through Vertex-AI.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理，我们将提交我们的提示到通过 Vertex-AI 可用的 Gemini Pro 1.5。
- en: 'The Vertex AI Gemini API is tailored for developers and enterprises seeking
    to incorporate Gemini models into scaled deployments. This enterprise-grade offering
    provides a robust set of features designed to meet the demands of modern, high-performance
    applications. With the Vertex AI Gemini API, you can benefit from enhanced enterprise
    security measures, ensuring that your data and models are protected with industry-leading
    safeguards. Additionally, it offers data residency options, allowing you to comply
    with regional data storage and processing regulations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI Gemini API 是为寻求将 Gemini 模型纳入规模部署的开发者和企业量身定制的。这个企业级产品提供了一套强大的功能，旨在满足现代高性能应用的需求。使用
    Vertex AI Gemini API，您可以受益于增强的企业安全措施，确保您的数据和模型得到行业领先的安全保障。此外，它还提供数据驻留选项，允许您遵守区域数据存储和处理法规：
- en: '[PRE3]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that there are configurations available to play with in the API. You can
    check the specific call in the code provided in the GitHub repository of the book.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，API 中有可用的配置选项进行测试。您可以在书籍的 GitHub 仓库提供的代码中检查具体的调用。
- en: Result post-processing
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果后处理
- en: Once the LLM has processed the 10-K reports, it will return the results in a
    structured JSON format. This JSON document will contain the extracted data points,
    organized in a hierarchical manner that aligns with the structure of the 10-K
    report itself. To effectively utilize these results, we will need to parse the
    JSON document and extract the relevant information.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 LLM 处理完 10-K 报告，它将以结构化的 JSON 格式返回结果。这个 JSON 文档将包含提取的数据点，以与 10-K 报告本身的层次结构相一致的方式组织。为了有效地利用这些结果，我们需要解析
    JSON 文档并提取相关信息。
- en: The next step in our pipeline is to ingest the parsed data into a database for
    efficient storage and retrieval. The specific ingestion strategy will depend on
    the type of database we choose to employ. For example, if we opt for a relational
    database, we will need to map the extracted data points to appropriate table structures,
    ensuring proper normalization and adherence to data integrity principles.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们管道的下一步是将解析后的数据导入数据库，以实现高效存储和检索。具体的导入策略将取决于我们选择的数据库类型。例如，如果我们选择关系型数据库，我们需要将提取的数据点映射到适当的表结构中，确保适当的归一化和遵守数据完整性原则。
- en: Alternatively, if we decide to use a document database, the ingestion process
    will be more straightforward, as these databases are designed to store hierarchical
    data structures, such as JSON documents, natively. In this case, we can directly
    ingest the parsed JSON results, leveraging the database’s ability to efficiently
    store and query complex data structures.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们决定使用文档数据库，导入过程将更加直接，因为这些数据库是设计用来存储原生层次数据结构的，例如 JSON 文档。在这种情况下，我们可以直接导入解析后的
    JSON 结果，利用数据库高效存储和查询复杂数据结构的能力。
- en: Regardless of the database type chosen, it is crucial to design an ingestion
    strategy that ensures data consistency, scalability, and performance. This may
    involve implementing strategies such as bulk ingestion, indexing, and partitioning
    to optimize the database’s performance and ensure efficient retrieval of the extracted
    data points.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种数据库类型，设计一个确保数据一致性、可扩展性和性能的导入策略至关重要。这可能涉及实施批量导入、索引和分区等策略，以优化数据库的性能并确保高效检索提取的数据点。
- en: In addition to storing the extracted data points, we can also consider generating
    embeddings for the various sections of the 10-K reports. Embeddings are vector
    representations of text that capture semantic meaning, enabling efficient similarity
    searches and retrieval. By generating embeddings for the report sections, we can
    integrate our dataset with a vector search pipeline, allowing users to perform
    advanced queries based on semantic similarity.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储提取的数据点之外，我们还可以考虑为10-K报告的各个部分生成嵌入。嵌入是捕获语义意义的文本的向量表示，使高效的相似性搜索和检索成为可能。通过为报告部分生成嵌入，我们可以将我们的数据集与向量搜索管道集成，使用户能够根据语义相似性执行高级查询。
- en: For a deep dive into embeddings generation and vector search integration, we
    will cover the **Retrieval Augmented Generation** (**RAG**) example in a dedicated
    chapter. This chapter will provide detailed insights into the process of generating
    embeddings, constructing vector databases, and implementing efficient vector search
    algorithms, enabling you to create powerful search and retrieval capabilities
    for your GenAI applications.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深入探讨嵌入生成和向量搜索集成，我们将在专门的章节中介绍**检索增强生成**（**RAG**）示例。本章将提供关于生成嵌入、构建向量数据库和实现高效向量搜索算法的详细见解，使您能够为您的GenAI应用程序创建强大的搜索和检索功能。
- en: Result presentation
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果展示
- en: When it comes to presenting the results obtained from processing the 10-K reports,
    it’s important to consider the fact that these results are ingested into a database.
    This means that the considerations you’ll need to make are similar to those you
    would have when developing an experience that leverages data available in a database.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到展示从处理10-K报告中获得的结果时，重要的是要考虑这些结果被摄入数据库的事实。这意味着您需要考虑的因素与您在开发利用数据库中可用数据的功能时需要考虑的因素相似。
- en: One of the primary considerations is the need for a tool or platform that can
    effectively aggregate and analyze data stored in a database. This could be a **business
    intelligence** (**BI**) tool, a data visualization platform, or even a custom-built
    application tailored to your specific needs. The chosen tool should provide robust
    querying capabilities, enabling you to extract and combine data from various tables
    or collections within the database.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 主要考虑因素之一是需要一个工具或平台，能够有效地聚合和分析存储在数据库中的数据。这可能是一个**商业智能**（**BI**）工具、数据可视化平台，甚至是一个根据您特定需求定制的应用程序。所选工具应提供强大的查询功能，使您能够从数据库中的各种表或集合中提取和组合数据。
- en: Additionally, the presentation layer should offer a range of visualization options,
    such as charts, graphs, and dashboards, to effectively communicate the insights
    derived from the data. These visualizations should be interactive, allowing users
    to explore the data from different perspectives, filter and sort the results,
    and drill down into specific areas of interest.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，展示层应提供一系列可视化选项，如图表、图形和仪表板，以有效地传达从数据中得出的见解。这些可视化应具有交互性，使用户能够从不同角度探索数据，过滤和排序结果，并深入到特定感兴趣的区域。
- en: Furthermore, the presentation layer should be designed with scalability and
    performance in mind. As the volume of data grows over time, the ability to handle
    large datasets and provide responsive user experiences becomes crucial. This may
    involve implementing techniques such as caching, indexing, and optimizing database
    queries to ensure efficient data retrieval and rendering.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，展示层的设计应考虑可扩展性和性能。随着时间的推移，数据量的增长，处理大型数据集并提供响应式用户体验的能力变得至关重要。这可能涉及实施缓存、索引和优化数据库查询等技术，以确保高效的数据检索和渲染。
- en: On the GitHub directory for this chapter, you will find the complete code and
    an analysis of how all the layers described in this chapter fit together.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的GitHub目录中，您将找到完整的代码以及如何将本章中描述的所有层组合在一起的分析。
- en: Summary
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored metadata extraction from financial documents, specifically
    10-K reports filed by publicly traded companies. We walked through the experience
    of working with a financial services firm that needs to extract key data points
    from these massive 10-K annual reports, leveraging the data extraction capabilities
    of LLMs.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了从财务文件中提取元数据，特别是上市公司提交的10-K报告。我们回顾了与一家需要从这些庞大的10-K年度报告中提取关键数据点的金融服务公司合作的经验，利用LLMs的数据提取能力。
- en: We defined the use case, and we leveraged the power of GenAI to navigate through
    the structured sections of a 10-K, pinpointing and extracting the most relevant
    information nuggets, following the guidance provided by a best practices document.
    We walked through the process, starting by crafting an effective prompt to guide
    the AI model. This involved studying an SEC resource that outlines the critical
    sections and data points that investors should focus on. Armed with this knowledge,
    we can iteratively refine our prompts to ensure accurate and efficient extraction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了用例，并利用GenAI的力量导航10-K报告的结构化部分，根据最佳实践文档的指导，定位和提取最相关的信息颗粒。我们详细介绍了这个过程，首先通过制定一个有效的提示来引导AI模型。这涉及到研究SEC资源，概述了投资者应关注的重点部分和数据点。有了这些知识，我们可以迭代地改进我们的提示，以确保准确和高效地提取。
- en: Then, we proposed a cloud-native, serverless architecture on Google Cloud to
    handle the batch processing of these documents. This scalable setup can leverage
    various services like Cloud Storage, Pub/Sub, and Cloud Functions, allowing us
    to seamlessly integrate the AI model and store the extracted data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们提出了一个基于Google Cloud的云原生、无服务器架构来处理这些文档的批量处理。这个可扩展的设置可以利用各种服务，如云存储、Pub/Sub和云函数，使我们能够无缝集成AI模型并存储提取的数据。
- en: The chapter also touched on post-processing steps, such as ingesting the extracted
    data into a database (relational or document-based), potentially generating embeddings
    for vector similarity searches, and presenting the results through BI tools or
    custom applications with interactive visualizations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涉及了后处理步骤，例如将提取的数据导入数据库（关系型或基于文档的），可能生成用于向量相似性搜索的嵌入，并通过BI工具或带有交互式可视化的自定义应用程序展示结果。
- en: In summary, this chapter offered you a practical blueprint to utilize GenAI
    to enhance the extraction and analysis of critical information from complex financial
    documents. It demonstrated how you can leverage this technology to make more informed
    decisions and uncover valuable insights, thereby optimizing your operational efficiency
    and strategic capabilities.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章为您提供了一个利用GenAI从复杂金融文档中提取和分析关键信息的实用蓝图。它展示了您如何利用这项技术做出更明智的决策并发现有价值的见解，从而优化您的运营效率和战略能力。
- en: In the next chapter, we will examine a summarization use case. This example
    will illustrate another instance of what a batch processing use case could look
    like.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一个摘要用例。这个例子将展示批量处理用例可能的样子。
- en: Join our community on Discord
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/genpat](Chapter_05.xhtml)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/genpat](Chapter_05.xhtml)'
- en: '![](img/QR_Code134841911667913109.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code134841911667913109.png)'
