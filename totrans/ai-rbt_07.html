<html><head></head><body>
<div id="_idContainer082">
<h1 class="chapter-number" id="_idParaDest-114"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-115"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.2.1">Teaching the Robot to Navigate and Avoid Stairs</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Let’s have a quick review of our quest to create a robot that picks up toys. </span><span class="koboSpan" id="kobo.3.2">We’ve created a toy detector and trained the robot arm. </span><span class="koboSpan" id="kobo.3.3">What’s next on our to-do list? </span><span class="koboSpan" id="kobo.3.4">We need to drive the robot to the location of the toy in order to pick it up. </span><span class="koboSpan" id="kobo.3.5">That </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">sounds important.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">This chapter covers </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">navigation</span></strong><span class="koboSpan" id="kobo.7.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">path planning</span></strong><span class="koboSpan" id="kobo.9.1"> for our toy-grabbing robot helper. </span><span class="koboSpan" id="kobo.9.2">You have to admit that this is one of the most difficult problems in robotics. </span><span class="koboSpan" id="kobo.9.3">There are two parts to the task – figuring out where you are (localization), and then figuring out where you want to go (path planning). </span><span class="koboSpan" id="kobo.9.4">Most robots at this point would be using some sort of </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">simultaneous localization and mapping</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.12.1">SLAM</span></strong><span class="koboSpan" id="kobo.13.1">) algorithm that would first map the room, and then figure out where the robot is within it. </span><span class="koboSpan" id="kobo.13.2">But is this really necessary? </span><span class="koboSpan" id="kobo.13.3">First of all, SLAM generally requires some sort of 3D sensor, which we don’t have, and a lot of processing, which we don’t want to do. </span><span class="koboSpan" id="kobo.13.4">We can also add that it does not use machine learning, and this is a book about </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">artificial </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.15.1">intelligence</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.16.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.17.1">AI</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">Is it possible to perform our task without making maps or ranging sensors? </span><span class="koboSpan" id="kobo.19.2">Can you think of any other robot that cleans rooms but does not do mapping? </span><span class="koboSpan" id="kobo.19.3">Of course you can. </span><span class="koboSpan" id="kobo.19.4">You probably even have a Roomba® (I have three), and most models do not do any mapping at all – they navigate by means of a pseudo-random statistical </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">cleaning routine.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">Our task in this chapter is to create a reliable navigation system for our robot that is adaptable to our mission of cleaning a single room or floor of toys, and that uses the sensors we </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">already have.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">The following topics will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.25.1">Understanding the </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">SLAM methodology</span></span></li>
<li><span class="koboSpan" id="kobo.27.1">Exploring alternative </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">navigation techniques</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Introducing the Floor Finder algorithm for </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">avoiding obstacles</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">Implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">neural networks</span></span></li>
</ul>
<h1 id="_idParaDest-116"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.33.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.34.1">We require the </span><strong class="bold"><span class="koboSpan" id="kobo.35.1">Robot Operating System Version 2</span></strong><span class="koboSpan" id="kobo.36.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.37.1">ROS 2</span></strong><span class="koboSpan" id="kobo.38.1">) for this chapter. </span><span class="koboSpan" id="kobo.38.2">This book uses the Foxy Fitzroy release: </span><a href="http://wiki.ros.org/foxy/Installation"><span class="koboSpan" id="kobo.39.1">http://wiki.ros.org/foxy/Installation</span></a><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">This chapter assumes that you have completed </span><a href="B19846_06.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.41.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.42.1">, where we gave the robot a voice and the ability to receive voice commands. </span><span class="koboSpan" id="kobo.42.2">We will be using the Mycroft interface and voice text-to-speech system, which is called Mimic: </span><a href="https://github.com/MycroftAI/mimic3"><span class="koboSpan" id="kobo.43.1">https://github.com/MycroftAI/mimic3</span></a><span class="koboSpan" id="kobo.44.1">. </span><span class="koboSpan" id="kobo.44.2">You’ll find the code for this chapter in the GitHub repository for this book </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">at </span></span><a href="https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e"><span class="No-Break"><span class="koboSpan" id="kobo.46.1">https://github.com/PacktPublishing/Artificial-Intelligence-for-Robotics-2e</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.47.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.48.1">We will also be using the </span><strong class="bold"><span class="koboSpan" id="kobo.49.1">Keras</span></strong><span class="koboSpan" id="kobo.50.1"> library for Python (</span><a href="https://keras.io"><span class="koboSpan" id="kobo.51.1">https://keras.io</span></a><span class="koboSpan" id="kobo.52.1">), which is a powerful library for machine learning applications and lets us build custom neural networks. </span><span class="koboSpan" id="kobo.52.2">You can install it using the </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.54.1">
pip install keras</span></pre> <p><span class="koboSpan" id="kobo.55.1">You will also need </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">PyTorch</span></strong><span class="koboSpan" id="kobo.57.1">, which is installed with </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">this command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.59.1">
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118</span></pre> <h1 id="_idParaDest-117"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.60.1">Task analysis</span></h1>
<p><span class="koboSpan" id="kobo.61.1">As we do for </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.62.1">each chapter, let’s review what we are aiming to accomplish. </span><span class="koboSpan" id="kobo.62.2">We will be driving the robot around the house, looking for toys. </span><span class="koboSpan" id="kobo.62.3">Once we have a toy, we will take that toy to the toy box and put it away by dropping it into the toy box. </span><span class="koboSpan" id="kobo.62.4">Then, the robot will go look for more toys. </span><span class="koboSpan" id="kobo.62.5">Along the way, we need to avoid obstacles and hazards, which include a set of stairs going downward that would definitely damage </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">the robot.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.64.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.65.1">I used a baby gate to cover the stairs for the first part of testing and put pillows on the stairs for the second part. </span><span class="koboSpan" id="kobo.65.2">There is no need to bounce the robot down the stairs while it is </span><span class="No-Break"><span class="koboSpan" id="kobo.66.1">still learning.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">We are going</span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.68.1"> to start with the assumption that nothing in this task list requires the robot to know where it is. </span><span class="koboSpan" id="kobo.68.2">Is that true? </span><span class="koboSpan" id="kobo.68.3">We need to find the toy box – that is important. </span><span class="koboSpan" id="kobo.68.4">Can we find the toy box without knowing where it is? </span><span class="koboSpan" id="kobo.68.5">The answer is, of course, that the robot can just search for the toy box using its camera until it locates it. </span><span class="koboSpan" id="kobo.68.6">We developed a technique for recognizing the toy box back in </span><a href="B19846_04.xhtml#_idTextAnchor126"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.69.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.70.1"> with a </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">neural network.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">Now, if the robot was doing a bigger job, such as cleaning a 1,000,000-square-foot warehouse, then we would need a map. </span><span class="koboSpan" id="kobo.72.2">But our task is to clean a single 16 x 16 room. </span><span class="koboSpan" id="kobo.72.3">The time lost searching for the toy box is not all that significant, considering we can’t get too far away, and we must drive to the toy box anyway. </span><span class="koboSpan" id="kobo.72.4">We will set this as a challenge, then, to accomplish our task without making </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">a map.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.74.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.75.1">I once oversaw the evaluation of a robot system created at the Massachusetts Institute of Technology. </span><span class="koboSpan" id="kobo.75.2">They had a navigation system that did not use a map, and I was quite skeptical. </span><span class="koboSpan" id="kobo.75.3">In my defense, the robot actually got lost during the test. </span><span class="koboSpan" id="kobo.75.4">Now, I’m making a mapless navigator, and they are welcome to </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">offer critique.</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">We also need to get the robot to do </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.79.1">Navigate the room avoiding obstacles (toys and furniture) and </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">hazards (stairs).</span></span></li>
<li><span class="koboSpan" id="kobo.81.1">Find toys in the room (with the toy detector we </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">created earlier).</span></span></li>
<li><span class="koboSpan" id="kobo.83.1">Drive to a location where the robot arm can reach </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">the toy.</span></span></li>
<li><span class="koboSpan" id="kobo.85.1">Pick up the toy with the </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">robot arm.</span></span></li>
<li><span class="koboSpan" id="kobo.87.1">Carry the toy to the </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">toy box.</span></span></li>
<li><span class="koboSpan" id="kobo.89.1">Put the toy in the </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">toy box.</span></span></li>
<li><span class="koboSpan" id="kobo.91.1">Go and find </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">another toy.</span></span></li>
<li><span class="koboSpan" id="kobo.93.1">If there are no more toys, </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">then stop.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.95.1">We’ve covered</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.96.1"> finding the toy and picking it up in other chapters. </span><span class="koboSpan" id="kobo.96.2">In this chapter, we will discuss driving up to the toy to pick </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">it up.</span></span></p>
<p><span class="koboSpan" id="kobo.98.1">I’m a big fan of the movie </span><em class="italic"><span class="koboSpan" id="kobo.99.1">The Princess Bride</span></em><span class="koboSpan" id="kobo.100.1">. </span><span class="koboSpan" id="kobo.100.2">It has sword fights, cliffs, two battles of wits, and </span><strong class="bold"><span class="koboSpan" id="kobo.101.1">Rodents of Unusual Size</span></strong><span class="koboSpan" id="kobo.102.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.103.1">ROUS</span></strong><span class="koboSpan" id="kobo.104.1">). </span><span class="koboSpan" id="kobo.104.2">It also has a lesson in planning that we can emulate. </span><span class="koboSpan" id="kobo.104.3">When </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.105.1">our heroes, Fezzik the Giant, Inigo Montoya, and Westley, plan on storming the castle to rescue the princess, the first things Westley asks are “What are our liabilities?” </span><span class="koboSpan" id="kobo.105.2">and “What are our assets?” </span><span class="koboSpan" id="kobo.105.3">Let’s do this for our </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">use case:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.107.1">Our liabilities</span></strong><span class="koboSpan" id="kobo.108.1">: We have a small robot with a very limited sensor and compute capability. </span><span class="koboSpan" id="kobo.108.2">We have a room full of misplaced toys and a set of deadly stairs the robot can </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">fall down.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.110.1">Our assets</span></strong><span class="koboSpan" id="kobo.111.1">: We have a robot with omni wheels that can drive around, a voice, one camera, and a robot arm. </span><span class="koboSpan" id="kobo.111.2">The robot has a datalink via Wi-Fi to a control computer. </span><span class="koboSpan" id="kobo.111.3">We have </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.112.1">this book. </span><span class="koboSpan" id="kobo.112.2">We have a toy box that is a distinctive color. </span><span class="koboSpan" id="kobo.112.3">And lots of </span><strong class="bold"><span class="koboSpan" id="kobo.113.1">Toys of Usual </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.114.1">Size</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.115.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.116.1">TOUS</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.118.1">The appropriate next step, whether we are designing robots or invading castles, is to do some brainstorming. </span><span class="koboSpan" id="kobo.118.2">How would you go about solving </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">this problem?</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">We could use SLAM and make a map, then locate the robot on the map, and use that to navigate. </span><span class="koboSpan" id="kobo.120.2">Although we ultimately will not be following this method, let’s quickly take a look at how </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">it works.</span></span></p>
<h1 id="_idParaDest-118"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.122.1">Understanding the SLAM methodology</span></h1>
<p><span class="koboSpan" id="kobo.123.1">SLAM is a</span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.124.1"> common methodology for navigating indoor robots. </span><span class="koboSpan" id="kobo.124.2">Before we get into the specifics, let’s look at two </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">key issues:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.126.1">The first problem we have in indoor robot driving is that we don’t have </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">a map</span></span></li>
<li><span class="koboSpan" id="kobo.128.1">The second problem we have is that we have no frame of reference to locate ourselves – GPS does not </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">work indoors</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.130.1">That is two problems – we need a map, and then we need a way to locate ourselves on that map. </span><span class="koboSpan" id="kobo.130.2">While SLAM starts with the letter </span><em class="italic"><span class="koboSpan" id="kobo.131.1">S</span></em><span class="koboSpan" id="kobo.132.1"> for “simultaneous,” in truth, most robots make a map, store it away, and then drive on it later. </span><span class="koboSpan" id="kobo.132.2">Of course, while maps are being made, the robot must make the map and then locate itself on the map – usually in </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">the center.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">How does SLAM work? </span><span class="koboSpan" id="kobo.134.2">The</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.135.1"> sensor usually associated with SLAM is the spinning LIDAR. </span><span class="koboSpan" id="kobo.135.2">You can think of LIDAR as laser radar – it uses a laser to measure the distance to objects and spins in a circle to collect data all around </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">the robot.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">We can summarize the SLAM method </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.139.1">The robot takes a measurement of the room by sweeping a laser rangefinder in </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">a circle.</span></span></li>
<li><span class="koboSpan" id="kobo.141.1">The data returned is a list of distance measurements, where the angular measure is a function of the position in the list. </span><span class="koboSpan" id="kobo.141.2">If we have a list of 360 measurements in a circle, then the first number in our list is 0 degrees, the next is 1 degree, and </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">so on.</span></span></li>
<li><span class="koboSpan" id="kobo.143.1">We can extract features in the LIDAR data by looking for corners, edges, jumps, </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">and discontinuities.</span></span></li>
<li><span class="koboSpan" id="kobo.145.1">We look at the angle and distance to each feature from succeeding measurements and create a function that gives the best estimate of how much the </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">robot moved.</span></span></li>
<li><span class="koboSpan" id="kobo.147.1">We use that information to transform the LIDAR data from the sensor-centric coordinate system to some sort of room coordinate system, usually by assuming that the starting position of the robot is coordinate 0,0. </span><span class="koboSpan" id="kobo.147.2">Our transform, or mathematical transformation, will be a combination of translation (movement) and rotation of the robot’s </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">body frame.</span></span></li>
<li><span class="koboSpan" id="kobo.149.1">One way of estimating this transform is to use </span><strong class="bold"><span class="koboSpan" id="kobo.150.1">particles</span></strong><span class="koboSpan" id="kobo.151.1">. </span><span class="koboSpan" id="kobo.151.2">We create samples of the robot’s movement space at every point possible that the robot could have moved, and randomly place dots along all points. </span><span class="koboSpan" id="kobo.151.3">We compute the transform for each of these samples and then test to see which sample best fits the data collected. </span><span class="koboSpan" id="kobo.151.4">This is called a </span><strong class="bold"><span class="koboSpan" id="kobo.152.1">particle filter</span></strong><span class="koboSpan" id="kobo.153.1"> and is </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.154.1">the technique I use for most of my </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">SLAM projects.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.156.1">For more details, you can refer </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">to </span></span><a href="https://www.cs.cmu.edu/~16831-f14/notes/F12/16831_lecture04_dfouhey.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.158.1">https://www.cs.cmu.edu/~16831-f14/notes/F12/16831_lecture04_dfouhey.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.159.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.160.1">It can be </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.161.1">difficult or impossible for SLAM to work in long, featureless hallways, for instance, as it simply has no information to work with – one lidar sweep looks just like the next. </span><span class="koboSpan" id="kobo.161.2">To help with this problem, many SLAM systems require the addition of other sensors to the robot, which measure wheel odometry or use optical flow to measure movement to provide additional data for the position estimate. </span><span class="koboSpan" id="kobo.161.3">The following is an illustration of a SLAM map of an office building made with ROS and displayed in RViz. </span><span class="koboSpan" id="kobo.161.4">The robot uses 500 particles for each LIDAR sample to estimate which changes in robot position best line up the lidar data with the data in the rest of the map. </span><span class="koboSpan" id="kobo.161.5">This is one of my earlier </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">robot projects:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<span class="koboSpan" id="kobo.163.1"><img alt="Figure 7.1 – A map generated by a SLAM navigation process" src="image/B19846_07_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.164.1">Figure 7.1 – A map generated by a SLAM navigation process</span></p>
<p><span class="koboSpan" id="kobo.165.1">What we have to do in the </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.166.1">SLAM process is </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.168.1">First, take a sweep that measures the distance from the robot to all of the objects in </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">the room.</span></span></li>
<li><span class="koboSpan" id="kobo.170.1">Then, we move the robot some distance – for example, three </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">inches forward.</span></span></li>
<li><span class="koboSpan" id="kobo.172.1">Then, we take another sweep and measure the </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">distances again.</span></span></li>
<li><span class="koboSpan" id="kobo.174.1">We now need to come up with a transformation that converts the data in the second sweep to line up with the data in the first sweep. </span><span class="koboSpan" id="kobo.174.2">To do this, there must be information in the two sweeps that can be correlated – corners, doorways, edges, </span><span class="No-Break"><span class="koboSpan" id="kobo.175.1">and furniture.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.176.1">You can get a very small robot LIDAR (e.g., the RPLidar from SLAMtec) for around $100, and use it to make maps. </span><span class="koboSpan" id="kobo.176.2">There is an excellent ROS package called </span><em class="italic"><span class="koboSpan" id="kobo.177.1">Hector Mapping</span></em><span class="koboSpan" id="kobo.178.1"> that makes using this LIDAR straightforward. </span><span class="koboSpan" id="kobo.178.2">You will find that SLAM is not a reliable process and will require several fits and starts to come up with a map that is usable. </span><span class="koboSpan" id="kobo.178.3">Once the map is created, you must keep it updated if anything in the room changes, such are re-arranging </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">the furniture.</span></span></p>
<p><span class="koboSpan" id="kobo.180.1">The SLAM process is actually very interesting, not for what happens in an individual scan, but in how scans are stitched together. </span><span class="koboSpan" id="kobo.180.2">There is an excellent video titled </span><em class="italic"><span class="koboSpan" id="kobo.181.1">Handheld Mapping in the Robocup 2011 Rescue Arena</span></em><span class="koboSpan" id="kobo.182.1"> that the authors of Hector SLAM, at the University of Darmstadt, Germany, put together, illustrating map making. </span><span class="koboSpan" id="kobo.182.2">It is available at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">link: </span></span><a href="https://www.youtube.com/watch?v=F8pdObV_df4list=PL0E462904E5D35E29"><span class="No-Break"><span class="koboSpan" id="kobo.184.1">https://www.youtube.com/watch?v=F8pdObV_df4list=PL0E462904E5D35E29</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.185.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.186.1">I wanted to give you a quick heads-up on SLAM so that we could discuss why we are not going to use it. </span><span class="koboSpan" id="kobo.186.2">SLAM is an important topic and is widely used for navigation, but it is not the only way to solve our </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.187.1">problem by any means. </span><span class="koboSpan" id="kobo.187.2">The weaknesses of SLAM for our purposes include </span><span class="No-Break"><span class="koboSpan" id="kobo.188.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.189.1">The need for some sort of sweeping sensor, such as LIDAR, ultrasound, or infrared, which can be expensive, mechanically complicated, and generate a lot of data. </span><span class="koboSpan" id="kobo.189.2">We want to keep our robot cheap, reliable, </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">and simple.</span></span></li>
<li><span class="koboSpan" id="kobo.191.1">SLAM often works better if the robot has wheel odometers, which don’t work on omni-wheeled vehicles such as our Albert. </span><span class="koboSpan" id="kobo.191.2">Omni wheels slide or skid over the surface in order to turn – we don’t have Ackerman steering, such as a car with wheels that point. </span><span class="koboSpan" id="kobo.191.3">When the wheel skids, it is moving over the surface without turning, which invalidates any sort of wheel odometry, which assumes that the wheels are always turning in contact with </span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">a surface.</span></span></li>
<li><span class="koboSpan" id="kobo.193.1">SLAM</span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.194.1"> does not deal with floorplans that are changing. </span><span class="koboSpan" id="kobo.194.2">The Albert robot has to deal with toys being distributed around the room, which would interfere with LIDAR and change the floorplan that SLAM uses to estimate position. </span><span class="koboSpan" id="kobo.194.3">The robot is also changing the floorplan as it picks up toys and puts </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">them away.</span></span></li>
<li><span class="koboSpan" id="kobo.196.1">SLAM is computationally expensive. </span><span class="koboSpan" id="kobo.196.2">It requires the use of sensors to develop maps and then compares real-time sensor data to the map to localize the robot, which is a </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">complex process.</span></span></li>
<li><span class="koboSpan" id="kobo.198.1">SLAM has problems if data is ambiguous, or if there are not enough features for the robot to estimate changes on. </span><span class="koboSpan" id="kobo.198.2">I’ve had problems with featureless hallways as well as rooms that are </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">highly symmetrical.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.200.1">So, why did I use this amount of space to talk about SLAM when I’m not going to teach you how to use it? </span><span class="koboSpan" id="kobo.200.2">Because you need to know what it is and how it works, because you may have a task that needs to make a map. </span><span class="koboSpan" id="kobo.200.3">There are lots of good tutorials on SLAM available, but very few on what I’m going to teach you next, which will be using AI to navigate safely without </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">a map.</span></span></p>
<h1 id="_idParaDest-119"><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.202.1">Exploring alternative navigation techniques</span></h1>
<p><span class="koboSpan" id="kobo.203.1">In this section, we’ll look at some </span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.204.1">potential alternative methods for navigation that we could use for our robot now that we’ve ruled out the </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">SLAM methodology:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.206.1">We could just drive around randomly, looking for toys. </span><span class="koboSpan" id="kobo.206.2">When we find a toy, the robot picks it up and then drives around randomly looking for the toy box. </span><span class="koboSpan" id="kobo.206.3">When it sees the toy box, it drives up to it and deposits the toy. </span><span class="koboSpan" id="kobo.206.4">But we still need a method to avoid running over obstacles. </span><span class="koboSpan" id="kobo.206.5">We could follow a process called </span><strong class="bold"><span class="koboSpan" id="kobo.207.1">structure from motion</span></strong><span class="koboSpan" id="kobo.208.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.209.1">SfM</span></strong><span class="koboSpan" id="kobo.210.1">) to get</span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.211.1"> depth information out of our single camera and use that to make a map. </span><span class="koboSpan" id="kobo.211.2">Structure from motion requires a lot of textures and edges, which houses may not have. </span><span class="koboSpan" id="kobo.211.3">It also leaves lots of voids (holes) that must be filled in the map. </span><span class="koboSpan" id="kobo.211.4">Structure from motion uses parallax in the video images to estimate the distance to the object in the camera’s field of view. </span><span class="koboSpan" id="kobo.211.5">There </span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.212.1">has been a lot of interesting work in this area, and I have used it to create some promising results. </span><span class="koboSpan" id="kobo.212.2">The video image has to have a lot of detail in it so that the process can match points from one video image to the next. </span><span class="koboSpan" id="kobo.212.3">Here is a survey article on various approaches to SfM, if you are interested you can </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">refer: </span></span><a href="https://www.ijcit.com/archives/volume6/issue6/IJCIT060609.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.214.1">https://www.ijcit.com/archives/volume6/issue6/IJCIT060609.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.215.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.216.1">You may have heard about a technique called </span><strong class="bold"><span class="koboSpan" id="kobo.217.1">floor finding</span></strong><span class="koboSpan" id="kobo.218.1">, which is used in other robots and self-driving cars. </span><span class="koboSpan" id="kobo.218.2">I learned a great deal about floor </span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.219.1">finding from the sophisticated algorithm written by Stephen Gentner in the software package </span><em class="italic"><span class="koboSpan" id="kobo.220.1">RoboRealm</span></em><span class="koboSpan" id="kobo.221.1">, which is an excellent tool for prototyping robot vision systems. </span><span class="koboSpan" id="kobo.221.2">You can find it </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">at </span></span><a href="http://www.roborealm.com"><span class="No-Break"><span class="koboSpan" id="kobo.223.1">http://www.roborealm.com</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.224.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.225.1">This floor finding technique is what we’ll be using in this chapter. </span><span class="koboSpan" id="kobo.225.2">Let’s discuss this in detail in the </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">next section.</span></span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.227.1">Introducing the Floor Finder technique</span></h1>
<p><span class="koboSpan" id="kobo.228.1">What I will be presenting in this</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.229.1"> chapter is my version of a Floor Finder technique that is different from RoboRealm, or other floor-finder algorithms, but that accomplishes the same results. </span><span class="koboSpan" id="kobo.229.2">Let’s break this simple concept down for ease </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">of understanding.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">We know that the floor directly in front of the robot is free from obstacles. </span><span class="koboSpan" id="kobo.231.2">We use the video image pixels of the area just in front of the robot as an example and look for the same texture to be repeated farther away. </span><span class="koboSpan" id="kobo.231.3">We are matching the texture of the part of the image we know is the floor with pixels farther away. </span><span class="koboSpan" id="kobo.231.4">If the textures match, we mark that area green to show that it is drivable and free of obstacles. </span><span class="koboSpan" id="kobo.231.5">We will be using bits of this technique in this chapter. </span><span class="koboSpan" id="kobo.231.6">By the way, did you notice that I said </span><em class="italic"><span class="koboSpan" id="kobo.232.1">texture</span></em><span class="koboSpan" id="kobo.233.1"> and not </span><em class="italic"><span class="koboSpan" id="kobo.234.1">color</span></em><span class="koboSpan" id="kobo.235.1">? </span><span class="koboSpan" id="kobo.235.2">We are not matching the color of the floor, because the floor is not all one color. </span><span class="koboSpan" id="kobo.235.3">I have a brown carpet in my upstairs game room, which still has considerable variation in coloring. </span><span class="koboSpan" id="kobo.235.4">Using color matching, which is simple, just won’t cut it. </span><span class="koboSpan" id="kobo.235.5">We have to match the texture, which can be described in terms of color, intensity (brightness), hue, and roughness (a measure of how smooth the color of the </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">surface is).</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">Let’s try some</span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.238.1"> quick experiments in this area with our image of the floor in my game room. </span><span class="koboSpan" id="kobo.238.2">There are several steps involved when doing this </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">for real:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.240.1">We start with the image we get from the camera. </span><span class="koboSpan" id="kobo.240.2">In order to accelerate processing and make the most efficient use of bandwidth, we set the native resolution of our camera – which has a full resolution of 1900 x 1200 – down to a mere 640 x 480. </span><span class="koboSpan" id="kobo.240.3">Since our robot is small, we are using a small computer – the Nvidia </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">Jetson Nano.</span></span></li>
<li><span class="koboSpan" id="kobo.242.1">We move that to our image processing program, using </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">OpenCV</span></strong><span class="koboSpan" id="kobo.244.1">, an open source computer</span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.245.1"> vision library that also has been incorporated </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">into ROS.</span></span></li>
<li><span class="koboSpan" id="kobo.247.1">Our first step is to blur the image</span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.248.1"> using the </span><strong class="bold"><span class="koboSpan" id="kobo.249.1">Gaussian blur</span></strong><span class="koboSpan" id="kobo.250.1"> function. </span><span class="koboSpan" id="kobo.250.2">The Gaussian blur uses a parabolic function to reduce the amount of high-frequency information in the image – it makes the image fuzzier by reducing the differences between neighboring pixels. </span><span class="koboSpan" id="kobo.250.3">To get enough blurring, I had to apply the blur function three times with a 5 x 5 </span><strong class="bold"><span class="koboSpan" id="kobo.251.1">convolution kernel</span></strong><span class="koboSpan" id="kobo.252.1">. </span><span class="koboSpan" id="kobo.252.2">A convolution kernel</span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.253.1"> is a matrix function – in this case, a 5 x 5 matrix of numbers. </span><span class="koboSpan" id="kobo.253.2">We use this function to modify a pixel based on its neighbors (the pixels around it). </span><span class="koboSpan" id="kobo.253.3">This smoothing makes the colors more uniform, reducing noise, and making the next steps easier. </span><span class="koboSpan" id="kobo.253.4">To blur the image, we take a bit from the surrounding pixels – two on either side – and add that to the center pixel. </span><span class="koboSpan" id="kobo.253.5">We discussed convolution kernels in </span><a href="B19846_04.xhtml#_idTextAnchor126"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.254.1">Chapter 4</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.255.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.256.1">We designate an area in front of the robot to be an area with a clear view of the floor. </span><span class="koboSpan" id="kobo.256.2">I used a triangular area, but a square area works as well. </span><span class="koboSpan" id="kobo.256.3">I picked each of the colors found within the triangle and grabbed all of the pixels that had a value with 15 units of that color. </span><span class="koboSpan" id="kobo.256.4">What does 15 units mean? </span><span class="koboSpan" id="kobo.256.5">Each color is encoded with an RGB value from 0 to 255. </span><span class="koboSpan" id="kobo.256.6">Our carpet color, brown, is around 162, 127, and 22 in red, green, and blue units. </span><span class="koboSpan" id="kobo.256.7">We select all the colors that are within 15 units of that color, which, for red, is from 147 to 177. </span><span class="koboSpan" id="kobo.256.8">This selects the areas of the image similar in color to our floor. </span><span class="koboSpan" id="kobo.256.9">Our wall is a very similar brown or beige, but fortunately, there is a white baseboard that we can isolate so that the robot does not try to climb </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">the walls.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.258.1">Color is</span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.259.1"> not the only way to match pixels on our floor. </span><span class="koboSpan" id="kobo.259.2">We can also look for pixels with a similar hue (shade of color, regardless of how bright or dark it is), pixels with the same </span><strong class="bold"><span class="koboSpan" id="kobo.260.1">saturation</span></strong><span class="koboSpan" id="kobo.261.1"> (darkness or lightness of color), and colors with the same value or </span><strong class="bold"><span class="koboSpan" id="kobo.262.1">luminosity</span></strong><span class="koboSpan" id="kobo.263.1"> (which is the same result as matching colors in a monochrome image or grayscale image). </span><span class="koboSpan" id="kobo.263.2">I compiled a chart illustrating </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">this principle:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer075">
<span class="koboSpan" id="kobo.265.1"><img alt="Figure 7.2 – Selecting pixels in an image by similarity of various attributes, such as color, hue, or saturation" src="image/B19846_07_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.266.1">Figure 7.2 – Selecting pixels in an image by similarity of various attributes, such as color, hue, or saturation</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.267.1">The preceding figure shows the ability of various selection attributes (color, hue, saturation, and luminosity) as a tool to perform floor finding for our robot. </span><span class="koboSpan" id="kobo.267.2">The hue attribute seems to provide the best results in this test. </span><span class="koboSpan" id="kobo.267.3">I tested it on another image to be sure it was working. </span><span class="koboSpan" id="kobo.267.4">It seems not to separate out the baseboards, which are not part of the safe area to </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">drive on.</span></span></p>
<ol>
<li value="5"><span class="koboSpan" id="kobo.269.1">We select </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.270.1">all of the pixels that match our floor colors and paint them green – or, to be more correct, we create a mask region in a copy of the image that has all of the pixels we want to be designated somehow. </span><span class="koboSpan" id="kobo.270.2">We can use the number 10, for instance. </span><span class="koboSpan" id="kobo.270.3">We make a blank buffer the size of our image and turn all of the pixels in that buffer to 10, which would be the floor in the </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">other image.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.272.1">Performing an erode function on the masked data can help in this regard. </span><span class="koboSpan" id="kobo.272.2">There may be small holes or noise where one or two pixels did not match our carpet colors exactly – say there is a spot where someone dropped a cookie. </span><span class="koboSpan" id="kobo.272.3">The erode function reduces the level of detail in the mask by selecting a small region – for example, 3 x 3, and setting the mask pixel to 10 only if all of the surrounding pixels are also 10. </span><span class="koboSpan" id="kobo.272.4">This reduces the border of the mask by one pixel and removes any small speckles or dots that may be one or two pixels big. </span><span class="koboSpan" id="kobo.272.5">You can see from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.273.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.274.1">.3</span></em><span class="koboSpan" id="kobo.275.1"> that I was quite successful in isolating the floor area with a very solid mask. </span><span class="koboSpan" id="kobo.275.2">Given that we now know where the floor is, we paint the other pixels in our mask red, or some number signifying that it is unsafe to travel there. </span><span class="koboSpan" id="kobo.275.3">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">use 255:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer076">
<span class="koboSpan" id="kobo.277.1"><img alt="Figure 7.3 – My version of the ﬂoor ﬁnder algorithm" src="image/B19846_07_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.278.1">Figure 7.3 – My version of the ﬂoor ﬁnder algorithm</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.279.1">Note that it </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.280.1">does a very good job in this case of identifying where it is safe to drive. </span><span class="koboSpan" id="kobo.280.2">The projected paths are required to prevent the robot from trying to drive up the wall. </span><span class="koboSpan" id="kobo.280.3">You get bonus points if you can identify the robot in </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">the corner.</span></span></p>
<ol>
<li value="6"><span class="koboSpan" id="kobo.282.1">Our next step may take some thought on your part. </span><span class="koboSpan" id="kobo.282.2">We need to identify the areas that are safe to drive. </span><span class="koboSpan" id="kobo.282.3">There are two cases when using this process that may cause </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">us problems:</span></span><ul><li><span class="koboSpan" id="kobo.284.1">We may have an object in the middle of the floor by itself – such as a toy – that has green pixels on either side </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">of it</span></span></li><li><span class="koboSpan" id="kobo.286.1">We may also have a concave region that the robot can get into but not </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">out of</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.288.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.289.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.290.1">.3</span></em><span class="koboSpan" id="kobo.291.1">, you can see that the algorithm painted the wall pixels green since they match the color of the floor. </span><span class="koboSpan" id="kobo.291.2">There is a strong red band of no-go pixels where the baseboard is. </span><span class="koboSpan" id="kobo.291.3">To detect these two cases, we project lines from the robot’s position up from the floor and identify the first red pixel we hit. </span><span class="koboSpan" id="kobo.291.4">That sets the boundary for where the robot can drive. </span><span class="koboSpan" id="kobo.291.5">You can get a similar result if you trace upward from the bottom of the image straight up until you hit a red pixel, and stop at the first one. </span><span class="koboSpan" id="kobo.291.6">Let’s try the Floor Finder process again, but add some toys to the image so that we can be sure we are getting the result </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">we want:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer077">
<span class="koboSpan" id="kobo.293.1"><img alt="Figure 7.4 – Adding toys to the image to determine if we are detecting toys as obstacles" src="image/B19846_07_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.294.1">Figure 7.4 – Adding toys to the image to determine if we are detecting toys as obstacles</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.295.1">That seems to be</span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.296.1"> working well. </span><span class="koboSpan" id="kobo.296.2">We are able to find a good path to drive on. </span><span class="koboSpan" id="kobo.296.3">Keep in mind that we are constantly updating the obstacle view with the Floor Finder and updating our path as we drive. </span><span class="koboSpan" id="kobo.296.4">There is one shortcoming of this process. </span><span class="koboSpan" id="kobo.296.5">If a toy matches the color and texture of the carpet, then we might have a lot of difficulty finding it. </span><span class="koboSpan" id="kobo.296.6">You can add strip of masking tape to objects to deal with this issue, giving the camera something </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">to see.</span></span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.298.1">Another trick we can use with this process is to use the fixed camera geometry to do distance and size estimates. </span><span class="koboSpan" id="kobo.298.2">We have a “locked-down” camera – it is fixed in position on the robot, a set height from the floor, and, therefore, distance along the floor can be measured from the </span><em class="italic"><span class="koboSpan" id="kobo.299.1">y</span></em><span class="koboSpan" id="kobo.300.1"> value of the pixels. </span><span class="koboSpan" id="kobo.300.2">We would need to carefully calibrate the camera by using a tape measure and a box to match pixel values to the distance along the same path line we drew from the robot base to the obstacle. </span><span class="koboSpan" id="kobo.300.3">The distances will be nonlinear and only valid out to the distance the pixels continue to change. </span><span class="koboSpan" id="kobo.300.4">Since the camera is perpendicular to the floor, we get a certain amount of perspective effect that diminishes to 0 about 20 feet from </span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.301.1">the camera. </span><span class="koboSpan" id="kobo.301.2">My calibration resulted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">following table:</span></span><table class="T---Table _idGenTablePara-1" id="table001-1"><colgroup><col/><col/><col/></colgroup><tbody><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><strong class="bold"><span class="koboSpan" id="kobo.303.1">Measurement </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.304.1">in inches</span></strong></span></p></td><td class="T---Table T---Body T---Body"><p><strong class="bold"><span class="koboSpan" id="kobo.305.1">Distance </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.306.1">from top</span></strong></span></p></td><td class="T---Table T---Body T---Body"><p><strong class="bold"><span class="koboSpan" id="kobo.307.1">Distance </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.308.1">from bottom</span></strong></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="koboSpan" id="kobo.309.1">0</span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.310.1">1080</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="koboSpan" id="kobo.311.1">0</span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.312.1">12</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.313.1">715</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.314.1">365</span></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.315.1">24</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.316.1">627</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.317.1">453</span></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.318.1">36</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.319.1">598.3</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.320.1">481.7</span></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.321.1">48</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.322.1">581.5</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.323.1">498.5</span></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.324.1">60</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.325.1">571.8</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.326.1">508.2</span></span></p></td></tr><tr class="T---Table"><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.327.1">72</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.328.1">565</span></span></p></td><td class="T---Table T---Body T---Body"><p><span class="No-Break"><span class="koboSpan" id="kobo.329.1">515</span></span></p></td></tr></tbody></table></li>
</ol>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.330.1">Table 7.5 – Table of measurements comparing pixels to inches for scale</span></p>
<p><span class="koboSpan" id="kobo.331.1">The following image shows the technique for measuring distance in the robot camera field of view. </span><span class="koboSpan" id="kobo.331.2">The object is located four feet away from the robot base along the tape measure. </span><span class="koboSpan" id="kobo.331.3">Albert uses a 180-degree fisheye lens on an HD-capable web camera. </span><span class="koboSpan" id="kobo.331.4">We need the wide field of view later in </span><a href="B19846_09.xhtml#_idTextAnchor294"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.332.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.333.1"> when we </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">do navigation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<span class="koboSpan" id="kobo.335.1"><img alt="Figure 7.5 – Determining the scale of the pixels in our navigation camera image" src="image/B19846_07_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.336.1">Figure 7.5 – Determining the scale of the pixels in our navigation camera image</span></p>
<p><span class="koboSpan" id="kobo.337.1">One thing to </span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.338.1">watch out for is narrow passages that the robot will not fit into. </span><span class="koboSpan" id="kobo.338.2">We can estimate widths based on distance and pixels. </span><span class="koboSpan" id="kobo.338.3">One common robot technique is to put a border around all the obstacles equal to 1/2 the width of the robot. </span><span class="koboSpan" id="kobo.338.4">If there are obstacles on both sides, then the two borders will meet and the robot will know it does </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">not fit.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">In the next section, we will </span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.341.1">create a </span><strong class="bold"><span class="koboSpan" id="kobo.342.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.343.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.344.1">CNN</span></strong><span class="koboSpan" id="kobo.345.1">) to take our images and turn them into robot commands – in essence, teaching our robot how to drive by seeing landmarks or features in the </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">video image.</span></span></p>
<h1 id="_idParaDest-121"><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.347.1">Implementing neural networks</span></h1>
<p><span class="koboSpan" id="kobo.348.1">So, what does a </span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.349.1">neural network do? </span><span class="koboSpan" id="kobo.349.2">We use a neural network to predict some association of an input with an output. </span><span class="koboSpan" id="kobo.349.3">When we use a CNN, we can associate a picture with some desired output. </span><span class="koboSpan" id="kobo.349.4">What we did in our previous chapter was to associate a class name (toys) with certain images. </span><span class="koboSpan" id="kobo.349.5">But what if we tried to associate something else </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">with images?</span></span></p>
<p><span class="koboSpan" id="kobo.351.1">How about this? </span><span class="koboSpan" id="kobo.351.2">We use a neural network to classify the images from our camera. </span><span class="koboSpan" id="kobo.351.3">We drive the robot around manually, using a joystick, and take a picture about four times a second. </span><span class="koboSpan" id="kobo.351.4">We record what the robot is doing in each picture – going forward, turning right, turning left, or backing up. </span><span class="koboSpan" id="kobo.351.5">We use that information to predict the robot’s motion command given the image. </span><span class="koboSpan" id="kobo.351.6">We make a CNN, with the camera image as the input and four outputs – commands for go forward, go left, or go right. </span><span class="koboSpan" id="kobo.351.7">This has the advantage of avoiding fixed obstacles and hazards automatically. </span><span class="koboSpan" id="kobo.351.8">When we get to the stairs (remember that I </span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.352.1">have stairs going down in my game room that would damage the robot), the robot will know to turn around, because that is what we did in training – we won’t deliberately drive the robot down the stairs during training (right?). </span><span class="koboSpan" id="kobo.352.2">We are teaching the robot to navigate the room </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">by example.</span></span></p>
<p><span class="koboSpan" id="kobo.354.1">You may be yelling at the book at this moment (and you should be) saying, “What about the toys?” </span><span class="koboSpan" id="kobo.354.2">Unless, of course, you are following my thought process and thinking to yourself, “Oh, that is why we just spent all that time talking about Floor Finder!” </span><span class="koboSpan" id="kobo.354.3">The neural network approach will get us around the room, and avoid the hazards and furniture, but will not help the robot to avoid toys, which are not in the training set. </span><span class="koboSpan" id="kobo.354.4">We can’t put them in this training set because the toys are never in the same place twice. </span><span class="koboSpan" id="kobo.354.5">We will use the Floor Finder to help avoid the toys. </span><span class="koboSpan" id="kobo.354.6">How do we combine the two? </span><span class="koboSpan" id="kobo.354.7">The neural network provides the longer-range goal to the robot, and the Floor Finder modifies that goal to avoid local, short-range objects. </span><span class="koboSpan" id="kobo.354.8">In our program, we evaluate the neural network first and then use Floor Finder to pick a </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">clear route.</span></span></p>
<p><span class="koboSpan" id="kobo.356.1">On that theme, we are also going to pull another trick for training our robot. </span><span class="koboSpan" id="kobo.356.2">Since our floor surface is subject to change, and may be covered with toys, we will leave that part out of the training images. </span><span class="koboSpan" id="kobo.356.3">Before sending the image to the neural network, we’ll cut the image in half and only use the top half. </span><span class="koboSpan" id="kobo.356.4">Since our camera is fixed and level with the floor, that gives us only the upper half of the room to use for navigation. </span><span class="koboSpan" id="kobo.356.5">Our image is a 180-degree wide angle, so we have a lot of information to work with. </span><span class="koboSpan" id="kobo.356.6">This should give us the resiliency to navigate under </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">any conditions:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<span class="koboSpan" id="kobo.358.1"><img alt="Figure 7.6 – The training set for driving the robot only includes the top of the image" src="image/B19846_07_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.359.1">Figure 7.6 – The training set for driving the robot only includes the top of the image</span></p>
<p><span class="koboSpan" id="kobo.360.1">Our second </span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.361.1">problem is locating the toy box. </span><span class="koboSpan" id="kobo.361.2">For that, we need to create a new training set of images, which will represent an alternative driving pattern. </span><span class="koboSpan" id="kobo.361.3">We start the robot in various random locations, and then simply drive to the toy box. </span><span class="koboSpan" id="kobo.361.4">We use exactly the same process we used before for navigation – we are creating a training set that tells the robot how to get to the toy box. </span><span class="koboSpan" id="kobo.361.5">The trick is to get a good sample of every possible starting location. </span><span class="koboSpan" id="kobo.361.6">We do have a bit of a break – if a point on the map (a position in the room) is already on one path, we don’t need to cover it again. </span><span class="koboSpan" id="kobo.361.7">In other words, all points that are included in another path don’t need to be repeated. </span><span class="koboSpan" id="kobo.361.8">We still want to have at least 1,000 images to train from both sets of images – the one that explores the room, and the set that drives to the </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">toy box.</span></span></p>
<p><span class="koboSpan" id="kobo.363.1">I created a simple program that just lets the operator drive the robot with a joystick. </span><span class="koboSpan" id="kobo.363.2">It automatically takes a snapshot once a second. </span><span class="koboSpan" id="kobo.363.3">Each frame is labeled by simply looking at the value for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.364.1">cmd_vel</span></strong><span class="koboSpan" id="kobo.365.1"> topic, which is how we control the motion of the robot base. </span><span class="koboSpan" id="kobo.365.2">If the angular velocity Z term (</span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">angular.z</span></strong><span class="koboSpan" id="kobo.367.1">) is positive, we are turning right. </span><span class="koboSpan" id="kobo.367.2">If it is negative, we are turning left, and if the term is zero (you guessed it), we are driving straight ahead. </span><span class="koboSpan" id="kobo.367.3">I created an independent program that works with the camera and stores a snapshot whenever it receives a </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">TAKE PIC LEFT</span></strong><span class="koboSpan" id="kobo.369.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">RIGHT</span></strong><span class="koboSpan" id="kobo.371.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">CENTER</span></strong><span class="koboSpan" id="kobo.373.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">BACK</span></strong><span class="koboSpan" id="kobo.375.1"> command on the ROS </span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">syscommand</span></strong><span class="koboSpan" id="kobo.377.1"> topic. </span><span class="koboSpan" id="kobo.377.2">These programs will be in the GitHub repository for the book – I’m not going to include them here. </span><span class="koboSpan" id="kobo.377.3">We put each category of picture in its </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">own </span></span><span class="No-Break"><a id="_idIndexMarker538"/></span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">subdirectory.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">You can think of the neural network as working </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">like this:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.382.1">We present an image to the </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">neural network.</span></span></li>
<li><span class="koboSpan" id="kobo.384.1">It selects features from that image and then selects the images in the training database that are most like the features in the </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">image provided.</span></span></li>
<li><span class="koboSpan" id="kobo.386.1">Each picture in the training database is associated with a driving command (left, center, right). </span><span class="koboSpan" id="kobo.386.2">So, if the image most closely resembles an image where the robot turned left, then the network will return </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.387.1">turn left</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.389.1">Now let’s look at these processes in </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">greater detail.</span></span></p>
<h2 id="_idParaDest-122"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.391.1">Processing the image</span></h2>
<p><span class="koboSpan" id="kobo.392.1">Now, we have several </span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.393.1">steps to take before we can present our data to the neural network for training. </span><span class="koboSpan" id="kobo.393.2">Our camera on the robot has way too much resolution for what we need for the network, and we want to use the minimum amount of data in the neural network we can get </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">away with:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<span class="koboSpan" id="kobo.395.1"><img alt="Figure 7.7 – Image processing for CNN" src="image/B19846_07_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.396.1">Figure 7.7 – Image processing for CNN</span></p>
<p><span class="koboSpan" id="kobo.397.1">Let’s break down</span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.398.1"> this process to make </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">this clearer:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.400.1">The first image in the preceding figure represents our </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">original image.</span></span></li>
<li><span class="koboSpan" id="kobo.402.1">Our first step is to downsample the image to 640 x 480. </span><span class="koboSpan" id="kobo.402.2">We cut the image in half and keep only the top half, which eliminates the floor from our consideration. </span><span class="koboSpan" id="kobo.402.3">We resize the rectangular image to 244 x 244, which is an appropriate size for our neural network </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">to process.</span></span></li>
<li><span class="koboSpan" id="kobo.404.1">We convert the image to greyscale, so</span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.405.1"> that we only have one channel to process, using this formula (proposed by the </span><strong class="bold"><span class="koboSpan" id="kobo.406.1">National Television Standards </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.407.1">Committee</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.408.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.409.1">NTSC</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">)):</span></span><p class="list-inset"><em class="italic"><span class="koboSpan" id="kobo.411.1">Greyscale = 0.299 * R + 0.587 * G + 0.114 * B</span></em></p></li>
<li><span class="koboSpan" id="kobo.412.1">Our next step is to equalize the image to take the entire range of possible values. </span><span class="koboSpan" id="kobo.412.2">The raw output of the camera contains neither pure white (</span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">255</span></strong><span class="koboSpan" id="kobo.414.1">) nor pure black (</span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">0</span></strong><span class="koboSpan" id="kobo.416.1">). </span><span class="koboSpan" id="kobo.416.2">The lowest value may be </span><strong class="source-inline"><span class="koboSpan" id="kobo.417.1">53</span></strong><span class="koboSpan" id="kobo.418.1"> and the highest, </span><strong class="source-inline"><span class="koboSpan" id="kobo.419.1">180</span></strong><span class="koboSpan" id="kobo.420.1">, for a range of </span><strong class="source-inline"><span class="koboSpan" id="kobo.421.1">127</span></strong><span class="koboSpan" id="kobo.422.1">. </span><span class="koboSpan" id="kobo.422.2">We scale the grayscale values by subtracting the low (</span><strong class="source-inline"><span class="koboSpan" id="kobo.423.1">53</span></strong><span class="koboSpan" id="kobo.424.1">) and multiplying by the scale factor (</span><strong class="source-inline"><span class="koboSpan" id="kobo.425.1">127</span></strong><span class="koboSpan" id="kobo.426.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">255</span></strong><span class="koboSpan" id="kobo.428.1">). </span><span class="koboSpan" id="kobo.428.2">This expands the range of the image to the full scale and eliminates a lot of the variation in lighting and illumination that may exist. </span><span class="koboSpan" id="kobo.428.3">We are trying to present consistent data to the </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">neural network.</span></span></li>
<li><span class="koboSpan" id="kobo.430.1">The </span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.431.1">next step is to perform a Gaussian blur function on the data. </span><span class="koboSpan" id="kobo.431.2">We want to reduce some of the high-frequency data in the image, to smooth out some of the edges. </span><span class="koboSpan" id="kobo.431.3">This is an optional step, and may not be necessary for your environment. </span><span class="koboSpan" id="kobo.431.4">I have a lot of detail in the robot’s field of view, and I feel that the blur will give us better results. </span><span class="koboSpan" id="kobo.431.5">It also fills in some of the gaps in the grayscale histogram left by the equalization process in our </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">previous step.</span></span></li>
<li><span class="koboSpan" id="kobo.433.1">We have to normalize the data to reduce the scale from </span><strong class="source-inline"><span class="koboSpan" id="kobo.434.1">0-255</span></strong><span class="koboSpan" id="kobo.435.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">0-1</span></strong><span class="koboSpan" id="kobo.437.1">. </span><span class="koboSpan" id="kobo.437.2">This is to satisfy the artificial neural network’s input requirements. </span><span class="koboSpan" id="kobo.437.3">To perform this operation, we just divide each pixel by </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">255</span></strong><span class="koboSpan" id="kobo.439.1">. </span><span class="koboSpan" id="kobo.439.2">We also must convert the data from the OpenCV image format to a NumPy array. </span><span class="koboSpan" id="kobo.439.3">All of this is part of normal </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">CNN preprocessing.</span></span></li>
<li><span class="koboSpan" id="kobo.441.1">Our neural network is a nine-layer CNN. </span><span class="koboSpan" id="kobo.441.2">I used this common architecture because it is a </span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.442.1">variation of </span><strong class="bold"><span class="koboSpan" id="kobo.443.1">LeNet</span></strong><span class="koboSpan" id="kobo.444.1">, which is widely used for this sort of task (</span><a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf"><span class="koboSpan" id="kobo.445.1">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</span></a><span class="koboSpan" id="kobo.446.1">). </span><span class="koboSpan" id="kobo.446.2">However, in our final step, rather than being a binary output determined by a binary classifier, we will </span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.447.1">use a </span><strong class="bold"><span class="koboSpan" id="kobo.448.1">Softmax classifier</span></strong><span class="koboSpan" id="kobo.449.1"> with four outputs – forward, left turn, or right turns. </span><span class="koboSpan" id="kobo.449.2">We can actually make more categories if we want to and have easy right and hard right turns rather than just one level of turns. </span><span class="koboSpan" id="kobo.449.3">I’m not using the full capability of the new omni wheels on my robot to keep this problem simple. </span><span class="koboSpan" id="kobo.449.4">Remember that the number of output categories must match our training set </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">labels exactly.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.451.1">In our CNN, the first six layers are pairs of CNNs with max pooling layers in between. </span><span class="koboSpan" id="kobo.451.2">This lets the network deal with incrementally larger details in the image. </span><span class="koboSpan" id="kobo.451.3">The final two layers</span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.452.1"> are fully connected with </span><strong class="bold"><span class="koboSpan" id="kobo.453.1">rectified linear units</span></strong><span class="koboSpan" id="kobo.454.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.455.1">ReLU</span></strong><span class="koboSpan" id="kobo.456.1">) activations. </span><span class="koboSpan" id="kobo.456.2">Remember that ReLU only takes the positive values from the other layers. </span><span class="koboSpan" id="kobo.456.3">Here is our final layer, which is a Softmax classifier with </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">four outputs:</span></span></p></li>
</ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<span class="koboSpan" id="kobo.458.1"><img alt="Figure 7.8 – Organization of our neural network" src="image/B19846_07_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.459.1">Figure 7.8 – Organization of our neural network</span></p>
<p><span class="koboSpan" id="kobo.460.1">Like any other</span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.461.1"> neural network training task, the next set of steps in the process involves splitting the input data into training sets and validation sets. </span><span class="koboSpan" id="kobo.461.2">Let’s learn how to train the neural </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">network next.</span></span></p>
<h2 id="_idParaDest-123"><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.463.1">Training the neural network for navigation</span></h2>
<p><span class="koboSpan" id="kobo.464.1">We’ll use 80% of our </span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.465.1">data on training and 20% on validation. </span><span class="koboSpan" id="kobo.465.2">We really can’t use a process that sweetens the data by duplicating images with random rotations, as we did with the toy recognition program, since we are not just recognizing images, but using them for direction. </span><span class="koboSpan" id="kobo.465.3">Changing rotations would mess up </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">our directions.</span></span></p>
<p><span class="koboSpan" id="kobo.467.1">Now, let’s put our training program together. </span><span class="koboSpan" id="kobo.467.2">This program was partially inspired by Adrian Rosebrock’s </span><em class="italic"><span class="koboSpan" id="kobo.468.1">pyImageSearch</span></em><span class="koboSpan" id="kobo.469.1"> blog and by the paper </span><em class="italic"><span class="koboSpan" id="kobo.470.1">Deep Obstacle Avoidance</span></em><span class="koboSpan" id="kobo.471.1"> by Sullivan and Lawson at the Naval Research Lab. </span><span class="koboSpan" id="kobo.471.2">You can follow </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">these steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.473.1">We need to collect our training data by driving the robot around and recording our driving movements. </span><span class="koboSpan" id="kobo.473.2">This separated our data into three sets – left turn, right turn, and go straight. </span><span class="koboSpan" id="kobo.473.3">We have our training images in three subfolders to match our labels. </span><span class="koboSpan" id="kobo.473.4">We read in our data, associate it with the labels, and preprocess the data to present it to the </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">neural network.</span></span></li>
</ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.475.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.476.1">I’m doing the training runs on my desktop computer, not on the Jetson Nano. </span><span class="koboSpan" id="kobo.476.2">We’ll deploy on the Jetson Nano later with our fully </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">trained networks.</span></span></p>
<ol>
<li value="2"><span class="koboSpan" id="kobo.478.1">Here are the imports that we need for this program – there are quite </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">a few:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.480.1">
# import the necessary packages
from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import Adam
from sklearn.model_selection import train_test_split from keras.preprocessing.image import img_to_array from keras.utils import to_categorical
import matplotlib.pyplot as plt import numpy as np
import cv2 import os
from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
from keras import backend as K</span></pre></li> <li><span class="koboSpan" id="kobo.481.1">Here is the setup for </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">the CNN:</span></span><ul><li><span class="koboSpan" id="kobo.483.1">We </span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.484.1">have three convolution layers, each followed by a </span><strong class="source-inline"><span class="koboSpan" id="kobo.485.1">maxpooling</span></strong><span class="koboSpan" id="kobo.486.1"> layer. </span><span class="koboSpan" id="kobo.486.2">Remember that each </span><strong class="source-inline"><span class="koboSpan" id="kobo.487.1">maxpooling</span></strong><span class="koboSpan" id="kobo.488.1"> layer will reduce the resolution of the image considered by the network by half, which is ¼ of the data, because we halve the width and the height. </span><span class="koboSpan" id="kobo.488.2">The convolution layers use the ReLU activation function since we don’t want any negative </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">pixel values.</span></span></li><li><span class="koboSpan" id="kobo.490.1">After the convolution layers, we have two fully connected layers with 500 </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">neurons each.</span></span></li><li><span class="koboSpan" id="kobo.492.1">The final layer is our three neuron output layers, with a Softmax classifier that will output the percentage of each classification (left, right, and center). </span><span class="koboSpan" id="kobo.492.2">The output will look like </span><strong class="source-inline"><span class="koboSpan" id="kobo.493.1">(0.8, 0.15, 0.05)</span></strong><span class="koboSpan" id="kobo.494.1">, with three numbers that add up </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">to 1.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.496.1">This is a generic convolution network class that can be reused for other things, as it is a general multi-class image </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">classification CNN:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.498.1">
class ConvNet():
    @staticmethod
    def create(width, height, depth, classes):
        # initialize the network
        network = Sequential()
        inputShape = (height, width, depth)
        # first set of CONV =&gt; RELU =&gt; POOL layers
        network.add(Conv2D(50, (10, 10), padding="same", input_shape=inputShape))
        network.add(Activation("relu"))
        network.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        # second set of CONV =&gt; RELU =&gt; POOL layers
        network.add(Conv2D(50, (5, 5), padding="same"))
        network.add(Activation("relu"))
        network.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        # third set of CONV =&gt; RELU =&gt; POOL layers
        network.add(Conv2D(50, (5, 5), padding="same"))
        network.add(Activation("relu"))
        network.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
        # Fully connected ReLU layers
        network.add(Flatten())
        network.add(Dense(500))
        network.add(Activation("relu"))
        network.add(Dense(500))
        network.add(Activation("relu"))
        # softmax classifier
        network.add(Dense(classes))
        network.add(Activation("softmax"))
        # return the constructed network architecture
        return network</span></pre></li> <li><span class="koboSpan" id="kobo.499.1">Now, we set up our learning regime. </span><span class="koboSpan" id="kobo.499.2">We will run 25 training runs, with a learning rate of 0.001. </span><span class="koboSpan" id="kobo.499.3">We set a batch size of 32 images per batch, which we can reduce if we end up running out </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">of memory:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.501.1">
EPOCHS = 25 LEARN_RATE = 1e-3
BATCH = 32 # batch size - modify if you run out of memory</span></pre></li> <li><span class="koboSpan" id="kobo.502.1">The next</span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.503.1"> section loads all of our images. </span><span class="koboSpan" id="kobo.503.2">We set the path here where the images reside. </span><span class="koboSpan" id="kobo.503.3">We put the three types of training images in folders named </span><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">left</span></strong><span class="koboSpan" id="kobo.505.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.506.1">right</span></strong><span class="koboSpan" id="kobo.507.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.509.1">center</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.511.1">
print ("Loading Images")
images=[]
labels=[]
#location of your images
imgPath = "c:\users\fxgovers\documents\book\chapter7\train\" imageDirs=["left","right","center"]
for imgDir in imageDirs:
 fullPath = imgPath + imgDir
 # find all the images in this directory 
 allFileNames = 
 os.listdir(fullPath) ifiles=[]
 label = imgDirs.index(imgDir) # use the integer version of the 
 label # 0= left, 1 = right, 2 = center
 for fname in allFileNames:
   if ".jpg" in fname:
      ifiles.append(fname)</span></pre></li> <li><span class="koboSpan" id="kobo.512.1">Now, you can refer back to my diagram (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.513.1">Figure 7</span></em></span><em class="italic"><span class="koboSpan" id="kobo.514.1">.7</span></em><span class="koboSpan" id="kobo.515.1">) of the process we will go through to preprocess the images. </span><span class="koboSpan" id="kobo.515.2">We will cut the image in half and just process the upper half of the picture. </span><span class="koboSpan" id="kobo.515.3">Then, we reduce the image to 244 x 244 to fit into the neural network, which needs square images. </span><span class="koboSpan" id="kobo.515.4">We will convert the image to grayscale (black and white) since we don’t need to consider color, just shapes. </span><span class="koboSpan" id="kobo.515.5">This cuts our data down further. </span><span class="koboSpan" id="kobo.515.6">We will equalize the image, which rescales the range of gray colors to fill the whole area from 0 to 255. </span><span class="koboSpan" id="kobo.515.7">This evens out the illumination </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.516.1">and sets </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">the contrast:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.518.1">
# process all of the images 
for ifname in ifiles:
  # load the image, pre-process it, and store it in the data list image = cv2.imread(ifname)
  # let's get the image to a known size regardless of what was collected 
  image = cv2.resize(image, (800, 600))
  halfImage = 800*300 # half the pixels
  # cut the image in half -we take the top half 
  image = image[0:halfimage]
  #size the image to what we want to put into the neural network image=cv2.resize(image,(224,224))
  # convert to grayscale
  image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #equalize the image to use the full range from 0 to 255 # this gets rid of a lot of illumination variation 
  image = cv2.equalizeHist(image)</span></pre></li> <li><span class="koboSpan" id="kobo.519.1">Next, we have the Gaussian blur. </span><span class="koboSpan" id="kobo.519.2">This is an optional item – you may want to remove it if your room does not have a lot of detail. </span><span class="koboSpan" id="kobo.519.3">My game room has lots of furniture, so I think reducing the noise will </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">improve performance:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.521.1">
# gaussian blur the image to remove high frequency noise # we use a 5x kernel
image = cv2.GaussianBlur(img,(5,5),0)</span></pre></li> <li><span class="koboSpan" id="kobo.522.1">We convert the image to a NumPy array of floats scaled from 0 to 1, instead of a set of integers from 0 to 255. </span><span class="koboSpan" id="kobo.522.2">This neural network toolkit only permits NumPy arrays for inputs. </span><span class="koboSpan" id="kobo.522.3">We also put the number associated with the labels (left = </span><strong class="source-inline"><span class="koboSpan" id="kobo.523.1">0</span></strong><span class="koboSpan" id="kobo.524.1">, right=</span><strong class="source-inline"><span class="koboSpan" id="kobo.525.1">1</span></strong><span class="koboSpan" id="kobo.526.1">, and center = </span><strong class="source-inline"><span class="koboSpan" id="kobo.527.1">2</span></strong><span class="koboSpan" id="kobo.528.1">) into the matching </span><strong class="source-inline"><span class="koboSpan" id="kobo.529.1">labels</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.530.1">NumPy array:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.531.1">
# convert to a numpy array image = img_to_array(image)
# normalize the data to be from 0 to 1
image2 = np.array(image, dtype="float") / 255.0 images=images.append(image) labels.append(label)
labels = np.array(labels) # convert to array</span></pre></li> <li><span class="koboSpan" id="kobo.532.1">We split</span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.533.1"> the data into two parts – a training set that we use to train the neural network, and the testing set that we validate the training set with. </span><span class="koboSpan" id="kobo.533.2">We’ll use 80% of the image samples for training and 20% </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">for testing:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.535.1">
# split data into testing data and training data 80/20
(trainData, testData, trainLabel, testLabel) = train_test_split(data, labels, test_size=0.20, random_state=42)</span></pre></li> <li><span class="koboSpan" id="kobo.536.1">We have to convert the labels to a tensor, which is just a particular </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">data format:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.538.1">
# convert the labels from integers to vectors 
trainLabel = to_categorical(trainLabel, num_classes=3) testLabel = to_categorical(testLabel, num_classes=3)</span></pre></li> <li><span class="koboSpan" id="kobo.539.1">Now, we build our actual neural network by instantiating the </span><strong class="source-inline"><span class="koboSpan" id="kobo.540.1">ConvNet</span></strong><span class="koboSpan" id="kobo.541.1"> object, which actually builds our CNN in Keras. </span><span class="koboSpan" id="kobo.541.2">We set up the optimizer, which is </span><strong class="bold"><span class="koboSpan" id="kobo.542.1">Adaptive Moment Estimation</span></strong><span class="koboSpan" id="kobo.543.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.544.1">ADAM</span></strong><span class="koboSpan" id="kobo.545.1">), a</span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.546.1"> type of adaptive gradient descent. </span><span class="koboSpan" id="kobo.546.2">ADAM acts against the error gradient like a heavy ball with friction – it has some momentum, but does not pick up </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">speed quickly:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.548.1">
# initialize the artificial neural network print("compiling CNN...")
cnn = ConvNet.build(width=224, height=224, depth=1, classes=3) opt = Adam(lr=LEARN_RATE, decay=LEARN_RATE / EPOCHS) model.compile(loss="categorical_crossentropy", optimizer=opt, metrics=["accuracy"])</span></pre></li> <li><span class="koboSpan" id="kobo.549.1">We train the network in this step. </span><span class="koboSpan" id="kobo.549.2">This will take quite some time to complete – from 15 minutes to an hour or two – depending on how many images you have. </span><span class="koboSpan" id="kobo.549.3">We want the training to come out somewhere above 80%. </span><span class="koboSpan" id="kobo.549.4">If not, add some epochs to see where the learning curve levels off. </span><span class="koboSpan" id="kobo.549.5">If that still does not do the trick, you need more training images. </span><span class="koboSpan" id="kobo.549.6">I’m aiming for 1,000 images in each set, which is roughly 50 minutes of driving the </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">robot around:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.551.1">
# train the network
print("Training network. </span><span class="koboSpan" id="kobo.551.2">This will take a while")
trainedNetwork = model.fit_generator(aug.flow(trainImage, trainLabel, batch_size=BATCH),
validation_data=(testImage, testLable), steps_per_epoch=len(trainImage) // BATCH,
epochs=EPOCHS, verbose=1) # save the model to disk
print("Writing network to disk") cnn.save("nav_model")</span></pre></li> <li><span class="koboSpan" id="kobo.552.1">We are</span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.553.1"> all done now, so we save the model we created to disk so that we can transfer it to the robot’s computer, the Nvidia </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">Jetson Nano.</span></span></li>
<li><span class="koboSpan" id="kobo.555.1">Now, make your second training set of driving from random locations to the toy box. </span><span class="koboSpan" id="kobo.555.2">Pick random spots and use the joystick to drive the robot to the toy box from each. </span><span class="koboSpan" id="kobo.555.3">Keep going until you have 1,000 images or so. </span><span class="koboSpan" id="kobo.555.4">Run these through the training program and label this model </span><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">toybox_model</span></strong><span class="koboSpan" id="kobo.557.1"> by changing the last line of </span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">the program:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.559.1">
cnn.save("toybox_model")</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.560.1">This is great – we have built and trained our neural network. </span><span class="koboSpan" id="kobo.560.2">Now, we need to put it to use to drive the robot around, which we’ll do in the </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">next section.</span></span></p>
<h2 id="_idParaDest-124"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.562.1">CNN robot control implementation</span></h2>
<p><span class="koboSpan" id="kobo.563.1">We need to</span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.564.1"> combine a program that sends out ROS commands with our neural network classification process. </span><span class="koboSpan" id="kobo.564.2">I added some commands through the ROS </span><strong class="source-inline"><span class="koboSpan" id="kobo.565.1">syscommand</span></strong><span class="koboSpan" id="kobo.566.1"> topic, which I use for non-periodic commands to</span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.567.1"> my robots. </span><strong class="source-inline"><span class="koboSpan" id="kobo.568.1">syscommand</span></strong><span class="koboSpan" id="kobo.569.1"> just publishes a string, so you can use it for just about anything. </span><span class="koboSpan" id="kobo.569.2">You can follow </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">these steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.571.1">We start with our imports from ROS, OpenCV2, and Keras, as we will be combining functions from all </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">three libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.573.1">
import roslib import sys import rospy import cv2
from std_msgs.msg import String
from sensor_msgs.msg import Image
from geometry_msgs.msg import Twist
from cv_bridge import CvBridge, CvBridgeError
from keras.preprocessing.image import img_to_array
from keras.models import load_model
import numpy as np</span></pre></li> <li><span class="koboSpan" id="kobo.574.1">This first section is the ROS interface. </span><span class="koboSpan" id="kobo.574.2">I like to encapsulate the ROS interface this way, with all of the publish and subscribe in one place. </span><span class="koboSpan" id="kobo.574.3">We have several topics to set up – we need to be able to send and receive commands on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">syscommand</span></strong><span class="koboSpan" id="kobo.576.1"> topic. </span><span class="koboSpan" id="kobo.576.2">We will be publishing commands to the robot’s motors on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.577.1">cmd_vel</span></strong><span class="koboSpan" id="kobo.578.1"> topic. </span><span class="koboSpan" id="kobo.578.2">We receive images from the camera on </span><strong class="source-inline"><span class="koboSpan" id="kobo.579.1">image_topic</span></strong><span class="koboSpan" id="kobo.580.1">. </span><span class="koboSpan" id="kobo.580.2">We use callbacks to handle the event when a topic is published elsewhere on the robot. </span><span class="koboSpan" id="kobo.580.3">These can be called at any time. </span><span class="koboSpan" id="kobo.580.4">We have more control when we publish to a topic, which is handled using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.581.1">pubTwist</span></strong><span class="koboSpan" id="kobo.582.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.583.1">pubCmd</span></strong><span class="koboSpan" id="kobo.584.1"> methods. </span><span class="koboSpan" id="kobo.584.2">I added flags to received commands and images so that we don’t accidentally process the same image or </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">command twice:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.586.1">
class ROSIF():
 def  init (self):
  self.bridge = CvBridge()
  self.image_sub = rospy.Subscriber("image_topic",Image,self.callback)
  self.cmd_sub = rospy.Subscriber( "syscommand",String,self.cmdCallback) self.cmd_pub = rospy.Publisher( "syscommand",String,queue_size=10)
  self.twist_pub = rospy.Publisher("cmd_vel",Twist,queue_size=10)
  self.newImage = False
  self.cmdReceived=""
def callback(self):
 try:
  self.image = self.bridge.imgmsg_to_cv2(data, "bgr8") 
  self.newImage = True
 except CvBridgeError as e:
  print(e)
def cmdCallback(self,data):
 # receieve a message on syscommand
 self.cmdReceived = data.data
def getCmd(self):
 cmd = self.cmdReceived
 self.cmdReceived = "" # clear the command so we dont do it twice
 return cmd</span></pre></li> <li><span class="koboSpan" id="kobo.587.1">This next function is the means for the rest of the program to get the latest image from the camera system, which is published on ROS on </span><strong class="source-inline"><span class="koboSpan" id="kobo.588.1">image_topic</span></strong><span class="koboSpan" id="kobo.589.1">. </span><span class="koboSpan" id="kobo.589.2">We grab the</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.590.1"> latest image and set the </span><strong class="source-inline"><span class="koboSpan" id="kobo.591.1">newImage</span></strong><span class="koboSpan" id="kobo.592.1"> variable to </span><strong class="source-inline"><span class="koboSpan" id="kobo.593.1">False</span></strong><span class="koboSpan" id="kobo.594.1">, so that we know next time whether we are trying to process the same image twice in a row. </span><span class="koboSpan" id="kobo.594.2">Each time we get a new image, we set </span><strong class="source-inline"><span class="koboSpan" id="kobo.595.1">newImage</span></strong><span class="koboSpan" id="kobo.596.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.597.1">True</span></strong><span class="koboSpan" id="kobo.598.1">, and each time we use an image, we set </span><strong class="source-inline"><span class="koboSpan" id="kobo.599.1">newImage</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.600.1">to </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.601.1">False</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.603.1">
def getImage(self):
  if self.newImage=True:
    self.newImage = False
    # reset the flag so we don't process twice return self.image
    self.newImage = False
    # we send back a list with zero elements
    img = []
    return img</span></pre></li> <li><span class="koboSpan" id="kobo.604.1">This section</span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.605.1"> sends speed commands to the robot to match what the CNN output predicts for us to do. </span><span class="koboSpan" id="kobo.605.2">The output of the CNN is one of three values: left, right, or straight ahead. </span><span class="koboSpan" id="kobo.605.3">These come out of the neural network as one of three enumerated values – </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">0</span></strong><span class="koboSpan" id="kobo.607.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.608.1">1</span></strong><span class="koboSpan" id="kobo.609.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.610.1">2</span></strong><span class="koboSpan" id="kobo.611.1">. </span><span class="koboSpan" id="kobo.611.2">We convert them back to left, right, and center values, and then use that information to send a motion command to the robot. </span><span class="koboSpan" id="kobo.611.3">The robot uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.612.1">Twist</span></strong><span class="koboSpan" id="kobo.613.1"> message to send motor commands. </span><span class="koboSpan" id="kobo.613.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.614.1">Twist</span></strong><span class="koboSpan" id="kobo.615.1"> data message is designed to accommodate very complex robots, quadcopters, and omni-wheel drive systems that can move in any direction, so it has a lot of extra values. </span><span class="koboSpan" id="kobo.615.2">We send a </span><strong class="source-inline"><span class="koboSpan" id="kobo.616.1">Twist.linear.x</span></strong><span class="koboSpan" id="kobo.617.1"> command to set the speed of the robot forward and backward, and a </span><strong class="source-inline"><span class="koboSpan" id="kobo.618.1">Twist.angular.z</span></strong><span class="koboSpan" id="kobo.619.1"> value to set the rotation, or turning, of the base. </span><span class="koboSpan" id="kobo.619.2">In our case, a positive </span><strong class="source-inline"><span class="koboSpan" id="kobo.620.1">angular.z</span></strong><span class="koboSpan" id="kobo.621.1"> rotation value goes to the right, and a negative value to the left. </span><span class="koboSpan" id="kobo.621.2">Our last statement publishes the data values on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.622.1">cmd_vel</span></strong><span class="koboSpan" id="kobo.623.1"> topic as a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">Twist</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.625.1"> message:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.626.1">
# publishing commands back to the robot
def pubCmd(self,cmdstr):
  self.cmd_pub.publish(String(cmdstr)):
def pubTwist(self,cmd):
  if cmd == 0: # turn left 
    turn = -2
    speed = 1
  if cmd==1:
    turn = 2
    speed = 1
  if cmd ==3:
    turn=0 
    speed = 1 # all stop
  if cmd==4:
    turn = 0
      speed = 0
    cmdTwist = Twist()
    cmdTwist.linear.x = speed
    cmdTwist.angular.z = turn self.twist_pub.publish(cmdTwist)</span></pre></li> <li><span class="koboSpan" id="kobo.627.1">We </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.628.1">create a function to do all of our image processing with one command. </span><span class="koboSpan" id="kobo.628.2">This is the exact replica of how we preprocessed </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.629.1">the images for the training program – just as you might think. </span><span class="koboSpan" id="kobo.629.2">You may think it a bit strange that I scale the image up, only to then scale it down again. </span><span class="koboSpan" id="kobo.629.3">The reason for this is to have detail for the vertical part of the image. </span><span class="koboSpan" id="kobo.629.4">If I scaled it down to 240 x 240 and then cut it in half, I would be stretching pixels afterward to get it square again. </span><span class="koboSpan" id="kobo.629.5">I like having extra pixels when scaling down. </span><span class="koboSpan" id="kobo.629.6">The big advantage of this technique is that it does not matter what resolution the incoming image is at – we will end up with the correctly sized and </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">cropped image.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.631.1">The other </span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.632.1">steps involve converting the image to grayscale, performing an equalization on the contrast range, which expands our color values to fill the available space, and performing a Gaussian blur to reduce noise. </span><span class="koboSpan" id="kobo.632.2">We normalize the image for the neural network by converting </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.633.1">our integer 0-255 grayscale values to floating point values from 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">to 1:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.635.1">
def processImage(img):
# need to process the image
image = cv2.resize(image, (640, 480))
halfImage = 640*240 # half the pixels
# cut the image in half -we take the top half image = image[0:halfimage]
#size the image to what we want to put into the neural network
image=cv2.resize(image,(224,224))
# convert to grayscale
image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
 #equalize the image to use the ful
     image = cv2.equalizeHist(image)
# gaussian blur the image to remove high freqency noise # we use a 5x kernel
image = cv2.GaussianBlur(img,(5,5),0) # convert to a numpy array
image = img_to_array(image)
# normalize the data to be from 0 to 1
image2 = np.array(image, dtype="float") / 255.0 return image2</span></pre></li> <li><span class="koboSpan" id="kobo.636.1">Now that we’re</span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.637.1"> set up, we go into the main program. </span><span class="koboSpan" id="kobo.637.2">We have to initialize our ROS node so that we can talk to the ROS publish/subscribe system. </span><span class="koboSpan" id="kobo.637.3">We create a variable, mode, that we use to control what branch of processing to go down. </span><span class="koboSpan" id="kobo.637.4">We make an interface to allow the operator to turn the navigation function on and off, and to select between normal </span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.638.1">navigation and our </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">toy-box-seeking mode.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.640.1">In this first section, we will load both neural network models that we </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">trained before:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.642.1">
# MAIN PROGRAM
ic = image_converter()
rosif = ROSIF()
rospy.init_node('ROS_cnn_nav')
mode = "OFF"
# load the model for regular navigation
navModel = load_model("nav_model")
toyboxModel = load_model("toybox_model")</span></pre></li> <li><span class="koboSpan" id="kobo.643.1">This section begins the processing loop that runs while the program is active. </span><span class="koboSpan" id="kobo.643.2">Running </span><strong class="source-inline"><span class="koboSpan" id="kobo.644.1">rospy.spin()</span></strong><span class="koboSpan" id="kobo.645.1"> tells the ROS system to process any message that may be waiting for us. </span><span class="koboSpan" id="kobo.645.2">Our final step is to pause the program for 0.02 seconds to allow the Raspberry Pi to process other data and run </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">other programs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.647.1">
while not rospy.is_shutdown():
  rospy.spin()
  time.sleep(0.02)</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.648.1">So, that concludes our navigation chapter. </span><span class="koboSpan" id="kobo.648.2">We’ve covered both obstacle avoidance and room navigation </span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.649.1">using a neural network to teach the robot </span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.650.1">to drive about using landmarks on the ceiling – and without </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">a map.</span></span></p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.652.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.653.1">This chapter introduced some concepts for robot navigation in an unstructured environment, which is to say, in the real world, where the designers of the robot don’t have control over the content of the space. </span><span class="koboSpan" id="kobo.653.2">We started by introducing SLAM, along with some of the strengths and weaknesses of map-based navigation. </span><span class="koboSpan" id="kobo.653.3">We talked about how Roomba navigates, by random interaction and statistical models. </span><span class="koboSpan" id="kobo.653.4">The method selected for our toy-gathering robot project, Albert, combined two algorithms that both relied mostly on </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">vision sensors.</span></span></p>
<p><span class="koboSpan" id="kobo.655.1">The first was the Floor Finder, a technique I learned when it was used by the winning entry in the DARPA Grand Challenge. </span><span class="koboSpan" id="kobo.655.2">The Floor Finder algorithm uses the near vision (next to the robot) to teach the far vision (away from the robot) what the texture of the floor is. </span><span class="koboSpan" id="kobo.655.3">We can then divide the room into things that are safe to drive on, and things that are not safe. </span><span class="koboSpan" id="kobo.655.4">This deals with our obstacle avoidance. </span><span class="koboSpan" id="kobo.655.5">Our navigation technique used a trained neural network to identify the path around the room by associating images of the room from the horizon up (the top half of the room) with directions to travel. </span><span class="koboSpan" id="kobo.655.6">This also served to teach the robot to stay away from the stairs. </span><span class="koboSpan" id="kobo.655.7">We discarded the bottom half of the room from the image for the neural network because that is where the toys are. </span><span class="koboSpan" id="kobo.655.8">We used the same process to train another neural network to find the </span><span class="No-Break"><span class="koboSpan" id="kobo.656.1">toy box.</span></span></p>
<p><span class="koboSpan" id="kobo.657.1">This process was the same as we saw in </span><a href="B19846_04.xhtml#_idTextAnchor126"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.658.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.659.1">, but the training images were all labeled with the path from that spot to the toy box. </span><span class="koboSpan" id="kobo.659.2">This combination gave us the ability to teach the robot to find its way around by vision, and without a map, just like </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">you do.</span></span></p>
<p><span class="koboSpan" id="kobo.661.1">In the next chapter, we’ll cover classifying objects, and review some other </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">path-planning methods.</span></span></p>
<h1 id="_idParaDest-126"><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.663.1">Questions</span></h1>
<ol>
<li><span class="koboSpan" id="kobo.664.1">Regarding SLAM, what sensor is most commonly used to create the data that SLAM needs to make </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">a map?</span></span></li>
<li><span class="koboSpan" id="kobo.666.1">Why does SLAM work better with wheel odometer </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">data available?</span></span></li>
<li><span class="koboSpan" id="kobo.668.1">In the Floor Finder algorithm, what does the Gaussian blur function do to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">the results?</span></span></li>
<li><span class="koboSpan" id="kobo.670.1">The final step in the Floor Finder is to trace upward from the robot position to the first red pixel. </span><span class="koboSpan" id="kobo.670.2">In what other way can this step be accomplished (referring to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.671.1">Figure 7</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.672.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">)?</span></span></li>
<li><span class="koboSpan" id="kobo.674.1">Why did we cut the image in half horizontally before doing our neural </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">network processing?</span></span></li>
<li><span class="koboSpan" id="kobo.676.1">What advantages does using the neural network approach provide that a technique such as SLAM </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">does not?</span></span></li>
<li><span class="koboSpan" id="kobo.678.1">If we used just a random driving function instead of the neural network, what new program or function would we have to add to the robot to achieve the </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">same results?</span></span></li>
<li><span class="koboSpan" id="kobo.680.1">How did we end up avoiding the stairs in the approach presented in the chapter? </span><span class="koboSpan" id="kobo.680.2">Do you feel this is adequate? </span><span class="koboSpan" id="kobo.680.3">Would you suggest any other means for accomplishing </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">this task?</span></span></li>
</ol>
<h1 id="_idParaDest-127"><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.682.1">Further reading</span></h1>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.683.1">Deep Obstacle Avoidance</span></em><span class="koboSpan" id="kobo.684.1"> by Sullivan and Lawson, published by Naval Research Labs, </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">Rosebrock, Adrian.</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.686.1">Artificial Intelligence with Python Cookbook</span></em><span class="koboSpan" id="kobo.687.1"> by Ben Auffarth, Packt </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">Publishing, 2020</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.689.1">Artificial Intelligence with Python – Second Edition</span></em><span class="koboSpan" id="kobo.690.1">, by Prateek Joshi, Packt </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">Publishing, 2020</span></span></li>
<li><em class="italic"><span class="koboSpan" id="kobo.692.1">Python Image Processing Cookbook</span></em><span class="koboSpan" id="kobo.693.1"> by Sandipan Dey, Packt </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">Publishing, 2020</span></span></li>
</ul>
</div>
</body></html>