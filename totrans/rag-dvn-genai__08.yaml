- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dynamic RAG with Chroma and Hugging Face Llama
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will take you into the pragmatism of dynamic RAG. In today’s rapidly
    evolving landscape, the ability to make swift, informed decisions is more crucial
    than ever. Decision-makers across various fields—from healthcare and scientific
    research to customer service management—increasingly require real-time data that
    is relevant only within the short period it is needed. A meeting may only require
    temporary yet highly prepared data. Hence, the concept of data permanence is shifting.
    Not all information must be stored indefinitely; instead, in many cases, the focus
    is shifting toward using precise, pertinent data tailored for specific needs at
    specific times, such as daily briefings or critical meetings.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces an innovative and efficient approach to handling such
    data through the embedding and creation of temporary Chroma collections. Each
    morning, a new collection is assembled containing just the necessary data for
    that day’s meetings, effectively avoiding long-term data accumulation and management
    overhead. This data might include medical reports for a healthcare team discussing
    patient treatments, customer interactions for service teams strategizing on immediate
    issues, or the latest scientific research data for researchers making day-to-day
    experimental decisions. We will then build a Python program to support dynamic
    and efficient decision-making in daily meetings, applying a methodology using
    a hard science (any of the natural or physical sciences) dataset for a daily meeting.
    This approach will highlight the flexibility and efficiency of modern data management.
    In this case, the team wants to obtain pertinent scientific information without
    searching the web or interacting with online AI assistants. The constraint is
    to have a free, open-source assistant that anyone can use, which is why we will
    use Chroma and Hugging Face resources.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a temporary Chroma collection. We will simulate
    the processing of a fresh dataset compiled daily, tailored to the specific agenda
    of upcoming meetings, ensuring relevance and conciseness. In this case, we will
    download the SciQ dataset from Hugging Face, which contains thousands of crowdsourced
    science questions, such as those related to physics, chemistry, and biology. Then,
    the program will embed the relevant data required for the day, guaranteeing that
    all discussion points are backed by the latest, most relevant data.
  prefs: []
  type: TYPE_NORMAL
- en: A user might choose to run queries before the meetings to confirm their accuracy
    and alignment with the day’s objective. Finally, as meetings progress, any arising
    questions trigger real-time data retrieval, augmented through **Large Language
    Model Meta AI** (**Llama**) technology to generate dynamic flashcards. These flashcards
    provide quick and precise responses to ensure discussions are both productive
    and informed. By the end of this chapter, you will have acquired the skills to
    implement open-source free dynamic RAG in a wide range of domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum that up, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of dynamic RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a dataset for dynamic RAG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Chroma collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding and upserting data in a Chroma collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch-querying a collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Querying a collection with a user request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Augmenting the input with the output of a query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Hugging Face’s framework for Meta Llama
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating a response based on the augmented input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by going through the architecture of dynamic RAG.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of dynamic RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you’re in a dynamic environment in which information changes daily.
    Each morning, you gather a fresh batch of 10,000+ questions and validated answers
    from across the globe. The challenge is to access this information quickly and
    effectively during meetings without needing long-term storage or complicated infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: This dynamic RAG method allows us to maintain a lean, responsive system that
    provides up-to-date information without the burden of ongoing data storage. It’s
    perfect for environments where data relevance is short-lived but critical for
    decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be applying this to a hard science dataset. However, this dynamic approach
    isn’t limited to our specific example. It has broad applications across various
    domains, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer support**: Daily updated FAQs can be accessed in real-time to provide
    quick responses to customer inquiries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Healthcare**: During meetings, medical teams can use the latest research
    and patient data to answer complex health-related questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Finance**: Financial analysts can query the latest market data to make informed
    decisions on investments and strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education**: Educators can access the latest educational resources and research
    to answer questions and enhance learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tech support**: IT teams can use updated technical documentation to solve
    issues and guide users effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sales and marketing**: Teams can quickly access the latest product information
    and market trends to answer client queries and strategize.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter implements one type of a dynamic RAG ecosystem. Your imagination
    is the limit, so feel free to apply this ecosystem to your own projects in different
    ways. For now, let’s see how the dynamic RAG components fit into the ecosystem
    we described in *Chapter 1*, *Why Retrieval Augmented Generation?*, in the *RAG
    ecosystem* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will streamline the integration and use of dynamic information in real-time
    decision-making contexts, such as daily meetings, in Python. Here’s a breakdown
    of this innovative strategy for each component and its ecosystem component label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](img/B31169_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: The dynamic RAG system'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporary Chroma collection creation (D1, D2, D3, E2)**: Every morning, a
    temporary Chroma collection is set up specifically for that day’s meeting. This
    collection is not meant to be saved post-meeting, serving only the day’s immediate
    needs and ensuring that data does not clutter the system in the long term.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding relevant data (D1, D2, D3, E2)**: The collection embeds critical
    data, such as customer support interactions, medical reports, or scientific facts.
    This embedding process tailors the content specifically to the meeting agenda,
    ensuring that all pertinent information is at the fingertips of the meeting participants.
    The data could include human feedback from documents and possibly other generative
    AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pre-meeting data validation (D4)**: Before the meeting begins, a batch of
    queries is run against this temporary Chroma collection to ensure that all data
    is accurate and appropriately aligned with the meeting’s objectives, thereby facilitating
    a smooth and informed discussion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time query handling (G1, G2, G3, G4)**: During the meeting, the system
    is designed to handle spontaneous queries from participants. A single question
    can trigger the retrieval of specific information, which is then used to augment
    Llama’s input, enabling it to generate flashcards dynamically. These flashcards
    are utilized to provide concise, accurate responses during the meeting, enhancing
    the efficiency and productivity of the discussion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will be using Chroma, a powerful, open-source, AI-native vector database
    designed to store, manage, and search embedded vectors in collections. Chroma
    contains everything we need to start, and we can run it on our machine. It is
    also very suitable for applications involving LLMs. Chroma collections are thus
    suitable for a temporary, cost-effective, and real-time RAG system. The dynamic
    RAG architecture of this chapter implemented with Chroma is innovative and practical.
    Here are some key points to consider in this fast-moving world:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Efficiency and cost-effectiveness**: Using Chroma for temporary storage and
    Llama for response generation ensures that the system is lightweight and doesn’t
    incur ongoing storage costs. This makes it ideal for environments where data is
    refreshed frequently and long-term storage isn’t necessary. It is very convincing
    for decision-makers who want lean systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: The system’s ephemeral nature allows for the integration of
    new data daily, ensuring that the most up-to-date information is always available.
    This can be particularly valuable in fast-paced environments in which information
    changes rapidly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: The approach is scalable to other similar datasets, provided
    they can be embedded and queried effectively. This makes it adaptable to various
    domains beyond the given example. Scaling is not only increasing volumes of data
    but also the ability to apply a framework to a wide range of domains and situations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User-friendliness**: The system’s design is straightforward, making it accessible
    to users who may not be deeply technical but need reliable answers quickly. This
    simplicity can enhance user engagement and satisfaction. Making users happy with
    cost-effective, transparent, and lightweight AI will surely boost their interest
    in RAG-driven generative AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now begin building a dynamic RAG program.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The environment focuses on open-source and free resources that we can run on
    our machine or a free Google Colab account. This chapter will run these resources
    on Google Colab with Hugging Face and Chroma.
  prefs: []
  type: TYPE_NORMAL
- en: We will first install Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will implement Hugging Face’s open-source resources to download a dataset
    for the Llama model. Sign up at [https://huggingface.co/](https://huggingface.co/)
    to obtain your Hugging Face API token. If you are using Google Colab, you can
    create a Google Secret in the sidebar and activate it. If so, you can comment
    the following cell—`# Save your Hugging Face token in a secure location`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The program first retrieves the Hugging Face API token. Make sure to store
    it in a safe place. You can choose to use Google Drive or enter it manually. Up
    to now, the installation seems to have run smoothly. We now install `datasets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: However, there are conflicts, such as `pyarrow`, with Google Colab’s pre-installed
    version, which is more recent. These conflicts between fast-moving packages are
    frequent. When Hugging Face updates its packages, this conflict will not appear
    anymore. But other conflicts may appear. This conflict will not stop us from downloading
    datasets. If it did, we would have to uninstall Google Colab packages and reinstall
    `pyarrow`, but other dependencies may possibly create issues. We must accept these
    challenges, as explained in the *Setting up the environment* section in *Chapter
    2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now install Hugging Face’s `transformers` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We also install accelerate to run PyTorch packages on GPUs, which is highly
    recommended for this notebook, among other features, such as mixed precision and
    accelerated processing times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will initialize `meta-llama/Llama-2-7b-chat-hf` as the tokenizer
    and chat model interactions. Llama is a series of transformer-based language models
    developed by Meta AI (formerly Facebook AI) that we can access through Hugging
    Face:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We access the model through Hugging Face’s pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transformers.pipeline` is the function used to create a pipeline for text
    generation. This pipeline abstracts away much of the complexity we must avoid
    in this dynamic RAG ecosystem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text-generation` specifies the type of task the pipeline is set up for. In
    this case, we want text generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model` specifies the model we selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch_dtype=torch.float16` sets the data type for PyTorch tensors to `float16`.
    This is a key factor for dynamic RAG, which reduces memory consumption and can
    speed up computation, particularly on GPUs that support half-precision computations.
    Half-precision computations use 16 bits: half of the standard 32-bit precision,
    for faster, lighter processing. This is exactly what we need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`device_map="auto"` instructs the pipeline to automatically determine the best
    device to run the model on (CPU, GPU, multi-GPU, etc.). This parameter is particularly
    important for optimizing performance and automatically distributing the model’s
    layers across available devices (like GPUs) in the most efficient manner possible.
    If multiple GPUs are available, it will distribute the load across them to maximize
    parallel processing. If you have access to a GPU, activate it to speed up the
    configuration of this pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face is ready; Chroma is required next.
  prefs: []
  type: TYPE_NORMAL
- en: Chroma
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following line installs Chroma, our open-source vector database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a close look at the following excerpt output, which displays the packages
    installed and, in particular, **Open Neural Network Exchange** (**ONNX**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ONNX ([https://onnxruntime.ai/](https://onnxruntime.ai/)) is a key component
    in this chapter’s dynamic RAG scenario because it is fully integrated with Chroma.
    ONNX is a standard format for representing **machine learning** (**ML**) models
    designed to enable models to be used across different frameworks and hardware
    without being locked into one ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using ONNX Runtime, which is a performance-focused engine for running
    ONNX models. It acts as a cross-platform accelerator for ML models, providing
    a flexible interface that allows integration with hardware-specific libraries.
    This makes it possible to optimize the models for various hardware configurations
    (CPUs, GPUs, and other accelerators). As for Hugging Face, it is recommended to
    activate a GPU if you have access to one for the program in this chapter. Also,
    we will select a model included within ONNX Runtime installation packages.
  prefs: []
  type: TYPE_NORMAL
- en: We have now installed the Hugging Face and Chroma resources we need, including
    ONNX Runtime. Hugging Face’s framework is used throughout the model life cycle,
    from accessing and deploying pre-trained models to training and fine-tuning them
    within its ecosystem. ONNX, among its many features, can intervene in the post-training
    phase to ensure a model’s compatibility and efficient execution across different
    hardware and software setups. Models might be developed and fine-tuned using Hugging
    Face’s tools and then converted to the ONNX format for broad, optimized deployment
    using ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use spaCy to compute the accuracy between the response we obtain
    when querying our vector store and the original completion text. The following
    command installs a medium-sized English language model from spaCy, tailored for
    general NLP tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This model, labeled `en_core_web_md`, originates from web text in English and
    is balanced for speed and accuracy, which we need for dynamic RAG. It is efficient
    for computing text similarity. You may need to restart the session once the package
    is installed.
  prefs: []
  type: TYPE_NORMAL
- en: We have now successfully installed the open-source, optimized, cost-effective
    resources we need for dynamic RAG and are ready to start running the program’s
    core.
  prefs: []
  type: TYPE_NORMAL
- en: Activating session time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working in real-life dynamic RAG projects, such as in this scenario, time
    is essential! For example, if the daily decision-making meeting is at 10 a.m.,
    the RAG preparation team might have to start preparing for this meeting at 8 a.m.
    to gather the data online, in processed company data batches, or in any other
    way necessary for the meeting’s goal.
  prefs: []
  type: TYPE_NORMAL
- en: First, activate a GPU if one is available. On Google Colab, for example, go
    to **Runtime** | **Change runtime type** and select a GPU if possible and available.
    If not, the notebook will take a bit longer but will run on a CPU. Then, go through
    each section in this chapter, running the notebook cell by cell to understand
    the process in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code activates a measure of the session time once the environment
    is installed all the way to the end of the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Finally, restart the session, go to **Runtime** again, and click on **Run all**.
    Once the program is finished, go to **Total session time**, the last section of
    the notebook. You will have an estimate of how long it takes for a preparation
    run. With the time left before a daily meeting, you can tweak the data, queries,
    and model parameters for your needs a few times.
  prefs: []
  type: TYPE_NORMAL
- en: This on-the-fly dynamic RAG approach will make any team that has these skills
    a precious asset in this fast-moving world. We will start the core of the program
    by downloading and preparing the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and preparing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the SciQ dataset created by Welbl, Liu, and Gardner (2017) with
    a method for generating high-quality, domain-specific multiple-choice science
    questions via *crowdsourcing*. The SciQ dataset consists of 13,679 multiple-choice
    questions crafted to aid the training of NLP models for science exams. The creation
    process involves two main steps: selecting relevant passages and generating questions
    with plausible distractors.'
  prefs: []
  type: TYPE_NORMAL
- en: In the context of using this dataset for an augmented generation of questions
    through a Chroma collection, we will implement the `question`, `correct_answer`,
    and `support` columns. The dataset also contains `distractor` columns with wrong
    answers, which we will drop.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will integrate the prepared dataset into a retrieval system that utilizes
    query augmentation techniques to enhance the retrieval of relevant questions based
    on specific scientific topics or question formats for Hugging Face’s Llama model.
    This will allow for the dynamic generation of augmented, real-time completions
    for Llama, as implemented in the chapter’s program. The program loads the training
    data from the `sciq` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset is filtered to detect the non-empty `support` and `correct_answer`
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now display the number of rows filtered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that we have 10,481 documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to clean the DataFrame to focus on the columns we need. Let’s drop
    the distractors (wrong answers to the questions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the correct answer and the support content that we will now merge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows the columns we need to prepare the data for retrieval in the
    completion columns, as shown in the excerpt of the DataFrame for a completion
    field in which `aerobic` is the correct answer because it is the connector and
    the rest of the text is the support content for the correct answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The program now displays the shape of the DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows we still have all the initial lines and four columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will display the names of the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, the output displays the four columns we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The data is now ready to be embedded and upserted.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding and upserting the data in a Chroma collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin by creating the Chroma client and defining a collection name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Before creating the collection and upserting the data to the collection, we
    need to verify whether the collection already exists or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will return `True` if the collection exists and `False` if it doesn’t:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If the collection doesn’t exist, we will create a collection with `collection_name`
    defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s peek into the structure of the dictionary of the collection we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the dictionary of each item of the collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s briefly go through the three key fields for our scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ids`: This field represents the unique identifiers for each item in the collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embeddings`: Embeddings are the embedded vectors of the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`documents`: This refers to the `completion` column in which we merged the
    correct answer and the support content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now need a lightweight rapid LLM model for our dynamic RAG environment.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Chroma will initialize a default model, which can be `all-MiniLM-L6-v2`. However,
    let’s make sure we are using this model and initialize it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The `all-MiniLM-L6-v2` model was designed with an optimal, enhanced method by
    Wang et al. (2021) for model compression, focusing on distilling self-attention
    relationships between components of transformer models. This approach is flexible
    in the number of attention heads between teacher and student models, improving
    compression efficiency. The model is fully integrated into Chroma with ONNX, as
    explained in the *Installing the environment* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The magic of this `MiniLM` model is based on compression and knowledge distillation
    through a teacher model and the student model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Teacher model**: This is the original, typically larger and more complex
    model such as BERT, RoBERTa, and XLM-R, in our case, that has been pre-trained
    on a comprehensive dataset. The teacher model possesses high accuracy and a deep
    understanding of the tasks it has been trained on. It serves as the source of
    knowledge that we aim to transfer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Student model**: This is our smaller, less complex model, `all-MiniLM-L6-v2`,
    which is trained to mimic the teacher model’s behavior, which will prove very
    effective for our dynamic RAG architecture. The goal is to have the student model
    replicate the performance of the teacher model as closely as possible but with
    significantly fewer parameters or computational expense.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, `all-MiniLM-L6-v2` will accelerate the embedding and querying process.
    We can see that in the age of superhuman LLM models, such as GPT-4o, we can perform
    daily tasks with smaller compressed and distilled models. Let’s embed the data
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding and storing the completions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Embedding and upserting data in a Chroma collection is seamless and concise.
    In this scenario, we’ll embed and upsert the whole `df` completions in a `completion_list`
    extracted from our `df` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We use the `collection_exists` status we defined when creating the collection
    to avoid loading the data twice. In this scenario, the collection is temporary;
    we just want to load it once and use it once. If you try to load the data in this
    temporary scenario a second time, you will get warnings. However, you can modify
    the code if you wish to try different datasets and methods, such as preparing
    a prototype at full speed for another project.
  prefs: []
  type: TYPE_NORMAL
- en: 'In any case, in this scenario, we first check if the collection exists and
    then upsert the `ids` and `documents` in the `complete_list` and store the `type`
    of data, which is `completion`, in the `metadatas` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we measure the response time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that, in this case, Chroma activated the default model through
    `onnx`, as explained in the introduction of this section and also in the *Installing
    the environment* section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output also shows that the processing time for 10,000+ documents is satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The response time might vary and depends on whether you are using a GPU. When
    using an accessible GPU, the time fits the needs required for dynamic RAG scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: With that, the Chroma vector store is now populated. Let’s take a peek at the
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying the embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The program now fetches the embeddings and displays the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that our completions have been vectorized, as we can see in
    the first embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output also displays the embedding length, which is interesting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The `all-MiniLM-L6-v2` model reduces the complexity of text data by mapping
    sentences and paragraphs into a 384-dimensional space. This is significantly lower
    than the typical dimensionality of one-hot encoded vectors, such as the 1,526
    dimensions of the OpenAI `text-embedding-ada-002`. This shows that `all-MiniLM-L6-v2`
    uses dense vectors, which use all dimensions of the vector space to encode information
    to produce nuanced semantic relationships between different documents as opposed
    to sparse vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse vector models, such as the **bag-of-words** (**BoW**) model, can be effective
    in some cases. However, their main limitation is that they don’t capture the order
    of words or the context around them, which can be crucial for understanding the
    meaning of text when training LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: We have now embedded the documents into dense vectors in a smaller dimensional
    space than full-blown LLMs and will produce satisfactory results.
  prefs: []
  type: TYPE_NORMAL
- en: Querying the collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code in this section executes a query against the Chroma vector store using
    its integrated semantic search functionality. It queries the vector representations
    of all the vectors in the Chroma collection questions in the initial dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The query requests one most relevant or similar document for each question with
    `n_results=1`, which you can modify if you wish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each question text is converted into a vector. Then, Chroma runs a vector similarity
    search by comparing the embedded vectors against our database of document vectors
    to find the closest match based on vector similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays a satisfactory response time for the 10,000+ queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now analyze the 10,000+ queries. We will use spaCy to evaluate a query’s
    result and compare it with the original completion. We first load the spaCy model
    we installed in the *Installing the environment* section of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The program then creates a similarity function that takes two arguments (the
    original completion, `text1`, and the retrieved text, `text2`) and returns the
    similarity value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now perform a full validation run on the 10,000 queries. As can be
    seen in the following code block, the validation begins by defining the variables
    we will need:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nbqd` to only display the first 100 and last 100 results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acc_counter` measures the results with a similarity score superior to 0.5,
    which you can modify to fit your needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`display_counter` to count the number of results we have displayed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The program goes through `nb` results, which, in our case, is the total length
    of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The code accesses the original completion and stores it in `original_completion`.
    Then, it retrieves the result and stores it in `retrieved_document`. Finally,
    it calls the similarity function we defined, `simple_text_similarity`. The original
    completion and the retrieved document store the similarity score in `similarity_score`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we introduce an accuracy metric. In this scenario, the threshold of the
    similarity score is set to `0.7`, which is reasonable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'If `similarity_score > 0.7`, then the accuracy counter, `acc_counter`, is incremented.
    The display counter, `display_counter`, is also incremented to only the first
    and last `nbqd` (maximum results to display) defined at the beginning of this
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The information displayed provides insights into the performance of the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays four key variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{q}` is the question asked, the query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{retrieved_document}` is the document retrieved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{original_completion}` is the original document in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{similarity_score:.2f}` is the similarity score between the original document
    and the document retrieved to measure the performance of each response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first output provides the information required for a human observer to control
    the result of the query and trace it back to the source.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first part of the output is the question, the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part of the output is the retrieved document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The third part of the output is the original completion. In this case, we can
    see that the retrieved document provides relevant information but not the exact
    original completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the output displays the similarity score calculated by spaCy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The score shows that although the original completion was not selected, the
    completion selected is relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'When all the results have been analyzed, the program calculates the accuracy
    obtained for the 10,000+ queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The calculation is based on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Acc` is the overall accuracy obtained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acc_counter` is the total of `Similarity` `scores > 0.7`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nb` is the number of queries. In this case, `nb=len(df)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`acc=acc_counter/nb` calculates the overall accuracy of all the results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code then displays the number of documents measured and the overall similarity
    score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that all the questions returned relevant results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This satisfactory overall similarity score shows that the system works in a
    closed environment. But we need to go further and see what happens in the open
    environment of heated discussions in a meeting!
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and retrieval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section is the one to use during real-time querying meetings. You can adapt
    the interface to your needs. We’ll focus on functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the first prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that there are two commented variants under the first prompt.
    Let’s clarify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initial question` is the exact text that comes from the initial dataset. It
    isn’t likely that an attendee in the meeting or a user will ask the question that
    way. But we can use it to verify if the system is working.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variant 1` is similar to the initial question and could be asked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`variant 2` diverges and may prove challenging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will select `variant 1` for this section and we should obtain a satisfactory
    result.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that, as for all AI programs, human control is mandatory! The more
    `variant 2` diverges with spontaneous questions, the more challenging it becomes
    for the system to remain stable and respond as we expect. This limit explains
    why, even if a dynamic RAG system can adapt rapidly, designing a solid system
    will require careful and continual improvements.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we query the collection as we did in the previous section with one prompt
    only this time, we will obtain a response rapidly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The response time is rapid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the retrieved document is relevant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully retrieved the result of our query. This semantic vector
    search might even be enough if the attendees of the meeting are satisfied with
    it. You will always have time to improve the configuration of RAG with Llama.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Llama will now take this response and write a brief NLP summary.
  prefs: []
  type: TYPE_NORMAL
- en: RAG with Llama
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We initialized `meta-llama/Llama-2-7b-chat-hf` in the *Installing the environment*
    section. We must now create a function to configure Llama 2’s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'You can tweak each parameter to your expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prompt`: The input text that the model uses to generate the output. It’s the
    starting point for the model’s response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`do_sample`: A Boolean value (`True` or `False`). When set to `True`, it enables
    stochastic sampling, meaning the model will pick tokens randomly based on their
    probability distribution, allowing for more varied outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_k`: This parameter limits the number of highest-probability vocabulary
    tokens to consider when selecting tokens in the sampling process. Setting it to
    `10` means the model will choose from the top 10 most likely next tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_return_sequences`: Specifies the number of independently generated responses
    to return. Here, it is set to `1`, meaning the function will return one sequence
    for each prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eos_token_id`: This token marks the end of a sequence in tokenized form. Once
    it is generated, the model stops generating further tokens. The end-of-sequence
    token is an `id` that points to Llama’s `eos_token`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_new_tokens`: Limits the number of new tokens the model can generate. Set
    to `100` here, it constrains the output to a maximum length of 100 tokens beyond
    the input prompt length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature`: This controls randomness in the sampling process. A temperature
    of `0.5` makes the model’s responses less random and more focused than a higher
    temperature but still allows for some diversity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`repetition_penalty`: A modifier that discourages the model from repeating
    the same token. A penalty of `2.0` means any token already used is less likely
    to be chosen again, promoting more diverse and less repetitive text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`truncation`: When enabled, it ensures the output does not exceed the maximum
    length specified by `max_new_tokens` by cutting off excess tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The prompt will contain the instruction for Llama in `iprompt` and the result
    obtained in the *Prompt and retrieval* section of the notebook. The result is
    appended to `iprompt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The augmented input for the Llama call is `lprompt`. The code will measure
    the time it takes and make the completion request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'We now retrieve the generated text from the response and display the time it
    took for Llama to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that Llama returned the completion in a reasonable time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s wrap the response in a nice format to display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays a technically reasonable completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: The summary produced by Llama is technically acceptable. To obtain another,
    possibly better result, as long as the session is not closed, the user can run
    a query and an augmented generation several times with different Llama parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can even try another LLM. Dynamic RAG doesn’t necessarily have to be 100%
    open-source. If necessary, we must be pragmatic and introduce whatever it takes.
    For example, the following prompt was submitted to ChatGPT with GPT-4o, which
    is the result of the query we used for Llama:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of OpenAI GPT-4o surpasses Llama 2 in this case and produces a satisfactory
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: If necessary, you can replace `meta-llama/Llama-2-7b-chat-hf` with GPT-4o, as
    implemented in *Chapter 4*, *Multimodal Modular RAG for Drone Technology*, and
    configure it to obtain this level of output. The only rule in dynamic RAG is performance.
    With that, we’ve seen that there are many ways to implement dynamic RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Once the session is over, we can delete it.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting the collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can manually delete the collection with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also close the session to delete the temporary dynamic RAG collection
    created. We can check and see whether the collection we created, `collection_name`,
    still exists or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'If we are still working on a collection in a session, the response will be
    `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: If we delete the collection with code or by closing the session, the response
    will be `False`. Let’s take a look at the total session time.
  prefs: []
  type: TYPE_NORMAL
- en: Total session time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code measures the time between the beginning of the session and
    immediately after the *Installing the environment* section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can have two meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: It can measure the time we worked on the preparation of the dynamic RAG scenario
    with the daily dataset for the Chroma collection, querying, and summarizing by
    Llama.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can measure the time it took to run the whole notebook without intervening
    at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this case, the session time is the result of a full run with no human intervention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The whole process takes less than 15 minutes, which fits the constraints of
    the preparation time in a dynamic RAG scenario. It leaves room for a few runs
    to tweak the system before the meeting. With that, we have successfully walked
    through a dynamic RAG process and will now summarize our journey.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a fast-evolving world, gathering information rapidly for decision-making
    provides a competitive advantage. Dynamic RAG is one way to bring AI into meeting
    rooms with rapid and cost-effective AI. We built a system that simulated the need
    to obtain answers to hard science questions in a daily meeting. After installing
    and analyzing the environment, we downloaded and prepared the SciQ dataset, a
    science question-and-answer dataset, to simulate a daily meeting during which
    hard science questions would be asked. The attendees don’t want to spend their
    time searching the web and wasting their time when decisions must be made. This
    could be for a marketing campaign, fact-checking an article, or any other situation
    in which hard science knowledge is required.
  prefs: []
  type: TYPE_NORMAL
- en: We created a Chroma collection vector store. We then embedded 10,000+ documents
    and inserted data and vectors into the Chroma vector store on our machine with
    `all-MiniLM-L6-v2`. The process proved cost-effective and sufficiently rapid.
    The collection was created locally, so there is no storage cost. The collection
    is temporary, so there is no useless space usage or cluttering. We then queried
    the collection to measure the accuracy of the system we set up. The results were
    satisfactory, so we processed the full dataset to confirm. Finally, we created
    the functionality for a user prompt and query function to use in real time during
    a meeting. The result of the query augmented the user’s input for `meta-llama/Llama-2-7b-chat-hf`,
    which transformed the query into a short summary.
  prefs: []
  type: TYPE_NORMAL
- en: The dynamic RAG example we implemented would require more work before being
    released into production. However, it provides a path to open-source, lightweight,
    RAG-driven generative AI for rapid data collection, embedding, and querying. If
    we need to store the retrieval data and don’t want to create large vector stores,
    we can integrate our datasets in an OpenAI GPT-4o-mini model, for example, through
    fine-tuning, as we will see in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with *Yes* or *No*:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the script ensure that the Hugging Face API token is never hardcoded directly
    into the notebook for security reasons?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the chapter’s program, is the `accelerate` library used here to facilitate
    the deployment of ML models on cloud-based platforms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is user authentication separate from the API token required to access the Chroma
    database in this script?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the notebook use Chroma for temporary storage of vectors during the dynamic
    retrieval process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the notebook configured to use real-time acceleration of queries through
    GPU optimization?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can this notebook’s session time measurements help in optimizing the dynamic
    RAG process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the script demonstrate Chroma’s capability to integrate with ML models
    for enhanced retrieval performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the script include functionality for adjusting the parameters of the Chroma
    database based on session performance metrics?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Crowdsourcing Multiple Choice Science Questions* by Johannes Welbl, Nelson
    F. Liu, Matt Gardner: [http://arxiv.org/abs/1707.06209](http://arxiv.org/abs/1707.06209).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing
    Pretrained Transformers* by Wenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong, Furu
    Wei: [https://arxiv.org/abs/2012.15828](https://arxiv.org/abs/2012.15828).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hugging Face Llama model documentation: [https://huggingface.co/docs/transformers/main/en/model_doc/llama](https://huggingface.co/docs/transformers/main/en/model_doc/llama).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ONNX: [https://onnxruntime.ai/](https://onnxruntime.ai/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of
    Pre-Trained Transformers* by Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang,
    Ming Zhou: [https://arxiv.org/abs/2002.10957](https://arxiv.org/abs/2002.10957).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LLaMA: Open and Efficient Foundation Language Models* by Hugo Touvron, Thibaut
    Lavril, Gautier Lzacard, et al.: [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building an ONNX Runtime package: [https://onnxruntime.ai/docs/build/custom.html#custom-build-packages](https://onnxruntime.ai/docs/build/custom.html#custom-build-packages).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
