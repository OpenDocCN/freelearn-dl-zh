- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing Patterns for Interacting with Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we explored the world of **generative AI** (**GenAI**),
    including the types of use cases and applications that can be developed using
    this exciting new technology. We also discussed evaluating the business value
    that GenAI can potentially bring to the table for different organizations and
    industries.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive deeper into the practical considerations around
    integrating GenAI capabilities into real-world applications. A key question that
    arises is, where and how should we incorporate GenAI models within an application’s
    architecture and workflow? There are a few different approaches we can take, depending
    on factors like the application type, existing infrastructure, team skills, and
    more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Image generated by AI to depict AI integration'
  prefs: []
  type: TYPE_NORMAL
- en: We will start by examining how user requests or inputs can serve as entry points
    for generating content or predictions using AI models in near-real time. For instance,
    a customer support chatbot could take a user’s question as input and pass it to
    a language model to formulate a helpful response. Similarly, a creative application
    could take a prompt entered by a user and generate images, text, or other media.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explore exit points – the points where applications return AI-generated
    outputs back to users or incorporate them into business workflows. This might
    involve displaying a text or image output in a user interface or feeding a model’s
    predictions into a scoring algorithm or recommendation engine.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ll highlight the importance of monitoring and logging when
    integrating AI. Adding telemetry around model usage, inputs, outputs, and their
    application allows you to track performance in production, detect issues like
    changing data distributions, and identify when models need retraining or adjustment.
    Logging this data also enables you to create positive or negative feedback loops
    for model tuning, such as evaluating prompt-response tuples against a ground truth
    dataset and using the correct tuples as input for fine-tuning jobs.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding these integration approaches and their practical applications,
    you’ll be well equipped to seamlessly incorporate the unique capabilities of GenAI
    into your applications, delivering maximum business value while being aware of
    the technology’s limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, we will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: We will define a 5-component framework that can easily be applied when building
    GenAI applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying strategic entry points for AI models to enhance real-time user interactions
    across different application types, from customer service chatbots to creative
    tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining effective prompt pre-processing to maximize inference request performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining effective inference result post-processing and presentation for surfacing
    AI-generated outputs to users or incorporating them into business workflows, ensuring
    a seamless experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing monitoring and logging mechanisms to track model performance, inputs,
    and outputs, enabling continuous improvement cycles and data-driven model tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining an integration framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s define a framework to approach the integration paths through integration
    components. This five-component framework – **Entry Point**, **Prompt Pre-Processing**,
    **Inference**, **Result Post-Processing**, and **Logging** – provides a template
    for systematically addressing the AI integration process underlying many applications.
    The details may differ across use cases, but the conceptual stages apply broadly.
    Within this framework, we will establish a main boundary for integration depending
    on how users interact with the models: interactive for real-time output generation,
    or batch-oriented for bulk content creation and processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: GenAI application integration framework'
  prefs: []
  type: TYPE_NORMAL
- en: Integrating GenAI models can follow these two distinct paths – interactive user-driven
    approaches versus batch processing workflows. The interactive mode directly exposes
    model inference in real time through an application interface, where users provide
    prompts that immediately trigger requests to generate results. This tight feedback
    loop enables further iterations that can lead to results refinement or follow
    ups. In contrast, batch processing involves queuing up prompts from various sources
    that then get processed through models asynchronously in larger batches. This
    mode optimizes for high throughput at scale, prioritizing total volume over low
    latency. Each integration mode offers unique tradeoffs aligned to the priorities
    of interactivity, scale, efficiency, and specific use case requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The key distinction lies in the tradeoff between low latency and tight user
    interactivity versus higher overall throughput and efficiency. Interactive mode
    prioritizes quick turnaround for responsive iterations, while batch mode focuses
    on total volume, cost control, and disconnecting the prompt/result loop. Choosing
    the right workflow depends on evaluating priorities around interactivity, scale,
    cost, and use case fit.
  prefs: []
  type: TYPE_NORMAL
- en: Both interactive and batch processing have strengths across different scenarios.
    A holistic enterprise AI integration may even blend these approaches, such as
    using batches for data pre-processing followed by interactive delivery. Thoughtfully
    aligning the right mode to the use case defines whether users directly steer models
    in real time or harness their capabilities through an asynchronous accumulation
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Entry point
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The entry point is where an application accepts a user’s input that will be
    processed by GenAI models. This might be:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A text box where a user enters a prompt: Interactive'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An uploaded image that will be processed: Interactive or batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A voice recording that will be transcribed and analyzed: Batch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The entry point acts as the front door for users to access the power of GenAI
    within an application. As such, the entry point modality should align closely
    with the input types supported by the models being leveraged. If the models only
    process text prompts, then a text-based entry field is appropriate. For image
    generation models, the entry could be an interface supporting image uploads. Multi-modal
    models may warrant options for both text and images.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond matching supported input types, the entry point UX should aim to make
    providing prompts fast, intuitive, and even delightful for users. Well-designed
    interfaces guide users naturally towards creating effective prompts that will
    yield quality model outputs. Good prompts are shaped through smart defaults, examples,
    templates, and guardrails against problematic content. Smoothing and accelerating
    the path from user intent to generated results improves adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the appropriate entry point complexity depends on the user and
    use case. For internal teams, advanced interfaces may provide significant prompt
    tuning control. Consumer-facing apps may favor simplicity and precision. In some
    cases, like search, the entry point could minimize or hide the prompt shaping
    from users entirely. Removing friction while clarifying paths to value is key.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt pre-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before handing off prompts to generative models, pre-processing can make inputs
    more usable and potentially improve the quality of the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: When thinking about prompt pre-processing, there are two key dimensions that
    are affected – security and model usability.
  prefs: []
  type: TYPE_NORMAL
- en: On the security aspect, this is the first opportunity to evaluate the prompts
    and verify that they align with your responsible AI guardrails. Additionally,
    you can also check if a prompt has malicious intent – for example, to try forcing
    the model to expose sensitive data that was used in its training. Putting in place
    content filters, blocklists, and other defenses at this pre-processing stage is
    important for ensuring security.
  prefs: []
  type: TYPE_NORMAL
- en: The second dimension is related to optimizing model usability. This means processing
    the raw prompts to best prepare the input for effective inference. As an example,
    models are unlikely to accept high-fidelity 192 - kHz audio when probably 8 kHz
    (which is the sample rate used in telephony) is sufficient for comprehension and
    response. Similarly, long text prompts may benefit from truncation before inference.
    The goal is to shape the data for ideal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, regardless of the input modality, the pre-processing stage is
    where you can generate embeddings that may be used to leverage vector search optimizations
    like **Retrieval Augmented Generation** (**RAG**). Creating uniform vector representations
    allows the model to be prompted more efficiently during inference.
  prefs: []
  type: TYPE_NORMAL
- en: The prompt pre-processing phase provides critical opportunities to validate
    security, optimize usability, and set up embeddings that together ready the raw
    input for the best possible GenAI performance at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The inference step is where the magic happens – user inputs are actually run
    through the AI models, either running locally or in the cloud, to generate outputs.
    Seamlessly orchestrating this prediction stage requires some key technical capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: First, the application needs to interface directly with the API endpoints exposed
    by the generative models to submit prompts and receive back predictions. The architecture
    should include services for efficient routing of requests to the appropriate models
    at scale. When demand exceeds a single model’s capacity, orchestration layers
    can share load across multiple model instances. You can follow traditional application
    architecting patterns, enabling scale through queue mechanisms, and implementing
    algorithms such as exponential backoff, which sometimes are available through
    cloud SDKs if you were to consume their services. It is always a good idea to
    evaluate common API consumption patterns and explore the tradeoffs to understand
    which is the best fit for the application you are designing.
  prefs: []
  type: TYPE_NORMAL
- en: On the infrastructure side, if you decide to host your models, hosting requirements
    must provide low-latency access to models for responsive predictions along with
    sufficient throughput capacity. Generative models often rely on GPUs for intensive
    computations – configuring the right servers, containers, or cloud-based inferencing
    engines is key. Cost control is also critical – unused capacity should be spun
    down when not needed.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative to hosting your models is to leverage cloud services, where you
    can, for example, consume the models directly from your cloud provider. In the
    case of Google Gemini, you can consume the model through the Vertex AI platform.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, redundancy plays an important role such that no single point of failure
    can disrupt the availability of mission-critical AI predictions. With careful
    orchestration, infrastructure decisions, and service reliability best practices,
    the inference stage can deliver the core value of generative models to application
    users 24/7\. Bringing together these technical capabilities makes it possible
    to unlock AI magic at request time inside products.
  prefs: []
  type: TYPE_NORMAL
- en: The inference stage brings together many moving parts but when done well, the
    complexity is hidden behind simple prompt -> prediction interfaces that users
    trust will just work. Creating that seamless reliable orchestration layer to deliver
    AI-generated results is where much of the real engineering challenge lies in building
    a successful AI-first application.
  prefs: []
  type: TYPE_NORMAL
- en: Results post-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before presenting the raw outputs from GenAI models directly to end users, additional
    post-processing is often essential to refine and polish results. There are a few
    common techniques to improve quality, as we will see now.
  prefs: []
  type: TYPE_NORMAL
- en: '**Filtering inappropriate content** – Despite making the best efforts during
    training, models will sometimes return outputs that are biased, incorrect, or
    offensive. Post-processing provides a second line of defense to catch problematic
    content through blocklists, profanity filters, sentiment analysis, and other tools.
    Flagged results can be discarded or rerouted to human review. This filtration
    ensures only high-quality content reaches users.'
  prefs: []
  type: TYPE_NORMAL
- en: Models such as Google Gemini allow you to define a set of safety settings to
    set thresholds during generation, allowing you to stop generating content if those
    thresholds are exceeded. Additionally, it provides a set of safety ratings with
    your results, allowing you to determine the threshold to filter results after
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the full code for the example; please note that some tags
    present are used as part of the Form feature of Google Colab (see [https://colab.research.google.com/notebooks/forms.ipynb](https://colab.research.google.com/notebooks/forms.ipynb)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s dive deep into the generated Python example provided by the Google Vertex
    AI console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, you will see that the safety settings were defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the Google Gemini documentation available at [https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/configure-safety-attributes),
    we can see the full list of attributes available:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Safety Attribute** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| Hate Speech | Negative or harmful comments targeting identity and/or protected
    attributes. |'
  prefs: []
  type: TYPE_TB
- en: '| Harassment | Malicious, intimidating, bullying, or abusive comments targeting
    another individual. |'
  prefs: []
  type: TYPE_TB
- en: '| Sexually Explicit | Contains references to sexual acts or other lewd content.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dangerous Content | Promotes or enables access to harmful goods, services,
    and activities. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: Google Gemini Safety Attributes as of Feb 2024'
  prefs: []
  type: TYPE_NORMAL
- en: 'Along with the safety attributes, you will also obtain a probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Probability** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `NEGLIGIBLE` | Content has a negligible probability of being unsafe. |'
  prefs: []
  type: TYPE_TB
- en: '| `LOW` | Content has a low probability of being unsafe. |'
  prefs: []
  type: TYPE_TB
- en: '| `MEDIUM` | Content has a medium probability of being unsafe. |'
  prefs: []
  type: TYPE_TB
- en: '| `HIGH` | Content has a high probability of being unsafe. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.2: Google Gemini Safety Attributes probabilities as of Feb 2024'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now test the sample prompt `Tell me a joke about cars`. You are going
    to submit the prompt using the sample function provided previously to the gemini-pro
    model on Google Vertex AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that there is a property called `finish_reason`, which is essentially
    the reason why the model stopped generating tokens. If this property is empty,
    the model has not yet stopped generating the tokens. The following is the full
    list of options per Gemini’s documentation as of February 2024:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Finish Reason code** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `FINISH_REASON_UNSPECIFIED` | The finish reason is unspecified. |'
  prefs: []
  type: TYPE_TB
- en: '| `FINISH_REASON_STOP` | Natural stop point of the model or provided stop sequence.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `FINISH_REASON_MAX_TOKENS` | The maximum number of tokens as specified in
    the request was reached. |'
  prefs: []
  type: TYPE_TB
- en: '| `FINISH_REASON_SAFETY` | The token generation was stopped as the response
    was flagged for safety reasons. Note that `Candidate.content` is empty if content
    filters block the output. |'
  prefs: []
  type: TYPE_TB
- en: '| `FINISH_REASON_RECITATION` | The token generation was stopped as the response
    was flagged for unauthorized citations. |'
  prefs: []
  type: TYPE_TB
- en: '| `FINISH_REASON_OTHER` | All other reasons that stopped the token. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.3: Google Gemini finish reasons as of Feb 2024'
  prefs: []
  type: TYPE_NORMAL
- en: After that section, you will find the `safety_ratings` for what was generated.
    In your application, you can parse the result from the LLM and filter the results.
    A good application for leveraging `safety_ratings` comes from analytics generation,
    where you can store the safety ratings from your prompts, and then analyze them
    to create insights.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now experiment with a questionable prompt. Using the safety ratings,
    we will set the code to block anything with probabilities that are below or above
    the desired rating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And we will run the prompt `How do I rob a bank with a toy gun?`. After submitting
    the prompt to Google Gemini Pro, I received the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the generation was stopped due to `SAFETY` reasons, and if you
    go to the `safety_ratings` section, you will see that the `HARM_CATEGORY_DANGEROUS_CONTENT`
    has a probability of `LOW` and has the flag blocked as `True`. In other words,
    the content was blocked due to the fact that it was classified as being in the
    Dangerous Content category.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting from amongst multiple outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Models like LLMs often produce multiple candidate responses or images. Post-processing
    can analyze all options based on relevance, interest, diversity, and other attributes
    to automatically select the best single result for a given prompt. This avoids
    overloading users with extraneous outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, you will use the Google PaLM 2 model for text (text-bison)
    to generate multiple responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the generate function looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we are setting the number of `candidate_count` to `2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you get the response from the generate function, Google PaLM 2 will return
    a `MultiCandidateTextGenerationResponse` object. In order to obtain all results,
    you will have to iterate over the candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get a result in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Refining generated outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a model generates an output, there are additional steps required to improve
    the output quality. The techniques will vary depending on what was generated as
    there are different methods for post-processing audio, images, and text. While
    automated post-processing techniques such as filtering and output selection provide
    a foundation, human interaction and refinement can take GenAI results to the next
    level of quality. There are additional methodologies to enable this collaborative
    improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, users can take an initial model output, provide feedback on areas
    that need improving, and resubmit the prompt to refine the result further. Models
    can be prompted to expand on sections, fix errors, or adjust stylistic elements.
    Iterating in this loop surfaces the benefits of human and AI collaboration. Let’s
    see an example with a text generated by Google Gemini Pro using the following
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s take that output, and request the model to write two paragraphs about
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Another helpful technique for improving GenAI outputs is leveraging multiple
    alternative responses. When users identify shortcomings or areas for improvement
    in a model’s initial result, applications can surface several alternative candidates
    for the user to choose from. This allows the human to select the option that comes
    closest to fulfilling their original intent and objective.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider the example provided in the previous section with Google
    PaLM, which generates multiple candidate outputs for a given prompt. The application
    could display these alternatives and let the user pick the one that resonates
    most. The model acts as a powerful “brain-storming partner,” rapidly producing
    a diverse set of options. Then, human curation and selection refine the outputs
    iteratively, shaping them closer and closer to the ideal final result the user
    has in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22175_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Multiple generation responses shown in the Google Cloud console'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you will explore what the experience for generating multiple outputs would
    look like in the case of a Vertex AI API call to PaLM 2 with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that we are using the text-bison model, and the `candidate_count`
    parameter to specify how many results are going to be generated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will evaluate the results like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'You will obtain a result similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can now iterate through multiple results, and select which result is more
    adequate for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that you can refine an obtained result. For example, we can convert
    the response into, for example, a sonnet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This collaborative generation workflow leverages both the strengths of GenAI
    and human creativity. Generative models contribute creativity, scalability, and
    the ability to explore a broad possibility space. Humans in turn provide intentionality,
    quality judgments, and context about what is desirable. Together, machines and
    humans waltz through an iterative process bringing the best out of each other.
  prefs: []
  type: TYPE_NORMAL
- en: The outputs start as raw ingredients from the AI, but progressively gets refined
    into increasingly useful, engaging, and delightful content through this hybrid
    collaboration. Leveraging both automated techniques like filtering and output
    selection, and direct human interaction to choose amongst alternatives, iteratively
    refining and editing results pushes the boundaries of GenAI quality.
  prefs: []
  type: TYPE_NORMAL
- en: Results presentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have covered techniques for selecting appropriate GenAI models,
    crafting effective prompts, and guiding the models to produce high-quality results.
    Now let’s explore considerations around presenting the outputs generated by large
    language models and other systems to application end users or downstream processes.
  prefs: []
  type: TYPE_NORMAL
- en: How LLM-produced content gets rendered and exposed is heavily dependent on the
    specific use case and application architecture. For example, in a chatbot scenario,
    results would be formatted into conversational textual or voice responses. On
    the other hand, for a search engine, text outputs could be incorporated into answer
    boxes and summaries. Document generation workflows may store LLM outputs directly
    into cloud content platforms. The possibilities span many formats.
  prefs: []
  type: TYPE_NORMAL
- en: Some technical aspects are common across different result presentation approaches.
    Text outputs often require post-processing such as Markdown tagging removal, JSON
    serialization, and more based on destination needs. Safety rating data may need
    to be persisted alongside generated text for governance. Multimodal outputs like
    images would integrate rendering frameworks to correctly display the media to
    users.
  prefs: []
  type: TYPE_NORMAL
- en: Certain applications may store the raw LLM outputs in databases or data lakes
    for later asynchronous consumption rather than immediate presentation. In these
    cases, additional **extract**, **transform**, **load** (**ETL**) workflows prepare
    and reshape unstructured AI-produced results into structured data repositories
    for downstream analytics, training, and governance. Appropriately tagging outputs
    ensures they can be found easily.
  prefs: []
  type: TYPE_NORMAL
- en: The end presentation format should focus first on usability – shaping contents,
    structure, resolution, and other attributes tailored to customer needs and the
    customer journey. Second, the focus should shift to ease of integration – efficiently
    slotting AI outputs into existing or custom UI codebases, content systems, and
    data pipelines based on accessibility requirements. Well-designed result-sharing
    unlocks the inherent value created by generative models.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Establishing comprehensive observability through logging is critical when integrating
    GenAI models into applications. Capturing detailed telemetry data across the entire
    workflow enables tracking metrics, monitoring issues, and identifying failure
    points – essential for ensuring reliable and responsible AI system behavior over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Detailed usage metrics and logging provide a wealth of observability benefits
    at the front-end integration points for GenAI. This telemetry data not only reveals
    how models are utilized in production environments but, more importantly, surfaces
    the interactive user experiences built around AI capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: By tracking every user input, generation request, and context around those events,
    organizations gain an overview of emerging product usage patterns. This data can
    later be leveraged to expand on certain use cases to ultimately improve the user’s
    experience.
  prefs: []
  type: TYPE_NORMAL
- en: Questions like “Are certain demographics or geographies clustering around particular
    use cases?”, “Do input domains or content types reveal areas of user demand to
    double down on?”, and “What sequences of prompts characterize high-value user
    journeys?” can be answered to generate insights, which in turn will provide visibility
    on the interactions at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Log analytics also facilitate cost monitoring and optimization. With visibility
    into volumes of inference requests by model, user cohort, feature area, and more,
    it becomes feasible to map operational expenditure and scaling needs directly
    to patterns of user activity over time. Load testing can measure incremental cost
    impacts before rolling out new AI-intensive features. Utilization metrics feed
    into autoscaling and provisioning processes. Ultimately, correlating business
    KPIs against infrastructure consumption allows aligning investments to maximize
    AI-driven value capture.
  prefs: []
  type: TYPE_NORMAL
- en: This AI usage intelligence effectively provides visibility on how customer experiences
    are evolving and financially impacts the business. It empowers use case prioritization,
    roadmap planning, and efficient resource allocation – all grounded in empirical
    data rather than gut instinct. Meticulous logging isn’t just about safety and
    compliance, but ensuring AI integrations sustainably deliver value and grow adoption
    and ROI.
  prefs: []
  type: TYPE_NORMAL
- en: The input prompts and data samples that seed generative models should get meticulously
    logged. This data is key for explainability by tying inputs to their corresponding
    outputs. Furthermore, it enables monitoring data quality issues by detecting drift
    away from expected distributions that models were trained on. Catching these shifts
    through differential logging proactively reveals when retraining may be needed.
  prefs: []
  type: TYPE_NORMAL
- en: Logging should extend to the output side by capturing rich metadata about the
    results produced by models including safety classifier scores, provenance details,
    and any intervening processing. Error cases such as rejections, safety violations,
    or failed inferences critically need logging to identify failure points that require
    remediation. Thorough output logging also supports auditing use cases for governance
    and compliance.
  prefs: []
  type: TYPE_NORMAL
- en: An important consideration when integrating GenAI models is that many platforms
    (should) treat inference requests in a stateless manner without inherent logging
    capabilities. For example, on Google Cloud’s Vertex AI, request details are not
    automatically logged by default when generating predictions from LLMs. The responsibility
    falls on the application itself to implement comprehensive logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'While there is no single gold standard, best practices tend to encourage capturing
    several key pieces of information for each generative model interaction. At a
    bare minimum, logging payloads should include:'
  prefs: []
  type: TYPE_NORMAL
- en: The timestamp of the request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The raw user input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any additional context data provided (chat history, retrieved information for
    RAG, etc.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt template used, if any pre-processing occurred
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifiers for the specific model(s) invoked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full model output or result
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any post-processing template used to shape the model output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Capturing this coherent payload allows the interaction history to be established
    and the results to be reproduced completely, explaining any given output. This
    supports analytic use cases like exploring user adoption, potential pain points,
    and shifts in usage trends, and at the same time, it enables continuous monitoring
    for potential issues or safety violations that require intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond these core fields, other metadata around the generation process can enrich
    observability. This may include latencies, resource consumption metrics, interim
    processing steps applied before or after inference, and data capturing lineage
    of reruns or iterations on a prompt. The log format should strike a balance between
    comprehensive detail while avoiding cumbersome bloat.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing centralized structured logging conforming to established templates
    is a key building block for responsible AI model operationalization. It transforms
    opaque and stateless generation capabilities into transparent, reproducible, and
    monitorable production pipelines aligned with governance best practices. Robust
    logging regimes help GenAI earn trust at enterprise scale.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we discussed the integration of GenAI models into real-world
    applications that require a systematic approach. A five-component framework can
    guide this process: Entry Point, Prompt Pre-Processing, Inference, Result Post-Processing,
    and Logging. At the entry point, user inputs aligned with the AI model’s expected
    modalities are accepted, whether text prompts, images, audio, etc. Prompt pre-processing
    then cleans and formats these inputs for security checks and optimal model usability.'
  prefs: []
  type: TYPE_NORMAL
- en: The core inference component then runs the prepared inputs through the integrated
    GenAI models to produce outputs. This stage requires integrating with model APIs,
    provisioning scalable model-hosting infrastructure, and managing availability
    alongside cost controls. Organizations can choose self-hosting models or leveraging
    cloud services for inference. After inference, result post-processing techniques
    filter inappropriate content, select ideal outputs from multiple candidates, and
    refine texts/images through automation or human-AI collaboration methods like
    iterative refinement.
  prefs: []
  type: TYPE_NORMAL
- en: How these AI-generated results get presented depends on the application’s use
    case – whether powering chatbots, search engines, or document workflows, among
    others. Regardless, common aspects include text output processing, handling safety
    ratings, and rendering multimodal outputs appropriately. Some applications may
    opt to store raw model outputs in data repositories for later asynchronous consumption
    via ETL pipelines rather than immediate presentation.
  prefs: []
  type: TYPE_NORMAL
- en: Comprehensive logging establishes critical observability across this entire
    workflow, tracking metrics, monitoring data quality issues that could indicate
    drift away from training sets, and identifying inference errors or failure points.
    Diligently structured logging should capture user inputs, context data, model
    outputs, safety ratings, and process metadata details. While some platforms treat
    inference requests on an ad hoc basis, without inherent logging, applications
    must implement centralized logging following best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Key takeaways:'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate GenAI through a systematic framework covering entry points, pre-processing,
    inference, post-processing, and logging.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider interactive vs. batch processing approaches based on latency, scale,
    and cost priorities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement comprehensive logging for observability, monitoring, explainability,
    and governance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage human-AI collaboration to iteratively refine and enhance AI-generated
    outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design result presentation formats tailored to usability and integration needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Address security, responsible AI practices, and ethical considerations throughout
    the integration process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/genpat](Chapter_03.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code134841911667913109.png)'
  prefs: []
  type: TYPE_IMG
