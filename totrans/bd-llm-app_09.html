<html><head></head><body>
<div><h1 class="chapterNumber">9</h1>
<h1 class="chapterTitle" id="_idParaDest-125">Working with Code</h1>
<p class="normal">In this chapter, we are going to cover another great capability of Large Language Models, that is, working with programming languages. In the previous chapter, we’ve already seen a glimpse of this capability, namely, SQL query generation in a SQL database. In this chapter, we are going to examine the other ways in which LLMs can be used with code, from “simple” code generation to interaction with code repositories and, finally, to the possibility of letting an application behave as if it were an algorithm. By the end of this chapter, you will be able to leverage LLMs to code-related projects, as well as build LLM-powered applications with natural language interfaces to work with code.</p>
<p class="normal">Throughout this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">Analysis of the main LLMs with top-performing code capabilities</li>
<li class="bulletList">Using LLMs for code understanding and generation</li>
<li class="bulletList">Building LLM-powered agents to “act as” algorithms</li>
<li class="bulletList">Leveraging Code Interpreter</li>
</ul>
<h1 class="heading-1" id="_idParaDest-126">Technical requirements</h1>
<p class="normal">To complete the tasks in this chapter, you will need the following:</p>
<ul>
<li class="bulletList">A Hugging Face account and user access token.</li>
<li class="bulletList">An OpenAI account and user access token.</li>
<li class="bulletList">Python 3.7.1 or a later version.</li>
<li class="bulletList">Python packages. Make sure you have the following Python packages installed: <code class="inlineCode">langchain</code>, <code class="inlineCode">python-dotenv</code>, <code class="inlineCode">huggingface_hub</code>, <code class="inlineCode">streamlit</code>, <code class="inlineCode">codeinterpreterapi</code>, and <code class="inlineCode">jupyter_kernel_gateway</code>. Those can be easily installed via <code class="inlineCode">pip</code> <code class="inlineCode">install</code> in your terminal.</li>
</ul>
<p class="normal">You can find all the code and examples in the book’s GitHub repository at <a href="Chapter_09.xhtml">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<h1 class="heading-1" id="_idParaDest-127">Choosing the right LLM for code</h1>
<p class="normal">In <em class="chapterRef">Chapter 3</em>, we described a decision<a id="_idIndexMarker651"/> framework to use in order<a id="_idIndexMarker652"/> to decide the proper LLM for a given application. Generally speaking, all LLMs are endowed with knowledge of code understanding and generation; however, some of them are particularly specialized in doing so. More specifically, there are some evaluation benchmarks – such as the HumanEval – that are specifically tailored to assessing LLMs’ capabilities of working with code. The leaderboard of HumanEval One is a good source for determining the top-performing models, available at <a href="https://paperswithcode.com/sota/code-generation-on-humaneval">https://paperswithcode.com/sota/code-generation-on-humaneval</a>. HumanEval is a benchmark introduced by OpenAI to assess the code generation capabilities of LLMs, where the model completes Python functions based on their signature and docstring. It has been used to evaluate models like Codex, demonstrating its effectiveness in measuring functional correctness.</p>
<p class="normal">In the following screenshot, you can see the situation of the leaderboard as of January 2024:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_09_01.png"/></figure>
<p class="packt_figref">Figure 9.1: HumanEval benchmark in January 2024</p>
<p class="normal">As you can see, the majority of the models are fine-tuned versions of the GPT-4 (as well as the GPT-4 itself), as it is the state-of-the-art LLM in basically all the domains. Nevertheless, there are many open-source models that reached stunning results in the field of code understanding and generation, some of which will be covered in the next sections. Another benchmark is <strong class="keyWord">Mostly Basic Programming Problems </strong>(<strong class="keyWord">MBPP</strong>), a dataset of 974 programming tasks in Python, designed<a id="_idIndexMarker653"/> to be solvable by entry-level programmers. Henceforth, when choosing your model for a code-specific task, it might be useful to have a look at these benchmarks as well as other similar code metrics (we will see throughout the chapter some further benchmarks for code-specific LLMs). </p>
<p class="normal">Staying within the scope of coding, below you can find three additional benchmarks often used in the market:</p>
<ul>
<li class="bulletList"><strong class="keyWord">MultiPL-E</strong>: An extension of HumanEval<a id="_idIndexMarker654"/> to many other languages, such as Java, C#, Ruby, and SQL.</li>
<li class="bulletList"><strong class="keyWord">DS-1000</strong>: A data science benchmark<a id="_idIndexMarker655"/> that tests if the model can write code for common data analysis tasks in Python.</li>
<li class="bulletList"><strong class="keyWord">Tech Assistant Prompt</strong>: A prompt that tests if the model<a id="_idIndexMarker656"/> can act as a technical assistant and answer programming-related requests.</li>
</ul>
<p class="normal">In this chapter, we are going<a id="_idIndexMarker657"/> to test <a id="_idIndexMarker658"/>different LLMs: two code-specific (CodeLlama and StarCoder) and one general-purpose, yet also with emerging capabilities in the field of code generation (Falcon LLM).</p>
<h1 class="heading-1" id="_idParaDest-128">Code understanding and generation</h1>
<p class="normal">The first experiment we are going<a id="_idIndexMarker659"/> to run will be code understanding<a id="_idIndexMarker660"/> and generation leveraging LLMs. This simple use case is at the base of the many AI code assistants that were developed since the launch of ChatGPT, first among all the GitHub Copilot.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">GitHub Copilot is an AI-powered tool<a id="_idIndexMarker661"/> that assists developers in writing code more efficiently. It analyzes code and comments to provide suggestions for individual lines and entire functions. The tool is developed by GitHub, OpenAI, and Microsoft and supports multiple programming languages. It can perform various tasks such as code completion, modification, explanation, and technical <a id="_idIndexMarker662"/>assistance.</p>
</div>
<p class="normal">In this experiment, we are going to try three different models: Falcon LLM, which we already explored in <em class="chapterRef">Chapter 3</em>; CodeLlama, a fine-tuned version of Meta AI’s Llama; and StarCoder, a code-specific model that we are going to investigate in the upcoming sections.</p>
<p class="normal">Since those models<a id="_idIndexMarker663"/> are pretty heavy to run on a local machine, for this purpose I’m going<a id="_idIndexMarker664"/> to use a Hugging Face Hub Inference Endpoint, with a GPU-powered virtual machine. You can link one model per Inference Endpoint and then embed it in your code, or use the convenient library <code class="inlineCode">HuggingFaceEndpoint</code>, available in LangChain.</p>
<p class="normal">To start using your Inference Endpoint, you can use the following code:</p>
<pre class="programlisting code"><code class="hljs-code">llm = HuggingFaceEndpoint(endpoint_url = "your_endpoint_url", task = 'text-generation',
        model_kwargs = {"max_new_tokens": 1100})
</code></pre>
<p class="normal">Alternatively, you can copy and paste the Python code provided on your endpoint’s webpage at <a href="https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name">https://ui.endpoints.huggingface.co/user_name/endpoints/your_endpoint_name</a>:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_09_02.png"/></figure>
<p class="packt_figref">Figure 9.2: User interface of the Hugging Face Inference Endpoint</p>
<p class="normal">To create your Hugging Face Inference Endpoint, you<a id="_idIndexMarker665"/> can follow the instructions at <a href="https://huggingface.co/docs/inference-endpoints/index">https://huggingface.co/docs/inference-endpoints/index</a>.</p>
<p class="normal">You can always leverage<a id="_idIndexMarker666"/> the free Hugging Face API<a id="_idIndexMarker667"/> as described in <em class="chapterRef">Chapter 4</em>, but you have to expect some latency when running the models.</p>
<h2 class="heading-2" id="_idParaDest-129">Falcon LLM</h2>
<p class="normal">Falcon LLM is an open-source model<a id="_idIndexMarker668"/> developed by Abu Dhabi’s <strong class="keyWord">Technology Innovation Institute</strong> (<strong class="keyWord">TII</strong>) and launched on the market<a id="_idIndexMarker669"/> in May 2023. It is an autoregressive, decoder-only transformer, trained on 1 trillion tokens, and has 40 billion parameters (although it has also been released as a lighter version with 7 billion parameters). As discussed in <em class="chapterRef">Chapter 3</em>, “small” language models are a representation of a new trend of LLMs, consisting of building lighter models (with fewer parameters) that focus instead on the quality of the training dataset.</p>
<p class="normal">To start using Falcon LLM, we can<a id="_idIndexMarker670"/> follow these steps:</p>
<ol>
<li class="numberedList" value="1">We can leverage the HuggingFaceHub wrapper available in LangChain (remember to set the Hugging Face API in the <code class="inlineCode">.env</code> file, passing your secrets as <code class="inlineCode">os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN</code>):
        <pre class="programlisting code-one"><code class="hljs-code">from langchain import HuggingFaceHub
from langchain import PromptTemplate, LLMChain
import os
load_dotenv()hugging_face_api = os.environ["HUGGINGFACEHUB_API_TOKEN"]
repo_id = "tiiuae/falcon-7b-instruct"
llm = HuggingFaceHub(
    repo_id=repo_id,  model_kwargs={"temperature": 0.2, "max_new_tokens": 1000}
)
</code></pre>
</li>
<li class="numberedList">Now that we’ve initialized the model, let’s ask it to generate the code for a simple webpage:
        <pre class="programlisting code-one"><code class="hljs-code">prompt = """
Generate a short html code to a simple webpage with a header, a subheader, and a text body.
&lt;!DOCTYPE html&gt;
&lt;html&gt;
"""
print(llm(prompt))
</code></pre>
</li>
</ol>
<p class="normal-one">The following is the corresponding output:</p>
<pre class="programlisting con-one"><code class="hljs-con">&lt;head&gt;
    &lt;title&gt;My Webpage&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
    &lt;h1&gt;My Webpage&lt;/h1&gt;
    &lt;h2&gt;Subheader&lt;/h2&gt;
    &lt;p&gt;This is the text body.&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre>
<ol>
<li class="numberedList" value="3">If you save it as an HTML file and execute it, the result will look like the following:</li>
</ol>
<figure class="mediaobject"><img alt="A close up of a text  Description automatically generated" src="img/B21714_09_03.png"/></figure>
<p class="packt_figref">Figure 9.3: Sample webpage generated by FalconLLM</p>
<ol>
<li class="numberedList" value="4">We can also try to generate<a id="_idIndexMarker671"/> a Python function to generate random passwords:
        <pre class="programlisting code-one"><code class="hljs-code">prompt = """
Generate a python program that create random password with lenght of 12 characters, 3 numbers, one capital letter.
"""
print(llm(prompt))
</code></pre>
</li>
</ol>
<p class="normal-one">Here is our output:</p>
<pre class="programlisting con-one"><code class="hljs-con">import random
def generate_password():
    chars = "abcdefghijklmnopqrstuvwxyz0123456789"
    lenght = 12
    num = random.randint(1, 9)
    cap = random.randint(1, 9)
    password = ""
    for i in range(lenght):
        password += chars[random.randint(0, 9)]
    password += num
    password += cap
    return password
print(generate_password())
</code></pre>
<p class="normal-one">We now have a function named <code class="inlineCode">generate_password()</code>, which uses random functions to generate a password as per our prompt.</p>
<ol>
<li class="numberedList" value="5">Finally, let’s do the opposite, asking the model to explain to us the above code:
        <pre class="programlisting code-one"><code class="hljs-code">prompt = """
Explain to me the following code:
def generate_password():
    chars = "abcdefghijklmnopqrstuvwxyz0123456789"
    lenght = 12
    num = random.randint(1, 9)
    cap = random.randint(1, 9)
    password = ""
    for i in range(lenght):
        password += chars[random.randint(0, 9)]
    password += num
    password += cap
    return password
print(generate_password())
"""
print(llm(prompt))
</code></pre>
</li>
</ol>
<p class="normal-one">Here is the obtained output:</p>
<pre class="programlisting con-one"><code class="hljs-con">&lt;p&gt;The code generates a random password of length 12 characters that contains a mix of letters, numbers, and special characters. The password is then printed to the console.&lt;/p&gt;
</code></pre>
<p class="normal">Overall, even if not code-specific, the model was able to correctly perform all the tasks. Note also that this is the “light” version<a id="_idIndexMarker672"/> of the model (7 billion parameters), yet its performance is great.</p>
<p class="normal">Let’s now investigate the capabilities of CodeLlama.</p>
<h2 class="heading-2" id="_idParaDest-130">CodeLlama</h2>
<p class="normal">CodeLlama is a family of LLMs<a id="_idIndexMarker673"/> for code based on Llama 2, which is a general-purpose language model developed by Meta AI (as discussed in <em class="chapterRef">Chapter 3</em>). CodeLlama can generate and discuss code in various programming languages, such as Python, C++, Java, PHP, and more. CodeLlama can also perform infilling, which is the ability to fill in missing parts of code based on the surrounding context, as well as follow instructions given in natural language and produce code that matches the desired functionality.</p>
<p class="normal">The model comes in three sizes (7B, 13B, and 34B parameters) and three flavors (base model, Python fine-tuned, and instruction-tuned) to cover a wide range of applications. CodeLlama is trained on sequences of 16k tokens and can handle inputs with up to 100k tokens.</p>
<p class="normal">In the model paper “Code Llama: Open Foundation Models for Code” by Rozière Baptiste et al, released in August 2023, the authors describe how the various models were tested against some of the most popular evaluation benchmarks in the domain of code understanding and generation, including HumanEval and MBPP, according to which CodeLlama models achieved a score up to 53% and 55%, respectively. On top of those remarkable results, it is stunning that the Python fine-tuned CodeLlama’s smallest size (7 billion parameters) outperformed the largest version of Llama 2 (70 billion parameters) on HumanEval and MBPP.</p>
<p class="normal">Now, let’s run some tests<a id="_idIndexMarker674"/> with this model. As per the previous section, we can initialize the model leveraging either the Hugging Face Inference API (pay per use) or the free Hugging Face API (with the constraint of higher latency). You can consume it as follows:</p>
<pre class="programlisting code"><code class="hljs-code">repo_id = "codellama/CodeLlama-7b-Instruct-hf"
llm = HuggingFaceHub(    repo_id=repo_id,  model_kwargs={"temperature": 0.2, "max_new_tokens": 1000}
)
</code></pre>
<p class="normal">Let’s now test it with some code tasks. The first task will be that of optimizing Python code so that it runs more efficiently. Let’s see how our model performs in this task. In the following code snippet, we simply prompt the model to regenerate the provided code in a more efficient way:</p>
<pre class="programlisting code"><code class="hljs-code">prompt = """
Regenerate the below code in a more efficient way.
```
def factorial(n):
    result = 1
    for i in range(1, n + 1):
        result *= i
    return result
# Example usage:
n = 5
print("Factorial of", n, "is", factorial(n))
```
"""
print(llm(prompt))
</code></pre>
<p class="normal">Here is our output:</p>
<pre class="programlisting con"><code class="hljs-con">### Solution
```
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
```
### Explanation
The solution is a recursive function that calls itself until the base case is reached.
### References
- [Wikipedia](https://en.wikipedia.org/wiki/Factorial)
- [Math is Fun](https://www.mathsisfun.com/numbers/factorial.html)
- [Mathematics Stack Exchange](https://math.stackexchange.com/questions/117042/what-is-the-difference-between-factorial-and-permutation)
- [Mathematics Stack Exchange](https://math.stackexchange.com/questions/117042/what-is-the-difference-between-factorial-and-permutation)
</code></pre>
<p class="normal">As you can see, the model<a id="_idIndexMarker675"/> was able to use a recursive approach, which is more efficient and “Pythonic.” It also provides a reference for the user to dive deeper into the mathematical theory behind the function.</p>
<p class="normal">Next, let’s leverage the model’s completion capabilities by initializing a function to remove non-ASCII characters.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal"><strong class="keyWord">American Standard Code for Information Interchange</strong> (<strong class="keyWord">ASCII</strong>) is a character<a id="_idIndexMarker676"/> encoding standard that uses 7 bits to represent 128 characters, such as letters, digits, punctuation marks, and control codes.</p>
<p class="normal">Non-ASCII characters are those that are not part of the ASCII standard and use more than 7 bits to encode. They include special characters such as letters with accents, glyphs, ideograms, and mathematical symbols. Non-ASCII characters can be encoded<a id="_idIndexMarker677"/> using different standards, such as Unicode, ISO 8859-1, Windows-1252, etc.</p>
<p class="normal">For example, the letter é is a non-ASCII character that can be encoded using Unicode as U+00E9 or using Windows-1252 as 0xE9.</p>
</div>
<p class="normal">Accordingly, here is the code<a id="_idIndexMarker678"/> to generate the function:</p>
<pre class="programlisting code"><code class="hljs-code">prompt = '''
Define a python function that remove non-ascii character from a string.
'''
print(llm(prompt))
</code></pre>
<p class="normal">The following is the function that we receive as the output:</p>
<pre class="programlisting code"><code class="hljs-code">def remove_non_ascii(string):
    return string.encode('ascii', 'ignore').decode('utf-8')
</code></pre>
<p class="normal">Let’s now leverage the model as a bug fixer, prompting it with the wrong function and also asking it to provide an explanation of why it is wrong and how it can be fixed:</p>
<pre class="programlisting code"><code class="hljs-code">prompt = """
Identify the bug in the following code:.
```
#wrong function
import random
a = random.randint(1, 12)
b = random.randint(1, 12)
for i in range(10):
    question = "What is " + a + " x " + b + "? "
    answer = input(question)
    if answer = a * b:
        print (Well done!)
    else:
        print("No.")
```
Explanation:
"""
print(llm(prompt))
</code></pre>
<p class="normal">Here is our output:</p>
<pre class="programlisting con"><code class="hljs-con">- The function is using the wrong syntax for the `randint` function. The correct syntax is `randint(start, end)`.
- The `for` loop is using the wrong syntax. The correct syntax is `for i in range(start, end)`.
- The `question` and `answer` variables are not being used in the loop.
- The `if` statement is not being used correctly. It should be `if answer == a * b:`.
- The `print` statement is not being used correctly. It should be `print(Well done!)`.
</code></pre>
<p class="normal">Finally, let’s ask the model in natural language<a id="_idIndexMarker679"/> to generate specific Python code for a given task that, in our example, will be that of writing a function that finds the longest substring of a given string containing only unique characters:</p>
<pre class="programlisting code"><code class="hljs-code">prompt = """
Write a Python function that finds the longest substring of a given string containing only unique characters.
"""
print(llm(prompt))
</code></pre>
<p class="normal">We then get the following function as our output:</p>
<pre class="programlisting con"><code class="hljs-con">def longest_unique_substring(s):
    unique_count = 0
    longest_substring = ""
    for i in range(1, len(s)):
        if s[i] not in unique_count:
            unique_count += 1
            longest_substring = s[i]
    return longest_substring
</code></pre>
<p class="normal">As per the Falcon LLM, in this case we used the light version of the model (7 billion parameters), still obtaining great results. This is a perfect example of how the task you want to address with your application must be a factor in deciding what LLM to use: if you are only interested in code generation, completion, infilling, debugging, or any other code-related tasks, a light and open-source model could be more than enough, rather than 70 billion parameters<a id="_idIndexMarker680"/> of a state-of-the-art GPT-4.</p>
<p class="normal">In the next section, we are going to cover the third and last LLM in the context of code generation and understanding.</p>
<h2 class="heading-2" id="_idParaDest-131">StarCoder</h2>
<p class="normal">The StarCoder model is an LLM for code<a id="_idIndexMarker681"/> that can perform various tasks, such as code completion, code modification, code explanation, and technical assistance. It was trained on permissively licensed data from GitHub, including from 80+ programming languages, Git commits, GitHub issues, and Jupyter notebooks. It has a context length of over 8,000 tokens, which enables it to process more input than any other open-source language model. It also has an improved license that simplifies the process for companies to integrate the model into their products.</p>
<p class="normal">The StarCoder model was evaluated on several benchmarks that test its ability to write and understand code in different languages and domains, including the aforementioned HumanEval and MBPP, where the model scored, respectively, 33.6% and 52.7%. Additionally, it was tested against MultiPL-E (where the model matched or outperformed the code-cushman-001 model from OpenAI on many languages), the DS-1000 (where the model clearly beat the code-cushman-001 model as well as all other open-access models), and the Tech Assistant Prompt (where the model was able to respond to various queries with relevant and accurate information).</p>
<p class="normal">According to a survey published on May 4 2023 by Hugging Face, StarCoder demonstrated great capabilities compared to other models, using HumanEval and MBPP as benchmarks. You can see an illustration of this study below:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_09_04.png"/></figure>
<p class="packt_figref">Figure 9.4: Results of evaluation benchmarks for various LLMs. Source: <a href="https://huggingface.co/blog/starcoder">https://huggingface.co/blog/starcoder</a></p>
<p class="normal">To start using StarCoder, we can follow<a id="_idIndexMarker682"/> these steps:</p>
<ol>
<li class="numberedList" value="1">We can leverage the HuggingFaceHub wrapper available in LangChain (remember to set the Hugging Face API in the <code class="inlineCode">.env</code> file):
        <pre class="programlisting code-one"><code class="hljs-code">import os
from dotenv import load_dotenv
load_dotenv()
hugging_face_api = os.environ["HUGGINGFACEHUB_API_TOKEN"]
</code></pre>
</li>
<li class="numberedList">Let’s set the <code class="inlineCode">repo_id</code> for the StarCoder model and initialize it:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain import HuggingFaceHub
from langchain import PromptTemplate, LLMChain
repo_id = "bigcode/starcoderplus"
llm = HuggingFaceHub(
    repo_id=repo_id,  model_kwargs={"temperature": 0.2, "max_new_tokens": 500}
)
</code></pre>
</li>
</ol>
<div><p class="normal"><strong class="keyWord">Note</strong></p>
<p class="normal">StarCoder is a gated model on the Hugging Face Hub, meaning that you will need to request access directly from the bigcode/starcoderplus repo before being able to connect to it.</p>
</div>
<p class="normal">Now that we’re set up, let’s start asking<a id="_idIndexMarker683"/> our model to compile some code. To start with, we will ask it to generate a Python function to generate the nth Fibonacci number:</p>
<pre class="programlisting code"><code class="hljs-code">prompt = """
How can I write a Python function to generate the nth Fibonacci number?
"""
print(llm(prompt))
</code></pre>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">The Fibonacci sequence is a mathematical<a id="_idIndexMarker684"/> series that begins with 0 and 1, and each subsequent number is the sum of the two preceding numbers. For instance, the first 10 numbers of the Fibonacci sequence are 0, 1, 1, 2, 3, 5, 8, 13, 21, and 34.</p>
<p class="normal">There are different ways to compute the nth Fibonacci number, which is denoted by F(n). One way is to use a recursive formula:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_09_001.png"/></figure>
<p class="normal">This means that to find F(n), we need to find F(n-1) and F(n-2) first, and then add them together. This works for any n greater than or equal to 2. For n equal to 0 or 1, we simply return n as the answer.</p>
</div>
<p class="normal">We then see the following output:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_09_05.png"/></figure>
<p class="packt_figref">Figure 9.5: Example of Fibonacci functions generated by StarCode</p>
<p class="normal">As you can see, it also proposed<a id="_idIndexMarker685"/> different approaches to solve the problem, alongside the explanation.</p>
<p class="normal">Let’s now ask the model to generate a webpage to play tic tac toe against the computer:</p>
<pre class="programlisting code"><code class="hljs-code">prompt = """
Generate the html code for a single page website that let the user play tic tac toe.
 """
print(llm(prompt))
</code></pre>
<p class="normal">Here is the corresponding output:</p>
<pre class="programlisting con"><code class="hljs-con"> ## How to use
```
git clone https://github.com/Mohamed-Elhawary/tic-tac-toe.git
cd tic-tac-toe
python3 -m http.server
```
## License
 [MIT](https://choosealicense.com/licenses/mit/)
&lt;|endoftext|&gt;
</code></pre>
<p class="normal">Interestingly enough, the model<a id="_idIndexMarker686"/> in this case didn’t generate the whole code; rather, it gave the instructions to clone and run a git repository that can achieve this result.</p>
<p class="normal">Finally, StarCoder is also available as an extension in VS Code<a id="_idIndexMarker687"/> to act as your code copilot. You can find it as <strong class="keyWord">HF Code Autocomplete</strong>, as shown in the following screenshot:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_09_06.png"/></figure>
<p class="packt_figref">Figure 9.6: Hugging Face Code Autocomplete extension, powered by StarCoder</p>
<p class="normal">Once enabled, you can see that, while compiling your code, StarCoder will provide suggestions to complete the code. For example:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_09_07.png"/></figure>
<p class="packt_figref">Figure 9.7: Screenshot of a suggested completion, given a function description</p>
<p class="normal">As you can see, I commented my code, describing a function to generate the nth Fibonacci number, and then started defining the function. Automatically, I’ve been provided with the StarCoder auto-completion suggestion.</p>
<p class="normal">Code understanding and generation are great capabilities of LLMs. On top of those capabilities, there are further applications that we can think about, going beyond code generation. In fact, the code can be seen also as a backend reasoning tool to propose solutions to complex problems, such as an energy optimization problem rather than an algorithm task. To do this, we can leverage LangChain<a id="_idIndexMarker688"/> to create powerful agents that can <em class="italic">act as if they were algorithms</em>. In the upcoming section, we will see how to do so.</p>
<h1 class="heading-1" id="_idParaDest-132">Act as an algorithm</h1>
<p class="normal">Some problems are complex<a id="_idIndexMarker689"/> by definition and difficult to solve leveraging “only” LLMs’ analytical reasoning skills. However, LLMs are still intelligent enough to understand the problems overall and leverage their coding capabilities to solve them.</p>
<p class="normal">In this context, LangChain<a id="_idIndexMarker690"/> provides a tool that empowers the LLM to reason “in Python,” meaning that the LLM-powered agent will leverage Python to solve complex problems. This tool is the Python REPL, which is a simple Python shell that can execute Python commands. The Python REPL is important because it allows users to perform complex calculations, generate code, and interact with language models using Python syntax. In this section, we will cover some examples of the tool’s capabilities.</p>
<p class="normal">Let’s first initialize our agent<a id="_idIndexMarker691"/> using the <code class="inlineCode">create_python_agent</code> class in LangChain. To do so, we<a id="_idIndexMarker692"/> will need to provide this class with an LLM and a tool, which, in our example, will be the Python REPL:</p>
<pre class="programlisting code"><code class="hljs-code">import os
from dotenv import load_dotenv
from langchain.agents.agent_types import AgentType
from langchain.chat_models import ChatOpenAI
 from langchain_experimental.agents.agent_toolkits.python.base import create_python_agent
from langchain_experimental.tools import PythonREPLTool
load_dotenv()
openai_api_key = os.environ['OPENAI_API_KEY']
model = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613")
agent_executor = create_python_agent(
    llm=model,
    tool=PythonREPLTool(),
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)
</code></pre>
<p class="normal">As always, before starting to work with the agent, let’s first inspect the default prompt:</p>
<pre class="programlisting code"><code class="hljs-code">print(agent_executor.agent.llm_chain.prompt.template)
</code></pre>
<p class="normal">Here is our output:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer program  Description automatically generated" src="img/B21714_09_08.png"/></figure>
<p class="packt_figref">Figure 9.8: Default prompt of the Python agent</p>
<p class="normal">Now, let’s start with an<a id="_idIndexMarker693"/> easy query, asking the model to generate a scatter plot based on sample attributes of basketball players:</p>
<pre class="programlisting code"><code class="hljs-code">query = """
In a different basketball game, we have the following player stats:
- Player A: 38 points, 10 rebounds, 7 assists
- Player B: 28 points, 9 rebounds, 6 assists
- Player C: 19 points, 6 rebounds, 3 assists
- Player D: 12 points, 4 rebounds, 2 assists
- Player E: 7 points, 2 rebounds, 1 assist
Could you create a scatter plot graph in Seaborn talk mode for each player, where the y-axis represents the number of points, the x-axis represents the number of rebounds, and use 'o' as the marker? Additionally, please label each point with the player's name and set the title as "Team Players."
"""
agent_executor.run(query)
</code></pre>
<p class="normal">We then get the following output:</p>
<pre class="programlisting con"><code class="hljs-con">Invoking: `Python_REPL` with `import seaborn as sns
import matplotlib.pyplot as plt
# Player stats
players = ['Player A', 'Player B', 'Player C', 'Player D', 'Player E']
points = [38, 28, 19, 12, 7]
rebounds = [10, 9, 6, 4, 2]
# Create scatter plot
sns.scatterplot(x=rebounds, y=points, marker='o')
# Label each point with player's name
for i, player in enumerate(players):
    plt.text(rebounds[i], points[i], player, ha='center', va='bottom')
# Set title
plt.title('Team Players')
# Show the plot
plt.show()`
</code></pre>
<p class="normal">This output is accompanied<a id="_idIndexMarker694"/> by the following graph based on the players’ statistics:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer program  Description automatically generated" src="img/B21714_09_09.png"/></figure>
<p class="packt_figref">Figure 9.9: Sample plot generated by the Python agent</p>
<p class="normal">Let’s look at another example. Say we want to predict the price of a house based on some features, such as the number of bedrooms or the size of the house. To do so, we can ask our agent to design and train a model to give us the result of a given house. For example, let’s consider the following prompt:</p>
<pre class="programlisting code"><code class="hljs-code">query = """
I want to predict the price of a house given the following information:
- the number of rooms
- the number of bathrooms
- the size of the house in square meters
Design and train a regression model to predict the price of a house. Generate and use synthetic data to train the model.
Once the model is trained, tell me the price of a house with the following features:
- 2 rooms
- 1 bathroom
- 100 square meters
"""
agent_executor.run(query)
</code></pre>
<p class="normal">Here, we ask the agent to train a regression<a id="_idIndexMarker695"/> model on synthetic data (representative of houses with various configurations of rooms, bathrooms, and area, each with an associated price as a dependent variable) to give us the estimated price of a house with the above features. Let’s see the output:</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
Invoking: `Python_REPL` with `import numpy as np
from sklearn.linear_model import LinearRegression
# Generate synthetic data
np.random.seed(0)
X = np.random.rand(100, 3)  # 100 houses with 3 features: rooms, bathrooms, size
y = 100000 * X[:, 0] + 200000 * X[:, 1] + 300000 * X[:, 2] + 50000  # Price = 100k * rooms + 200k * bathrooms + 300k * size + 50k
# Train the regression model
model = LinearRegression()
model.fit(X, y)
# Predict the price of a house with the given features
features = np.array([[2, 1, 100]])
predicted_price = model.predict(features)
predicted_price`
responded: {content}
The predicted price of a house with 2 rooms, 1 bathroom, and 100 square meters is approximately $550,000.
&gt; Finished chain.
'The predicted price of a house with 2 rooms, 1 bathroom, and 100 square meters is approximately $550,000.'
</code></pre>
<p class="normal">As you can see, the agent was able to generate synthetic training data, train a proper regression model using the <code class="inlineCode">sklearn</code> libraries, and predict with the model the price of the house we provided.</p>
<p class="normal">With this approach, we can program an agent to act as an algorithm in real-time scenarios. Imagine, for example, that we want to design an agent that is capable of solving optimization problems in a smart building environment. The goal<a id="_idIndexMarker696"/> is to optimize the <strong class="keyWord">Heating, Ventilation and Air Conditioning</strong> (<strong class="keyWord">HVAC</strong>) setpoints in the building to minimize energy costs while ensuring occupant comfort. Let’s define the variables and constraints of the problem: the objective is to adjust<a id="_idIndexMarker697"/> the temperature setpoints within the specified comfort ranges for each of the three zones while considering the varying energy costs per degree, per hour. </p>
<p class="normal">The goal is to strike a balance between energy efficiency and occupant comfort. Below, you can find a description of the problem and also the initialization of our variables and constraints (energy cost per zone, initial temperature per zone, and comfort range per zone):</p>
<pre class="programlisting code"><code class="hljs-code">query = """
**Problem**:
You are tasked with optimizing the HVAC setpoints in a smart building to minimize energy costs while ensuring occupant comfort. The building has three zones, and you can adjust the temperature setpoints for each zone. The cost function for energy consumption is defined as:
- Zone 1: Energy cost = $0.05 per degree per hour
- Zone 2: Energy cost = $0.07 per degree per hour
- Zone 3: Energy cost = $0.06 per degree per hour
You need to find the optimal set of temperature setpoints for the three zones to minimize the energy cost while maintaining a comfortable temperature. The initial temperatures in each zone are as follows:
- Zone 1: 72°F
- Zone 2: 75°F
- Zone 3: 70°F
The comfort range for each zone is as follows:
- Zone 1: 70°F to 74°F
- Zone 2: 73°F to 77°F
- Zone 3: 68°F to 72°F
**Question**:
What is the minimum total energy cost (in dollars per hour) you can achieve by adjusting the temperature setpoints for the three zones within their respective comfort ranges?
"""
agent_executor.run(query)
</code></pre>
<p class="normal">We then get the following <a id="_idIndexMarker698"/>output (you can find the whole reasoning chain in the book’s GitHub repository):</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
Invoking: `Python_REPL` with `import scipy.optimize as opt
# Define the cost function
def cost_function(x):
    zone1_temp = x[0]
    zone2_temp = x[1]
    zone3_temp = x[2]
   
    # Calculate the energy cost for each zone
    zone1_cost = 0.05 * abs(zone1_temp - 72)
    zone2_cost = 0.07 * abs(zone2_temp - 75)
    zone3_cost = 0.06 * abs(zone3_temp - 70)
[…]
&gt; Finished chain.
'The minimum total energy cost that can be achieved by adjusting the temperature setpoints for the three zones within their respective comfort ranges is $0.15 per hour.'
</code></pre>
<p class="normal">The agent was able to solve<a id="_idIndexMarker699"/> the smart building optimization problem, finding the minimum total energy cost, given some constraints. Staying in the scope of optimization problems, there are further use cases that these models could address with a similar approach, including:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Supply chain optimization</strong>: Optimize the logistics<a id="_idIndexMarker700"/> and distribution of goods to minimize transportation costs, reduce inventory, and ensure timely deliveries.</li>
<li class="bulletList"><strong class="keyWord">Portfolio optimization</strong>: In finance, use algorithms to construct investment portfolios that maximize returns while managing risk.</li>
<li class="bulletList"><strong class="keyWord">Route planning</strong>: Plan optimal routes for delivery trucks, emergency services, or ride-sharing platforms to minimize travel time and fuel consumption.</li>
<li class="bulletList"><strong class="keyWord">Manufacturing process optimization</strong>: Optimize manufacturing processes to minimize waste, energy consumption, and production costs while maintaining product quality.</li>
<li class="bulletList"><strong class="keyWord">Healthcare resource allocation</strong>: Allocate healthcare resources like hospital beds, medical staff, and equipment efficiently during a pandemic or other healthcare crisis.</li>
<li class="bulletList"><strong class="keyWord">Network routing</strong>: Optimize data routing in computer networks to reduce latency, congestion, and energy consumption.</li>
<li class="bulletList"><strong class="keyWord">Fleet management</strong>: Optimize the use of a fleet of vehicles, such as taxis or delivery vans, to reduce operating costs and improve service quality.</li>
<li class="bulletList"><strong class="keyWord">Inventory management</strong>: Determine optimal inventory levels and reorder points to minimize storage costs while preventing stockouts.</li>
<li class="bulletList"><strong class="keyWord">Agricultural planning</strong>: Optimize crop planting and harvesting schedules based on weather patterns and market demand to maximize yield and profits.</li>
<li class="bulletList"><strong class="keyWord">Telecommunications network design</strong>: Design the layout of telecommunications networks to provide coverage while minimizing infrastructure costs.</li>
<li class="bulletList"><strong class="keyWord">Waste management</strong>: Optimize routes for garbage collection trucks to reduce fuel consumption and emissions.</li>
<li class="bulletList"><strong class="keyWord">Airline crew scheduling</strong>: Create efficient flight crew schedules that adhere to labor regulations<a id="_idIndexMarker701"/> and minimize costs for airlines.</li>
</ul>
<p class="normal">The Python REPL agent<a id="_idIndexMarker702"/> is amazing; however, it comes with some caveats:</p>
<ul>
<li class="bulletList">It does not allow for FileIO, meaning that it cannot read and write with your local file system.</li>
<li class="bulletList">It forgets the variables after every run, meaning that you cannot keep trace of your initialized variables after the model’s response.</li>
</ul>
<p class="normal">To bypass these caveats, in the next section, we are going to cover an open-source project built on top of the LangChain agent: the Code Interpreter API.</p>
<h1 class="heading-1" id="_idParaDest-133">Leveraging Code Interpreter</h1>
<p class="normal">The name “Code Interpreter” was coined<a id="_idIndexMarker703"/> by OpenAI, referring to the recently developed plugin for ChatGPT. The Code Interpreter plugin allows ChatGPT to write and execute computer code in various programming languages. This enables ChatGPT to perform tasks such as calculations, data analysis, and generating visualizations.</p>
<p class="normal">The Code Interpreter plugin is one of the tools designed specifically for language models with safety as a core principle. It helps ChatGPT access up-to-date information, run computations, or use third-party services. The plugin is currently in private beta and is available for selected developers and ChatGPT Plus users.</p>
<p class="normal">While OpenAI’s Code Interpreter still doesn’t offer an API, there are some open-source projects that adapted the concept of this plugin in an open-source Python library. In this section, we are going to leverage the work of Shroominic, available at <a href="https://github.com/shroominic/codeinterpreter-api">https://github.com/shroominic/codeinterpreter-api</a>. You can install it via <code class="inlineCode">pip install codeinterpreterapi</code>.</p>
<p class="normal">According to the blog post published by Shroominic, the author of the Code Interpreter API (which you can read at <a href="https://blog.langchain.dev/code-interpreter-api/">https://blog.langchain.dev/code-interpreter-api/</a>), it is based on the LangChain agent <code class="inlineCode">OpenAIFunctionsAgent</code>.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">OpenAIFunctionsAgent is a type of agent<a id="_idIndexMarker704"/> that can use the OpenAI functions’ ability to respond to the user’s prompts using an LLM. The agent is driven by a model that supports using OpenAI functions, and it has access to a set of tools that it can use to interact with the user.</p>
<p class="normal">The OpenAIFunctionsAgent can also integrate custom functions. For example, you can define custom functions to get the current stock price or stock performance using Yahoo Finance. The OpenAIFunctionsAgent can use the ReAct framework to decide which tool to use, and it can use memory to remember the previous conversation interactions.</p>
</div>
<p class="normal">The API comes already with some tools, such as the possibility to navigate the web to get up-to-date information.</p>
<p class="normal">Yet the greatest difference<a id="_idIndexMarker705"/> from the Python REPL tool that we covered in the previous section is that the Code Interpreter API can actually execute the code it generates. In fact, when a Code Interpreter session starts, a miniature of a Jupyter Kernel is launched on your device, thanks to the underlying<a id="_idIndexMarker706"/> Python execution environment called CodeBox.</p>
<p class="normal">To start using the code interpreter in your notebook, you can install all the dependencies as follows:</p>
<pre class="programlisting code"><code class="hljs-code">!pip install "codeinterpreterapi[all]"
</code></pre>
<p class="normal">In this case, I will ask it to generate a plot of COVID-19 cases in a specific time range:</p>
<pre class="programlisting code"><code class="hljs-code">from codeinterpreterapi import CodeInterpreterSession
import os
from dotenv import load_dotenv
load_dotenv()
api_key = os.environ['OPENAI_API_KEY']
# create a session
async with CodeInterpreterSession() as session:
    # generate a response based on user input
    response = await session.generate_response(
        "Generate a plot of the evolution of Covid-19 from March to June 2020, taking data from web."
    )
    # output the response
print("AI: ", response.content)
    for file in response.files:
        file.show_image()
</code></pre>
<p class="normal">Here is the generated<a id="_idIndexMarker707"/> output, including a graph that shows the number of global confirmed cases in the specified time period:</p>
<pre class="programlisting con"><code class="hljs-con">AI:  Here is the plot showing the evolution of global daily confirmed COVID-19 cases from March to June 2020. As you can see, the number of cases has been increasing over time during this period. Please note that these numbers are cumulative. Each point on the graph represents the total number of confirmed cases up to that date, not just the new cases on that day.
</code></pre>
<figure class="mediaobject"><img alt="A graph with a line going up  Description automatically generated" src="img/B21714_09_10.png"/></figure>
<p class="packt_figref">Figure 9.10: Line chart generated by the Code Intepreter API</p>
<p class="normal">As you can see, the Code Interpreter<a id="_idIndexMarker708"/> answered the question with an explanation as well as a plot.</p>
<p class="normal">Let’s try another one, this time also leveraging its real-time capabilities of searching for up-to-date information. In the following snippet, we ask the model to plot the price of the S&amp;P 500 index over the last 5 days:</p>
<pre class="programlisting code"><code class="hljs-code">async with CodeInterpreterSession() as session:
    # generate a response based on user input
    response = await session.generate_response(
        "Generate a plot of the price of S&amp;P500 index in the last 5 days."
    )
    # output the response
print("AI: ", response.content)
    for file in response.files:
        file.show_image()
</code></pre>
<p class="normal">We then get the following<a id="_idIndexMarker709"/> output, together with a line graph showing the price of the S&amp;P 500 index over the last 5 days:</p>
<pre class="programlisting con"><code class="hljs-con">AI:  Here is the plot of the S&amp;P 500 index for the last 5 days. The y-axis represents the closing price of the index, and the x-axis represents the date.
</code></pre>
<figure class="mediaobject"><img alt="A graph with numbers and a line  Description automatically generated" src="img/B21714_09_11.png"/></figure>
<p class="packt_figref">Figure 9.11: S&amp;P 500 index price plotted by the Code Interpreter API</p>
<p class="normal">Finally, we can provide local files to the Code Interpreter so that it can perform some analyses on that specific data. For example, I’ve downloaded the Titanic dataset from Kaggle at <a href="https://www.kaggle.com/datasets/brendan45774/test-file">https://www.kaggle.com/datasets/brendan45774/test-file</a>. The Titanic dataset is a popular dataset for machine learning that describes the survival status of individual passengers on the Titanic. It contains information such as age, sex, class, fare, and whether they survived or not.</p>
<p class="normal">Once the dataset had downloaded, I passed<a id="_idIndexMarker710"/> it as a parameter to the model as follows:</p>
<pre class="programlisting code"><code class="hljs-code">from codeinterpreterapi import CodeInterpreterSession, File
#os.environ["HUGGINGFACEHUB_API_TOKEN"]
os.environ['OPENAI_API_KEY'] = "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"
os.environ['VERBOSE'] = "True"
async with CodeInterpreterSession() as session:
        # define the user request
        user_request = "Analyze this dataset and plot something interesting about it."
        files = [
            File.from_path("drive/MyDrive/titanic.csv"),
        ]
        # generate the response
        response = await session.generate_response(
            user_request, files=files
        )
        # output to the user
print("AI: ", response.content)
        for file in response.files:
            file.show_image()
</code></pre>
<p class="normal">We then get the following output:</p>
<pre class="programlisting con"><code class="hljs-con">AI:  The plot shows the survival count based on the passenger class. It appears that passengers in the 3rd class had a significantly lower survival rate compared to those in the 1st and 2nd classes. This could suggest that the class of the passengers might have influenced their survival, possibly due to factors such as the location of their cabins and access to lifeboats.
These are just a few examples of the kind of insights we can extract from this dataset. Depending on the specific questions you're interested in, we could perform further analysis. For example, we could look at the survival rate based on age, or investigate whether the fare passengers paid had any influence on their survival.
</code></pre>
<figure class="mediaobject"><img alt="A screenshot of a graph  Description automatically generated" src="img/B21714_09_12.png"/></figure>
<p class="packt_figref">Figure 9.12: Sample plots generated by the Code Interpreter API</p>
<p class="normal">As you can see, the model was able to generate to bar charts showing the survival status grouped by sex (in the first plot) and then by class (in the second plot).</p>
<p class="normal">The Code Interpreter plugin, together<a id="_idIndexMarker711"/> with code-specific LLMs and the Python agent, are great examples of how LLMs are having a huge impact on the world of software development. This can be summarized in two main capabilities:</p>
<ul>
<li class="bulletList">LLMs can understand and generate code, since they have been trained on a huge amount of programming languages, GitHub repos, StackOverflow conversations, and so on. Henceforth, along with natural language, programming languages are part of their parametric knowledge.</li>
<li class="bulletList">LLMs can understand a user’s intent and act as a reasoning engine to activate tools like Python REPL or Code Interpreter, which are then able to provide a response by working with code.</li>
</ul>
<p class="normal">Overall, LLMs are going well beyond the elimination of the gap between natural language and machine language: rather, they are integrating the two so that they can leverage each other to respond to a user’s query.</p>
<h1 class="heading-1" id="_idParaDest-134">Summary</h1>
<p class="normal">In this chapter, we explored multiple ways in which LLMs can be leveraged to work with code. Armed with a refresher of how to evaluate LLMs and the specific evaluation benchmarks to take into account when choosing an LLM for code-related tasks, we delved into practical experimentations.</p>
<p class="normal">We started from the “plain vanilla” application that we have all tried at least once using ChatGPT, which is code understanding and generation. For this purpose, we leveraged three different models – Falcon LLM, CodeLlama, and StarCoder – each resulting in very good results.</p>
<p class="normal">We then moved forward with the additional applications that LLMs’ coding capabilities can have in the real world. In fact, we saw how code-specific knowledge can be used as a booster to solve complex problems, such as algorithmic or optimization tasks. Furthermore, we covered how code knowledge can not only be used in the backend reasoning of an LLM but also actually executed in a working notebook, leveraging the open-source version of the Code Interpreter API.</p>
<p class="normal">With this chapter, we are getting closer to the end of Part 2. So far, we have covered the multiple capabilities of LLMs, while always handling language data (natural or code). In the next chapter, we will see how to go a step further toward multi-modality and build powerful multi-modal agents that can handle data in multiple formats.</p>
<h1 class="heading-1" id="_idParaDest-135">References</h1>
<ul>
<li class="bulletList">The open-source version of the Code Interpreter API: <a href="https://github.com/shroominic/codeinterpreter-api">https://github.com/shroominic/codeinterpreter-api</a></li>
<li class="bulletList">StarCoder: <a href="https://huggingface.co/blog/starcoder">https://huggingface.co/blog/starcoder</a></li>
<li class="bulletList">The LangChain agent for the Python REPL: <a href="https://python.langchain.com/docs/integrations/toolkits/python">https://python.langchain.com/docs/integrations/toolkits/python</a></li>
<li class="bulletList">A LangChain blog about the Code Interpreter API: <a href="https://blog.langchain.dev/code-interpreter-api/">https://blog.langchain.dev/code-interpreter-api/</a></li>
<li class="bulletList">The Titanic dataset: <a href="https://www.kaggle.com/datasets/brendan45774/test-file">https://www.kaggle.com/datasets/brendan45774/test-file</a></li>
<li class="bulletList">The HF Inference Endpoint: <a href="https://huggingface.co/docs/inference-endpoints/index ">https://huggingface.co/docs/inference-endpoints/index</a></li>
<li class="bulletList">The CodeLlama model card: <a href="https://huggingface.co/codellama/CodeLlama-7b-hf">https://huggingface.co/codellama/CodeLlama-7b-hf</a></li>
<li class="bulletList">Code Llama: Open Foundation Models for Code, <em class="italic">Rozière. B., et al</em> (2023): <a href="https://arxiv.org/abs/2308.12950">https://arxiv.org/abs/2308.12950</a></li>
<li class="bulletList">The Falcon LLM model card: <a href="https://huggingface.co/tiiuae/falcon-7b-instruct">https://huggingface.co/tiiuae/falcon-7b-instruct</a></li>
<li class="bulletList">The StarCoder model card: <a href="https://huggingface.co/bigcode/starcoder">https://huggingface.co/bigcode/starcoder</a></li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:<a href=" https://packt.link/llm "/></p>
<p class="normal"><a href=" https://packt.link/llm ">https://packt.link/llm</a></p>
<p class="normal"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png"/></p>
</div>
</body></html>