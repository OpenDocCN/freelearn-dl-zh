- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Why Retrieval Augmented Generation?
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么需要检索增强生成？
- en: Even the most advanced generative AI models can only generate responses based
    on the data they have been trained on. They cannot provide accurate answers to
    questions about information outside their training data. Generative AI models
    simply don’t know that they don’t know! This leads to inaccurate or inappropriate
    outputs, sometimes called hallucinations, bias, or, simply said, nonsense.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是最先进的生成式AI模型也只能根据它们训练的数据生成响应。它们无法提供关于其训练数据之外信息的准确答案。生成式AI模型根本不知道它们不知道！这导致输出不准确或不恰当，有时被称为幻觉、偏见，或者简单地说，胡言乱语。
- en: '**Retrieval Augmented Generation** (**RAG**) is a framework that addresses
    this limitation by combining retrieval-based approaches with generative models.
    It retrieves relevant data from external sources in real time and uses this data
    to generate more accurate and contextually relevant responses. Generative AI models
    integrated with RAG retrievers are revolutionizing the field with their unprecedented
    efficiency and power. One of the key strengths of RAG is its adaptability. It
    can be seamlessly applied to any type of data, be it text, images, or audio. This
    versatility makes RAG ecosystems a reliable and efficient tool for enhancing generative
    AI capabilities.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索增强生成**（**RAG**）是一个框架，通过结合基于检索的方法和生成模型来解决这个问题。它实时从外部来源检索相关数据，并使用这些数据生成更准确和上下文相关的响应。集成了RAG检索器的生成式AI模型正在以前所未有的效率和力量改变这个领域。RAG的一个关键优势是其适应性。它可以无缝应用于任何类型的数据，无论是文本、图像还是音频。这种多功能性使RAG生态系统成为增强生成式AI能力的可靠和高效工具。'
- en: A project manager, however, already encounters a wide range of generative AI
    platforms, frameworks, and models such as Hugging Face, Google Vertex AI, OpenAI,
    LangChain, and more. An additional layer of emerging RAG frameworks and platforms
    will only add complexity with Pinecone, Chroma, Activeloop, LlamaIndex, and so
    on. All these Generative AI and RAG frameworks often overlap, creating an incredible
    number of possible configurations. Finding the right configuration of models and
    RAG resources for a specific project, therefore, can be challenging for a project
    manager. There is no silver bullet. The challenge is tremendous, but the rewards,
    when achieved, are immense!
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，项目经理已经遇到了各种生成式AI平台、框架和模型，例如Hugging Face、Google Vertex AI、OpenAI、LangChain等等。新增的一层新兴的RAG框架和平台，如Pinecone、Chroma、Activeloop、LlamaIndex等，只会增加复杂性。因此，为特定项目找到合适的模型和RAG资源配置对项目经理来说可能具有挑战性。没有一劳永逸的解决方案。挑战巨大，但一旦实现，回报将是巨大的！
- en: 'We will begin this chapter by defining the RAG framework at a high level. Then,
    we will define the three main RAG configurations: naïve RAG, advanced RAG, and
    modular RAG. We will also compare RAG and fine-tuning and determine when to use
    these approaches. RAG can only exist within an ecosystem, and we will design and
    describe one in this chapter. Data needs to come from somewhere and be processed.
    Retrieval requires an organized environment to retrieve data, and generative AI
    models have input constraints.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，从高层次上定义RAG框架。然后，我们将定义三种主要的RAG配置：朴素RAG、高级RAG和模块化RAG。我们还将比较RAG和微调，并确定何时使用这些方法。RAG只能存在于一个生态系统中，我们将在本章设计和描述一个。数据需要从某个地方来，并经过处理。检索需要一个有组织的环境来检索数据，而生成式AI模型有输入限制。
- en: Finally, we will dive into the practical aspect of this chapter. We will build
    a Python program from scratch to run entry-level naïve RAG with keyword search
    and matching. We will also code an advanced RAG system with vector search and
    index-based retrieval. Finally, we will build a modular RAG that takes both naïve
    and advanced RAG into account. By the end of this chapter, you will acquire a
    theoretical understanding of the RAG framework and practical experience in building
    a RAG-driven generative AI program. This hands-on approach will deepen your understanding
    and equip you for the following chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将深入探讨本章的实践方面。我们将从头开始构建一个Python程序，以运行基于关键字搜索和匹配的入门级朴素RAG。我们还将编写一个基于向量搜索和基于索引检索的高级RAG系统。最后，我们将构建一个模块化RAG，它同时考虑了朴素RAG和高级RAG。到本章结束时，你将获得对RAG框架的理论理解，并在构建RAG驱动的生成式AI程序方面获得实践经验。这种动手方法将加深你的理解，并为你准备后续章节。
- en: 'In a nutshell, this chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章涵盖了以下主题：
- en: Defining the RAG framework
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义RAG框架
- en: The RAG ecosystem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG生态系统
- en: Naïve keyword search and match RAG in Python
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现简单的关键词搜索和匹配RAG
- en: Advanced RAG with vector-search and index-based RAG in Python
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中使用向量搜索和基于索引的RAG的高级RAG
- en: Building a modular RAG program
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建模块化的RAG程序
- en: Let’s begin by defining RAG.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义RAG开始。
- en: What is RAG?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是RAG？
- en: When a generative AI model doesn’t know how to answer accurately, some say it
    is hallucinating or producing bias. Simply said, it just produces nonsense. However,
    it all boils down to the impossibility of providing an adequate response when
    the model’s training didn’t include the information requested beyond the classical
    model configuration issues. This confusion often leads to random sequences of
    the most probable outputs, not the most accurate ones.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个生成式AI模型不知道如何准确回答时，有人说它是在幻想或产生偏见。简单来说，它只是产生了胡言乱语。然而，所有这些都归结于当模型的训练没有包括超出经典模型配置请求的信息时，无法提供充分响应的不可能性。这种混淆通常会导致随机序列的最可能输出，而不是最准确的输出。
- en: 'RAG begins where generative AI ends by providing the information an LLM model
    lacks to answer accurately. RAG was designed (Lewis et al., 2020) for LLMs. The
    RAG framework will perform optimized information retrieval tasks, and the generation
    ecosystem will add this information to the input (user query or automated prompt)
    to produce improved output. The RAG framework can be summed up at a high level
    in the following figure:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RAG在生成式AI结束的地方开始，它提供了LLM模型缺乏的信息以准确回答。RAG是为LLM设计的（Lewis等人，2020）。RAG框架将执行优化的信息检索任务，生成生态系统将添加这些信息到输入（用户查询或自动提示）以产生改进的输出。RAG框架可以用以下图示在高级别上总结：
- en: '![A diagram of a library  Description automatically generated](img/B31169_01_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图书馆的示意图 自动生成描述](img/B31169_01_01.png)'
- en: 'Figure 1.1: The two main components of RAG-driven generative AI'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1：由RAG驱动的生成式AI的两个主要组件
- en: Think of yourself as a student in a library. You have an essay to write on RAG.
    Like ChatGPT, for example, or any other AI copilot, you have learned how to read
    and write. As with any **Large Language Model** (**LLM**), you are sufficiently
    trained to read advanced information, summarize it, and write content. However,
    like any superhuman AI you will find from Hugging Face, Vertex AI, or OpenAI,
    there are many things you don’t know.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将自己想象成一个图书馆里的学生。你需要写一篇关于RAG的论文。比如，像ChatGPT一样，或者任何其他AI辅助工具，你已经学会了如何阅读和写作。就像任何**大型语言模型**（**LLM**）一样，你已经接受了足够的训练来阅读高级信息、总结信息并撰写内容。然而，就像你从Hugging
    Face、Vertex AI或OpenAI找到的任何超人类AI一样，有许多事情你并不知道。
- en: In the *retrieval* phase, you search the library for books on the topic you
    need (the left side of *Figure 1.1*). Then, you go back to your seat, perform
    a retrieval task by yourself or a co-student, and extract the information you
    need from those books. In the *generation* phase (the right side of *Figure 1.1*),
    you begin to write your essay. You are a RAG-driven generative human agent, much
    like a RAG-driven generative AI framework.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在**检索**阶段，你会在图书馆里寻找关于你需要的话题的书籍（*图1.1*的左侧）。然后，你回到自己的座位，自己或与同学一起执行检索任务，并从这些书籍中提取所需的信息。在**生成**阶段（*图1.1*的右侧），你开始撰写你的论文。你是一个由RAG驱动的生成式人类代理，就像一个由RAG驱动的生成式AI框架一样。
- en: As you continue to write your essay on RAG, you stumble across some tough topics.
    You don’t have the time to go through all the information available physically!
    You, as a generative human agent, are stuck, just as a generative AI model would
    be. You may try to write something, just as a generative AI model does when its
    output makes little sense. But you, like the generative AI agent, will not realize
    whether the content is accurate or not until somebody corrects your essay and
    you get a grade that will rank your essay.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当你继续在RAG上撰写你的论文时，你会遇到一些棘手的话题。你没有时间去浏览所有可用的信息！作为生成式人类代理的你，就像生成式AI模型一样陷入了困境。你可能尝试写一些东西，就像生成式AI模型在其输出几乎没有意义时所做的。但就像生成式AI代理一样，你不会意识到内容是否准确，直到有人纠正你的论文，你得到一个能对你的论文进行排名的成绩。
- en: At this point, you have reached your limit and decide to turn to a RAG generative
    AI copilot to ensure you get the correct answers. However, you are puzzled by
    the number of LLM models and RAG configurations available. You need first to understand
    the resources available and how RAG is organized. Let’s go through the main RAG
    configurations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经达到了自己的极限，决定转向一个RAG生成式AI辅助工具以确保你得到正确的答案。然而，你被可用的LLM模型和RAG配置的数量所困惑。首先，你需要了解可用的资源以及RAG是如何组织的。让我们来了解一下主要的RAG配置。
- en: Naïve, advanced, and modular RAG configurations
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 天真、高级和模块化RAG配置
- en: 'A RAG framework necessarily contains two main components: a retriever and a
    generator. The generator can be any LLM or foundation multimodal AI platform or
    model, such as GPT-4o, Gemini, Llama, or one of the hundreds of variations of
    the initial architectures. The retriever can be any of the emerging frameworks,
    methods, and tools such as Activeloop, Pinecone, LlamaIndex, LangChain, Chroma,
    and many more.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个RAG框架必然包含两个主要组件：一个检索器和一个生成器。生成器可以是任何LLM或基础多模态AI平台或模型，例如GPT-4o、Gemini、Llama或初始架构的数百种变体之一。检索器可以是任何新兴的框架、方法和工具，如Activeloop、Pinecone、LlamaIndex、LangChain、Chroma等。
- en: 'The issue now is to decide which of the three types of RAG frameworks (Gao
    et al., 2024) will fit the needs of a project. We will illustrate these three
    approaches in code in the *Naïve, advanced, and modular RAG in code* section of
    this chapter:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是决定三种类型的RAG框架（Gao et al., 2024）中哪一种将适合项目的需求。我们将在本章的“代码中的天真、高级和模块化RAG”部分用代码展示这三种方法：
- en: '**Naïve RAG**: This type of RAG framework doesn’t involve complex data embedding
    and indexing. It can be efficient to access reasonable amounts of data through
    keywords, for example, to augment a user’s input and obtain a satisfactory response.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**天真RAG**：这种类型的RAG框架不涉及复杂的数据嵌入和索引。通过关键词访问合理数量的数据可能很有效，例如，增强用户的输入并获得满意的响应。'
- en: '**Advanced RAG**: This type of RAG involves more complex scenarios, such as
    with vector search and indexed-base retrieval applied. Advanced RAG can be implemented
    with a wide range of methods. It can process multiple data types, as well as multimodal
    data, which can be structured or unstructured.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高级RAG**：这种类型的RAG涉及更复杂的场景，例如使用向量搜索和基于索引的检索。高级RAG可以用多种方法实现。它可以处理多种数据类型，以及结构化或非结构化的多模态数据。'
- en: '**Modular RAG**: Modular RAG broadens the horizon to include any scenario that
    involves naïve RAG, advanced RAG, machine learning, and any algorithm needed to
    complete a complex project.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模块化RAG**：模块化RAG将视野扩展到包括任何涉及天真RAG、高级RAG、机器学习和完成复杂项目所需任何算法的场景。'
- en: However, before going further, we need to decide if we should implement RAG
    or fine-tune a model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在继续之前，我们需要决定是应该实现RAG还是微调模型。
- en: RAG versus fine-tuning
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG与微调的比较
- en: RAG is not always an alternative to fine-tuning, and fine-tuning cannot always
    replace RAG. If we accumulate too much data in RAG datasets, the system may become
    too cumbersome to manage. On the other hand, we cannot fine-tune a model with
    dynamic, ever-changing data such as daily weather forecasts, stock market values,
    corporate news, and all forms of daily events.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RAG并不总是微调的替代品，微调也不能总是替代RAG。如果我们积累过多的数据在RAG数据集中，系统可能会变得过于繁琐难以管理。另一方面，我们无法对动态、不断变化的数据进行模型微调，例如每日天气预报、股市价值、公司新闻以及所有形式的日常事件。
- en: 'The decision of whether to implement RAG or fine-tune a model relies on the
    proportion of parametric versus non-parametric information. The fundamental difference
    between a model trained from scratch or fine-tuned and RAG can be summed up in
    terms of parametric and non-parametric knowledge:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 是否实现RAG或微调模型的决定取决于参数化信息与非参数化信息的比例。从头开始训练或微调的模型与RAG之间的基本区别可以用参数化和非参数化知识来概括：
- en: '**Parametric**: In a RAG-driven generative AI ecosystem, the parametric part
    refers to the generative AI model’s parameters (weights) learned through training
    data. This means the model’s knowledge is stored in these learned weights and
    biases. The original training data is transformed into a mathematical form, which
    we call a parametric representation. Essentially, the model “remembers” what it
    learned from the data, but the data itself is not stored explicitly.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数化**：在一个由RAG驱动的生成式AI生态系统中，参数化部分指的是通过训练数据学习到的生成式AI模型的参数（权重）。这意味着模型的知识存储在这些学习到的权重和偏差中。原始训练数据被转换成一种数学形式，我们称之为参数化表示。本质上，模型“记住”了从数据中学到的东西，但数据本身并没有明确存储。'
- en: '**Non-Parametric**: In contrast, the non-parametric part of a RAG ecosystem
    involves storing explicit data that can be accessed directly. This means that
    the data remains available and can be queried whenever needed. Unlike parametric
    models, where knowledge is embedded indirectly in the weights, non-parametric
    data in RAG allows us to see and use the actual data for each output.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非参数**：相比之下，RAG生态系统的非参数部分涉及存储可以直接访问的显式数据。这意味着数据始终保持可用，并且可以在需要时查询。与参数化模型不同，其中知识间接地嵌入在权重中，RAG中的非参数数据允许我们查看和使用每个输出的实际数据。'
- en: The difference between RAG and fine-tuning relies on the amount of static (parametric)
    and dynamic (non-parametric) ever-evolving data the generative AI model must process.
    A system that relies too heavily on RAG might become overloaded and cumbersome
    to manage. A system that relies too much on fine-tuning a generative model will
    display its inability to adapt to daily information updates.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: RAG与微调之间的区别在于生成式AI模型必须处理的静态（参数化）和动态（非参数）不断变化的数据量。过度依赖RAG的系统可能会变得过载且难以管理。过度依赖微调生成模型的系统将显示出其无法适应日常信息更新的能力。
- en: There is a decision-making threshold illustrated in *Figure 1.2* that shows
    that a RAG-driven generative AI project manager will have to evaluate the potential
    of the ecosystem’s trained parametric generative AI model before implementing
    a non-parametric (explicit data) RAG framework. The potential of the RAG component
    requires careful evaluation as well.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1.2中有一个决策阈值，展示了RAG驱动的生成式AI项目经理在实施非参数（显式数据）RAG框架之前，必须评估生态系统训练的参数化生成式AI模型的潜力。RAG组件的潜力也需要仔细评估。
- en: '![A diagram of a temperature measurement  Description automatically generated](img/B31169_01_02.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![温度测量图  自动生成描述](img/B31169_01_02.png)'
- en: 'Figure 1.2: The decision-making threshold between enhancing RAG or fine-tuning
    an LLM'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2：增强RAG或微调LLM之间的决策阈值
- en: In the end, the balance between enhancing the retriever and the generator in
    a RAG-driven generative AI ecosystem depends on a project’s specific requirements
    and goals. RAG and fine-tuning are not mutually exclusive.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在RAG驱动的生成式AI生态系统中，增强检索器和生成器之间的平衡取决于项目的具体需求和目标。RAG和微调不是相互排斥的。
- en: 'RAG can be used to improve a model’s overall efficiency, together with fine-tuning,
    which serves as a method to enhance the performance of both the retrieval and
    generation components within the RAG framework. We will fine-tune a proportion
    of the retrieval data in *Chapter 9*, *Empowering AI Models: Fine-Tuning RAG Data
    and Human Feedback*.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: RAG可以用来提高模型的整体效率，与微调相结合，作为增强RAG框架内检索和生成组件性能的方法。我们将在第9章“赋予AI模型力量：微调RAG数据和人类反馈”中微调一部分检索数据。
- en: We will now see how a RAG-driven generative AI involves an ecosystem with many
    components.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看到RAG驱动的生成式AI如何涉及一个由许多组件组成的生态系统。
- en: The RAG ecosystem
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG生态系统
- en: 'RAG-driven generative AI is a framework that can be implemented in many configurations.
    RAG’s framework runs within a broad ecosystem, as shown in *Figure 1.3*. However,
    no matter how many retrieval and generation frameworks you encounter, it all boils
    down to the following four domains and questions that go with them:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: RAG驱动的生成式AI是一个可以在许多配置中实现的框架。RAG的框架运行在一个广泛的生态系统中，如图1.3所示。然而，无论你遇到多少检索和生成框架，最终都会归结为以下四个领域及其相关问题：
- en: '**Data**: Where is the data coming from? Is it reliable? Is it sufficient?
    Are there copyright, privacy, and security issues?'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据**：数据从哪里来？它是可靠的吗？是否足够？存在版权、隐私和安全问题吗？'
- en: '**Storage**: How is the data going to be stored before or after processing
    it? What amount of data will be stored?'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储**：在处理之前或之后，数据将如何存储？将存储多少数据？'
- en: '**Retrieval**: How will the correct data be retrieved to augment the user’s
    input before it is sufficient for the generative model? What type of RAG framework
    will be successful for a project?'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索**：如何检索正确数据以增强用户输入，使其在生成模型足够之前？哪种类型的RAG框架将对项目成功？'
- en: '**Generation**: Which generative AI model will fit into the type of RAG framework
    chosen?'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成**：哪种生成式AI模型将适合所选的RAG框架？'
- en: 'The data, storage, and generation domains depend heavily on the type of RAG
    framework you choose. Before making that choice, we need to evaluate the proportion
    of parametric and non-parametric knowledge in the ecosystem we are implementing.
    *Figure 1.3* represents the RAG framework, which includes the main components
    regardless of the types of RAG implemented:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据、存储和生成领域在很大程度上取决于你选择的RAG框架类型。在做出选择之前，我们需要评估我们正在实施的生态系统中参数化和非参数化知识的比例。“图1.3”表示RAG框架，它包括无论实施何种类型RAG的主要组件：
- en: '![A diagram of a process  Description automatically generated](img/B31169_01_03.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![一个流程图  自动生成的描述](img/B31169_01_03.png)'
- en: 'Figure 1.3: The Generative RAG-ecosystem'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3：生成式RAG生态系统
- en: The **Retriever (D)** handles data collection, processing, storage, and retrieval
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索器（D）**负责数据收集、处理、存储和检索'
- en: The **Generator (G)** handles input augmentation, prompt engineering, and generation
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器（G）**负责输入增强、提示工程和生成'
- en: The **Evaluator (E)** handles mathematical metrics, human evaluation, and feedback
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估器（E）**负责处理数学指标、人工评估和反馈'
- en: The **Trainer (T)** handles the initial pre-trained model and fine-tuning the
    model
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练器（T）**负责初始预训练模型和微调模型'
- en: Each of these four components relies on their respective ecosystems, which form
    the overall RAG-driven generative AI pipeline. We will refer to the domains D,
    G, E, and T in the following sections. Let’s begin with the retriever.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个组件中的每一个都依赖于它们各自的生态系统，这些生态系统共同构成了RAG驱动的生成式AI管道的整体。在接下来的章节中，我们将提到D、G、E和T这些领域。让我们从检索器开始。
- en: The retriever (D)
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索器（D）
- en: The retriever component of a RAG ecosystem collects, processes, stores, and
    retrieves data. The starting point of a RAG ecosystem is thus an ingestion data
    process, of which the first step is to collect data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: RAG生态系统中的检索器组件负责收集、处理、存储和检索数据。因此，RAG生态系统的起点是一个数据摄取过程，其第一步是收集数据。
- en: Collect (D1)
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集（D1）
- en: In today’s world, AI data is as diverse as our media playlists. It can be anything
    from a chunk of text in a blog post to a meme or even the latest hit song streamed
    through headphones. And it doesn’t stop there—the files themselves come in all
    shapes and sizes. Think of PDFs filled with all kinds of details, web pages, plain
    text files that get straight to the point, neatly organized JSON files, catchy
    MP3 tunes, videos in MP4 format, or images in PNG and JPG.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，AI数据与我们媒体播放列表的多样性一样。它可以是从博客文章中的一段文本到表情包，甚至是通过耳机流过的最新热门歌曲。而且，这还远不止于此——文件本身也形形色色。想想看，PDF文件中充满了各种细节，网页、直接切入主题的纯文本文件，整齐组织的JSON文件，吸引人的MP3旋律，MP4格式的视频，或PNG和JPG格式的图片。
- en: Furthermore, a large proportion of this data is unstructured and found in unpredictable
    and complex ways. Fortunately, many platforms, such as Pinecone, OpenAI, Chroma,
    and Activeloop, provide ready-to-use tools to process and store this jungle of
    data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大量数据是非结构化的，以不可预测和复杂的方式存在。幸运的是，许多平台，如Pinecone、OpenAI、Chroma和Activeloop，提供了现成的工具来处理和存储这个数据丛林。
- en: Process (D2)
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理过程（D2）
- en: In the data collection phase (D1) of multimodal data processing, various types
    of data, such as text, images, and videos, can be extracted from websites using
    web scraping techniques or any other source of information. These data objects
    are then transformed to create uniform feature representations. For example, data
    can be chunked (broken into smaller parts), embedded (transformed into vectors),
    and indexed to enhance searchability and retrieval efficiency.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在多模态数据处理的数据收集阶段（D1）中，可以使用网络爬虫技术或其他信息来源，从网站中提取各种类型的数据，如文本、图像和视频。然后，这些数据对象被转换以创建统一的特征表示。例如，数据可以被分块（分割成更小的部分）、嵌入（转换为向量）和索引，以增强搜索性和检索效率。
- en: We will introduce these techniques, starting with the *Building Hybrid Adaptive
    RAG in Python* section of this chapter. In the following chapters, we will continue
    building more complex data processing functions.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将介绍这些技术，从本章的“在Python中构建混合自适应RAG”部分开始。在接下来的章节中，我们将继续构建更复杂的数据处理功能。
- en: Storage (D3)
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 存储（D3）
- en: At this stage of the pipeline, we have collected and begun processing a large
    amount of diverse data from the internet—videos, pictures, texts, you name it.
    Now, what can we do with all that data to make it useful?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个管道阶段，我们已经收集并开始处理来自互联网的大量多样化数据——视频、图片、文本，等等。现在，我们如何利用所有这些数据使其变得有用？
- en: That’s where vector stores like Deep Lake, Pinecone, and Chroma come into play.
    Think of these as super smart libraries that don’t just store your data but convert
    it into mathematical entities as vectors, enabling powerful computations. They
    can also apply a variety of indexing methods and other techniques for rapid access.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是像Deep Lake、Pinecone和Chroma这样的向量存储器发挥作用的地方。想象一下，这些是超级智能的库，它们不仅存储你的数据，而且将其转换为向量等数学实体，从而实现强大的计算。它们还可以应用各种索引方法和其他技术，以实现快速访问。
- en: Instead of keeping the data in static spreadsheets and files, we turn it into
    a dynamic, searchable system that can power anything from chatbots to search engines.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是将数据保留在静态的电子表格和文件中，而是将其转变为动态的、可搜索的系统，它可以支持从聊天机器人到搜索引擎的各种应用。
- en: Retrieval query (D4)
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检索查询（D4）
- en: The retrieval process is triggered by the user input or automated input (G1).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 检索过程由用户输入或自动化输入（G1）触发。
- en: To retrieve data quickly, we load it into vector stores and datasets after transforming
    it into a suitable format. Then, using a combination of keyword searches, smart
    embeddings, and indexing, we can retrieve the data efficiently. Cosine similarity,
    for example, finds items that are closely related, ensuring that the search results
    are not just fast but also highly relevant.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速检索数据，我们在将其转换为合适的格式后，将其加载到向量存储和数据集中。然后，通过结合关键词搜索、智能嵌入和索引，我们可以有效地检索数据。例如，余弦相似性可以找到密切相关的项目，确保搜索结果不仅快速，而且高度相关。
- en: Once the data is retrieved, we then augment the input.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检索到数据，我们就增强输入。
- en: The generator (G)
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成器（G）
- en: The lines are blurred in the RAG ecosystem between input and retrieval, as shown
    in *Figure 1.3*, representing the RAG framework and ecosystem. The user input
    (G1), automated or human, interacts with the retrieval query (D4) to augment the
    input before sending it to the generative model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG生态系统中，输入和检索之间的界限变得模糊，如*图1.3*所示，它代表了RAG框架和生态系统。用户输入（G1），无论是自动的还是人类的，都会与检索查询（D4）交互，以增强输入，然后再将其发送到生成模型。
- en: The generative flow begins with an input.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 生成流程从输入开始。
- en: Input (G1)
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入（G1）
- en: The input can be a batch of automated tasks (processing emails, for example)
    or human prompts through a **User Interface** (**UI**). This flexibility allows
    you to seamlessly integrate AI into various professional environments, enhancing
    productivity across industries.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输入可以是一批自动化任务（例如处理电子邮件）或通过**用户界面**（**UI**）的人类提示。这种灵活性允许你无缝地将AI集成到各种专业环境中，提高各行业的生产力。
- en: Augmented input with HF (G2)
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有HF的增强输入（G2）
- en: '**Human feedback** (**HF**) can be added to the input, as described in the
    *Human feedback (E2) under Evaluator (E)* section. Human feedback will make a
    RAG ecosystem considerably adaptable and provide full control over data retrieval
    and generative AI inputs. In the *Building hybrid adaptive RAG in Python* section
    of this chapter, we will build augmented input with human feedback.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**人类反馈**（**HF**）可以添加到输入中，如*评估者（E）下的*人类反馈（E2）*部分所述。人类反馈将使RAG生态系统具有相当大的适应性，并能够完全控制数据检索和生成AI输入。在本章的*在Python中构建混合自适应RAG*部分，我们将构建带有人类反馈的增强输入。'
- en: Prompt engineering (G3)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示工程（G3）
- en: Both the retriever (D) and the generator (G) rely heavily on prompt engineering
    to prepare the standard and augmented message that the generative AI model will
    have to process. Prompt engineering brings the retriever’s output and the user
    input together.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器（D）和生成器（G）都严重依赖提示工程来准备生成AI模型将需要处理的标准化和增强信息。提示工程将检索器的输出和用户输入结合起来。
- en: Generation and output (G4)
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成和输出（G4）
- en: The choice of a generative AI model depends on the goals of a project. Llama,
    Gemini, GPT, and other models can fit various requirements. However, the prompt
    must meet each model’s specifications. Frameworks such as LangChain, which we
    will implement in this book, help streamline the integration of various AI models
    into applications by providing adaptable interfaces and tools.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 生成AI模型的选择取决于项目的目标。Llama、Gemini、GPT和其他模型可以满足各种需求。然而，提示必须满足每个模型的规格。例如，我们将在这本书中实现的LangChain框架，通过提供可适应的接口和工具，有助于简化各种AI模型到应用的集成。
- en: The evaluator (E)
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估者（E）
- en: We often rely on mathematical metrics to assess the performance of a generative
    AI model. However, these metrics only give us part of the picture. It’s important
    to remember that the ultimate test of an AI’s effectiveness comes down to human
    evaluation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常依赖数学指标来评估生成式AI模型的表现。然而，这些指标只给出了部分画面。重要的是要记住，AI有效性的最终测试归结为人类评估。
- en: Metrics (E1)
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指标（E1）
- en: A model cannot be evaluated without mathematical metrics, such as cosine similarity,
    as with any AI system. These metrics ensure that the retrieved data is relevant
    and accurate. By quantifying the relationships and relevance of data points, they
    provide a solid foundation for assessing the model’s performance and reliability.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在缺乏数学指标的情况下无法评估，例如余弦相似度，这与任何AI系统一样。这些指标确保检索到的数据是相关且准确的。通过量化数据点之间的关系和相关性，它们为评估模型性能和可靠性提供了坚实的基础。
- en: Human feedback (E2)
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 人类反馈（E2）
- en: No generative AI system, whether RAG-driven or not, and whether the mathematical
    metrics seem sufficient or not, can elude human evaluation. It is ultimately human
    evaluation that decides if a system designed for human users will be accepted
    or rejected, praised or criticized.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是RAG驱动的还是非RAG驱动的生成式AI系统，以及数学指标是否似乎足够，都无法逃避人类评估。最终，人类评估决定为人类用户设计的系统将被接受或拒绝，被赞扬或批评。
- en: Adaptive RAG introduces the human, real-life, pragmatic feedback factor that
    will improve a RAG-driven generative AI ecosystem. We will implement adaptive
    RAG in *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应RAG引入了人类、现实生活、实用反馈因素，这将改善由RAG驱动的生成式AI生态系统。我们将在**第五章**，**通过专家人类反馈提升RAG性能**中实现自适应RAG。
- en: The trainer (T)
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练者（T）
- en: A standard generative AI model is pre-trained with a vast amount of general-purpose
    data. Then, we can fine-tune (T2) the model with domain-specific data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的生成式AI模型使用大量通用数据进行预训练。然后，我们可以使用特定领域的数据对模型进行微调（T2）。
- en: 'We will take this further by integrating static RAG data into the fine-tuning
    process in *Chapter 9*, *Empowering AI Models: Fine-Tuning RAG Data and Human
    Feedback*. We will also integrate human feedback, which provides valuable information
    that can be integrated into the fine-tuning process in a variant of **Reinforcement
    Learning from Human Feedback** (**RLHF**).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在**第九章**，**赋予AI模型能力：微调RAG数据和人类反馈**中进一步将静态RAG数据集成到微调过程中。我们还将集成人类反馈，这提供了可以集成到微调过程中的宝贵信息，这可以通过**从人类反馈中进行强化学习**（**RLHF**）的变体来实现。
- en: We are now ready to code entry-level naïve, advanced, and modular RAG in Python.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始用Python编写入门级、高级和模块化RAG的代码。
- en: Naïve, advanced, and modular RAG in code
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码中的朴素、高级和模块化RAG
- en: This section introduces naïve, advanced, and modular RAG through basic educational
    examples. The program builds keyword matching, vector search, and index-based
    retrieval methods. Using OpenAI’s GPT models, it generates responses based on
    input queries and retrieved documents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过基本教育示例介绍了朴素、高级和模块化RAG。程序构建了关键词匹配、向量搜索和基于索引的检索方法。使用OpenAI的GPT模型，它根据输入查询和检索到的文档生成响应。
- en: 'The goal of the notebook is for a conversational agent to answer questions
    on RAG in general. We will build the retriever from the bottom up, from scratch,
    in Python and run the generator with OpenAI GPT-4o in eight sections of code divided
    into two parts:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的目标是让对话代理回答有关RAG的一般性问题。我们将从底层开始，从头开始用Python构建检索器，并在八个代码部分中运行生成器，这些部分分为两部分：
- en: '**Part 1: Foundations and Basic Implementation**'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**第一部分：基础和基本实现**'
- en: '**Environment** setup for OpenAI API integration'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**环境**设置以集成OpenAI API'
- en: '**Generator** function using GPT-4o'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**生成器**函数使用GPT-4o'
- en: '**Data** setup with a list of documents (`db_records`)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据**设置，使用文档列表（`db_records`）'
- en: '**Query** for user input'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**查询**用户输入'
- en: '**Part 2: Advanced Techniques and Evaluation**'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**第二部分：高级技术和评估**'
- en: '**Retrieval metrics** to measure retrieval responses'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检索指标**以衡量检索响应'
- en: '**Naïve RAG** with a keyword search and matching function'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**朴素RAG**，具有关键词搜索和匹配功能'
- en: '**Advanced RAG** with vector search and index-based search'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高级RAG**，具有向量搜索和基于索引的搜索'
- en: '**Modular RAG** implementing flexible retrieval methods'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模块化RAG**实现灵活的检索方法'
- en: To get started, open `RAG_Overview.ipynb` in the GitHub repository. We will
    begin by establishing the foundations of the notebook and exploring the basic
    implementation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，请打开GitHub仓库中的`RAG_Overview.ipynb`。我们将首先建立笔记本的基础，并探索基本实现。
- en: 'Part 1: Foundations and basic implementation'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1部分：基础和基本实现
- en: In this section, we will set up the environment, create a function for the generator,
    define a function to print a formatted response, and define the user query.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设置环境，为生成器创建一个函数，定义一个用于打印格式化响应的函数，并定义用户查询。
- en: The first step is to install the environment.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是安装环境。
- en: The section titles of the following implementation of the notebook follow the
    structure in the code. Thus, you can follow the code in the notebook or read this
    self-contained section.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以下笔记本实现的章节标题遵循代码中的结构。因此，你可以遵循笔记本中的代码或阅读这个自包含的部分。
- en: 1\. Environment
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1. 环境
- en: 'The main package to install is OpenAI to access GPT-4o through an API:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 主要要安装的包是OpenAI，通过API访问GPT-4o：
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Make sure to freeze the OpenAI version you install. In RAG framework ecosystems,
    we will have to install several packages to run advanced RAG configurations. Once
    we have stabilized an installation, we will freeze the version of the packages
    installed to minimize potential conflicts between the libraries and modules we
    implement.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 确保冻结你安装的OpenAI版本。在RAG框架生态系统中，我们将需要安装几个包来运行高级RAG配置。一旦我们稳定了安装，我们将冻结已安装的包的版本，以最小化我们实现的库和模块之间的潜在冲突。
- en: Once you have installed `openai`, you will have to create an account on OpenAI
    (if you don’t have one) and obtain an API key. Make sure to check the costs and
    payment plans before running the API.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了`openai`，你将需要在OpenAI上创建一个账户（如果你还没有的话）并获取一个API密钥。在运行API之前，请确保检查费用和支付计划。
- en: 'Once you have a key, store it in a safe place and retrieve it as follows from
    Google Drive, for example, as shown in the following code:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了密钥，请将其存储在安全的地方，并按以下方式从Google Drive等地方检索，例如，如下代码所示：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can use Google Drive or any other method you choose to store your key.
    You can read the key from a file, or you can also choose to enter the key directly
    in the code:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Google Drive或任何其他你选择的方法来存储你的密钥。你可以从文件中读取密钥，或者你也可以选择直接在代码中输入密钥：
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With that, we have set up the main resources for our project. We will now write
    a generation function for the OpenAI model.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经为我们的项目设置了主要资源。现在我们将为OpenAI模型编写一个生成函数。
- en: 2\. The generator
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. 生成器
- en: 'The code imports `openai` to generate content and `time` to measure the time
    the requests take:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代码导入`openai`以生成内容，并导入`time`来测量请求所需的时间：
- en: '[PRE3]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We now create a function that creates a prompt with an instruction and the
    user input:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建一个函数，该函数创建一个包含指令和用户输入的提示：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The function will try to call `gpt-4o`, adding additional information for the
    model:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将尝试调用`gpt-4o`，为模型添加额外的信息：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that the instruction messages remain general in this scenario so that the
    model remains flexible. The `temperature` is low (more precise) and set to `0.1`.
    If you wish for the system to be more creative, you can set `temperature` to a
    higher value, such as `0.7`. However, in this case, it is recommended to ask for
    precise responses.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在此场景中，指令消息保持通用，以便模型保持灵活性。`temperature`（温度）设置得较低（更精确），为`0.1`。如果你希望系统更具创造性，可以将`temperature`设置为更高的值，例如`0.7`。然而，在这种情况下，建议请求精确的响应。
- en: 'We can add `textwrap` to format the response as a nice paragraph when we call
    the generative AI model:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们调用生成式AI模型时，我们可以添加`textwrap`来格式化响应为一个漂亮的段落：
- en: '[PRE6]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The generator is now ready to be called when we need it. Due to the probabilistic
    nature of generative AI models, it might produce different outputs each time we
    call it.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器现在已准备好，在我们需要时调用。由于生成式AI模型的概率性质，它可能会在每次调用时产生不同的输出。
- en: The program now implements the data retrieval functionality.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在实现了数据检索功能。
- en: 3\. The Data
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 数据
- en: Data collection includes text, images, audio, and video. In this notebook, we
    will focus on **data retrieval** through naïve, advanced, and modular configurations,
    not data collection. We will collect and embed data later in *Chapter 2*, *RAG
    Embedding Vector Stores with Deep Lake and OpenAI*. As such, we will assume that
    the data we need has been processed and thus collected, cleaned, and split into
    sentences. We will also assume that the process included loading the sentences
    into a Python list named `db_records`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集包括文本、图像、音频和视频。在本笔记本中，我们将关注通过天真、高级和模块化配置进行**数据检索**，而不是数据收集。我们将在第2章“使用Deep
    Lake和OpenAI的RAG嵌入向量存储”中收集和嵌入数据。因此，我们将假设我们所需的数据已经处理并收集，因此已经清理并分割成句子。我们还将假设该过程包括将句子加载到名为`db_records`的Python列表中。
- en: 'This approach illustrates three aspects of the RAG ecosystem we described in
    *The RAG ecosystem* section and the components of the system described in *Figure
    1.3*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法说明了我们在*RAG生态系统*部分中描述的RAG生态系统的三个方面以及*图1.3*中描述的系统组件：
- en: The **retriever (D)** has three **data processing** components, **collect (D1)**,
    **process (D2)**, and **storage (D3)**, which are preparatory phases of the retriever.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索器（D**）有三个**数据处理**组件，**收集（D1**），**处理（D2**）和**存储（D3**），它们是检索器的准备阶段。'
- en: The **retriever query (D4)** is thus independent of the first three phases (collect,
    process, and storage) of the retriever.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，**检索器查询（D4**）独立于检索器的第一阶段（收集、处理和存储）。
- en: The data processing phase will often be done independently and prior to activating
    the retriever query, as we will implement starting in *Chapter 2*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理阶段通常会在激活检索器查询之前独立完成，正如我们将在*第二章*中实施的那样。
- en: 'This program assumes that data processing has been completed and the dataset
    is ready:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序假设数据处理已完成，数据集已准备好：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can display a formatted version of the dataset:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示数据集的格式化版本：
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output joins the sentences in `db_records` for display, as printed in this
    excerpt, but `db_records` remains unchanged:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将`db_records`中的句子连接起来以供显示，正如以下摘录中所示，但`db_records`保持不变：
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The program is now ready to process a query.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 程序现在准备好处理查询。
- en: 4.The query
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4. 查询
- en: The **retriever** (**D4** in *Figure 1.3*) query process depends on how the
    data was processed, but the query itself is simply user input or automated input
    from another AI agent. We all dream of users who introduce the best input into
    software systems, but unfortunately, in real life, unexpected inputs lead to unpredictable
    behaviors. We must, therefore, build systems that take imprecise inputs into account.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**检索器**（*图1.3*中的**D4**）查询过程取决于数据处理的方式，但查询本身只是用户输入或来自另一个AI代理的自动化输入。我们都梦想着用户能够向软件系统引入最佳输入，但在现实生活中，意外的输入会导致不可预测的行为。因此，我们必须构建能够考虑不精确输入的系统。'
- en: 'In this section, we will imagine a situation in which hundreds of users in
    an organization have heard the word “RAG” associated with “LLM” and “vector stores.”
    Many of them would like to understand what these terms mean to keep up with a
    software team that’s deploying a conversational agent in their department. After
    a couple of days, the terms they heard become fuzzy in their memory, so they ask
    the conversational agent, GPT-4o in this case, to explain what they remember with
    the following query:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设想一个场景，其中组织中的数百名用户已经听到“RAG”与“LLM”和“向量存储”相关联。其中许多人希望了解这些术语对部署在部门中的对话代理的软件团队意味着什么，以保持同步。经过几天后，他们听到的术语在记忆中变得模糊，所以他们向对话代理，在本例中为GPT-4o，提出以下查询来解释他们记得的内容：
- en: '[PRE10]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In this case, we will simply store the main query of the topic of this program
    in `query`, which represents the junction between the retriever and the generator.
    It will trigger a configuration of RAG (naïve, advanced, and modular). The choice
    of configuration will depend on the goals of each project.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将简单地存储程序主题的主要查询在`query`中，它代表了检索器和生成器之间的连接点。它将触发RAG（朴素、高级和模块化）的配置。配置的选择将取决于每个项目的目标。
- en: 'The program takes the query and sends it to a GPT-4o model to be processed
    and then displays the formatted output:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 程序接收查询并将其发送到GPT-4o模型进行处理，然后显示格式化后的输出：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is revealing. Even the most powerful generative AI models cannot
    guess what a user, who knows nothing about AI, is trying to find out in good faith.
    In this case, GPT-4o will answer as shown in this excerpt of the output:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果揭示了真相。即使是最强大的生成式AI模型也无法猜出对AI一无所知的用户在真诚地试图找出什么。在这种情况下，GPT-4o将回答如下输出摘录所示：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output will seem like a hallucination, but is it really? The user wrote
    the query with the good intentions of every beginner trying to learn a new topic.
    GPT-4o, in good faith, did what it could with the limited context it had with
    its probabilistic algorithm, which might even produce a different response each
    time we run it. However, GPT-4o is being wary of the query. It wasn’t very clear,
    so it ends the response with the following output that asks the user for more
    context:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来像是一种幻觉，但真的是这样吗？用户撰写查询时，带着每个初学者试图学习新主题的善意。GPT-4o，出于善意，尽其所能地使用其有限的上下文和概率算法进行处理，这甚至可能每次运行时产生不同的响应。然而，GPT-4o对查询持谨慎态度。它并不非常清晰，因此以以下输出结束响应，要求用户提供更多上下文：
- en: '[PRE13]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The user is puzzled, not knowing what to do, and GPT-4o is awaiting further
    instructions. The software team has to do something!
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 用户感到困惑，不知道该怎么办，GPT-4o 正在等待进一步的指令。软件团队必须采取行动！
- en: Generative AI is based on probabilistic algorithms. As such, the response provided
    might vary from one run to another, providing similar (but not identical) responses.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式 AI 基于概率算法。因此，提供的响应可能因运行而异，提供相似（但不相同）的响应。
- en: That is when RAG comes in to save the situation. We will leave this query as
    it is for the whole notebook and see if a RAG-driven GPT-4o system can do better.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是RAG介入拯救情况的时候。我们将在这个笔记本中保留这个查询，看看由RAG驱动的GPT-4o系统是否能做得更好。
- en: 'Part 2: Advanced techniques and evaluation'
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 2 部分：高级技术和评估
- en: In *Part 2*, we will introduce naïve, advanced, and modular RAG. The goal is
    to introduce the three methods, not to process complex documents, which we will
    implement throughout the following chapters of this book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 2 部分* 中，我们将介绍朴素、高级和模块化 RAG。目标是介绍这三种方法，而不是处理复杂的文档，这些文档我们将在这本书的后续章节中实现。
- en: Let’s first begin by defining retrieval metrics to measure the accuracy of the
    documents we retrieve.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先定义检索指标来衡量我们检索的文档的准确性。
- en: 1\. Retrieval metrics
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1. 检索指标
- en: This section explores retrieval metrics, first focusing on the role of cosine
    similarity in assessing the relevance of text documents. Then we will implement
    enhanced similarity metrics by incorporating synonym expansion and text preprocessing
    to improve the accuracy of similarity calculations between texts.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了检索指标，首先关注余弦相似度在评估文本文档相关性中的作用。然后，我们将通过结合同义词扩展和文本预处理来提高文本相似度计算的准确性，实现增强的相似度指标。
- en: We will explore more metrics in the *Metrics calculation and display* section
    in *Chapter 7*, *Building Scalable Knowledge-Graph-Based RAG with Wikipedia API
    and LlamaIndex*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第 7 章“使用 Wikipedia API 和 LlamaIndex 构建可扩展的知识图谱 RAG”的“*指标计算和显示*”部分探索更多指标。
- en: In this chapter, let’s begin with cosine similarity.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，让我们从余弦相似度开始。
- en: Cosine similarity
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 余弦相似度
- en: Cosine similarity measures the cosine of the angle between two vectors. In our
    case, the two vectors are the user query and each document in a corpus.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度衡量两个向量之间角度的余弦值。在我们的情况下，这两个向量是用户查询和语料库中的每个文档。
- en: 'The program first imports the class and function we need:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先导入我们需要的类和函数：
- en: '[PRE14]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`TfidfVectorizer` imports the class that converts text documents into a matrix
    of TF-IDF features. **Term Frequency-Inverse Document Frequency** (**TF-IDF**)
    quantifies the relevance of a word to a document in a collection, distinguishing
    common words from those significant to specific texts. TF-IDF will thus quantify
    word relevance in documents using frequency across the document and inverse frequency
    across the corpus. `cosine_similarity` imports the function we will use to calculate
    the similarity between vectors.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer` 导入将文本文档转换为 TF-IDF 特征矩阵的类。**词频-逆文档频率**（**TF-IDF**）量化了单词在文档集合中对文档的相关性，区分了常见单词和特定文本中重要的单词。因此，TF-IDF
    将使用文档中的频率和语料库中的逆频率来量化文档中单词的相关性。`cosine_similarity` 导入我们将用于计算向量之间相似度的函数。'
- en: '`calculate_cosine_similarity(text1, text2)` then calculates the cosine similarity
    between the query (`text1`) and each record of the dataset.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_cosine_similarity(text1, text2)` 然后计算查询（`text1`）和数据集中的每个记录之间的余弦相似度。'
- en: 'The function converts the query text (`text1`) and each record (`text2`) in
    the dataset into a vector with a vectorizer. Then, it calculates and returns the
    cosine similarity between the two vectors:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将查询文本（`text1`）和数据集中的每个记录（`text2`）转换为向量器生成的向量。然后，它计算并返回两个向量之间的余弦相似度：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The key parameters of this function are:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数的关键参数包括：
- en: '`stop_words=''english`: Ignores common English words to focus on meaningful
    content'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop_words=''english''`: 忽略常见的英语单词，以关注有意义的内客'
- en: '`use_idf=True`: Enables inverse document frequency weighting'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_idf=True`: 启用逆文档频率加权'
- en: '`norm=''l2''`: Applies L2 normalization to each output vector'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`norm=''l2''`: 对每个输出向量应用 L2 归一化'
- en: '`ngram_range=(1, 2)`: Considers both single words and two-word combinations'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_range=(1, 2)`: 考虑单词和双词组合'
- en: '`sublinear_tf=True`: Applies logarithmic term frequency scaling'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sublinear_tf=True`: 应用对数词频缩放'
- en: '`analyzer=''word''`: Analyzes text at the word level'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`analyzer=''word''`: 在单词级别分析文本'
- en: Cosine similarity can be limited in some cases. Cosine similarity has limitations
    when dealing with ambiguous queries because it strictly measures the similarity
    based on the angle between vector representations of text. If a user asks a vague
    question like “What is rag?” in the program of this chapter and the database primarily
    contains information on “RAG” as in “retrieval-augmented generation” for AI, not
    “rag cloths,” the cosine similarity score might be low. This low score occurs
    because the mathematical model lacks contextual understanding to differentiate
    between the different meanings of “rag.” It only computes similarity based on
    the presence and frequency of similar words in the text, without grasping the
    user’s intent or the broader context of the query. Thus, even if the answers provided
    are technically accurate within the available dataset, the cosine similarity may
    not reflect the relevance accurately if the query’s context isn’t well-represented
    in the data.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度在某些情况下可能有限制。当处理模糊查询时，余弦相似度存在局限性，因为它严格基于文本向量表示之间的角度来衡量相似度。如果一个用户在本章的程序中提出一个模糊的问题，比如“什么是rag？”而数据库主要包含关于“RAG”作为人工智能中的“检索增强生成”的信息，而不是“rag布”，余弦相似度得分可能会很低。这种低分发生是因为数学模型缺乏对“rag”不同含义的区分能力。它只根据文本中相似词的出现和频率来计算相似度，而没有理解用户的意图或查询的更广泛上下文。因此，即使提供的答案在可用数据集中在技术上准确，如果查询的上下文在数据中没有得到良好表示，余弦相似度可能无法准确反映相关性。
- en: In this case, we can try enhanced similarity.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们可以尝试增强相似度。
- en: Enhanced similarity
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强相似度
- en: Enhanced similarity introduces calculations that leverage natural language processing
    tools to better capture semantic relationships between words. Using libraries
    like spaCy and NLTK, it preprocesses texts to reduce noise, expands terms with
    synonyms from WordNet, and computes similarity based on the semantic richness
    of the expanded vocabulary. This method aims to improve the accuracy of similarity
    assessments between two texts by considering a broader context than typical direct
    comparison methods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 增强相似度引入了利用自然语言处理工具的计算，以更好地捕捉词语之间的语义关系。使用如spaCy和NLTK等库，它对文本进行预处理以减少噪声，使用WordNet中的同义词扩展术语，并基于扩展词汇的语义丰富度计算相似度。这种方法旨在通过考虑比典型直接比较方法更广泛的上下文来提高两个文本之间相似度评估的准确性。
- en: 'The code contains four main functions:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 代码包含四个主要函数：
- en: '`get_synonyms(word)`: Retrieves synonyms for a given word from WordNet'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_synonyms(word)`: 从WordNet检索给定单词的同义词'
- en: '`preprocess_text(text)`: Converts all text to lowercase, lemmatizes gets the
    (roots of words), and filters stopwords (common words) and punctuation from text'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_text(text)`: 将所有文本转换为小写，词形还原获取词根，并从文本中过滤掉停用词（常见单词）和标点符号'
- en: '`expand_with_synonyms(words)`: Enhances a list of words by adding their synonyms'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expand_with_synonyms(words)`: 通过添加同义词来增强单词列表'
- en: '`calculate_enhanced_similarity(text1, text2)`: Computes cosine similarity between
    preprocessed and synonym-expanded text vectors'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`calculate_enhanced_similarity(text1, text2)`: 计算预处理和同义词扩展文本向量之间的余弦相似度'
- en: The `calculate_enhanced_similarity(text1, text2)` function takes two texts and
    ultimately returns the cosine similarity score between two processed and synonym-expanded
    texts. This score quantifies the textual similarity based on their semantic content
    and enhanced word sets.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`calculate_enhanced_similarity(text1, text2)`函数接受两个文本，并最终返回两个处理和同义词扩展文本之间的余弦相似度得分。这个得分量化了基于它们的语义内容和增强词集的文本相似度。'
- en: 'The code begins by downloading and importing the necessary libraries and then
    runs the four functions beginning with `calculate_enhanced_similarity(text1, text2)`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先下载并导入必要的库，然后运行以`calculate_enhanced_similarity(text1, text2)`开始的四个函数：
- en: '[PRE16]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Enhanced similarity takes this a bit further in terms of metrics. However, integrating
    RAG with generative AI presents multiple challenges.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 增强相似度在度量方面更进一步，然而，将RAG与生成式人工智能集成面临着多个挑战。
- en: 'No matter which metric we implement, we will face the following limitations:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们实现哪种度量标准，我们都会面临以下限制：
- en: '**Input versus Document Length**: User queries are often short, while retrieved
    documents are longer and richer, complicating direct similarity evaluations.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入长度与文档长度对比**：用户查询通常较短，而检索到的文档则较长且内容丰富，这增加了直接相似度评估的复杂性。'
- en: '**Creative Retrieval**: Systems may creatively select longer documents that
    meet user expectations but yield poor metric scores due to unexpected content
    alignment.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创意检索**：系统可能会创造性地选择满足用户期望但内容对齐意外导致指标得分较差的较长的文档。'
- en: '**Need for Human Feedback**: Often, human judgment is crucial to accurately
    assess the relevance and effectiveness of retrieved content, as automated metrics
    may not fully capture user satisfaction. We will explore this critical aspect
    of RAG in *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要人类反馈**：通常，人类判断对于准确评估检索内容的相关性和有效性至关重要，因为自动指标可能无法完全捕捉用户满意度。我们将在 *第 5 章* 中探讨
    RAG 的这一关键方面，即 *使用专家人类反馈提升 RAG 性能*。'
- en: We will always have to find the right balance between mathematical metrics and
    human feedback.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将始终需要在数学指标和人类反馈之间找到正确的平衡。
- en: We are now ready to create an example with naïve RAG.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好使用简单的 RAG 创建一个示例。
- en: 2\. Naïve RAG
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2. 简单 RAG
- en: 'Naïve RAG with keyword search and matching can prove efficient with well-defined
    documents within an organization, such as legal and medical documents. These documents
    generally have clear titles or labels for images, for example. In this naïve RAG
    function, we will implement keyword search and matching. To achieve this, we will
    apply a straightforward retrieval method in the code:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在组织内部，如法律和医学文件等定义明确的文档中，简单的 RAG 带有关键词搜索和匹配可以证明是高效的。这些文档通常有清晰的标题或图像标签。在这个简单的
    RAG 函数中，我们将实现关键词搜索和匹配。为了实现这一点，我们将在代码中应用一种直接检索方法：
- en: Split the query into individual keywords
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将查询拆分为单个关键词
- en: Split each record in the dataset into keywords
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集中的每个记录拆分为关键词
- en: Determine the length of the common matches
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定共同匹配的长度
- en: Choose the record with the best score
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择得分最高的记录
- en: 'The generation method will:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 生成方法将：
- en: Augment the user input with the result of the retrieval query
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检索查询的结果增强用户输入
- en: Request the generation model, which is `gpt-4o` in this case
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求生成模型，在这种情况下是 `gpt-4o`
- en: Display the response
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示响应
- en: Let’s write the keyword search and matching function.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写关键词搜索和匹配函数。
- en: Keyword search and matching
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关键词搜索和匹配
- en: 'The best matching function first initializes the best scores:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳匹配函数首先初始化最佳得分：
- en: '[PRE17]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The query is then split into keywords. Each record is also split into words
    to find the common words, measure the length of common content, and find the best
    match:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将查询拆分为关键词。每个记录也被拆分为单词以找到共同单词，测量共同内容的长度，并找到最佳匹配：
- en: '[PRE18]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We now call the function, format the response, and print it:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在调用函数，格式化响应，并打印出来：
- en: '[PRE19]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The main query of this notebook will be `query = "define a rag store"` to see
    if each RAG method produces an acceptable output.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本笔记本的主要查询将是 `query = "定义一个 RAG 存储"`，以查看每种 RAG 方法是否产生可接受的输出。
- en: 'The keyword search finds the best record in the list of sentences in the dataset:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词搜索在数据集中的句子列表中找到最佳记录：
- en: '[PRE20]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s run the metrics.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行指标。
- en: Metrics
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标
- en: 'We created the similarity metrics in the *1\. Retrieval metrics* section of
    this chapter. We will first apply cosine similarity:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的 *1. 检索指标* 节中创建了相似度指标。我们首先应用余弦相似度：
- en: '[PRE21]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output similarity is low, as explained in the *1\. Retrieval metrics* section
    of this chapter. The user input is short and the response is longer and complete:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章第 *1. 检索指标* 节所述，输出相似度较低。用户输入较短，而响应较长且完整：
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Enhanced similarity will produce a better score:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 增强相似度将产生更好的得分：
- en: '[PRE23]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The score produced is higher with enhanced functionality:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 增强功能产生的得分更高：
- en: '[PRE24]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The output of the query will now augment the user input.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 查询的输出现在将增强用户输入。
- en: Augmented input
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 增强输入
- en: 'The augmented input is the concatenation of the user input and the best matching
    record of the dataset detected with the keyword search:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 增强输入是用户输入和数据集中通过关键词搜索检测到的最佳匹配记录的串联：
- en: '[PRE25]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The augmented input is displayed if necessary for maintenance reasons:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要维护原因，将显示增强输入：
- en: '[PRE26]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output then shows that the augmented input is ready:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 输出随后显示增强输入已准备好：
- en: '[PRE27]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The input is now ready for the generation process.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输入现在已准备好进行生成过程。
- en: Generation
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成
- en: 'We are now ready to call GPT-4o and display the formatted response:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好调用 GPT-4o 并显示格式化的响应：
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The following excerpt of the response shows that GPT-4o understands the input
    and provides an interesting, pertinent response:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对响应的摘录，显示了 GPT-4o 能够理解输入并提供有趣、相关的响应：
- en: '[PRE29]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Naïve RAG can be sufficient in many situations. However, if the volume of documents
    becomes too large or the content becomes more complex, then advanced RAG configurations
    will provide better results. Let’s now explore advanced RAG.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的RAG在很多情况下可能足够用。然而，如果文档量变得过大或内容变得更加复杂，那么高级RAG配置将提供更好的结果。现在让我们来探索高级RAG。
- en: 3\. Advanced RAG
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3. 高级RAG
- en: As datasets grow larger, keyword search methods might prove too long to run.
    For instance, if we have hundreds of documents and each document contains hundreds
    of sentences, it will become challenging to use keyword search only. Using an
    index will reduce the computational load to just a fraction of the total data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集的增大，关键词搜索方法可能运行时间过长。例如，如果我们有数百份文档，并且每份文档包含数百个句子，那么仅使用关键词搜索将变得具有挑战性。使用索引将减少计算负载，只需处理总数据的一小部分。
- en: In this section, we will go beyond searching text with keywords. We will see
    how RAG transforms text data into numerical representations, enhancing search
    efficiency and processing speed. Unlike traditional methods that directly parse
    text, RAG first converts documents and user queries into vectors, numerical forms
    that speed up calculations. In simple terms, a vector is a list of numbers representing
    various features of text. Simple vectors might count word occurrences (term frequency),
    while more complex vectors, known as embeddings, capture deeper linguistic patterns.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将超越使用关键词搜索文本。我们将了解RAG如何将文本数据转换为数值表示，从而提高搜索效率和处理速度。与直接解析文本的传统方法不同，RAG首先将文档和用户查询转换为向量，这些向量是数值形式，可以加快计算速度。简单来说，向量是一系列数字，代表文本的各种特征。简单的向量可能只计算单词出现次数（词频），而更复杂的向量，称为嵌入，可以捕捉更深层次的语用模式。
- en: 'In this section, we will implement vector search and index-based search:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现向量搜索和基于索引的搜索：
- en: '**Vector Search**: We will convert each sentence in our dataset into a numerical
    vector. By calculating the cosine similarity between the query vector (the user
    query) and these document vectors, we can quickly find the most relevant documents.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量搜索**：我们将把数据集中的每个句子转换为数值向量。通过计算查询向量（用户查询）与这些文档向量之间的余弦相似度，我们可以快速找到最相关的文档。'
- en: '**Index-Based Search**: In this case, all sentences are converted into vectors
    using **TF-IDF** (**Term Frequency-Inverse Document Frequency**), a statistical
    measure used to evaluate how important a word is to a document in a collection.
    These vectors act as indices in a matrix, allowing quick similarity comparisons
    without parsing each document fully.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于索引的搜索**：在这种情况下，所有句子都使用**TF-IDF**（**词频-逆文档频率**）转换为向量，这是一种用于评估一个词在集合中的文档中重要性的统计度量。这些向量作为矩阵中的索引，允许快速进行相似度比较，而无需完全解析每个文档。'
- en: Let’s start with vector search and see these concepts in action.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向量搜索开始，看看这些概念是如何付诸实践的。
- en: 3.1.Vector search
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 向量搜索
- en: Vector search converts the user query and the documents into numerical values
    as vectors, enabling mathematical calculations that *retrieve relevant data faster
    when dealing with large volumes of data*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 向量搜索将用户查询和文档转换为数值向量，从而实现数学计算，*在处理大量数据时能够更快地检索相关数据*。
- en: 'The program runs through each record of the dataset to find the best matching
    document by computing the cosine similarity of the query vector and each record
    in the dataset:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 程序遍历数据集的每一条记录，通过计算查询向量（用户查询）与数据集中每条记录的余弦相似度来找到最佳匹配的文档：
- en: '[PRE30]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The code then calls the vector search function and displays the best record
    found:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 然后代码调用向量搜索函数并显示找到的最佳记录：
- en: '[PRE31]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is satisfactory:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是令人满意的：
- en: '[PRE32]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The response is the best one found, like with naïve RAG. This shows that there
    is no silver bullet. Each RAG technique has its merits. The metrics will confirm
    this observation.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是找到的最佳结果，就像原始RAG一样。这表明没有万能的解决方案。每种RAG技术都有其优点。指标将证实这一观察结果。
- en: Metrics
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指标
- en: 'The metrics are the same for both similarity methods as for naïve RAG because
    the same document was retrieved:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 两种相似度方法以及原始RAG的指标是相同的，因为检索的是相同的文档：
- en: '[PRE33]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果为：
- en: '[PRE34]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And with enhanced similarity, we obtain the same output as for naïve RAG:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 通过增强相似度，我们获得了与原始RAG相同的输出：
- en: '[PRE35]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output confirms the trend:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果证实了这一趋势：
- en: '[PRE36]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: So why use vector search if it produces the same outputs as naïve RAG? Well,
    in a small dataset, everything looks easy. But when we’re dealing with datasets
    of millions of complex documents, keyword search will not capture subtleties that
    vectors can. Let’s now augment the user query with this information retrieved.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么还要使用向量搜索，如果它与简单的RAG产生相同的输出呢？好吧，在小数据集中，一切看起来都很简单。但是当我们处理数百万个复杂文档的数据集时，关键词搜索将无法捕捉到向量可以捕捉到的细微差别。现在让我们用检索到的这些信息来增强用户查询。
- en: Augmented input
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强输入
- en: 'We add the information retrieved to the user query with no other aid and display
    the result:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检索到的信息添加到用户查询中，不添加其他辅助信息，并显示结果：
- en: '[PRE37]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We only added a space between the user query and the retrieved information;
    nothing else. The output is satisfactory:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只在用户查询和检索到的信息之间添加了一个空格；没有其他东西。输出是令人满意的：
- en: '[PRE38]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Let’s now see how the generative AI model reacts to this augmented input.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看生成式AI模型对这种增强输入的反应。
- en: Generation
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成
- en: 'We now call GPT-4o with the augmented input and display the formatted output:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用增强后的输入调用GPT-4o并显示格式化的输出：
- en: '[PRE39]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The response makes sense, as shown in the following excerpt:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 响应是有意义的，如下面的摘录所示：
- en: '[PRE40]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: While vector search significantly speeds up the process of finding relevant
    documents by sequentially going through each record, its efficiency can decrease
    as the dataset size increases. To address this scalability issue, indexed search
    offers a more advanced solution. Let’s now see how index-based search can accelerate
    document retrieval.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然向量搜索通过顺序遍历每个记录显著加快了查找相关文档的过程，但随着数据集大小的增加，其效率可能会降低。为了解决这个可扩展性问题，索引搜索提供了一种更高级的解决方案。现在让我们看看基于索引的搜索如何加速文档检索。
- en: 3.2\. Index-based search
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2. 基于索引的搜索
- en: Index-based search compares the vector of a user query not with the direct vector
    of a document’s content but with an indexed vector that represents this content.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 基于索引的搜索比较用户查询的向量不是与文档内容的直接向量，而是与表示该内容的索引向量。
- en: 'The program first imports the class and function we need:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 程序首先导入所需的类和函数：
- en: '[PRE41]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`TfidfVectorizer` imports the class that converts text documents into a matrix
    of TF-IDF features. TF-IDF will quantify word relevance in documents using frequency
    across the document. The function finds the best matches using the cosine similarity
    function to calculate the similarity between the query and the weighted vectors
    of the matrix:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`TfidfVectorizer`导入将文本文档转换为TF-IDF特征矩阵的类。TF-IDF将使用文档中的频率来量化单词在文档中的相关性。该函数使用余弦相似度函数计算查询与矩阵中加权向量的相似度：'
- en: '[PRE42]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The function’s main tasks are:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的主要任务包括：
- en: '**Transform Query**: Converts the input query into TF-IDF vector format using
    the provided vectorizer'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换查询**：使用提供的向量器将输入查询转换为TF-IDF向量格式'
- en: '**Calculate Similarities**: Computes the cosine similarity between the query
    vector and all vectors in the tfidf_matrix'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算相似度**：计算查询向量和tfidf_matrix中所有向量的余弦相似度'
- en: '**Identify Best Match**: Finds the index (`best_index`) of the highest similarity
    score in the results'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别最佳匹配**：在结果中找到最高相似度分数的索引（`best_index`）'
- en: '**Retrieve Best Score**: Extracts the highest cosine similarity score (`best_score`)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索最佳分数**：提取最高的余弦相似度分数（`best_score`）'
- en: The output is the best similarity score found and the best index.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是找到的最佳相似度分数和最佳索引。
- en: 'The following code first calls the dataset vectorizer and then searches for
    the most similar record through its index:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码首先调用数据集向量器，然后通过其索引搜索最相似的记录：
- en: '[PRE43]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Finally, the results are displayed:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，结果会显示：
- en: '[PRE44]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The system finds the best similar document to the user’s input query:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 系统会找到与用户输入查询最相似的文档：
- en: '[PRE45]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We can see that the fuzzy user query produced a reliable output at the retrieval
    level before running GPT-4o.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在运行GPT-4o之前，模糊的用户查询在检索级别产生了可靠的输出。
- en: The metrics that follow in the program are the same as for naïve and advanced
    RAG with vector search. This is normal because the document found is the closest
    to the user’s input query. We will be introducing more complex documents for RAG
    starting in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*.
    For now, let’s have a look at the features that influence how the words are represented
    in vectors.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 程序中接下来的指标与简单和高级RAG使用向量搜索时的指标相同。这是正常的，因为找到的文档与用户的输入查询最接近。我们将在*第二章*，*使用Deep Lake和OpenAI的RAG嵌入向量存储*中介绍更复杂的文档。现在，让我们看看影响单词在向量中表示方式的特征。
- en: Feature extraction
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特征提取
- en: 'Before augmenting the input with this document, run the following cell, which
    calls the `setup_vectorizer(records)` function again but displays the matrix so
    that you can see its format. This is shown in the following excerpt for the words
    “accurate” and “additional” in one of the sentences:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在将此文档添加到输入之前，运行以下单元格，该单元格再次调用`setup_vectorizer(records)`函数，但显示矩阵以便您可以看到其格式。以下是从句子中提取的“准确”和“附加”单词的示例：
- en: '![A black and white image of a number  Description automatically generated
    with medium confidence](img/B31169_01_04.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![一张黑白数字图片，描述自动生成，中等置信度](img/B31169_01_04.png)'
- en: 'Figure 1.4: Format of the matrix'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：矩阵格式
- en: Let’s now augment the input.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们增加输入。
- en: Augmented input
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强输入
- en: 'We will simply add the query to the best matching record in a minimal way to
    see how GPT-4o will react and display the output:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简单地以最小方式将查询添加到最佳匹配的记录中，以查看GPT-4o将如何反应并显示输出：
- en: '[PRE46]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is close to or the same as with vector search, but the retrieval
    method is faster:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出接近或与向量搜索相同，但检索方法更快：
- en: '[PRE47]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We will now plug this augmented input into the generative AI model.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将这个增强的输入插入到生成式AI模型中。
- en: Generation
  id: totrans-317
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成
- en: 'We now call GPT-4o with the augmented input and display the output:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用增强的输入调用GPT-4o并显示输出：
- en: '[PRE48]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output makes sense for the user who entered the initial fuzzy query:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入初始模糊查询的用户，输出是有意义的：
- en: '[PRE49]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: This approach worked well in a closed environment within an organization in
    a specific domain. In an open environment, the user might have to elaborate before
    submitting a request.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在特定领域组织内的封闭环境中工作得很好。在开放环境中，用户在提交请求之前可能需要详细说明。
- en: In this section, we saw that a TF-IDF matrix pre-computes document vectors,
    enabling faster, simultaneous comparisons without repeated vector transformations.
    We have seen how vector and index-based search can improve retrieval. However,
    in one project, we may need to apply naïve and advanced RAG depending on the documents
    we need to retrieve. Let’s now see how modular RAG can improve our system.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解到TF-IDF矩阵预先计算文档向量，这使得可以更快、同时地进行比较，而无需重复向量转换。我们已经看到基于向量和索引的搜索如何提高检索效率。然而，在一个项目中，我们可能需要根据需要检索的文档应用朴素和高级的RAG。现在让我们看看模块化RAG如何改进我们的系统。
- en: 4\. Modular RAG
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4. 模块化RAG
- en: 'Should we use keyword search, vector search, or index-based search when implementing
    RAG? Each approach has its merits. The choice will depend on several factors:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现RAG时，我们应该使用关键词搜索、向量搜索还是基于索引的搜索？每种方法都有其优点。选择将取决于几个因素：
- en: '**Keyword search** suits simple retrieval'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键词搜索**适用于简单的检索'
- en: '**Vector search** is ideal for semantic-rich documents'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**向量搜索**适用于语义丰富的文档'
- en: '**Index-based search** offers speed with large data.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于索引的搜索**在大数据量下提供速度。'
- en: However, all three methods can perfectly fit together in a project. In one scenario,
    for example, a keyword search can help find clearly defined document labels, such
    as the titles of PDF files and labeled images, before they are processed. Then,
    indexed search will group the documents into indexed subsets. Finally, the retrieval
    program can search the indexed dataset, find a subset, and only use vector search
    to go through a limited number of documents to find the most relevant one.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有三种方法都可以完美地结合到一个项目中。例如，在一个场景中，关键词搜索可以帮助在处理之前找到明确定义的文档标签，如PDF文件的标题和标记的图像。然后，索引搜索将文档分组到索引子集中。最后，检索程序可以在索引数据集中搜索，找到一个子集，并且仅使用向量搜索遍历有限数量的文档以找到最相关的文档。
- en: In this section, we will create a `RetrievalComponent` class that can be called
    at each step of a project to perform the task required. The code sums up the three
    methods we have built in this chapter and that we can sum for the `RetrievalComponent`
    through its main members.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建一个`RetrievalComponent`类，该类可以在项目的每个步骤中被调用以执行所需的任务。代码总结了我们在本章构建的三个方法，以及我们可以通过其主成员对`RetrievalComponent`进行总结的方法。
- en: 'The following code initializes the class with search method choice and prepares
    a vectorizer if needed. `self` refers to the current instance of the class to
    access its variables, methods, and functions:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码初始化类时选择搜索方法，并在需要时准备矢量器。`self`指的是当前类的实例，以便访问其变量、方法和函数：
- en: '[PRE50]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this case, the vector search is activated.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，激活了向量搜索。
- en: 'The `fit` method builds a TF-IDF matrix from records, and is applicable for
    vector or indexed search methods:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法从记录中构建TF-IDF矩阵，适用于向量或索引搜索方法：'
- en: '[PRE51]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The retrieve method directs the query to the appropriate search method:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 检索方法将查询引导到适当的搜索方法：
- en: '[PRE52]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The keyword search method finds the best match by counting common keywords
    between queries and documents:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词搜索方法通过计算查询和文档之间的共同关键词来找到最佳匹配：
- en: '[PRE53]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The vector search method computes similarities between query TF-IDF and document
    matrix and returns the best match:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 向量搜索方法计算查询TF-IDF和文档矩阵之间的相似性，并返回最佳匹配：
- en: '[PRE54]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The indexed search method uses a precomputed TF-IDF matrix for fast retrieval
    of the best-matching document:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 索引搜索方法使用预计算的TF-IDF矩阵以快速检索最佳匹配文档：
- en: '[PRE55]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We can now activate modular RAG strategies.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以激活模块化RAG策略。
- en: Modular RAG strategies
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模块化RAG策略
- en: 'We can call the retrieval component for any RAG configuration we wish when
    needed:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要时，我们可以调用任何我们想要的RAG配置的检索组件：
- en: '[PRE56]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In this case, the vector search method was activated.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，激活了向量搜索方法。
- en: 'The following cells select the best record, as in the *3.1\. Vector search*
    section, augment the input, call the generative model, and display the output
    as shown in the following excerpt:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 以下单元格选择最佳记录，如第3.1节“向量搜索”中所述，增强输入，调用生成模型，并显示如下摘录：
- en: '[PRE57]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We have built a program that demonstrated how different search methodologies—keyword,
    vector, and index-based—can be effectively integrated into a RAG system. Each
    method has its unique strengths and addresses specific needs within a data retrieval
    context. The choice of method depends on the dataset size, query type, and performance
    requirements, which we will explore in the following chapters.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建了一个程序，演示了如何将不同的搜索方法——关键词、向量和基于索引的——有效地集成到RAG系统中。每种方法都有其独特的优势，并在数据检索的特定场景中满足特定需求。方法的选择取决于数据集大小、查询类型和性能要求，我们将在以下章节中探讨。
- en: It’s now time to summarize our explorations in this chapter and move to the
    next level!
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候总结本章的探索并进入下一个层次了！
- en: Summary
  id: totrans-353
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'RAG for generative AI relies on two main components: a retriever and a generator.
    The retriever processes data and defines a search method, such as fetching labeled
    documents with keywords—the generator’s input, an LLM, benefits from augmented
    information when producing sequences. We went through the three main configurations
    of the RAG framework: naïve RAG, which accesses datasets through keywords and
    other entry-level search methods; advanced RAG, which introduces embeddings and
    indexes to improve the search methods; and modular RAG, which can combine naïve
    and advanced RAG as well as other ML methods.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI的RAG依赖于两个主要组件：检索器和生成器。检索器处理数据并定义搜索方法，例如通过关键词检索标记文档——生成器的输入，一个LLM，在生成序列时受益于增强信息。我们探讨了RAG框架的三个主要配置：天真RAG，通过关键词和其他入门级搜索方法访问数据集；高级RAG，引入嵌入和索引以改进搜索方法；以及模块化RAG，可以结合天真和高级RAG以及其他ML方法。
- en: The RAG framework relies on datasets that can contain dynamic data. A generative
    AI model relies on parametric data through its weights. These two approaches are
    not mutually exclusive. If the RAG datasets become too cumbersome, fine-tuning
    can prove useful. When fine-tuned models cannot respond to everyday information,
    RAG can come in handy. RAG frameworks also rely heavily on the ecosystem that
    provides the critical functionality to make the systems work. We went through
    the main components of the RAG ecosystem, from the retriever to the generator,
    for which the trainer is necessary, and the evaluator. Finally, we built an entry-level
    naïve, advanced, and modular RAG program in Python, leveraging keyword matching,
    vector search, and index-based retrieval, augmenting the input of GPT-4o.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: RAG框架依赖于可以包含动态数据的数据集。生成式AI模型通过其权重依赖于参数数据。这两种方法并不相互排斥。如果RAG数据集过于庞大，微调可能很有用。当微调模型无法响应用户日常信息时，RAG可以派上用场。RAG框架还严重依赖于提供关键功能以使系统工作的生态系统。我们探讨了RAG生态系统的主体组件，从检索器到生成器，其中需要训练师和评估者。最后，我们用Python构建了一个入门级的天真、高级和模块化RAG程序，利用关键词匹配、向量搜索和基于索引的检索，增强GPT-4o的输入。
- en: Our next step in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and
    OpenAI*, is to embed data in vectors. We will store the vectors in vector stores
    to enhance the speed and precision of the retrieval functions of a RAG ecosystem.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下一章“RAG嵌入向量存储与Deep Lake和OpenAI”中的下一步是嵌入数据到向量中。我们将向量存储在向量存储中，以增强RAG生态系统检索函数的速度和精度。
- en: Questions
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Answer the following questions with *Yes* or *No*:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 用*是*或*否*回答以下问题：
- en: Is RAG designed to improve the accuracy of generative AI models?
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG是否旨在提高生成式AI模型的准确性？
- en: Does a naïve RAG configuration rely on complex data embedding?
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 天真的RAG配置是否依赖于复杂的数据嵌入？
- en: Is fine-tuning always a better option than using RAG?
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微调是否总是比使用RAG更好的选择？
- en: Does RAG retrieve data from external sources in real time to enhance responses?
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG是否从外部源实时检索数据以增强响应？
- en: Can RAG be applied only to text-based data?
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG是否只能应用于基于文本的数据？
- en: Is the retrieval process in RAG triggered by a user or automated input?
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG中的检索过程是由用户或自动输入触发的吗？
- en: Are cosine similarity and TF-IDF both metrics used in advanced RAG configurations?
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 余弦相似度和TF-IDF是否都是高级RAG配置中使用的指标？
- en: Does the RAG ecosystem include only data collection and generation components?
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RAG生态系统是否仅包括数据收集和生成组件？
- en: Can advanced RAG configurations process multimodal data such as images and audio?
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高级RAG配置能否处理多模态数据，如图像和音频？
- en: Is human feedback irrelevant in evaluating RAG systems?
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在评估RAG系统时，人类反馈是否无关紧要？
- en: References
  id: totrans-369
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks* by Patrick
    Lewis, Ethan Perez, Aleksandra Piktus, et al.: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《知识密集型NLP任务的检索增强生成》* 由帕特里克·刘易斯、埃丹·佩雷斯、亚历山德拉·皮克图斯等人撰写：[https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
- en: '*Retrieval-Augmented Generation for Large Language Models: A Survey* by Yunfan
    Gao, Yun Xiong, Xinyu Gao, et al.: [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997)'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《大型语言模型的检索增强生成：综述》* 由高云帆、熊云、高新宇等人撰写：[https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997)'
- en: 'OpenAI models: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI模型：[https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)
- en: Further reading
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To understand why RAG-driven Generative AI transparency is recommended, please
    see [https://hai.stanford.edu/news/introducing-foundation-model-transparency-index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解为什么推荐使用RAG驱动的生成式AI透明度，请参阅[https://hai.stanford.edu/news/introducing-foundation-model-transparency-index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
- en: Join our community on Discord
  id: totrans-375
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.packt.link/rag](https://www.packt.link/rag)'
- en: '![](img/QR_Code50409000288080484.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code50409000288080484.png)'
