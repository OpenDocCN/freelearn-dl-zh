- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why Retrieval Augmented Generation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even the most advanced generative AI models can only generate responses based
    on the data they have been trained on. They cannot provide accurate answers to
    questions about information outside their training data. Generative AI models
    simply don’t know that they don’t know! This leads to inaccurate or inappropriate
    outputs, sometimes called hallucinations, bias, or, simply said, nonsense.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval Augmented Generation** (**RAG**) is a framework that addresses
    this limitation by combining retrieval-based approaches with generative models.
    It retrieves relevant data from external sources in real time and uses this data
    to generate more accurate and contextually relevant responses. Generative AI models
    integrated with RAG retrievers are revolutionizing the field with their unprecedented
    efficiency and power. One of the key strengths of RAG is its adaptability. It
    can be seamlessly applied to any type of data, be it text, images, or audio. This
    versatility makes RAG ecosystems a reliable and efficient tool for enhancing generative
    AI capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: A project manager, however, already encounters a wide range of generative AI
    platforms, frameworks, and models such as Hugging Face, Google Vertex AI, OpenAI,
    LangChain, and more. An additional layer of emerging RAG frameworks and platforms
    will only add complexity with Pinecone, Chroma, Activeloop, LlamaIndex, and so
    on. All these Generative AI and RAG frameworks often overlap, creating an incredible
    number of possible configurations. Finding the right configuration of models and
    RAG resources for a specific project, therefore, can be challenging for a project
    manager. There is no silver bullet. The challenge is tremendous, but the rewards,
    when achieved, are immense!
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin this chapter by defining the RAG framework at a high level. Then,
    we will define the three main RAG configurations: naïve RAG, advanced RAG, and
    modular RAG. We will also compare RAG and fine-tuning and determine when to use
    these approaches. RAG can only exist within an ecosystem, and we will design and
    describe one in this chapter. Data needs to come from somewhere and be processed.
    Retrieval requires an organized environment to retrieve data, and generative AI
    models have input constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will dive into the practical aspect of this chapter. We will build
    a Python program from scratch to run entry-level naïve RAG with keyword search
    and matching. We will also code an advanced RAG system with vector search and
    index-based retrieval. Finally, we will build a modular RAG that takes both naïve
    and advanced RAG into account. By the end of this chapter, you will acquire a
    theoretical understanding of the RAG framework and practical experience in building
    a RAG-driven generative AI program. This hands-on approach will deepen your understanding
    and equip you for the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the RAG framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RAG ecosystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve keyword search and match RAG in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced RAG with vector-search and index-based RAG in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a modular RAG program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining RAG.
  prefs: []
  type: TYPE_NORMAL
- en: What is RAG?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a generative AI model doesn’t know how to answer accurately, some say it
    is hallucinating or producing bias. Simply said, it just produces nonsense. However,
    it all boils down to the impossibility of providing an adequate response when
    the model’s training didn’t include the information requested beyond the classical
    model configuration issues. This confusion often leads to random sequences of
    the most probable outputs, not the most accurate ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG begins where generative AI ends by providing the information an LLM model
    lacks to answer accurately. RAG was designed (Lewis et al., 2020) for LLMs. The
    RAG framework will perform optimized information retrieval tasks, and the generation
    ecosystem will add this information to the input (user query or automated prompt)
    to produce improved output. The RAG framework can be summed up at a high level
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a library  Description automatically generated](img/B31169_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: The two main components of RAG-driven generative AI'
  prefs: []
  type: TYPE_NORMAL
- en: Think of yourself as a student in a library. You have an essay to write on RAG.
    Like ChatGPT, for example, or any other AI copilot, you have learned how to read
    and write. As with any **Large Language Model** (**LLM**), you are sufficiently
    trained to read advanced information, summarize it, and write content. However,
    like any superhuman AI you will find from Hugging Face, Vertex AI, or OpenAI,
    there are many things you don’t know.
  prefs: []
  type: TYPE_NORMAL
- en: In the *retrieval* phase, you search the library for books on the topic you
    need (the left side of *Figure 1.1*). Then, you go back to your seat, perform
    a retrieval task by yourself or a co-student, and extract the information you
    need from those books. In the *generation* phase (the right side of *Figure 1.1*),
    you begin to write your essay. You are a RAG-driven generative human agent, much
    like a RAG-driven generative AI framework.
  prefs: []
  type: TYPE_NORMAL
- en: As you continue to write your essay on RAG, you stumble across some tough topics.
    You don’t have the time to go through all the information available physically!
    You, as a generative human agent, are stuck, just as a generative AI model would
    be. You may try to write something, just as a generative AI model does when its
    output makes little sense. But you, like the generative AI agent, will not realize
    whether the content is accurate or not until somebody corrects your essay and
    you get a grade that will rank your essay.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have reached your limit and decide to turn to a RAG generative
    AI copilot to ensure you get the correct answers. However, you are puzzled by
    the number of LLM models and RAG configurations available. You need first to understand
    the resources available and how RAG is organized. Let’s go through the main RAG
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve, advanced, and modular RAG configurations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A RAG framework necessarily contains two main components: a retriever and a
    generator. The generator can be any LLM or foundation multimodal AI platform or
    model, such as GPT-4o, Gemini, Llama, or one of the hundreds of variations of
    the initial architectures. The retriever can be any of the emerging frameworks,
    methods, and tools such as Activeloop, Pinecone, LlamaIndex, LangChain, Chroma,
    and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The issue now is to decide which of the three types of RAG frameworks (Gao
    et al., 2024) will fit the needs of a project. We will illustrate these three
    approaches in code in the *Naïve, advanced, and modular RAG in code* section of
    this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Naïve RAG**: This type of RAG framework doesn’t involve complex data embedding
    and indexing. It can be efficient to access reasonable amounts of data through
    keywords, for example, to augment a user’s input and obtain a satisfactory response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advanced RAG**: This type of RAG involves more complex scenarios, such as
    with vector search and indexed-base retrieval applied. Advanced RAG can be implemented
    with a wide range of methods. It can process multiple data types, as well as multimodal
    data, which can be structured or unstructured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modular RAG**: Modular RAG broadens the horizon to include any scenario that
    involves naïve RAG, advanced RAG, machine learning, and any algorithm needed to
    complete a complex project.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, before going further, we need to decide if we should implement RAG
    or fine-tune a model.
  prefs: []
  type: TYPE_NORMAL
- en: RAG versus fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RAG is not always an alternative to fine-tuning, and fine-tuning cannot always
    replace RAG. If we accumulate too much data in RAG datasets, the system may become
    too cumbersome to manage. On the other hand, we cannot fine-tune a model with
    dynamic, ever-changing data such as daily weather forecasts, stock market values,
    corporate news, and all forms of daily events.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision of whether to implement RAG or fine-tune a model relies on the
    proportion of parametric versus non-parametric information. The fundamental difference
    between a model trained from scratch or fine-tuned and RAG can be summed up in
    terms of parametric and non-parametric knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parametric**: In a RAG-driven generative AI ecosystem, the parametric part
    refers to the generative AI model’s parameters (weights) learned through training
    data. This means the model’s knowledge is stored in these learned weights and
    biases. The original training data is transformed into a mathematical form, which
    we call a parametric representation. Essentially, the model “remembers” what it
    learned from the data, but the data itself is not stored explicitly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-Parametric**: In contrast, the non-parametric part of a RAG ecosystem
    involves storing explicit data that can be accessed directly. This means that
    the data remains available and can be queried whenever needed. Unlike parametric
    models, where knowledge is embedded indirectly in the weights, non-parametric
    data in RAG allows us to see and use the actual data for each output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The difference between RAG and fine-tuning relies on the amount of static (parametric)
    and dynamic (non-parametric) ever-evolving data the generative AI model must process.
    A system that relies too heavily on RAG might become overloaded and cumbersome
    to manage. A system that relies too much on fine-tuning a generative model will
    display its inability to adapt to daily information updates.
  prefs: []
  type: TYPE_NORMAL
- en: There is a decision-making threshold illustrated in *Figure 1.2* that shows
    that a RAG-driven generative AI project manager will have to evaluate the potential
    of the ecosystem’s trained parametric generative AI model before implementing
    a non-parametric (explicit data) RAG framework. The potential of the RAG component
    requires careful evaluation as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a temperature measurement  Description automatically generated](img/B31169_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: The decision-making threshold between enhancing RAG or fine-tuning
    an LLM'
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the balance between enhancing the retriever and the generator in
    a RAG-driven generative AI ecosystem depends on a project’s specific requirements
    and goals. RAG and fine-tuning are not mutually exclusive.
  prefs: []
  type: TYPE_NORMAL
- en: 'RAG can be used to improve a model’s overall efficiency, together with fine-tuning,
    which serves as a method to enhance the performance of both the retrieval and
    generation components within the RAG framework. We will fine-tune a proportion
    of the retrieval data in *Chapter 9*, *Empowering AI Models: Fine-Tuning RAG Data
    and Human Feedback*.'
  prefs: []
  type: TYPE_NORMAL
- en: We will now see how a RAG-driven generative AI involves an ecosystem with many
    components.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RAG-driven generative AI is a framework that can be implemented in many configurations.
    RAG’s framework runs within a broad ecosystem, as shown in *Figure 1.3*. However,
    no matter how many retrieval and generation frameworks you encounter, it all boils
    down to the following four domains and questions that go with them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data**: Where is the data coming from? Is it reliable? Is it sufficient?
    Are there copyright, privacy, and security issues?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage**: How is the data going to be stored before or after processing
    it? What amount of data will be stored?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieval**: How will the correct data be retrieved to augment the user’s
    input before it is sufficient for the generative model? What type of RAG framework
    will be successful for a project?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation**: Which generative AI model will fit into the type of RAG framework
    chosen?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data, storage, and generation domains depend heavily on the type of RAG
    framework you choose. Before making that choice, we need to evaluate the proportion
    of parametric and non-parametric knowledge in the ecosystem we are implementing.
    *Figure 1.3* represents the RAG framework, which includes the main components
    regardless of the types of RAG implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B31169_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: The Generative RAG-ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: The **Retriever (D)** handles data collection, processing, storage, and retrieval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Generator (G)** handles input augmentation, prompt engineering, and generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Evaluator (E)** handles mathematical metrics, human evaluation, and feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Trainer (T)** handles the initial pre-trained model and fine-tuning the
    model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these four components relies on their respective ecosystems, which form
    the overall RAG-driven generative AI pipeline. We will refer to the domains D,
    G, E, and T in the following sections. Let’s begin with the retriever.
  prefs: []
  type: TYPE_NORMAL
- en: The retriever (D)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The retriever component of a RAG ecosystem collects, processes, stores, and
    retrieves data. The starting point of a RAG ecosystem is thus an ingestion data
    process, of which the first step is to collect data.
  prefs: []
  type: TYPE_NORMAL
- en: Collect (D1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In today’s world, AI data is as diverse as our media playlists. It can be anything
    from a chunk of text in a blog post to a meme or even the latest hit song streamed
    through headphones. And it doesn’t stop there—the files themselves come in all
    shapes and sizes. Think of PDFs filled with all kinds of details, web pages, plain
    text files that get straight to the point, neatly organized JSON files, catchy
    MP3 tunes, videos in MP4 format, or images in PNG and JPG.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, a large proportion of this data is unstructured and found in unpredictable
    and complex ways. Fortunately, many platforms, such as Pinecone, OpenAI, Chroma,
    and Activeloop, provide ready-to-use tools to process and store this jungle of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Process (D2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the data collection phase (D1) of multimodal data processing, various types
    of data, such as text, images, and videos, can be extracted from websites using
    web scraping techniques or any other source of information. These data objects
    are then transformed to create uniform feature representations. For example, data
    can be chunked (broken into smaller parts), embedded (transformed into vectors),
    and indexed to enhance searchability and retrieval efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce these techniques, starting with the *Building Hybrid Adaptive
    RAG in Python* section of this chapter. In the following chapters, we will continue
    building more complex data processing functions.
  prefs: []
  type: TYPE_NORMAL
- en: Storage (D3)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this stage of the pipeline, we have collected and begun processing a large
    amount of diverse data from the internet—videos, pictures, texts, you name it.
    Now, what can we do with all that data to make it useful?
  prefs: []
  type: TYPE_NORMAL
- en: That’s where vector stores like Deep Lake, Pinecone, and Chroma come into play.
    Think of these as super smart libraries that don’t just store your data but convert
    it into mathematical entities as vectors, enabling powerful computations. They
    can also apply a variety of indexing methods and other techniques for rapid access.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of keeping the data in static spreadsheets and files, we turn it into
    a dynamic, searchable system that can power anything from chatbots to search engines.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval query (D4)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The retrieval process is triggered by the user input or automated input (G1).
  prefs: []
  type: TYPE_NORMAL
- en: To retrieve data quickly, we load it into vector stores and datasets after transforming
    it into a suitable format. Then, using a combination of keyword searches, smart
    embeddings, and indexing, we can retrieve the data efficiently. Cosine similarity,
    for example, finds items that are closely related, ensuring that the search results
    are not just fast but also highly relevant.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is retrieved, we then augment the input.
  prefs: []
  type: TYPE_NORMAL
- en: The generator (G)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The lines are blurred in the RAG ecosystem between input and retrieval, as shown
    in *Figure 1.3*, representing the RAG framework and ecosystem. The user input
    (G1), automated or human, interacts with the retrieval query (D4) to augment the
    input before sending it to the generative model.
  prefs: []
  type: TYPE_NORMAL
- en: The generative flow begins with an input.
  prefs: []
  type: TYPE_NORMAL
- en: Input (G1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The input can be a batch of automated tasks (processing emails, for example)
    or human prompts through a **User Interface** (**UI**). This flexibility allows
    you to seamlessly integrate AI into various professional environments, enhancing
    productivity across industries.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented input with HF (G2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Human feedback** (**HF**) can be added to the input, as described in the
    *Human feedback (E2) under Evaluator (E)* section. Human feedback will make a
    RAG ecosystem considerably adaptable and provide full control over data retrieval
    and generative AI inputs. In the *Building hybrid adaptive RAG in Python* section
    of this chapter, we will build augmented input with human feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering (G3)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both the retriever (D) and the generator (G) rely heavily on prompt engineering
    to prepare the standard and augmented message that the generative AI model will
    have to process. Prompt engineering brings the retriever’s output and the user
    input together.
  prefs: []
  type: TYPE_NORMAL
- en: Generation and output (G4)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The choice of a generative AI model depends on the goals of a project. Llama,
    Gemini, GPT, and other models can fit various requirements. However, the prompt
    must meet each model’s specifications. Frameworks such as LangChain, which we
    will implement in this book, help streamline the integration of various AI models
    into applications by providing adaptable interfaces and tools.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluator (E)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We often rely on mathematical metrics to assess the performance of a generative
    AI model. However, these metrics only give us part of the picture. It’s important
    to remember that the ultimate test of an AI’s effectiveness comes down to human
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics (E1)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A model cannot be evaluated without mathematical metrics, such as cosine similarity,
    as with any AI system. These metrics ensure that the retrieved data is relevant
    and accurate. By quantifying the relationships and relevance of data points, they
    provide a solid foundation for assessing the model’s performance and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Human feedback (E2)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: No generative AI system, whether RAG-driven or not, and whether the mathematical
    metrics seem sufficient or not, can elude human evaluation. It is ultimately human
    evaluation that decides if a system designed for human users will be accepted
    or rejected, praised or criticized.
  prefs: []
  type: TYPE_NORMAL
- en: Adaptive RAG introduces the human, real-life, pragmatic feedback factor that
    will improve a RAG-driven generative AI ecosystem. We will implement adaptive
    RAG in *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*.
  prefs: []
  type: TYPE_NORMAL
- en: The trainer (T)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A standard generative AI model is pre-trained with a vast amount of general-purpose
    data. Then, we can fine-tune (T2) the model with domain-specific data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take this further by integrating static RAG data into the fine-tuning
    process in *Chapter 9*, *Empowering AI Models: Fine-Tuning RAG Data and Human
    Feedback*. We will also integrate human feedback, which provides valuable information
    that can be integrated into the fine-tuning process in a variant of **Reinforcement
    Learning from Human Feedback** (**RLHF**).'
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to code entry-level naïve, advanced, and modular RAG in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve, advanced, and modular RAG in code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section introduces naïve, advanced, and modular RAG through basic educational
    examples. The program builds keyword matching, vector search, and index-based
    retrieval methods. Using OpenAI’s GPT models, it generates responses based on
    input queries and retrieved documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the notebook is for a conversational agent to answer questions
    on RAG in general. We will build the retriever from the bottom up, from scratch,
    in Python and run the generator with OpenAI GPT-4o in eight sections of code divided
    into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Part 1: Foundations and Basic Implementation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Environment** setup for OpenAI API integration'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Generator** function using GPT-4o'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Data** setup with a list of documents (`db_records`)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Query** for user input'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Part 2: Advanced Techniques and Evaluation**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval metrics** to measure retrieval responses'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Naïve RAG** with a keyword search and matching function'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Advanced RAG** with vector search and index-based search'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Modular RAG** implementing flexible retrieval methods'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To get started, open `RAG_Overview.ipynb` in the GitHub repository. We will
    begin by establishing the foundations of the notebook and exploring the basic
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 1: Foundations and basic implementation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will set up the environment, create a function for the generator,
    define a function to print a formatted response, and define the user query.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to install the environment.
  prefs: []
  type: TYPE_NORMAL
- en: The section titles of the following implementation of the notebook follow the
    structure in the code. Thus, you can follow the code in the notebook or read this
    self-contained section.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main package to install is OpenAI to access GPT-4o through an API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to freeze the OpenAI version you install. In RAG framework ecosystems,
    we will have to install several packages to run advanced RAG configurations. Once
    we have stabilized an installation, we will freeze the version of the packages
    installed to minimize potential conflicts between the libraries and modules we
    implement.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have installed `openai`, you will have to create an account on OpenAI
    (if you don’t have one) and obtain an API key. Make sure to check the costs and
    payment plans before running the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a key, store it in a safe place and retrieve it as follows from
    Google Drive, for example, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use Google Drive or any other method you choose to store your key.
    You can read the key from a file, or you can also choose to enter the key directly
    in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have set up the main resources for our project. We will now write
    a generation function for the OpenAI model.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The code imports `openai` to generate content and `time` to measure the time
    the requests take:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create a function that creates a prompt with an instruction and the
    user input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The function will try to call `gpt-4o`, adding additional information for the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that the instruction messages remain general in this scenario so that the
    model remains flexible. The `temperature` is low (more precise) and set to `0.1`.
    If you wish for the system to be more creative, you can set `temperature` to a
    higher value, such as `0.7`. However, in this case, it is recommended to ask for
    precise responses.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can add `textwrap` to format the response as a nice paragraph when we call
    the generative AI model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The generator is now ready to be called when we need it. Due to the probabilistic
    nature of generative AI models, it might produce different outputs each time we
    call it.
  prefs: []
  type: TYPE_NORMAL
- en: The program now implements the data retrieval functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. The Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data collection includes text, images, audio, and video. In this notebook, we
    will focus on **data retrieval** through naïve, advanced, and modular configurations,
    not data collection. We will collect and embed data later in *Chapter 2*, *RAG
    Embedding Vector Stores with Deep Lake and OpenAI*. As such, we will assume that
    the data we need has been processed and thus collected, cleaned, and split into
    sentences. We will also assume that the process included loading the sentences
    into a Python list named `db_records`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach illustrates three aspects of the RAG ecosystem we described in
    *The RAG ecosystem* section and the components of the system described in *Figure
    1.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: The **retriever (D)** has three **data processing** components, **collect (D1)**,
    **process (D2)**, and **storage (D3)**, which are preparatory phases of the retriever.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **retriever query (D4)** is thus independent of the first three phases (collect,
    process, and storage) of the retriever.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data processing phase will often be done independently and prior to activating
    the retriever query, as we will implement starting in *Chapter 2*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This program assumes that data processing has been completed and the dataset
    is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can display a formatted version of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output joins the sentences in `db_records` for display, as printed in this
    excerpt, but `db_records` remains unchanged:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The program is now ready to process a query.
  prefs: []
  type: TYPE_NORMAL
- en: 4.The query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **retriever** (**D4** in *Figure 1.3*) query process depends on how the
    data was processed, but the query itself is simply user input or automated input
    from another AI agent. We all dream of users who introduce the best input into
    software systems, but unfortunately, in real life, unexpected inputs lead to unpredictable
    behaviors. We must, therefore, build systems that take imprecise inputs into account.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will imagine a situation in which hundreds of users in
    an organization have heard the word “RAG” associated with “LLM” and “vector stores.”
    Many of them would like to understand what these terms mean to keep up with a
    software team that’s deploying a conversational agent in their department. After
    a couple of days, the terms they heard become fuzzy in their memory, so they ask
    the conversational agent, GPT-4o in this case, to explain what they remember with
    the following query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we will simply store the main query of the topic of this program
    in `query`, which represents the junction between the retriever and the generator.
    It will trigger a configuration of RAG (naïve, advanced, and modular). The choice
    of configuration will depend on the goals of each project.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program takes the query and sends it to a GPT-4o model to be processed
    and then displays the formatted output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is revealing. Even the most powerful generative AI models cannot
    guess what a user, who knows nothing about AI, is trying to find out in good faith.
    In this case, GPT-4o will answer as shown in this excerpt of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will seem like a hallucination, but is it really? The user wrote
    the query with the good intentions of every beginner trying to learn a new topic.
    GPT-4o, in good faith, did what it could with the limited context it had with
    its probabilistic algorithm, which might even produce a different response each
    time we run it. However, GPT-4o is being wary of the query. It wasn’t very clear,
    so it ends the response with the following output that asks the user for more
    context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The user is puzzled, not knowing what to do, and GPT-4o is awaiting further
    instructions. The software team has to do something!
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI is based on probabilistic algorithms. As such, the response provided
    might vary from one run to another, providing similar (but not identical) responses.
  prefs: []
  type: TYPE_NORMAL
- en: That is when RAG comes in to save the situation. We will leave this query as
    it is for the whole notebook and see if a RAG-driven GPT-4o system can do better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Advanced techniques and evaluation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Part 2*, we will introduce naïve, advanced, and modular RAG. The goal is
    to introduce the three methods, not to process complex documents, which we will
    implement throughout the following chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s first begin by defining retrieval metrics to measure the accuracy of the
    documents we retrieve.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Retrieval metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section explores retrieval metrics, first focusing on the role of cosine
    similarity in assessing the relevance of text documents. Then we will implement
    enhanced similarity metrics by incorporating synonym expansion and text preprocessing
    to improve the accuracy of similarity calculations between texts.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore more metrics in the *Metrics calculation and display* section
    in *Chapter 7*, *Building Scalable Knowledge-Graph-Based RAG with Wikipedia API
    and LlamaIndex*.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, let’s begin with cosine similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cosine similarity measures the cosine of the angle between two vectors. In our
    case, the two vectors are the user query and each document in a corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first imports the class and function we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`TfidfVectorizer` imports the class that converts text documents into a matrix
    of TF-IDF features. **Term Frequency-Inverse Document Frequency** (**TF-IDF**)
    quantifies the relevance of a word to a document in a collection, distinguishing
    common words from those significant to specific texts. TF-IDF will thus quantify
    word relevance in documents using frequency across the document and inverse frequency
    across the corpus. `cosine_similarity` imports the function we will use to calculate
    the similarity between vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '`calculate_cosine_similarity(text1, text2)` then calculates the cosine similarity
    between the query (`text1`) and each record of the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The function converts the query text (`text1`) and each record (`text2`) in
    the dataset into a vector with a vectorizer. Then, it calculates and returns the
    cosine similarity between the two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The key parameters of this function are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`stop_words=''english`: Ignores common English words to focus on meaningful
    content'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_idf=True`: Enables inverse document frequency weighting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`norm=''l2''`: Applies L2 normalization to each output vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ngram_range=(1, 2)`: Considers both single words and two-word combinations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sublinear_tf=True`: Applies logarithmic term frequency scaling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`analyzer=''word''`: Analyzes text at the word level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine similarity can be limited in some cases. Cosine similarity has limitations
    when dealing with ambiguous queries because it strictly measures the similarity
    based on the angle between vector representations of text. If a user asks a vague
    question like “What is rag?” in the program of this chapter and the database primarily
    contains information on “RAG” as in “retrieval-augmented generation” for AI, not
    “rag cloths,” the cosine similarity score might be low. This low score occurs
    because the mathematical model lacks contextual understanding to differentiate
    between the different meanings of “rag.” It only computes similarity based on
    the presence and frequency of similar words in the text, without grasping the
    user’s intent or the broader context of the query. Thus, even if the answers provided
    are technically accurate within the available dataset, the cosine similarity may
    not reflect the relevance accurately if the query’s context isn’t well-represented
    in the data.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we can try enhanced similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Enhanced similarity introduces calculations that leverage natural language processing
    tools to better capture semantic relationships between words. Using libraries
    like spaCy and NLTK, it preprocesses texts to reduce noise, expands terms with
    synonyms from WordNet, and computes similarity based on the semantic richness
    of the expanded vocabulary. This method aims to improve the accuracy of similarity
    assessments between two texts by considering a broader context than typical direct
    comparison methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code contains four main functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`get_synonyms(word)`: Retrieves synonyms for a given word from WordNet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preprocess_text(text)`: Converts all text to lowercase, lemmatizes gets the
    (roots of words), and filters stopwords (common words) and punctuation from text'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`expand_with_synonyms(words)`: Enhances a list of words by adding their synonyms'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`calculate_enhanced_similarity(text1, text2)`: Computes cosine similarity between
    preprocessed and synonym-expanded text vectors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `calculate_enhanced_similarity(text1, text2)` function takes two texts and
    ultimately returns the cosine similarity score between two processed and synonym-expanded
    texts. This score quantifies the textual similarity based on their semantic content
    and enhanced word sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code begins by downloading and importing the necessary libraries and then
    runs the four functions beginning with `calculate_enhanced_similarity(text1, text2)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Enhanced similarity takes this a bit further in terms of metrics. However, integrating
    RAG with generative AI presents multiple challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'No matter which metric we implement, we will face the following limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input versus Document Length**: User queries are often short, while retrieved
    documents are longer and richer, complicating direct similarity evaluations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creative Retrieval**: Systems may creatively select longer documents that
    meet user expectations but yield poor metric scores due to unexpected content
    alignment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Need for Human Feedback**: Often, human judgment is crucial to accurately
    assess the relevance and effectiveness of retrieved content, as automated metrics
    may not fully capture user satisfaction. We will explore this critical aspect
    of RAG in *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will always have to find the right balance between mathematical metrics and
    human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to create an example with naïve RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Naïve RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Naïve RAG with keyword search and matching can prove efficient with well-defined
    documents within an organization, such as legal and medical documents. These documents
    generally have clear titles or labels for images, for example. In this naïve RAG
    function, we will implement keyword search and matching. To achieve this, we will
    apply a straightforward retrieval method in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the query into individual keywords
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split each record in the dataset into keywords
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the length of the common matches
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the record with the best score
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The generation method will:'
  prefs: []
  type: TYPE_NORMAL
- en: Augment the user input with the result of the retrieval query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request the generation model, which is `gpt-4o` in this case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s write the keyword search and matching function.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword search and matching
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The best matching function first initializes the best scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The query is then split into keywords. Each record is also split into words
    to find the common words, measure the length of common content, and find the best
    match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We now call the function, format the response, and print it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The main query of this notebook will be `query = "define a rag store"` to see
    if each RAG method produces an acceptable output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The keyword search finds the best record in the list of sentences in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s run the metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We created the similarity metrics in the *1\. Retrieval metrics* section of
    this chapter. We will first apply cosine similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output similarity is low, as explained in the *1\. Retrieval metrics* section
    of this chapter. The user input is short and the response is longer and complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Enhanced similarity will produce a better score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The score produced is higher with enhanced functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The output of the query will now augment the user input.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented input
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The augmented input is the concatenation of the user input and the best matching
    record of the dataset detected with the keyword search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The augmented input is displayed if necessary for maintenance reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output then shows that the augmented input is ready:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The input is now ready for the generation process.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We are now ready to call GPT-4o and display the formatted response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The following excerpt of the response shows that GPT-4o understands the input
    and provides an interesting, pertinent response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Naïve RAG can be sufficient in many situations. However, if the volume of documents
    becomes too large or the content becomes more complex, then advanced RAG configurations
    will provide better results. Let’s now explore advanced RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Advanced RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As datasets grow larger, keyword search methods might prove too long to run.
    For instance, if we have hundreds of documents and each document contains hundreds
    of sentences, it will become challenging to use keyword search only. Using an
    index will reduce the computational load to just a fraction of the total data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will go beyond searching text with keywords. We will see
    how RAG transforms text data into numerical representations, enhancing search
    efficiency and processing speed. Unlike traditional methods that directly parse
    text, RAG first converts documents and user queries into vectors, numerical forms
    that speed up calculations. In simple terms, a vector is a list of numbers representing
    various features of text. Simple vectors might count word occurrences (term frequency),
    while more complex vectors, known as embeddings, capture deeper linguistic patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will implement vector search and index-based search:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector Search**: We will convert each sentence in our dataset into a numerical
    vector. By calculating the cosine similarity between the query vector (the user
    query) and these document vectors, we can quickly find the most relevant documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index-Based Search**: In this case, all sentences are converted into vectors
    using **TF-IDF** (**Term Frequency-Inverse Document Frequency**), a statistical
    measure used to evaluate how important a word is to a document in a collection.
    These vectors act as indices in a matrix, allowing quick similarity comparisons
    without parsing each document fully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with vector search and see these concepts in action.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.Vector search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector search converts the user query and the documents into numerical values
    as vectors, enabling mathematical calculations that *retrieve relevant data faster
    when dealing with large volumes of data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program runs through each record of the dataset to find the best matching
    document by computing the cosine similarity of the query vector and each record
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then calls the vector search function and displays the best record
    found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The response is the best one found, like with naïve RAG. This shows that there
    is no silver bullet. Each RAG technique has its merits. The metrics will confirm
    this observation.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The metrics are the same for both similarity methods as for naïve RAG because
    the same document was retrieved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And with enhanced similarity, we obtain the same output as for naïve RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms the trend:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: So why use vector search if it produces the same outputs as naïve RAG? Well,
    in a small dataset, everything looks easy. But when we’re dealing with datasets
    of millions of complex documents, keyword search will not capture subtleties that
    vectors can. Let’s now augment the user query with this information retrieved.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented input
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We add the information retrieved to the user query with no other aid and display
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We only added a space between the user query and the retrieved information;
    nothing else. The output is satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now see how the generative AI model reacts to this augmented input.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We now call GPT-4o with the augmented input and display the formatted output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The response makes sense, as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: While vector search significantly speeds up the process of finding relevant
    documents by sequentially going through each record, its efficiency can decrease
    as the dataset size increases. To address this scalability issue, indexed search
    offers a more advanced solution. Let’s now see how index-based search can accelerate
    document retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Index-based search
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Index-based search compares the vector of a user query not with the direct vector
    of a document’s content but with an indexed vector that represents this content.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program first imports the class and function we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`TfidfVectorizer` imports the class that converts text documents into a matrix
    of TF-IDF features. TF-IDF will quantify word relevance in documents using frequency
    across the document. The function finds the best matches using the cosine similarity
    function to calculate the similarity between the query and the weighted vectors
    of the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The function’s main tasks are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transform Query**: Converts the input query into TF-IDF vector format using
    the provided vectorizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculate Similarities**: Computes the cosine similarity between the query
    vector and all vectors in the tfidf_matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identify Best Match**: Finds the index (`best_index`) of the highest similarity
    score in the results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieve Best Score**: Extracts the highest cosine similarity score (`best_score`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output is the best similarity score found and the best index.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code first calls the dataset vectorizer and then searches for
    the most similar record through its index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the results are displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The system finds the best similar document to the user’s input query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the fuzzy user query produced a reliable output at the retrieval
    level before running GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: The metrics that follow in the program are the same as for naïve and advanced
    RAG with vector search. This is normal because the document found is the closest
    to the user’s input query. We will be introducing more complex documents for RAG
    starting in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*.
    For now, let’s have a look at the features that influence how the words are represented
    in vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Before augmenting the input with this document, run the following cell, which
    calls the `setup_vectorizer(records)` function again but displays the matrix so
    that you can see its format. This is shown in the following excerpt for the words
    “accurate” and “additional” in one of the sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A black and white image of a number  Description automatically generated
    with medium confidence](img/B31169_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Format of the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now augment the input.
  prefs: []
  type: TYPE_NORMAL
- en: Augmented input
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will simply add the query to the best matching record in a minimal way to
    see how GPT-4o will react and display the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is close to or the same as with vector search, but the retrieval
    method is faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We will now plug this augmented input into the generative AI model.
  prefs: []
  type: TYPE_NORMAL
- en: Generation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We now call GPT-4o with the augmented input and display the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output makes sense for the user who entered the initial fuzzy query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: This approach worked well in a closed environment within an organization in
    a specific domain. In an open environment, the user might have to elaborate before
    submitting a request.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw that a TF-IDF matrix pre-computes document vectors,
    enabling faster, simultaneous comparisons without repeated vector transformations.
    We have seen how vector and index-based search can improve retrieval. However,
    in one project, we may need to apply naïve and advanced RAG depending on the documents
    we need to retrieve. Let’s now see how modular RAG can improve our system.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Modular RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Should we use keyword search, vector search, or index-based search when implementing
    RAG? Each approach has its merits. The choice will depend on several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Keyword search** suits simple retrieval'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector search** is ideal for semantic-rich documents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Index-based search** offers speed with large data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, all three methods can perfectly fit together in a project. In one scenario,
    for example, a keyword search can help find clearly defined document labels, such
    as the titles of PDF files and labeled images, before they are processed. Then,
    indexed search will group the documents into indexed subsets. Finally, the retrieval
    program can search the indexed dataset, find a subset, and only use vector search
    to go through a limited number of documents to find the most relevant one.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will create a `RetrievalComponent` class that can be called
    at each step of a project to perform the task required. The code sums up the three
    methods we have built in this chapter and that we can sum for the `RetrievalComponent`
    through its main members.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code initializes the class with search method choice and prepares
    a vectorizer if needed. `self` refers to the current instance of the class to
    access its variables, methods, and functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the vector search is activated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fit` method builds a TF-IDF matrix from records, and is applicable for
    vector or indexed search methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The retrieve method directs the query to the appropriate search method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The keyword search method finds the best match by counting common keywords
    between queries and documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The vector search method computes similarities between query TF-IDF and document
    matrix and returns the best match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The indexed search method uses a precomputed TF-IDF matrix for fast retrieval
    of the best-matching document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We can now activate modular RAG strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Modular RAG strategies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We can call the retrieval component for any RAG configuration we wish when
    needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the vector search method was activated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following cells select the best record, as in the *3.1\. Vector search*
    section, augment the input, call the generative model, and display the output
    as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We have built a program that demonstrated how different search methodologies—keyword,
    vector, and index-based—can be effectively integrated into a RAG system. Each
    method has its unique strengths and addresses specific needs within a data retrieval
    context. The choice of method depends on the dataset size, query type, and performance
    requirements, which we will explore in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: It’s now time to summarize our explorations in this chapter and move to the
    next level!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RAG for generative AI relies on two main components: a retriever and a generator.
    The retriever processes data and defines a search method, such as fetching labeled
    documents with keywords—the generator’s input, an LLM, benefits from augmented
    information when producing sequences. We went through the three main configurations
    of the RAG framework: naïve RAG, which accesses datasets through keywords and
    other entry-level search methods; advanced RAG, which introduces embeddings and
    indexes to improve the search methods; and modular RAG, which can combine naïve
    and advanced RAG as well as other ML methods.'
  prefs: []
  type: TYPE_NORMAL
- en: The RAG framework relies on datasets that can contain dynamic data. A generative
    AI model relies on parametric data through its weights. These two approaches are
    not mutually exclusive. If the RAG datasets become too cumbersome, fine-tuning
    can prove useful. When fine-tuned models cannot respond to everyday information,
    RAG can come in handy. RAG frameworks also rely heavily on the ecosystem that
    provides the critical functionality to make the systems work. We went through
    the main components of the RAG ecosystem, from the retriever to the generator,
    for which the trainer is necessary, and the evaluator. Finally, we built an entry-level
    naïve, advanced, and modular RAG program in Python, leveraging keyword matching,
    vector search, and index-based retrieval, augmenting the input of GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: Our next step in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and
    OpenAI*, is to embed data in vectors. We will store the vectors in vector stores
    to enhance the speed and precision of the retrieval functions of a RAG ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with *Yes* or *No*:'
  prefs: []
  type: TYPE_NORMAL
- en: Is RAG designed to improve the accuracy of generative AI models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does a naïve RAG configuration rely on complex data embedding?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is fine-tuning always a better option than using RAG?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does RAG retrieve data from external sources in real time to enhance responses?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can RAG be applied only to text-based data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the retrieval process in RAG triggered by a user or automated input?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are cosine similarity and TF-IDF both metrics used in advanced RAG configurations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the RAG ecosystem include only data collection and generation components?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can advanced RAG configurations process multimodal data such as images and audio?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is human feedback irrelevant in evaluating RAG systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks* by Patrick
    Lewis, Ethan Perez, Aleksandra Piktus, et al.: [https://arxiv.org/abs/2005.11401](https://arxiv.org/abs/2005.11401)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Retrieval-Augmented Generation for Large Language Models: A Survey* by Yunfan
    Gao, Yun Xiong, Xinyu Gao, et al.: [https://arxiv.org/abs/2312.10997](https://arxiv.org/abs/2312.10997)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI models: [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand why RAG-driven Generative AI transparency is recommended, please
    see [https://hai.stanford.edu/news/introducing-foundation-model-transparency-index](https://hai.stanford.edu/news/introducing-foundation-model-transparency-index)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
