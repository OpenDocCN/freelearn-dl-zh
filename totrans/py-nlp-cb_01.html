<html><head></head><body>
		<div id="_idContainer006" class="calibre2">
			<h1 id="_idParaDest-14" class="chapter-number"><a id="_idTextAnchor013" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.1.1">1</span></h1>
			<h1 id="_idParaDest-15" class="calibre7"><a id="_idTextAnchor014" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.2.1">Learning NLP Basics</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.3.1">While working on this book, we were focusing on including recipes that should be useful for a wide variety of NLP projects. </span><span class="kobospan" id="kobo.3.2">They range from simple to advanced, from dealing with grammar to dealing with visualizations, and in many of them, options for languages other than English are included. </span><span class="kobospan" id="kobo.3.3">In this new edition, we have included new topics that cover using GPT and other large language models, explainable AI, a new chapter on transformers, and natural language understanding. </span><span class="kobospan" id="kobo.3.4">We hope you find the </span><span><span class="kobospan" id="kobo.4.1">book useful.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.5.1">The format of the book is that of a </span><em class="italic"><span class="kobospan" id="kobo.6.1">programming cookbook</span></em><span class="kobospan" id="kobo.7.1">, where each recipe is a short mini-project with a concrete goal and a sequence of steps that need to be performed. </span><span class="kobospan" id="kobo.7.2">There are few theoretical explanations and a focus on the practical goals and what needs to be done to </span><span><span class="kobospan" id="kobo.8.1">achieve them.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.9.1">Before we can get on with the real work of NLP, we need to prepare our text for processing. </span><span class="kobospan" id="kobo.9.2">This chapter will show you how to do it. </span><span class="kobospan" id="kobo.9.3">By the end of the chapter, you will be able to have a list of words in a text with their parts of speech and lemmas or stems, and with very frequent </span><span><span class="kobospan" id="kobo.10.1">words removed.</span></span></p>
			<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.11.1">Natural Language Toolkit</span></strong><span class="kobospan" id="kobo.12.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.13.1">NLTK</span></strong><span class="kobospan" id="kobo.14.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.15.1">spaCy</span></strong><span class="kobospan" id="kobo.16.1"> are two </span><a id="_idIndexMarker000" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.17.1">important packages that</span><a id="_idIndexMarker001" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.18.1"> we will be working with in this chapter and throughout the book. </span><span class="kobospan" id="kobo.18.2">Some other packages we will be using in the book are PyTorch and Hugging Face Transformers. </span><span class="kobospan" id="kobo.18.3">We will also utilize the OpenAI API with the </span><span><span class="kobospan" id="kobo.19.1">GPT models.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.20.1">The recipes included in this chapter are </span><span><span class="kobospan" id="kobo.21.1">as follows:</span></span></p>
			<ul class="calibre15">
				<li class="calibre14"><span class="kobospan" id="kobo.22.1">Dividing text </span><span><span class="kobospan" id="kobo.23.1">into sentences</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.24.1">Dividing sentences into words – </span><span><span class="kobospan" id="kobo.25.1">tokenizatio</span><a id="_idTextAnchor015" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.26.1">n</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.27.1">Part of </span><span><span class="kobospan" id="kobo.28.1">speech tagging</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.29.1">Combining similar words – </span><span><span class="kobospan" id="kobo.30.1">lemmatization</span></span></li>
				<li class="calibre14"><span><span class="kobospan" id="kobo.31.1">Removing stopwords</span></span></li>
			</ul>
			<h1 id="_idParaDest-16" class="calibre7"><a id="_idTextAnchor016" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.32.1">Technical requirements</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.33.1">Throughout this book, we will use </span><strong class="bold"><span class="kobospan" id="kobo.34.1">Poetry</span></strong><span class="kobospan" id="kobo.35.1"> to manage the Python package installations. </span><span class="kobospan" id="kobo.35.2">You can use the latest version of Poetry since it conserves the previous versions’ functionality. </span><span class="kobospan" id="kobo.35.3">Once you install Poetry, managing which packages to install will be very easy. </span><span class="kobospan" id="kobo.35.4">We will be using </span><strong class="bold"><span class="kobospan" id="kobo.36.1">Python 3.9</span></strong><span class="kobospan" id="kobo.37.1"> throughout the book. </span><span class="kobospan" id="kobo.37.2">You will also need to have </span><strong class="bold"><span class="kobospan" id="kobo.38.1">Jupyter</span></strong><span class="kobospan" id="kobo.39.1"> installed in order to run </span><span><span class="kobospan" id="kobo.40.1">the notebooks.</span></span></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.41.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.42.1">You may try to use Google Colab in order to run the notebooks but you will need to tweak the code to make it work </span><span><span class="kobospan" id="kobo.43.1">with Colab.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.44.1">Follow these </span><span><span class="kobospan" id="kobo.45.1">installation steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.46.1">Install </span><span><strong class="bold"><span class="kobospan" id="kobo.47.1">Git</span></strong></span><span><span class="kobospan" id="kobo.48.1">: </span></span><a href="https://github.com/git-guides/install-git" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.49.1">https://github.com/git-guides/install-git</span></span></a><span><span class="kobospan" id="kobo.50.1">.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.51.1">Install </span><span><strong class="bold"><span class="kobospan" id="kobo.52.1">Poetry</span></strong></span><span><span class="kobospan" id="kobo.53.1">: </span></span><a href="https://python-poetry.org/docs/#installation" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.54.1">https://python-poetry.org/docs/#installation</span></span></a><span><span class="kobospan" id="kobo.55.1">.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.56.1">Install </span><span><strong class="bold"><span class="kobospan" id="kobo.57.1">Jupyter</span></strong></span><span><span class="kobospan" id="kobo.58.1">: </span></span><a href="https://jupyter.org/install" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.59.1">https://jupyter.org/install</span></span></a><span><span class="kobospan" id="kobo.60.1">.</span></span></li>
				<li class="calibre14"><span class="kobospan" id="kobo.61.1">Clone the GitHub repository that contains all the code from this book (</span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.62.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition</span></a><span class="kobospan" id="kobo.63.1">) by issuing the following command in </span><span><span class="kobospan" id="kobo.64.1">the terminal:</span></span></li>
			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.65.1">
git clone https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition.git</span></pre>			<ol class="calibre13">
				<li value="5" class="calibre14"><span class="kobospan" id="kobo.66.1">In the directory that contains the </span><strong class="source-inline1"><span class="kobospan" id="kobo.67.1">pyproject.toml</span></strong><span class="kobospan" id="kobo.68.1"> file, run the commands using </span><span><span class="kobospan" id="kobo.69.1">the terminal:</span></span></li>
			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.70.1">
poetry install
poetry shell</span></pre>			<ol class="calibre13">
				<li value="6" class="calibre14"><span class="kobospan" id="kobo.71.1">Start the </span><span><span class="kobospan" id="kobo.72.1">notebook engine:</span></span></li>
			</ol>
			<pre class="console"><span class="kobospan1" id="kobo.73.1">
jupyter notebook</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.74.1">Now, you should be able to run all the notebooks in your </span><span><span class="kobospan" id="kobo.75.1">cloned repository.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.76.1">If you prefer not to use Poetry, you can set up a virtual environment using the </span><strong class="source-inline"><span class="kobospan" id="kobo.77.1">requirements.txt</span></strong><span class="kobospan" id="kobo.78.1"> file provided in the book repository. </span><span class="kobospan" id="kobo.78.2">You can do this in one of two ways. </span><span class="kobospan" id="kobo.78.3">You can </span><span><span class="kobospan" id="kobo.79.1">use </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.80.1">pip</span></strong></span><span><span class="kobospan" id="kobo.81.1">:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.82.1">
pip install -r requirements.txt</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.83.1">You can also </span><span><span class="kobospan" id="kobo.84.1">use </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.85.1">conda</span></strong></span><span><span class="kobospan" id="kobo.86.1">:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.87.1">
conda create --name &lt;env_name&gt; --file requirements.txt</span></pre>			<h1 id="_idParaDest-17" class="calibre7"><a id="_idTextAnchor017" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.88.1">Dividing text into sentences</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.89.1">When we</span><a id="_idIndexMarker002" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.90.1"> work with text, we can work with text units on different scales: the document itself, such as a newspaper article, the paragraph, the sentence, or the word. </span><span class="kobospan" id="kobo.90.2">Sentences are the main unit of processing in many NLP tasks. </span><span class="kobospan" id="kobo.90.3">For example, when we send data over to </span><strong class="bold"><span class="kobospan" id="kobo.91.1">Large Language Models</span></strong><span class="kobospan" id="kobo.92.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.93.1">LLMs</span></strong><span class="kobospan" id="kobo.94.1">), we</span><a id="_idIndexMarker003" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.95.1"> frequently want to add some context to the prompt. </span><span class="kobospan" id="kobo.95.2">In some cases, we would like that context to include sentences from a text so that the </span><a id="_idIndexMarker004" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.96.1">model can extract some important information from that text. </span><span class="kobospan" id="kobo.96.2">In this section, we will show you how to divide a text </span><span><span class="kobospan" id="kobo.97.1">into sentences.</span></span></p>
			<h2 id="_idParaDest-18" class="calibre5"><a id="_idTextAnchor018" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.98.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.99.1">For this part, we will be using the text of the book </span><em class="italic"><span class="kobospan" id="kobo.100.1">The Adventures of Sherlock Holmes</span></em><span class="kobospan" id="kobo.101.1">. </span><span class="kobospan" id="kobo.101.2">You can find the whole text in the book’s GitHub file (</span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.102.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt</span></a><span class="kobospan" id="kobo.103.1">). </span><span class="kobospan" id="kobo.103.2">For this recipe we will need just the beginning of the book, which can be found in the file </span><span><span class="kobospan" id="kobo.104.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.105.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</span></span></a><span><span class="kobospan" id="kobo.106.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.107.1">In order to do this task, you will need the NLTK package and its sentence tokenizers, which are part of the Poetry file. </span><span class="kobospan" id="kobo.107.2">Directions to install Poetry are described in the </span><em class="italic"><span class="kobospan" id="kobo.108.1">Technical </span></em><span><em class="italic"><span class="kobospan" id="kobo.109.1">requirements</span></em></span><span><span class="kobospan" id="kobo.110.1"> section.</span></span></p>
			<h2 id="_idParaDest-19" class="calibre5"><a id="_idTextAnchor019" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.111.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.112.1">We will </span><a id="_idIndexMarker005" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.113.1">now divide the text of a small piece of </span><em class="italic"><span class="kobospan" id="kobo.114.1">The Adventures of Sherlock Holmes</span></em><span class="kobospan" id="kobo.115.1">, outputting a list of sentences. </span><span class="kobospan" id="kobo.115.2">(Reference notebook: </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.116.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb</span></a><span class="kobospan" id="kobo.117.1">.) Here, we assume that you</span><a id="_idIndexMarker006" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.118.1"> are running the notebook, so the paths are all relative to the </span><span><span class="kobospan" id="kobo.119.1">notebook location:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.120.1">Import the file utility functions from the </span><strong class="source-inline1"><span class="kobospan" id="kobo.121.1">util</span></strong> <span><span class="kobospan" id="kobo.122.1">folder (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.123.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb</span></span></a><span><span class="kobospan" id="kobo.124.1">):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.125.1">
%run -i "../util/file_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.126.1">Read in the book </span><span><span class="kobospan" id="kobo.127.1">part text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.128.1">
sherlock_holmes_part_of_text = read_text_file("../data/sherlock_holmes_1.txt")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.129.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.130.1">read_text_file</span></strong><span class="kobospan" id="kobo.131.1"> function is located in the </span><strong class="source-inline"><span class="kobospan" id="kobo.132.1">util</span></strong><span class="kobospan" id="kobo.133.1"> notebook we imported previously. </span><span class="kobospan" id="kobo.133.2">Here is its </span><span><span class="kobospan" id="kobo.134.1">source code:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.135.1">def read_text_file(filename):
    file = open(filename, "r", encoding="utf-8")
    return file.read()</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.136.1">Print out the resulting text to make sure everything worked correctly and the </span><span><span class="kobospan" id="kobo.137.1">file loaded:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.138.1">
print(sherlock_holmes_part_of_text)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.139.1">The beginning of the printout will look </span><span><span class="kobospan" id="kobo.140.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.141.1">To Sherlock Holmes she is always _the_ woman. </span><span class="kobospan1" id="kobo.141.2">I have seldom heard him
mention her under any other name. </span><span class="kobospan1" id="kobo.141.3">In his eyes she eclipses and
predominates the whole of her sex…</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.142.1">Import </span><a id="_idIndexMarker007" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.143.1">the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.144.1">nltk</span></strong></span><span><span class="kobospan" id="kobo.145.1"> package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.146.1">
import nltk</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.147.1">If this is the </span><a id="_idIndexMarker008" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.148.1">first time you are running the code, you will need to download tokenizer data. </span><span class="kobospan" id="kobo.148.2">You will not need to run this command </span><span><span class="kobospan" id="kobo.149.1">after that:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.150.1">
nltk.download('punkt')</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.151.1">Initialize </span><span><span class="kobospan" id="kobo.152.1">the tokenizer:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.153.1">
tokenizer = nltk.data.load("tokenizers/punkt/english.pickle")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.154.1">Divide the text into sentences using the tokenizer. </span><span class="kobospan" id="kobo.154.2">The result will be a list </span><span><span class="kobospan" id="kobo.155.1">of sentences:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.156.1">
sentences_nltk = tokenizer.tokenize(
    sherlock_holmes_part_of_text)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.157.1">Print </span><span><span class="kobospan" id="kobo.158.1">the result:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.159.1">
print(sentences_nltk)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.160.1">It should look like this. </span><span class="kobospan" id="kobo.160.2">There are newlines inside the sentences that come from the book formatting. </span><span class="kobospan" id="kobo.160.3">They are not necessarily </span><span><span class="kobospan" id="kobo.161.1">sentence endings:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.162.1">['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\nmention her under any other name.', 'In his eyes she eclipses and\npredominates the whole of her sex.', 'It was not that he felt any emotion\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\nwas, I take it, the most perfect reasoning and observing machine that\nthe world has seen, but as a lover he would have placed himself in a\nfalse position.', 'He never spoke of the softer passions, save with a gibe\nand a sneer.', 'They were admirable things for the observer—excellent for\ndrawing the veil from men's motives and actions.', 'But for the trained\nreasoner to admit such intrusions into his own delicate and finely\nadjusted temperament was to introduce a distracting factor which might\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\ninstrument, or a crack in one of his own high-power lenses, would not\nbe more disturbing than a strong emotion in a nature such as his.', 'And\nyet there was but one woman to him, and that woman was the late Irene\nAdler, of dubious and questionable memory.']</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.163.1">Print the</span><a id="_idIndexMarker009" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.164.1"> number of sentences in the result; there</span><a id="_idIndexMarker010" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.165.1"> should be 11 sentences </span><span><span class="kobospan" id="kobo.166.1">in total:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.167.1">
print(len(sentences_nltk))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.168.1">This gives </span><span><span class="kobospan" id="kobo.169.1">the result:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.170.1">11</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.171.1">Although it might seem straightforward to divide a text into sentences by just using a regular expression to split it at the periods, in reality, it is more complicated. </span><span class="kobospan" id="kobo.171.2">We use periods in places other than ends of sentences; for example, after abbreviations – for example, “Dr. </span><span class="kobospan" id="kobo.171.3">Smith will see you now.” </span><span class="kobospan" id="kobo.171.4">Similarly, while all sentences in English start with a capital letter, we also use capital letters for proper names. </span><span class="kobospan" id="kobo.171.5">The approach used in </span><strong class="source-inline"><span class="kobospan" id="kobo.172.1">nltk</span></strong><span class="kobospan" id="kobo.173.1"> takes all these points into consideration; it is an implementation of an unsupervised algorithm presented </span><span><span class="kobospan" id="kobo.174.1">in </span></span><a href="https://aclanthology.org/J06-4003.pdf" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.175.1">https://aclanthology.org/J06-4003.pdf</span></span></a><span><span class="kobospan" id="kobo.176.1">.</span></span></p>
			<h2 id="_idParaDest-20" class="calibre5"><a id="_idTextAnchor020" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.177.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.178.1">We can also use a different strategy to parse the text into sentences, employing the other very</span><a id="_idIndexMarker011" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.179.1"> popular NLP package, </span><strong class="bold"><span class="kobospan" id="kobo.180.1">spaCy</span></strong><span class="kobospan" id="kobo.181.1">. </span><span class="kobospan" id="kobo.181.2">Here is how </span><span><span class="kobospan" id="kobo.182.1">it works:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.183.1">Import the </span><span><span class="kobospan" id="kobo.184.1">spaCy package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.185.1">
import spacy</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.186.1">The first time you run the notebook, you will need to download a spaCy model.  </span><span class="kobospan" id="kobo.186.2">The model is trained on a large amount of English text and there are several tools that can be used with it, including the sentence tokenizer. </span><span class="kobospan" id="kobo.186.3">Here, I’m downloading the smallest model, but you might try other ones (</span><span><span class="kobospan" id="kobo.187.1">see </span></span><a href="https://spacy.io/usage/models/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.188.1">https://spacy.io/usage/models/</span></span></a><span><span class="kobospan" id="kobo.189.1">):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.190.1">
!python -m spacy download en_core_web_sm</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.191.1">Initialize the </span><span><span class="kobospan" id="kobo.192.1">spaCy engine:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.193.1">
nlp = spacy.load("en_core_web_sm")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.194.1">Process the text using the spaCy engine. </span><span class="kobospan" id="kobo.194.2">This line assumes that you have the </span><strong class="source-inline1"><span class="kobospan" id="kobo.195.1">sherlock_holmes_part_of_text</span></strong><span class="kobospan" id="kobo.196.1"> variable initialized. </span><span class="kobospan" id="kobo.196.2">If not, you need to run one of the earlier cells where the text is read into </span><span><span class="kobospan" id="kobo.197.1">this variable:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.198.1">
doc = nlp(sherlock_holmes_part_of_text)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.199.1">Get the sentences from the processed </span><strong class="source-inline1"><span class="kobospan" id="kobo.200.1">doc</span></strong><span class="kobospan" id="kobo.201.1"> object, and print the resulting array and </span><span><span class="kobospan" id="kobo.202.1">its length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.203.1">
sentences_spacy = [sentence.text for sentence in doc.sents]
print(sentences_spacy)
print(len(sentences_spacy))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.204.1">The result will look </span><span><span class="kobospan" id="kobo.205.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.206.1">['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\nmention her under any other name.', 'In his eyes she eclipses and\npredominates the whole of her sex.', 'It was not that he felt any emotion\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\nwas, I take it, the most perfect reasoning and observing machine that\nthe world has seen, but as a lover he would have placed himself in a\nfalse position.', 'He never spoke of the softer passions, save with a gibe\nand a sneer.', 'They were admirable things for the observer—excellent for\ndrawing the veil from men's motives and actions.', 'But for the trained\nreasoner to admit such intrusions into his own delicate and finely\nadjusted temperament was to introduce a distracting factor which might\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\ninstrument, or a crack in one of his own high-power lenses, would not\nbe more disturbing than a strong emotion in a nature such as his.', 'And\nyet there was but one woman to him, and that woman was the late Irene\nAdler, of dubious and questionable memory.']
11</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.207.1">An important</span><a id="_idIndexMarker012" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.208.1"> difference between spaCy and NLTK is the time it takes to complete the sentence-splitting process. </span><span class="kobospan" id="kobo.208.2">The reason for this is that spaCy loads a language model and uses several tools in addition to the tokenizer, while the NLTK tokenizer has only one function: to separate the text into sentences. </span><span class="kobospan" id="kobo.208.3">We can time the execution by using the </span><strong class="source-inline"><span class="kobospan" id="kobo.209.1">time</span></strong><span class="kobospan" id="kobo.210.1"> package and putting the code to split the sentences into the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.211.1">main</span></strong></span><span><span class="kobospan" id="kobo.212.1"> function:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.213.1">
import time
def split_into_sentences_nltk(text):
    sentences = tokenizer.tokenize(text)
    return sentences
def split_into_sentences_spacy(text):
    doc = nlp(text)
    sentences = [sentence.text for sentence in doc.sents]
    return sentences
start = time.time()
split_into_sentences_nltk(sherlock_holmes_part_of_text)
print(f"NLTK: {time.time() - start} s")
start = time.time()
split_into_sentences_spacy(sherlock_holmes_part_of_text)
print(f"spaCy: {time.time() - start} s")</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.214.1">The spaCy algorithm</span><a id="_idIndexMarker013" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.215.1"> takes 0.019 seconds, while the NLTK algorithm takes 0.0002. </span><span class="kobospan" id="kobo.215.2">The time is calculated by subtracting the current time (</span><strong class="source-inline"><span class="kobospan" id="kobo.216.1">time.time()</span></strong><span class="kobospan" id="kobo.217.1">) from the start time that is set at the beginning of the code block. </span><span class="kobospan" id="kobo.217.2">It is possible that you will get slightly </span><span><span class="kobospan" id="kobo.218.1">different values.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.219.1">The reason why you might use spaCy is if you are doing other processing with the package along with splitting it into sentences. </span><span class="kobospan" id="kobo.219.2">The spaCy processor does many other things, and that is why it takes longer. </span><span class="kobospan" id="kobo.219.3">If you are using other features of spaCy, there is no reason to use NLTK just for sentence splitting, and it’s better to employ spaCy for the </span><span><span class="kobospan" id="kobo.220.1">whole pipeline.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.221.1">It is also possible to use </span><a id="_idIndexMarker014" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.222.1">only the tokenizer without other tools from spaCy. </span><span class="kobospan" id="kobo.222.2">Please see their documentation for more </span><span><span class="kobospan" id="kobo.223.1">information: </span></span><a href="https://spacy.io/usage/processing-pipelines" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.224.1">https://spacy.io/usage/processing-pipelines</span></span></a><span><span class="kobospan" id="kobo.225.1">.</span></span></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.226.1">Important note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.227.1">spaCy might be slower, but it is doing many more things in the background, and if you are using its other features, use it for sentence splitting </span><span><span class="kobospan" id="kobo.228.1">as well.</span></span></p>
			<h2 id="_idParaDest-21" class="calibre5"><a id="_idTextAnchor021" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.229.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.230.1">You can use NLTK and spaCy to divide texts in languages other than English. </span><span class="kobospan" id="kobo.230.2">NLTK includes tokenizer models for Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Italian, Norwegian, Polish, Portuguese, Slovene, Spanish, Swedish, and Turkish. </span><span class="kobospan" id="kobo.230.3">In order to load those models, use the name of the language followed by the </span><strong class="source-inline"><span class="kobospan" id="kobo.231.1">.</span></strong><span><strong class="source-inline"><span class="kobospan" id="kobo.232.1">pickle</span></strong></span><span><span class="kobospan" id="kobo.233.1"> extension:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.234.1">
tokenizer = nltk.data.load("tokenizers/punkt/spanish.pickle")</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.235.1">See the NLTK documentation</span><a id="_idIndexMarker015" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.236.1"> to find out </span><span><span class="kobospan" id="kobo.237.1">more: </span></span><a href="https://www.nltk.org/index.html" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.238.1">https://www.nltk.org/index.html</span></span></a><span><span class="kobospan" id="kobo.239.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.240.1">Likewise, spaCy has models for other languages: Chinese, Dutch, English, French, German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. </span><span class="kobospan" id="kobo.240.2">These models are trained on text in those languages. </span><span class="kobospan" id="kobo.240.3">In order to use those models, you would have to download them separately. </span><span class="kobospan" id="kobo.240.4">For example, for Spanish, use this command to download </span><span><span class="kobospan" id="kobo.241.1">the model:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.242.1">
python -m spacy download es_core_news_sm</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.243.1">Then, put this line in the code to </span><span><span class="kobospan" id="kobo.244.1">use it:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.245.1">
nlp = spacy.load("es_core_news_sm")</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.246.1">See the spaCy documentation to find out </span><span><span class="kobospan" id="kobo.247.1">more: </span></span><a href="https://spacy.io/usage/models" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.248.1">https://spacy.io/usage/models</span></span></a><span><span class="kobospan" id="kobo.249.1">.</span></span></p>
			<h1 id="_idParaDest-22" class="calibre7"><a id="_idTextAnchor022" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.250.1">Dividing sentences into words – tokenization</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.251.1">In many </span><a id="_idIndexMarker016" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.252.1">instances, we</span><a id="_idIndexMarker017" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.253.1"> rely on individual words when we do NLP tasks. </span><span class="kobospan" id="kobo.253.2">This happens, for example, when we build semantic models of</span><a id="_idIndexMarker018" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.254.1"> texts by relying on the semantics – of individual words, or when we are looking for words with a specific part of speech. </span><span class="kobospan" id="kobo.254.2">To divide text into words, we can use NLTK and spaCy to do this task </span><span><span class="kobospan" id="kobo.255.1">for us.</span></span></p>
			<h2 id="_idParaDest-23" class="calibre5"><a id="_idTextAnchor023" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.256.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.257.1">For this part, we will</span><a id="_idIndexMarker019" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.258.1"> be using the same text of the book </span><em class="italic"><span class="kobospan" id="kobo.259.1">The Adventures of Sherlock Holmes</span></em><span class="kobospan" id="kobo.260.1">. </span><span class="kobospan" id="kobo.260.2">You can find the whole text in the book’s GitHub repository. </span><span class="kobospan" id="kobo.260.3">For this recipe, we will need just the beginning of the book, which can be found in the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.261.1">sherlock_holmes_1.txt</span></strong></span><span><span class="kobospan" id="kobo.262.1"> file.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.263.1">In order to do this task, you will need the NLTK and spaCy packages, which are part of the Poetry file. </span><span class="kobospan" id="kobo.263.2">Directions to install Poetry are described in the </span><em class="italic"><span class="kobospan" id="kobo.264.1">Technical </span></em><span><em class="italic"><span class="kobospan" id="kobo.265.1">requirements</span></em></span><span><span class="kobospan" id="kobo.266.1"> section.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.267.1">(Notebook </span><span><span class="kobospan" id="kobo.268.1">reference: </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.269.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb</span></span></a><span><span class="kobospan" id="kobo.270.1">.)</span></span></p>
			<h2 id="_idParaDest-24" class="calibre5"><a id="_idTextAnchor024" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.271.1">How to do it</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.272.1">The process is </span><span><span class="kobospan" id="kobo.273.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.274.1">Import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.275.1">file_utils</span></strong><span class="kobospan" id="kobo.276.1"> notebook. </span><span class="kobospan" id="kobo.276.2">Effectively, we run the </span><strong class="source-inline1"><span class="kobospan" id="kobo.277.1">file_utils</span></strong><span class="kobospan" id="kobo.278.1"> notebook inside this one so we have access to its defined functions </span><span><span class="kobospan" id="kobo.279.1">and variables:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.280.1">
%run -i "../util/file_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.281.1">Read in the book </span><span><span class="kobospan" id="kobo.282.1">snippet text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.283.1">
sherlock_holmes_part_of_text = read_text_file("../data/sherlock_holmes_1.txt")
print(sherlock_holmes_part_of_text)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.284.1">The result should look </span><span><span class="kobospan" id="kobo.285.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.286.1">To Sherlock Holmes she is always _the_ woman. </span><span class="kobospan1" id="kobo.286.2">I have seldom heard him
mention her under any other name. </span><span class="kobospan1" id="kobo.286.3">In his eyes she eclipses and
predominates the whole of her sex... </span><span class="kobospan1" id="kobo.286.4">[Output truncated]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.287.1">Import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.288.1">nltk</span></strong></span><span><span class="kobospan" id="kobo.289.1"> package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.290.1">
import nltk</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.291.1">Divide </span><a id="_idIndexMarker020" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.292.1">the input into words. </span><span class="kobospan" id="kobo.292.2">Here, we use the NLTK </span><a id="_idIndexMarker021" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.293.1">word tokenizer to split the text into individual </span><a id="_idIndexMarker022" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.294.1">words. </span><span class="kobospan" id="kobo.294.2">The output of the function is a Python list of </span><span><span class="kobospan" id="kobo.295.1">the words:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.296.1">
words = nltk.tokenize.word_tokenize(
    sherlock_holmes_part_of_text)
print(words)
print(len(words))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.297.1">The output will be the list of words in the text and the length of the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.298.1">words</span></strong></span><span><span class="kobospan" id="kobo.299.1"> list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.300.1">
['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer—excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', ''', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']
230</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.301.1">The output</span><a id="_idIndexMarker023" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.302.1"> is a list, where</span><a id="_idIndexMarker024" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.303.1"> each token is either a word or a punctuation mark. </span><span class="kobospan" id="kobo.303.2">The NLTK tokenizer uses a set of rules to split the text into words. </span><span class="kobospan" id="kobo.303.3">It splits but </span><a id="_idIndexMarker025" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.304.1">does not expand contractions, such as </span><em class="italic"><span class="kobospan" id="kobo.305.1">don’t → do n’t</span></em><span class="kobospan" id="kobo.306.1"> and </span><em class="italic"><span class="kobospan" id="kobo.307.1">men’s → men ’s</span></em><span class="kobospan" id="kobo.308.1">, as in the preceding example. </span><span class="kobospan" id="kobo.308.2">It treats punctuation and quotes as separate tokens, so the result includes words with no </span><span><span class="kobospan" id="kobo.309.1">other marks.</span></span></p>
			<h2 id="_idParaDest-25" class="calibre5"><a id="_idTextAnchor025" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.310.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.311.1">Sometimes, it is useful </span><a id="_idIndexMarker026" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.312.1">not to split some words and use them as one unit. </span><span class="kobospan" id="kobo.312.2">One example of this is in </span><a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><span><em class="italic"><span class="kobospan" id="kobo.313.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.314.1">, in the </span><em class="italic"><span class="kobospan" id="kobo.315.1">Representing phrases – phrase2vec</span></em><span class="kobospan" id="kobo.316.1"> recipe, where we store phrases and not just individual words. </span><span class="kobospan" id="kobo.316.2">The NLTK package allows</span><a id="_idIndexMarker027" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.317.1"> us to do that using its custom </span><span><span class="kobospan" id="kobo.318.1">tokenizer, </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.319.1">MWETokenizer</span></strong></span><span><span class="kobospan" id="kobo.320.1">:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.321.1">Import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.322.1">MWETokenizer</span></strong></span><span><span class="kobospan" id="kobo.323.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.324.1">
from nltk.tokenize import MWETokenizer</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.325.1">Initialize the tokenizer and indicate that the words </span><strong class="source-inline1"><span class="kobospan" id="kobo.326.1">dim sum dinner</span></strong><span class="kobospan" id="kobo.327.1"> should not </span><span><span class="kobospan" id="kobo.328.1">be split:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.329.1">
tokenizer = MWETokenizer([('dim', 'sum', 'dinner')])</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.330.1">Add more words that should be </span><span><span class="kobospan" id="kobo.331.1">kept together:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.332.1">
tokenizer.add_mwe(('best', 'dim', 'sum'))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.333.1">Use the tokenizer to split </span><span><span class="kobospan" id="kobo.334.1">a sentence:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.335.1">
tokens = tokenizer.tokenize('Last night I went for dinner in an Italian restaurant. </span><span class="kobospan1" id="kobo.335.2">The pasta was delicious.'.split())
print(tokens)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.336.1">The result will contain the tokens split the same way </span><span><span class="kobospan" id="kobo.337.1">as previously:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.338.1">['Last', 'night', 'I', 'went', 'for', 'dinner', 'in', 'an', 'Italian', 'restaurant.', 'The', 'pasta', 'was', 'delicious.']</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.339.1">Split a </span><span><span class="kobospan" id="kobo.340.1">different sentence:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.341.1">
tokens = tokenizer.tokenize('I went out to a dim sum dinner last night. </span><span class="kobospan1" id="kobo.341.2">This restaurant has the best dim sum in town.'.split())
print(tokens)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.342.1">In this case, the tokenizer will put the phrases together into one unit and insert underscores instead </span><span><span class="kobospan" id="kobo.343.1">of spaces:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.344.1">['I', 'went', 'out', 'to', 'a', 'dim_sum_dinner', 'last', 'night.', 'This', 'restaurant', 'has', 'the_best_dim_sum', 'in', 'town.']</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.345.1">We can also use</span><a id="_idIndexMarker028" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.346.1"> spaCy to do the tokenization. </span><span class="kobospan" id="kobo.346.2">Word tokenization is one task in a larger array of tasks that spaCy accomplishes while </span><span><span class="kobospan" id="kobo.347.1">processing text.</span></span></p>
			<h2 id="_idParaDest-26" class="calibre5"><a id="_idTextAnchor026" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.348.1">There's still more</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.349.1">If</span><a id="_idIndexMarker029" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.350.1"> you are doing further processing on the text, it makes sense to use spaCy. </span><span class="kobospan" id="kobo.350.2">Here is how </span><span><span class="kobospan" id="kobo.351.1">it works:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.352.1">Import the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.353.1">spacy</span></strong></span><span><span class="kobospan" id="kobo.354.1"> package:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.355.1">
import spacy</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.356.1">Execute this command only if you have </span><span><span class="kobospan" id="kobo.357.1">not before:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.358.1">
!python -m spacy download en_core_web_sm</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.359.1">Initialize the spaCy engine using the </span><span><span class="kobospan" id="kobo.360.1">English model:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.361.1">
nlp = spacy.load("en_core_web_sm")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.362.1">Divide the text </span><span><span class="kobospan" id="kobo.363.1">into sentences:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.364.1">
doc = nlp(sherlock_holmes_part_of_text)
words = [token.text for token in doc]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.365.1">Print </span><span><span class="kobospan" id="kobo.366.1">the result:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.367.1">
print(words)
print(len(words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.368.1">The output will be </span><span><span class="kobospan" id="kobo.369.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.370.1">['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', '\n', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', '\n', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', '\n', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', '\n', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', '\n', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', '\n', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', '\n', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', '\n', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer', '—', 'excellent', 'for', '\n', 'drawing', 'the', 'veil', 'from', 'men', ''s', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', '\n', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', '\n', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', '\n', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', '\n', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', '-', 'power', 'lenses', ',', 'would', 'not', '\n', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', '\n', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', '\n', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']
251</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.371.1">You will notice that the length of the word list is longer when using spaCy than NLTK. </span><span class="kobospan" id="kobo.371.2">One of the reasons </span><a id="_idIndexMarker030" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.372.1">is that spaCy keeps the newlines, and each newline is a separate token. </span><span class="kobospan" id="kobo.372.2">The other </span><a id="_idIndexMarker031" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.373.1">difference is that spaCy splits words with a dash, such as </span><em class="italic"><span class="kobospan" id="kobo.374.1">high-power</span></em><span class="kobospan" id="kobo.375.1">. </span><span class="kobospan" id="kobo.375.2">You can find the exact difference between the two lists by running the </span><span><span class="kobospan" id="kobo.376.1">following line:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.377.1">
print(set(words_spacy)-set(words_nltk))</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.378.1">This should result in the </span><span><span class="kobospan" id="kobo.379.1">following output:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.380.1">
{'high', 'power', 'observer', '-', '_', '—', 'excellent', ''s', '\n'}</span></pre>			<p class="callout-heading"><span class="kobospan" id="kobo.381.1">Important note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.382.1">If you are doing other processing with spaCy, it makes sense to use it. </span><span class="kobospan" id="kobo.382.2">Otherwise, NLTK word tokenization </span><span><span class="kobospan" id="kobo.383.1">is sufficient.</span></span></p>
			<h2 id="_idParaDest-27" class="calibre5"><a id="_idTextAnchor027" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.384.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.385.1">The NLTK package only has word tokenization </span><span><span class="kobospan" id="kobo.386.1">for English.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.387.1">spaCy has models for other languages: Chinese, Dutch, English, French, German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. </span><span class="kobospan" id="kobo.387.2">In order to use those models, you would have to download them separately. </span><span class="kobospan" id="kobo.387.3">For example, for Spanish, use this command to download </span><span><span class="kobospan" id="kobo.388.1">the model:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.389.1">
python -m spacy download es_core_news_sm</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.390.1">Then, put this line in the code to </span><span><span class="kobospan" id="kobo.391.1">use it:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.392.1">
nlp = spacy.load("es_core_news_sm")</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.393.1">See the spaCy documentation to find out </span><span><span class="kobospan" id="kobo.394.1">more: </span></span><a href="https://spacy.io/usage/models" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.395.1">https://spacy.io/usage/models</span></span></a><span><span class="kobospan" id="kobo.396.1">.</span></span></p>
			<div class="calibre9"/><h1 id="_idParaDest-28" class="calibre7"><a id="_idTextAnchor028" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.397.1">Part of speech tagging</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.398.1">In many cases, NLP processing </span><a id="_idIndexMarker032" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.399.1">depends on determining the parts of speech of the words in the text. </span><span class="kobospan" id="kobo.399.2">For example, when we want to find out the named entities that appear in a text, we need to know the parts of speech of the words. </span><span class="kobospan" id="kobo.399.3">In this recipe, we will again consider NLTK and </span><span><span class="kobospan" id="kobo.400.1">spaCy algorithms.</span></span></p>
			<h2 id="_idParaDest-29" class="calibre5"><a id="_idTextAnchor029" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.401.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.402.1">For this part, we will be using the same text of the book </span><em class="italic"><span class="kobospan" id="kobo.403.1">The Adventures of Sherlock Holmes</span></em><span class="kobospan" id="kobo.404.1">. </span><span class="kobospan" id="kobo.404.2">You can find the whole text in the book’s Github repository. </span><span class="kobospan" id="kobo.404.3">For this recipe, we will need just the beginning of the book, which can be found in the file </span><span><span class="kobospan" id="kobo.405.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.406.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</span></span></a><span><span class="kobospan" id="kobo.407.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.408.1">In order to do this task, you will need the NLTK and spaCy packages, described in the </span><em class="italic"><span class="kobospan" id="kobo.409.1">Technical </span></em><span><em class="italic"><span class="kobospan" id="kobo.410.1">requirements</span></em></span><span><span class="kobospan" id="kobo.411.1"> section.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.412.1">We will also complete this task using the OpenAI API’s GPT model to demonstrate that it can complete it as well as spaCy and NLTK. </span><span class="kobospan" id="kobo.412.2">For this part to run, you will need the </span><strong class="source-inline"><span class="kobospan" id="kobo.413.1">openai</span></strong><span class="kobospan" id="kobo.414.1"> package, which is included in the Poetry environment. </span><span class="kobospan" id="kobo.414.2">You will also need your own OpenAI </span><span><span class="kobospan" id="kobo.415.1">API key.</span></span></p>
			<h2 id="_idParaDest-30" class="calibre5"><a id="_idTextAnchor030" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.416.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.417.1">In this recipe, we will use the spaCy package to label words with their parts </span><span><span class="kobospan" id="kobo.418.1">of speech.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.419.1">The process is </span><span><span class="kobospan" id="kobo.420.1">as follows:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.421.1">Import the </span><strong class="source-inline1"><span class="kobospan" id="kobo.422.1">util</span></strong><span class="kobospan" id="kobo.423.1"> file and the language </span><strong class="source-inline1"><span class="kobospan" id="kobo.424.1">util</span></strong><span class="kobospan" id="kobo.425.1"> file. </span><span class="kobospan" id="kobo.425.2">The language </span><strong class="source-inline1"><span class="kobospan" id="kobo.426.1">util</span></strong><span class="kobospan" id="kobo.427.1"> file contains an import of spaCy and NLTK, as well as an initialization of the small spaCy model into the </span><strong class="source-inline1"><span class="kobospan" id="kobo.428.1">small_model</span></strong><span class="kobospan" id="kobo.429.1"> object. </span><span class="kobospan" id="kobo.429.2">These files also include functions to read in text from a file and tokenization functions using spaCy </span><span><span class="kobospan" id="kobo.430.1">and NLTK:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.431.1">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.432.1">We will define the function that will output parts of speech for every word. </span><span class="kobospan" id="kobo.432.2">In this function, we first process the input text using the spaCy model that results in a </span><strong class="source-inline1"><span class="kobospan" id="kobo.433.1">Document</span></strong><span class="kobospan" id="kobo.434.1"> object. </span><span class="kobospan" id="kobo.434.2">The resulting </span><strong class="source-inline1"><span class="kobospan" id="kobo.435.1">Document</span></strong><span class="kobospan" id="kobo.436.1"> object contains an iterator with </span><strong class="source-inline1"><span class="kobospan" id="kobo.437.1">Token</span></strong><span class="kobospan" id="kobo.438.1"> objects, and each </span><strong class="source-inline1"><span class="kobospan" id="kobo.439.1">Token</span></strong><span class="kobospan" id="kobo.440.1"> object has information about parts </span><span><span class="kobospan" id="kobo.441.1">of speech.</span></span><p class="calibre3"><span class="kobospan" id="kobo.442.1">We use</span><a id="_idIndexMarker033" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.443.1"> this information to create the two lists, one with words and the other one with their respective parts </span><span><span class="kobospan" id="kobo.444.1">of speech.</span></span></p><p class="calibre3"><span class="kobospan" id="kobo.445.1">Finally, we zip the two lists to pair the words with the parts of speech and return the resulting list of tuples. </span><span class="kobospan" id="kobo.445.2">We do this in order to easily print the whole list with their corresponding parts of speech. </span><span class="kobospan" id="kobo.445.3">When you use part of speech tagging in your code, you can just iterate through the list </span><span><span class="kobospan" id="kobo.446.1">of tokens:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.447.1">
def pos_tag_spacy(text, model):
    doc = model(text)
    words = [token.text for token in doc]
    pos = [token.pos_ for token in doc]
    return list(zip(words, pos))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.448.1">Read in </span><span><span class="kobospan" id="kobo.449.1">the text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.450.1">
text = read_text_file("../data/sherlock_holmes_1.txt")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.451.1">Run the preceding function using the text and the model </span><span><span class="kobospan" id="kobo.452.1">as input:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.453.1">
words_with_pos = pos_tag_spacy(text, small_model)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.454.1">Print </span><span><span class="kobospan" id="kobo.455.1">the output:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.456.1">
print(words_with_pos)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.457.1">Part of the result is shown in the following; for the complete output, please see the Jupyter </span><span><span class="kobospan" id="kobo.458.1">notebook (</span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.459.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb</span></span></a><span><span class="kobospan" id="kobo.460.1">):</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.461.1">[('To', 'ADP'),
 ('Sherlock', 'PROPN'),
 ('Holmes', 'PROPN'),
 ('she', 'PRON'),
 ('is', 'AUX'),
 ('always', 'ADV'),
 ('_', 'PUNCT'),
 ('the', 'DET'),
 ('_', 'PROPN'),
 ('woman', 'NOUN'),
 ('.', 'PUNCT'),
 ('I', 'PRON'),
 ('have', 'AUX'),
 ('seldom', 'ADV'),
 ('heard', 'VERB'),
 ('him', 'PRON'),
 ('\n', 'SPACE'),
 ('mention', 'VERB'),
 ('her', 'PRON'),
 ('under', 'ADP'),
 ('any', 'DET'),
 ('other', 'ADJ'),
 ('name', 'NOUN'),
 ('.', 'PUNCT'),…</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.462.1">The resulting list </span><a id="_idIndexMarker034" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.463.1">contains tuples of words and parts of speech. </span><span class="kobospan" id="kobo.463.2">The list of part of speech tags is available </span><span><span class="kobospan" id="kobo.464.1">here: </span></span><a href="https://universaldependencies.org/u/pos/" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.465.1">https://universaldependencies.org/u/pos/</span></span></a><span><span class="kobospan" id="kobo.466.1">.</span></span></p>
			<h2 id="_idParaDest-31" class="calibre5"><a id="_idTextAnchor031" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.467.1">There’s more</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.468.1">We can compare spaCy’s performance to NLTK in this task. </span><span class="kobospan" id="kobo.468.2">Here are the steps for getting the parts of speech </span><span><span class="kobospan" id="kobo.469.1">with NLTK:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.470.1">The imports have been taken care of in the language </span><strong class="source-inline1"><span class="kobospan" id="kobo.471.1">util</span></strong><span class="kobospan" id="kobo.472.1"> file that we imported, so the first thing we do is create a function that outputs parts of speech for the words that are input. </span><span class="kobospan" id="kobo.472.2">In it, we utilize the </span><strong class="source-inline1"><span class="kobospan" id="kobo.473.1">word_tokenize_nltk</span></strong><span class="kobospan" id="kobo.474.1"> function that is also imported from the language </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.475.1">util</span></strong></span><span><span class="kobospan" id="kobo.476.1"> notebook:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.477.1">
def pos_tag_nltk(text):
    words = word_tokenize_nltk(text)
    words_with_pos = nltk.pos_tag(words)
    return words_with_pos</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.478.1">Next, we apply the function to the text that we read </span><span><span class="kobospan" id="kobo.479.1">in previously:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.480.1">
words_with_pos = pos_tag_nltk(text)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.481.1">Print out </span><span><span class="kobospan" id="kobo.482.1">the result:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.483.1">
print(words_with_pos)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.484.1">Part of the output is shown in the following. </span><span class="kobospan" id="kobo.484.2">For the complete output, please see the </span><span><span class="kobospan" id="kobo.485.1">Jupyter notebook:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.486.1">[('To', 'TO'),
 ('Sherlock', 'NNP'),
 ('Holmes', 'NNP'),
 ('she', 'PRP'),
 ('is', 'VBZ'),
 ('always', 'RB'),
 ('_the_', 'JJ'),
 ('woman', 'NN'),
 ('.', '.'),
 ('I', 'PRP'),
 ('have', 'VBP'),
 ('seldom', 'VBN'),
 ('heard', 'RB'),
 ('him', 'PRP'),
 ('mention', 'VB'),
 ('her', 'PRP'),
 ('under', 'IN'),
 ('any', 'DT'),
 ('other', 'JJ'),
 ('name', 'NN'),
 ('.', '.'),…</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.487.1">The list of part of speech </span><a id="_idIndexMarker035" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.488.1">tags that NLTK uses is different from what SpaCy uses, and can be accessed by running the </span><span><span class="kobospan" id="kobo.489.1">following commands:</span></span></p>
			<pre class="console"><span class="kobospan1" id="kobo.490.1">
python
&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download('tagsets')
&gt;&gt;&gt; nltk.help.upenn_tagset()</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.491.1">Comparing the performance, we see that spaCy takes 0.02 seconds, while NLTK takes 0.01 seconds (your numbers might be different), so their performance is similar, with NLTK being a</span><a id="_idIndexMarker036" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.492.1"> little better. </span><span class="kobospan" id="kobo.492.2">However, the part of speech information is already available in the spaCy objects after the initial processing has been done, so if you are doing any further processing, spaCy is a </span><span><span class="kobospan" id="kobo.493.1">better choice.</span></span></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.494.1">Important note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.495.1">spaCy does all of its processing at once, and the results are stored in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.496.1">Doc</span></strong><span class="kobospan" id="kobo.497.1"> object. </span><span class="kobospan" id="kobo.497.2">The part of speech information is available by iterating through </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.498.1">Token</span></strong></span><span><span class="kobospan" id="kobo.499.1"> objects.</span></span></p>
			<h2 id="_idParaDest-32" class="calibre5"><a id="_idTextAnchor032" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.500.1">There’s more</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.501.1">We can </span><a id="_idIndexMarker037" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.502.1">use the OpenAI API with the GPT-3.5 and GPT-4 models to perform various tasks, including many NLP ones. </span><span class="kobospan" id="kobo.502.2">Here, we show how to use the OpenAI API to get NLTK-style parts of speech for input text. </span><span class="kobospan" id="kobo.502.3">You can also specify in the prompt the output format and the style of the part of speech tags. </span><span class="kobospan" id="kobo.502.4">For this code to run correctly, you will need your own OpenAI </span><span><span class="kobospan" id="kobo.503.1">API key:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.504.1">Import </span><strong class="source-inline1"><span class="kobospan" id="kobo.505.1">openai</span></strong><span class="kobospan" id="kobo.506.1"> and create the OpenAI client using your API key. </span><span class="kobospan" id="kobo.506.2">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.507.1">OPEN_AI_KEY</span></strong><span class="kobospan" id="kobo.508.1"> constant variable is set in the </span><strong class="source-inline1"><span class="kobospan" id="kobo.509.1">../</span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.510.1">util/file_utils.ipynb</span></strong></span><span><span class="kobospan" id="kobo.511.1"> file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.512.1">
from openai import OpenAI
client = OpenAI(api_key=OPEN_AI_KEY)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.513.1">Set </span><span><span class="kobospan" id="kobo.514.1">the prompt:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.515.1">
prompt="""Decide what the part of speech tags are for a sentence.
</span><span class="kobospan1" id="kobo.515.2">Preserve original capitalization.
</span><span class="kobospan1" id="kobo.515.3">Return the list in the format of a python tuple: (word, part of speech).
</span><span class="kobospan1" id="kobo.515.4">Sentence: In his eyes she eclipses and predominates the whole of her sex."""</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.516.1">Send the request to the OpenAI API. </span><span class="kobospan" id="kobo.516.2">Some of the important parameters that we send to the </span><a id="_idIndexMarker038" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.517.1">API are the model we want to use, the temperature, which affects how much the response from the model will vary, and the maximum amount of tokens the model should return as </span><span><span class="kobospan" id="kobo.518.1">a completion:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.519.1">
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=256,
    top_p=1.0,
    frequency_penalty=0,
    presence_penalty=0,
    messages=[
        {"role": "system", 
         "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ],
)</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.520.1">Print </span><span><span class="kobospan" id="kobo.521.1">the response:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.522.1">
print(response)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.523.1">The output will look </span><span><span class="kobospan" id="kobo.524.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.525.1">ChatCompletion(id='chatcmpl-9hCq34UAzMiNiqNGopt2U8ZmZM5po', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are the part of speech tags for the sentence "In his eyes she eclipses and predominates the whole of her sex" in the format of a Python tuple:\n\n[(\'In\', \'IN\'), (\'his\', \'PRP$\'), (\'eyes\', \'NNS\'), (\'she\', \'PRP\'), (\'eclipses\', \'VBZ\'), (\'and\', \'CC\'), (\'predominates\', \'VBZ\'), (\'the\', \'DT\'), (\'whole\', \'JJ\'), (\'of\', \'IN\'), (\'her\', \'PRP$\'), (\'sex\', \'NN\')]', role='assistant', function_call=None, tool_calls=None))], created=1720084483, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=120, prompt_tokens=74, total_tokens=194))</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.526.1">To see</span><a id="_idIndexMarker039" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.527.1"> just the GPT output, do </span><span><span class="kobospan" id="kobo.528.1">the following:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.529.1">
print(response.choices[0].message.content)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.530.1">The output will be </span><span><span class="kobospan" id="kobo.531.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.532.1">Here are the part of speech tags for the sentence "In his eyes she eclipses and predominates the whole of her sex" in the format of a Python tuple:
[('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN')]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.533.1">We can use the </span><strong class="source-inline1"><span class="kobospan" id="kobo.534.1">literal_eval</span></strong><span class="kobospan" id="kobo.535.1"> function to transform the response into a tuple. </span><span class="kobospan" id="kobo.535.2">We request that the GPT model return only the answer without additional explanations so that there is no free text inside the answer and we can process it automatically. </span><span class="kobospan" id="kobo.535.3">We do this in order to be able to compare the output of the OpenAI API to the </span><span><span class="kobospan" id="kobo.536.1">NLTK output:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.537.1">
from ast import literal_eval
def pos_tag_gpt(text, client):
    prompt = f"""Decide what the part of speech tags are for a sentence.
</span><span class="kobospan1" id="kobo.537.2">    Preserve original capitalization.
</span><span class="kobospan1" id="kobo.537.3">    Return the list in the format of a python tuple: (word, part of speech).
</span><span class="kobospan1" id="kobo.537.4">    Do not include any other explanations.
</span><span class="kobospan1" id="kobo.537.5">    Sentence: {text}."""
</span><span class="kobospan1" id="kobo.537.6">    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0,
        max_tokens=256,
        top_p=1.0,
        frequency_penalty=0,
        presence_penalty=0,
        messages=[
            {"role": "system", 
             "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
    )
    result = response.choices[0].message.content
    result = result.replace("\n", "")
    result = list(literal_eval(result))
    return result</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.538.1">Now, let’s time </span><a id="_idIndexMarker040" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.539.1">the GPT function so we can compare its performance to the other methods we </span><span><span class="kobospan" id="kobo.540.1">used previously:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.541.1">
start = time.time()
first_sentence = "In his eyes she eclipses and predominates the whole of her sex."
</span><span class="kobospan1" id="kobo.541.2">words_with_pos = pos_tag_gpt(first_sentence, OPEN_AI_KEY)
print(words_with_pos)
print(f"GPT: {time.time() - start} s")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.542.1">The result will look </span><span><span class="kobospan" id="kobo.543.1">like this:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.544.1">[('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.')]
GPT: 2.4942469596862793 s</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.545.1">The output of GPT is very similar to NLTK, but </span><span><span class="kobospan" id="kobo.546.1">slightly different:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.547.1">
words_with_pos_nltk = pos_tag_nltk(first_sentence)
print(words_with_pos == words_with_pos_nltk)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.548.1">This outputs </span><span><span class="kobospan" id="kobo.549.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.550.1">False</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.551.1">The difference between GPT and NLTK is that GPT tags the word whole as an adjective and NLTK tags it as a noun. </span><span class="kobospan" id="kobo.551.2">In this context, NLTK </span><span><span class="kobospan" id="kobo.552.1">is correct.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.553.1">We see that the LLM outputs very similar results but is about 400 times slower </span><span><span class="kobospan" id="kobo.554.1">than NLTK.</span></span></p>
			<h2 id="_idParaDest-33" class="calibre5"><a id="_idTextAnchor033" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.555.1">See also</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.556.1">If you would like to tag a text in another language, you can do so by using spaCy’s models for other languages. </span><span class="kobospan" id="kobo.556.2">For example, we can load the Spanish spaCy model to run it on </span><span><span class="kobospan" id="kobo.557.1">Spanish text:</span></span></p>
			<pre class="source-code"><span class="kobospan1" id="kobo.558.1">
nlp = spacy.load("es_core_news_sm")</span></pre>			<p class="calibre3"><span class="kobospan" id="kobo.559.1">In the case that spaCy doesn’t have a model for the language you are working with, you can train your own model with spaCy. </span><span><span class="kobospan" id="kobo.560.1">See </span></span><a href="https://spacy.io/usage/training#tagger-parser" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.561.1">https://spacy.io/usage/training#tagger-parser</span></span></a><span><span class="kobospan" id="kobo.562.1">.</span></span></p>
			<h1 id="_idParaDest-34" class="calibre7"><a id="_idTextAnchor034" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.563.1">Combining similar words – lemmatization</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.564.1">We can find the canonical form of</span><a id="_idIndexMarker041" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.565.1"> the word using </span><strong class="bold"><span class="kobospan" id="kobo.566.1">lemmatization</span></strong><span class="kobospan" id="kobo.567.1">. </span><span class="kobospan" id="kobo.567.2">For</span><a id="_idIndexMarker042" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.568.1"> example, the lemma of the word </span><em class="italic"><span class="kobospan" id="kobo.569.1">cats</span></em><span class="kobospan" id="kobo.570.1"> is </span><em class="italic"><span class="kobospan" id="kobo.571.1">cat</span></em><span class="kobospan" id="kobo.572.1">, and the lemma for the word </span><em class="italic"><span class="kobospan" id="kobo.573.1">ran</span></em><span class="kobospan" id="kobo.574.1"> is </span><em class="italic"><span class="kobospan" id="kobo.575.1">run</span></em><span class="kobospan" id="kobo.576.1">. </span><span class="kobospan" id="kobo.576.2">This is useful when we are trying to match some word and don’t want to list out all the </span><a id="_idIndexMarker043" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.577.1">possible forms. </span><span class="kobospan" id="kobo.577.2">Instead, we can just use </span><span><span class="kobospan" id="kobo.578.1">its lemma.</span></span></p>
			<h2 id="_idParaDest-35" class="calibre5"><a id="_idTextAnchor035" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.579.1">Getting ready</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.580.1">We will be using the spaCy package for </span><span><span class="kobospan" id="kobo.581.1">this recipe.</span></span></p>
			<h2 id="_idParaDest-36" class="calibre5"><a id="_idTextAnchor036" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.582.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.583.1">When the spaCy model processes a piece of text, the resulting </span><strong class="source-inline"><span class="kobospan" id="kobo.584.1">Document</span></strong><span class="kobospan" id="kobo.585.1"> object contains an iterator over the </span><strong class="source-inline"><span class="kobospan" id="kobo.586.1">Token</span></strong><span class="kobospan" id="kobo.587.1"> objects within it, as we saw in the </span><em class="italic"><span class="kobospan" id="kobo.588.1">Part of speech tagging</span></em><span class="kobospan" id="kobo.589.1"> recipe. </span><span class="kobospan" id="kobo.589.2">These </span><strong class="source-inline"><span class="kobospan" id="kobo.590.1">Token</span></strong><span class="kobospan" id="kobo.591.1"> objects contain the lemma information for each word in </span><span><span class="kobospan" id="kobo.592.1">the text.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.593.1">Here are the steps for getting </span><span><span class="kobospan" id="kobo.594.1">the lemmas:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.595.1">Import the file and language </span><strong class="source-inline1"><span class="kobospan" id="kobo.596.1">utils</span></strong><span class="kobospan" id="kobo.597.1"> files. </span><span class="kobospan" id="kobo.597.2">This will import spaCy and initialize the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.598.1">small_model</span></strong></span><span><span class="kobospan" id="kobo.599.1"> object:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.600.1">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.601.1">Create a list </span><a id="_idIndexMarker044" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.602.1">of words we want </span><span><span class="kobospan" id="kobo.603.1">to lemmatize:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.604.1">
words = ["leaf", "leaves", "booking", "writing", "completed", "stemming"]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.605.1">Create a </span><strong class="source-inline1"><span class="kobospan" id="kobo.606.1">Document</span></strong><span class="kobospan" id="kobo.607.1"> object for each of </span><span><span class="kobospan" id="kobo.608.1">the words:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.609.1">
docs = [small_model(word) for word in words]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.610.1">Print the words and their lemmas for each of the words in </span><span><span class="kobospan" id="kobo.611.1">the list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.612.1">
for doc in docs:
    for token in doc:
        print(token, token.lemma_)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.613.1">The result will be </span><span><span class="kobospan" id="kobo.614.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.615.1">leaf leaf
leaves leave
booking book
writing write
completed complete
stemming stem</span></pre><p class="calibre3"><span class="kobospan" id="kobo.616.1">The result shows correct lemmatization for all words. </span><span class="kobospan" id="kobo.616.2">However, some words are ambiguous. </span><span class="kobospan" id="kobo.616.3">For example, the word </span><em class="italic"><span class="kobospan" id="kobo.617.1">leaves</span></em><span class="kobospan" id="kobo.618.1"> could either be a verb, in which case the lemma is correct, or a noun, in which case this is the wrong lemma. </span><span class="kobospan" id="kobo.618.2">If we give the spaCy continuous text instead of individual words, it is likely to correctly disambiguate </span><span><span class="kobospan" id="kobo.619.1">the words.</span></span></p></li>				<li class="calibre14"><span class="kobospan" id="kobo.620.1">Now, apply</span><a id="_idIndexMarker045" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.621.1"> lemmatization to the longer text. </span><span class="kobospan" id="kobo.621.2">Here, we read in a small portion of the </span><em class="italic"><span class="kobospan" id="kobo.622.1">Sherlock Holmes</span></em><span class="kobospan" id="kobo.623.1"> text and lemmatize its </span><span><span class="kobospan" id="kobo.624.1">every word:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.625.1">
Text = read_text_file(../data/sherlock_holmes_1.txt")
doc = small_model(text)
for token in doc:
    print(token, token.lemma_)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.626.1">The partial result will be </span><span><span class="kobospan" id="kobo.627.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.628.1">To to
Sherlock Sherlock
Holmes Holmes
she she
is be
always always
_ _
the the
_ _
woman woman
. </span><span class="kobospan1" id="kobo.628.2">….</span></pre></li>			</ol>
			<h2 id="_idParaDest-37" class="calibre5"><a id="_idTextAnchor037" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.629.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.630.1">We can use the spaCy lemmatizer object to find out whether a word is in its base form or not. </span><span class="kobospan" id="kobo.630.2">We might do this while manipulating the grammar of the sentence, for example, in the task of turning a passive sentence into an active one. </span><span class="kobospan" id="kobo.630.3">We can get to the lemmatizer object by manipulating the spaCy pipeline, which includes various tools that are applied to the text. </span><span class="kobospan" id="kobo.630.4">See </span><a href="https://spacy.io/usage/processing-pipelines/" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.631.1">https://spacy.io/usage/processing-pipelines/</span></a><span class="kobospan" id="kobo.632.1"> for more information. </span><span class="kobospan" id="kobo.632.2">Here are </span><span><span class="kobospan" id="kobo.633.1">the steps:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.634.1">The pipeline components are located in a list of tuples, </span><strong class="source-inline1"><span class="kobospan" id="kobo.635.1">(component name, component)</span></strong><span class="kobospan" id="kobo.636.1">. </span><span class="kobospan" id="kobo.636.2">To get the lemmatizer component, we need to loop through </span><span><span class="kobospan" id="kobo.637.1">this list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.638.1">
lemmatizer = None
for name, proc in small_model.pipeline:
    if name == "lemmatizer":
        lemmatizer = proc</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.639.1">Now, we can apply the </span><strong class="source-inline1"><span class="kobospan" id="kobo.640.1">is_base_form</span></strong><span class="kobospan" id="kobo.641.1"> function call to every word in the Sherlock </span><span><span class="kobospan" id="kobo.642.1">Holmes text:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.643.1">
for token in doc:
    print(f"{token} is in its base form: 
        {lemmatizer.is_base_form(token)}")</span></pre><p class="calibre3"><span class="kobospan" id="kobo.644.1">The partial result will be </span><span><span class="kobospan" id="kobo.645.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.646.1">To is in its base form: False
Sherlock is in its base form: False
Holmes is in its base form: False
she is in its base form: False
is is in its base form: False
always is in its base form: False
_ is in its base form: False
the is in its base form: False
_ is in its base form: False
woman is in its base form: True
. </span><span class="kobospan1" id="kobo.646.2">is in its base form: False…</span></pre></li>			</ol>
			<h1 id="_idParaDest-38" class="calibre7"><a id="_idTextAnchor038" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.647.1">Removing stopwords</span></h1>
			<p class="calibre3"><span class="kobospan" id="kobo.648.1">When we</span><a id="_idIndexMarker046" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.649.1"> work with words, especially if we are considering the words’ semantics, we sometimes need to exclude some very frequent words that do not bring any substantial meaning into the sentence (words such as </span><em class="italic"><span class="kobospan" id="kobo.650.1">but</span></em><span class="kobospan" id="kobo.651.1">, </span><em class="italic"><span class="kobospan" id="kobo.652.1">can</span></em><span class="kobospan" id="kobo.653.1">, </span><em class="italic"><span class="kobospan" id="kobo.654.1">we</span></em><span class="kobospan" id="kobo.655.1">, etc.). </span><span class="kobospan" id="kobo.655.2">For example, if we want to get a rough sense of the topic of a text, we could count its most frequent words. </span><span class="kobospan" id="kobo.655.3">However, in any text, the most frequent words will be stopwords, so we want to remove them before processing. </span><span class="kobospan" id="kobo.655.4">This recipe shows how to do that. </span><span class="kobospan" id="kobo.655.5">The stopwords list we are using in this recipe comes from the NLTK package and might not include all the words you need. </span><span class="kobospan" id="kobo.655.6">You will need to modify the </span><span><span class="kobospan" id="kobo.656.1">list accordingly.</span></span></p>
			<p class="calibre3"><span><span class="kobospan" id="kobo.657.1">Getting ready</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.658.1">We will remove stopwords using spaCy and NLTK; these packages are part of the Poetry environment that we </span><span><span class="kobospan" id="kobo.659.1">installed earlier.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.660.1">We will be using the </span><em class="italic"><span class="kobospan" id="kobo.661.1">Sherlock Holmes</span></em><span class="kobospan" id="kobo.662.1"> text referred to earlier. </span><span class="kobospan" id="kobo.662.2">For this recipe, we will need just the beginning of the book, which can be found in the file </span><span><span class="kobospan" id="kobo.663.1">at </span></span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1"><span><span class="kobospan" id="kobo.664.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</span></span></a><span><span class="kobospan" id="kobo.665.1">.</span></span></p>
			<p class="calibre3"><span class="kobospan" id="kobo.666.1">In </span><em class="italic"><span class="kobospan" id="kobo.667.1">step 1</span></em><span class="kobospan" id="kobo.668.1">, we run the utilities notebooks. </span><span class="kobospan" id="kobo.668.2">In </span><em class="italic"><span class="kobospan" id="kobo.669.1">step 2</span></em><span class="kobospan" id="kobo.670.1">, we import the </span><strong class="source-inline"><span class="kobospan" id="kobo.671.1">nltk</span></strong><span class="kobospan" id="kobo.672.1"> package and its stopwords list. </span><span class="kobospan" id="kobo.672.2">In </span><em class="italic"><span class="kobospan" id="kobo.673.1">step 3</span></em><span class="kobospan" id="kobo.674.1">, we download the stopwords data, if necessary. </span><span class="kobospan" id="kobo.674.2">In </span><em class="italic"><span class="kobospan" id="kobo.675.1">step 4</span></em><span class="kobospan" id="kobo.676.1">, we print out the stopwords list. </span><span class="kobospan" id="kobo.676.2">In </span><em class="italic"><span class="kobospan" id="kobo.677.1">step 5</span></em><span class="kobospan" id="kobo.678.1">, we read in a small portion of the </span><em class="italic"><span class="kobospan" id="kobo.679.1">Sherlock Holmes</span></em><span class="kobospan" id="kobo.680.1"> book. </span><span class="kobospan" id="kobo.680.2">In </span><em class="italic"><span class="kobospan" id="kobo.681.1">step 6</span></em><span class="kobospan" id="kobo.682.1">, we tokenize the text and print its length, which is 230. </span><span class="kobospan" id="kobo.682.2">In </span><em class="italic"><span class="kobospan" id="kobo.683.1">step 7</span></em><span class="kobospan" id="kobo.684.1">, we remove the stopwords from the original words list by using a list comprehension. </span><span class="kobospan" id="kobo.684.2">Then, we print the length of the result and see that the list length has been reduced to 105. </span><span class="kobospan" id="kobo.684.3">You will notice that in the list comprehension, we check whether the </span><em class="italic"><span class="kobospan" id="kobo.685.1">lowercase</span></em><span class="kobospan" id="kobo.686.1"> version of the word is in the stopwords list since all the stopwords </span><span><span class="kobospan" id="kobo.687.1">are lowercase.</span></span></p>
			<h2 id="_idParaDest-39" class="calibre5"><a id="_idTextAnchor039" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.688.1">How to do it…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.689.1">In the recipe, we</span><a id="_idIndexMarker047" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.690.1"> will read in the text file, tokenize the text, and remove the stopwords from </span><span><span class="kobospan" id="kobo.691.1">the list:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.692.1">Run the file and language </span><span><span class="kobospan" id="kobo.693.1">utilities notebooks:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.694.1">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.695.1">Import the NLTK </span><span><span class="kobospan" id="kobo.696.1">stopwords list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.697.1">
from nltk.corpus import stopwords</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.698.1">The first time you run the notebook, download the </span><strong class="source-inline1"><span class="kobospan" id="kobo.699.1">stopwords</span></strong><span class="kobospan" id="kobo.700.1"> data. </span><span class="kobospan" id="kobo.700.2">You don’t need to download the stopwords again the next time you run </span><span><span class="kobospan" id="kobo.701.1">the code:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.702.1">
nltk.download('stopwords')</span></pre></li>			</ol>
			<p class="callout-heading"><span class="kobospan" id="kobo.703.1">Note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.704.1">Here is a list of languages that NLTK supports for stopwords: Arabic, Azerbaijani, Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Kazakh, Nepali, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, </span><span><span class="kobospan" id="kobo.705.1">and Turkish.</span></span></p>
			<ol class="calibre13">
				<li value="4" class="calibre14"><span class="kobospan" id="kobo.706.1">You can see the stopwords that come with NLTK by printing </span><span><span class="kobospan" id="kobo.707.1">the list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.708.1">
print(stopwords.words('english'))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.709.1">The result will be </span><span><span class="kobospan" id="kobo.710.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.711.1">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.712.1">Read in the </span><span><span class="kobospan" id="kobo.713.1">text file:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.714.1">
text = read_text_file("../data/sherlock_holmes_1.txt")</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.715.1">Tokenize</span><a id="_idIndexMarker048" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.716.1"> the text and print the length of the </span><span><span class="kobospan" id="kobo.717.1">resulting list:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.718.1">
words = word_tokenize_nltk(text)
print(len(words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.719.1">The result will be </span><span><span class="kobospan" id="kobo.720.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.721.1">230</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.722.1">Remove the stopwords from the list using a list comprehension and print the length of the result. </span><span class="kobospan" id="kobo.722.2">You will notice that in the list comprehension, we check whether the </span><em class="italic"><span class="kobospan" id="kobo.723.1">lowercase</span></em><span class="kobospan" id="kobo.724.1"> version of the word is in the stopwords list since all the stopwords </span><span><span class="kobospan" id="kobo.725.1">are lowercase.</span></span><pre class="source-code"><span class="kobospan1" id="kobo.726.1">
words = [word for word in words if word not in stopwords.words("english")]
print(len(words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.727.1">The result will be </span><span><span class="kobospan" id="kobo.728.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.729.1">105</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.730.1">The code then filters</span><a id="_idIndexMarker049" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.731.1"> the stopwords from the text and leaves the words from the text only if they do not also appear in the stopwords list. </span><span class="kobospan" id="kobo.731.2">As we see from the lengths of the two lists, one unfiltered and the other without the stopwords, we remove more than half </span><span><span class="kobospan" id="kobo.732.1">the words.</span></span></p>
			<p class="callout-heading"><span class="kobospan" id="kobo.733.1">Important note</span></p>
			<p class="callout"><span class="kobospan" id="kobo.734.1">You might find that some of the words in the stopwords list provided are not necessary or are missing. </span><span class="kobospan" id="kobo.734.2">You will need to modify the list accordingly. </span><span class="kobospan" id="kobo.734.3">The NLTK stopwords list is a Python list and you can add and remove elements using the standard Python </span><span><span class="kobospan" id="kobo.735.1">list functions.</span></span></p>
			<h2 id="_idParaDest-40" class="calibre5"><a id="_idTextAnchor040" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.736.1">There’s more…</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.737.1">We can also remove stopwords using spaCy. </span><span class="kobospan" id="kobo.737.2">Here is how to </span><span><span class="kobospan" id="kobo.738.1">do it:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.739.1">Assign the stopwords to a variable </span><span><span class="kobospan" id="kobo.740.1">for convenience:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.741.1">
stopwords = small_model.Defaults.stop_words</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.742.1">Tokenize the text and print </span><span><span class="kobospan" id="kobo.743.1">its length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.744.1">
words = word_tokenize_nltk(text)
print(len(words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.745.1">It will give us the </span><span><span class="kobospan" id="kobo.746.1">following result:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.747.1">230</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.748.1">Remove the </span><a id="_idIndexMarker050" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.749.1">stopwords from the list using a list comprehension and print the </span><span><span class="kobospan" id="kobo.750.1">resulting length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.751.1">
words = [word for word in words if word.lower() not in stopwords]
print(len(words))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.752.1">The result will be very similar to </span><span><span class="kobospan" id="kobo.753.1">NLTK :</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.754.1">106</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.755.1">The stopwords from spaCy are stored in a set and we can add more words </span><span><span class="kobospan" id="kobo.756.1">to it:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.757.1">
print(len(stopwords))
stopwords.add("new")
print(len(stopwords))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.758.1">The result will be </span><span><span class="kobospan" id="kobo.759.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.760.1">327
328</span></pre><p class="calibre3"><span class="kobospan" id="kobo.761.1">Similarly, we can remove words </span><span><span class="kobospan" id="kobo.762.1">if necessary:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.763.1">print(len(stopwords))
stopwords.remove("new")
print(len(stopwords))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.764.1">The result will be </span><span><span class="kobospan" id="kobo.765.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.766.1">328
327</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.767.1">We can also compile a stopwords list using the text we are working with and calculate the frequencies of the words in it. </span><span class="kobospan" id="kobo.767.2">This provides you with an automatic way of removing stopwords, without </span><a id="_idIndexMarker051" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.768.1">the need for </span><span><span class="kobospan" id="kobo.769.1">manual review.</span></span></p>
			<h2 id="_idParaDest-41" class="calibre5"><a id="_idTextAnchor041" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.770.1">There's still more</span></h2>
			<p class="calibre3"><span class="kobospan" id="kobo.771.1">In this section, we will show you two ways of doing so. </span><span class="kobospan" id="kobo.771.2">You will need to use the file at </span><a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt" class="calibre6 pcalibre pcalibre1"><span class="kobospan" id="kobo.772.1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt</span></a><span class="kobospan" id="kobo.773.1">. </span><span class="kobospan" id="kobo.773.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.774.1">FreqDist</span></strong><span class="kobospan" id="kobo.775.1"> object in the NLTK package counts the number of occurrences of each word that we later use to find the most frequent words and remove them </span><span><span class="kobospan" id="kobo.776.1">as stopwords:</span></span></p>
			<ol class="calibre13">
				<li class="calibre14"><span class="kobospan" id="kobo.777.1">Import the NTLK </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.778.1">FreqDist</span></strong></span><span><span class="kobospan" id="kobo.779.1"> class:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.780.1">
from nltk.probability import FreqDist</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.781.1">Define the function that will compile a list </span><span><span class="kobospan" id="kobo.782.1">of stopwords:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.783.1">
def compile_stopwords_list_frequency(text, cut_off=0.02):
    words = word_tokenize_nltk(text)
    freq_dist = FreqDist(word.lower() for word in words)
    words_with_frequencies = [
        (word, freq_dist[word]) for word in freq_dist.keys()]
    sorted_words = sorted(words_with_frequencies, 
        key=lambda tup: tup[1])
    stopwords = []
    if (type(cut_off) is int):
        # First option: use a frequency cutoff
        stopwords = [tuple[0] for tuple in sorted_words 
            if tuple[1] &gt; cut_off]
    elif (type(cut_off) is float):
        # Second option: use a percentage of the words
        length_cutoff = int(cut_off*len(sorted_words))
        stopwords = [tuple[0] for tuple in 
            sorted_words[-length_cutoff:]]
    else:
        raise TypeError("The cut off needs to be either a float (percentage) or an int (frequency cut off)")
    return stopwords</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.784.1">Define the </span><a id="_idIndexMarker052" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.785.1">stopwords list using the default settings, and print out the result and </span><span><span class="kobospan" id="kobo.786.1">its length:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.787.1">
text = read_text_file("../data/sherlock_holmes.txt")
stopwords = compile_stopwords_list_frequency(text)
print(stopwords)
print(len(stopwords))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.788.1">The result will be </span><span><span class="kobospan" id="kobo.789.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.790.1">['make', 'myself', 'night', 'until', 'street', 'few', 'why', 'thought', 'take', 'friend', 'lady', 'side', 'small', 'still', 'these', 'find', 'st.', 'every', 'watson', 'too', 'round', 'young', 'father', 'left', 'day', 'yet', 'first', 'once', 'took', 'its', 'eyes', 'long', 'miss', 'through', 'asked', 'most', 'saw', 'oh', 'morning', 'right', 'last', 'like', 'say', 'tell', 't', 'sherlock', 'their', 'go', 'own', 'after', 'away', 'never', 'good', 'nothing', 'case', 'however', 'quite', 'found', 'made', 'house', 'such', 'heard', 'way', 'yes', 'hand', 'much', 'matter', 'where', 'might', 'just', 'room', 'any', 'face', 'here', 'back', 'door', 'how', 'them', 'two', 'other', 'came', 'time', 'did', 'than', 'come', 'before', 'must', 'only', 'know', 'about', 'shall', 'think', 'more', 'over', 'us', 'well', 'am', 'or', 'may', 'they', ';', 'our', 'should', 'now', 'see', 'down', 'can', 'some', 'if', 'will', 'mr.', 'little', 'who', 'into', 'do', 'has', 'could', 'up', 'man', 'out', 'when', 'would', 'an', 'are', 'by', '!', 'were', 's', 'then', 'one', 'all', 'on', 'no', 'what', 'been', 'your', 'very', 'him', 'her', 'she', 'so', ''', 'holmes', 'upon', 'this', 'said', 'from', 'there', 'we', 'me', 'be', 'but', 'not', 'for', '?', 'at', 'which', 'with', 'had', 'as', 'have', 'my', ''', 'is', 'his', 'was', 'you', 'he', 'it', 'that', 'in', '"', 'a', 'of', 'to', '"', 'and', 'i', '.', 'the', ',']
181</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.791.1">Now, use the function</span><a id="_idIndexMarker053" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.792.1"> with the frequency cut-off of 5% (use the top 5% of the most frequent words </span><span><span class="kobospan" id="kobo.793.1">as stopwords):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.794.1">
text = read_text_file("../data/sherlock_holmes.txt")
stopwords = compile_stopwords_list_frequency(text, cut_off=0.05)
print(len(stopwords))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.795.1">The result will be </span><span><span class="kobospan" id="kobo.796.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.797.1">452</span></pre></li>				<li class="calibre14"><span class="kobospan" id="kobo.798.1">Now, use the absolute frequency cut-off of </span><strong class="source-inline1"><span class="kobospan" id="kobo.799.1">100</span></strong><span class="kobospan" id="kobo.800.1"> (take the words that have a frequency greater </span><span><span class="kobospan" id="kobo.801.1">than 100):</span></span><pre class="source-code"><span class="kobospan1" id="kobo.802.1">
stopwords = compile_stopwords_list_frequency(text, cut_off=100)
print(stopwords)
print(len(stopwords))</span></pre><p class="calibre3"><span class="kobospan" id="kobo.803.1">And the result is </span><span><span class="kobospan" id="kobo.804.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.805.1">['away', 'never', 'good', 'nothing', 'case', 'however', 'quite', 'found', 'made', 'house', 'such', 'heard', 'way', 'yes', 'hand', 'much', 'matter', 'where', 'might', 'just', 'room', 'any', 'face', 'here', 'back', 'door', 'how', 'them', 'two', 'other', 'came', 'time', 'did', 'than', 'come', 'before', 'must', 'only', 'know', 'about', 'shall', 'think', 'more', 'over', 'us', 'well', 'am', 'or', 'may', 'they', ';', 'our', 'should', 'now', 'see', 'down', 'can', 'some', 'if', 'will', 'mr.', 'little', 'who', 'into', 'do', 'has', 'could', 'up', 'man', 'out', 'when', 'would', 'an', 'are', 'by', '!', 'were', 's', 'then', 'one', 'all', 'on', 'no', 'what', 'been', 'your', 'very', 'him', 'her', 'she', 'so', ''', 'holmes', 'upon', 'this', 'said', 'from', 'there', 'we', 'me', 'be', 'but', 'not', 'for', '?', 'at', 'which', 'with', 'had', 'as', 'have', 'my', ''', 'is', 'his', 'was', 'you', 'he', 'it', 'that', 'in', '"', 'a', 'of', 'to', '"', 'and', 'i', '.', 'the', ',']
131</span></pre></li>			</ol>
			<p class="calibre3"><span class="kobospan" id="kobo.806.1">The function</span><a id="_idIndexMarker054" class="calibre6 pcalibre pcalibre1"/><span class="kobospan" id="kobo.807.1"> that creates the stopwords list takes in the text and the </span><strong class="source-inline"><span class="kobospan" id="kobo.808.1">cut_off</span></strong><span class="kobospan" id="kobo.809.1"> parameter. </span><span class="kobospan" id="kobo.809.2">It could be a float representing the percentage of frequency-ranked words that will be in the stopwords list. </span><span class="kobospan" id="kobo.809.3">Alternatively, it could be an integer that represents the absolute threshold frequency, with words above it considered stopwords. </span><span class="kobospan" id="kobo.809.4">In the function, we first tokenize the words from the book, then create a </span><strong class="source-inline"><span class="kobospan" id="kobo.810.1">FreqDist</span></strong><span class="kobospan" id="kobo.811.1"> object, and then create a list of tuples (word, word’s frequency) using the frequency distribution. </span><span class="kobospan" id="kobo.811.2">We sort the list using the word frequency. </span><span class="kobospan" id="kobo.811.3">We then check the </span><strong class="source-inline"><span class="kobospan" id="kobo.812.1">cut_off</span></strong><span class="kobospan" id="kobo.813.1"> parameter’s type and raise an error if it is not a float or an integer. </span><span class="kobospan" id="kobo.813.2">If it is an integer, we return all words whose frequency is higher than the parameter as stopwords. </span><span class="kobospan" id="kobo.813.3">If it is a float, we calculate the number of words to be returned using the parameter as </span><span><span class="kobospan" id="kobo.814.1">the percentage.</span></span></p>
		</div>
	</body></html>