<html><head></head><body>
		<div><h1 id="_idParaDest-14" class="chapter-number"><a id="_idTextAnchor013" class="calibre6 pcalibre pcalibre1"/>1</h1>
			<h1 id="_idParaDest-15" class="calibre7"><a id="_idTextAnchor014" class="calibre6 pcalibre pcalibre1"/>Learning NLP Basics</h1>
			<p class="calibre3">While working on this book, we were focusing on including recipes that should be useful for a wide variety of NLP projects. They range from simple to advanced, from dealing with grammar to dealing with visualizations, and in many of them, options for languages other than English are included. In this new edition, we have included new topics that cover using GPT and other large language models, explainable AI, a new chapter on transformers, and natural language understanding. We hope you find the book useful.</p>
			<p class="calibre3">The format of the book is that of a <em class="italic">programming cookbook</em>, where each recipe is a short mini-project with a concrete goal and a sequence of steps that need to be performed. There are few theoretical explanations and a focus on the practical goals and what needs to be done to achieve them.</p>
			<p class="calibre3">Before we can get on with the real work of NLP, we need to prepare our text for processing. This chapter will show you how to do it. By the end of the chapter, you will be able to have a list of words in a text with their parts of speech and lemmas or stems, and with very frequent words removed.</p>
			<p class="calibre3"><strong class="bold">Natural Language Toolkit</strong> (<strong class="bold">NLTK</strong>) and <strong class="bold">spaCy</strong> are two <a id="_idIndexMarker000" class="calibre6 pcalibre pcalibre1"/>important packages that<a id="_idIndexMarker001" class="calibre6 pcalibre pcalibre1"/> we will be working with in this chapter and throughout the book. Some other packages we will be using in the book are PyTorch and Hugging Face Transformers. We will also utilize the OpenAI API with the GPT models.</p>
			<p class="calibre3">The recipes included in this chapter are as follows:</p>
			<ul class="calibre15">
				<li class="calibre14">Dividing text into sentences</li>
				<li class="calibre14">Dividing sentences into words – tokenizatio<a id="_idTextAnchor015" class="calibre6 pcalibre pcalibre1"/>n</li>
				<li class="calibre14">Part of speech tagging</li>
				<li class="calibre14">Combining similar words – lemmatization</li>
				<li class="calibre14">Removing stopwords</li>
			</ul>
			<h1 id="_idParaDest-16" class="calibre7"><a id="_idTextAnchor016" class="calibre6 pcalibre pcalibre1"/>Technical requirements</h1>
			<p class="calibre3">Throughout this book, we will use <strong class="bold">Poetry</strong> to manage the Python package installations. You can use the latest version of Poetry since it conserves the previous versions’ functionality. Once you install Poetry, managing which packages to install will be very easy. We will be using <strong class="bold">Python 3.9</strong> throughout the book. You will also need to have <strong class="bold">Jupyter</strong> installed in order to run the notebooks.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You may try to use Google Colab in order to run the notebooks but you will need to tweak the code to make it work with Colab.</p>
			<p class="calibre3">Follow these installation steps:</p>
			<ol class="calibre13">
				<li class="calibre14">Install <strong class="bold">Git</strong>: <a href="https://github.com/git-guides/install-git" class="calibre6 pcalibre pcalibre1">https://github.com/git-guides/install-git</a>.</li>
				<li class="calibre14">Install <strong class="bold">Poetry</strong>: <a href="https://python-poetry.org/docs/#installation" class="calibre6 pcalibre pcalibre1">https://python-poetry.org/docs/#installation</a>.</li>
				<li class="calibre14">Install <strong class="bold">Jupyter</strong>: <a href="https://jupyter.org/install" class="calibre6 pcalibre pcalibre1">https://jupyter.org/install</a>.</li>
				<li class="calibre14">Clone the GitHub repository that contains all the code from this book (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition</a>) by issuing the following command in the terminal:</li>
			</ol>
			<pre class="console">
git clone https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition.git</pre>			<ol class="calibre13">
				<li value="5" class="calibre14">In the directory that contains the <strong class="source-inline1">pyproject.toml</strong> file, run the commands using the terminal:</li>
			</ol>
			<pre class="console">
poetry install
poetry shell</pre>			<ol class="calibre13">
				<li value="6" class="calibre14">Start the notebook engine:</li>
			</ol>
			<pre class="console">
jupyter notebook</pre>			<p class="calibre3">Now, you should be able to run all the notebooks in your cloned repository.</p>
			<p class="calibre3">If you prefer not to use Poetry, you can set up a virtual environment using the <code>requirements.txt</code> file provided in the book repository. You can do this in one of two ways. You can use <code>pip</code>:</p>
			<pre class="console">
pip install -r requirements.txt</pre>			<p class="calibre3">You can also use <code>conda</code>:</p>
			<pre class="console">
conda create --name &lt;env_name&gt; --file requirements.txt</pre>			<h1 id="_idParaDest-17" class="calibre7"><a id="_idTextAnchor017" class="calibre6 pcalibre pcalibre1"/>Dividing text into sentences</h1>
			<p class="calibre3">When we<a id="_idIndexMarker002" class="calibre6 pcalibre pcalibre1"/> work with text, we can work with text units on different scales: the document itself, such as a newspaper article, the paragraph, the sentence, or the word. Sentences are the main unit of processing in many NLP tasks. For example, when we send data over to <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>), we<a id="_idIndexMarker003" class="calibre6 pcalibre pcalibre1"/> frequently want to add some context to the prompt. In some cases, we would like that context to include sentences from a text so that the <a id="_idIndexMarker004" class="calibre6 pcalibre pcalibre1"/>model can extract some important information from that text. In this section, we will show you how to divide a text into sentences.</p>
			<h2 id="_idParaDest-18" class="calibre5"><a id="_idTextAnchor018" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this part, we will be using the text of the book <em class="italic">The Adventures of Sherlock Holmes</em>. You can find the whole text in the book’s GitHub file (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt</a>). For this recipe we will need just the beginning of the book, which can be found in the file at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</a>.</p>
			<p class="calibre3">In order to do this task, you will need the NLTK package and its sentence tokenizers, which are part of the Poetry file. Directions to install Poetry are described in the <em class="italic">Technical </em><em class="italic">requirements</em> section.</p>
			<h2 id="_idParaDest-19" class="calibre5"><a id="_idTextAnchor019" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">We will <a id="_idIndexMarker005" class="calibre6 pcalibre pcalibre1"/>now divide the text of a small piece of <em class="italic">The Adventures of Sherlock Holmes</em>, outputting a list of sentences. (Reference notebook: <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_text_into_sentences_1.1.ipynb</a>.) Here, we assume that you<a id="_idIndexMarker006" class="calibre6 pcalibre pcalibre1"/> are running the notebook, so the paths are all relative to the notebook location:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the file utility functions from the <strong class="source-inline1">util</strong> folder (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/util/file_utils.ipynb</a>):<pre class="source-code">
%run -i "../util/file_utils.ipynb"</pre></li>				<li class="calibre14">Read in the book part text:<pre class="source-code">
sherlock_holmes_part_of_text = read_text_file("../data/sherlock_holmes_1.txt")</pre><p class="calibre3">The <code>read_text_file</code> function is located in the <code>util</code> notebook we imported previously. Here is its source code:</p><pre class="source-code">def read_text_file(filename):
    file = open(filename, "r", encoding="utf-8")
    return file.read()</pre></li>				<li class="calibre14">Print out the resulting text to make sure everything worked correctly and the file loaded:<pre class="source-code">
print(sherlock_holmes_part_of_text)</pre><p class="calibre3">The beginning of the printout will look like this:</p><pre class="source-code">To Sherlock Holmes she is always _the_ woman. I have seldom heard him
mention her under any other name. In his eyes she eclipses and
predominates the whole of her sex…</pre></li>				<li class="calibre14">Import <a id="_idIndexMarker007" class="calibre6 pcalibre pcalibre1"/>the <strong class="source-inline1">nltk</strong> package:<pre class="source-code">
import nltk</pre></li>				<li class="calibre14">If this is the <a id="_idIndexMarker008" class="calibre6 pcalibre pcalibre1"/>first time you are running the code, you will need to download tokenizer data. You will not need to run this command after that:<pre class="source-code">
nltk.download('punkt')</pre></li>				<li class="calibre14">Initialize the tokenizer:<pre class="source-code">
tokenizer = nltk.data.load("tokenizers/punkt/english.pickle")</pre></li>				<li class="calibre14">Divide the text into sentences using the tokenizer. The result will be a list of sentences:<pre class="source-code">
sentences_nltk = tokenizer.tokenize(
    sherlock_holmes_part_of_text)</pre></li>				<li class="calibre14">Print the result:<pre class="source-code">
print(sentences_nltk)</pre><p class="calibre3">It should look like this. There are newlines inside the sentences that come from the book formatting. They are not necessarily sentence endings:</p><pre class="source-code">['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\nmention her under any other name.', 'In his eyes she eclipses and\npredominates the whole of her sex.', 'It was not that he felt any emotion\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\nwas, I take it, the most perfect reasoning and observing machine that\nthe world has seen, but as a lover he would have placed himself in a\nfalse position.', 'He never spoke of the softer passions, save with a gibe\nand a sneer.', 'They were admirable things for the observer—excellent for\ndrawing the veil from men's motives and actions.', 'But for the trained\nreasoner to admit such intrusions into his own delicate and finely\nadjusted temperament was to introduce a distracting factor which might\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\ninstrument, or a crack in one of his own high-power lenses, would not\nbe more disturbing than a strong emotion in a nature such as his.', 'And\nyet there was but one woman to him, and that woman was the late Irene\nAdler, of dubious and questionable memory.']</pre></li>				<li class="calibre14">Print the<a id="_idIndexMarker009" class="calibre6 pcalibre pcalibre1"/> number of sentences in the result; there<a id="_idIndexMarker010" class="calibre6 pcalibre pcalibre1"/> should be 11 sentences in total:<pre class="source-code">
print(len(sentences_nltk))</pre><p class="calibre3">This gives the result:</p><pre class="source-code">11</pre></li>			</ol>
			<p class="calibre3">Although it might seem straightforward to divide a text into sentences by just using a regular expression to split it at the periods, in reality, it is more complicated. We use periods in places other than ends of sentences; for example, after abbreviations – for example, “Dr. Smith will see you now.” Similarly, while all sentences in English start with a capital letter, we also use capital letters for proper names. The approach used in <code>nltk</code> takes all these points into consideration; it is an implementation of an unsupervised algorithm presented in <a href="https://aclanthology.org/J06-4003.pdf" class="calibre6 pcalibre pcalibre1">https://aclanthology.org/J06-4003.pdf</a>.</p>
			<h2 id="_idParaDest-20" class="calibre5"><a id="_idTextAnchor020" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can also use a different strategy to parse the text into sentences, employing the other very<a id="_idIndexMarker011" class="calibre6 pcalibre pcalibre1"/> popular NLP package, <strong class="bold">spaCy</strong>. Here is how it works:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the spaCy package:<pre class="source-code">
import spacy</pre></li>				<li class="calibre14">The first time you run the notebook, you will need to download a spaCy model.  The model is trained on a large amount of English text and there are several tools that can be used with it, including the sentence tokenizer. Here, I’m downloading the smallest model, but you might try other ones (see <a href="https://spacy.io/usage/models/" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/models/</a>):<pre class="source-code">
!python -m spacy download en_core_web_sm</pre></li>				<li class="calibre14">Initialize the spaCy engine:<pre class="source-code">
nlp = spacy.load("en_core_web_sm")</pre></li>				<li class="calibre14">Process the text using the spaCy engine. This line assumes that you have the <strong class="source-inline1">sherlock_holmes_part_of_text</strong> variable initialized. If not, you need to run one of the earlier cells where the text is read into this variable:<pre class="source-code">
doc = nlp(sherlock_holmes_part_of_text)</pre></li>				<li class="calibre14">Get the sentences from the processed <strong class="source-inline1">doc</strong> object, and print the resulting array and its length:<pre class="source-code">
sentences_spacy = [sentence.text for sentence in doc.sents]
print(sentences_spacy)
print(len(sentences_spacy))</pre><p class="calibre3">The result will look like this:</p><pre class="source-code">['To Sherlock Holmes she is always _the_ woman.', 'I have seldom heard him\nmention her under any other name.', 'In his eyes she eclipses and\npredominates the whole of her sex.', 'It was not that he felt any emotion\nakin to love for Irene Adler.', 'All emotions, and that one particularly,\nwere abhorrent to his cold, precise but admirably balanced mind.', 'He\nwas, I take it, the most perfect reasoning and observing machine that\nthe world has seen, but as a lover he would have placed himself in a\nfalse position.', 'He never spoke of the softer passions, save with a gibe\nand a sneer.', 'They were admirable things for the observer—excellent for\ndrawing the veil from men's motives and actions.', 'But for the trained\nreasoner to admit such intrusions into his own delicate and finely\nadjusted temperament was to introduce a distracting factor which might\nthrow a doubt upon all his mental results.', 'Grit in a sensitive\ninstrument, or a crack in one of his own high-power lenses, would not\nbe more disturbing than a strong emotion in a nature such as his.', 'And\nyet there was but one woman to him, and that woman was the late Irene\nAdler, of dubious and questionable memory.']
11</pre></li>			</ol>
			<p class="calibre3">An important<a id="_idIndexMarker012" class="calibre6 pcalibre pcalibre1"/> difference between spaCy and NLTK is the time it takes to complete the sentence-splitting process. The reason for this is that spaCy loads a language model and uses several tools in addition to the tokenizer, while the NLTK tokenizer has only one function: to separate the text into sentences. We can time the execution by using the <code>time</code> package and putting the code to split the sentences into the <code>main</code> function:</p>
			<pre class="source-code">
import time
def split_into_sentences_nltk(text):
    sentences = tokenizer.tokenize(text)
    return sentences
def split_into_sentences_spacy(text):
    doc = nlp(text)
    sentences = [sentence.text for sentence in doc.sents]
    return sentences
start = time.time()
split_into_sentences_nltk(sherlock_holmes_part_of_text)
print(f"NLTK: {time.time() - start} s")
start = time.time()
split_into_sentences_spacy(sherlock_holmes_part_of_text)
print(f"spaCy: {time.time() - start} s")</pre>			<p class="calibre3">The spaCy algorithm<a id="_idIndexMarker013" class="calibre6 pcalibre pcalibre1"/> takes 0.019 seconds, while the NLTK algorithm takes 0.0002. The time is calculated by subtracting the current time (<code>time.time()</code>) from the start time that is set at the beginning of the code block. It is possible that you will get slightly different values.</p>
			<p class="calibre3">The reason why you might use spaCy is if you are doing other processing with the package along with splitting it into sentences. The spaCy processor does many other things, and that is why it takes longer. If you are using other features of spaCy, there is no reason to use NLTK just for sentence splitting, and it’s better to employ spaCy for the whole pipeline.</p>
			<p class="calibre3">It is also possible to use <a id="_idIndexMarker014" class="calibre6 pcalibre pcalibre1"/>only the tokenizer without other tools from spaCy. Please see their documentation for more information: <a href="https://spacy.io/usage/processing-pipelines" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/processing-pipelines</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">spaCy might be slower, but it is doing many more things in the background, and if you are using its other features, use it for sentence splitting as well.</p>
			<h2 id="_idParaDest-21" class="calibre5"><a id="_idTextAnchor021" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">You can use NLTK and spaCy to divide texts in languages other than English. NLTK includes tokenizer models for Czech, Danish, Dutch, Estonian, Finnish, French, German, Greek, Italian, Norwegian, Polish, Portuguese, Slovene, Spanish, Swedish, and Turkish. In order to load those models, use the name of the language followed by the <code>.</code><code>pickle</code> extension:</p>
			<pre class="source-code">
tokenizer = nltk.data.load("tokenizers/punkt/spanish.pickle")</pre>			<p class="calibre3">See the NLTK documentation<a id="_idIndexMarker015" class="calibre6 pcalibre pcalibre1"/> to find out more: <a href="https://www.nltk.org/index.html" class="calibre6 pcalibre pcalibre1">https://www.nltk.org/index.html</a>.</p>
			<p class="calibre3">Likewise, spaCy has models for other languages: Chinese, Dutch, English, French, German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. These models are trained on text in those languages. In order to use those models, you would have to download them separately. For example, for Spanish, use this command to download the model:</p>
			<pre class="console">
python -m spacy download es_core_news_sm</pre>			<p class="calibre3">Then, put this line in the code to use it:</p>
			<pre class="source-code">
nlp = spacy.load("es_core_news_sm")</pre>			<p class="calibre3">See the spaCy documentation to find out more: <a href="https://spacy.io/usage/models" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/models</a>.</p>
			<h1 id="_idParaDest-22" class="calibre7"><a id="_idTextAnchor022" class="calibre6 pcalibre pcalibre1"/>Dividing sentences into words – tokenization</h1>
			<p class="calibre3">In many <a id="_idIndexMarker016" class="calibre6 pcalibre pcalibre1"/>instances, we<a id="_idIndexMarker017" class="calibre6 pcalibre pcalibre1"/> rely on individual words when we do NLP tasks. This happens, for example, when we build semantic models of<a id="_idIndexMarker018" class="calibre6 pcalibre pcalibre1"/> texts by relying on the semantics – of individual words, or when we are looking for words with a specific part of speech. To divide text into words, we can use NLTK and spaCy to do this task for us.</p>
			<h2 id="_idParaDest-23" class="calibre5"><a id="_idTextAnchor023" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this part, we will<a id="_idIndexMarker019" class="calibre6 pcalibre pcalibre1"/> be using the same text of the book <em class="italic">The Adventures of Sherlock Holmes</em>. You can find the whole text in the book’s GitHub repository. For this recipe, we will need just the beginning of the book, which can be found in the <code>sherlock_holmes_1.txt</code> file.</p>
			<p class="calibre3">In order to do this task, you will need the NLTK and spaCy packages, which are part of the Poetry file. Directions to install Poetry are described in the <em class="italic">Technical </em><em class="italic">requirements</em> section.</p>
			<p class="calibre3">(Notebook reference: <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/dividing_sentences_into_words_1.2.ipynb</a>.)</p>
			<h2 id="_idParaDest-24" class="calibre5"><a id="_idTextAnchor024" class="calibre6 pcalibre pcalibre1"/>How to do it</h2>
			<p class="calibre3">The process is as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the <strong class="source-inline1">file_utils</strong> notebook. Effectively, we run the <strong class="source-inline1">file_utils</strong> notebook inside this one so we have access to its defined functions and variables:<pre class="source-code">
%run -i "../util/file_utils.ipynb"</pre></li>				<li class="calibre14">Read in the book snippet text:<pre class="source-code">
sherlock_holmes_part_of_text = read_text_file("../data/sherlock_holmes_1.txt")
print(sherlock_holmes_part_of_text)</pre><p class="calibre3">The result should look like this:</p><pre class="source-code">To Sherlock Holmes she is always _the_ woman. I have seldom heard him
mention her under any other name. In his eyes she eclipses and
predominates the whole of her sex... [Output truncated]</pre></li>				<li class="calibre14">Import the <strong class="source-inline1">nltk</strong> package:<pre class="source-code">
import nltk</pre></li>				<li class="calibre14">Divide <a id="_idIndexMarker020" class="calibre6 pcalibre pcalibre1"/>the input into words. Here, we use the NLTK <a id="_idIndexMarker021" class="calibre6 pcalibre pcalibre1"/>word tokenizer to split the text into individual <a id="_idIndexMarker022" class="calibre6 pcalibre pcalibre1"/>words. The output of the function is a Python list of the words:<pre class="source-code">
words = nltk.tokenize.word_tokenize(
    sherlock_holmes_part_of_text)
print(words)
print(len(words))</pre></li>				<li class="calibre14">The output will be the list of words in the text and the length of the <strong class="source-inline1">words</strong> list:<pre class="source-code">
['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_the_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer—excellent', 'for', 'drawing', 'the', 'veil', 'from', 'men', ''', 's', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high-power', 'lenses', ',', 'would', 'not', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']
230</pre></li>			</ol>
			<p class="calibre3">The output<a id="_idIndexMarker023" class="calibre6 pcalibre pcalibre1"/> is a list, where<a id="_idIndexMarker024" class="calibre6 pcalibre pcalibre1"/> each token is either a word or a punctuation mark. The NLTK tokenizer uses a set of rules to split the text into words. It splits but <a id="_idIndexMarker025" class="calibre6 pcalibre pcalibre1"/>does not expand contractions, such as <em class="italic">don’t → do n’t</em> and <em class="italic">men’s → men ’s</em>, as in the preceding example. It treats punctuation and quotes as separate tokens, so the result includes words with no other marks.</p>
			<h2 id="_idParaDest-25" class="calibre5"><a id="_idTextAnchor025" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">Sometimes, it is useful <a id="_idIndexMarker026" class="calibre6 pcalibre pcalibre1"/>not to split some words and use them as one unit. One example of this is in <a href="B18411_03.xhtml#_idTextAnchor067" class="calibre6 pcalibre pcalibre1"><em class="italic">Chapter 3</em></a>, in the <em class="italic">Representing phrases – phrase2vec</em> recipe, where we store phrases and not just individual words. The NLTK package allows<a id="_idIndexMarker027" class="calibre6 pcalibre pcalibre1"/> us to do that using its custom tokenizer, <code>MWETokenizer</code>:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the <strong class="source-inline1">MWETokenizer</strong> class:<pre class="source-code">
from nltk.tokenize import MWETokenizer</pre></li>				<li class="calibre14">Initialize the tokenizer and indicate that the words <strong class="source-inline1">dim sum dinner</strong> should not be split:<pre class="source-code">
tokenizer = MWETokenizer([('dim', 'sum', 'dinner')])</pre></li>				<li class="calibre14">Add more words that should be kept together:<pre class="source-code">
tokenizer.add_mwe(('best', 'dim', 'sum'))</pre></li>				<li class="calibre14">Use the tokenizer to split a sentence:<pre class="source-code">
tokens = tokenizer.tokenize('Last night I went for dinner in an Italian restaurant. The pasta was delicious.'.split())
print(tokens)</pre><p class="calibre3">The result will contain the tokens split the same way as previously:</p><pre class="source-code">['Last', 'night', 'I', 'went', 'for', 'dinner', 'in', 'an', 'Italian', 'restaurant.', 'The', 'pasta', 'was', 'delicious.']</pre></li>				<li class="calibre14">Split a different sentence:<pre class="source-code">
tokens = tokenizer.tokenize('I went out to a dim sum dinner last night. This restaurant has the best dim sum in town.'.split())
print(tokens)</pre><p class="calibre3">In this case, the tokenizer will put the phrases together into one unit and insert underscores instead of spaces:</p><pre class="source-code">['I', 'went', 'out', 'to', 'a', 'dim_sum_dinner', 'last', 'night.', 'This', 'restaurant', 'has', 'the_best_dim_sum', 'in', 'town.']</pre></li>			</ol>
			<p class="calibre3">We can also use<a id="_idIndexMarker028" class="calibre6 pcalibre pcalibre1"/> spaCy to do the tokenization. Word tokenization is one task in a larger array of tasks that spaCy accomplishes while processing text.</p>
			<h2 id="_idParaDest-26" class="calibre5"><a id="_idTextAnchor026" class="calibre6 pcalibre pcalibre1"/>There's still more</h2>
			<p class="calibre3">If<a id="_idIndexMarker029" class="calibre6 pcalibre pcalibre1"/> you are doing further processing on the text, it makes sense to use spaCy. Here is how it works:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the <strong class="source-inline1">spacy</strong> package:<pre class="source-code">
import spacy</pre></li>				<li class="calibre14">Execute this command only if you have not before:<pre class="source-code">
!python -m spacy download en_core_web_sm</pre></li>				<li class="calibre14">Initialize the spaCy engine using the English model:<pre class="source-code">
nlp = spacy.load("en_core_web_sm")</pre></li>				<li class="calibre14">Divide the text into sentences:<pre class="source-code">
doc = nlp(sherlock_holmes_part_of_text)
words = [token.text for token in doc]</pre></li>				<li class="calibre14">Print the result:<pre class="source-code">
print(words)
print(len(words))</pre><p class="calibre3">The output will be as follows:</p><pre class="source-code">['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', '_', 'the', '_', 'woman', '.', 'I', 'have', 'seldom', 'heard', 'him', '\n', 'mention', 'her', 'under', 'any', 'other', 'name', '.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', '\n', 'predominates', 'the', 'whole', 'of', 'her', 'sex', '.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', '\n', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', '.', 'All', 'emotions', ',', 'and', 'that', 'one', 'particularly', ',', '\n', 'were', 'abhorrent', 'to', 'his', 'cold', ',', 'precise', 'but', 'admirably', 'balanced', 'mind', '.', 'He', '\n', 'was', ',', 'I', 'take', 'it', ',', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', '\n', 'the', 'world', 'has', 'seen', ',', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', '\n', 'false', 'position', '.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', ',', 'save', 'with', 'a', 'gibe', '\n', 'and', 'a', 'sneer', '.', 'They', 'were', 'admirable', 'things', 'for', 'the', 'observer', '—', 'excellent', 'for', '\n', 'drawing', 'the', 'veil', 'from', 'men', ''s', 'motives', 'and', 'actions', '.', 'But', 'for', 'the', 'trained', '\n', 'reasoner', 'to', 'admit', 'such', 'intrusions', 'into', 'his', 'own', 'delicate', 'and', 'finely', '\n', 'adjusted', 'temperament', 'was', 'to', 'introduce', 'a', 'distracting', 'factor', 'which', 'might', '\n', 'throw', 'a', 'doubt', 'upon', 'all', 'his', 'mental', 'results', '.', 'Grit', 'in', 'a', 'sensitive', '\n', 'instrument', ',', 'or', 'a', 'crack', 'in', 'one', 'of', 'his', 'own', 'high', '-', 'power', 'lenses', ',', 'would', 'not', '\n', 'be', 'more', 'disturbing', 'than', 'a', 'strong', 'emotion', 'in', 'a', 'nature', 'such', 'as', 'his', '.', 'And', '\n', 'yet', 'there', 'was', 'but', 'one', 'woman', 'to', 'him', ',', 'and', 'that', 'woman', 'was', 'the', 'late', 'Irene', '\n', 'Adler', ',', 'of', 'dubious', 'and', 'questionable', 'memory', '.']
251</pre></li>			</ol>
			<p class="calibre3">You will notice that the length of the word list is longer when using spaCy than NLTK. One of the reasons <a id="_idIndexMarker030" class="calibre6 pcalibre pcalibre1"/>is that spaCy keeps the newlines, and each newline is a separate token. The other <a id="_idIndexMarker031" class="calibre6 pcalibre pcalibre1"/>difference is that spaCy splits words with a dash, such as <em class="italic">high-power</em>. You can find the exact difference between the two lists by running the following line:</p>
			<pre class="source-code">
print(set(words_spacy)-set(words_nltk))</pre>			<p class="calibre3">This should result in the following output:</p>
			<pre class="source-code">
{'high', 'power', 'observer', '-', '_', '—', 'excellent', ''s', '\n'}</pre>			<p class="callout-heading">Important note</p>
			<p class="callout">If you are doing other processing with spaCy, it makes sense to use it. Otherwise, NLTK word tokenization is sufficient.</p>
			<h2 id="_idParaDest-27" class="calibre5"><a id="_idTextAnchor027" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">The NLTK package only has word tokenization for English.</p>
			<p class="calibre3">spaCy has models for other languages: Chinese, Dutch, English, French, German, Greek, Italian, Japanese, Portuguese, Romanian, Spanish, and others. In order to use those models, you would have to download them separately. For example, for Spanish, use this command to download the model:</p>
			<pre class="console">
python -m spacy download es_core_news_sm</pre>			<p class="calibre3">Then, put this line in the code to use it:</p>
			<pre class="source-code">
nlp = spacy.load("es_core_news_sm")</pre>			<p class="calibre3">See the spaCy documentation to find out more: <a href="https://spacy.io/usage/models" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/models</a>.</p>
			<div><h1 id="_idParaDest-28" class="calibre7"><a id="_idTextAnchor028" class="calibre6 pcalibre pcalibre1"/>Part of speech tagging</h1>
			<p class="calibre3">In many cases, NLP processing <a id="_idIndexMarker032" class="calibre6 pcalibre pcalibre1"/>depends on determining the parts of speech of the words in the text. For example, when we want to find out the named entities that appear in a text, we need to know the parts of speech of the words. In this recipe, we will again consider NLTK and spaCy algorithms.</p>
			<h2 id="_idParaDest-29" class="calibre5"><a id="_idTextAnchor029" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">For this part, we will be using the same text of the book <em class="italic">The Adventures of Sherlock Holmes</em>. You can find the whole text in the book’s Github repository. For this recipe, we will need just the beginning of the book, which can be found in the file at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</a>.</p>
			<p class="calibre3">In order to do this task, you will need the NLTK and spaCy packages, described in the <em class="italic">Technical </em><em class="italic">requirements</em> section.</p>
			<p class="calibre3">We will also complete this task using the OpenAI API’s GPT model to demonstrate that it can complete it as well as spaCy and NLTK. For this part to run, you will need the <code>openai</code> package, which is included in the Poetry environment. You will also need your own OpenAI API key.</p>
			<h2 id="_idParaDest-30" class="calibre5"><a id="_idTextAnchor030" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">In this recipe, we will use the spaCy package to label words with their parts of speech.</p>
			<p class="calibre3">The process is as follows:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the <strong class="source-inline1">util</strong> file and the language <strong class="source-inline1">util</strong> file. The language <strong class="source-inline1">util</strong> file contains an import of spaCy and NLTK, as well as an initialization of the small spaCy model into the <strong class="source-inline1">small_model</strong> object. These files also include functions to read in text from a file and tokenization functions using spaCy and NLTK:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">We will define the function that will output parts of speech for every word. In this function, we first process the input text using the spaCy model that results in a <strong class="source-inline1">Document</strong> object. The resulting <strong class="source-inline1">Document</strong> object contains an iterator with <strong class="source-inline1">Token</strong> objects, and each <strong class="source-inline1">Token</strong> object has information about parts of speech.<p class="calibre3">We use<a id="_idIndexMarker033" class="calibre6 pcalibre pcalibre1"/> this information to create the two lists, one with words and the other one with their respective parts of speech.</p><p class="calibre3">Finally, we zip the two lists to pair the words with the parts of speech and return the resulting list of tuples. We do this in order to easily print the whole list with their corresponding parts of speech. When you use part of speech tagging in your code, you can just iterate through the list of tokens:</p><pre class="source-code">
def pos_tag_spacy(text, model):
    doc = model(text)
    words = [token.text for token in doc]
    pos = [token.pos_ for token in doc]
    return list(zip(words, pos))</pre></li>				<li class="calibre14">Read in the text:<pre class="source-code">
text = read_text_file("../data/sherlock_holmes_1.txt")</pre></li>				<li class="calibre14">Run the preceding function using the text and the model as input:<pre class="source-code">
words_with_pos = pos_tag_spacy(text, small_model)</pre></li>				<li class="calibre14">Print the output:<pre class="source-code">
print(words_with_pos)</pre><p class="calibre3">Part of the result is shown in the following; for the complete output, please see the Jupyter notebook (<a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/Chapter01/part_of_speech_tagging_1.3.ipynb</a>):</p><pre class="source-code">[('To', 'ADP'),
 ('Sherlock', 'PROPN'),
 ('Holmes', 'PROPN'),
 ('she', 'PRON'),
 ('is', 'AUX'),
 ('always', 'ADV'),
 ('_', 'PUNCT'),
 ('the', 'DET'),
 ('_', 'PROPN'),
 ('woman', 'NOUN'),
 ('.', 'PUNCT'),
 ('I', 'PRON'),
 ('have', 'AUX'),
 ('seldom', 'ADV'),
 ('heard', 'VERB'),
 ('him', 'PRON'),
 ('\n', 'SPACE'),
 ('mention', 'VERB'),
 ('her', 'PRON'),
 ('under', 'ADP'),
 ('any', 'DET'),
 ('other', 'ADJ'),
 ('name', 'NOUN'),
 ('.', 'PUNCT'),…</pre></li>			</ol>
			<p class="calibre3">The resulting list <a id="_idIndexMarker034" class="calibre6 pcalibre pcalibre1"/>contains tuples of words and parts of speech. The list of part of speech tags is available here: <a href="https://universaldependencies.org/u/pos/" class="calibre6 pcalibre pcalibre1">https://universaldependencies.org/u/pos/</a>.</p>
			<h2 id="_idParaDest-31" class="calibre5"><a id="_idTextAnchor031" class="calibre6 pcalibre pcalibre1"/>There’s more</h2>
			<p class="calibre3">We can compare spaCy’s performance to NLTK in this task. Here are the steps for getting the parts of speech with NLTK:</p>
			<ol class="calibre13">
				<li class="calibre14">The imports have been taken care of in the language <strong class="source-inline1">util</strong> file that we imported, so the first thing we do is create a function that outputs parts of speech for the words that are input. In it, we utilize the <strong class="source-inline1">word_tokenize_nltk</strong> function that is also imported from the language <strong class="source-inline1">util</strong> notebook:<pre class="source-code">
def pos_tag_nltk(text):
    words = word_tokenize_nltk(text)
    words_with_pos = nltk.pos_tag(words)
    return words_with_pos</pre></li>				<li class="calibre14">Next, we apply the function to the text that we read in previously:<pre class="source-code">
words_with_pos = pos_tag_nltk(text)</pre></li>				<li class="calibre14">Print out the result:<pre class="source-code">
print(words_with_pos)</pre><p class="calibre3">Part of the output is shown in the following. For the complete output, please see the Jupyter notebook:</p><pre class="source-code">[('To', 'TO'),
 ('Sherlock', 'NNP'),
 ('Holmes', 'NNP'),
 ('she', 'PRP'),
 ('is', 'VBZ'),
 ('always', 'RB'),
 ('_the_', 'JJ'),
 ('woman', 'NN'),
 ('.', '.'),
 ('I', 'PRP'),
 ('have', 'VBP'),
 ('seldom', 'VBN'),
 ('heard', 'RB'),
 ('him', 'PRP'),
 ('mention', 'VB'),
 ('her', 'PRP'),
 ('under', 'IN'),
 ('any', 'DT'),
 ('other', 'JJ'),
 ('name', 'NN'),
 ('.', '.'),…</pre></li>			</ol>
			<p class="calibre3">The list of part of speech <a id="_idIndexMarker035" class="calibre6 pcalibre pcalibre1"/>tags that NLTK uses is different from what SpaCy uses, and can be accessed by running the following commands:</p>
			<pre class="console">
python
&gt;&gt;&gt; import nltk
&gt;&gt;&gt; nltk.download('tagsets')
&gt;&gt;&gt; nltk.help.upenn_tagset()</pre>			<p class="calibre3">Comparing the performance, we see that spaCy takes 0.02 seconds, while NLTK takes 0.01 seconds (your numbers might be different), so their performance is similar, with NLTK being a<a id="_idIndexMarker036" class="calibre6 pcalibre pcalibre1"/> little better. However, the part of speech information is already available in the spaCy objects after the initial processing has been done, so if you are doing any further processing, spaCy is a better choice.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">spaCy does all of its processing at once, and the results are stored in the <strong class="source-inline1">Doc</strong> object. The part of speech information is available by iterating through <strong class="source-inline1">Token</strong> objects.</p>
			<h2 id="_idParaDest-32" class="calibre5"><a id="_idTextAnchor032" class="calibre6 pcalibre pcalibre1"/>There’s more</h2>
			<p class="calibre3">We can <a id="_idIndexMarker037" class="calibre6 pcalibre pcalibre1"/>use the OpenAI API with the GPT-3.5 and GPT-4 models to perform various tasks, including many NLP ones. Here, we show how to use the OpenAI API to get NLTK-style parts of speech for input text. You can also specify in the prompt the output format and the style of the part of speech tags. For this code to run correctly, you will need your own OpenAI API key:</p>
			<ol class="calibre13">
				<li class="calibre14">Import <strong class="source-inline1">openai</strong> and create the OpenAI client using your API key. The <strong class="source-inline1">OPEN_AI_KEY</strong> constant variable is set in the <strong class="source-inline1">../</strong><strong class="source-inline1">util/file_utils.ipynb</strong> file:<pre class="source-code">
from openai import OpenAI
client = OpenAI(api_key=OPEN_AI_KEY)</pre></li>				<li class="calibre14">Set the prompt:<pre class="source-code">
prompt="""Decide what the part of speech tags are for a sentence.
Preserve original capitalization.
Return the list in the format of a python tuple: (word, part of speech).
Sentence: In his eyes she eclipses and predominates the whole of her sex."""</pre></li>				<li class="calibre14">Send the request to the OpenAI API. Some of the important parameters that we send to the <a id="_idIndexMarker038" class="calibre6 pcalibre pcalibre1"/>API are the model we want to use, the temperature, which affects how much the response from the model will vary, and the maximum amount of tokens the model should return as a completion:<pre class="source-code">
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    temperature=0,
    max_tokens=256,
    top_p=1.0,
    frequency_penalty=0,
    presence_penalty=0,
    messages=[
        {"role": "system", 
         "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ],
)</pre></li>				<li class="calibre14">Print the response:<pre class="source-code">
print(response)</pre><p class="calibre3">The output will look like this:</p><pre class="source-code">ChatCompletion(id='chatcmpl-9hCq34UAzMiNiqNGopt2U8ZmZM5po', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are the part of speech tags for the sentence "In his eyes she eclipses and predominates the whole of her sex" in the format of a Python tuple:\n\n[(\'In\', \'IN\'), (\'his\', \'PRP$\'), (\'eyes\', \'NNS\'), (\'she\', \'PRP\'), (\'eclipses\', \'VBZ\'), (\'and\', \'CC\'), (\'predominates\', \'VBZ\'), (\'the\', \'DT\'), (\'whole\', \'JJ\'), (\'of\', \'IN\'), (\'her\', \'PRP$\'), (\'sex\', \'NN\')]', role='assistant', function_call=None, tool_calls=None))], created=1720084483, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=120, prompt_tokens=74, total_tokens=194))</pre></li>				<li class="calibre14">To see<a id="_idIndexMarker039" class="calibre6 pcalibre pcalibre1"/> just the GPT output, do the following:<pre class="source-code">
print(response.choices[0].message.content)</pre><p class="calibre3">The output will be as follows:</p><pre class="source-code">Here are the part of speech tags for the sentence "In his eyes she eclipses and predominates the whole of her sex" in the format of a Python tuple:
[('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'JJ'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN')]</pre></li>				<li class="calibre14">We can use the <strong class="source-inline1">literal_eval</strong> function to transform the response into a tuple. We request that the GPT model return only the answer without additional explanations so that there is no free text inside the answer and we can process it automatically. We do this in order to be able to compare the output of the OpenAI API to the NLTK output:<pre class="source-code">
from ast import literal_eval
def pos_tag_gpt(text, client):
    prompt = f"""Decide what the part of speech tags are for a sentence.
    Preserve original capitalization.
    Return the list in the format of a python tuple: (word, part of speech).
    Do not include any other explanations.
    Sentence: {text}."""
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        temperature=0,
        max_tokens=256,
        top_p=1.0,
        frequency_penalty=0,
        presence_penalty=0,
        messages=[
            {"role": "system", 
             "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ],
    )
    result = response.choices[0].message.content
    result = result.replace("\n", "")
    result = list(literal_eval(result))
    return result</pre></li>				<li class="calibre14">Now, let’s time <a id="_idIndexMarker040" class="calibre6 pcalibre pcalibre1"/>the GPT function so we can compare its performance to the other methods we used previously:<pre class="source-code">
start = time.time()
first_sentence = "In his eyes she eclipses and predominates the whole of her sex."
words_with_pos = pos_tag_gpt(first_sentence, OPEN_AI_KEY)
print(words_with_pos)
print(f"GPT: {time.time() - start} s")</pre><p class="calibre3">The result will look like this:</p><pre class="source-code">[('In', 'IN'), ('his', 'PRP$'), ('eyes', 'NNS'), ('she', 'PRP'), ('eclipses', 'VBZ'), ('and', 'CC'), ('predominates', 'VBZ'), ('the', 'DT'), ('whole', 'NN'), ('of', 'IN'), ('her', 'PRP$'), ('sex', 'NN'), ('.', '.')]
GPT: 2.4942469596862793 s</pre></li>				<li class="calibre14">The output of GPT is very similar to NLTK, but slightly different:<pre class="source-code">
words_with_pos_nltk = pos_tag_nltk(first_sentence)
print(words_with_pos == words_with_pos_nltk)</pre><p class="calibre3">This outputs the following:</p><pre class="source-code">False</pre></li>			</ol>
			<p class="calibre3">The difference between GPT and NLTK is that GPT tags the word whole as an adjective and NLTK tags it as a noun. In this context, NLTK is correct.</p>
			<p class="calibre3">We see that the LLM outputs very similar results but is about 400 times slower than NLTK.</p>
			<h2 id="_idParaDest-33" class="calibre5"><a id="_idTextAnchor033" class="calibre6 pcalibre pcalibre1"/>See also</h2>
			<p class="calibre3">If you would like to tag a text in another language, you can do so by using spaCy’s models for other languages. For example, we can load the Spanish spaCy model to run it on Spanish text:</p>
			<pre class="source-code">
nlp = spacy.load("es_core_news_sm")</pre>			<p class="calibre3">In the case that spaCy doesn’t have a model for the language you are working with, you can train your own model with spaCy. See <a href="https://spacy.io/usage/training#tagger-parser" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/training#tagger-parser</a>.</p>
			<h1 id="_idParaDest-34" class="calibre7"><a id="_idTextAnchor034" class="calibre6 pcalibre pcalibre1"/>Combining similar words – lemmatization</h1>
			<p class="calibre3">We can find the canonical form of<a id="_idIndexMarker041" class="calibre6 pcalibre pcalibre1"/> the word using <strong class="bold">lemmatization</strong>. For<a id="_idIndexMarker042" class="calibre6 pcalibre pcalibre1"/> example, the lemma of the word <em class="italic">cats</em> is <em class="italic">cat</em>, and the lemma for the word <em class="italic">ran</em> is <em class="italic">run</em>. This is useful when we are trying to match some word and don’t want to list out all the <a id="_idIndexMarker043" class="calibre6 pcalibre pcalibre1"/>possible forms. Instead, we can just use its lemma.</p>
			<h2 id="_idParaDest-35" class="calibre5"><a id="_idTextAnchor035" class="calibre6 pcalibre pcalibre1"/>Getting ready</h2>
			<p class="calibre3">We will be using the spaCy package for this recipe.</p>
			<h2 id="_idParaDest-36" class="calibre5"><a id="_idTextAnchor036" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">When the spaCy model processes a piece of text, the resulting <code>Document</code> object contains an iterator over the <code>Token</code> objects within it, as we saw in the <em class="italic">Part of speech tagging</em> recipe. These <code>Token</code> objects contain the lemma information for each word in the text.</p>
			<p class="calibre3">Here are the steps for getting the lemmas:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the file and language <strong class="source-inline1">utils</strong> files. This will import spaCy and initialize the <strong class="source-inline1">small_model</strong> object:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Create a list <a id="_idIndexMarker044" class="calibre6 pcalibre pcalibre1"/>of words we want to lemmatize:<pre class="source-code">
words = ["leaf", "leaves", "booking", "writing", "completed", "stemming"]</pre></li>				<li class="calibre14">Create a <strong class="source-inline1">Document</strong> object for each of the words:<pre class="source-code">
docs = [small_model(word) for word in words]</pre></li>				<li class="calibre14">Print the words and their lemmas for each of the words in the list:<pre class="source-code">
for doc in docs:
    for token in doc:
        print(token, token.lemma_)</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">leaf leaf
leaves leave
booking book
writing write
completed complete
stemming stem</pre><p class="calibre3">The result shows correct lemmatization for all words. However, some words are ambiguous. For example, the word <em class="italic">leaves</em> could either be a verb, in which case the lemma is correct, or a noun, in which case this is the wrong lemma. If we give the spaCy continuous text instead of individual words, it is likely to correctly disambiguate the words.</p></li>				<li class="calibre14">Now, apply<a id="_idIndexMarker045" class="calibre6 pcalibre pcalibre1"/> lemmatization to the longer text. Here, we read in a small portion of the <em class="italic">Sherlock Holmes</em> text and lemmatize its every word:<pre class="source-code">
Text = read_text_file(../data/sherlock_holmes_1.txt")
doc = small_model(text)
for token in doc:
    print(token, token.lemma_)</pre><p class="calibre3">The partial result will be as follows:</p><pre class="source-code">To to
Sherlock Sherlock
Holmes Holmes
she she
is be
always always
_ _
the the
_ _
woman woman
. ….</pre></li>			</ol>
			<h2 id="_idParaDest-37" class="calibre5"><a id="_idTextAnchor037" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can use the spaCy lemmatizer object to find out whether a word is in its base form or not. We might do this while manipulating the grammar of the sentence, for example, in the task of turning a passive sentence into an active one. We can get to the lemmatizer object by manipulating the spaCy pipeline, which includes various tools that are applied to the text. See <a href="https://spacy.io/usage/processing-pipelines/" class="calibre6 pcalibre pcalibre1">https://spacy.io/usage/processing-pipelines/</a> for more information. Here are the steps:</p>
			<ol class="calibre13">
				<li class="calibre14">The pipeline components are located in a list of tuples, <strong class="source-inline1">(component name, component)</strong>. To get the lemmatizer component, we need to loop through this list:<pre class="source-code">
lemmatizer = None
for name, proc in small_model.pipeline:
    if name == "lemmatizer":
        lemmatizer = proc</pre></li>				<li class="calibre14">Now, we can apply the <strong class="source-inline1">is_base_form</strong> function call to every word in the Sherlock Holmes text:<pre class="source-code">
for token in doc:
    print(f"{token} is in its base form: 
        {lemmatizer.is_base_form(token)}")</pre><p class="calibre3">The partial result will be as follows:</p><pre class="source-code">To is in its base form: False
Sherlock is in its base form: False
Holmes is in its base form: False
she is in its base form: False
is is in its base form: False
always is in its base form: False
_ is in its base form: False
the is in its base form: False
_ is in its base form: False
woman is in its base form: True
. is in its base form: False…</pre></li>			</ol>
			<h1 id="_idParaDest-38" class="calibre7"><a id="_idTextAnchor038" class="calibre6 pcalibre pcalibre1"/>Removing stopwords</h1>
			<p class="calibre3">When we<a id="_idIndexMarker046" class="calibre6 pcalibre pcalibre1"/> work with words, especially if we are considering the words’ semantics, we sometimes need to exclude some very frequent words that do not bring any substantial meaning into the sentence (words such as <em class="italic">but</em>, <em class="italic">can</em>, <em class="italic">we</em>, etc.). For example, if we want to get a rough sense of the topic of a text, we could count its most frequent words. However, in any text, the most frequent words will be stopwords, so we want to remove them before processing. This recipe shows how to do that. The stopwords list we are using in this recipe comes from the NLTK package and might not include all the words you need. You will need to modify the list accordingly.</p>
			<p class="calibre3">Getting ready</p>
			<p class="calibre3">We will remove stopwords using spaCy and NLTK; these packages are part of the Poetry environment that we installed earlier.</p>
			<p class="calibre3">We will be using the <em class="italic">Sherlock Holmes</em> text referred to earlier. For this recipe, we will need just the beginning of the book, which can be found in the file at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes_1.txt</a>.</p>
			<p class="calibre3">In <em class="italic">step 1</em>, we run the utilities notebooks. In <em class="italic">step 2</em>, we import the <code>nltk</code> package and its stopwords list. In <em class="italic">step 3</em>, we download the stopwords data, if necessary. In <em class="italic">step 4</em>, we print out the stopwords list. In <em class="italic">step 5</em>, we read in a small portion of the <em class="italic">Sherlock Holmes</em> book. In <em class="italic">step 6</em>, we tokenize the text and print its length, which is 230. In <em class="italic">step 7</em>, we remove the stopwords from the original words list by using a list comprehension. Then, we print the length of the result and see that the list length has been reduced to 105. You will notice that in the list comprehension, we check whether the <em class="italic">lowercase</em> version of the word is in the stopwords list since all the stopwords are lowercase.</p>
			<h2 id="_idParaDest-39" class="calibre5"><a id="_idTextAnchor039" class="calibre6 pcalibre pcalibre1"/>How to do it…</h2>
			<p class="calibre3">In the recipe, we<a id="_idIndexMarker047" class="calibre6 pcalibre pcalibre1"/> will read in the text file, tokenize the text, and remove the stopwords from the list:</p>
			<ol class="calibre13">
				<li class="calibre14">Run the file and language utilities notebooks:<pre class="source-code">
%run -i "../util/file_utils.ipynb"
%run -i "../util/lang_utils.ipynb"</pre></li>				<li class="calibre14">Import the NLTK stopwords list:<pre class="source-code">
from nltk.corpus import stopwords</pre></li>				<li class="calibre14">The first time you run the notebook, download the <strong class="source-inline1">stopwords</strong> data. You don’t need to download the stopwords again the next time you run the code:<pre class="source-code">
nltk.download('stopwords')</pre></li>			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Here is a list of languages that NLTK supports for stopwords: Arabic, Azerbaijani, Danish, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Kazakh, Nepali, Norwegian, Portuguese, Romanian, Russian, Spanish, Swedish, and Turkish.</p>
			<ol class="calibre13">
				<li value="4" class="calibre14">You can see the stopwords that come with NLTK by printing the list:<pre class="source-code">
print(stopwords.words('english'))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]</pre></li>				<li class="calibre14">Read in the text file:<pre class="source-code">
text = read_text_file("../data/sherlock_holmes_1.txt")</pre></li>				<li class="calibre14">Tokenize<a id="_idIndexMarker048" class="calibre6 pcalibre pcalibre1"/> the text and print the length of the resulting list:<pre class="source-code">
words = word_tokenize_nltk(text)
print(len(words))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">230</pre></li>				<li class="calibre14">Remove the stopwords from the list using a list comprehension and print the length of the result. You will notice that in the list comprehension, we check whether the <em class="italic">lowercase</em> version of the word is in the stopwords list since all the stopwords are lowercase.<pre class="source-code">
words = [word for word in words if word not in stopwords.words("english")]
print(len(words))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">105</pre></li>			</ol>
			<p class="calibre3">The code then filters<a id="_idIndexMarker049" class="calibre6 pcalibre pcalibre1"/> the stopwords from the text and leaves the words from the text only if they do not also appear in the stopwords list. As we see from the lengths of the two lists, one unfiltered and the other without the stopwords, we remove more than half the words.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You might find that some of the words in the stopwords list provided are not necessary or are missing. You will need to modify the list accordingly. The NLTK stopwords list is a Python list and you can add and remove elements using the standard Python list functions.</p>
			<h2 id="_idParaDest-40" class="calibre5"><a id="_idTextAnchor040" class="calibre6 pcalibre pcalibre1"/>There’s more…</h2>
			<p class="calibre3">We can also remove stopwords using spaCy. Here is how to do it:</p>
			<ol class="calibre13">
				<li class="calibre14">Assign the stopwords to a variable for convenience:<pre class="source-code">
stopwords = small_model.Defaults.stop_words</pre></li>				<li class="calibre14">Tokenize the text and print its length:<pre class="source-code">
words = word_tokenize_nltk(text)
print(len(words))</pre><p class="calibre3">It will give us the following result:</p><pre class="source-code">230</pre></li>				<li class="calibre14">Remove the <a id="_idIndexMarker050" class="calibre6 pcalibre pcalibre1"/>stopwords from the list using a list comprehension and print the resulting length:<pre class="source-code">
words = [word for word in words if word.lower() not in stopwords]
print(len(words))</pre><p class="calibre3">The result will be very similar to NLTK :</p><pre class="source-code">106</pre></li>				<li class="calibre14">The stopwords from spaCy are stored in a set and we can add more words to it:<pre class="source-code">
print(len(stopwords))
stopwords.add("new")
print(len(stopwords))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">327
328</pre><p class="calibre3">Similarly, we can remove words if necessary:</p><pre class="source-code">print(len(stopwords))
stopwords.remove("new")
print(len(stopwords))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">328
327</pre></li>			</ol>
			<p class="calibre3">We can also compile a stopwords list using the text we are working with and calculate the frequencies of the words in it. This provides you with an automatic way of removing stopwords, without <a id="_idIndexMarker051" class="calibre6 pcalibre pcalibre1"/>the need for manual review.</p>
			<h2 id="_idParaDest-41" class="calibre5"><a id="_idTextAnchor041" class="calibre6 pcalibre pcalibre1"/>There's still more</h2>
			<p class="calibre3">In this section, we will show you two ways of doing so. You will need to use the file at <a href="https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt" class="calibre6 pcalibre pcalibre1">https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/blob/main/data/sherlock_holmes.txt</a>. The <code>FreqDist</code> object in the NLTK package counts the number of occurrences of each word that we later use to find the most frequent words and remove them as stopwords:</p>
			<ol class="calibre13">
				<li class="calibre14">Import the NTLK <strong class="source-inline1">FreqDist</strong> class:<pre class="source-code">
from nltk.probability import FreqDist</pre></li>				<li class="calibre14">Define the function that will compile a list of stopwords:<pre class="source-code">
def compile_stopwords_list_frequency(text, cut_off=0.02):
    words = word_tokenize_nltk(text)
    freq_dist = FreqDist(word.lower() for word in words)
    words_with_frequencies = [
        (word, freq_dist[word]) for word in freq_dist.keys()]
    sorted_words = sorted(words_with_frequencies, 
        key=lambda tup: tup[1])
    stopwords = []
    if (type(cut_off) is int):
        # First option: use a frequency cutoff
        stopwords = [tuple[0] for tuple in sorted_words 
            if tuple[1] &gt; cut_off]
    elif (type(cut_off) is float):
        # Second option: use a percentage of the words
        length_cutoff = int(cut_off*len(sorted_words))
        stopwords = [tuple[0] for tuple in 
            sorted_words[-length_cutoff:]]
    else:
        raise TypeError("The cut off needs to be either a float (percentage) or an int (frequency cut off)")
    return stopwords</pre></li>				<li class="calibre14">Define the <a id="_idIndexMarker052" class="calibre6 pcalibre pcalibre1"/>stopwords list using the default settings, and print out the result and its length:<pre class="source-code">
text = read_text_file("../data/sherlock_holmes.txt")
stopwords = compile_stopwords_list_frequency(text)
print(stopwords)
print(len(stopwords))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">['make', 'myself', 'night', 'until', 'street', 'few', 'why', 'thought', 'take', 'friend', 'lady', 'side', 'small', 'still', 'these', 'find', 'st.', 'every', 'watson', 'too', 'round', 'young', 'father', 'left', 'day', 'yet', 'first', 'once', 'took', 'its', 'eyes', 'long', 'miss', 'through', 'asked', 'most', 'saw', 'oh', 'morning', 'right', 'last', 'like', 'say', 'tell', 't', 'sherlock', 'their', 'go', 'own', 'after', 'away', 'never', 'good', 'nothing', 'case', 'however', 'quite', 'found', 'made', 'house', 'such', 'heard', 'way', 'yes', 'hand', 'much', 'matter', 'where', 'might', 'just', 'room', 'any', 'face', 'here', 'back', 'door', 'how', 'them', 'two', 'other', 'came', 'time', 'did', 'than', 'come', 'before', 'must', 'only', 'know', 'about', 'shall', 'think', 'more', 'over', 'us', 'well', 'am', 'or', 'may', 'they', ';', 'our', 'should', 'now', 'see', 'down', 'can', 'some', 'if', 'will', 'mr.', 'little', 'who', 'into', 'do', 'has', 'could', 'up', 'man', 'out', 'when', 'would', 'an', 'are', 'by', '!', 'were', 's', 'then', 'one', 'all', 'on', 'no', 'what', 'been', 'your', 'very', 'him', 'her', 'she', 'so', ''', 'holmes', 'upon', 'this', 'said', 'from', 'there', 'we', 'me', 'be', 'but', 'not', 'for', '?', 'at', 'which', 'with', 'had', 'as', 'have', 'my', ''', 'is', 'his', 'was', 'you', 'he', 'it', 'that', 'in', '"', 'a', 'of', 'to', '"', 'and', 'i', '.', 'the', ',']
181</pre></li>				<li class="calibre14">Now, use the function<a id="_idIndexMarker053" class="calibre6 pcalibre pcalibre1"/> with the frequency cut-off of 5% (use the top 5% of the most frequent words as stopwords):<pre class="source-code">
text = read_text_file("../data/sherlock_holmes.txt")
stopwords = compile_stopwords_list_frequency(text, cut_off=0.05)
print(len(stopwords))</pre><p class="calibre3">The result will be as follows:</p><pre class="source-code">452</pre></li>				<li class="calibre14">Now, use the absolute frequency cut-off of <strong class="source-inline1">100</strong> (take the words that have a frequency greater than 100):<pre class="source-code">
stopwords = compile_stopwords_list_frequency(text, cut_off=100)
print(stopwords)
print(len(stopwords))</pre><p class="calibre3">And the result is the following:</p><pre class="source-code">['away', 'never', 'good', 'nothing', 'case', 'however', 'quite', 'found', 'made', 'house', 'such', 'heard', 'way', 'yes', 'hand', 'much', 'matter', 'where', 'might', 'just', 'room', 'any', 'face', 'here', 'back', 'door', 'how', 'them', 'two', 'other', 'came', 'time', 'did', 'than', 'come', 'before', 'must', 'only', 'know', 'about', 'shall', 'think', 'more', 'over', 'us', 'well', 'am', 'or', 'may', 'they', ';', 'our', 'should', 'now', 'see', 'down', 'can', 'some', 'if', 'will', 'mr.', 'little', 'who', 'into', 'do', 'has', 'could', 'up', 'man', 'out', 'when', 'would', 'an', 'are', 'by', '!', 'were', 's', 'then', 'one', 'all', 'on', 'no', 'what', 'been', 'your', 'very', 'him', 'her', 'she', 'so', ''', 'holmes', 'upon', 'this', 'said', 'from', 'there', 'we', 'me', 'be', 'but', 'not', 'for', '?', 'at', 'which', 'with', 'had', 'as', 'have', 'my', ''', 'is', 'his', 'was', 'you', 'he', 'it', 'that', 'in', '"', 'a', 'of', 'to', '"', 'and', 'i', '.', 'the', ',']
131</pre></li>			</ol>
			<p class="calibre3">The function<a id="_idIndexMarker054" class="calibre6 pcalibre pcalibre1"/> that creates the stopwords list takes in the text and the <code>cut_off</code> parameter. It could be a float representing the percentage of frequency-ranked words that will be in the stopwords list. Alternatively, it could be an integer that represents the absolute threshold frequency, with words above it considered stopwords. In the function, we first tokenize the words from the book, then create a <code>FreqDist</code> object, and then create a list of tuples (word, word’s frequency) using the frequency distribution. We sort the list using the word frequency. We then check the <code>cut_off</code> parameter’s type and raise an error if it is not a float or an integer. If it is an integer, we return all words whose frequency is higher than the parameter as stopwords. If it is a float, we calculate the number of words to be returned using the parameter as the percentage.</p>
		</div>
	</body></html>