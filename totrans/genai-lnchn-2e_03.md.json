["```py\n    conda create -n langchain-book python=3.11\n    ```", "```py\n    conda activate langchain-book\n    ```", "```py\n    conda install jupyter\n    pip install langchain langchain-openai jupyter\n    ```", "```py\n    jupyter notebook\n    ```", "```py\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"<your token>\"\n```", "```py\nexport OPENAI_API_KEY=<your token>\n```", "```py\nimport os\nOPENAI_API_KEY =  \"... \"\n# I'm omitting all other keys\ndef set_environment():\n    variable_dict = globals().items()\n for key, value in variable_dict:\n if \"API\" in key or \"ID\" in key:\n             os.environ[key] = value\n```", "```py\n# In .gitignore\nconfig.py\n.env\n**/api_keys.txt\n# Other sensitive files\n```", "```py\nfrom config import set_environment\nset_environment()\n```", "```py\nfrom langchain_openai import OpenAI\nfrom langchain_google_genai import GoogleGenerativeAI\n# Initialize OpenAI model\nopenai_llm = OpenAI()\n# Initialize a Gemini model\ngemini_pro = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n```", "```py\n# Either one or both can be used with the same interface\nresponse = openai_llm.invoke(\"Tell me a joke about light bulbs!\")\nprint(response)\n```", "```py\nfrom config import set_environment\nset_environment()\n```", "```py\nWhy did the light bulb go to therapy?\nBecause it was feeling a little dim!\n```", "```py\nresponse = gemini_pro.invoke(\"Tell me a joke about light bulbs!\")\n```", "```py\nWhy did the light bulb get a speeding ticket?\nBecause it was caught going over the watt limit!\n```", "```py\nfrom langchain_community.llms import FakeListLLM\n# Create a fake LLM that always returns the same response\nfake_llm = FakeListLLM(responses=[\"Hello\"])\nresult = fake_llm.invoke(\"Any input will return Hello\")\nprint(result)  # Output: Hello\n```", "```py\nhuman: turn1\nai: answer1\nhuman: turn2\nai: answer2\n```", "```py\n    SystemMessage(content=\"You're a helpful programming assistant\")\n    ```", "```py\n    HumanMessage(content=\"Write a Python function to calculate factorial\")\n    ```", "```py\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.messages import SystemMessage, HumanMessage\nchat = ChatAnthropic(model=\"claude-3-opus-20240229\")\nmessages = [\n```", "```py\n    SystemMessage(content=\"You're a helpful programming assistant\"),\n    HumanMessage(content=\"Write a Python function to calculate factorial\")\n]\nresponse = chat.invoke(messages)\nprint(response)\n```", "```py\n```", "```py\nLet's break that down. The factorial function is designed to take an integer n as input and calculate its factorial. It starts by checking if n is negative, and if so, it raises a ValueError since factorials aren't defined for negative numbers. If n is zero, the function returns 1, which makes sense because, by definition, the factorial of 0 is 1.\nWhen dealing with positive numbers, the function kicks things off by setting a variable result to 1\\. From there, it enters a loop that runs from 1 to n, inclusive, thanks to the range function. During each step of the loop, it multiplies the result by the current number, gradually building up the factorial. Once the loop completes, the function returns the final calculated value. You can call this function by providing a non-negative integer as an argument. Here are a few examples:\n```", "```py\n```", "```py\nNote that the factorial function grows very quickly, so calculating the factorial of large numbers may exceed the maximum representable value in Python. In such cases, you might need to use a different approach or a library that supports arbitrary-precision arithmetic.\n```", "```py\nfrom langchain_openai.chat_models import ChatOpenAI\nchat = ChatOpenAI(model_name='gpt-4o')\n```", "```py\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_core.prompts import ChatPromptTemplate\n# Create a template\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an experienced programmer and mathematical analyst.\"),\n    (\"user\", \"{problem}\")\n])\n# Initialize Claude with extended thinking enabled\nchat = ChatAnthropic(\n    model_name=\"claude-3-7-sonnet-20240326\",  # Use latest model version\n    max_tokens=64_000,                        # Total response length limit\n    thinking={\"type\": \"enabled\", \"budget_tokens\": 15000},  # Allocate tokens for thinking\n)\n# Create and run a chain\nchain = template | chat\n# Complex algorithmic problem\nproblem = \"\"\"\n```", "```py\nDesign an algorithm to find the kth largest element in an unsorted array\nwith the optimal time complexity. Analyze the time and space complexity\nof your solution and explain why it's optimal.\n\"\"\"\n# Get response with thinking included\nresponse = chat.invoke([HumanMessage(content=problem)])\nprint(response.content)\n```", "```py\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are a problem-solving assistant.\"),\n    (\"user\", \"{problem}\")\n])\n# Initialize with reasoning_effort parameter\nchat = ChatOpenAI(\n    model=\"o3-mini\",\"\n    reasoning_effort=\"high\"  # Options: \"low\", \"medium\", \"high\"\n)\nchain = template | chat\nresponse = chain.invoke({\"problem\": \"Calculate the optimal strategy for...\"})\n```", "```py\nchat = ChatOpenAI(model=\"gpt-4o\")\nchain = template | chat\nresponse = chain.invoke({\"problem\": \"Calculate the optimal strategy for...\"})\n```", "```py\nfrom langchain_openai import OpenAI\n# For factual, consistent responses\nfactual_llm = OpenAI(temperature=0.1, max_tokens=256)\n# For creative brainstorming\ncreative_llm = OpenAI(temperature=0.8, top_p=0.95, max_tokens=512)\n```", "```py\n     def generate_prompt(question, context=None):\n     if context:\n     return f\"Context information: {context}\\n\\nAnswer this question concisely: {question}\"\n     return f\"Answer this question concisely: {question}\"\n     # example use:\n          prompt_text = generate_prompt(\"What is the capital of France?\")\n    ```", "```py\n    from langchain_core.prompts import PromptTemplate\n    # Define once, reuse everywhere\n    question_template = PromptTemplate.from_template( \"Answer this question concisely: {question}\" )\n    question_with_context_template = PromptTemplate.from_template( \"Context information: {context}\\n\\nAnswer this question concisely: {question}\" )\n    # Generate prompts by filling in variables\n    prompt_text = question_template.format(question=\"What is the capital of France?\")\n    ```", "```py\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\ntemplate = ChatPromptTemplate.from_messages([\n    (\"system\", \"You are an English to French translator.\"),\n    (\"user\", \"Translate this to French: {text}\")\n])\nchat = ChatOpenAI()\nformatted_messages = template.format_messages(text=\"Hello, how are you?\")\nresponse = chat.invoke(formatted_messages)\nprint(response)\n```", "```py\n# 1\\. Basic sequential chain: Just prompt to LLM\nbasic_chain = prompt | llm | StrOutputParser()\n```", "```py\nchain = RunnableSequence(first= prompt, middle=[llm], last= output_parser)\nLCEL also supports adding transformations and custom functions:\nwith_transformation = prompt | llm | (lambda x: x.upper()) | StrOutputParser()\n```", "```py\ndecision_chain = prompt | llm | (lambda x: route_based_on_content(x)) | {\n \"summarize\": summarize_chain,\n \"analyze\": analyze_chain\n}\n```", "```py\n# Function to Runnable\nlength_func = lambda x: len(x)\nchain = prompt | length_func | output_parser\n# Is converted to:\nchain = prompt | RunnableLambda(length_func) | output_parser\n```", "```py\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_openai import ChatOpenAI\n# Create components\nprompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\nllm = ChatOpenAI()\noutput_parser = StrOutputParser()\n```", "```py\n# Chain them together using LCEL\nchain = prompt | llm | output_parser\n#  Execute the workflow with a single call\nresult = chain.invoke({\"topic\": \"programming\"})\nprint(result)\n```", "```py\nWhy don't programmers like nature?\nIt has too many bugs!\n```", "```py\nformatted_prompt = prompt.invoke({\"topic\": \"programming\"})\nllm_output = llm.invoke(formatted_prompt)\nresult = output_parser.invoke(llm_output)\n```", "```py\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_google_genai import GoogleGenerativeAI\nfrom langchain_core.output_parsers import StrOutputParser\n# Initialize the model\nllm = GoogleGenerativeAI(model=\"gemini-1.5-pro\")\n```", "```py\n# First chain generates a story\nstory_prompt = PromptTemplate.from_template(\"Write a short story about {topic}\")\nstory_chain = story_prompt | llm | StrOutputParser()\n# Second chain analyzes the story\nanalysis_prompt = PromptTemplate.from_template(\n \"Analyze the following story's mood:\\n{story}\"\n)\nanalysis_chain = analysis_prompt | llm | StrOutputParser()\n```", "```py\n# Combine chains\nstory_with_analysis = story_chain | analysis_chain\n# Run the combined chain\nstory_analysis = story_with_analysis.invoke({\"topic\": \"a rainy day\"})\nprint(\"\\nAnalysis:\", story_analysis)\n```", "```py\nAnalysis: The mood of the story is predominantly **calm, peaceful, and subtly romantic.** There's a sense of gentle melancholy brought on by the rain and the quiet emptiness of the bookshop, but this is balanced by a feeling of warmth and hope.\n```", "```py\nfrom langchain_core.runnables import RunnablePassthrough\n# Using RunnablePassthrough.assign to preserve data\nenhanced_chain = RunnablePassthrough.assign(\n    story=story_chain  # Add 'story' key with generated content\n).assign(\n    analysis=analysis_chain  # Add 'analysis' key with analysis of the story\n)\n# Execute the chain\n```", "```py\nresult = enhanced_chain.invoke({\"topic\": \"a rainy day\"})\nprint(result.keys())  # Output: dict_keys(['topic', 'story', 'analysis'])  # dict_keys(['topic', 'story', 'analysis'])\n```", "```py\nfrom operator import itemgetter\n# Alternative approach using dictionary construction\nmanual_chain = (\n    RunnablePassthrough() |  # Pass through input\n    {\n \"story\": story_chain,  # Add story result\n \"topic\": itemgetter(\"topic\")  # Preserve original topic\n    } |\n    RunnablePassthrough().assign(  # Add analysis based on story\n        analysis=analysis_chain\n    )\n)\nresult = manual_chain.invoke({\"topic\": \"a rainy day\"})\nprint(result.keys())  # Output: dict_keys(['story', 'topic', 'analysis'])\n```", "```py\n# Simplified dictionary construction\nsimple_dict_chain = story_chain | {\"analysis\": analysis_chain}\nresult = simple_dict_chain.invoke({\"topic\": \"a rainy day\"}) print(result.keys()) # Output: dict_keys(['analysis', 'output'])\n```", "```py\n    pip install langchain-ollama\n    ```", "```py\n    ollama pull deepseek-r1:1.5b\n    ```", "```py\n    ollama serve\n    ```", "```py\nfrom langchain_ollama import ChatOllama\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\n# Initialize Ollama with your chosen model\nlocal_llm = ChatOllama(\n    model=\"deepseek-r1:1.5b\",\n    temperature=0,\n)\n# Create an LCEL chain using the local model\nprompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\nlocal_chain = prompt | local_llm | StrOutputParser()\n# Use the chain with your local model\nresult = local_chain.invoke({\"concept\": \"quantum computing\"})\nprint(result)\n```", "```py\nfrom langchain_core.messages import SystemMessage, HumanMessage\nfrom langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n# Create a pipeline with a small model:\nllm = HuggingFacePipeline.from_model_id(\n    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n    task=\"text-generation\",\n    pipeline_kwargs=dict(\n        max_new_tokens=512,\n        do_sample=False,\n        repetition_penalty=1.03,\n    ),\n)\nchat_model = ChatHuggingFace(llm=llm)\n# Use it like any other LangChain LLM\nmessages = [\n    SystemMessage(content=\"You're a helpful assistant\"),\n    HumanMessage(\n        content=\"Explain the concept of machine learning in simple terms\"\n    ),\n]\nai_msg = chat_model.invoke(messages)\nprint(ai_msg.content)\n```", "```py\n    #  Configure model with optimized memory and processing settings\n    from langchain_ollama import ChatOllama\n    llm = ChatOllama(\n      model=\"mistral:q4_K_M\", # 4-bit quantized model (smaller memory footprint)\n      num_gpu=1, # Number of GPUs to utilize (adjust based on hardware)\n     num_thread=4 # Number of CPU threads for parallel processing\n    )\n    ```", "```py\ndef safe_model_call(llm, prompt, max_retries=2):\n \"\"\"Safely call a local model with retry logic and graceful\n    failure\"\"\"\n    retries = 0\n while retries <= max_retries:\n try:\n return llm.invoke(prompt)\n except RuntimeError as e:\n # Common error with local models when running out of VRAM\n if \"CUDA out of memory\" in str(e):\n print(f\"GPU memory error, waiting and retrying ({retries+1}/{max_retries+1})\")\n                time.sleep(2)  # Give system time to free resources\n                retries += 1\n else:\n print(f\"Runtime error: {e}\")\n return \"An error occurred while processing your request.\"\n except Exception as e:\n print(f\"Unexpected error calling model: {e}\")\n return \"An error occurred while processing your request.\"\n # If we exhausted retries\n return \"Model is currently experiencing high load. Please try again later.\"\n# Use the safety wrapper in your LCEL chain\nfrom langchain_core.prompts import PromptTemplate\n```", "```py\nfrom langchain_core.runnables import RunnableLambda\nprompt = PromptTemplate.from_template(\"Explain {concept} in simple terms\")\nsafe_llm = RunnableLambda(lambda x: safe_model_call(llm, x))\nsafe_chain = prompt | safe_llm\nresponse = safe_chain.invoke({\"concept\": \"quantum computing\"})\n```", "```py\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\ndalle = DallEAPIWrapper(\n   model_name=\"dall-e-3\",  # Options: \"dall-e-2\" (default) or \"dall-e-3\"\n   size=\"1024x1024\",       # Image dimensions\n    quality=\"standard\",     # \"standard\" or \"hd\" for DALL-E 3\n    n=1 # Number of images to generate (only for DALL-E 2)\n)\n# Generate an image\nimage_url = dalle.run(\"A detailed technical diagram of a quantum computer\")\n# Display the image in a notebook\nfrom IPython.display import Image, display\ndisplay(Image(url=image_url))\n# Or save it locally\nimport requests\nresponse = requests.get(image_url)\n```", "```py\nwith open(\"generated_library.png\", \"wb\") as f:\n    f.write(response.content)\n```", "```py\nfrom langchain_community.llms import Replicate\n# Initialize the text-to-image model with Stable Diffusion 3.5 Large\ntext2image = Replicate(\n    model=\"stability-ai/stable-diffusion-3.5-large\",\n    model_kwargs={\n \"prompt_strength\": 0.85,\n \"cfg\": 4.5,\n \"steps\": 40,\n \"aspect_ratio\": \"1:1\",\n \"output_format\": \"webp\",\n \"output_quality\": 90\n    }\n)\n# Generate an image\nimage_url = text2image.invoke(\n \"A detailed technical diagram of an AI agent\"\n)\n```", "```py\nimport base64\nfrom langchain_google_genai.chat_models import ChatGoogleGenerativeAI\nfrom langchain_core.messages.human import HumanMessage\nwith open(\"stable-diffusion.png\", 'rb') as image_file:\n    image_bytes = image_file.read()\n    base64_bytes = base64.b64encode(image_bytes).decode(\"utf-8\")\nprompt = [\n   {\"type\": \"text\", \"text\": \"Describe the image: \"},\n   {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_bytes}\"}},\n]\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-1.5-pro\",\n    temperature=0,\n)\nresponse = llm.invoke([HumanMessage(content=prompt)])\nprint(response.content)\nThe image presents a futuristic, stylized depiction of a humanoid robot's upper body against a backdrop of glowing blue digital displays. The robot's head is rounded and predominantly white, with sections of dark, possibly metallic, material around the face and ears.  The face itself features glowing orange eyes and a smooth, minimalist design, lacking a nose or mouth in the traditional human sense.  Small, bright dots, possibly LEDs or sensors, are scattered across the head and body, suggesting advanced technology and intricate construction.\nThe robot's neck and shoulders are visible, revealing a complex internal structure of dark, interconnected parts, possibly wires or cables, which contrast with the white exterior. The shoulders and upper chest are also white, with similar glowing dots and hints of the internal mechanisms showing through. The overall impression is of a sleek, sophisticated machine.\n```", "```py\nThe background is a grid of various digital interfaces, displaying graphs, charts, and other abstract data visualizations. These elements are all in shades of blue, creating a cool, technological ambiance that complements the robot's appearance. The displays vary in size and complexity, adding to the sense of a sophisticated control panel or monitoring system. The combination of the robot and the background suggests a theme of advanced robotics, artificial intelligence, or data analysis.\n```", "```py\nprompt = [\n   {\"type\": \"text\", \"text\": \"Describe the video in a few sentences.\"},\n   {\"type\": \"media\", \"file_uri\": video_uri, \"mime_type\": \"video/mp4\"},\n]\nresponse = llm.invoke([HumanMessage(content=prompt)])\nprint(response.content)\n```", "```py\noffset_hint = {\n \"start_offset\": {\"seconds\": 10},\n \"end_offset\": {\"seconds\": 20},\n       }\nprompt = [\n   {\"type\": \"text\", \"text\": \"Describe the video in a few sentences.\"},\n   {\"type\": \"media\", \"file_uri\": video_uri, \"mime_type\": \"video/mp4\", \"video_metadata\": offset_hint},\n]\nresponse = llm.invoke([HumanMessage(content=prompt)])\nprint(response.content)\n```", "```py\nprompt = ChatPromptTemplate.from_messages(\n   [(\"user\",\n    [{\"type\": \"image_url\",\n \"image_url\": {\"url\": \"data:image/jpeg;base64,{image_bytes_str}\"},\n      }])]\n)\nprompt.invoke({\"image_bytes_str\": \"test-url\"})\n```", "```py\nfrom langchain_core.messages import HumanMessage\nfrom langchain_openai import ChatOpenAI\ndef analyze_image(image_url: str, question: str) -> str:\n    chat = ChatOpenAI(model=\"gpt-4o-mini\", max_tokens=256)\n\n    message = HumanMessage(\n        content=[\n            {\n \"type\": \"text\",\n \"text\": question\n            },\n            {\n \"type\": \"image_url\",\n \"image_url\": {\n \"url\": image_url,\n \"detail\": \"auto\"\n                }\n            }\n```", "```py\n        ]\n    )\n\n    response = chat.invoke([message])\n return response.content\n# Example usage\nimage_url = \"https://replicate.delivery/yhqm/pMrKGpyPDip0LRciwSzrSOKb5ukcyXCyft0IBElxsT7fMrLUA/out-0.png\"\nquestions = [\n \"What objects do you see in this image?\",\n \"What is the overall mood or atmosphere?\",\n \"Are there any people in the image?\"\n]\nfor question in questions:\n print(f\"\\nQ: {question}\")\n print(f\"A: {analyze_image(image_url, question)}\")\n```", "```py\nQ: What objects do you see in this image?\nA: The image features a futuristic cityscape with tall, sleek skyscrapers. The buildings appear to have a glowing or neon effect, suggesting a high-tech environment. There is a large, bright sun or light source in the sky, adding to the vibrant atmosphere. A road or pathway is visible in the foreground, leading toward the city, possibly with light streaks indicating motion or speed. Overall, the scene conveys a dynamic, otherworldly urban landscape.\nQ: What is the overall mood or atmosphere?\nA: The overall mood or atmosphere of the scene is futuristic and vibrant. The glowing outlines of the skyscrapers and the bright sunset create a sense of energy and possibility. The combination of deep colors and light adds a dramatic yet hopeful tone, suggesting a dynamic and evolving urban environment.\nQ: Are there any people in the image?\nA: There are no people in the image. It appears to be a futuristic cityscape with tall buildings and a sunset.\n```", "```py\n        from langchain_openai import ChatOpenAI\n        chat = ChatOpenAI(model=\"gpt-4-vision-preview\")\n        ```", "```py\n        from langchain_community.llms import Ollama\n        local_model = Ollama(model=\"llava\")\n        ```"]