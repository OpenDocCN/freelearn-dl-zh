["```py\n# !conda install -y pandas\n# !conda install -y numpy\n```", "```py\nimport pandas as pd\nimport numpy as np\n```", "```py\ntrain_df = pd.read_csv(\"data/train.csv\")\ntrain_df.head()\n```", "```py\nval_df = pd.read_csv(\"data/valid.csv\")\nval_df.head()\n```", "```py\ntest_df = pd.read_csv(\"data/test.csv\")\ntest_df.head()\n```", "```py\n# !pip install --upgrade git+https://github.com/pytorch/text\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchtext\n```", "```py\nuse_gpu = True\nif use_gpu:\n    assert torch.cuda.is_available(), 'You either do not have a GPU or is not accessible to PyTorch'\n```", "```py\ntorch.cuda.device_count()\n> 1\n```", "```py\nfrom torchtext.data import Field\n```", "```py\nLABEL = Field(sequential=False, use_vocab=False)\n```", "```py\ntokenize = lambda x: x.split()\nTEXT = Field(sequential=True, tokenize=tokenize, lower=True)\n```", "```py\nfrom torchtext.data import TabularDataset\n```", "```py\ntv_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n                 (\"comment_text\", TEXT), (\"toxic\", LABEL),\n                 (\"severe_toxic\", LABEL), (\"threat\", LABEL),\n                 (\"obscene\", LABEL), (\"insult\", LABEL),\n                 (\"identity_hate\", LABEL)]\n```", "```py\ntrn, vld = TabularDataset.splits(\n        path=\"data\", # the root directory where the data lies\n        train='train.csv', validation=\"valid.csv\",\n        format='csv',\n        skip_header=True, # make sure to pass this to ensure header doesn't get proceesed as data!\n        fields=tv_datafields)\n```", "```py\ntst_datafields = [(\"id\", None), # we won't be needing the id, so we pass in None as the field\n                 (\"comment_text\", TEXT)\n                 ]\n```", "```py\ntst = TabularDataset(\n        path=\"data/test.csv\", # the file path\n        format='csv',\n        skip_header=True, # if your csv header has a header, make sure to pass this to ensure it doesn't get proceesed as data!\n        fields=tst_datafields)\n```", "```py\ntrn, vld, tst\n\n> (<torchtext.data.dataset.TabularDataset at 0x1d6c86f1320>,\n <torchtext.data.dataset.TabularDataset at 0x1d6c86f1908>,\n <torchtext.data.dataset.TabularDataset at 0x1d6c86f16d8>)\n```", "```py\ntrn[0], vld[0], tst[0]\n> (<torchtext.data.example.Example at 0x1d6c86f1940>,\n <torchtext.data.example.Example at 0x1d6c86fed30>,\n <torchtext.data.example.Example at 0x1d6c86fecc0>)\n```", "```py\ntrn[0].__dict__.keys()\n> dict_keys(['comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate']\n```", "```py\ntrn[0].__dict__['comment_text'][:5]\n> ['explanation', 'why', 'the', 'edits', 'made']\n```", "```py\nTEXT.build_vocab(trn)\n```", "```py\nTEXT.vocab\n> <torchtext.vocab.Vocab at 0x1d6c65615c0>\n```", "```py\ntype(TEXT.vocab.freqs)\n> collections.Counter\n```", "```py\nTEXT.vocab.freqs.most_common(5)\n> [('the', 78), ('to', 41), ('you', 33), ('of', 30), ('and', 26)]\n```", "```py\ntype(TEX\n\nT.vocab.itos), type(TEXT.vocab.stoi), len(TEXT.vocab.itos), len(TEXT.vocab.stoi.keys())\n> (list, collections.defaultdict, 784, 784)\n```", "```py\nTEXT.vocab.stoi['and'], TEXT.vocab.itos[7]\n> (7, 'and')\n```", "```py\nfrom torchtext.data import Iterator, BucketIterator\n```", "```py\n[ [3, 15, 2, 7], \n  [4, 1], \n  [5, 5, 6, 8, 1] ]\n```", "```py\n[ [3, 15, 2, 7, 0],\n  [4, 1, 0, 0, 0],\n  [5, 5, 6, 8, 1] ]\n```", "```py\ntrain_iter, val_iter = BucketIterator.splits(\n        (trn, vld), # we pass in the datasets we want the iterator to draw data from\n        batch_sizes=(32, 32),\n        sort_key=lambda x: len(x.comment_text), # the BucketIterator needs to be told what function it should use to group the data.\n        sort_within_batch=False,\n        repeat=False # we pass repeat=False because we want to wrap this Iterator layer.\n)\n```", "```py\ntrain_iter\n\n> <torchtext.data.iterator.BucketIterator at 0x1d6c8776518>\n\nbatch = next(train_iter.__iter__())\nbatch\n\n> [torchtext.data.batch.Batch of size 25]\n        [.comment_text]:[torch.LongTensor of size 494x25]\n        [.toxic]:[torch.LongTensor of size 25]\n        [.severe_toxic]:[torch.LongTensor of size 25]\n        [.threat]:[torch.LongTensor of size 25]\n        [.obscene]:[torch.LongTensor of size 25]\n        [.insult]:[torch.LongTensor of size 25]\n        [.identity_hate]:[torch.LongTensor of size 25]\n```", "```py\nbatch.__dict__.keys()\n> dict_keys(['batch_size', 'dataset', 'fields', 'comment_text', 'toxic', 'severe_toxic', 'threat', 'obscene', 'insult', 'identity_hate'])\n```", "```py\nbatch.__dict__['dataset'], trn, batch.__dict__['dataset']==trn\n```", "```py\ntest_iter = Iterator(tst, batch_size=64, sort=False, sort_within_batch=False, repeat=False)\n```", "```py\nnext(test_iter.__iter__())\n> [torchtext.data.batch.Batch of size 33]\n  [.comment_text]:[torch.LongTensor of size 158x33]\n```", "```py\nclass BatchWrapper: \n  def __init__(self, dl, x_var, y_vars): \n      self.dl, self.x_var, self.y_vars = dl, x_var, y_vars # we pass in the list of attributes for x and y\n\n  def __iter__(self): \n      for batch in self.dl: \n          x = getattr(batch, self.x_var) # we assume only one input in this wrapper \n          if self.y_vars is not None: \n                # we will concatenate y into a single tensor \n                y = torch.cat([getattr(batch, feat).unsqueeze(1) for feat in self.y_vars], dim=1).float()\n                 else: y = torch.zeros((1)) if use_gpu: yield (x.cuda(), y.cuda()) else: yield (x, y) \n\n   def __len__(self): return len(self.dl)\n\n```", "```py\ntrain_dl = BatchWrapper(train_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n\nvalid_dl = BatchWrapper(val_iter, \"comment_text\", [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n\ntest_dl = BatchWrapper(test_iter, \"comment_text\", None)\n```", "```py\nnext(train_dl.__iter__())\n\n> (tensor([[ 453,   63,   15,  ...,  454,  660,  778],\n         [ 523,    4,  601,  ...,   78,   11,  650],\n         ...,\n         [   1,    1,    1,  ...,    1,    1,    1],\n         [   1,    1,    1,  ...,    1,    1,    1]], device='cuda:0'),\n tensor([[ 0.,  0.,  0.,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  0.,  0.,  0.],\n         ...,\n         [ 0.,  0.,  0.,  0.,  0.,  0.],\n         [ 0.,  0.,  0.,  0.,  0.,  0.]], device='cuda:0'))\n```", "```py\nclass SimpleLSTMBaseline(nn.Module):\n    def __init__(self, hidden_dim, emb_dim=300,\n                 spatial_dropout=0.05, recurrent_dropout=0.1, num_linear=2):\n        super().__init__() # don't forget to call this!\n        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)\n        self.encoder = nn.LSTM(emb_dim, hidden_dim, num_layers=num_linear, dropout=recurrent_dropout)\n        self.linear_layers = []\n        for _ in range(num_linear - 1):\n            self.linear_layers.append(nn.Linear(hidden_dim, hidden_dim))\n        self.linear_layers = nn.ModuleList(self.linear_layers)\n        self.predictor = nn.Linear(hidden_dim, 6)\n\n    def forward(self, seq):\n        hdn, _ = self.encoder(self.embedding(seq))\n        feature = hdn[-1, :, :]\n        for layer in self.linear_layers:\n            feature = layer(feature)\n        preds = self.predictor(feature)\n        return preds\n```", "```py\nem_sz = 300\nnh = 500\nmodel = SimpleLSTMBaseline(nh, emb_dim=em_sz)\nprint(model)\n\nSimpleLSTMBaseline(\n  (embedding): Embedding(784, 300)\n  (encoder): LSTM(300, 500, num_layers=2, dropout=0.1)\n  (linear_layers): ModuleList(\n    (0): Linear(in_features=500, out_features=500, bias=True)\n  )\n  (predictor): Linear(in_features=500, out_features=6, bias=True)\n)\n```", "```py\ndef model_size(model: torch.nn)->int:\n    \"\"\"\n    Calculates the number of trainable parameters in any model\n\n    Returns:\n        params (int): the total count of all model weights\n    \"\"\"\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n#     model_parameters = model.parameters()\n    params = sum([np.prod(p.size()) for p in model_parameters])\n    return params\n\nprint(f'{model_size(model)/10**6} million parameters')\n> 4.096706 million parameters\n```", "```py\nif use_gpu:\n    model = model.cuda()\n```", "```py\nfrom torch import optim\nopt = optim.Adam(model.parameters(), lr=1e-2)\nloss_func = nn.BCEWithLogitsLoss().cuda()\n```", "```py\nepochs = 3\n```", "```py\nfrom tqdm import tqdm\nfor epoch in range(1, epochs + 1):\n    running_loss = 0.0\n    running_corrects = 0\n    model.train() # turn on training mode\n    for x, y in tqdm(train_dl): # thanks to our wrapper, we can intuitively iterate over our data!\n        opt.zero_grad()\n        preds = model(x)\n        loss = loss_func(preds, y)\n        loss.backward()\n        opt.step()\n\n        running_loss += loss.item() * x.size(0)\n\n    epoch_loss = running_loss / len(trn)\n\n    # calculate the validation loss for this epoch\n    val_loss = 0.0\n    model.eval() # turn on evaluation mode\n    for x, y in valid_dl:\n        preds = model(x)\n        loss = loss_func(preds, y)\n        val_loss += loss.item() * x.size(0)\n\n    val_loss /= len(vld)\n    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n```", "```py\nloss.backward()\n```", "```py\n100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2.34it/s]\n\nEpoch: 1, Training Loss: 13.5037, Validation Loss: 4.6498\n100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 4.58it/s]\n\nEpoch: 2, Training Loss: 7.8243, Validation Loss: 24.5401\n\n100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 3.35it/s]\n\nEpoch: 3, Training Loss: 57.4577, Validation Loss: 4.0107\n```", "```py\ntest_preds = []\nmodel.eval()\nfor x, y in tqdm(test_dl):\n    preds = model(x)\n    # if you're data is on the GPU, you need to move the data back to the cpu\n    preds = preds.data.cpu().numpy()\n    # the actual outputs of the model are logits, so we need to pass these values to the sigmoid function\n    preds = 1 / (1 + np.exp(-preds))\n    test_preds.append(preds)\ntest_preds = np.hstack(test_preds)\n```", "```py\ntest_df = pd.read_csv(\"data/test.csv\")\nfor i, col in enumerate([\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]):\n    test_df[col] = test_preds[:, i]\n```", "```py\ntest_df.head(3)\n```"]