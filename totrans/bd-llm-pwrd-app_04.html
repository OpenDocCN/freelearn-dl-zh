<html><head></head><body>
<div class="calibre1" id="_idContainer096">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">4</span></h1>
<h1 class="chaptertitle" id="_idParaDest-55"><span class="kobospan" id="kobo.2.1">Prompt Engineering</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">In </span><em class="italic"><span class="kobospan" id="kobo.4.1">Chapter 2</span></em><span class="kobospan" id="kobo.5.1">, we introduced the concept of prompt engineering as the process of designing and optimizing prompts – the text</span><a id="_idIndexMarker257" class="calibre3"/><span class="kobospan" id="kobo.6.1"> input that guides the behavior of a </span><strong class="screentext"><span class="kobospan" id="kobo.7.1">large language model</span></strong><span class="kobospan" id="kobo.8.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.9.1">LLM</span></strong><span class="kobospan" id="kobo.10.1">) – for LLMs for a wide variety of applications and research topics. </span><span class="kobospan" id="kobo.10.2">Since prompts have a massive impact on LLM performance, prompt engineering is a crucial activity while designing LLM-powered applications. </span><span class="kobospan" id="kobo.10.3">In fact, there are several techniques that can be implemented not only to refine your LLM’s responses but also to reduce risks associated with hallucination and bias.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.11.1">In this chapter, we are going to cover the emerging techniques in the field of prompt engineering, starting from basic approaches up to advanced frameworks. </span><span class="kobospan" id="kobo.11.2">By the end of this chapter, you will have the foundations to build functional and solid prompts for your LLM-powered applications, which will also be relevant in the upcoming chapters.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.12.1">We will go through the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.13.1">Introduction to prompt engineering</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.14.1">Basic principles of prompt engineering</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.15.1">Advanced techniques of prompt engineering</span></li>
</ul>
<h1 class="heading" id="_idParaDest-56"><span class="kobospan" id="kobo.16.1">Technical requirements</span></h1>
<p class="normal"><span class="kobospan" id="kobo.17.1">To complete the tasks in this chapter, you will require the following:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.18.1">OpenAI account and API</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.19.1">Python 3.7.1 or later version</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.20.1">You can find all the code and examples in the book’s GitHub repository at </span><a href="Chapter_04.xhtml" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.21.1">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</span></span></a><span class="kobospan" id="kobo.22.1">.</span></p>
<h1 class="heading" id="_idParaDest-57"><span class="kobospan" id="kobo.23.1">What is prompt engineering?</span></h1>
<p class="normal"><span class="kobospan" id="kobo.24.1">A prompt is a text input</span><a id="_idIndexMarker258" class="calibre3"/><span class="kobospan" id="kobo.25.1"> that guides the behavior of an LLM to generate a text output.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.26.1">Prompt engineering is the process of designing effective prompts that elicit high-quality and relevant output from LLMs. </span><span class="kobospan" id="kobo.26.2">Prompt engineering requires creativity, understanding of the LLM, and precision.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.27.1">The following figure shows an example of how a well-written prompt can instruct the same model to perform three different tasks:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.28.1"><img alt="" role="presentation" src="../Images/B21714_04_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.29.1">Figure 4.1: Example of prompt engineering to specialize LLMs</span></p>
<p class="normal1"><span class="kobospan" id="kobo.30.1">As you might imagine, the prompt becomes one of the key elements for an LLM-powered application’s success. </span><span class="kobospan" id="kobo.30.2">As such, it is pivotal to invest time and resources in this step, following some best practices and principles that we are going to cover in the next sections.</span></p>
<h1 class="heading" id="_idParaDest-58"><span class="kobospan" id="kobo.31.1">Principles of prompt engineering</span></h1>
<p class="normal"><span class="kobospan" id="kobo.32.1">Generally speaking, there are no fixed</span><a id="_idIndexMarker259" class="calibre3"/><span class="kobospan" id="kobo.33.1"> rules to obtain the “perfect” prompt since there are too many variables to be taken into account (the type of model used, the goal of the application, the supporting infrastructure, and so on). </span><span class="kobospan" id="kobo.33.2">Nevertheless, there are some clear principles that have proven to produce positive effects if incorporated into the prompt. </span><span class="kobospan" id="kobo.33.3">Let’s examine some of them.</span></p>
<h2 class="heading1" id="_idParaDest-59"><span class="kobospan" id="kobo.34.1">Clear instructions</span></h2>
<p class="normal"><span class="kobospan" id="kobo.35.1">The principle of giving clear instructions</span><a id="_idIndexMarker260" class="calibre3"/><span class="kobospan" id="kobo.36.1"> is to provide the model with enough information and guidance to perform the task correctly and efficiently. </span><span class="kobospan" id="kobo.36.2">Clear instructions should include the following elements:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.37.1">The goal or objective of the task, such as “write a poem” or “summarize an article”</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.38.1">The format or structure of the expected output, such as “use four lines with rhyming words” or “use bullet points with no more than 10 words each”</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.39.1">The constraints or limitations of the task, such as “do not use any profanity” or “do not copy any text from the source”</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.40.1">The context or background of the task, such as “the poem is about autumn” or “the article is from a scientific journal”</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.41.1">Let’s say, for example, that we want our model to fetch any kind of instructions from text and return to us a tutorial in a bullet list. </span><span class="kobospan" id="kobo.41.2">Also, if there are no instructions in the provided text, the model should inform us about that. </span><span class="kobospan" id="kobo.41.3">Here are the steps:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.42.1">First, we need to initialize our model. </span><span class="kobospan" id="kobo.42.2">For this purpose, we are going to leverage OpenAI’s GPT-3.5-turbo model. </span><span class="kobospan" id="kobo.42.3">We first install the </span><code class="inlinecode"><span class="kobospan" id="kobo.43.1">openai</span></code><span class="kobospan" id="kobo.44.1"> library:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.45.1">$pip install openai == </span><span class="hljs-number"><span class="kobospan" id="kobo.46.1">0.28</span></span>
</code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.47.1">To initialize the model, I used the </span><code class="inlinecode"><span class="kobospan" id="kobo.48.1">openai</span></code><span class="kobospan" id="kobo.49.1"> Python library and set the OpenAI API key as the environmental variable:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.50.1">import</span></span><span class="kobospan" id="kobo.51.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.52.1">import</span></span><span class="kobospan" id="kobo.53.1"> openai
openai.api_key = os.environment.get('OPENAI_API_KEY')
response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.54.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.55.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.56.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.57.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.58.1">"role"</span></span><span class="kobospan" id="kobo.59.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.60.1">"system"</span></span><span class="kobospan" id="kobo.61.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.62.1">"content"</span></span><span class="kobospan" id="kobo.63.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.64.1">"role"</span></span><span class="kobospan" id="kobo.65.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.66.1">"user"</span></span><span class="kobospan" id="kobo.67.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.68.1">"content"</span></span><span class="kobospan" id="kobo.69.1">: instructions},
    ]
)
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.70.1">As you can see, the chat model comes</span><a id="_idIndexMarker261" class="calibre3"/><span class="kobospan" id="kobo.71.1"> with two variables placeholders: </span><code class="inlinecode"><span class="kobospan" id="kobo.72.1">system message</span></code><span class="kobospan" id="kobo.73.1"> (or metaprompt), where we define how we want our model to behave, and </span><code class="inlinecode"><span class="kobospan" id="kobo.74.1">instructions</span></code><span class="kobospan" id="kobo.75.1"> (or query), where the user will ask the model its questions.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="3"><span class="kobospan" id="kobo.76.1">Then, it takes the user’s query (in this case, the text instructions). </span><span class="kobospan" id="kobo.76.2">For this scenario, I set the two variables </span><code class="inlinecode"><span class="kobospan" id="kobo.77.1">system_message</span></code><span class="kobospan" id="kobo.78.1"> and </span><code class="inlinecode"><span class="kobospan" id="kobo.79.1">instructions</span></code><span class="kobospan" id="kobo.80.1"> as follows:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.81.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.82.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.83.1">You are an AI assistant that helps humans by generating tutorials given a text.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.84.1">You will be provided with a text. </span><span class="kobospan" id="kobo.84.2">If the text contains any kind of istructions on how to proceed with something, generate a tutorial in a bullet list.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.85.1">Otherwise, inform the user that the text does not contain any instructions.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.86.1">Text:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.87.1">"""</span></span><span class="kobospan" id="kobo.88.1">
instructions = </span><span class="hljs-string"><span class="kobospan" id="kobo.89.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.90.1">To prepare the known sauce from Genova, Italy, you can start by toasting the pine nuts to then coarsely</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.91.1">chop them in a kitchen mortar together with basil and garlic. </span><span class="kobospan" id="kobo.91.2">Then, add half of the oil in the kitchen mortar and season with salt and pepper.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.92.1">Finally, transfer the pesto to a bowl and stir in the grated Parmesan cheese.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.93.1">"""</span></span>
</code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.94.1">Now let’s test our model:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.95.1">print</span></span><span class="kobospan" id="kobo.96.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.97.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.98.1">choices'</span></span><span class="kobospan" id="kobo.99.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.100.1">0</span></span><span class="kobospan" id="kobo.101.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.102.1">'message'</span></span><span class="kobospan" id="kobo.103.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.104.1">'content'</span></span><span class="kobospan" id="kobo.105.1">])
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.106.1">We get the following output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.107.1">Sure, I can generate a tutorial based on the given text. </span><span class="kobospan" id="kobo.107.2">Here's a bullet list on how to prepare the Genovese pesto sauce:
- Toast the pine nuts on a pan until they are slightly browned.
</span><span class="kobospan" id="kobo.107.3">- Coarsely chop the toasted pine nuts, along with basil leaves and garlic cloves, using a kitchen mortar.
</span><span class="kobospan" id="kobo.107.4">- Add half of the olive oil into the kitchen mortar, and season the mixture with salt and pepper.
</span><span class="kobospan" id="kobo.107.5">- Grind the mixture until it becomes a smooth paste.
</span><span class="kobospan" id="kobo.107.6">- Transfer the pesto to a bowl, and stir in the grated Parmesan cheese.
</span><span class="kobospan" id="kobo.107.7">- You can add more olive oil to achieve the desired consistency for the sauce.
</span><span class="kobospan" id="kobo.107.8">- Your Genovese pesto sauce is now ready. </span><span class="kobospan" id="kobo.107.9">It can be served with pasta, vegetables, or as a dip for bread. </span><span class="kobospan" id="kobo.107.10">Enjoy!
</span></code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="5"><span class="kobospan" id="kobo.108.1">Note that if we pass the model</span><a id="_idIndexMarker262" class="calibre3"/><span class="kobospan" id="kobo.109.1"> another text that does not contain any instructions, it will be able to respond as we instructed it:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.110.1">response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.111.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.112.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.113.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.114.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.115.1">"role"</span></span><span class="kobospan" id="kobo.116.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.117.1">"system"</span></span><span class="kobospan" id="kobo.118.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.119.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.120.1">content"</span></span><span class="kobospan" id="kobo.121.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.122.1">"role"</span></span><span class="kobospan" id="kobo.123.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.124.1">"user"</span></span><span class="kobospan" id="kobo.125.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.126.1">"content"</span></span><span class="kobospan" id="kobo.127.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.128.1">'the sun is shining and dogs are running on the beach.'</span></span><span class="kobospan" id="kobo.129.1">},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.130.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.131.1">print</span></span><span class="kobospan" id="kobo.132.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.133.1">'choices'</span></span><span class="kobospan" id="kobo.134.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.135.1">0</span></span><span class="kobospan" id="kobo.136.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.137.1">'message'</span></span><span class="kobospan" id="kobo.138.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.139.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.140.1">content'</span></span><span class="kobospan" id="kobo.141.1">])
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.142.1">The following is the corresponding output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.143.1">As there are no instructions provided in the text you have given me, it is not possible to create a tutorial. </span><span class="kobospan" id="kobo.143.2">May I have a different text to work with?
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.144.1">By giving clear instructions, you can help the model understand what you want it to do and how you want it to do it. </span><span class="kobospan" id="kobo.144.2">This can improve the quality and relevance of the model’s output and reduce the need for further revisions or corrections.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.145.1">However, sometimes, there are scenarios</span><a id="_idIndexMarker263" class="calibre3"/><span class="kobospan" id="kobo.146.1"> where clarity is not enough. </span><span class="kobospan" id="kobo.146.2">We might need to infer the way of thinking of our LLM to make it more robust with respect to its task. </span><span class="kobospan" id="kobo.146.3">In the next section, we are going to examine one of these techniques, which will be very useful in the case of accomplishing complex tasks.</span></p>
<h2 class="heading1" id="_idParaDest-60"><span class="kobospan" id="kobo.147.1">Split complex tasks into subtasks</span></h2>
<p class="normal"><span class="kobospan" id="kobo.148.1">As discussed earlier, prompt engineering</span><a id="_idIndexMarker264" class="calibre3"/><span class="kobospan" id="kobo.149.1"> is a technique that involves designing effective inputs for LLMs to perform various tasks. </span><span class="kobospan" id="kobo.149.2">Sometimes, the tasks are too complex or ambiguous for a single prompt to handle, and it is better to split them into simpler subtasks that can be solved by different prompts.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.150.1">Here are some examples of splitting complex tasks into subtasks:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.151.1">Text summarization:</span></strong><span class="kobospan" id="kobo.152.1"> A complex task that involves</span><a id="_idIndexMarker265" class="calibre3"/><span class="kobospan" id="kobo.153.1"> generating a concise and accurate summary of a long text. </span><span class="kobospan" id="kobo.153.2">This task can be split into subtasks such as:</span><ul class="calibre17">
<li class="bulletlist2"><span class="kobospan" id="kobo.154.1">Extracting the main points or keywords from the text</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.155.1">Rewriting the main points or keywords in a coherent and fluent way</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.156.1">Trimming the summary to fit a desired length or format</span></li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.157.1">Machine translation:</span></strong><span class="kobospan" id="kobo.158.1"> A complex task that involves</span><a id="_idIndexMarker266" class="calibre3"/><span class="kobospan" id="kobo.159.1"> translating a text from one language to another. </span><span class="kobospan" id="kobo.159.2">This task can be split into subtasks such as:</span><ul class="calibre17">
<li class="bulletlist2"><span class="kobospan" id="kobo.160.1">Detecting the source language of the text</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.161.1">Converting the text into an intermediate representation that preserves the meaning and structure of the original text</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.162.1">Generating the text in the target language from the intermediate representation</span></li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.163.1">Poem generation</span></strong><span class="kobospan" id="kobo.164.1">: A creative task that involves</span><a id="_idIndexMarker267" class="calibre3"/><span class="kobospan" id="kobo.165.1"> producing a poem that follows a certain style, theme, or mood. </span><span class="kobospan" id="kobo.165.2">This task can be split into subtasks such as:</span><ul class="calibre17">
<li class="bulletlist2"><span class="kobospan" id="kobo.166.1">Choosing a poetic form (such as sonnet, haiku, limerick, etc.) and a rhyme scheme (such as ABAB, AABB, ABCB, etc.) for the poem</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.167.1">Generating a title and a topic for the poem based on the user’s input or preference</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.168.1">Generating the lines or verses of the poem that match the chosen form, rhyme scheme, and topic</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.169.1">Refining and polishing the poem</span><a id="_idIndexMarker268" class="calibre3"/><span class="kobospan" id="kobo.170.1"> to ensure coherence, fluency, and originality</span></li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.171.1">Code generation</span></strong><span class="kobospan" id="kobo.172.1">: A technical task that involves</span><a id="_idIndexMarker269" class="calibre3"/><span class="kobospan" id="kobo.173.1"> producing a code snippet that performs a specific function or task. </span><span class="kobospan" id="kobo.173.2">This task can be split into subtasks such as:</span><ul class="calibre17">
<li class="bulletlist2"><span class="kobospan" id="kobo.174.1">Choosing a programming language (such as Python, Java, C++, etc.) and a framework or library (such as TensorFlow, PyTorch, React, etc.) for the code</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.175.1">Generating a function name and a list of parameters and return values for the code based on the user’s input or specification</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.176.1">Generating the body of the function that implements the logic and functionality of the code</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.177.1">Adding comments and documentation to explain the code and its usage</span></li>
</ul>
</li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.178.1">Let’s consider the following</span><a id="_idIndexMarker270" class="calibre3"/><span class="kobospan" id="kobo.179.1"> example in Python, where we will ask our model to generate a summary of an article:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.180.1">We will leverage OpenAI’s GPT-3.5-turbo model in a manner similar to the example discussed earlier in this chapter:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.181.1">import</span></span><span class="kobospan" id="kobo.182.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.183.1">import</span></span><span class="kobospan" id="kobo.184.1"> openai
openai.api_key = os.environ.get(</span><span class="hljs-string"><span class="kobospan" id="kobo.185.1">"OPENAI_API_KEY"</span></span><span class="kobospan" id="kobo.186.1">)
response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.187.1">"gpt-35-turbo"</span></span><span class="kobospan" id="kobo.188.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.189.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.190.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.191.1">"role"</span></span><span class="kobospan" id="kobo.192.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.193.1">"system"</span></span><span class="kobospan" id="kobo.194.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.195.1">"content"</span></span><span class="kobospan" id="kobo.196.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.197.1">"role"</span></span><span class="kobospan" id="kobo.198.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.199.1">"user"</span></span><span class="kobospan" id="kobo.200.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.201.1">"content"</span></span><span class="kobospan" id="kobo.202.1">: article},
    ]
)
</span></code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.203.1">Let’s set both the </span><code class="inlinecode"><span class="kobospan" id="kobo.204.1">system_message</span></code><span class="kobospan" id="kobo.205.1"> and </span><code class="inlinecode"><span class="kobospan" id="kobo.206.1">article</span></code><span class="kobospan" id="kobo.207.1"> variables as follows (you can find the entire scripts in the book’s GitHub repository):
        </span><pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.208.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.209.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.210.1">You are an AI assistant that summarizes articles.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.211.1">To complete this task, do the following subtasks:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.212.1">Read the provided article context comprehensively and identify the main topic and key points</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.213.1">Generate a paragraph summary of the current article context that captures the essential information and conveys the main idea</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.214.1">Print each step of the process.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.215.1">Article:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.216.1">"""</span></span><span class="kobospan" id="kobo.217.1">
article = </span><span class="hljs-string"><span class="kobospan" id="kobo.218.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.219.1">Recurrent neural networks, long short-term memory, and gated recurrent neural networks</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.220.1">in particular, […]</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.221.1">"""</span></span>
</code></pre>
</li>
<li class="bulletlist1"><span class="kobospan" id="kobo.222.1">To see the output, you</span><a id="_idIndexMarker271" class="calibre3"/><span class="kobospan" id="kobo.223.1"> can run the following code:
        </span><pre class="programlisting2"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.224.1">print</span></span><span class="kobospan" id="kobo.225.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.226.1">'choices'</span></span><span class="kobospan" id="kobo.227.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.228.1">0</span></span><span class="kobospan" id="kobo.229.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.230.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.231.1">message'</span></span><span class="kobospan" id="kobo.232.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.233.1">'content'</span></span><span class="kobospan" id="kobo.234.1">])
</span></code></pre>
</li>
</ol>
<p class="normal-one"><span class="kobospan" id="kobo.235.1">Here is the obtained output:</span></p>
<pre class="programlisting3"><code class="hljs-con"><span class="kobospan" id="kobo.236.1">Summary:
The article discusses the use of recurrent neural networks, specifically long short-term memory and gated recurrent neural networks, in sequence modeling and transduction problems. </span><span class="kobospan" id="kobo.236.2">These models have achieved great success but are limited by their inherently sequential nature. </span><span class="kobospan" id="kobo.236.3">Attention mechanisms have become popular in addressing this issue but are usually used in conjunction with recurrent networks. </span><span class="kobospan" id="kobo.236.4">The authors propose the Transformer, an architecture that relies solely on attention mechanisms instead of recurrence. </span><span class="kobospan" id="kobo.236.5">The Transformer allows for greater parallelization and can achieve state-of-the-art results in machine translation after only twelve hours of training on eight GPUs.
</span><span class="kobospan" id="kobo.236.6">Steps:
1. </span><span class="kobospan" id="kobo.236.7">The article discusses the success and limitations of recurrent neural networks in sequence modeling and transduction problems.
</span><span class="kobospan" id="kobo.236.8">2. </span><span class="kobospan" id="kobo.236.9">Attention mechanisms have become popular in addressing the limitations of recurrence but are usually used alongside recurrent networks.
</span><span class="kobospan" id="kobo.236.10">3. </span><span class="kobospan" id="kobo.236.11">The authors propose the Transformer, a model architecture that relies solely on attention mechanisms and allows for greater parallelization.
</span><span class="kobospan" id="kobo.236.12">4. </span><span class="kobospan" id="kobo.236.13">The Transformer can achieve state-of-the-art results in machine translation after only twelve hours of training on eight GPUs.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.237.1">As you can see, the model was able to produce a high-quality summary based on the key topics extracted (and displayed) from the given article. </span><span class="kobospan" id="kobo.237.2">The fact that we prompted the model to split the task into subtasks “forced” it to reduce the complexity of each subtask, hence improving the quality of the final result. </span><span class="kobospan" id="kobo.237.3">This approach can also lead to noticeable results when we deal with scenarios such as mathematical problems since it enhances the analytical reasoning capabilities of the model.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.238.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.239.1">In a landscape of many different LLMs, it is crucial to know that the very same system message may not be as efficient in all models. </span><span class="kobospan" id="kobo.239.2">A system message that perfectly works with GPT-4 might not be as efficient when applied to Llama 2, for example. </span><span class="kobospan" id="kobo.239.3">Therefore, it is pivotal to design the prompt in accordance with the type of LLM you decide to pick for your application.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.240.1">Splitting complex tasks</span><a id="_idIndexMarker272" class="calibre3"/><span class="kobospan" id="kobo.241.1"> into easier subtasks is a powerful technique; nevertheless, it does not address one of the main risks of LLM-generated content, that is, having a wrong output. </span><span class="kobospan" id="kobo.241.2">In the next two sections, we are going to see some techniques that are mainly aimed at addressing this risk.</span></p>
<h2 class="heading1" id="_idParaDest-61"><span class="kobospan" id="kobo.242.1">Ask for justification</span></h2>
<p class="normal"><span class="kobospan" id="kobo.243.1">LLMs are built in such a way</span><a id="_idIndexMarker273" class="calibre3"/><span class="kobospan" id="kobo.244.1"> that they predict the next token based on the previous ones without looking back at their generations. </span><span class="kobospan" id="kobo.244.2">This might lead the model to output wrong content to the user, yet in a very convincing way. </span><span class="kobospan" id="kobo.244.3">If the LLM-powered application does not provide a specific reference to that response, it might be hard to validate the ground truth behind it. </span><span class="kobospan" id="kobo.244.4">Henceforth, specifying in the prompt to support the LLM’s answer with some reflections and justification could prompt the model to recover from its actions. </span><span class="kobospan" id="kobo.244.5">Furthermore, asking for justification might be useful also in case of answers that are right but we simply don’t know the LLM’s reasoning behind it. </span><span class="kobospan" id="kobo.244.6">For example, let’s say we want our LLM to solve riddles. </span><span class="kobospan" id="kobo.244.7">To do so, we can instruct it as follows:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.245.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.246.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.247.1">You are an AI assistant specialized in solving riddles.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.248.1">Given a riddle, solve it the best you can.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.249.1">Provide a clear justification of your answer and the reasoning behind it.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.250.1">Riddle:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.251.1">"""</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.252.1">As you can see, I’ve specified in the metaprompt</span><a id="_idIndexMarker274" class="calibre3"/><span class="kobospan" id="kobo.253.1"> to the LLM to justify its answer and also provide its reasoning. </span><span class="kobospan" id="kobo.253.2">Let’s see how it works:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.254.1">riddle = </span><span class="hljs-string"><span class="kobospan" id="kobo.255.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.256.1">What has a face and two hands, but no arms or legs?</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.257.1">"""</span></span><span class="kobospan" id="kobo.258.1">
response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.259.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.260.1">,
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.261.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.262.1">role"</span></span><span class="kobospan" id="kobo.263.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.264.1">"system"</span></span><span class="kobospan" id="kobo.265.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.266.1">"content"</span></span><span class="kobospan" id="kobo.267.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.268.1">"role"</span></span><span class="kobospan" id="kobo.269.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.270.1">"user"</span></span><span class="kobospan" id="kobo.271.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.272.1">"content"</span></span><span class="kobospan" id="kobo.273.1">: riddle},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.274.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.275.1">print</span></span><span class="kobospan" id="kobo.276.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.277.1">'choices'</span></span><span class="kobospan" id="kobo.278.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.279.1">0</span></span><span class="kobospan" id="kobo.280.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.281.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.282.1">message'</span></span><span class="kobospan" id="kobo.283.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.284.1">'content'</span></span><span class="kobospan" id="kobo.285.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.286.1">The following is the obtained output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.287.1">The answer to this riddle is a clock.
</span><span class="kobospan" id="kobo.287.2">A clock has a face with numbers and two hands (the hour and minute hand), which rotate around the center of the clock to show the time. </span><span class="kobospan" id="kobo.287.3">Although it doesn't have any arms or legs like humans do, it still has the ability to measure time, which is important in our daily lives.
</span><span class="kobospan" id="kobo.287.4">Hence, a clock is the correct answer to this riddle.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.288.1">Justifications are a great tool to make your model more reliable and robust since they force it to “rethink” its output, as well as provide us with a view of how the reasoning was set to solve the problem.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.289.1">With a similar approach, we could also intervene at different prompt levels to improve our LLM’s performance. </span><span class="kobospan" id="kobo.289.2">For example, we might discover that the model is systematically tackling a mathematical problem in the wrong way; henceforth, we might want to suggest the right approach directly at the metaprompt level. </span><span class="kobospan" id="kobo.289.3">Another example might be that of asking the model to generate multiple outputs – along with their justifications – to evaluate different reasoning</span><a id="_idIndexMarker275" class="calibre3"/><span class="kobospan" id="kobo.290.1"> techniques and prompt the best one in the metaprompt.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.291.1">In the next section, we are going to focus on one of these examples, more specifically, the possibility of generating multiple outputs and then picking the most likely one.</span></p>
<h2 class="heading1" id="_idParaDest-62"><span class="kobospan" id="kobo.292.1">Generate many outputs, then use the model to pick the best one</span></h2>
<p class="normal"><span class="kobospan" id="kobo.293.1">As we saw in the previous section, LLMs</span><a id="_idIndexMarker276" class="calibre3"/><span class="kobospan" id="kobo.294.1"> are built in such a way that they predict the next token based on the previous ones without looking back at their generations. </span><span class="kobospan" id="kobo.294.2">If this is the case, if one sampled token is the wrong one (in other words, if the model is unlucky), the LLM will keep generating wrong tokens and, henceforth, wrong content. </span><span class="kobospan" id="kobo.294.3">Now, the bad news is that, unlike humans, LLMs cannot recover from errors on their own. </span><span class="kobospan" id="kobo.294.4">This means that, if we ask them, they acknowledge the error, but we need to explicitly prompt them to think about that.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.295.1">One way to overcome this limitation is to broaden the space of probabilities of picking the right token. </span><span class="kobospan" id="kobo.295.2">Rather than generating just one response, we can prompt the model to generate multiple responses, and then pick the one that is most suitable for the user’s query. </span><span class="kobospan" id="kobo.295.3">This splits the job into two subtasks for our LLM:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><span class="kobospan" id="kobo.296.1">Generating multiple responses to the user’s query</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.297.1">Comparing those responses and picking the best one, according to some criteria we can specify in the metaprompt</span></li>
</ol>
<p class="normal1"><span class="kobospan" id="kobo.298.1">Let’s see an example, following up from the riddles examined in the previous section:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.299.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.300.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.301.1">You are an AI assistant specialized in solving riddles.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.302.1">Given a riddle, you have to generate three answers to the riddle.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.303.1">For each answer, be specific about the reasoning you made.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.304.1">Then, among the three answers, select the one that is most plausible given the riddle.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.305.1">Riddle:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.306.1">"""</span></span><span class="kobospan" id="kobo.307.1">
riddle = </span><span class="hljs-string"><span class="kobospan" id="kobo.308.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.309.1">What has a face and two hands, but no arms or legs?</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.310.1">"""</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.311.1">In this case, I’ve prompted</span><a id="_idIndexMarker277" class="calibre3"/><span class="kobospan" id="kobo.312.1"> the model to generate three answers to the riddle, then to give me the most likely, justifying why. </span><span class="kobospan" id="kobo.312.2">Let’s see the result:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.313.1">response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.314.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.315.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.316.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.317.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.318.1">"role"</span></span><span class="kobospan" id="kobo.319.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.320.1">"system"</span></span><span class="kobospan" id="kobo.321.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.322.1">"content"</span></span><span class="kobospan" id="kobo.323.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.324.1">"role"</span></span><span class="kobospan" id="kobo.325.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.326.1">"user"</span></span><span class="kobospan" id="kobo.327.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.328.1">"content"</span></span><span class="kobospan" id="kobo.329.1">: riddle},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.330.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.331.1">print</span></span><span class="kobospan" id="kobo.332.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.333.1">'choices'</span></span><span class="kobospan" id="kobo.334.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.335.1">0</span></span><span class="kobospan" id="kobo.336.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.337.1">'message'</span></span><span class="kobospan" id="kobo.338.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.339.1">'content'</span></span><span class="kobospan" id="kobo.340.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.341.1">We then get the following output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.342.1">Answer 1: A clock. </span><span class="kobospan" id="kobo.342.2">A clock has a face with numbers and two hands to show minutes and hours. </span><span class="kobospan" id="kobo.342.3">It does not have arms or legs because it is an object, and it does not have intrinsic limbs.
</span><span class="kobospan" id="kobo.342.4">Answer 2: A watch. </span><span class="kobospan" id="kobo.342.5">Similar to a clock, a watch also has a face with numbers and two hands to indicate the time. </span><span class="kobospan" id="kobo.342.6">Although it is usually worn on a person's wrist, it does not have arms or legs.
</span><span class="kobospan" id="kobo.342.7">Answer 3: A wall. </span><span class="kobospan" id="kobo.342.8">While this answer may be less obvious, a wall does, in a way, have a face and two hands. </span><span class="kobospan" id="kobo.342.9">The face is its outer surface, and the hands are the clock that usually hangs on the wall.
</span><span class="kobospan" id="kobo.342.10">Among the three answers, the most plausible one is answer 1: a clock. </span><span class="kobospan" id="kobo.342.11">The riddle distinctly mentions "two hands" and it's logical to associate hands with those that are found in clocks to indicate time.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.343.1">As you can see, the model selected the most plausible answer along with a justification of its choice. </span><span class="kobospan" id="kobo.343.2">It is interesting to note that “clock” and “watch” might seem similar responses; however, the model specified that “watch” is usually worn on a person’s wrist and, even though it doesn’t mean it has arms or legs, this element might have lowered the probability of being the correct answer.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.344.1">What would you have picked?</span></p>
<p class="normal1"><span class="kobospan" id="kobo.345.1">As discussed earlier, forcing</span><a id="_idIndexMarker278" class="calibre3"/><span class="kobospan" id="kobo.346.1"> the model to tackle a problem with different approaches is a way to collect multiple samples of reasonings, which might serve as further instructions in the metaprompt. </span><span class="kobospan" id="kobo.346.2">For example, if we want the model to always propose something that is not the most straightforward solution to a problem – in other words, if we want it to “think differently” – we might force it to solve a problem in N ways and then use the most creative reasoning as a framework in the metaprompt.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.347.1">The last element we are going to examine is the overall structure we want to give to our metaprompt. </span><span class="kobospan" id="kobo.347.2">In fact, in previous examples, we saw a sample system message with some statements and instructions. </span><span class="kobospan" id="kobo.347.3">In the next section, we will see how the order and “strength” of those statements and instructions are not invariants.</span></p>
<h2 class="heading1" id="_idParaDest-63"><span class="kobospan" id="kobo.348.1">Repeat instructions at the end</span></h2>
<p class="normal"><span class="kobospan" id="kobo.349.1">LLMs tend not to process the metaprompt</span><a id="_idIndexMarker279" class="calibre3"/><span class="kobospan" id="kobo.350.1"> attributing the same weight or imprortance to all the sections. </span><span class="kobospan" id="kobo.350.2">In fact, in his blog post </span><em class="italic"><span class="kobospan" id="kobo.351.1">Large Language Model Prompt Engineering for Complex Summarization</span></em><span class="kobospan" id="kobo.352.1">, John Stewart (a software engineer at Microsoft) found some interesting outcomes</span><a id="_idIndexMarker280" class="calibre3"/><span class="kobospan" id="kobo.353.1"> from arranging prompt sections (</span><a href="https://devblogs.microsoft.com/ise/gpt-summary-prompt-engineering/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.354.1">https://devblogs.microsoft.com/ise/gpt-summary-prompt-engineering/</span></span></a><span class="kobospan" id="kobo.355.1">). </span><span class="kobospan" id="kobo.355.2">More specifically, after several experimentations, he found that repeating the main instruction at the end of the prompt can help</span><a id="_idIndexMarker281" class="calibre3"/><span class="kobospan" id="kobo.356.1"> the model overcome its inner </span><strong class="screentext"><span class="kobospan" id="kobo.357.1">recency bias</span></strong><span class="kobospan" id="kobo.358.1">.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.359.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.360.1">Recency bias is the tendency</span><a id="_idIndexMarker282" class="calibre3"/><span class="kobospan" id="kobo.361.1"> of LLMs to give more weight to the information that appears near the end of a prompt, and ignore or forget the information that appears earlier. </span><span class="kobospan" id="kobo.361.2">This can lead to inaccurate or inconsistent responses that do not take into account the whole context of the task. </span><span class="kobospan" id="kobo.361.3">For example, if the prompt is a long conversation between two people, the model may only focus on the last few messages and disregard the previous ones.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.362.1">Let’s look at some ways to overcome recency bias:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.363.1">One possible way to overcome</span><a id="_idIndexMarker283" class="calibre3"/><span class="kobospan" id="kobo.364.1"> recency bias is to break down the task into smaller steps or subtasks and provide feedback or guidance along the way. </span><span class="kobospan" id="kobo.364.2">This can help the model focus on each step and avoid getting lost in irrelevant details. </span><span class="kobospan" id="kobo.364.3">We’ve covered this technique in the </span><em class="italic"><span class="kobospan" id="kobo.365.1">Split complex tasks into subtasks</span></em><span class="kobospan" id="kobo.366.1"> section in, which we discussed splitting complex tasks into easier subtasks.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.367.1">Another way to overcome recency bias with prompt engineering techniques is to repeat the instructions or the main goal of the task at the end of the prompt. </span><span class="kobospan" id="kobo.367.2">This can help remind the model of what it is supposed to do and what kind of response it should generate.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.368.1">For instance, let’s say we want our model to output the sentiment of a whole chat history between an AI agent and the user. </span><span class="kobospan" id="kobo.368.2">We want to make sure that the model will output the sentiment in lowercase and without punctuation.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.369.1">Let’s consider the following </span><a id="_idIndexMarker284" class="calibre3"/><span class="kobospan" id="kobo.370.1">example (the conversation is truncated, but you can find the whole code in the book’s GitHub repository). </span><span class="kobospan" id="kobo.370.2">In this case, the key instruction is that of having as output only the sentiment in lowercase and without punctuation:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.371.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.372.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.373.1">You are a sentiment analyzer. </span><span class="kobospan" id="kobo.373.2">You classify conversations into three categories: positive, negative, or neutral.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.374.1">Return only the sentiment, in lowercase and without punctuation.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.375.1">Conversation:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.376.1">"""</span></span><span class="kobospan" id="kobo.377.1">
conversation = </span><span class="hljs-string"><span class="kobospan" id="kobo.378.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.379.1">Customer: Hi, I need some help with my order.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.380.1">AI agent: Hello, welcome to our online store. </span><span class="kobospan" id="kobo.380.2">I'm an AI agent and I'm here to assist you.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.381.1">Customer: I ordered a pair of shoes yesterday, but I haven't received a confirmation email yet. </span><span class="kobospan" id="kobo.381.2">Can you check the status of my order?</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.382.1">[…]</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.383.1">"""</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.384.1">In this scenario, we have key instructions before the conversation, so let’s initialize our model and feed it with the two variables </span><code class="inlinecode"><span class="kobospan" id="kobo.385.1">system_message</span></code><span class="kobospan" id="kobo.386.1"> and </span><code class="inlinecode"><span class="kobospan" id="kobo.387.1">conversation</span></code><span class="kobospan" id="kobo.388.1">:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.389.1">response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.390.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.391.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.392.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.393.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.394.1">"role"</span></span><span class="kobospan" id="kobo.395.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.396.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.397.1">system"</span></span><span class="kobospan" id="kobo.398.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.399.1">"content"</span></span><span class="kobospan" id="kobo.400.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.401.1">"role"</span></span><span class="kobospan" id="kobo.402.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.403.1">"user"</span></span><span class="kobospan" id="kobo.404.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.405.1">"content"</span></span><span class="kobospan" id="kobo.406.1">: conversation},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.407.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.408.1">print</span></span><span class="kobospan" id="kobo.409.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.410.1">'choices'</span></span><span class="kobospan" id="kobo.411.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.412.1">0</span></span><span class="kobospan" id="kobo.413.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.414.1">'message'</span></span><span class="kobospan" id="kobo.415.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.416.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.417.1">content'</span></span><span class="kobospan" id="kobo.418.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.419.1">Here is the output that we receive:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.420.1">Neutral
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.421.1">The model didn’t follow the instruction</span><a id="_idIndexMarker285" class="calibre3"/><span class="kobospan" id="kobo.422.1"> of having only lowercase letters. </span><span class="kobospan" id="kobo.422.2">Let’s try to repeat the instruction also at the end of the prompt:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.423.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.424.1">f"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.425.1">You are a sentiment analyzer. </span><span class="kobospan" id="kobo.425.2">You classify conversations into three categories: positive, negative, or neutral.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.426.1">Return only the sentiment, in lowercase and without punctuation.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.427.1">Conversation:</span></span>
<span class="hljs-subst"><span class="kobospan" id="kobo.428.1">{conversation}</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.429.1">Remember to return only the sentiment, in lowercase and without punctuation</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.430.1">"""</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.431.1">Again, let’s invoke our model with the updated </span><code class="inlinecode"><span class="kobospan" id="kobo.432.1">system_message</span></code><span class="kobospan" id="kobo.433.1">:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.434.1">response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.435.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.436.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.437.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.438.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.439.1">"role"</span></span><span class="kobospan" id="kobo.440.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.441.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.442.1">user"</span></span><span class="kobospan" id="kobo.443.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.444.1">"content"</span></span><span class="kobospan" id="kobo.445.1">: system_message},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.446.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.447.1">print</span></span><span class="kobospan" id="kobo.448.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.449.1">'choices'</span></span><span class="kobospan" id="kobo.450.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.451.1">0</span></span><span class="kobospan" id="kobo.452.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.453.1">'message'</span></span><span class="kobospan" id="kobo.454.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.455.1">'content'</span></span><span class="kobospan" id="kobo.456.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.457.1">Here is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.458.1">neutral
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.459.1">As you can see, now the model was able to provide exactly the output we desired. </span><span class="kobospan" id="kobo.459.2">This approach is particularly useful whenever we have a conversation history to keep storing in the context window. </span><span class="kobospan" id="kobo.459.3">If this is the case, having the main instructions at the beginning might induce the model not to have</span><a id="_idIndexMarker286" class="calibre3"/><span class="kobospan" id="kobo.460.1"> them in mind once it also goes through the whole history, hence reducing their strength.</span></p>
<h2 class="heading1" id="_idParaDest-64"><span class="kobospan" id="kobo.461.1">Use delimiters</span></h2>
<p class="normal"><span class="kobospan" id="kobo.462.1">The last principle to be covered</span><a id="_idIndexMarker287" class="calibre3"/><span class="kobospan" id="kobo.463.1"> is related to the format we want to give to our metaprompt. </span><span class="kobospan" id="kobo.463.2">This helps our LLM to better understand its intents as well as relate different sections and paragraphs to each other.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.464.1">To achieve this, we can use delimiters within our prompt. </span><span class="kobospan" id="kobo.464.2">A delimiter can be any sequence of characters or symbols that is clearly mapping a schema rather than a concept. </span><span class="kobospan" id="kobo.464.3">For example, we can consider the following sequences to be delimiters:</span></p>
<ul class="calibre14">
<li class="bulletlist"><code class="inlinecode"><span class="kobospan" id="kobo.465.1">&gt;&gt;&gt;&gt;</span></code></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.466.1">====</span></code></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.467.1">------</span></code></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.468.1">####</span></code></li>
<li class="bulletlist1"><code class="inlinecode"><span class="kobospan" id="kobo.469.1">` ` ` ` `</span></code></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.470.1">This leads to a series of benefits, including:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.471.1">Clear separation: Delimiters mark distinct sections within a prompt, separating instructions, examples, and desired output.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.472.1">Guidance for LLMs: Proper use of delimiters removes ambiguity, guiding the model effectively.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.473.1">Enhanced precision: Delimiters improve prompt understanding, resulting in more relevant responses.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.474.1">Improved coherence: Effective use of delimiters organizes instructions, inputs, and outputs, leading to coherent responses.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.475.1">Let’s consider, for example, a metaprompt</span><a id="_idIndexMarker288" class="calibre3"/><span class="kobospan" id="kobo.476.1"> that aims at instructing the model to translate user’s tasks into Python code, providing an example to do so:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.477.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.478.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.479.1">You are a Python expert who produces Python code as per the user's request.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.480.1">===&gt;START EXAMPLE</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.481.1">---User Query---</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.482.1">Give me a function to print a string of text.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.483.1">---User Output---</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.484.1">Below you can find the described function:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.485.1">```def my_print(text):</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.486.1">     return print(text)</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.487.1">```</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.488.1">&lt;===END EXAMPLE</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.489.1">"""</span></span><span class="kobospan" id="kobo.490.1">
query = </span><span class="hljs-string"><span class="kobospan" id="kobo.491.1">"generate a Python function to calculate the nth Fibonacci number"</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.492.1">In the above example, we’ve used delimiters to both specify the beginning and end of an example for a one-shot learning approach and, within the example, specify the Python code snippet.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.493.1">Let’s see how it works:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.494.1">response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.495.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.496.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.497.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.498.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.499.1">"role"</span></span><span class="kobospan" id="kobo.500.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.501.1">"system"</span></span><span class="kobospan" id="kobo.502.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.503.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.504.1">content"</span></span><span class="kobospan" id="kobo.505.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.506.1">"role"</span></span><span class="kobospan" id="kobo.507.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.508.1">"user"</span></span><span class="kobospan" id="kobo.509.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.510.1">"content"</span></span><span class="kobospan" id="kobo.511.1">: query},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.512.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.513.1">print</span></span><span class="kobospan" id="kobo.514.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.515.1">'choices'</span></span><span class="kobospan" id="kobo.516.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.517.1">0</span></span><span class="kobospan" id="kobo.518.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.519.1">'message'</span></span><span class="kobospan" id="kobo.520.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.521.1">'content'</span></span><span class="kobospan" id="kobo.522.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.523.1">Here is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.524.1">Sure, here's a Python function to calculate the nth Fibonacci number:
```python def fibonacci(n):
if n &lt; 0:
return None
elif n == 0:
return 0
elif n == 1:
return 1
else:
return fibonacci(n-1) + fibonacci(n-2) ```
The function takes a non-negative integer `n` as input and returns the nth Fibonacci number. </span><span class="kobospan" id="kobo.524.2">If `n` is less than 0, the function returns `None`. </span><span class="kobospan" id="kobo.524.3">If `n` is 0 or 1, the function returns the value of `n`. </span><span class="kobospan" id="kobo.524.4">In all other cases, the function recursively calls itself with `n-1` and `n-2` and adds their results to compute the nth Fibonacci number.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.525.1">As you can see, it also printed the code with backticks, as shown within the system message.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.526.1">All the principles examined up to this point are general rules that can make your LLM-powered application more robust. </span><span class="kobospan" id="kobo.526.2">Those techniques should be kept in mind regardless of the type of application you are developing since they are general best practices that improve your LLM</span><a id="_idIndexMarker289" class="calibre3"/><span class="kobospan" id="kobo.527.1"> performance. </span><span class="kobospan" id="kobo.527.2">In the following section, we are going to see some advanced techniques for prompt engineering.</span></p>
<h1 class="heading" id="_idParaDest-65"><span class="kobospan" id="kobo.528.1">Advanced techniques</span></h1>
<p class="normal"><span class="kobospan" id="kobo.529.1">Advanced techniques might be implemented for specific scenarios and address the way the model reasons and thinks about the answer before providing it to the final user. </span><span class="kobospan" id="kobo.529.2">Let’s look at some of these in the upcoming sections.</span></p>
<h2 class="heading1" id="_idParaDest-66"><span class="kobospan" id="kobo.530.1">Few-shot approach</span></h2>
<p class="normal"><span class="kobospan" id="kobo.531.1">In their paper </span><em class="italic"><span class="kobospan" id="kobo.532.1">Language Models are Few-Shot Learners</span></em><span class="kobospan" id="kobo.533.1">, Tom Brown et al. </span><span class="kobospan" id="kobo.533.2">demonstrate</span><a id="_idIndexMarker290" class="calibre3"/><span class="kobospan" id="kobo.534.1"> that GPT-3 can achieve strong performance on many NLP tasks in a few-shot setting. </span><span class="kobospan" id="kobo.534.2">This means that for all tasks, GPT-3 is applied without any fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.535.1">This is an example and evidence of how the concept of few-shot learning – which means providing the model with examples of how we would like it to respond – is a powerful technique that enables model customization without interfering with the overall architecture.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.536.1">For example, let’s say we want our model to generate a tagline for a new product line of climbing shoes we’ve just coined – Elevation Embrace. </span><span class="kobospan" id="kobo.536.2">We have an idea of what the tagline should be like – concise and direct. </span><span class="kobospan" id="kobo.536.3">We could explain it to the model in plain text; however, it might be more effective simply to provide it with some examples of similar projects.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.537.1">Let’s see an implementation with code:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.538.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.539.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.540.1">You are an AI marketing assistant. </span><span class="kobospan" id="kobo.540.2">You help users to create taglines for new product names.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.541.1">Given a product name, produce a tagline similar to the following examples:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.542.1">Peak Pursuit - Conquer Heights with Comfort</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.543.1">Summit Steps - Your Partner for Every Ascent</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.544.1">Crag Conquerors - Step Up, Stand Tall</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.545.1">Product name:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.546.1">"""</span></span><span class="kobospan" id="kobo.547.1">
product_name = </span><span class="hljs-string"><span class="kobospan" id="kobo.548.1">'Elevation Embrace'</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.549.1">Let’s see how our model will handle this request:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.550.1">response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.551.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.552.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.553.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.554.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.555.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.556.1">role"</span></span><span class="kobospan" id="kobo.557.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.558.1">"system"</span></span><span class="kobospan" id="kobo.559.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.560.1">"content"</span></span><span class="kobospan" id="kobo.561.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.562.1">"role"</span></span><span class="kobospan" id="kobo.563.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.564.1">"user"</span></span><span class="kobospan" id="kobo.565.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.566.1">"content"</span></span><span class="kobospan" id="kobo.567.1">: product_name},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.568.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.569.1">print</span></span><span class="kobospan" id="kobo.570.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.571.1">'choices'</span></span><span class="kobospan" id="kobo.572.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.573.1">0</span></span><span class="kobospan" id="kobo.574.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.575.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.576.1">message'</span></span><span class="kobospan" id="kobo.577.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.578.1">'content'</span></span><span class="kobospan" id="kobo.579.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.580.1">The following is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.581.1">Tagline idea: Embrace the Heights with Confidence.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.582.1">As you can see, it maintained</span><a id="_idIndexMarker291" class="calibre3"/><span class="kobospan" id="kobo.583.1"> the style, length, and also writing convention of the provided taglines. </span><span class="kobospan" id="kobo.583.2">This is extremely useful when you want your model to follow examples you already have, such as fixed templates.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.584.1">Note that, most of the time, few-shot learning is powerful enough to customize a model even in extremely specialized scenarios, where we could think about fine-tuning as the proper tool. </span><span class="kobospan" id="kobo.584.2">In fact, proper few-shot learning could be as effective as a fine-tuning process.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.585.1">Let’s look at another example. </span><span class="kobospan" id="kobo.585.2">Let’s say we want to develop a model that specializes in sentiment analysis. </span><span class="kobospan" id="kobo.585.3">To do so, we provide it with a series of examples of texts with different sentiments, alongside the output we would like – positive or negative. </span><span class="kobospan" id="kobo.585.4">Note that this set of examples is nothing but a small training set for supervised learning tasks; the only difference from fine-tuning is that we are not updating the model’s parameters.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.586.1">To provide you with a concrete representation of what was said above, let’s provide our model with just two examples for each label:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.587.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.588.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.589.1">You are a binary classifier for sentiment analysis.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.590.1">Given a text, based on its sentiment, you classify it into one of two categories: positive or negative.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.591.1">You can use the following texts as examples:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.592.1">Text: "I love this product! </span><span class="kobospan" id="kobo.592.2">It's fantastic and works perfectly."</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.593.1">Positive</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.594.1">Text: "I'm really disappointed with the quality of the food."</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.595.1">Negative</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.596.1">Text: "This is the best day of my life!"</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.597.1">Positive</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.598.1">Text: "I can't stand the noise in this restaurant."</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.599.1">Negative</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.600.1">ONLY return the sentiment as output (without punctuation).</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.601.1">Text:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.602.1">"""</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.603.1">To test our classifier, I’ve used the IMDb database</span><a id="_idIndexMarker292" class="calibre3"/><span class="kobospan" id="kobo.604.1"> of movie reviews</span><a id="_idIndexMarker293" class="calibre3"/><span class="kobospan" id="kobo.605.1"> available on Kaggle at </span><a href="https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/data" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.606.1">https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/data</span></span></a><span class="kobospan" id="kobo.607.1">. </span><span class="kobospan" id="kobo.607.2">As you can see, the dataset contains many movie reviews along with their associated sentiment – positive or negative. </span><span class="kobospan" id="kobo.607.3">Let’s substitute the binary label of 0–1 with a verbose label of Negative–Positive:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.608.1">import</span></span><span class="kobospan" id="kobo.609.1"> numpy </span><span class="hljs-keyword"><span class="kobospan" id="kobo.610.1">as</span></span><span class="kobospan" id="kobo.611.1"> np
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.612.1">import</span></span><span class="kobospan" id="kobo.613.1"> pandas </span><span class="hljs-keyword"><span class="kobospan" id="kobo.614.1">as</span></span><span class="kobospan" id="kobo.615.1"> pd
df = pd .read_csv(</span><span class="hljs-string"><span class="kobospan" id="kobo.616.1">'movie.csv'</span></span><span class="kobospan" id="kobo.617.1">, encoding=</span><span class="hljs-string"><span class="kobospan" id="kobo.618.1">'utf-8'</span></span><span class="kobospan" id="kobo.619.1">)
df[</span><span class="hljs-string"><span class="kobospan" id="kobo.620.1">'label'</span></span><span class="kobospan" id="kobo.621.1">] = df[</span><span class="hljs-string"><span class="kobospan" id="kobo.622.1">'label'</span></span><span class="kobospan" id="kobo.623.1">].replace({</span><span class="hljs-number"><span class="kobospan" id="kobo.624.1">0</span></span><span class="kobospan" id="kobo.625.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.626.1">'Negative'</span></span><span class="kobospan" id="kobo.627.1">, </span><span class="hljs-number"><span class="kobospan" id="kobo.628.1">1</span></span><span class="kobospan" id="kobo.629.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.630.1">'Positive'</span></span><span class="kobospan" id="kobo.631.1">})
df.head()
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.632.1">This gives us the first few records of the dataset, which are as follows:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.633.1"><img alt="" role="presentation" src="../Images/B21714_04_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.634.1">Figure 4.2: First observations of the movie dataset</span></p>
<p class="normal1"><span class="kobospan" id="kobo.635.1">Now, we want to test the performance of our model over a sample of 10 observations of this dataset:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.636.1">df = df.sample(n=</span><span class="hljs-number"><span class="kobospan" id="kobo.637.1">10</span></span><span class="kobospan" id="kobo.638.1">, random_state=</span><span class="hljs-number"><span class="kobospan" id="kobo.639.1">42</span></span><span class="kobospan" id="kobo.640.1">)
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.641.1">def</span></span> <span class="hljs-title"><span class="kobospan" id="kobo.642.1">process_text</span></span><span class="kobospan" id="kobo.643.1">(</span><span><span class="kobospan" id="kobo.644.1">text</span></span><span class="kobospan" id="kobo.645.1">):
    response = openai.ChatCompletion.create(
        model=</span><span class="hljs-string"><span class="kobospan" id="kobo.646.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.647.1">,
        messages=[
            {</span><span class="hljs-string"><span class="kobospan" id="kobo.648.1">"role"</span></span><span class="kobospan" id="kobo.649.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.650.1">"system"</span></span><span class="kobospan" id="kobo.651.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.652.1">"</span></span><span class="hljs-string"><span class="kobospan" id="kobo.653.1">content"</span></span><span class="kobospan" id="kobo.654.1">: system_message},
            {</span><span class="hljs-string"><span class="kobospan" id="kobo.655.1">"role"</span></span><span class="kobospan" id="kobo.656.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.657.1">"user"</span></span><span class="kobospan" id="kobo.658.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.659.1">"content"</span></span><span class="kobospan" id="kobo.660.1">: text},
        ]
    )
    </span><span class="hljs-keyword"><span class="kobospan" id="kobo.661.1">return</span></span><span class="kobospan" id="kobo.662.1"> response[</span><span class="hljs-string"><span class="kobospan" id="kobo.663.1">'choices'</span></span><span class="kobospan" id="kobo.664.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.665.1">0</span></span><span class="kobospan" id="kobo.666.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.667.1">'message'</span></span><span class="kobospan" id="kobo.668.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.669.1">'content'</span></span><span class="kobospan" id="kobo.670.1">]
df[</span><span class="hljs-string"><span class="kobospan" id="kobo.671.1">'predicted'</span></span><span class="kobospan" id="kobo.672.1">] = df[</span><span class="hljs-string"><span class="kobospan" id="kobo.673.1">'</span></span><span class="hljs-string"><span class="kobospan" id="kobo.674.1">text'</span></span><span class="kobospan" id="kobo.675.1">].apply(process_text)
</span><span class="hljs-built_in"><span class="kobospan" id="kobo.676.1">print</span></span><span class="kobospan" id="kobo.677.1">(df)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.678.1">The following is our output:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.679.1"><img alt="A text on a white background  Description automatically generated" src="../Images/B21714_04_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.680.1">Figure 4.3: Output of a GPT-3.5 model with few-shot examples</span></p>
<p class="normal1"><span class="kobospan" id="kobo.681.1">As you can see, by comparing</span><a id="_idIndexMarker294" class="calibre3"/><span class="kobospan" id="kobo.682.1"> the </span><code class="inlinecode"><span class="kobospan" id="kobo.683.1">label</span></code><span class="kobospan" id="kobo.684.1"> and </span><code class="inlinecode"><span class="kobospan" id="kobo.685.1">predicted</span></code><span class="kobospan" id="kobo.686.1"> columns, the model was able to correctly classify all the reviews, without even fine-tuning! </span><span class="kobospan" id="kobo.686.2">This is just an example of what you can achieve – in terms of model specialization – with the technique of few-shot learning.</span></p>
<h2 class="heading1" id="_idParaDest-67"><span class="kobospan" id="kobo.687.1">Chain of thought</span></h2>
<p class="normal"><span class="kobospan" id="kobo.688.1">Introduced in the</span><a id="_idIndexMarker295" class="calibre3"/><span class="kobospan" id="kobo.689.1"> paper </span><em class="italic"><span class="kobospan" id="kobo.690.1">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</span></em><span class="kobospan" id="kobo.691.1"> by Wei et al., </span><strong class="screentext"><span class="kobospan" id="kobo.692.1">chain of thought</span></strong><span class="kobospan" id="kobo.693.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.694.1">CoT</span></strong><span class="kobospan" id="kobo.695.1">) is a technique that enables complex reasoning capabilities through intermediate reasoning steps. </span><span class="kobospan" id="kobo.695.2">It also encourages the model to explain its reasoning, “forcing” it not to be too fast and risking giving the wrong response (as we saw in previous sections).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.696.1">Let’s say that we want to prompt our LLM to solve generic first-degree equations. </span><span class="kobospan" id="kobo.696.2">To do so, we are going to provide it with a basic reasoning list that it might want to follow:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.697.1">system_message = </span><span class="hljs-string"><span class="kobospan" id="kobo.698.1">"""</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.699.1">To solve a generic first-degree equation, follow these steps:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.700.1">1. </span><span class="kobospan" id="kobo.700.2">**Identify the Equation:** Start by identifying the equation you want to solve. </span><span class="kobospan" id="kobo.700.3">It should be in the form of "ax + b = c," where 'a' is the coefficient of the variable, 'x' is the variable, 'b' is a constant, and 'c' is another constant.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.701.1">2. </span><span class="kobospan" id="kobo.701.2">**Isolate the Variable:** Your goal is to isolate the variable 'x' on one side of the equation. </span><span class="kobospan" id="kobo.701.3">To do this, perform the following steps:</span></span>
<span class="hljs-string"> </span>
<span class="hljs-string"><span class="kobospan" id="kobo.702.1">   a. </span><span class="kobospan" id="kobo.702.2">**Add or Subtract Constants:** Add or subtract 'b' from both sides of the equation to move constants to one side.</span></span>
<span class="hljs-string"> </span>
<span class="hljs-string"><span class="kobospan" id="kobo.703.1">   b. </span><span class="kobospan" id="kobo.703.2">**Divide by the Coefficient:** Divide both sides by 'a' to isolate 'x'. </span><span class="kobospan" id="kobo.703.3">If 'a' is zero, the equation may not have a unique solution.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.704.1">3. </span><span class="kobospan" id="kobo.704.2">**Simplify:** Simplify both sides of the equation as much as possible.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.705.1">4. </span><span class="kobospan" id="kobo.705.2">**Solve for 'x':** Once 'x' is isolated on one side, you have the solution. </span><span class="kobospan" id="kobo.705.3">It will be in the form of 'x = value.'</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.706.1">5. </span><span class="kobospan" id="kobo.706.2">**Check Your Solution:** Plug the found value of 'x' back into the original equation to ensure it satisfies the equation. </span><span class="kobospan" id="kobo.706.3">If it does, you've found the correct solution.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.707.1">6. </span><span class="kobospan" id="kobo.707.2">**Express the Solution:** Write down the solution in a clear and concise form.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.708.1">7. </span><span class="kobospan" id="kobo.708.2">**Consider Special Cases:** Be aware of special cases where there may be no solution or infinitely many solutions, especially if 'a' equals zero.</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.709.1">Equation:</span></span>
<span class="hljs-string"><span class="kobospan" id="kobo.710.1">"""</span></span>
</code></pre>
<p class="normal1"><span class="kobospan" id="kobo.711.1">Let’s see how it can</span><a id="_idIndexMarker296" class="calibre3"/><span class="kobospan" id="kobo.712.1"> be implemented:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.713.1">equation = </span><span class="hljs-string"><span class="kobospan" id="kobo.714.1">"3x + 5 = 11"</span></span><span class="kobospan" id="kobo.715.1">
response = openai.ChatCompletion.create(
    model=</span><span class="hljs-string"><span class="kobospan" id="kobo.716.1">"gpt-3.5-turbo"</span></span><span class="kobospan" id="kobo.717.1">, </span><span class="hljs-comment"><span class="kobospan" id="kobo.718.1"># engine = "deployment_name".</span></span><span class="kobospan" id="kobo.719.1">
    messages=[
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.720.1">"role"</span></span><span class="kobospan" id="kobo.721.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.722.1">"system"</span></span><span class="kobospan" id="kobo.723.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.724.1">"content"</span></span><span class="kobospan" id="kobo.725.1">: system_message},
        {</span><span class="hljs-string"><span class="kobospan" id="kobo.726.1">"role"</span></span><span class="kobospan" id="kobo.727.1">: </span><span class="hljs-string"><span class="kobospan" id="kobo.728.1">"user"</span></span><span class="kobospan" id="kobo.729.1">, </span><span class="hljs-string"><span class="kobospan" id="kobo.730.1">"content"</span></span><span class="kobospan" id="kobo.731.1">: equation},
    ]
)
</span><span class="hljs-comment"><span class="kobospan" id="kobo.732.1">#print(response)</span></span>
<span class="hljs-built_in"><span class="kobospan" id="kobo.733.1">print</span></span><span class="kobospan" id="kobo.734.1">(response[</span><span class="hljs-string"><span class="kobospan" id="kobo.735.1">'choices'</span></span><span class="kobospan" id="kobo.736.1">][</span><span class="hljs-number"><span class="kobospan" id="kobo.737.1">0</span></span><span class="kobospan" id="kobo.738.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.739.1">'message'</span></span><span class="kobospan" id="kobo.740.1">][</span><span class="hljs-string"><span class="kobospan" id="kobo.741.1">'content'</span></span><span class="kobospan" id="kobo.742.1">])
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.743.1">The following is our output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.744.1">1. </span><span class="kobospan" id="kobo.744.2">Identify the Equation: The equation is 3x + 5 = 11.
</span><span class="kobospan" id="kobo.744.3">2. </span><span class="kobospan" id="kobo.744.4">Isolate the Variable:
  
   a. </span><span class="kobospan" id="kobo.744.5">Subtract Constants: Subtract 5 from both sides to move constants to one side:
  
      3x + 5 - 5 = 11 - 5
  
   Simplifying, we get:
  
      3x = 6
  
   b. </span><span class="kobospan" id="kobo.744.6">Divide by the Coefficient: Divide both sides by 3 to isolate 'x':
  
      3x/3 = 6/3
  
   Simplifying, we get:
  
      x = 2
  
3. </span><span class="kobospan" id="kobo.744.7">Simplify: Both sides are already simplified.
</span><span class="kobospan" id="kobo.744.8">4. </span><span class="kobospan" id="kobo.744.9">Solve for 'x': x = 2.
</span><span class="kobospan" id="kobo.744.10">5. </span><span class="kobospan" id="kobo.744.11">Check Your Solution: Plug x = 2 back into the original equation:
   3(2) + 5 = 11
  
   Simplifying, we get:
  
   6 + 5 = 11
  
   The left side equals the right side, so the solution is correct.
</span><span class="kobospan" id="kobo.744.12">6. </span><span class="kobospan" id="kobo.744.13">Express the Solution: The solution to the equation 3x + 5 = 11 is x = 2.
</span><span class="kobospan" id="kobo.744.14">7. </span><span class="kobospan" id="kobo.744.15">Consider Special Cases: Since the coefficient 3 is not zero, there are no special cases to consider.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.745.1">As you can see, the model clearly followed the seven steps specified in the metaprompt, which also allows the model to “take its time” to perform this task. </span><span class="kobospan" id="kobo.745.2">Note that you can also combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.746.1">With CoT, we are prompting</span><a id="_idIndexMarker297" class="calibre3"/><span class="kobospan" id="kobo.747.1"> the model to generate intermediate reasoning steps. </span><span class="kobospan" id="kobo.747.2">This is also a component of another reasoning technique, which we are going to examine in the next section.</span></p>
<h2 class="heading1" id="_idParaDest-68"><span class="kobospan" id="kobo.748.1">ReAct</span></h2>
<p class="normal"><span class="kobospan" id="kobo.749.1">Introduced in the</span><a id="_idIndexMarker298" class="calibre3"/><span class="kobospan" id="kobo.750.1"> paper </span><em class="italic"><span class="kobospan" id="kobo.751.1">ReAct: Synergizing Reasoning and Acting in Language Models</span></em><span class="kobospan" id="kobo.752.1"> by Yao et al., </span><strong class="screentext"><span class="kobospan" id="kobo.753.1">ReAct</span></strong><span class="kobospan" id="kobo.754.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.755.1">Reason and Act</span></strong><span class="kobospan" id="kobo.756.1">) is a general paradigm that combines reasoning and acting with LLMs. </span><span class="kobospan" id="kobo.756.2">ReAct prompts the language model to generate verbal reasoning traces and actions for a task, and also receives observations from external sources such as web searches or databases. </span><span class="kobospan" id="kobo.756.3">This allows the language model to perform dynamic reasoning and quickly adapt its action plan based on external information. </span><span class="kobospan" id="kobo.756.4">For example, you can prompt the language model to answer a question by first reasoning about the question, then performing an action to send a query to the web, then receiving an observation from the search results, and then continuing with this thought, action, observation loop until it reaches a conclusion.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.757.1">The difference between CoT and ReAct approaches is that CoT prompts the language model to generate intermediate reasoning steps for a task, while ReAct prompts the language model to generate intermediate reasoning steps, actions, and observations for a task.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.758.1">Note that the “action” phase is generally related to the possibility for our LLM to interact with external tools, such as a web search.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.759.1">For example, let’s say we want</span><a id="_idIndexMarker299" class="calibre3"/><span class="kobospan" id="kobo.760.1"> to ask our model for some up-to-date information about the upcoming Olympic games. </span><span class="kobospan" id="kobo.760.2">To do so, we are going to build a smart LangChain agent (as described in </span><em class="italic"><span class="kobospan" id="kobo.761.1">Chapter 2</span></em><span class="kobospan" id="kobo.762.1">) leveraging </span><code class="inlinecode"><span class="kobospan" id="kobo.763.1">SerpAPIWrapperWrapper</span></code><span class="kobospan" id="kobo.764.1"> (to wrap the </span><code class="inlinecode"><span class="kobospan" id="kobo.765.1">SerpApi</span></code><span class="kobospan" id="kobo.766.1"> to navigate the web), the </span><code class="inlinecode"><span class="kobospan" id="kobo.767.1">AgentType</span></code><span class="kobospan" id="kobo.768.1"> tool (to decide which type of agent to use for our goal), and other prompt-related modules (to make it easier to “templatize” our instructions). </span><span class="kobospan" id="kobo.768.2">Let’s see how we can do this (I won’t dive deeper into each component of the following code since the next chapter will be entirely focused on LangChain and its main components):</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-keyword"><span class="kobospan" id="kobo.769.1">import</span></span><span class="kobospan" id="kobo.770.1"> os
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.771.1">from</span></span><span class="kobospan" id="kobo.772.1"> dotenv </span><span class="hljs-keyword"><span class="kobospan" id="kobo.773.1">import</span></span><span class="kobospan" id="kobo.774.1"> load_dotenv
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.775.1">from</span></span><span class="kobospan" id="kobo.776.1"> langchain </span><span class="hljs-keyword"><span class="kobospan" id="kobo.777.1">import</span></span><span class="kobospan" id="kobo.778.1"> SerpAPIWrapper
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.779.1">from</span></span><span class="kobospan" id="kobo.780.1"> langchain.agents </span><span class="hljs-keyword"><span class="kobospan" id="kobo.781.1">import</span></span><span class="kobospan" id="kobo.782.1"> AgentType, initialize_agent
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.783.1">from</span></span><span class="kobospan" id="kobo.784.1"> langchain.chat_models </span><span class="hljs-keyword"><span class="kobospan" id="kobo.785.1">import</span></span><span class="kobospan" id="kobo.786.1"> ChatOpenAI
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.787.1">from</span></span><span class="kobospan" id="kobo.788.1"> langchain.tools </span><span class="hljs-keyword"><span class="kobospan" id="kobo.789.1">import</span></span><span class="kobospan" id="kobo.790.1"> BaseTool, StructuredTool, Tool, tool
</span><span class="hljs-keyword"><span class="kobospan" id="kobo.791.1">from</span></span><span class="kobospan" id="kobo.792.1"> langchain.schema </span><span class="hljs-keyword"><span class="kobospan" id="kobo.793.1">import</span></span><span class="kobospan" id="kobo.794.1"> HumanMessage
model = ChatOpenAI(
    model_name=</span><span class="hljs-string"><span class="kobospan" id="kobo.795.1">'gpt-35-turbo'</span></span><span class="kobospan" id="kobo.796.1">
)
load_dotenv()
key = os.environ[</span><span class="hljs-string"><span class="kobospan" id="kobo.797.1">"SERPAPI_API_KEY"</span></span><span class="kobospan" id="kobo.798.1">]
search = SerpAPIWrapper()
tools = [
    Tool.from_function(
        func=search.run,
        name=</span><span class="hljs-string"><span class="kobospan" id="kobo.799.1">"Search"</span></span><span class="kobospan" id="kobo.800.1">,
        description=</span><span class="hljs-string"><span class="kobospan" id="kobo.801.1">"useful for when you need to answer questions about current events"</span></span><span class="kobospan" id="kobo.802.1">
    )
    ]
agent_executor = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=</span><span class="hljs-literal"><span class="kobospan" id="kobo.803.1">True</span></span><span class="kobospan" id="kobo.804.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.805.1">As you can see, for this purpose, I’ve used a pre-built</span><a id="_idIndexMarker300" class="calibre3"/><span class="kobospan" id="kobo.806.1"> agent type available in LangChain called </span><code class="inlinecode"><span class="kobospan" id="kobo.807.1">ZERO_SHOT_REACT_DESCRIPTION</span></code><span class="kobospan" id="kobo.808.1">. </span><span class="kobospan" id="kobo.808.2">It comes with a precompiled prompt that follows the ReAct approach. </span><span class="kobospan" id="kobo.808.3">Let’s inspect that prompt:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="hljs-built_in"><span class="kobospan" id="kobo.809.1">print</span></span><span class="kobospan" id="kobo.810.1">(agent_executor.agent.llm_chain.prompt.template)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.811.1">Here is the corresponding output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.812.1">Answer the following questions as best you can. </span><span class="kobospan" id="kobo.812.2">You have access to the following tools:
Search: useful for when you need to answer questions about current events
Use the following format:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Search]
Action Input: the input to the action
Observation: the result of the action
... </span><span class="kobospan" id="kobo.812.3">(this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
Begin!
</span><span class="kobospan" id="kobo.812.4">Question: {input}
Thought:{agent_scratchpad}
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.813.1">Let’s now test our agent by asking something about the upcoming Olympic games and zooming in on the intermediate steps:</span></p>
<pre class="programlisting"><code class="hljs-code"><span class="kobospan" id="kobo.814.1">agent_executor(</span><span class="hljs-string"><span class="kobospan" id="kobo.815.1">'who are going to be the italian male athletes for climbing at the Paris 2024 Olympics?'</span></span><span class="kobospan" id="kobo.816.1">)
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.817.1">This is the output with intermediate steps:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.818.1">&gt; Entering new AgentExecutor chain... </span><span class="kobospan" id="kobo.818.2">I should search for recent news or updates about the Italian male athletes for climbing at the Paris 2024 Olympics Action: Search Action Input: "Italian male athletes climbing Paris 2024 Olympics"
&gt;Observation: A select group of climbers ensured their participation in the 2024 Paris Olympics. </span><span class="kobospan" id="kobo.818.3">Among them, Italy's Matteo Zurloni and Indonesia's Desak Made Rita Kusuma Dewi didn't just secure their Olympic berths; they also took home the world titles in their respective categories. </span><span class="kobospan" id="kobo.818.4">Thought: I need to refine my search to find a more comprehensive list of Italian male climbers for Paris 2024 Olympics Action: Search Action
&gt;Input: "List of Italian male climbers Paris 2024 Olympics" Observation: Italy fielded a squad of five male gymnasts for Paris after advancing to the final round ... </span><span class="kobospan" id="kobo.818.5">Paris 2024 qualification berths, and medals table - complete list".
</span><span class="kobospan" id="kobo.818.6">&gt;Thought:This information does not seem to be readily available, I should try contacting the Italian climbing federation for more information Action: Search Action Input: "contact information for Italian climbing federation" Observation: IFSC HEADQUARTERS: Corso Vinzaglio 12 10121, Torino, ITALY. </span><span class="kobospan" id="kobo.818.7">Opening Hours: Monday - Friday 9:00 AM - 5:00 PM CEST. </span><span class="kobospan" id="kobo.818.8">IFSC OFFICE:. </span><span class="kobospan" id="kobo.818.9">Thought: I can reach out to the Italian climbing federation to ask for a list of their male athletes participating in the Paris 2024 Olympics
&gt;Action: Contact Action Input: Email or phone call to the Italian climbing federation requesting information on the male athletes for Paris 2024 Olympics Observation: Contact is not a valid tool, try one of [Search]. </span><span class="kobospan" id="kobo.818.10">Thought: I will refine my search further to see if any new information has been released about the Italian male climbers for the Paris 2024 Olympics
&gt;Action: Search Action Input: "Latest updates on Italian male climbers for Paris 2024 Olympics" Observation: Aleksandra Miroslaw and Bassa Mawem have secured quota spots to next summer's Paris 2024 Olympic Games following respective wins at the IFSC ...
</span><span class="kobospan" id="kobo.818.11">&gt;Thought: Based on the information I have gathered, it seems that there is no comprehensive list of Italian male climbers for the Paris 2024 Olympics readily available. </span><span class="kobospan" id="kobo.818.12">However, I do know that Matteo Zurloni is one of the Italian male climbers who has secured a spot at the Olympics.
</span><span class="kobospan" id="kobo.818.13">&gt;Final Answer: Matteo Zurloni is one of the Italian male climbers who has secured a spot at the Paris 2024 Olympics, but a comprehensive list of Italian male climbers for the Olympics is not readily available.
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.819.1">Here is the obtained output:</span></p>
<pre class="programlisting1"><code class="hljs-con"><span class="kobospan" id="kobo.820.1">'Matteo Zurloni is one of the Italian male climbers who has secured a spot at the Paris 2024 Olympics, but a comprehensive list of Italian male climbers for the Olympics is not readily available.'
</span></code></pre>
<p class="normal1"><span class="kobospan" id="kobo.821.1">At the time of this question (7th of October 2023), the answer is definitely correct. </span><span class="kobospan" id="kobo.821.2">Note how the model went through several iterations of </span><code class="inlinecode"><span class="kobospan" id="kobo.822.1">Observation</span></code><span class="kobospan" id="kobo.823.1">/</span><code class="inlinecode"><span class="kobospan" id="kobo.824.1">Thought</span></code><span class="kobospan" id="kobo.825.1">/</span><code class="inlinecode"><span class="kobospan" id="kobo.826.1">Action</span></code><span class="kobospan" id="kobo.827.1"> until it reached the conclusion. </span><span class="kobospan" id="kobo.827.2">This is a great example of how prompting a model to think step by step and explicitly define each step of the reasoning makes it “wiser” and more cautious before answering. </span><span class="kobospan" id="kobo.827.3">It is also a great technique to prevent hallucination.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.828.1">Overall, prompt engineering</span><a id="_idIndexMarker301" class="calibre3"/><span class="kobospan" id="kobo.829.1"> is a powerful discipline, still in its emerging phase yet already widely adopted within LLM-powered applications. </span><span class="kobospan" id="kobo.829.2">In the following chapters, we are going to see concrete applications of this technique.</span></p>
<h1 class="heading" id="_idParaDest-69"><span class="kobospan" id="kobo.830.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.831.1">In this chapter, we covered many aspects of the activity of prompt engineering, a core step in the context of improving the performance of LLMs within your application, as well as customizing it depending on the scenario. </span><span class="kobospan" id="kobo.831.2">Prompt engineering is an emerging discipline that is paving the way for a new category of applications, infused with LLMs.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.832.1">We started with an introduction to the concept of prompt engineering and why it is important, and then moved toward the basic principles – including clear instructions, asking for justification, etc. </span><span class="kobospan" id="kobo.832.2">Then, we moved on to more advanced techniques that are meant to shape the reasoning approach of our LLM: few-shot learning, CoT, and ReAct.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.833.1">In the next chapters, we will see those techniques in action by building real-world applications using LLMs.</span></p>
<h1 class="heading" id="_idParaDest-70"><span class="kobospan" id="kobo.834.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.835.1">ReAct approach: </span><a href="https://arxiv.org/abs/2210.03629" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.836.1">https://arxiv.org/abs/2210.03629</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.837.1">What is prompt engineering?: </span><a href="https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.838.1">https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.839.1">Prompt engineering techniques: </span><a href="https://blog.mrsharm.com/prompt-engineering-guide/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.840.1">https://blog.mrsharm.com/prompt-engineering-guide/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.841.1">Prompt engineering principles: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.842.1">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.843.1">Recency bias: </span><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#repeat-instructions-at-the-end" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.844.1">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#repeat-instructions-at-the-end</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.845.1">Large Language Model Prompt Engineering for Complex Summarization: </span><a href="https://devblogs.microsoft.com/ise/2023/06/27/gpt-summary-prompt-engineering/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.846.1">https://devblogs.microsoft.com/ise/2023/06/27/gpt-summary-prompt-engineering/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.847.1">Language Models are Few-Shot Learners: </span><a href="https://arxiv.org/pdf/2005.14165.pdf" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.848.1">https://arxiv.org/pdf/2005.14165.pdf</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.849.1">IMDb dataset: </span><a href="https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/code" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.850.1">https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/code</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.851.1">ReAct: </span><a href="https://arxiv.org/abs/2210.03629" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.852.1">https://arxiv.org/abs/2210.03629</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.853.1">Chain of Thought Prompting Elicits Reasoning in Large Language Models: </span><a href="https://arxiv.org/abs/2201.11903" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.854.1">https://arxiv.org/abs/2201.11903</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.855.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.856.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.857.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.858.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>