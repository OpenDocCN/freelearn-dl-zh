<html><head></head><body>
<div><h1 class="chapternumber">4</h1>
<h1 class="chaptertitle" id="_idParaDest-55">Prompt Engineering</h1>
<p class="normal">In <em class="italic">Chapter 2</em>, we introduced the concept of prompt engineering as the process of designing and optimizing prompts – the text<a id="_idIndexMarker257" class="calibre3"/> input that guides the behavior of a <strong class="screentext">large language model</strong> (<strong class="screentext">LLM</strong>) – for LLMs for a wide variety of applications and research topics. Since prompts have a massive impact on LLM performance, prompt engineering is a crucial activity while designing LLM-powered applications. In fact, there are several techniques that can be implemented not only to refine your LLM’s responses but also to reduce risks associated with hallucination and bias.</p>
<p class="normal1">In this chapter, we are going to cover the emerging techniques in the field of prompt engineering, starting from basic approaches up to advanced frameworks. By the end of this chapter, you will have the foundations to build functional and solid prompts for your LLM-powered applications, which will also be relevant in the upcoming chapters.</p>
<p class="normal1">We will go through the following topics:</p>
<ul class="calibre14">
<li class="bulletlist">Introduction to prompt engineering</li>
<li class="bulletlist1">Basic principles of prompt engineering</li>
<li class="bulletlist1">Advanced techniques of prompt engineering</li>
</ul>
<h1 class="heading" id="_idParaDest-56">Technical requirements</h1>
<p class="normal">To complete the tasks in this chapter, you will require the following:</p>
<ul class="calibre14">
<li class="bulletlist">OpenAI account and API</li>
<li class="bulletlist1">Python 3.7.1 or later version</li>
</ul>
<p class="normal1">You can find all the code and examples in the book’s GitHub repository at <a href="Chapter_04.xhtml" class="calibre3">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<h1 class="heading" id="_idParaDest-57">What is prompt engineering?</h1>
<p class="normal">A prompt is a text input<a id="_idIndexMarker258" class="calibre3"/> that guides the behavior of an LLM to generate a text output.</p>
<p class="normal1">Prompt engineering is the process of designing effective prompts that elicit high-quality and relevant output from LLMs. Prompt engineering requires creativity, understanding of the LLM, and precision.</p>
<p class="normal1">The following figure shows an example of how a well-written prompt can instruct the same model to perform three different tasks:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_04_01.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 4.1: Example of prompt engineering to specialize LLMs</p>
<p class="normal1">As you might imagine, the prompt becomes one of the key elements for an LLM-powered application’s success. As such, it is pivotal to invest time and resources in this step, following some best practices and principles that we are going to cover in the next sections.</p>
<h1 class="heading" id="_idParaDest-58">Principles of prompt engineering</h1>
<p class="normal">Generally speaking, there are no fixed<a id="_idIndexMarker259" class="calibre3"/> rules to obtain the “perfect” prompt since there are too many variables to be taken into account (the type of model used, the goal of the application, the supporting infrastructure, and so on). Nevertheless, there are some clear principles that have proven to produce positive effects if incorporated into the prompt. Let’s examine some of them.</p>
<h2 class="heading1" id="_idParaDest-59">Clear instructions</h2>
<p class="normal">The principle of giving clear instructions<a id="_idIndexMarker260" class="calibre3"/> is to provide the model with enough information and guidance to perform the task correctly and efficiently. Clear instructions should include the following elements:</p>
<ul class="calibre14">
<li class="bulletlist">The goal or objective of the task, such as “write a poem” or “summarize an article”</li>
<li class="bulletlist1">The format or structure of the expected output, such as “use four lines with rhyming words” or “use bullet points with no more than 10 words each”</li>
<li class="bulletlist1">The constraints or limitations of the task, such as “do not use any profanity” or “do not copy any text from the source”</li>
<li class="bulletlist1">The context or background of the task, such as “the poem is about autumn” or “the article is from a scientific journal”</li>
</ul>
<p class="normal1">Let’s say, for example, that we want our model to fetch any kind of instructions from text and return to us a tutorial in a bullet list. Also, if there are no instructions in the provided text, the model should inform us about that. Here are the steps:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">First, we need to initialize our model. For this purpose, we are going to leverage OpenAI’s GPT-3.5-turbo model. We first install the <code class="inlinecode">openai</code> library:
        <pre class="programlisting2"><code class="hljs-code">$pip install openai == 0.28
</code></pre>
</li>
<li class="bulletlist1">To initialize the model, I used the <code class="inlinecode">openai</code> Python library and set the OpenAI API key as the environmental variable:
        <pre class="programlisting2"><code class="hljs-code">import os
import openai
openai.api_key = os.environment.get('OPENAI_API_KEY')
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": instructions},
    ]
)
</code></pre>
</li>
</ol>
<p class="normal-one">As you can see, the chat model comes<a id="_idIndexMarker261" class="calibre3"/> with two variables placeholders: <code class="inlinecode">system message</code> (or metaprompt), where we define how we want our model to behave, and <code class="inlinecode">instructions</code> (or query), where the user will ask the model its questions.</p>
<ol class="calibre15">
<li class="bulletlist1" value="3">Then, it takes the user’s query (in this case, the text instructions). For this scenario, I set the two variables <code class="inlinecode">system_message</code> and <code class="inlinecode">instructions</code> as follows:
        <pre class="programlisting2"><code class="hljs-code">system_message = """
You are an AI assistant that helps humans by generating tutorials given a text.
You will be provided with a text. If the text contains any kind of istructions on how to proceed with something, generate a tutorial in a bullet list.
Otherwise, inform the user that the text does not contain any instructions.
Text:
"""
instructions = """
To prepare the known sauce from Genova, Italy, you can start by toasting the pine nuts to then coarsely
chop them in a kitchen mortar together with basil and garlic. Then, add half of the oil in the kitchen mortar and season with salt and pepper.
Finally, transfer the pesto to a bowl and stir in the grated Parmesan cheese.
"""
</code></pre>
</li>
<li class="bulletlist1">Now let’s test our model:
        <pre class="programlisting2"><code class="hljs-code">print(response['choices'][0]['message']['content'])
</code></pre>
</li>
</ol>
<p class="normal-one">We get the following output:</p>
<pre class="programlisting3"><code class="hljs-con">Sure, I can generate a tutorial based on the given text. Here's a bullet list on how to prepare the Genovese pesto sauce:
- Toast the pine nuts on a pan until they are slightly browned.
- Coarsely chop the toasted pine nuts, along with basil leaves and garlic cloves, using a kitchen mortar.
- Add half of the olive oil into the kitchen mortar, and season the mixture with salt and pepper.
- Grind the mixture until it becomes a smooth paste.
- Transfer the pesto to a bowl, and stir in the grated Parmesan cheese.
- You can add more olive oil to achieve the desired consistency for the sauce.
- Your Genovese pesto sauce is now ready. It can be served with pasta, vegetables, or as a dip for bread. Enjoy!
</code></pre>
<ol class="calibre15">
<li class="bulletlist1" value="5">Note that if we pass the model<a id="_idIndexMarker262" class="calibre3"/> another text that does not contain any instructions, it will be able to respond as we instructed it:
        <pre class="programlisting2"><code class="hljs-code">response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": 'the sun is shining and dogs are running on the beach.'},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
</li>
</ol>
<p class="normal-one">The following is the corresponding output:</p>
<pre class="programlisting3"><code class="hljs-con">As there are no instructions provided in the text you have given me, it is not possible to create a tutorial. May I have a different text to work with?
</code></pre>
<p class="normal1">By giving clear instructions, you can help the model understand what you want it to do and how you want it to do it. This can improve the quality and relevance of the model’s output and reduce the need for further revisions or corrections.</p>
<p class="normal1">However, sometimes, there are scenarios<a id="_idIndexMarker263" class="calibre3"/> where clarity is not enough. We might need to infer the way of thinking of our LLM to make it more robust with respect to its task. In the next section, we are going to examine one of these techniques, which will be very useful in the case of accomplishing complex tasks.</p>
<h2 class="heading1" id="_idParaDest-60">Split complex tasks into subtasks</h2>
<p class="normal">As discussed earlier, prompt engineering<a id="_idIndexMarker264" class="calibre3"/> is a technique that involves designing effective inputs for LLMs to perform various tasks. Sometimes, the tasks are too complex or ambiguous for a single prompt to handle, and it is better to split them into simpler subtasks that can be solved by different prompts.</p>
<p class="normal1">Here are some examples of splitting complex tasks into subtasks:</p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext">Text summarization:</strong> A complex task that involves<a id="_idIndexMarker265" class="calibre3"/> generating a concise and accurate summary of a long text. This task can be split into subtasks such as:<ul class="calibre17">
<li class="bulletlist2">Extracting the main points or keywords from the text</li>
<li class="bulletlist3">Rewriting the main points or keywords in a coherent and fluent way</li>
<li class="bulletlist3">Trimming the summary to fit a desired length or format</li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext">Machine translation:</strong> A complex task that involves<a id="_idIndexMarker266" class="calibre3"/> translating a text from one language to another. This task can be split into subtasks such as:<ul class="calibre17">
<li class="bulletlist2">Detecting the source language of the text</li>
<li class="bulletlist3">Converting the text into an intermediate representation that preserves the meaning and structure of the original text</li>
<li class="bulletlist3">Generating the text in the target language from the intermediate representation</li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext">Poem generation</strong>: A creative task that involves<a id="_idIndexMarker267" class="calibre3"/> producing a poem that follows a certain style, theme, or mood. This task can be split into subtasks such as:<ul class="calibre17">
<li class="bulletlist2">Choosing a poetic form (such as sonnet, haiku, limerick, etc.) and a rhyme scheme (such as ABAB, AABB, ABCB, etc.) for the poem</li>
<li class="bulletlist3">Generating a title and a topic for the poem based on the user’s input or preference</li>
<li class="bulletlist3">Generating the lines or verses of the poem that match the chosen form, rhyme scheme, and topic</li>
<li class="bulletlist3">Refining and polishing the poem<a id="_idIndexMarker268" class="calibre3"/> to ensure coherence, fluency, and originality</li>
</ul>
</li>
<li class="bulletlist1"><strong class="screentext">Code generation</strong>: A technical task that involves<a id="_idIndexMarker269" class="calibre3"/> producing a code snippet that performs a specific function or task. This task can be split into subtasks such as:<ul class="calibre17">
<li class="bulletlist2">Choosing a programming language (such as Python, Java, C++, etc.) and a framework or library (such as TensorFlow, PyTorch, React, etc.) for the code</li>
<li class="bulletlist3">Generating a function name and a list of parameters and return values for the code based on the user’s input or specification</li>
<li class="bulletlist3">Generating the body of the function that implements the logic and functionality of the code</li>
<li class="bulletlist3">Adding comments and documentation to explain the code and its usage</li>
</ul>
</li>
</ul>
<p class="normal1">Let’s consider the following<a id="_idIndexMarker270" class="calibre3"/> example in Python, where we will ask our model to generate a summary of an article:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">We will leverage OpenAI’s GPT-3.5-turbo model in a manner similar to the example discussed earlier in this chapter:
        <pre class="programlisting2"><code class="hljs-code">import os
import openai
openai.api_key = os.environ.get("OPENAI_API_KEY")
response = openai.ChatCompletion.create(
    model="gpt-35-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": article},
    ]
)
</code></pre>
</li>
<li class="bulletlist1">Let’s set both the <code class="inlinecode">system_message</code> and <code class="inlinecode">article</code> variables as follows (you can find the entire scripts in the book’s GitHub repository):
        <pre class="programlisting2"><code class="hljs-code">system_message = """
You are an AI assistant that summarizes articles.
To complete this task, do the following subtasks:
Read the provided article context comprehensively and identify the main topic and key points
Generate a paragraph summary of the current article context that captures the essential information and conveys the main idea
Print each step of the process.
Article:
"""
article = """
Recurrent neural networks, long short-term memory, and gated recurrent neural networks
in particular, […]
"""
</code></pre>
</li>
<li class="bulletlist1">To see the output, you<a id="_idIndexMarker271" class="calibre3"/> can run the following code:
        <pre class="programlisting2"><code class="hljs-code">print(response['choices'][0]['message']['content'])
</code></pre>
</li>
</ol>
<p class="normal-one">Here is the obtained output:</p>
<pre class="programlisting3"><code class="hljs-con">Summary:
The article discusses the use of recurrent neural networks, specifically long short-term memory and gated recurrent neural networks, in sequence modeling and transduction problems. These models have achieved great success but are limited by their inherently sequential nature. Attention mechanisms have become popular in addressing this issue but are usually used in conjunction with recurrent networks. The authors propose the Transformer, an architecture that relies solely on attention mechanisms instead of recurrence. The Transformer allows for greater parallelization and can achieve state-of-the-art results in machine translation after only twelve hours of training on eight GPUs.
Steps:
1. The article discusses the success and limitations of recurrent neural networks in sequence modeling and transduction problems.
2. Attention mechanisms have become popular in addressing the limitations of recurrence but are usually used alongside recurrent networks.
3. The authors propose the Transformer, a model architecture that relies solely on attention mechanisms and allows for greater parallelization.
4. The Transformer can achieve state-of-the-art results in machine translation after only twelve hours of training on eight GPUs.
</code></pre>
<p class="normal1">As you can see, the model was able to produce a high-quality summary based on the key topics extracted (and displayed) from the given article. The fact that we prompted the model to split the task into subtasks “forced” it to reduce the complexity of each subtask, hence improving the quality of the final result. This approach can also lead to noticeable results when we deal with scenarios such as mathematical problems since it enhances the analytical reasoning capabilities of the model.</p>
<div><p class="normal1"><strong class="screentext">Note</strong></p>
<p class="normal1">In a landscape of many different LLMs, it is crucial to know that the very same system message may not be as efficient in all models. A system message that perfectly works with GPT-4 might not be as efficient when applied to Llama 2, for example. Therefore, it is pivotal to design the prompt in accordance with the type of LLM you decide to pick for your application.</p>
</div>
<p class="normal1">Splitting complex tasks<a id="_idIndexMarker272" class="calibre3"/> into easier subtasks is a powerful technique; nevertheless, it does not address one of the main risks of LLM-generated content, that is, having a wrong output. In the next two sections, we are going to see some techniques that are mainly aimed at addressing this risk.</p>
<h2 class="heading1" id="_idParaDest-61">Ask for justification</h2>
<p class="normal">LLMs are built in such a way<a id="_idIndexMarker273" class="calibre3"/> that they predict the next token based on the previous ones without looking back at their generations. This might lead the model to output wrong content to the user, yet in a very convincing way. If the LLM-powered application does not provide a specific reference to that response, it might be hard to validate the ground truth behind it. Henceforth, specifying in the prompt to support the LLM’s answer with some reflections and justification could prompt the model to recover from its actions. Furthermore, asking for justification might be useful also in case of answers that are right but we simply don’t know the LLM’s reasoning behind it. For example, let’s say we want our LLM to solve riddles. To do so, we can instruct it as follows:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
You are an AI assistant specialized in solving riddles.
Given a riddle, solve it the best you can.
Provide a clear justification of your answer and the reasoning behind it.
Riddle:
"""
</code></pre>
<p class="normal1">As you can see, I’ve specified in the metaprompt<a id="_idIndexMarker274" class="calibre3"/> to the LLM to justify its answer and also provide its reasoning. Let’s see how it works:</p>
<pre class="programlisting"><code class="hljs-code">riddle = """
What has a face and two hands, but no arms or legs?
"""
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": riddle},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">The following is the obtained output:</p>
<pre class="programlisting1"><code class="hljs-con">The answer to this riddle is a clock.
A clock has a face with numbers and two hands (the hour and minute hand), which rotate around the center of the clock to show the time. Although it doesn't have any arms or legs like humans do, it still has the ability to measure time, which is important in our daily lives.
Hence, a clock is the correct answer to this riddle.
</code></pre>
<p class="normal1">Justifications are a great tool to make your model more reliable and robust since they force it to “rethink” its output, as well as provide us with a view of how the reasoning was set to solve the problem.</p>
<p class="normal1">With a similar approach, we could also intervene at different prompt levels to improve our LLM’s performance. For example, we might discover that the model is systematically tackling a mathematical problem in the wrong way; henceforth, we might want to suggest the right approach directly at the metaprompt level. Another example might be that of asking the model to generate multiple outputs – along with their justifications – to evaluate different reasoning<a id="_idIndexMarker275" class="calibre3"/> techniques and prompt the best one in the metaprompt.</p>
<p class="normal1">In the next section, we are going to focus on one of these examples, more specifically, the possibility of generating multiple outputs and then picking the most likely one.</p>
<h2 class="heading1" id="_idParaDest-62">Generate many outputs, then use the model to pick the best one</h2>
<p class="normal">As we saw in the previous section, LLMs<a id="_idIndexMarker276" class="calibre3"/> are built in such a way that they predict the next token based on the previous ones without looking back at their generations. If this is the case, if one sampled token is the wrong one (in other words, if the model is unlucky), the LLM will keep generating wrong tokens and, henceforth, wrong content. Now, the bad news is that, unlike humans, LLMs cannot recover from errors on their own. This means that, if we ask them, they acknowledge the error, but we need to explicitly prompt them to think about that.</p>
<p class="normal1">One way to overcome this limitation is to broaden the space of probabilities of picking the right token. Rather than generating just one response, we can prompt the model to generate multiple responses, and then pick the one that is most suitable for the user’s query. This splits the job into two subtasks for our LLM:</p>
<ol class="calibre15">
<li class="bulletlist1" value="1">Generating multiple responses to the user’s query</li>
<li class="bulletlist1">Comparing those responses and picking the best one, according to some criteria we can specify in the metaprompt</li>
</ol>
<p class="normal1">Let’s see an example, following up from the riddles examined in the previous section:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
You are an AI assistant specialized in solving riddles.
Given a riddle, you have to generate three answers to the riddle.
For each answer, be specific about the reasoning you made.
Then, among the three answers, select the one that is most plausible given the riddle.
Riddle:
"""
riddle = """
What has a face and two hands, but no arms or legs?
"""
</code></pre>
<p class="normal1">In this case, I’ve prompted<a id="_idIndexMarker277" class="calibre3"/> the model to generate three answers to the riddle, then to give me the most likely, justifying why. Let’s see the result:</p>
<pre class="programlisting"><code class="hljs-code">response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": riddle},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">We then get the following output:</p>
<pre class="programlisting1"><code class="hljs-con">Answer 1: A clock. A clock has a face with numbers and two hands to show minutes and hours. It does not have arms or legs because it is an object, and it does not have intrinsic limbs.
Answer 2: A watch. Similar to a clock, a watch also has a face with numbers and two hands to indicate the time. Although it is usually worn on a person's wrist, it does not have arms or legs.
Answer 3: A wall. While this answer may be less obvious, a wall does, in a way, have a face and two hands. The face is its outer surface, and the hands are the clock that usually hangs on the wall.
Among the three answers, the most plausible one is answer 1: a clock. The riddle distinctly mentions "two hands" and it's logical to associate hands with those that are found in clocks to indicate time.
</code></pre>
<p class="normal1">As you can see, the model selected the most plausible answer along with a justification of its choice. It is interesting to note that “clock” and “watch” might seem similar responses; however, the model specified that “watch” is usually worn on a person’s wrist and, even though it doesn’t mean it has arms or legs, this element might have lowered the probability of being the correct answer.</p>
<p class="normal1">What would you have picked?</p>
<p class="normal1">As discussed earlier, forcing<a id="_idIndexMarker278" class="calibre3"/> the model to tackle a problem with different approaches is a way to collect multiple samples of reasonings, which might serve as further instructions in the metaprompt. For example, if we want the model to always propose something that is not the most straightforward solution to a problem – in other words, if we want it to “think differently” – we might force it to solve a problem in N ways and then use the most creative reasoning as a framework in the metaprompt.</p>
<p class="normal1">The last element we are going to examine is the overall structure we want to give to our metaprompt. In fact, in previous examples, we saw a sample system message with some statements and instructions. In the next section, we will see how the order and “strength” of those statements and instructions are not invariants.</p>
<h2 class="heading1" id="_idParaDest-63">Repeat instructions at the end</h2>
<p class="normal">LLMs tend not to process the metaprompt<a id="_idIndexMarker279" class="calibre3"/> attributing the same weight or imprortance to all the sections. In fact, in his blog post <em class="italic">Large Language Model Prompt Engineering for Complex Summarization</em>, John Stewart (a software engineer at Microsoft) found some interesting outcomes<a id="_idIndexMarker280" class="calibre3"/> from arranging prompt sections (<a href="https://devblogs.microsoft.com/ise/gpt-summary-prompt-engineering/" class="calibre3">https://devblogs.microsoft.com/ise/gpt-summary-prompt-engineering/</a>). More specifically, after several experimentations, he found that repeating the main instruction at the end of the prompt can help<a id="_idIndexMarker281" class="calibre3"/> the model overcome its inner <strong class="screentext">recency bias</strong>.</p>
<div><p class="normal1"><strong class="screentext">Definition</strong></p>
<p class="normal1">Recency bias is the tendency<a id="_idIndexMarker282" class="calibre3"/> of LLMs to give more weight to the information that appears near the end of a prompt, and ignore or forget the information that appears earlier. This can lead to inaccurate or inconsistent responses that do not take into account the whole context of the task. For example, if the prompt is a long conversation between two people, the model may only focus on the last few messages and disregard the previous ones.</p>
</div>
<p class="normal1">Let’s look at some ways to overcome recency bias:</p>
<ul class="calibre14">
<li class="bulletlist">One possible way to overcome<a id="_idIndexMarker283" class="calibre3"/> recency bias is to break down the task into smaller steps or subtasks and provide feedback or guidance along the way. This can help the model focus on each step and avoid getting lost in irrelevant details. We’ve covered this technique in the <em class="italic">Split complex tasks into subtasks</em> section in, which we discussed splitting complex tasks into easier subtasks.</li>
<li class="bulletlist1">Another way to overcome recency bias with prompt engineering techniques is to repeat the instructions or the main goal of the task at the end of the prompt. This can help remind the model of what it is supposed to do and what kind of response it should generate.</li>
</ul>
<p class="normal-one">For instance, let’s say we want our model to output the sentiment of a whole chat history between an AI agent and the user. We want to make sure that the model will output the sentiment in lowercase and without punctuation.</p>
<p class="normal1">Let’s consider the following <a id="_idIndexMarker284" class="calibre3"/>example (the conversation is truncated, but you can find the whole code in the book’s GitHub repository). In this case, the key instruction is that of having as output only the sentiment in lowercase and without punctuation:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
You are a sentiment analyzer. You classify conversations into three categories: positive, negative, or neutral.
Return only the sentiment, in lowercase and without punctuation.
Conversation:
"""
conversation = """
Customer: Hi, I need some help with my order.
AI agent: Hello, welcome to our online store. I'm an AI agent and I'm here to assist you.
Customer: I ordered a pair of shoes yesterday, but I haven't received a confirmation email yet. Can you check the status of my order?
[…]
"""
</code></pre>
<p class="normal1">In this scenario, we have key instructions before the conversation, so let’s initialize our model and feed it with the two variables <code class="inlinecode">system_message</code> and <code class="inlinecode">conversation</code>:</p>
<pre class="programlisting"><code class="hljs-code">response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": conversation},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">Here is the output that we receive:</p>
<pre class="programlisting1"><code class="hljs-con">Neutral
</code></pre>
<p class="normal1">The model didn’t follow the instruction<a id="_idIndexMarker285" class="calibre3"/> of having only lowercase letters. Let’s try to repeat the instruction also at the end of the prompt:</p>
<pre class="programlisting"><code class="hljs-code">system_message = f"""
You are a sentiment analyzer. You classify conversations into three categories: positive, negative, or neutral.
Return only the sentiment, in lowercase and without punctuation.
Conversation:
{conversation}
Remember to return only the sentiment, in lowercase and without punctuation
"""
</code></pre>
<p class="normal1">Again, let’s invoke our model with the updated <code class="inlinecode">system_message</code>:</p>
<pre class="programlisting"><code class="hljs-code">response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "user", "content": system_message},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">Here is the corresponding output:</p>
<pre class="programlisting1"><code class="hljs-con">neutral
</code></pre>
<p class="normal1">As you can see, now the model was able to provide exactly the output we desired. This approach is particularly useful whenever we have a conversation history to keep storing in the context window. If this is the case, having the main instructions at the beginning might induce the model not to have<a id="_idIndexMarker286" class="calibre3"/> them in mind once it also goes through the whole history, hence reducing their strength.</p>
<h2 class="heading1" id="_idParaDest-64">Use delimiters</h2>
<p class="normal">The last principle to be covered<a id="_idIndexMarker287" class="calibre3"/> is related to the format we want to give to our metaprompt. This helps our LLM to better understand its intents as well as relate different sections and paragraphs to each other.</p>
<p class="normal1">To achieve this, we can use delimiters within our prompt. A delimiter can be any sequence of characters or symbols that is clearly mapping a schema rather than a concept. For example, we can consider the following sequences to be delimiters:</p>
<ul class="calibre14">
<li class="bulletlist"><code class="inlinecode">&gt;&gt;&gt;&gt;</code></li>
<li class="bulletlist1"><code class="inlinecode">====</code></li>
<li class="bulletlist1"><code class="inlinecode">------</code></li>
<li class="bulletlist1"><code class="inlinecode">####</code></li>
<li class="bulletlist1"><code class="inlinecode">` ` ` ` `</code></li>
</ul>
<p class="normal1">This leads to a series of benefits, including:</p>
<ul class="calibre14">
<li class="bulletlist">Clear separation: Delimiters mark distinct sections within a prompt, separating instructions, examples, and desired output.</li>
<li class="bulletlist1">Guidance for LLMs: Proper use of delimiters removes ambiguity, guiding the model effectively.</li>
<li class="bulletlist1">Enhanced precision: Delimiters improve prompt understanding, resulting in more relevant responses.</li>
<li class="bulletlist1">Improved coherence: Effective use of delimiters organizes instructions, inputs, and outputs, leading to coherent responses.</li>
</ul>
<p class="normal1">Let’s consider, for example, a metaprompt<a id="_idIndexMarker288" class="calibre3"/> that aims at instructing the model to translate user’s tasks into Python code, providing an example to do so:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
You are a Python expert who produces Python code as per the user's request.
===&gt;START EXAMPLE
---User Query---
Give me a function to print a string of text.
---User Output---
Below you can find the described function:
```def my_print(text):
     return print(text)
```
&lt;===END EXAMPLE
"""
query = "generate a Python function to calculate the nth Fibonacci number"
</code></pre>
<p class="normal1">In the above example, we’ve used delimiters to both specify the beginning and end of an example for a one-shot learning approach and, within the example, specify the Python code snippet.</p>
<p class="normal1">Let’s see how it works:</p>
<pre class="programlisting"><code class="hljs-code">response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": query},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">Here is our output:</p>
<pre class="programlisting1"><code class="hljs-con">Sure, here's a Python function to calculate the nth Fibonacci number:
```python def fibonacci(n):
if n &lt; 0:
return None
elif n == 0:
return 0
elif n == 1:
return 1
else:
return fibonacci(n-1) + fibonacci(n-2) ```
The function takes a non-negative integer `n` as input and returns the nth Fibonacci number. If `n` is less than 0, the function returns `None`. If `n` is 0 or 1, the function returns the value of `n`. In all other cases, the function recursively calls itself with `n-1` and `n-2` and adds their results to compute the nth Fibonacci number.
</code></pre>
<p class="normal1">As you can see, it also printed the code with backticks, as shown within the system message.</p>
<p class="normal1">All the principles examined up to this point are general rules that can make your LLM-powered application more robust. Those techniques should be kept in mind regardless of the type of application you are developing since they are general best practices that improve your LLM<a id="_idIndexMarker289" class="calibre3"/> performance. In the following section, we are going to see some advanced techniques for prompt engineering.</p>
<h1 class="heading" id="_idParaDest-65">Advanced techniques</h1>
<p class="normal">Advanced techniques might be implemented for specific scenarios and address the way the model reasons and thinks about the answer before providing it to the final user. Let’s look at some of these in the upcoming sections.</p>
<h2 class="heading1" id="_idParaDest-66">Few-shot approach</h2>
<p class="normal">In their paper <em class="italic">Language Models are Few-Shot Learners</em>, Tom Brown et al. demonstrate<a id="_idIndexMarker290" class="calibre3"/> that GPT-3 can achieve strong performance on many NLP tasks in a few-shot setting. This means that for all tasks, GPT-3 is applied without any fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.</p>
<p class="normal1">This is an example and evidence of how the concept of few-shot learning – which means providing the model with examples of how we would like it to respond – is a powerful technique that enables model customization without interfering with the overall architecture.</p>
<p class="normal1">For example, let’s say we want our model to generate a tagline for a new product line of climbing shoes we’ve just coined – Elevation Embrace. We have an idea of what the tagline should be like – concise and direct. We could explain it to the model in plain text; however, it might be more effective simply to provide it with some examples of similar projects.</p>
<p class="normal1">Let’s see an implementation with code:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
You are an AI marketing assistant. You help users to create taglines for new product names.
Given a product name, produce a tagline similar to the following examples:
Peak Pursuit - Conquer Heights with Comfort
Summit Steps - Your Partner for Every Ascent
Crag Conquerors - Step Up, Stand Tall
Product name:
"""
product_name = 'Elevation Embrace'
</code></pre>
<p class="normal1">Let’s see how our model will handle this request:</p>
<pre class="programlisting"><code class="hljs-code">response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": product_name},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">The following is our output:</p>
<pre class="programlisting1"><code class="hljs-con">Tagline idea: Embrace the Heights with Confidence.
</code></pre>
<p class="normal1">As you can see, it maintained<a id="_idIndexMarker291" class="calibre3"/> the style, length, and also writing convention of the provided taglines. This is extremely useful when you want your model to follow examples you already have, such as fixed templates.</p>
<p class="normal1">Note that, most of the time, few-shot learning is powerful enough to customize a model even in extremely specialized scenarios, where we could think about fine-tuning as the proper tool. In fact, proper few-shot learning could be as effective as a fine-tuning process.</p>
<p class="normal1">Let’s look at another example. Let’s say we want to develop a model that specializes in sentiment analysis. To do so, we provide it with a series of examples of texts with different sentiments, alongside the output we would like – positive or negative. Note that this set of examples is nothing but a small training set for supervised learning tasks; the only difference from fine-tuning is that we are not updating the model’s parameters.</p>
<p class="normal1">To provide you with a concrete representation of what was said above, let’s provide our model with just two examples for each label:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
You are a binary classifier for sentiment analysis.
Given a text, based on its sentiment, you classify it into one of two categories: positive or negative.
You can use the following texts as examples:
Text: "I love this product! It's fantastic and works perfectly."
Positive
Text: "I'm really disappointed with the quality of the food."
Negative
Text: "This is the best day of my life!"
Positive
Text: "I can't stand the noise in this restaurant."
Negative
ONLY return the sentiment as output (without punctuation).
Text:
"""
</code></pre>
<p class="normal1">To test our classifier, I’ve used the IMDb database<a id="_idIndexMarker292" class="calibre3"/> of movie reviews<a id="_idIndexMarker293" class="calibre3"/> available on Kaggle at <a href="https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/data" class="calibre3">https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/data</a>. As you can see, the dataset contains many movie reviews along with their associated sentiment – positive or negative. Let’s substitute the binary label of 0–1 with a verbose label of Negative–Positive:</p>
<pre class="programlisting"><code class="hljs-code">import numpy as np
import pandas as pd
df = pd .read_csv('movie.csv', encoding='utf-8')
df['label'] = df['label'].replace({0: 'Negative', 1: 'Positive'})
df.head()
</code></pre>
<p class="normal1">This gives us the first few records of the dataset, which are as follows:</p>
<figure class="mediaobject"><img alt="" role="presentation" src="img/B21714_04_02.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 4.2: First observations of the movie dataset</p>
<p class="normal1">Now, we want to test the performance of our model over a sample of 10 observations of this dataset:</p>
<pre class="programlisting"><code class="hljs-code">df = df.sample(n=10, random_state=42)
def process_text(text):
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": text},
        ]
    )
    return response['choices'][0]['message']['content']
df['predicted'] = df['text'].apply(process_text)
print(df)
</code></pre>
<p class="normal1">The following is our output:</p>
<figure class="mediaobject"><img alt="A text on a white background  Description automatically generated" src="img/B21714_04_03.png" class="calibre4"/></figure>
<p class="packt_figref">Figure 4.3: Output of a GPT-3.5 model with few-shot examples</p>
<p class="normal1">As you can see, by comparing<a id="_idIndexMarker294" class="calibre3"/> the <code class="inlinecode">label</code> and <code class="inlinecode">predicted</code> columns, the model was able to correctly classify all the reviews, without even fine-tuning! This is just an example of what you can achieve – in terms of model specialization – with the technique of few-shot learning.</p>
<h2 class="heading1" id="_idParaDest-67">Chain of thought</h2>
<p class="normal">Introduced in the<a id="_idIndexMarker295" class="calibre3"/> paper <em class="italic">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em> by Wei et al., <strong class="screentext">chain of thought</strong> (<strong class="screentext">CoT</strong>) is a technique that enables complex reasoning capabilities through intermediate reasoning steps. It also encourages the model to explain its reasoning, “forcing” it not to be too fast and risking giving the wrong response (as we saw in previous sections).</p>
<p class="normal1">Let’s say that we want to prompt our LLM to solve generic first-degree equations. To do so, we are going to provide it with a basic reasoning list that it might want to follow:</p>
<pre class="programlisting"><code class="hljs-code">system_message = """
To solve a generic first-degree equation, follow these steps:
1. **Identify the Equation:** Start by identifying the equation you want to solve. It should be in the form of "ax + b = c," where 'a' is the coefficient of the variable, 'x' is the variable, 'b' is a constant, and 'c' is another constant.
2. **Isolate the Variable:** Your goal is to isolate the variable 'x' on one side of the equation. To do this, perform the following steps:
 
   a. **Add or Subtract Constants:** Add or subtract 'b' from both sides of the equation to move constants to one side.
 
   b. **Divide by the Coefficient:** Divide both sides by 'a' to isolate 'x'. If 'a' is zero, the equation may not have a unique solution.
3. **Simplify:** Simplify both sides of the equation as much as possible.
4. **Solve for 'x':** Once 'x' is isolated on one side, you have the solution. It will be in the form of 'x = value.'
5. **Check Your Solution:** Plug the found value of 'x' back into the original equation to ensure it satisfies the equation. If it does, you've found the correct solution.
6. **Express the Solution:** Write down the solution in a clear and concise form.
7. **Consider Special Cases:** Be aware of special cases where there may be no solution or infinitely many solutions, especially if 'a' equals zero.
Equation:
"""
</code></pre>
<p class="normal1">Let’s see how it can<a id="_idIndexMarker296" class="calibre3"/> be implemented:</p>
<pre class="programlisting"><code class="hljs-code">equation = "3x + 5 = 11"
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo", # engine = "deployment_name".
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": equation},
    ]
)
#print(response)
print(response['choices'][0]['message']['content'])
</code></pre>
<p class="normal1">The following is our output:</p>
<pre class="programlisting1"><code class="hljs-con">1. Identify the Equation: The equation is 3x + 5 = 11.
2. Isolate the Variable:
  
   a. Subtract Constants: Subtract 5 from both sides to move constants to one side:
  
      3x + 5 - 5 = 11 - 5
  
   Simplifying, we get:
  
      3x = 6
  
   b. Divide by the Coefficient: Divide both sides by 3 to isolate 'x':
  
      3x/3 = 6/3
  
   Simplifying, we get:
  
      x = 2
  
3. Simplify: Both sides are already simplified.
4. Solve for 'x': x = 2.
5. Check Your Solution: Plug x = 2 back into the original equation:
   3(2) + 5 = 11
  
   Simplifying, we get:
  
   6 + 5 = 11
  
   The left side equals the right side, so the solution is correct.
6. Express the Solution: The solution to the equation 3x + 5 = 11 is x = 2.
7. Consider Special Cases: Since the coefficient 3 is not zero, there are no special cases to consider.
</code></pre>
<p class="normal1">As you can see, the model clearly followed the seven steps specified in the metaprompt, which also allows the model to “take its time” to perform this task. Note that you can also combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.</p>
<p class="normal1">With CoT, we are prompting<a id="_idIndexMarker297" class="calibre3"/> the model to generate intermediate reasoning steps. This is also a component of another reasoning technique, which we are going to examine in the next section.</p>
<h2 class="heading1" id="_idParaDest-68">ReAct</h2>
<p class="normal">Introduced in the<a id="_idIndexMarker298" class="calibre3"/> paper <em class="italic">ReAct: Synergizing Reasoning and Acting in Language Models</em> by Yao et al., <strong class="screentext">ReAct</strong> (<strong class="screentext">Reason and Act</strong>) is a general paradigm that combines reasoning and acting with LLMs. ReAct prompts the language model to generate verbal reasoning traces and actions for a task, and also receives observations from external sources such as web searches or databases. This allows the language model to perform dynamic reasoning and quickly adapt its action plan based on external information. For example, you can prompt the language model to answer a question by first reasoning about the question, then performing an action to send a query to the web, then receiving an observation from the search results, and then continuing with this thought, action, observation loop until it reaches a conclusion.</p>
<p class="normal1">The difference between CoT and ReAct approaches is that CoT prompts the language model to generate intermediate reasoning steps for a task, while ReAct prompts the language model to generate intermediate reasoning steps, actions, and observations for a task.</p>
<p class="normal1">Note that the “action” phase is generally related to the possibility for our LLM to interact with external tools, such as a web search.</p>
<p class="normal1">For example, let’s say we want<a id="_idIndexMarker299" class="calibre3"/> to ask our model for some up-to-date information about the upcoming Olympic games. To do so, we are going to build a smart LangChain agent (as described in <em class="italic">Chapter 2</em>) leveraging <code class="inlinecode">SerpAPIWrapperWrapper</code> (to wrap the <code class="inlinecode">SerpApi</code> to navigate the web), the <code class="inlinecode">AgentType</code> tool (to decide which type of agent to use for our goal), and other prompt-related modules (to make it easier to “templatize” our instructions). Let’s see how we can do this (I won’t dive deeper into each component of the following code since the next chapter will be entirely focused on LangChain and its main components):</p>
<pre class="programlisting"><code class="hljs-code">import os
from dotenv import load_dotenv
from langchain import SerpAPIWrapper
from langchain.agents import AgentType, initialize_agent
from langchain.chat_models import ChatOpenAI
from langchain.tools import BaseTool, StructuredTool, Tool, tool
from langchain.schema import HumanMessage
model = ChatOpenAI(
    model_name='gpt-35-turbo'
)
load_dotenv()
key = os.environ["SERPAPI_API_KEY"]
search = SerpAPIWrapper()
tools = [
    Tool.from_function(
        func=search.run,
        name="Search",
        description="useful for when you need to answer questions about current events"
    )
    ]
agent_executor = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
</code></pre>
<p class="normal1">As you can see, for this purpose, I’ve used a pre-built<a id="_idIndexMarker300" class="calibre3"/> agent type available in LangChain called <code class="inlinecode">ZERO_SHOT_REACT_DESCRIPTION</code>. It comes with a precompiled prompt that follows the ReAct approach. Let’s inspect that prompt:</p>
<pre class="programlisting"><code class="hljs-code">print(agent_executor.agent.llm_chain.prompt.template)
</code></pre>
<p class="normal1">Here is the corresponding output:</p>
<pre class="programlisting1"><code class="hljs-con">Answer the following questions as best you can. You have access to the following tools:
Search: useful for when you need to answer questions about current events
Use the following format:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [Search]
Action Input: the input to the action
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: I now know the final answer
Final Answer: the final answer to the original input question
Begin!
Question: {input}
Thought:{agent_scratchpad}
</code></pre>
<p class="normal1">Let’s now test our agent by asking something about the upcoming Olympic games and zooming in on the intermediate steps:</p>
<pre class="programlisting"><code class="hljs-code">agent_executor('who are going to be the italian male athletes for climbing at the Paris 2024 Olympics?')
</code></pre>
<p class="normal1">This is the output with intermediate steps:</p>
<pre class="programlisting1"><code class="hljs-con">&gt; Entering new AgentExecutor chain... I should search for recent news or updates about the Italian male athletes for climbing at the Paris 2024 Olympics Action: Search Action Input: "Italian male athletes climbing Paris 2024 Olympics"
&gt;Observation: A select group of climbers ensured their participation in the 2024 Paris Olympics. Among them, Italy's Matteo Zurloni and Indonesia's Desak Made Rita Kusuma Dewi didn't just secure their Olympic berths; they also took home the world titles in their respective categories. Thought: I need to refine my search to find a more comprehensive list of Italian male climbers for Paris 2024 Olympics Action: Search Action
&gt;Input: "List of Italian male climbers Paris 2024 Olympics" Observation: Italy fielded a squad of five male gymnasts for Paris after advancing to the final round ... Paris 2024 qualification berths, and medals table - complete list".
&gt;Thought:This information does not seem to be readily available, I should try contacting the Italian climbing federation for more information Action: Search Action Input: "contact information for Italian climbing federation" Observation: IFSC HEADQUARTERS: Corso Vinzaglio 12 10121, Torino, ITALY. Opening Hours: Monday - Friday 9:00 AM - 5:00 PM CEST. IFSC OFFICE:. Thought: I can reach out to the Italian climbing federation to ask for a list of their male athletes participating in the Paris 2024 Olympics
&gt;Action: Contact Action Input: Email or phone call to the Italian climbing federation requesting information on the male athletes for Paris 2024 Olympics Observation: Contact is not a valid tool, try one of [Search]. Thought: I will refine my search further to see if any new information has been released about the Italian male climbers for the Paris 2024 Olympics
&gt;Action: Search Action Input: "Latest updates on Italian male climbers for Paris 2024 Olympics" Observation: Aleksandra Miroslaw and Bassa Mawem have secured quota spots to next summer's Paris 2024 Olympic Games following respective wins at the IFSC ...
&gt;Thought: Based on the information I have gathered, it seems that there is no comprehensive list of Italian male climbers for the Paris 2024 Olympics readily available. However, I do know that Matteo Zurloni is one of the Italian male climbers who has secured a spot at the Olympics.
&gt;Final Answer: Matteo Zurloni is one of the Italian male climbers who has secured a spot at the Paris 2024 Olympics, but a comprehensive list of Italian male climbers for the Olympics is not readily available.
</code></pre>
<p class="normal1">Here is the obtained output:</p>
<pre class="programlisting1"><code class="hljs-con">'Matteo Zurloni is one of the Italian male climbers who has secured a spot at the Paris 2024 Olympics, but a comprehensive list of Italian male climbers for the Olympics is not readily available.'
</code></pre>
<p class="normal1">At the time of this question (7th of October 2023), the answer is definitely correct. Note how the model went through several iterations of <code class="inlinecode">Observation</code>/<code class="inlinecode">Thought</code>/<code class="inlinecode">Action</code> until it reached the conclusion. This is a great example of how prompting a model to think step by step and explicitly define each step of the reasoning makes it “wiser” and more cautious before answering. It is also a great technique to prevent hallucination.</p>
<p class="normal1">Overall, prompt engineering<a id="_idIndexMarker301" class="calibre3"/> is a powerful discipline, still in its emerging phase yet already widely adopted within LLM-powered applications. In the following chapters, we are going to see concrete applications of this technique.</p>
<h1 class="heading" id="_idParaDest-69">Summary</h1>
<p class="normal">In this chapter, we covered many aspects of the activity of prompt engineering, a core step in the context of improving the performance of LLMs within your application, as well as customizing it depending on the scenario. Prompt engineering is an emerging discipline that is paving the way for a new category of applications, infused with LLMs.</p>
<p class="normal1">We started with an introduction to the concept of prompt engineering and why it is important, and then moved toward the basic principles – including clear instructions, asking for justification, etc. Then, we moved on to more advanced techniques that are meant to shape the reasoning approach of our LLM: few-shot learning, CoT, and ReAct.</p>
<p class="normal1">In the next chapters, we will see those techniques in action by building real-world applications using LLMs.</p>
<h1 class="heading" id="_idParaDest-70">References</h1>
<ul class="calibre16">
<li class="bulletlist">ReAct approach: <a href="https://arxiv.org/abs/2210.03629" class="calibre3">https://arxiv.org/abs/2210.03629</a></li>
<li class="bulletlist1">What is prompt engineering?: <a href="https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering" class="calibre3">https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-prompt-engineering</a></li>
<li class="bulletlist1">Prompt engineering techniques: <a href="https://blog.mrsharm.com/prompt-engineering-guide/" class="calibre3">https://blog.mrsharm.com/prompt-engineering-guide/</a></li>
<li class="bulletlist1">Prompt engineering principles: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions" class="calibre3">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions</a></li>
<li class="bulletlist1">Recency bias: <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#repeat-instructions-at-the-end" class="calibre3">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions#repeat-instructions-at-the-end</a></li>
<li class="bulletlist1">Large Language Model Prompt Engineering for Complex Summarization: <a href="https://devblogs.microsoft.com/ise/2023/06/27/gpt-summary-prompt-engineering/" class="calibre3">https://devblogs.microsoft.com/ise/2023/06/27/gpt-summary-prompt-engineering/</a></li>
<li class="bulletlist1">Language Models are Few-Shot Learners: <a href="https://arxiv.org/pdf/2005.14165.pdf" class="calibre3">https://arxiv.org/pdf/2005.14165.pdf</a></li>
<li class="bulletlist1">IMDb dataset: <a href="https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/code" class="calibre3">https://www.kaggle.com/datasets/yasserh/imdb-movie-ratings-sentiment-analysis/code</a></li>
<li class="bulletlist1">ReAct: <a href="https://arxiv.org/abs/2210.03629" class="calibre3">https://arxiv.org/abs/2210.03629</a></li>
<li class="bulletlist1">Chain of Thought Prompting Elicits Reasoning in Large Language Models: <a href="https://arxiv.org/abs/2201.11903" class="calibre3">https://arxiv.org/abs/2201.11903</a></li>
</ul>
<h1 class="heading">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3">https://packt.link/llm</a></p>
<p class="normal1"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png" class="calibre4"/></p>
</div>
</body></html>