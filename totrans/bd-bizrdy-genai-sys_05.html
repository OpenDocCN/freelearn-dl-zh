<html><head></head><body>
  <div><h1 class="chapterNumber"><a id="_idTextAnchor140"/>5</h1>
    <h1 class="chapterTitle" id="_idParaDest-135"><a id="_idTextAnchor141"/>Adding Multimodal, Multifunctional Reasoning with Chain of Thought</h1>
    <p class="normal">At this point in our journey, we’ve built the core framework of our GenAISys. We have a responsive, small-scale, ChatGPT-like interactive interface. We expanded beyond typical one-to-one copilot interactions, creating a collaborative multi-user environment where an AI agent actively participates in discussions. We further extended this human-centric design by integrating RAG, giving our AI agent access to a Pinecone index capable of managing both instruction scenarios and data. Finally, we built a flexible GenAISys that allows users to activate or deactivate the AI agent during collaborative meetings. In short, we have created a human-centric AI system that augments human teams rather than attempting to replace people with machine intelligence.</p>
    <p class="normal">However, despite its human-centric nature, the exponential growth of global transcontinental supply chains and the vast daily flow of goods, services, and digital content require significant levels of automation. For example, we cannot realistically expect social media platforms such as Meta, X, or LinkedIn to employ millions of people to moderate billions of messages—including images, audio, and video files—every day. Similarly, companies such as Amazon cannot manage millions of online transactions and physical deliveries exclusively through human efforts. Automation is essential to augment human decision-making and reasoning, particularly for critical tasks at scale. Therefore, in this chapter, we will extend the GenAISys framework by adding multimodal capabilities and reasoning functionalities. To address the challenges of cross-domain automation, we will implement image generation and analysis and begin integrating machine learning. Our objective is to build a new agentic AI layer into our GenAISys.</p>
    <p class="normal">We will begin by outlining features that we are integrating into our existing GenAISys framework. Given the broadening scope of our GenAISys, we will introduce <strong class="keyWord">chain-of-thought</strong> (<strong class="keyWord">CoT</strong>) reasoning processes<a id="_idIndexMarker332"/> to orchestrate and manage complex tasks effectively. We will then incorporate computer vision capabilities. This includes building an image generation function with DALL-E and an image analysis function using GPT-4o. Next, we will add audio functionality for those who prefer voice interactions—using <strong class="keyWord">speech to text</strong> (<strong class="keyWord">STT</strong>) for input prompts and <strong class="keyWord">text to speech</strong> (<strong class="keyWord">TTS</strong>) for responses. Lastly, we’ll introduce a decision tree classifier as a machine learning endpoint within the GenAISys, capable of predicting activities. By the end of this chapter, we will have successfully extended the GenAISys into a fully interactive, multimodal reasoning platform ready to tackle complex cross-domain use cases.</p>
    <p class="normal">In all, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">The architecture of the additional functions for our GenAISys</li>
      <li class="bulletList">Implementing a widget image file processing</li>
      <li class="bulletList">Implementing a widget to enable voice dialogues</li>
      <li class="bulletList">Image generation with DALL-E</li>
      <li class="bulletList">Image analysis with GPT-4o</li>
      <li class="bulletList">Building an endpoint for machine learning with a decision tree classifier</li>
      <li class="bulletList">Implementing CoT reasoning</li>
    </ul>
    <p class="normal">Let’s begin by designing an enhanced interface for our GenAISys with additional AI capabilities.</p>
    <h1 class="heading-1" id="_idParaDest-136"><a id="_idTextAnchor142"/>Enhancing the event-driven GenAISys interface</h1>
    <p class="normal">So far, the <a id="_idIndexMarker333"/>GenAISys framework we’ve developed is event-driven, activated by user inputs (human- or system-generated) that trigger specific AI agent functions. In this chapter, we’ll expand the GenAISys by adding several new capabilities:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Voice interaction</strong>, allowing users to manage the GenAISys through speech</li>
      <li class="bulletList">A new <strong class="keyWord">machine learning endpoint</strong> using a decision tree classifier for predictive tasks</li>
      <li class="bulletList"><strong class="keyWord">Multimodal functionality</strong>, including image generation with DALL-E and image analysis using GPT-4o</li>
      <li class="bulletList">A <strong class="keyWord">CoT</strong> reasoning orchestrator to coordinate sophisticated, self-reflective instruction scenarios</li>
    </ul>
    <p class="normal">Let’s start by examining the expanded GenAISys architecture shown in <em class="italic">Figure 5.1</em>:</p>
    <figure class="mediaobject"><img alt="Figure 5.1: Architecture of the enhanced GenAISys interface" src="img/B32304_05_1.png"/></figure>
    <p class="packt_figref">Figure 5.1: Architecture of the enhanced GenAISys interface</p>
    <p class="normal">This <a id="_idIndexMarker334"/>figure (which is an extended version of <em class="italic">Figure 4.1</em> from the previous chapter) highlights the new capabilities we’ll integrate into our GenAISys:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">I1</strong> – <strong class="keyWord">AI controller</strong>: Enhanced with CoT reasoning, enabling automated sequences of tasks as needed and incorporating a widget to manage voice-based user interactions</li>
      <li class="bulletList"><strong class="keyWord">I2</strong> – <strong class="keyWord">Multi-user chatbot</strong>: Maintained exactly as designed in previous chapters</li>
      <li class="bulletList"><strong class="keyWord">F1</strong> – <strong class="keyWord">Generative AI model</strong>: Extended to handle multimodal tasks</li>
      <li class="bulletList"><strong class="keyWord">F2</strong> – <strong class="keyWord">Memory retention</strong>: Continues unchanged from earlier chapters</li>
      <li class="bulletList"><strong class="keyWord">F3</strong> – <strong class="keyWord">Modular RAG</strong>: Continues unchanged from earlier chapters</li>
      <li class="bulletList"><strong class="keyWord">F4</strong> – <strong class="keyWord">Multifunctional capabilities</strong>: New additions covering audio and image processing, including a decision tree classifier for making predictions<div><p class="normal"> <strong class="keyWord">Reminder</strong></p>
          <p class="normal">The decision to present the main components of the GenAISys architecture without arrows is a deliberate choice designed to convey a core concept: modularity and architectural flexibility. The figure is not a rigid blueprint but rather a conceptual toolkit. It shows you the powerful components at your disposal—<strong class="keyWord">I1. AI controller</strong>, <strong class="keyWord">I2. Multi-user chatbot</strong>, <strong class="keyWord">F1. Generative AI model</strong>, <strong class="keyWord">F2. Memory retention</strong>, <strong class="keyWord">F3. Modular RAG</strong>, and <strong class="keyWord">F4. Multifunctional capabilities</strong>—as independent, interoperable blocks.</p>
        </div>
      </li>
    </ul>
    <p class="normal">We are expanding the functionality of GenAISys as built in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a> by adding new layers rather than replacing existing components. Our emphasis here is on enhancement and seamless integration. The following figure provides a high-level flowchart demonstrating how the additional capabilities<a id="_idIndexMarker335"/> will integrate into our existing GenAISys architecture:</p>
    <figure class="mediaobject"><img alt="Figure 5.2: Flowchart of additional functions to the GenAISys" src="img/B32304_05_2.png"/></figure>
    <p class="packt_figref">Figure 5.2: Flowchart of additional functions to the GenAISys</p>
    <p class="normal">The following additional functions will be integrated into our existing GenAISys interface:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Start</strong>: Initializes two new widgets—one for TTS functionality and another to handle image files</li>
      <li class="bulletList"><strong class="keyWord">User Input</strong>: Now includes optional voice input, enabled if the user chooses</li>
      <li class="bulletList"><strong class="keyWord">Generate Bot </strong>and <strong class="keyWord">Generate Bot Response</strong>: These processes connect directly to the existing <code class="inlineCode">VBox</code> interface, displaying reasoning steps clearly whenever the AI agent utilizes CoT logic</li>
    </ul>
    <p class="normal">To achieve this<a id="_idIndexMarker336"/> expanded functionality, we will develop the following key features:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">STT and TTS</strong>: Integrated using <strong class="keyWord">Google Text-to-Speech</strong> (<strong class="keyWord">gTTS</strong>)</li>
      <li class="bulletList"><strong class="keyWord">Machine learning endpoint</strong>: Implementing a decision tree classifier for predictive capabilities</li>
      <li class="bulletList"><strong class="keyWord">Image generation and analysis</strong>: Powered by OpenAI’s DALL-E and GPT-4o models</li>
      <li class="bulletList"><strong class="keyWord">CoT reasoning</strong>: Orchestrating tasks, functions, and extensions, thus providing GenAISys with explicit machine (not human) reasoning abilities</li>
    </ul>
    <p class="normal">Although we are adding several new functions, including reasoning functionality (CoT), we will introduce only a single new package installation, gTTS, to minimize complexity in this chapter. Our primary focus remains on building a reliable architecture with optimal dependency management. To begin, let’s explore the updated elements of the IPython interface and the enhancements to the AI agent.</p>
    <h2 class="heading-2" id="_idParaDest-137"><a id="_idTextAnchor143"/>IPython interface and AI agent enhancements</h2>
    <p class="normal">The GenAISys architecture we’ve developed can now be viewed as comprising three interconnected layers, as shown in <em class="italic">Figure 5.3</em>. These enhancements blur the lines between orchestration, control, and agent functionality, as these roles are now distributed across multiple layers:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Layer 1 (IPython interface)</strong> manages user<a id="_idIndexMarker337"/> and system inputs through event-driven widgets, orchestrating tasks based on user interactions (inputs and checkboxes).</li>
      <li class="bulletList"><strong class="keyWord">Layer 2 (AI agent) </strong>controls the generative AI models (in our case, OpenAI models) and can trigger a CoT reasoning sequence.</li>
      <li class="bulletList"><strong class="keyWord">Layer 3 (functions and agents)</strong> contains functions triggered by the AI agent. Notably, the CoT function itself acts as an agent, capable of orchestrating generative AI tasks, machine learning, and additional functions as needed.</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 5.3: The three layers of the event-driven GenAISys" src="img/B32304_05_3.png"/></figure>
    <p class="packt_figref">Figure 5.3: The three layers of the event-driven GenAISys</p>
    <p class="normal">This high-level architecture integrates orchestrators, controllers, and agents, each broken down into specific Python functionalities. Let’s start by exploring <strong class="keyWord">Layer 1</strong>, the IPython interface, from a functional standpoint.</p>
    <h3 class="heading-3" id="_idParaDest-138"><a id="_idTextAnchor144"/>Layer 1: IPython interface</h3>
    <p class="normal">The IPython interface<a id="_idIndexMarker338"/> now incorporates three new features (highlighted in yellow in <em class="italic">Figure 5.4</em>): a voice widget, a file-handling widget, and a dedicated reasoning interface triggered by user inputs and AI agent activities. These enhancements bring the interface total to six interactive widgets and functions.</p>
    <figure class="mediaobject"><img alt="Figure 5.4: Voice, file, and reasoning features are added to the IPython interface" src="img/B32304_05_4.png"/></figure>
    <p class="packt_figref">Figure 5.4: Voice, file, and reasoning features are added to the IPython interface</p>
    <p class="normal">Let’s go through each widget and function:</p>
    <ol>
      <li class="numberedList" value="1"><strong class="keyWord">User selection</strong> remains as designed in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a>. It is central to the collaborative design of the GenAISys and remains unchanged.</li>
      <li class="numberedList"><strong class="keyWord">User input</strong> is also retained from <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a> without modification; this widget remains central for capturing user prompts.</li>
      <li class="numberedList">The<strong class="keyWord"> AI agent</strong>, as<a id="_idIndexMarker339"/> described in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a>, activates or deactivates the generative AI agent (<code class="inlineCode">chat_with_gpt</code>).</li>
      <li class="numberedList">The<strong class="keyWord"> voice widget</strong> enables voice-based interactions through STT and TTS. We’re using cost-free, built-in functionality for STT:<ul>
          <li class="bulletList"><strong class="keyWord">Windows</strong>: Press the Windows key + <em class="italic">H</em></li>
          <li class="bulletList"><strong class="keyWord">macOS</strong>: Enable <strong class="screenText">Dictation</strong> under <strong class="screenText">Keyboard settings</strong> and choose a custom shortcut</li>
        </ul>
      </li>
    </ol>
    <p class="normal">For TTS, the gTTS service is utilized and controlled via a checkbox set to <code class="inlineCode">False</code> by default:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create a checkbox to toggle text-to-speech
tts_checkbox = Checkbox(
    value=False,
    description='Voice Output',
    layout=Layout(width='20%')
)
</code></pre>
    <p class="normal">If the AI agent’s checkbox is checked, then the TTS function is called:</p>
    <pre class="programlisting code"><code class="hljs-code">if agent_checkbox.value:
…
if tts_checkbox.value:
            text_to_speech(response)
</code></pre>
    <p class="normal">The resulting MP3 file (<code class="inlineCode">response.mp3</code>) is automatically played in the <code class="inlineCode">update_display()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">def update_display():
…
#Audio display
    if os.path.exists("/content/response.mp3"):
      display(Audio("/content/response.mp3", autoplay=True))
      !rm /content/response.mp3
</code></pre>
    <ol>
      <li class="numberedList" value="5"><strong class="keyWord">The files widget </strong>is a new widget that activates file management. It will display images generated and saved by the generative AI model (DALL-E) triggered in the AI agent function, <code class="inlineCode">chat_with_gpt</code>. It is controlled via another checkbox, initially set to <code class="inlineCode">False</code>:
        <pre class="programlisting code"><code class="hljs-code"># Create a checkbox to toggle agent response
files_checkbox = Checkbox(
    value=False,
    description='Files',
    layout=Layout(width='20%')
)
</code></pre>
      </li>
    </ol>
    <p class="normal">If an image<a id="_idIndexMarker340"/> exists, it is displayed with the <strong class="keyWord">Python Image Library</strong> (<strong class="keyWord">PIL</strong>) in the <code class="inlineCode">update_display()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">    if os.path.exists("/content/c_image.png") and files_checkbox.value==True:
    # Open the image using PIL
    original_image = PILImage.open("/content/c_image.png")
    # Resize the image to 50% of its original size
    new_size = (original_image.width //2, original_image.height//2)
    resized_image = original_image.resize(new_size)
    # Display the resized image
    display(resized_image)
</code></pre>
    <ol>
      <li class="numberedList" value="6"><strong class="keyWord">Reasoning activated </strong>is another new widget of the GenAISys. The user input will trigger an event in the AI agent, and that, in turn, will trigger a CoT reasoning process. The reasoning interface will display the thought process of the CoT in real time. The reasoning output widget is created at the start of a session:
        <pre class="programlisting code"><code class="hljs-code"># Create an output widget for reasoning steps
reasoning_output = Output(
    layout=Layout(border="1px solid black", padding="10px",
        margin="10px", width="100%")
)
</code></pre>
      </li>
    </ol>
    <p class="normal">The widget will receive outputs from the CoT process and display them independently from <code class="inlineCode">VBox</code> and persistently in the <code class="inlineCode">update_display()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">def update_display():
…
# Display reasoning_output persistently
    display(reasoning_output)…
</code></pre>
    <p class="normal">The <code class="inlineCode">VBox</code> interface now contains all interactive widgets, including the newly added TTS and files widgets:</p>
    <pre class="programlisting code"><code class="hljs-code">if conversation_active:
        display(
            VBox(
                [user_selector, input_box, agent_checkbox,
                tts_checkbox, files_checkbox],
                layout=Layout(display='flex', flex_flow='column',
                    align_items='flex-start', width='100%')
            )
        )
</code></pre>
    <p class="normal">Given the<a id="_idIndexMarker341"/> length and complexity of responses from the AI agent (especially during CoT processes), we introduced an enhanced formatting feature using Markdown. The <code class="inlineCode">update_display()</code> function now formats entries clearly, calling a dedicated formatting function:</p>
    <pre class="programlisting code"><code class="hljs-code">def update_display():
    clear_output(wait=True)
    for entry in user_histories[active_user]:
        formatted_entry = format_entry(entry)
        display(Markdown(formatted_entry))
</code></pre>
    <p class="normal">The <code class="inlineCode">format_entry(entry)</code> function formats the user’s (blue) and assistant’s (green) responses, ensuring readability:</p>
    <pre class="programlisting code"><code class="hljs-code">def format_entry(entry):
    """Format the content of an entry for Markdown display."""
    if entry['role'] == 'user':
        formatted_content = format_json_as_markdown(entry['content'])
            if isinstance(entry['content'], (dict, list))
            else entry['content']
        formatted_content = formatted_content.replace("\n", "&lt;br&gt;")  # Process newlines outside the f-string
        return f"**&lt;span style='color: blue;'&gt;{active_user}:&lt;/span&gt;** {formatted_content}"
…
    elif entry['role'] == 'assistant':
        formatted_content = format_json_as_markdown(entry['content'])
        …
        return f"**&lt;span style='color: green;'&gt;Agent:&lt;/span&gt;** {formatted_content}"
  
</code></pre>
    <p class="normal">This design emphasizes that the IPython interface (<strong class="keyWord">Layer 1</strong>) is purely to orchestrate user interactions and trigger underlying layers of functions and agents. This architecture ensures that you have <a id="_idIndexMarker342"/>the flexibility you need if you want to call the functions and agents directly without a user interface.</p>
    <p class="normal">With the IPython interface described, let’s explore the enhanced capabilities in <strong class="keyWord">Layer 2</strong>, the AI agent.</p>
    <h3 class="heading-3" id="_idParaDest-139"><a id="_idTextAnchor145"/>Layer 2: AI agent</h3>
    <p class="normal">The<a id="_idIndexMarker343"/> AI agent invoked by the IPython interface in <strong class="keyWord">Layer 1</strong> remains the <code class="inlineCode">chat_with_gpt</code> function, reinforcing the conversational nature of GenAISys. With the introduction of reasoning capabilities, the conversation can now occur directly between AI agents as well.</p>
    <p class="normal">The <code class="inlineCode">chat_with_gpt</code> function has been expanded with several new features. If necessary, review the core functionalities described in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a>.</p>
    <p class="normal">Let’s explore the new enhancements added to the AI agent:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">continue_functions=True</code> has been introduced at the beginning of the function to ensure that only one requested task is executed at a time.</li>
      <li class="bulletList"><code class="inlineCode">continue_functions</code> is set to <code class="inlineCode">False</code> at the end of the Pinecone query process, triggered by the presence of the <code class="inlineCode">Pinecone</code> keyword in the user message. This stops any additional unintended task executions.</li>
      <li class="bulletList">The new function, <code class="inlineCode">reason.chain_of_thought_reasoning</code>, described later, in the <em class="italic">Reasoning with CoT</em> section, is called under specific conditions:
        <pre class="programlisting code"><code class="hljs-code">if "Use reasoning" in user_message and "customer" in user_message and "activities" in user_message and continue_functions==True:
</code></pre>
      </li>
    </ul>
    <p class="normal">The <code class="inlineCode">continue_functions==True</code> condition ensures the reasoning function is called with the initial user query. A sample customer activities file is also downloaded as part of this process:</p>
    <pre class="programlisting code"><code class="hljs-code">initial_query = user_message
download("Chapter05","customer_activities.csv")
reasoning_steps = reason.chain_of_thought_reasoning(initial_query)
</code></pre>
    <p class="normal">In the example use case for this chapter, a team can automatically access and query a regularly updated customer activity data source. The sample file provided contains 10,000 records of historical customer activities, including customer IDs, locations, activity types, and activity ratings:</p>
    <figure class="mediaobject"><img alt="Figure 5.5: The customer ratings of historical sites" src="img/B32304_05_5.png"/></figure>
    <p class="packt_figref">Figure 5.5: The customer ratings of historical sites</p>
    <p class="normal">A <a id="_idIndexMarker344"/>decision tree classifier later utilizes this dataset within the CoT reasoning function to predict the most popular customer activity. Once the response is generated, it is added to the output, and <code class="inlineCode">continue</code> is set to <code class="inlineCode">False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">aug_output=reasoning_steps
continue_functions=False
</code></pre>
    <ul>
      <li class="bulletList">The new function, <code class="inlineCode">reason.generate_image</code>, that we will implement in the <em class="italic">Image generation and analysis</em> section has also been integrated. It is called as follows:
        <pre class="programlisting code"><code class="hljs-code">prompt = user_message
image_url = reason.generate_image(prompt, model="dall-e-3", 
    size="1024x1024", quality="standard", n=1)
</code></pre>
      </li>
    </ul>
    <p class="normal">The generated image URL is returned, and the image itself is downloaded and saved locally for display or further processing:</p>
    <pre class="programlisting code"><code class="hljs-code"># Save the image locally
save_path = "c_image.png"
image_data = requests.get(image_url).content
with open(save_path, "wb") as file:
    file.write(image_data)
</code></pre>
    <p class="normal">A corresponding message is added to the output, and the <code class="inlineCode">continue</code> flag is set to <code class="inlineCode">False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">aug_output="Image created"
continue_functions=False
</code></pre>
    <ul>
      <li class="bulletList">The function previously known as <code class="inlineCode">openai_api.make_openai_api_call</code> is now renamed <code class="inlineCode">reason.make_openai_api_call</code>. It maintains the same functionality as in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a> but is now part of the GenAISys reasoning library. The memory management <code class="inlineCode">if user_memory…else</code> condition, which takes the complete user history or just the present user message into account, has been updated with explicit conditions that check both the state of <code class="inlineCode">user_memory</code> and the <code class="inlineCode">continue_functions</code> flag:
        <pre class="programlisting code"><code class="hljs-code">if user_memory==False and continue_functions==True:    
…
if user_memory==True and continue_functions==True: …
</code></pre>
      </li>
    </ul>
    <p class="normal">The AI agent thus <a id="_idIndexMarker345"/>acts as an intermediate orchestrator, calling and managing the execution of lower-layer functions rather than executing them directly. The Pinecone interface remains the top layer that invokes the AI agent, which in turn interacts with the specific functions within <strong class="keyWord">Layer 3</strong>.</p>
    <h3 class="heading-3" id="_idParaDest-140"><a id="_idTextAnchor146"/>Layer 3: Functions</h3>
    <p class="normal">In this layer, our<a id="_idIndexMarker346"/> focus is on the new functionalities introduced to enable advanced reasoning through the CoT cognitive agent. Pinecone indexing and standard OpenAI calls remain as implemented in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a>. The primary additions in this chapter are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Image generation and analysis</strong> using DALL-E and GPT-4o, respectively</li>
      <li class="bulletList"><strong class="keyWord">CoT reasoning</strong>, which introduces a cognitive agent capable of orchestrating tasks</li>
      <li class="bulletList"><strong class="keyWord">Voice interaction capabilities</strong> enabled through gTTS</li>
      <li class="bulletList"><strong class="keyWord">A machine learning endpoint</strong> leveraging a decision tree classifier</li>
    </ul>
    <p class="normal">We will explore these functionalities in the upcoming sections of this chapter, as follows:</p>
    <ul>
      <li class="bulletList">The environment setup and initialization for gTTS and machine learning are detailed in the <em class="italic">Setting up the environment</em> section</li>
      <li class="bulletList">Image functionalities are covered in the <em class="italic">Image generation and analysis</em> section</li>
      <li class="bulletList">The reasoning orchestration is built in the <em class="italic">Reasoning with CoT</em> section</li>
    </ul>
    <p class="normal">By the end of this chapter, our enhanced three-layer GenAISys will have new, robust capabilities designed to expand even further in subsequent chapters. Let’s now dive deeper into these enhancements, beginning with the environment setup.</p>
    <h1 class="heading-1" id="_idParaDest-141"><a id="_idTextAnchor147"/>Setting up the environment</h1>
    <p class="normal">In this section, we<a id="_idIndexMarker347"/> will enhance, expand, and rearrange the environment previously built to finalize the GenAISys framework. These changes are essential for the upcoming use cases in subsequent chapters. Open the <code class="inlineCode">Multimodal_reasoning_with_Chain_of_Thought.ipynb</code> notebook within the Chapter05 directory on GitHub (<a href="https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main">https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main</a>).</p>
    <p class="normal">Regarding <a id="_idIndexMarker348"/>package installations, the <em class="italic">Setting up the environment</em> section in the notebook remains largely unchanged from the previous chapter (<code class="inlineCode">Event-driven_GenAISys_framework.ipynb</code>), with just one new addition: <em class="italic">Google Text-to-Speech (gTTS)</em>.</p>
    <p class="normal">However, several significant updates have been made to support the CoT generative AI reasoning features. Let’s examine each of these updates, starting with the <em class="italic">OpenAI</em> section.</p>
    <h2 class="heading-2" id="_idParaDest-142"><a id="_idTextAnchor148"/>OpenAI</h2>
    <p class="normal">The first<a id="_idIndexMarker349"/> two files we download remain the same as in previous chapters. The third and fourth files, however, are new and have been added to support advanced functionality:</p>
    <pre class="programlisting code"><code class="hljs-code">from grequests import download
download("commons","requirements01.py")
download("commons","openai_setup.py")
<strong class="hljs-slc">download(</strong><strong class="hljs-string-slc">"commons"</strong><strong class="hljs-slc">,</strong><strong class="hljs-string-slc">"reason.py"</strong><strong class="hljs-slc">)</strong>
<strong class="hljs-slc">download(</strong><strong class="hljs-string-slc">"commons","machine_learning.py")</strong>
</code></pre>
    <p class="normal"><code class="inlineCode">reason.py</code> now contains the generative AI library with the functions built in the previous chapters and the ones we are adding in this chapter. These functions in the generative AI library and their status are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">make_openai_api_call(input, mrole,mcontent,user_role)</code> is a general-purpose OpenAI API call described in the <em class="italic">Setting up the environment</em> section of <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>. It is now imported as follows:
        <pre class="programlisting code"><code class="hljs-code">from reason import make_openai_api_call
</code></pre>
      </li>
      <li class="bulletList"><code class="inlineCode">image_analysis</code> is the image analysis function that can describe an image or use the image as a starting point to generate content such as a story. This function is described in the <em class="italic">Image generation and analysis</em> section of this chapter.</li>
      <li class="bulletList"><code class="inlineCode">generate_image</code> is a new function that generates images with DALL-E, detailed in the <em class="italic">Image generation and analysis</em> section of this chapter.</li>
      <li class="bulletList"><code class="inlineCode">chain_of_thought_reasoning</code> is a new CoT logic function of the GenAISys we are building. We will implement it in the <em class="italic">Reasoning with CoT </em>section of this chapter. It can call functions from other libraries, such as <code class="inlineCode">machine_learning</code>.</li>
    </ul>
    <p class="normal"><code class="inlineCode">machine_learning.py</code> will now contain a decision tree classifier in a function named <code class="inlineCode">ml_agent</code>. The function takes two arguments:</p>
    <pre class="programlisting code"><code class="hljs-code">ml_agent(ml_agent(feature1_value, feature2_column)
</code></pre>
    <p class="normal">In our example use case, <code class="inlineCode">feature1_value</code> will represent a customer location, and <code class="inlineCode">feature2_column</code> will represent customer activities. The <code class="inlineCode">ml_agent</code> classifier will predict the most popular customer activity for a specific location based on historical data.</p>
    <p class="normal">We import <code class="inlineCode">ml_agent</code> from <code class="inlineCode">machine_learning.py</code> as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"># Import the function from a custom machine learning file
import os
import machine_learning
from machine_learning import ml_agent
</code></pre>
    <p class="normal">The remaining<a id="_idIndexMarker350"/> OpenAI setup subsections, including package installation and API key initialization, remain identical to previous chapters. Let’s now initialize our new functionalities.</p>
    <h2 class="heading-2" id="_idParaDest-143"><a id="_idTextAnchor149"/>Initializing gTTS, machine learning, and CoT</h2>
    <p class="normal">We will initialize<a id="_idIndexMarker351"/> the following new functions:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">gTTS</strong> is installed with <code class="inlineCode">!pip install gTTS==2.5.4</code>, which is an open source, free TTS library that fits prototyping purposes: <a href="https://pypi.org/project/gTTS/">https://pypi.org/project/gTTS/</a>. <code class="inlineCode">`click`</code>, a command-line library, is required for gTTS. The first cell checks if we wish to use gTTS by setting <code class="inlineCode">use_gtts</code> to <code class="inlineCode">True</code>:
        <pre class="programlisting code"><code class="hljs-code">use_gtts = True #activates Google TTS in Google Colab if True and deactivates if False
</code></pre>
      </li>
    </ul>
    <p class="normal">The second cell of the notebook will check for and set up the correct <code class="inlineCode">`click`</code> version if <code class="inlineCode">use_gtts</code> is set to <code class="inlineCode">True</code>. If an update is needed, it will then display a clear message in the notebook output prompting you to manually restart the runtime. After restarting, simply click <code class="inlineCode">`Run All`</code> to continue. The code will display an HTML message to restart if the version is updated:</p>
    <pre class="programlisting code"><code class="hljs-code">import importlib.metadata
from IPython.display import display, HTML # Required for the message
# ... (define required_click_version, current_click_version, and html_message as in your code) ...
if current_click_version != required_click_version: 
    # --- Commands to uninstall and install ‘click’ would go here --- 
    # Example: !pip uninstall -y click 
    # Example: !pip install click==8.1.8
    # Display the styled message prompting for manual restart 
    display(HTML(html_message)) 
    # Stop the Python cell execution gracefully, prompting restart 
    raise SystemExit(“Please restart the Colab runtime to apply changes.”)
else: 
    print(f”--- ‘click’ is already at the correct version ({required_click_version}). No action needed. ---”)
</code></pre>
    <p class="normal">If <code class="inlineCode">use_gtts</code> is set to <code class="inlineCode">True</code>, we install gTTS and define a TTS conversion function:</p>
    <pre class="programlisting code"><code class="hljs-code"># use_gtts activates Google TTS in Google Colab if True and deactivates if False
if use_gtts: 
  !pip install gTTS==2.5.4 
  from gtts import gTTS 
  from IPython.display import Audio
def text_to_speech(text): 
    # Convert text to speech and save as an MP3 file 
    if use_gtts: 
      if not isinstance(text, str): 
          text = str(text) # Making sure the text is a string not a list 
      tts = gTTS(text) 
      tts.save(“response.mp3”)
</code></pre>
    <p class="normal">This function will be activated in the IPython interface when the AI agent returns a response, as explained earlier in the <em class="italic">Layer 1: IPython interface</em> section.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The ml_agent algorithm endpoint</strong> is<a id="_idIndexMarker352"/> imported from <code class="inlineCode">machine_learning.py</code>:
        <pre class="programlisting code"><code class="hljs-code"># Import the function from the custom OpenAI API file
import os
import machine_learning
from machine_learning import ml_agent
</code></pre>
      </li>
    </ul>
    <p class="normal">This decision tree classifier function will predict popular customer activities based on historical data, enhancing<a id="_idIndexMarker353"/> our GenAISys’s predictive capabilities.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">The CoT reasoning</strong> framework is <a id="_idIndexMarker354"/>imported from <code class="inlineCode">reason.py</code>:
        <pre class="programlisting code"><code class="hljs-code"># Import the function from the custom OpenAI API file
import os
import reason
from reason import chain_of_thought_reasoning
</code></pre>
      </li>
    </ul>
    <p class="normal">The Pinecone installation, initialization, and queries are then defined as explained in <em class="italic">Chapters 3</em> and <em class="italic">4</em>. Take some time to revisit those chapters if needed, as we will reuse the functions previously developed. We’re now prepared to build the image generation and analysis functions.</p>
    <h1 class="heading-1" id="_idParaDest-144"><a id="_idTextAnchor150"/>Image generation and analysis</h1>
    <p class="normal">In this section, we will <a id="_idIndexMarker355"/>begin by creating a flexible image generation function using OpenAI’s DALL-E model. Following that, we’ll build a function for image analysis. The objective is to <a id="_idIndexMarker356"/>enhance GenAISys with computer vision capabilities while preserving its responsive, event-driven functionality, as illustrated in <em class="italic">Figure 5.6</em>:</p>
    <figure class="mediaobject"><img alt="Figure 5.6: Generating images with flexible event-driven triggers" src="img/B32304_05_6.png"/></figure>
    <p class="packt_figref">Figure 5.6: Generating images with flexible event-driven triggers</p>
    <p class="normal">The preceding<a id="_idIndexMarker357"/> figure is an evolution of the architecture we first developed in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a>. It has been augmented to include new capabilities: activation of speech (voice) features, management of image files, enhanced display functionality, and reasoning through CoT. In this section, our focus will specifically be on integrating and demonstrating computer vision capabilities alongside the enhanced display functionality.</p>
    <p class="normal">The image generation <a id="_idIndexMarker358"/>and analysis processes are designed to be flexible:</p>
    <ul>
      <li class="bulletList">No mandatory selection or explicit widget activation is required for image generation or analysis. We could easily add explicit widgets labeled <strong class="screenText">Image Generation</strong> or <strong class="screenText">Image Analysis</strong> if a use case demands it. However, the approach we’re adopting here is intentionally flexible, paving the way for integration within more complex, automated reasoning workflows such as CoT.</li>
      <li class="bulletList">The <strong class="screenText">Files</strong> checkbox widget serves two distinct purposes:<ul>
          <li class="bulletList">If <em class="italic">unchecked</em>, an image is generated by DALL-E, saved to a file, but not displayed. This allows images to be generated quietly in the background for later use or storage.</li>
          <li class="bulletList">If <em class="italic">checked</em>, the generated or analyzed image will be displayed in the user interface, as illustrated in <em class="italic">Figure 5.7</em>.</li>
        </ul>
      </li>
      <li class="bulletList">The AI conversational agent automatically activates image generation or analysis based on user prompts. These vision capabilities can also trigger automated reasoning processes, enabling the system to execute comprehensive CoT tasks seamlessly.</li>
    </ul>
    <p class="normal">Note <a id="_idIndexMarker359"/>that the display<a id="_idIndexMarker360"/> will only display image files if the <strong class="screenText">Files</strong> widget is checked. Let’s now dive deeper into how these vision features are integrated within the GenAISys interface. Specifically, we’ll demonstrate the scenario where the <strong class="screenText">Files</strong> checkbox is activated (checked), as depicted in <em class="italic">Figure 5.7</em>:</p>
    <figure class="mediaobject"><img alt="Figure 5.7: The files checkbox is checked so that the image will be displayed" src="img/B32304_05_7.png"/></figure>
    <p class="packt_figref">Figure 5.7: The files checkbox is checked so that the image will be displayed</p>
    <p class="normal">With the <strong class="screenText">Files</strong> checkbox selected, the image generated by DALL-E in response to the user’s prompt will be immediately displayed, as shown in <em class="italic">Figure 5.8</em>:</p>
    <figure class="mediaobject"><img alt="Figure 5.8: Entering a prompt and displaying the image generated" src="img/B32304_05_8.png"/></figure>
    <p class="packt_figref">Figure 5.8: Entering a prompt and displaying the image generated</p>
    <p class="normal">If the <strong class="screenText">Files</strong> option<a id="_idIndexMarker361"/> is not checked, the image will be generated and saved but not<a id="_idIndexMarker362"/> displayed. Similarly, image display functionality also applies to analyzing images downloaded from external sources. When the <strong class="screenText">Files</strong> checkbox is unchecked, the analysis runs without visually displaying the image. We are now ready to examine the implementation details of the image generation function.</p>
    <h2 class="heading-2" id="_idParaDest-145"><a id="_idTextAnchor151"/>Image generation</h2>
    <p class="normal">The function to generate<a id="_idIndexMarker363"/> an image is located in the custom generative AI library, <code class="inlineCode">reason.py</code>, in the <code class="inlineCode">commons</code> directory. A user prompt or a CoT framework can trigger this function. The name of the function is <code class="inlineCode">generate_image</code>, and it takes five arguments:</p>
    <pre class="programlisting code"><code class="hljs-code">def generate_image(
    prompt, model="dall-e-3", size="1024x1024", quality="standard", n=1
):
</code></pre>
    <p class="normal">The five arguments are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">prompt</code>: The query related to the image that is provided by the user or the system.</li>
      <li class="bulletList"><code class="inlineCode">model</code>: The OpenAI model to use. In this case, the default value is <code class="inlineCode">gpt-4o</code>.</li>
      <li class="bulletList"><code class="inlineCode">size</code>: The size of the image. The default size of the image is <code class="inlineCode">1024x1024</code>.</li>
      <li class="bulletList"><code class="inlineCode">quality</code>: Defines the quality of the image. The default value is <code class="inlineCode">standard</code>, which costs less than the higher-quality <code class="inlineCode">hd</code> option.</li>
      <li class="bulletList"><code class="inlineCode">n</code>: Defines the number of images to generate. The default value is <code class="inlineCode">1</code>.</li>
    </ul>
    <p class="normal">The function returns the URL of the generated image. The code first initializes the OpenAI client:</p>
    <pre class="programlisting code"><code class="hljs-code">def generate_image(
    prompt, model="dall-e-3", size="1024x1024", quality="standard", n=1
):
    # Initialize the OpenAI client
    client = OpenAI()
</code></pre>
    <p class="normal">The DALL-E model<a id="_idIndexMarker364"/> is then called via the OpenAI API with the specified parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Generate the image using the OpenAI API
    response = client.images.generate(
        model=model,
        prompt=prompt,
        size=size,
        quality=quality,
        n=n,
    )
</code></pre>
    <div><p class="normal"> The parameters are described in detail in <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a> in the <em class="italic">Setting up the environment</em> section.</p>
    </div>
    <p class="normal">Once the content, messages, and<a id="_idIndexMarker365"/> parameters are defined, the OpenAI API is called:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Make the API call
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        **params  # Unpack the parameters dictionary
    )
</code></pre>
    <p class="normal">The URL of the image is extracted from <code class="inlineCode">response</code> and returned:</p>
    <pre class="programlisting code"><code class="hljs-code"># Extract and return the image URL from the response
    return response. data[0].url
</code></pre>
    <p class="normal">Once an image has been generated or retrieved, we can choose to display or analyze it, depending on our needs.</p>
    <h2 class="heading-2" id="_idParaDest-146"><a id="_idTextAnchor152"/>Image analysis</h2>
    <p class="normal">The function<a id="_idIndexMarker366"/> to analyze an image is also located <a id="_idIndexMarker367"/>in the custom generative AI library,<code class="inlineCode"> reason.py</code>, in the <code class="inlineCode">commons</code> directory. This function, named <code class="inlineCode">image_analysis</code>, is defined as follows, and takes three arguments:</p>
    <pre class="programlisting code"><code class="hljs-code">def image_analysis(image_path_or_url, query_text, model="gpt-4o"):
</code></pre>
    <p class="normal">The three arguments are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">image_path_or_url (str)</code>: The path to access a local image file or the URL of the image.</li>
      <li class="bulletList"><code class="inlineCode">query_text (str)</code>: The query related to the image that is provided by the user or the system</li>
      <li class="bulletList"><code class="inlineCode">model (str)</code>: The OpenAI model to use. In this case, the default value is <code class="inlineCode">gpt-4o</code>, which possesses vision capabilities(generation and analysis).</li>
    </ul>
    <p class="normal">The function<a id="_idIndexMarker368"/> initializes the content structure for the API call with the provided query text:</p>
    <pre class="programlisting code"><code class="hljs-code"># Initialize the content list with the query text
    content = [{"type": "text", "text": query_text}]
</code></pre>
    <p class="normal">The<a id="_idIndexMarker369"/> function then searches for the image in a URL or a local file:</p>
    <pre class="programlisting code"><code class="hljs-code">    if image_path_or_url.startswith(("http://", "https://")):
        # It's a URL; add it to the content
        content.append({"type": "image_url", 
            "image_url": {"url": image_path_or_url}})
    else:
        # It's a local file; read and encode the image data
        with open(image_path_or_url, "rb") as image_file:
            image_data = base64.b64encode(
                image_file.read()).decode('utf-8')
</code></pre>
    <p class="normal">If the image is in a URL, it is appended to the content. If the image is a local file, it is encoded in Base64 and formatted as a UTF-8 string. This format enables embedding the image data within text-based systems (such as JSON or HTML). A data URL is then created and appended to the content:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create a data URL for the image
    data_url = f"data:image/png;base64,{image_data}"
    content.append({"type": "image_url", "image_url": {"url": data_url}})
</code></pre>
    <p class="normal">The OpenAI message is created with the context that contains the query information and the image:</p>
    <pre class="programlisting code"><code class="hljs-code"># Create the message object
    messages = [{"role": "user", "content": content}]
</code></pre>
    <p class="normal">The API call includes a set of standard parameters, detailed in <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a> (in the <em class="italic">Setting up the environment</em> section):</p>
    <pre class="programlisting code"><code class="hljs-code"># Define the parameters
    params = {
        "max_tokens": 300,
        "temperature": 0,
        "top_p": 1,
        "frequency_penalty": 0,
        "presence_penalty": 0,
</code></pre>
    <p class="normal">Once the content, messages, and<a id="_idIndexMarker370"/> parameters are defined, the OpenAI API is called:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Make the API call
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        **params  # Unpack the parameters dictionary
    )
</code></pre>
    <p class="normal">For further<a id="_idIndexMarker371"/> integration, particularly with RAG using Pinecone in <a href="Chapter_6.xhtml#_idTextAnchor166"><em class="italic">Chapter 6</em></a>, the response is saved as text in a file. This enables subsequent use and retrieval:</p>
    <pre class="programlisting code"><code class="hljs-code"># Save the result to a file
    with open("image_text.txt", "w") as file:
        file.write(response.choices[0].message.content)
return response.choices[0].message.content
</code></pre>
    <p class="normal">This <code class="inlineCode">image_analysis</code> function will also be called by the CoT reasoning process built later in this chapter, where <code class="inlineCode">query_text</code> will be dynamically created and passed into the function:</p>
    <pre class="programlisting code"><code class="hljs-code">    response = image_analysis(image_url, query_text)
</code></pre>
    <p class="normal">We now have fully functional computer vision components integrated into our GenAISys. With these capabilities, we are ready to build the CoT reasoning process.</p>
    <h1 class="heading-1" id="_idParaDest-147"><a id="_idTextAnchor153"/>Reasoning with CoT</h1>
    <p class="normal">The <a id="_idIndexMarker372"/>exponential acceleration of global markets has led to billions of micro-tasks being generated daily across platforms such as social media, e-marketing sites, production lines, and SaaS platforms. Without robust automation, keeping pace with these real-time demands is impossible. Speed and efficiency have become paramount, requiring tasks to be executed in real time or near-real time. Recent advances in AI have significantly helped us adapt to these market paradigms, where we must handle an increasing volume of tasks in increasingly shorter timeframes. However, as we increase the number and scope of AI functions to solve problems, it is becoming confusing for users to run complex scenarios with copilots. It is also quite challenging for a team of developers to create a GenAISys that contains the functions they need and includes a clear and intuitive sequence of operations for problem-solving.</p>
    <p class="normal">In this section, we <a id="_idIndexMarker373"/>address these challenges by implementing CoT reasoning. CoT reasoning breaks complex tasks into smaller, more manageable steps where the output of one step becomes the input for the next. This process mimics (without replacing) human-like reasoning. It reduces cognitive overload for users, allowing them to focus primarily on decision-making. Additionally, CoT reasoning makes the AI agent’s internal thought process transparent, providing real-time explainability of each reasoning step.</p>
    <p class="normal">The goal of this section is to build a CoT reasoning process using Python, leveraging the flexible and interactive GenAISys framework we’ve developed. Specifically, we will apply CoT to simulate customer-preference analysis for an online travel platform, generate creative suggestions for activities, produce images using DALL-E, and create storytelling narratives based on these images with GPT-4o.</p>
    <p class="normal">At first glance, a CoT cognitive agent might seem similar to traditional sequences of functions found in classical software development. Hence, let’s first clarify the important distinctions between them before we dive into the code.</p>
    <h2 class="heading-2" id="_idParaDest-148"><a id="_idTextAnchor154"/>CoT in GenAISys versus traditional software sequences</h2>
    <p class="normal">Seasoned software <a id="_idIndexMarker374"/>developers are used to implementing complex sequences of functions. To bridge the conceptual gap between traditional software sequences and cognitive CoT reasoning (which mimics rather than replaces human cognition), let’s first distinguish their purposes clearly:</p>
    <ul>
      <li class="bulletList">A <strong class="keyWord">traditional sequence</strong> of non-AI or AI functions consists of a series of steps executed independently, following a black-box model in which the output of one function serves as the static input of the next.</li>
      <li class="bulletList">In a <strong class="keyWord">CoT reasoning process</strong>, the steps mimic human-like reasoning. Each step goes beyond a simple function and follows a logical progression. Each new process builds on the output of the previous step, as we will see when we implement CoT. We will observe the GenAISys’s “thinking process” displayed in real time through our interactive interface. The process is transparent and explainable, as it is visualized in real time within the IPython interface. We can see what the system is doing and isolate any function to investigate the process if necessary.</li>
    </ul>
    <p class="normal">Another critical aspect of CoT is its <em class="italic">intermediate reasoning</em>:</p>
    <ul>
      <li class="bulletList">Each step in a CoT process builds on the previous one, but not all steps are static. For instance, when DALL·E generates an image, it creates something entirely new—not retrieved from a database. This relies on a generative AI model, not pre-programmed content.</li>
      <li class="bulletList">The next step in the process isn’t pre-generated, like a fixed list of messages. For example, when DALL-E generates an image, we will ask GPT-4o to perform a storytelling task that it will invent <em class="italic">ex nihilo</em> based on the input it received. Alternatively, we could ask GPT-4o to simply describe the image—without needing to change or fine-tune the model.</li>
    </ul>
    <p class="normal">CoT reasoning <a id="_idIndexMarker375"/>offers <em class="italic">cognitive alignment</em> closer to human thinking patterns. We humans break monolithic problems into smaller parts, process each part, and then assemble the intermediate conclusions to reach a global solution. The human-like framework of the CoT process we are building in this chapter makes the GenAISys more intuitive and creative, mimicking (not replacing) human problem-solving methods. In the following chapters, notably in <a href="Chapter_6.xhtml#_idTextAnchor166"><em class="italic">Chapter 6</em></a>, we’ll further expand and enhance the CoT reasoning capabilities. The takeaway here is that CoT involves sequences of tasks, but in a more flexible and creative way than classical non-AI or AI sequences. Let’s move on and define the cognitive flow of CoT reasoning.</p>
    <h2 class="heading-2" id="_idParaDest-149"><a id="_idTextAnchor155"/>Cognitive flow of CoT reasoning</h2>
    <p class="normal">Instead of the<a id="_idIndexMarker376"/> traditional term flowchart, we’ll use the term <em class="italic">cognitive flow</em> to describe the CoT process we are implementing. This term emphasizes<a id="_idIndexMarker377"/> the human-like reasoning and dynamic problem-solving capabilities of our AI agent, differentiating clearly from classical software flowcharts. A classic flowchart provides a visual representation of a sequence of functions. A reasoning CoT cognitive flow or cognitive workflow maps the logical progression of the AI agent’s thought process from one step to another. The cognitive flow shows how the AI agent mimics human reasoning.</p>
    <p class="normal">Let’s first walk through the cognitive flow we will implement in Python, visualized in <em class="italic">Figure 5.9</em>. The Python functions we’ll use reside in <code class="inlineCode">reason.py</code>, located in the <code class="inlineCode">commons</code> directory, and are described in detail in the <em class="italic">OpenAI</em> subsection of this chapter’s <em class="italic">Setting up the environment </em>section.</p>
    <figure class="mediaobject"><img alt="Figure 5.9: Cognitive flow of the CoT process" src="img/B32304_05_9.png"/></figure>
    <p class="packt_figref">Figure 5.9: Cognitive flow of the CoT process</p>
    <p class="normal">The cognitive <a id="_idIndexMarker378"/>flow for <a id="_idIndexMarker379"/>our CoT reasoning process consists of five main phases, orchestrated by the <code class="inlineCode">chain_of_thought_reasoning()</code> function. This sequence begins with <strong class="keyWord">Start</strong>.</p>
    <h3 class="heading-3" id="_idParaDest-150"><a id="_idTextAnchor156"/>Start</h3>
    <p class="normal">The CoT reasoning process begins when it receives input text provided by the AI agent. The AI agent analyzes the user input and then triggers the CoT function, as described earlier in the <em class="italic">Layer 2: AI agent</em> section. At the start of the CoT function, two key initializations occur: the reasoning memory (<code class="inlineCode">steps = []</code>) is initialized, and the reasoning display widget is activated within the IPython interactive interface:</p>
    <pre class="programlisting code"><code class="hljs-code">steps = []
    # Display the reasoning_output widget in the interface
    display(reasoning_output)
</code></pre>
    <p class="normal"><code class="inlineCode">display(reasoning_output)</code> triggers the <code class="inlineCode">display</code> widget, which enables real-time updates in the<a id="_idIndexMarker380"/> interactive IPython interface, ensuring the CoT process remains transparent and easily interpretable by users.</p>
    <h3 class="heading-3" id="_idParaDest-151"><a id="_idTextAnchor157"/>Step 1: ML-baseline</h3>
    <p class="normal">The first step, <strong class="keyWord">ML-baseline</strong>, activates <a id="_idIndexMarker381"/>the machine learning endpoint (<code class="inlineCode">machine_learning.ml_agent()</code>). It utilizes a decision tree classifier to analyze customer data dynamically and predict activities of interest. The function takes a location (for example, <code class="inlineCode">"Rome"</code>) and <code class="inlineCode">"ACTIVITY"</code> as the target column for the prediction:</p>
    <pre class="programlisting code"><code class="hljs-code"># Step 1: Analysis of the customer database and prediction
    steps.append("Process: Performing machine learning analysis of the customer database. \n")
    with reasoning_output:
        reasoning_output.clear_output(wait=True)
        print(steps[-1])  # Print the current step
    time.sleep(2)  # Simulate processing time
    result_ml = machine_learning.ml_agent("Rome", "ACTIVITY")
    steps.append(f"Machine learning analysis result: {result_ml}")
</code></pre>
    <p class="normal">This block of code is repeated for each reasoning step:</p>
    <ul>
      <li class="bulletList">Each part of the thought process begins with a comment like so: <code class="inlineCode"># Step 1: Analysis of the customer database and prediction</code></li>
      <li class="bulletList"><code class="inlineCode">steps.append("Process: Performing machine learning analysis of the customer database. \n")</code> appends a description of the step to the reasoning memory step list</li>
      <li class="bulletList"><code class="inlineCode">with reasoning_output</code> initiates a code block for the display widget</li>
      <li class="bulletList"><code class="inlineCode">reasoning_output.clear_output(wait=True)</code> clears <code class="inlineCode">reasoning_output t</code></li>
      <li class="bulletList"><code class="inlineCode">print(steps[-1]) # Print the current step</code> prints the most recent step added</li>
      <li class="bulletList"><code class="inlineCode">time.sleep(2) # processing time</code> introduces a two-second delay</li>
      <li class="bulletList"><code class="inlineCode">result_ml =machine_learning.ml_agent("Rome", "ACTIVITY")</code> calls <code class="inlineCode">ml_agent</code></li>
      <li class="bulletList"><code class="inlineCode">steps.append(f"Machine learning analysis result: {result_ml}")</code> appends the result returned by the machine learning function to the list of steps</li>
    </ul>
    <p class="normal">The output<a id="_idIndexMarker382"/> from <code class="inlineCode">machine_learning.ml_agent</code>, which predicts the top customer-preferred activity for the location <code class="inlineCode">"Rome"</code>, becomes the input for the subsequent step, suggesting creative activities.</p>
    <p class="normal">Before moving on to the next step, let’s briefly explore the underlying decision tree classifier inside <code class="inlineCode">machine_learning.py</code>.</p>
    <h4 class="heading-4">Decision tree classifier</h4>
    <p class="normal">A decision tree classifier<a id="_idIndexMarker383"/> is well suited for our task because it is a machine learning model that makes predictions by splitting data into a tree-like structure based on feature values. It works by recursively choosing the optimal feature at each split until it reaches a defined stopping condition, such as a maximum depth or a minimum sample size per leaf. At each step, the possibilities narrow down until a single prediction emerges.</p>
    <p class="normal">To run it, we first import the required libraries for handling data and building the decision tree. We also disable warnings to avoid cluttering the IPython output:</p>
    <pre class="programlisting code"><code class="hljs-code">import pandas as pd
import random
from sklearn.preprocessing import LabelEncoder  # For encoding categorical variables
from sklearn.tree import DecisionTreeClassifier  # For training the Decision Tree model
import warnings
warnings.simplefilter(action='ignore', category=UserWarning)
</code></pre>
    <p class="normal">Next, we define our classifier function, <code class="inlineCode">ml_agent()</code>, with two parameters:</p>
    <pre class="programlisting code"><code class="hljs-code">def ml_agent(feature1_value, feature2_column):
</code></pre>
    <p class="normal">The two parameters are the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">feature1_value</code>: The value of the location we want to predict activities for.</li>
      <li class="bulletList"><code class="inlineCode">feature2_column</code>: The target column (<code class="inlineCode">"ACTIVITY"</code>) we want to predict.</li>
    </ul>
    <p class="normal">The function starts by loading the customer activities dataset into a pandas DataFrame:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Load the dataset from a CSV file into a DataFrame
    df = pd.read_csv("customer_activities.csv")
</code></pre>
    <p class="normal">Then, we encode the categorical variables (<code class="inlineCode">LOCATION</code> and <code class="inlineCode">ACTIVITY</code>) for processing:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Create LabelEncoder objects for encoding categorical variables
    le_location = LabelEncoder()
    le_activity = LabelEncoder()
# Encode categorical values
    df["LOCATION_ENCODED"] = le_location.fit_transform(df["LOCATION"])
    df["ACTIVITY_ENCODED"] = le_activity.fit_transform(df["ACTIVITY"])
</code></pre>
    <p class="normal">If no specific location (<code class="inlineCode">feature1_value</code>) is provided, the function selects the most frequent location by default:</p>
    <pre class="programlisting code"><code class="hljs-code"># Select default location if feature1_value is empty
    if not feature1_value.strip():  # If empty string or only spaces
        feature1_value = df["LOCATION"].mode()[0]  # Most common location
</code></pre>
    <p class="normal">We then<a id="_idIndexMarker384"/> prepare the features (<code class="inlineCode">X</code>) and the target variable (<code class="inlineCode">y</code>) from our encoded data:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Select the encoded 'LOCATION' column as the feature (X)
    X = df[["LOCATION_ENCODED"]]
    # Select the encoded 'ACTIVITY' column as the target variable (y)
    y = df["ACTIVITY_ENCODED"]
</code></pre>
    <p class="normal">With our data prepared, we train the decision tree model:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Train a Decision Tree Classifier on the dataset
    model = DecisionTreeClassifier(random_state=42)
    model.fit(X, y)
</code></pre>
    <p class="normal">Setting <code class="inlineCode">random_state=42</code> ensures consistent results each time we run the code. Now, we encode the provided (or default) location input to prepare it for prediction:</p>
    <pre class="programlisting code"><code class="hljs-code">   # Encode the input location using the same LabelEncoder
   feature1_encoded = le_location.transform([feature1_value])[0]
</code></pre>
    <p class="normal">The Python <code class="inlineCode">.transform</code> method on the <code class="inlineCode">le_location</code> object converts the categorical string into its unique integer code.</p>
    <p class="normal">The function is now ready to predict the most probable activity and convert it back to its original label. We will use the Python <code class="inlineCode">.predict</code> method of our trained model to see what it predicts for this new data point:</p>
    <pre class="programlisting code"><code class="hljs-code">    # Predict the encoded activity for the given location
    predicted_activity_encoded = model.predict([[feature1_encoded]])[0]
    # Convert the predicted numerical activity back to its original label
    predicted_activity = le_activity.inverse_transform(
        [predicted_activity_encoded]
    )[0]
</code></pre>
    <p class="normal">Finally, the function constructs a customer’s descriptive output message tailored to the predicted activity:</p>
    <pre class="programlisting code"><code class="hljs-code"> # Generate output text
    text = (f"The customers liked the {predicted_activity} because it reminded them of how "
            f"our democracies were born and how it works today. "
            f"They would like more activities during their trips that provide insights into "
            f"the past to understand our lives.")
</code></pre>
    <p class="normal">This <a id="_idIndexMarker385"/>descriptive output is returned to the CoT function:</p>
    <pre class="programlisting code"><code class="hljs-code">    return text
</code></pre>
    <p class="normal">To invoke the classifier from the CoT function, we use the following:</p>
    <pre class="programlisting code"><code class="hljs-code">result_ml = ml_agent("", "ACTIVITY")
print(result_ml)
</code></pre>
    <p class="normal">We’re letting the classifier find the location and activity. The expected output, in this case, will be the following:</p>
    <pre class="programlisting con"><code class="hljs-con">Machine learning analysis result: The customers liked the Forum of Rome because it reminded them of how our democracies were born and how it works today. They would like more activities during their trips that provide insights into the past to understand our lives.
</code></pre>
    <p class="normal">Let’s now use the output of this step to suggest activities.</p>
    <h3 class="heading-3" id="_idParaDest-152"><a id="_idTextAnchor158"/>Step 2: Suggest activities</h3>
    <p class="normal">This <a id="_idIndexMarker386"/>step follows the same logic and structure as <em class="italic">Step 1</em>. The name of the process is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">steps.append("Process: Searching for activities that fit the customer needs. \n")
</code></pre>
    <p class="normal">The output from <em class="italic">Step 1</em> (<code class="inlineCode">result_ml</code>) becomes part of the instruction sent to GPT-4o to augment the input context. The combined query (<code class="inlineCode">umessage</code>) for GPT-4o becomes as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">umessage = (
        "What activities could you suggest to provide more activities and excitement in holiday trips."
        + result_ml
    )
</code></pre>
    <p class="normal">At this stage, the instructions are tailored specifically for our travel-focused domain. In <a href="Chapter_6.xhtml#_idTextAnchor166"><em class="italic">Chapter 6</em></a>, we’ll evolve these instructions to become dynamic event-based variables. Here, we continue using<a id="_idIndexMarker387"/> the established GenAISys OpenAI API call we built in earlier chapters:</p>
    <pre class="programlisting code"><code class="hljs-code">mrole = "system"
    mcontent = (
        "You are an assistant that explains your reasoning step by step before providing the answer. "
        "Use structured steps to break down the query."
    )
    user_role = "user"
    task_response = make_openai_api_call(umessage, mrole, mcontent, user_role)
</code></pre>
    <p class="normal">The output received from GPT-4o (<code class="inlineCode">task_response</code>) will serve as the input for the next step (<em class="italic">Step 3</em>). The method of appending and displaying the reasoning steps remains consistent with <em class="italic">Step 1</em>.</p>
    <h3 class="heading-3" id="_idParaDest-153"><a id="_idTextAnchor159"/>Step 3: Generate image</h3>
    <p class="normal">This step <a id="_idIndexMarker388"/>begins by taking the detailed suggestion received from the previous step (<code class="inlineCode">task_response</code>) and passing it directly as the prompt to DALL-E’s image generation function. The structure and logic here are consistent with the previous steps, now focused on generating images:</p>
    <pre class="programlisting code"><code class="hljs-code">prompt = task_response
image_url = generate_image(prompt)
</code></pre>
    <p class="normal">Once generated, the image is downloaded and saved locally as <code class="inlineCode">c_image.png</code>. This image file will then be displayed through the IPython interface if the <strong class="screenText">Files</strong> widget is checked, as explained in the <em class="italic">Layer 1: IPython interface</em> section:</p>
    <pre class="programlisting code"><code class="hljs-code">    …
    save_path = "c_image.png"
    image_data = requests.get(image_url).content
    with open(save_path, "wb") as file:
        file.write(image_data)
    steps.append(f"Image saved as {save_path}")
    …
</code></pre>
    <p class="normal">With the image now generated and saved, the CoT process advances to analyzing this newly created image.</p>
    <h3 class="heading-3" id="_idParaDest-154"><a id="_idTextAnchor160"/>Step 4: Analyze image</h3>
    <p class="normal">The input for <a id="_idIndexMarker389"/>this analysis step is the URL of the image generated in <em class="italic">Step 3</em>, stored as <code class="inlineCode">image_url</code>. As mentioned earlier, in this notebook, the query text is currently set as a generic, yet travel-specific, request to GPT-4o. In subsequent chapters, this query text will become event-driven and more dynamic.</p>
    <p class="normal">For our image analysis, we instruct the generative AI model to craft an engaging story based on the generated image:</p>
    <pre class="programlisting code"><code class="hljs-code">query_text = "Providing an engaging story based on the generated image"
</code></pre>
    <p class="normal">The code encapsulating the instructions is the same as in the previous steps. The CoT function now activates the <code class="inlineCode">image_analysis</code> function as described previously in the <em class="italic">Image generation and analysis</em> section:</p>
    <pre class="programlisting code"><code class="hljs-code">    response = image_analysis(image_url, query_text)
  
</code></pre>
    <p class="normal">The output is returned to the <code class="inlineCode">response</code> variable and saved in the <code class="inlineCode">image_text.txt</code> file for further use. This marks the completion of the CoT reasoning steps.</p>
    <h3 class="heading-3" id="_idParaDest-155"><a id="_idTextAnchor161"/>End</h3>
    <p class="normal">Upon completing all reasoning tasks, the CoT function signals the end of the process by clearing and updating the IPython display:</p>
    <pre class="programlisting code"><code class="hljs-code">   # Clear output and notify completion
    with reasoning_output:
        reasoning_output.clear_output(wait=True)
        print("All steps completed!")
    return steps
</code></pre>
    <p class="normal">The IPython interface takes over from here. Let’s now run the CoT from a user perspective.</p>
    <h1 class="heading-1" id="_idParaDest-156"><a id="_idTextAnchor162"/>Running CoT reasoning from a user perspective</h1>
    <p class="normal">In this <a id="_idIndexMarker390"/>section, we’ll seamlessly run the complex GenAISys we’ve been building since the beginning of the book. A single prompt will trigger the entire CoT process.</p>
    <p class="normal">We’ll simulate a user activating the reasoning capabilities of the GenAISys to obtain comprehensive ideation for an online travel agency. Specifically, we aim to predict customer-preferred activities, generate engaging images, and create storytelling narratives to evoke customers’ episodic memories. These episodic memories might be real-world experiences or dreams of visiting a place and engaging in particular activities.</p>
    <p class="normal">To run this scenario, make sure to check the <strong class="screenText">AI Agent</strong> and <strong class="screenText">Files</strong> checkboxes and enter the following prompt carefully:</p>
    <pre class="programlisting code"><code class="hljs-code">Use reasoning to suggest customer activities.
</code></pre>
    <p class="normal">The <code class="inlineCode">Use</code>, <code class="inlineCode">reasoning</code>, <code class="inlineCode">customer</code>, and <code class="inlineCode">activities</code> keywords will be recognized by the AI agent and trigger the CoT process we built in this chapter. Alternatively, we could have implemented a drop-down menu or performed a similarity search in the Pinecone index to retrieve specific instruction scenarios. STT input is also possible. In this chapter, however, we’ll use typed prompts with keywords to clearly illustrate the CoT process.</p>
    <div><p class="normal"> In <a href="Chapter_7.xhtml#_idTextAnchor191"><em class="italic">Chapter 7</em></a>, we’ll build a central keyword registry and an orchestrator to further optimize the AI agent’s decision-making process.</p>
    </div>
    <p class="normal">Once the user presses <em class="italic">Enter</em>, all we have to do is sit back and watch just as we would with online ChatGPT-like copilots. The<a id="_idIndexMarker391"/> first process is to analyze the customer base to find the top-ranking activity based on daily data, as shown here.</p>
    <figure class="mediaobject"><img alt="Figure 5.10: Searching for activities" src="img/B32304_05_10.png"/></figure>
    <p class="packt_figref">Figure 5.10: Searching for activities</p>
    <p class="normal">Once the whole process is complete, the decision tree classifier returns the results:</p>
    <pre class="programlisting con"><code class="hljs-con">..Machine learning analysis result: The customers liked the Forum of Rome because it reminded them of how…
</code></pre>
    <p class="normal">The next stage involves searching for suitable activities matching customer preferences:</p>
    <figure class="mediaobject"><img alt="Figure 5.11: Searching for activities matching customer needs" src="img/B32304_05_11.png"/></figure>
    <p class="packt_figref">Figure 5.11: Searching for activities matching customer needs</p>
    <p class="normal">The creative output from GPT-4o provides structured steps to enhance the online offerings:</p>
    <pre class="programlisting con"><code class="hljs-con">Activity suggestions: To enhance holiday trips with more activities, especially focusing on cultural experiences, we can consider a variety of options. Here's a structured approach to brainstorming and suggesting activities:
…### Step 3: Suggest Activities
1. <strong class="hljs-con-slc">Historical Tours and Sites</strong>:
- <strong class="hljs-con-slc">Athens, Greece</strong>: Visit the Acropolis and the Agora, where democracy was born. Include guided tours that explain the significance of these sites.
- <strong class="hljs-con-slc">Philadelphia, USA</strong>: Explore Independence Hall and the Liberty Bell, focusing on the birth of modern democracy.
- <strong class="hljs-con-slc">Westminster, UK</strong>: Tour the Houses of Parliament and learn about the evolution of the British democratic system…
</code></pre>
    <p class="normal">Next, the CoT instructs DALL-E to generate an engaging image based on these suggested activities:</p>
    <figure class="mediaobject"><img alt="Figure 5.12: Image generation based on the output of the previous step" src="img/B32304_05_12.png"/></figure>
    <p class="packt_figref">Figure 5.12: Image generation based on the output of the previous step</p>
    <p class="normal">Because the <strong class="screenText">Files</strong> checkbox is<a id="_idIndexMarker392"/> checked, the generated image is displayed. This image is a rather creative one and will vary with each run:</p>
    <figure class="mediaobject"><img alt="Figure 5.13: A cultural and historical image" src="img/B32304_05_13.png"/></figure>
    <p class="packt_figref">Figure 5.13: A cultural and historical image</p>
    <p class="normal">In this case, the image contains text such as <code class="inlineCode">…understanding of history and its impact on modern life.</code>, which perfectly fits our request.</p>
    <div><p class="normal"> Note that each run might produce a different output due to context variations and the stochastic (probabilistic) nature of generative AI models such as GPT-4o.</p>
    </div>
    <p class="normal">The next process involves asking GPT-4o to create a narrative for a storytelling promotion that leverages episodic memory of past real-life experiences or imagined trips:</p>
    <figure class="mediaobject"><img alt="Figure 5.14: Creating an engaging story based on the image generated" src="img/B32304_05_14.png"/></figure>
    <p class="packt_figref">Figure 5.14: Creating an engaging story based on the image generated</p>
    <p class="normal">The narrative output from GPT-4o, shown, is illustrative and will vary, as noted earlier:</p>
    <pre class="programlisting con"><code class="hljs-con">…Story response: In the bustling town of New Haven, a place where history and technology intertwined, a young historian named Clara discovered an ancient artifact that would change everything. The artifact, a mysterious tablet, was said to hold the secrets of the past, capable of bringing historical figures to life through augmented reality…
</code></pre>
    <p class="normal">Once the <a id="_idIndexMarker393"/>CoT sequence concludes, the GenAISys maintains its reasoning state, waiting for new standalone prompts or further CoT runs:</p>
    <figure class="mediaobject"><img alt="Figure 5.15: Reasoning is persistently activated in the GenAISys" src="img/B32304_05_15.png"/></figure>
    <p class="packt_figref">Figure 5.15: Reasoning is persistently activated in the GenAISys</p>
    <p class="normal">The <em class="italic">Load and display the conversation history</em> and <em class="italic">Load and summarize the conversation history</em> sections in the notebook utilize the same functions detailed in <a href="Chapter_4.xhtml#_idTextAnchor110"><em class="italic">Chapter 4</em></a>.</p>
    <p class="normal">We’ve now successfully built a small-scale ChatGPT-like GenAISys equipped with custom features, including multi-user support, domain-specific RAG, and tailored CoT capabilities. In the upcoming chapters, we’ll apply this GenAISys framework across several practical business domains.</p>
    <h1 class="heading-1" id="_idParaDest-157"><a id="_idTextAnchor163"/>Summary</h1>
    <p class="normal">In this chapter, we have completed the basic framework of the GenAISys, consisting of three layers. The first layer is an IPython interactive interface that acts as an orchestrator. It now includes voice capability, file display, and CoT features, alongside user inputs, user selections, and the AI agent widget.</p>
    <p class="normal">The second layer is the AI agent orchestrator, triggered by user prompts. This demonstrates that within the GenAISys, the boundaries between orchestration and control functions are somewhat blurred due to the interactive nature of these components. The AI agent distributes tasks between the Pinecone index for querying and the OpenAI API agent for generative tasks, such as content and image generation. The AI agent can also trigger the CoT process, and we will further enhance its capabilities in the following chapters.</p>
    <p class="normal">The third and final layer contains the core functionality of the GenAISys, which involves AI workers powered by GPT-4o and DALL-E. In this chapter, we introduced DALL-E for image generation and utilized GPT-4o to provide insightful comments on these images. Additionally, we implemented a decision tree classifier to predict customer activities, incorporating machine learning capabilities into our GenAISys.</p>
    <p class="normal">Introducing the CoT feature marked our initial step toward creating seamless reasoning capabilities from an end user perspective. Complex tasks require sophisticated AI systems that can emulate human reasoning. Therefore, we will expand upon the reasoning abilities of the GenAISys, among other features, in the next chapter.</p>
    <h1 class="heading-1" id="_idParaDest-158"><a id="_idTextAnchor164"/>Questions</h1>
    <ol>
      <li class="numberedList" value="1">The seamless interface of an online generative AI system shows that the system is easy to build. (True or False)</li>
      <li class="numberedList">Selecting a <strong class="keyWord">large language model</strong> (<strong class="keyWord">LLM</strong>) is sufficient to build a GenAISys. (True or False)</li>
      <li class="numberedList">A generative AI application requires an event-driven interactive interface. (True or False)</li>
      <li class="numberedList">An AI system can mimic human reasoning. (True or False)</li>
      <li class="numberedList">A <strong class="keyWord">chain-of-thought</strong> (<strong class="keyWord">CoT</strong>) process is just a sequence of classical functions. (True or False)</li>
      <li class="numberedList">A CoT can process natural language but not computer vision. (True or False)</li>
      <li class="numberedList">A CoT is a cognitive agent that can include non-AI or AI functions. (True or False)</li>
      <li class="numberedList">Reasoning GenAISys can group a set of tasks for an end user. (True or False)</li>
      <li class="numberedList">The continual acceleration of the economy requires more automation, including AI. (True or False)</li>
      <li class="numberedList">A human-centric reasoning GenAISys can boost the productivity of a team. (True or False)</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-159"><a id="_idTextAnchor165"/>References</h1>
    <ul>
      <li class="bulletList">Chan, Andy, Cassidy Ezell, Michael Kaufmann, Kevin Wei, Laurel Hammond, Hunter Bradley, Elliot Bluemke, Nandhini Rajkumar, David Krueger, Nikita Kolt, Lukas Heim, and Markus Anderljung. “Visibility into AI Agents.” In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency (FAccT ‘24), Rio de Janeiro, Brazil, June 3–6, 2024. New York: ACM, 2024. <a href="https://arxiv.org/pdf/2401.13138">https://arxiv.org/pdf/2401.13138</a>.</li>
      <li class="bulletList">Putta, Praveen, Eric Mills, Naman Garg, Soham Motwani, Chelsea Finn, Divyansh Garg, and Rohan Rafailov. “Agent Q: Advanced Reasoning and Learning for Autonomous AI Agents.” Last modified 2024. <a href="https://arxiv.org/abs/2408.07199">https://arxiv.org/abs/2408.07199</a>.</li>
      <li class="bulletList">Wiesinger, Jannis, Peter Marlow, and Vladimir Vuskovic. “Agents.” Kaggle Whitepaper. Accessed July 8, 2025. <a href="https://www.kaggle.com/whitepaper-agents">https://www.kaggle.com/whitepaper-agents</a>.</li>
      <li class="bulletList">OpenAI. OpenAI API Documentation. Accessed July 8, 2025. <a href="https://platform.openai.com/docs/api-reference/introduction">https://platform.openai.com/docs/api-reference/introduction</a>.</li>
    </ul>
    <div><table class="table-container" id="table001-3">
        <tbody>
          <tr>
            <td class="table-cell">
              <h4 class="heading-4">Unlock this book’s exclusive benefits now</h4>
              <p class="normal">Scan this QR code or go to <a href="http://packtpub.com/unlock">packtpub.com/unlock</a>, then search for this book by name.</p>
            </td>
            <td class="table-cell" rowspan="2">
              <figure class="mediaobject"><img alt="A qr code on a white background  AI-generated content may be incorrect." src="img/Unlock.png"/></figure>
            </td>
          </tr>
          <tr>
            <td class="table-cell">
              <p class="normal"><em class="italic">Note: Keep your purchase invoice ready before you start.</em></p>
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</body></html>