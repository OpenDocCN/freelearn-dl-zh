- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging OpenAI’s Models for Enterprise-Scale Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll focus on the enterprise-level applications of **generative
    AI** (**GenAI**) and, more specifically, of OpenAI’s models. We will see how different
    industries have been massively impacted by GenAI in recent years, and what kinds
    of trending patterns and applications have emerged.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest advancements in various industries (including healthcare, financial
    services, retail, and more), driven by the outstanding capabilities of powerful
    LLMs, highlighting the most trending use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architectural framework behind custom applications powered by OpenAI’s models,
    unveiling the versatility and adoption of the models’ APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Azure OpenAI, the Microsoft cloud-based service that mirrors
    OpenAI’s Playground and offers OpenAI’s models directly within the perimeter of
    Azure subscriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the main GenAI patterns
    across various industries, and how to leverage OpenAI’s models’ APIs within your
    own applications. Plus, you will have a clearer understanding of the cloud-scale
    service of Azure OpenAI and how to incorporate ethical considerations when developing
    AI-based solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An OpenAI account, chat model, and embedding model deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Optional] An Azure subscription and Azure OpenAI instance, with chat model
    and embedding model deployments'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or a later version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can refer to the following repository for the OpenAI Python SDKs: https://github.com/openai/openai-python.'
  prefs: []
  type: TYPE_NORMAL
- en: How GenAI is disrupting industries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs, and GenAI in general, are revolutionizing various industries by introducing
    unprecedented levels of automation, creativity, and efficiency. In recent years,
    we’ve witnessed a huge wave of innovation across different industries that all
    agree that not seizing the GenAI opportunity would mean falling behind in a competitive
    market.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Healthcare
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In healthcare, GenAI and LLMs are enhancing diagnostics, personalized medicine,
    and administrative tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diagnostics**: LLMs like GPT-4 are being used to analyze medical images,
    predict diseases, and suggest treatment plans. For instance, AI-powered tools
    can now analyze radiology images with high accuracy, identifying early signs of
    conditions like cancer or heart disease, often outperforming human radiologists
    in speed and consistency. A great example of the latest advancements in the computer
    vision field is given in an article by Tyler J. Bradshaw et al., “Large Language
    Models and Large Multimodal Models in Medical Imaging: A Primer for Physicians”,
    published in *The Journal of Nuclear Medicine* (you can find it at https://jnm.snmjournals.org/content/early/2025/01/16/jnumed.124.268072).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized medicine**: GenAI is helping in the development of personalized
    treatment plans by analyzing patient data, including genetic information. This
    has led to tailored therapies that improve outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Administrative efficiency**: LLMs are streamlining administrative tasks such
    as patient record management and appointment scheduling. AI chatbots can handle
    patient queries, reducing the workload on medical staff.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenAI has partnered with Summer Health, a healthcare service that provides
    fast and convenient access to pediatric care through text messaging. The collaboration
    aims to enhance the capabilities of Summer Health’s platform by integrating OpenAI’s
    advanced language models. This integration enables more efficient and accurate
    responses to parents’ healthcare inquiries, providing quick, reliable medical
    advice for children’s health concerns. This has led to increased efficiency and
    improved timeliness, with data being kept anonymous. The AI-driven platform helps
    streamline communication between parents and healthcare professionals, improving
    the overall experience and accessibility of pediatric care.
  prefs: []
  type: TYPE_NORMAL
- en: '*Source:* https://openai.com/index/summer-health/.'
  prefs: []
  type: TYPE_NORMAL
- en: Finance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In finance, GenAI and LLMs are transforming risk management, customer service,
    and investment strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Claim management**: LLMs are employed to automate the summarization, review,
    triage, and adjudication of claims. For instance, Munich Re developed an LLM-powered
    solution for claim management that led to a streamlined claims process, reduced
    manual effort, and improved decision-making accuracy (https://www.munichre.com/us-life/en/insights/future-of-risk/large-language-models-in-underwriting-and-claims.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer service**: AI-driven chatbots and virtual assistants are now common
    in the finance sector, handling customer inquiries, processing transactions, and
    providing financial advice. ING’s AI assistant is a prime example of a virtual
    assistant that helps customers manage their finances by providing insights, reminders,
    and transaction details (https://www.mckinsey.com/industries/financial-services/how-we-help-clients/banking-on-innovation-how-ing-uses-generative-ai-to-put-people-first).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Investment strategies**: Hedge funds and investment firms are using GenAI
    to create predictive models that inform trading decisions. AI algorithms analyze
    market data to identify patterns and make real-time trading decisions. BlackRock’s
    Aladdin platform is one such example, leveraging AI to manage investments and
    assess market risks (https://www.blackrock.com/aladdin/solutions/aladdin-copilot).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Moody’s Corporation, a leading global provider of credit ratings, research,
    and risk analysis, has partnered with Microsoft to develop enhanced risk data
    analytics and research solutions powered by GenAI. This collaboration combines
    Moody’s vast expertise in financial risk and data analytics with Microsoft’s advanced
    AI technology. The result is a set of tools that offer real-time insights into
    financial risks, enabling more precise decision-making and improved risk management
    for financial institutions and other stakeholders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: https://news.microsoft.com/2023/06/29/moodys-and-microsoft-develop-enhanced-risk-data-analytics-research-and-collaboration-solutions-powered-by-generative-ai/?msockid=2dc01bb6f864693933ed0eb3f9a668dc.'
  prefs: []
  type: TYPE_NORMAL
- en: Retail and e-commerce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In retail and e-commerce, GenAI and LLMs are enhancing customer experience,
    inventory management, and personalized marketing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer experience**: AI-powered chatbots provide personalized customer
    service, helping shoppers find products, resolve issues, and make purchases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inventory management**: LLMs help retailers predict demand and optimize inventory
    levels by analyzing sales data, seasonal trends, and customer behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized marketing**: GenAI is enabling hyper-personalized marketing
    campaigns. By analyzing customer data, AI can create targeted advertisements and
    product recommendations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coca-Cola has launched an innovative initiative inviting digital artists to
    create unique artworks using a new AI-powered platform developed in collaboration
    with **Google Cloud Platform** (**GCP**). This platform allows artists to generate
    digital content by blending Coca-Cola’s iconic branding elements with their creativity.
    The initiative, called “Create Real Magic,” leverages advanced AI tools to inspire
    and empower artists, facilitating the creation of digital art that resonates with
    Coca-Cola’s brand ethos. This project highlights how AI can be used to bridge
    creativity and technology in the retail and consumer goods industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: https://brandthechange.com/creativity/create-real-magic-inside-coca-colas-first-ai-powered-campaign/#:~:text=The%20Coca-Cola%20Company%20has%20partnered%20with%20OpenAI%20and,using%20iconic%20creative%20assets%20from%20the%20Coca-Cola%20archives.'
  prefs: []
  type: TYPE_NORMAL
- en: Manufacturing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In manufacturing, GenAI and LLMs are driving automation, quality control, and
    supply chain optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automation**: AI-powered robots and systems are automating repetitive tasks,
    such as assembly line work and material handling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control**: LLMs are used to monitor production processes in real
    time, identifying defects or inefficiencies. AI systems can analyze data from
    sensors and cameras to detect anomalies in products, ensuring higher quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supply chain optimization**: AI models help manufacturers optimize their
    supply chains by predicting demand, managing inventory, and selecting suppliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Iveco Group, a leading global manufacturer of commercial vehicles, has partnered
    with Microsoft to integrate Azure OpenAI Service into its business processes.
    The customer developed an internal smart chatbot called “Chat IVG”, which can
    be used for questions and answers and to extract information from the organization’s
    own data and documents. Plus, numerous use cases and autonomous projects are being
    developed and deployed in production, either leveraging Chat IVG’s specific customizations
    or using its architecture as a foundation. Chat IVG is driving significant impact
    by enhancing internal business user experiences, boosting productivity across
    various business units, and enabling faster, more efficient [customer support.](https://www.microsoft.com/en/customers/story/1706380538888475836-iveco-group-azure-openai-service-manufacturing-italy)
  prefs: []
  type: TYPE_NORMAL
- en: '[Source: https://www.microsoft.com/en/customers/story/1706380538888475836-iveco-group-azure-openai-](https://www.microsoft.com/en/customers/story/1706380538888475836-iveco-group-azure-openai-service-manufacturing-italy)service-manufacturing-italy.'
  prefs: []
  type: TYPE_NORMAL
- en: Media and entertainment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In media and entertainment, GenAI and LLMs are revolutionizing content creation,
    audience engagement, and media distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content creation**: GenAI is being used to generate content, from writing
    articles to composing music. For example, The Washington Post uses AI to write
    short news articles and reports, freeing up journalists to focus on more complex
    stories. In music, AI platforms like OpenAI’s MuseNet can compose original music
    tracks in various styles, aiding musicians in the creative process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Audience engagement**: LLMs analyze user data to deliver personalized content
    recommendations, keeping audiences engaged. Netflix uses AI to recommend movies
    and TV shows based on viewers’ preferences, significantly increasing viewer retention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Media distribution**: AI is also optimizing media distribution by analyzing
    audience demographics and consumption patterns. Spotify uses AI to curate personalized
    playlists, ensuring that users discover new music tailored to their tastes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Microsoft’s Xbox division has announced a multi-year partnership with Inworld
    AI to develop advanced GenAI tools for game development. This collaboration aims
    to enhance character dialogue and narrative creation by integrating Inworld’s
    expertise in GenAI with Microsoft’s Azure OpenAI Service and insights from Microsoft
    Research. The goal is to empower game developers to create more dynamic and immersive
    gaming experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: https://developer.microsoft.com/en-us/games/articles/2023/11/xbox-and-inworld-ai-partnership-announcement/.'
  prefs: []
  type: TYPE_NORMAL
- en: Legal services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the legal industry, GenAI and LLMs are transforming research, contract analysis,
    and case prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Legal research**: AI tools are accelerating legal research by analyzing vast
    amounts of legal documents, case laws, and statutes. For example, ROSS Intelligence
    uses AI to provide lawyers with relevant case laws and legal precedents in seconds,
    which would otherwise take hours to find manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contract analysis**: LLMs are used to review and analyze contracts, identifying
    key terms, risks, and compliance issues. This helps in speeding up negotiations
    and ensuring that contracts are airtight. Kira Systems is one example where AI
    reviews contracts for due diligence, identifying clauses and potential risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case prediction**: GenAI is being used to predict the outcomes of legal cases
    based on historical data. By analyzing past cases, AI can provide lawyers with
    insights into likely judgments, helping them strategize better. Lex Machina, for
    example, uses AI to predict how judges might rule in intellectual property disputes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Ironclad, a leading digital contracting platform, has partnered with OpenAI
    to integrate advanced AI capabilities into its legal workflows. By leveraging
    OpenAI’s language models, Ironclad enhances its platform’s ability to automate
    contract analysis, generate and review legal documents, and provide insights to
    legal teams more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: This integration allows for faster, more accurate contract processing, reducing
    the time spent on manual reviews and enabling legal teams to focus on higher-value
    tasks. The collaboration underscores the growing role of AI in transforming the
    legal industry by improving accuracy and productivity in contract management.
  prefs: []
  type: TYPE_NORMAL
- en: Education
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In education, GenAI and LLMs are transforming learning experiences, personalized
    education, and administrative tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learning experiences**: AI-driven platforms are creating personalized learning
    paths for students based on their strengths and weaknesses. For instance, platforms
    like Coursera use AI to recommend courses and resources tailored to each learner’s
    progress and preferences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personalized education**: LLMs can tutor students by answering questions,
    explaining concepts, and providing feedback on assignments. Khan Academy’s AI-powered
    tutor is an example, offering personalized help to students struggling with specific
    topics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Administrative tasks**: AI is also being used to automate administrative
    tasks such as grading and scheduling. For instance, Turnitin uses AI to grade
    essays and detect plagiarism, saving educators time and ensuring academic integrity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Khan Academy has partnered with OpenAI to incorporate advanced AI capabilities
    into its educational platform. By integrating OpenAI’s language models, Khan Academy
    is able to provide personalized tutoring, answer student queries, and assist with
    learning in a more interactive and dynamic way. This collaboration aims to enhance
    the educational experience by offering students real-time assistance and tailored
    support, making learning more accessible and effective. The AI-powered tools help
    students grasp complex concepts, provide instant feedback, and adapt to individual
    learning styles, further democratizi[ng education through technology.](https://openai.com/index/khan-academy/)
  prefs: []
  type: TYPE_NORMAL
- en: '[Sourc](https://openai.com/index/khan-academy/)e: https://openai.com/index/khan-academy/.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above examples are just a subset of the possibilities that GenAI has enabled
    in various industries. However, there is an element that unites all the examples
    covered: in each scenario, a custom application was built leveraging an LLM API.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding OpenAI models’ APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In *Chapter 1* of this book, we saw how LLMs have introduced a paradigm shift
    in the landscape of AI: different from the tailored, highly specialized models
    that featured AI in the “before ChatGPT era,” LLMs are now able to be generalized
    and tackle different tasks depending on the user’s query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, there is one additional element that sets LLMs apart from previous
    models: in fact, LLMs typically come as pre-trained objects that anyone – even
    without any experience in the field of AI – can use with the easiest way of interacting:
    natural language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, no one is stopping you from designing and training your LLM from
    scratch, but be aware that this will require, at least:'
  prefs: []
  type: TYPE_NORMAL
- en: Technical knowledge on how to design the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A huge amount of training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized infrastructure that can support the training and inference stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of time to invest in the project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the above elements used to be a barrier to entry for many AI developers in
    the past, now the paradigm has shifted. The new focus, in fact, is how to efficiently
    build everything that lives *around* an LLM, such as the system message, **vector
    databases** (**VectorDBs**), plugins, and so forth. That’s the reason why using
    LLMs’ APIs is now the validated pattern for building GenAI applications.
  prefs: []
  type: TYPE_NORMAL
- en: What is a model API?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before talking about OpenAI models’ APIs, let’s first refresh our definition
    of what an API is.
  prefs: []
  type: TYPE_NORMAL
- en: An **application programming interface** (**API**) is a set of rules and tools
    that allows different software applications to communicate with each other. It’s
    like a translator that helps different programs or systems work together by sharing
    data and functionality in a standardized way. For example, when you use an app
    to check the weather, the app uses an API to get the weather information from
    a weather service.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: A weather app using an API to gather information'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, when it comes to LLMs’ APIs, the mechanism is similar. More specifically,
    LLMs’ APIs fall within the category of **Representational State Transfer** (**REST**)
    APIs, meaning that they:'
  prefs: []
  type: TYPE_NORMAL
- en: Use standard HTTP methods (POST for sending prompts, GET for retrieving data).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicate over HTTP/HTTPS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return responses in JSON format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follow a stateless model, meaning each request is independent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A **REST API** is a web-based API that follows REST principles, using HTTP methods
    like GET, POST, PUT, and DELETE to interact with resources via URLs. It is stateless,
    meaning each request is independent, and it typically exchanges data in JSON format.
    Other types of APIs include **SOAP**, which relies on XML for structured messaging
    and strict security; **GraphQL**, which allows clients to request specific data
    for more flexibility; **gRPC**, which uses Protocol Buffers for efficient microservice
    communication; **WebSockets**, which enables real-time, two-way communication;
    and **Streaming APIs**, which provide continuous data flow, often used for AI
    responses and stock market feeds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s explore how you might use an OpenAI model’s API to create a marketing
    assistant. This assistant helps marketers generate content like social media posts,
    email drafts, ad copy, or blog post ideas. Let’s break down the whole process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sending a request**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A marketer using your application might type a prompt like, “Create a social
    media post promoting our new eco-friendly product line.”
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Your marketing assistant sends this prompt to the OpenAI model’s API as part
    of a request. The request includes the prompt and any specific instructions, including
    the model to use – let’s say, the GPT-4o.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Processing by the model**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The OpenAI API receives the request and processes the prompt using the specified
    model (in our case, the GPT-4o).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The model generates a response by analyzing the input and drawing on its extensive
    knowledge base. It considers factors like the target audience, common marketing
    phrases, and the desired tone to create relevant content.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Receiving the response**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The API sends the generated content back to your marketing assistant application
    as a response.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, the model might generate something like: “Excited to launch our
    new eco-friendly product line! Sustainable, stylish, and perfect for the conscious
    consumer. Join us in making a positive impact—shop now and save the planet, one
    product at a time! #EcoFriendly #Sustainability.”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Displaying the response**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Your application receives the content from the API and displays it to the marketer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The marketer can then review, edit, and publish the content as needed, saving
    time and effort in the content creation process.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional features**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Customization**: The marketer can further customize the request. For example,
    they might ask for a series of posts or request variations to test different marketing
    angles.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedback loop**: The application might also allow the marketer to rate the
    generated content. This feedback could be used to fine-tune future requests, improving
    the relevance and quality of the content over time.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Behind the scenes**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**API key and authentication**: To use the OpenAI API, your application needs
    an API key (a unique alphanumeric string used to authenticate and identify applications
    or projects making requests to an API), which ensures that only authorized users
    can access the service.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling multiple requests**: The OpenAI API is designed to handle multiple
    requests at once, meaning it can serve many marketers simultaneously without slowing
    down.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rate limits and cost**: Depending on the API usage, there might be rate limits
    (e.g., how many requests can be sent per minute) and costs associated with the
    amount of text processed. Your application would need to manage these factors,
    perhaps by prioritizing certain requests or batching them.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The possibility of consuming OpenAI models via APIs gives developers great flexibility
    when it comes to customizing the application logic around the LLM. In the next
    section, we are going to see how to leverage those APIs in practice with Python.
  prefs: []
  type: TYPE_NORMAL
- en: How to use OpenAI models’ APIs with the Python SDK
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To use OpenAI models’ APIs in your programming IDE, you first need to create
    an access token from your OpenAI account.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When consuming OpenAI’s APIs, you will incur a cost that is proportional to
    the model’s usage. More specifically, OpenAI’s pricing model is **per token**,
    where a token represents a chunk of text (about 4 characters in English). To estimate
    your tokens’ consumption – hence your cost – you can refer to this articl[e: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-t](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)hem.'
  prefs: []
  type: TYPE_NORMAL
- en: Each API request consumes tokens based on input (prompt) and output (response).
    Pricing varies by model, with **more powerful models costing more per token.**
  prefs: []
  type: TYPE_NORMAL
- en: You can find OpenAI’s pricing model at https://openai.com/api/pricing/.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, you can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigat[e to https://platform.openai.com/api](https://platform.openai.com/api-keys)-keys.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on `+ Create new secret key`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B31559_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: OpenAI API platform'
  prefs: []
  type: TYPE_NORMAL
- en: This will create a new API key that you can save in a key vault of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you create the API key, you can use it to consume your model with the
    following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The above example leverages the Python SDK. However, you can also do your call
    with Node.js or curl, as specified in the OpenAI documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The schema of your client might vary depending on the model you are using and
    the data format you are passing as a prompt. For example, if you are using the
    gpt-4o-mini for image processing, your client will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: response = client.chat.completions.create(
  prefs: []
  type: TYPE_NORMAL
- en: model=”gpt-4o-mini”,
  prefs: []
  type: TYPE_NORMAL
- en: messages=[
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '“role”: “user”,'
  prefs: []
  type: TYPE_NORMAL
- en: '“content”: ['
  prefs: []
  type: TYPE_NORMAL
- en: '{“type”: “text”, “text”: prompt},'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '“type”: “image_url”,'
  prefs: []
  type: TYPE_NORMAL
- en: '“image_url”: {“url”: f”data:{img_type};base64,{img_b64_str}”},'
  prefs: []
  type: TYPE_NORMAL
- en: '},'
  prefs: []
  type: TYPE_NORMAL
- en: '],'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '],'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the OpenAI Python library at the following GitHub repos[itory:
    https://github.com/openai/openai](https://github.com/openai/openai-python)-python.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect how the response is built (I truncated the content of the response):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, there are many components that make up the response object:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: This is a unique identifier for the API call. In this case, `chatcmpl-9znQeWUbRyGmy3pWf7VfFWAppMCo7`
    is the specific ID associated with this particular chat completion request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`choices`: This is an array containing the different possible responses (choices)
    generated by the model. In this response, there’s only one choice (`index 0`),
    which is typical for most single-response completions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`index`: Indicates the position of this particular choice in the list of choices
    (in this case, `0`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`finish_reason`: Indicates why the model stopped generating tokens. **stop**
    usually means the model naturally reached the end of its response without needing
    to be cut off.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`logprobs`: If enabled, this would contain the log probabilities of each token
    in the completion. It is `None` here, indicating that you did not request this
    information.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`message`: Contains the content of the response (`''content''`) and the role
    of the speaker (`''role''`):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`content`: The actual text generated by the assistant, which in this case is
    a response regarding Azure AI services that support customer-managed keys'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`role`: The role of the speaker in the conversation, which is ‘assistant’ here,
    indicating the response came from the AI assistant'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`content_filter_results`: This contains the content filtering results for the
    response, checking for any harmful content in categories like hate, self-harm,
    sexual content, and violence. In this case, all categories are marked as `''safe''`
    and `''filtered'': False`, indicating no problematic content was detected.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`created`: This is a timestamp representing when the response was generated.
    The number `1724515040` is the UNIX timestamp (seconds since January 1, 1970).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model`: This indicates the version of the model that generated the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`object`: This indicates the type of object returned. In this case, `''chat.completion''`
    signifies that this is a completion from the chat API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system_fingerprint`: This is an internal identifier used by OpenAI for tracking
    or diagnosing the system that handled the request. `''fp_abc28019ad''` is the
    specific fingerprint for this transaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`usage`: This object tracks the token usage for the API call:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`completion_tokens`: The number of tokens used in the generated response (193
    tokens)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_tokens`: The number of tokens used in the input prompt (55 tokens)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`total_tokens`: The total number of tokens consumed in the request, which is
    the sum of the prompt and completion tokens (248 tokens)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt_filter_results`: This array contains the results of content filtering
    applied to the input prompt before generating the response. It ensures that the
    prompt does not contain harmful content. Like the `content_filter_results` in
    the choices section, it includes checks for hate, self-harm, sexual content, and
    violence. All are marked as `''safe''` and `''filtered'': False`, indicating no
    issues were found.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among all the output parameters, the `content_filter_results` might be particularly
    relevant when it comes to managing potentially harmful results. In fact, you might
    want to enforce a more conservative approach when it comes to potentially harmful
    content, in either input or output. If this is the case, you could simply enforce
    a deterministic rule that prevents the model from further processing any request
    that triggers a given level of risk.
  prefs: []
  type: TYPE_NORMAL
- en: This is a meaningful example of how leveraging OpenAI models’ APIs allows for
    great flexibility when it comes to building application logic around LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Architectural patterns to build applications with models’ APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The rise of GenAI and LLMs paved the way for a revolution in the field of software
    development. In fact, from “modern applications” – referring to microservices-based
    architectures and rapid innovation with CI/CD – we now talk about “intelligent
    applications” that are infused with GenAI models defined by natural language interaction,
    data-driven experience, and velocity of adaptation to new models’ releases.
  prefs: []
  type: TYPE_NORMAL
- en: 'An intelligent app can be described with the following illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer network  Description automatically generated with
    medium confidence](img/B31559_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Anatomy of an intelligent application powered by an LLM'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above architecture, we depict the anatomy of an intelligent application
    with the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: It has a natural language interface (it might be text- or voice-based).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is powered by an LLM that acts as the “brain” of the app.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a knowledge base that the model can query, typically with **retrieval
    augmented generation** (**RAG**) techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has a set of tools or plugins that it can use to interact with the external
    environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This new paradigm of software development brings a set of new application components
    that are typical of AI-powered applications. Let’s explore these new components
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: New application components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main shift in terms of AI development refers to the way we work with models:
    from producing models, now the trend is consuming models that, as we mentioned
    several times, are nothing but APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This shift leads to a series of new software components (or adjustments of
    existing components) in the landscape of development:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Models**: The model is simply the type of LLM we decide to embed in our application.
    There are two main categories of models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Proprietary LLMs**: Models that are owned by specific companies or organizations.
    Examples include GPT-4o, developed by OpenAI, or Gemini, developed by Google.
    As their source code and architecture are not available, those models cannot be
    re-trained from scratch on custom data, but they can be fine-tuned if needed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open-source**: Models with code and architecture freely available and distributed,
    hence they can also be trained from scratch on custom data. Examples include Falcon
    LLM, developed by Abu Dhabi’s **Technology Innovation Institute** (**TII**), and
    Llama, developed by Meta.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System message**: This is the set of instructions that we provide the model
    with, and that influence the style and behavior of our AI app. There are many
    features that we can shape directly within the meta-prompt, including:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing hallucination by specifying that the model only refers to the provided
    knowledge base (this process is called “grounding”)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing responsible AI practices by specifying, for example, not to respond
    to malicious queries or not to generate potentially harmful responses
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructing the model to always ask an additional question to consolidate the
    context before answering
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory and VectorDB**: When we talk about memory in the context of AI apps,
    we need to differentiate between two types of memory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Short-term memory**: This is the capability of the app to keep the interactions
    between the user and LLMs in a context window. It means that each message feeds
    the existing meta-prompt of the model, without the user repeating something already
    mentioned.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-term memory**: This type of memory refers to the external knowledge
    base we provide the model with using embeddings. When this is the case, we typically
    leverage VectorDBs, a new type of database (or new feature of an existing database)
    that stores the numerical representations of the provided documents.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: A VectorDB is a type of database that stores and retrieves information based
    on vectorized embeddings, the numerical representations that capture the meaning
    and context of text. By using a VectorDB, you can perform semantic search and
    retrieval based on the similarity of meanings rather than keywords. Some examples
    of a VectorDB are Chroma, FAISS, Elasticsearch, Milvus, Pinecone, Qdrant, and
    Weaviate.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Tools/plugins**: These can be seen as additional modules or components that
    can be integrated into the LLM to extend its functionality or adapt it to specific
    tasks and applications. These plugins act as add-ons, enhancing the capabilities
    of the LLM beyond its core language generation or comprehension abilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea behind plugins is to make LLMs more versatile and adaptable, allowing
    developers and users to customize the behavior of the language model for their
    specific needs. Plugins can be created to perform various tasks, and they can
    be seamlessly incorporated into the LLM’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an illustration of the main components of an LLM-powered application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer program  Description automatically generated](img/B31559_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: High-level architecture of LLM-powered applications'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the picture above, the core of the high-level architecture
    is the **AI orchestrator**. With the AI orchestrator, we refer to lightweight
    libraries that make it easier to embed and orchestrate LLMs within applications.
  prefs: []
  type: TYPE_NORMAL
- en: AI orchestrators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since LLMs went viral toward the end of 2022, many libraries have begun to
    arise in the market. In the next sections, we are going to focus on three of them:
    LangChain, Semantic Kernel, and Haystack.'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**LangChain** was launched as an open-source project by Harrison Chase, in
    October 2022\. It can be used in both Python and JS/TS.'
  prefs: []
  type: TYPE_NORMAL
- en: LangChain is a framework for developing applications powered by language models,
    making them data-aware (with grounding) and agentic – meaning able to interact
    with external environments.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain provides modular abstractions for the components necessary to work
    with language models that we previously mentioned, such as prompts, memory, and
    plugins. Alongside those components, LangChain also offers pre-built **chains**,
    which are structured concatenations of components. These chains can be pre-built
    for specific use cases or be customized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, LangChain has the following core modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Models**: These are the LLMs or large foundation models that will be the
    engine of the application. LangChain supports proprietary models, such as those
    available in OpenAI and Azure OpenAI, and open-source models consumable from the
    **Hugging Face Hub**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Hugging Face is a company and a community that builds and shares state-of-the-art
    models and tools for **natural language processing** (**NLP**) and other machine
    learning domains. It developed the Hugging Face Hub, a platform where people can
    create, discover, and collaborate on machine learning models and LLMs, datasets,
    and demos. The Hugging Face Hub hosts over 120k models, 20k datasets, and 50k
    demos in various domains and tasks, such as audio, vision, and language.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Alongside models, LangChain also offers many prompt-related components that
    make it easier to manage the prompt flow.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data connections**: These refer to the building blocks needed to retrieve
    the additional non-parametric knowledge we want to provide the model with. Examples
    of data connections are document loaders or text embedding models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory**: It allows the application to keep references to the user’s interactions,
    in both the short and long term. It is typically based on vectorized embeddings
    stored in a VectorDB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chains**: These are predetermined sequences of actions and calls to LLMs
    that make it easier to build complex applications that require chaining LLMs with
    each other or with other components. An example of a chain might be: take the
    user query, chunk it into smaller pieces, embed those chunks, search for similar
    embeddings in a VectorDB, use the top three most similar chunks in the VectorDB
    as context to provide the answer, generate the answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agents**: Agents are entities that drive decision-making within LLM-powered
    applications. They have access to a suite of tools and can decide which tool to
    call based on the user input and the context. Agents are dynamic and adaptive,
    meaning that they can change or adjust their actions based on the situation or
    the goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haystack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Haystack** is a Python-based framework developed by *deepset*, a startup
    founded in 2018 in Berlin by Milos Rusic, Malte Pietsch, and Timo Möller. deepset
    provides developers with the tools to build an NLP-based application, and with
    the introduction of Haystack, they are taking it to the next level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Haystack has the following core components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Nodes**: These are components that perform a specific task or function, such
    as a retriever, a reader, a generator, a summarizer, etc. Nodes can be LLMs or
    other utilities that interact with LLMs or other resources. Among LLMs, Haystack
    supports proprietary models, such as those available in OpenAI and Azure OpenAI,
    and open-source models consumable from the Hugging Face Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipelines**: These are sequences of calls to nodes that perform natural language
    tasks or interact with other resources. Pipelines can be querying pipelines or
    indexing pipelines, depending on whether they perform searches on a set of documents
    or prepare documents for search. Pipelines are predetermined and hardcoded, meaning
    that they do not change or adapt based on the user input or the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agent**: This is an entity that uses LLMs to generate accurate responses
    to complex queries. An agent has access to a set of tools, which can be pipelines
    or nodes, and it can decide which tool to call based on the user input and the
    context. An agent is dynamic and adaptive, meaning that it can change or adjust
    its actions based on the situation or the goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tools**: There are functions that an agent can call to perform natural language
    tasks or interact with other resources. Tools can be pipelines or nodes that are
    available to the agent and they can be grouped into toolkits, which are sets of
    tools that can accomplish specific objectives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DocumentStores**: These are backends that store and retrieve documents for
    search. DocumentStores can be based on different technologies, including VectorDBs
    (such as FAISS, Milvus, or Elasticsearch).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haystack is renowned for its simplicity and ease of use, featuring a modular
    architecture that allows developers to construct customizable pipelines for tasks
    like semantic search and question-answering. This design makes it particularly
    suitable for **RAG** applications, where efficient data retrieval is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Kernel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Semantic Kernel** is the third open-source SDK we are going to explore in
    this chapter. It was developed by Microsoft, originally in C#, and is now also
    available in Python.'
  prefs: []
  type: TYPE_NORMAL
- en: This framework takes its name from the concept of a “kernel,” which, generally
    speaking, refers to the core or essence of a system. In the context of this framework,
    a kernel is meant to act as the engine that addresses users’ input by chaining
    and concatenating a series of components into pipelines, encouraging **function
    composition.**
  prefs: []
  type: TYPE_NORMAL
- en: '**Definition**'
  prefs: []
  type: TYPE_NORMAL
- en: In mathematics, function composition is a way to combine two functions to create
    a new function. The idea is to use the output of one function as the input to
    another function, forming a chain of functions. The composition of two functions,
    f and g, is denoted as![](img/B31559_10_001.png), where the function ![](img/B31559_10_001.png)
    is applied first, followed by the function ![](img/B31559_10_002.png).
  prefs: []
  type: TYPE_NORMAL
- en: Function composition in computer science is a powerful concept that allows for
    the creation of more sophisticated and reusable code by combining smaller functions
    into larger ones. It enhances modularity and code organization, making programs
    easier to read and maintain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Kernel has the following main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Models**: These are the LLMs or large foundation models that will be the
    engine of the application. Semantic Kernel supports proprietary models, such as
    those available in OpenAI and Azure OpenAI, and open-source models consumable
    from the Hugging Face Hub.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory**: This allows the application to keep references to the user’s interactions,
    in both the short and long term. Within the framework of Semantic Kernel, memories
    can be accessed in three ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key**-**value pairs**: This consists of saving environment variables that
    store simple information, such as names or dates.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local storage**: This consists of saving information to a file that can be
    retrieved by its filename, such as a CSV or JSON file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic memory search**: This is similar to LangChain’s and Haystack’s memory,
    as it uses embeddings to represent and search for text information based on its
    meaning.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functions**: Functions can be seen as skills that mix LLM prompts and code,
    with the goal of making users’ requests interpretable and actionable. There are
    two types of functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic functions**: These are basically a templated prompt, which is a
    natural language query that specifies the input and output format for the LLM,
    also incorporating prompt configuration, which sets the parameters for the LLM.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Native functions**: These refer to the native computer code that can route
    the intent captured by the semantic function and perform the related task.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To give an example, a semantic function could ask the LLM to write a short paragraph
    about AI, while a native function could actually post it on social media like
    LinkedIn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Plugins**: These are connectors toward external sources or systems that are
    meant to provide additional information or the ability to perform autonomous actions.
    Semantic Kernel offers out-of-the-box plugins, such as the Microsoft Graph connector
    kit, but you can build a custom plugin by leveraging functions (both native and
    semantic, or a mix of the two).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Planner**: As LLMs can be seen as reasoning engines, they can also be leveraged
    to auto-create chains or pipelines to address new users’ needs. This goal is achieved
    with a planner, which is a function that takes as input a user’s task and produces
    the set of actions, plugins, and functions needed to achieve the goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Below is an illustration of the anatomy of Semantic Kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Technical perspective of what''s happening](img/B31559_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Anatomy of Semantic Kernel. Source: https://learn.microsoft.com/en-us/semantic-kernel/overview/'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the three frameworks offer more or less similar core components, sometimes
    called a different taxonomy, yet covering all the blocks illustrated within the
    concept of the copilot system. So, a natural question might be, “Which one should
    I use to build my LLM-powered application?”
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are some criteria you might want to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The programming language you are comfortable with or prefer to use**. Different
    frameworks may support different programming languages or have different levels
    of compatibility or integration with them. For example, Semantic Kernel supports
    C#, Python, and Java, while LangChain and Haystack are mainly based on Python
    (even though LangChain also introduced JS/TS support). You may want to choose
    a framework that matches your existing skills or preferences, or that allows you
    to use the language that is most suitable for your application domain or environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The type and complexity of the natural language tasks you want to perform
    or support**. Different frameworks may have different capabilities or features
    for handling various natural language tasks, such as summarization, generation,
    translation, reasoning, etc. For example, LangChain and Haystack provide utilities
    and components for orchestrating and executing natural language tasks, while Semantic
    Kernel allows you to use natural language semantic functions to invoke LLMs and
    services. You may want to choose a framework that offers the functionality and
    flexibility you need or want for your application goals or scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The level of customization and control you need or want over the LLMs and
    their parameters or options**. Different frameworks may have different ways of
    accessing, configuring, and fine-tuning the LLMs and their parameters or options,
    such as model selection, prompt design, inference speed, output format, etc. For
    example, Semantic Kernel provides connectors that make it easy to add memories
    and models to your AI app, while LangChain and Haystack allow you to plug in different
    components for the DocumentStore, retriever, reader, generator, summarizer, and
    evaluator. You may want to choose a framework that gives you the level of customization
    and control you need or want over the LLMs and their parameters or options.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The availability and quality of the documentation, tutorials, examples, and
    community support for the framework**. Different frameworks may have different
    levels of documentation, tutorials, examples, and community support that can help
    you learn, use, and troubleshoot the framework. For example, Semantic Kernel has
    a website with documentation, tutorials, examples, and a Discord community; LangChain
    has a GitHub repository with documentation, examples, and issues; Haystack has
    a website with documentation, tutorials, demos, blog posts, and a Slack community.
    You may want to choose a framework that has the availability and quality of documentation,
    tutorials, examples, and community support that can help you get started and solve
    problems with the framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, there is no right or wrong answer! All three orchestrators discussed above
    are extremely valid. However, some features might be more relevant to specific
    use cases or developers’ preferences. Make your choice based on that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Introducing the public cloud: Azure OpenAI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2016, OpenAI agreed to leverage Microsoft’s Azure cloud infrastructure to
    run its AI experiments, which led, in 2019, to a $1 billion investment from the
    tech giant [into Sam Altman’s company (https://new](https://news.microsoft.com/2019/07/22/openai-forms-exclusive-computing-partnership-with-microsoft-to-build-new-azure-ai-supercomputing-technologies/)upercomputing-technologies/).
  prefs: []
  type: TYPE_NORMAL
- en: 'This marked the beginning of a strategic partnership between the two companies,
    aiming at developing AI models and technologies that can be used for the benefit
    of humanity. This partnership is based on the following three main pillars:'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft and OpenAI will jointly build new Azure supercomputing infrastructure
    to train AI models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI will make its models and technologies consumable from the Azure cloud.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microsoft will become OpenAI’s preferred partner for commercializing new AI
    solutions to the market.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since then, the two companies kept investing and researching, and finally,
    in January 2023, OpenAI models were made available on Microsoft Azure as a managed
    service: **Azure OpenAI Service** (in short, **AOAI**).'
  prefs: []
  type: TYPE_NORMAL
- en: With the general availability of the AOAI Service, a new milestone was reached,
    and the Microsoft AI portfolio has been extended with the powerful LLMs of OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: AOAI Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The AOAI Service is a product of Microsoft that provides both a playground
    and APIs to interact and consume all of OpenAI’s powerful language models. It
    is important to highlight that the models are exactly the same: the only difference
    is that, if you are consuming them via AOAI, you are leveraging your own Azure
    subscription and automatically inheriting all the enterprise features that are
    typical of the Microsoft public cloud, including security, role-based access control,
    data privacy, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create your AOAI resource, follow these instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: Navig[ate to your Azure portal at](https://ms.portal.azure.com/) https://ms.portal.azure.com.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Click on **Create a Resource**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type *azure openai* and click on **Create**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fill in the required information and click on **Review + create**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Steps to create an AOAI resource'
  prefs: []
  type: TYPE_NORMAL
- en: 'This process might take a few minutes. Once it is ready, you can directly jump
    to its user-friendly interface, the AOAI Studio, to test your models before deploying
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: AOAI Studio and chat playground'
  prefs: []
  type: TYPE_NORMAL
- en: To use AOAI models, you have to initiate a deployment, which is a serverless
    compute instance you can attach to a model.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31559_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Creating a new AOAI deployment via the Azure OpenAI portal'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, exactly like we did for OpenAI models’ APIs in the previous section,
    from the AOAI Studio, you can consume your deployed models via APIs. For a quick
    start, you can navigate to the **Chat playground** and click on the **View Code**
    button. A script will be ready to be copied and pasted into your favorite programming
    IDE, along with the secret keys needed to access the resource:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31559_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Consuming deployed models via APIs'
  prefs: []
  type: TYPE_NORMAL
- en: By doing so, you can seamlessly incorporate your Azure OpenAI’s LLMs within
    your own application.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we had an overview of how GenAI is disrupting
    industries, from increasing the efficiency of internal processes to enhancing
    customers’ journeys with personalized experiences. Many of these applications
    can be achieved through a high margin of customization, and pre-built, consumer-facing
    applications like ChatGPT might not be enough.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why we introduced OpenAI models’ APIs. With the models’ APIs, you can
    leverage the power of the model behind ChatGPT within your own application, tailored
    to your own industry and scenarios. Developing AI-powered applications, however,
    requires a new set of components that have also marked a new paradigm in software
    development.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we saw how, from 2023, OpenAI models (both in Playground and via APIs)
    have been made available through Microsoft Azure as a managed service: Azure OpenAI.
    This has paved the way for a new wave of adoption from large enterprises that
    benefit from all the security and governance layers that already exist within
    the public cloud (which is, by design, enterprise-ready).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will provide a recap of everything covered in this book,
    including the latest announcements and releases that have occurred. We will also
    focus on reflections and final thoughts about the exponential growth of generative
    AI technologies in just a few months and what to expect in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI Forms Exclusive Computing Partnership with Microsoft to Build New Azure
    AI Supercomputing Technologies: https://news.microsoft.com/20[19/07/22/openai-forms-exclusive-computing-partnership-with-](https://news.microsoft.com/2019/07/22/openai-forms-exclusive-computing-partnership-with-microsoft-to-build-new-azure-ai-supercomputing-technologies/)-ai-supercomputing-technologies/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'General Availability of Azure OpenAI Service Expands Access to Large, Advanced
    AI Models with Added Enterprise Benefits: https://azure.microsoft.com/[en-us/blog/general-availability-of-azure-openai-service-expa](https://azure.microsoft.com/en-us/blog/general-availability-of-azure-openai-service-expands-access-to-large-advanced-ai-models-with-added-enterprise-benefits/)-with-added-enterprise-benefits/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft CEO Satya Nadella: Humans and A.I. Can Work Together to Solve Society’s
    Challenges: https://slate.com/technology/2016/06/microsoft-ceo-satya-nadella-humans-and-a-i-can-work-together-to-solve-societys-challenges.html'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft Calls for Government Regulation of Facial Recognition Technology:
    https://www.geekwire.com/[2018/microsoft-calls-government-regulatio](https://www.geekwire.com/2018/microsoft-calls-government-regulation-facial-recognition-technology/)n-facial-recognition-technology/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Six Principles to Guide Micros[oft’s Facial Recognition Work: https://blogs.microso](https://blogs.microsoft.com/on-the-issues/2018/12/17/six-principles-to-guide-microsofts-facial-recognition-work/)rosofts-facial-recognition-work/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Responsible AI Principles and Approach: https://www.microsoft.com/en-us/ai/principles-and-approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mic[rosoft Responsible AI Toolbox:](https://responsibleaitoolbox.ai/) https://responsibleaitoolbox.ai/
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Human Parity on CommonsenseQA: Augmenting Self-Attention with External Attention:
    https://www.microsoft.com/e[n-us/research/publication/human-parity-on-commonsenseqa-augmenti](https://www.microsoft.com/en-us/research/publication/human-parity-on-commonsenseqa-augmenting-self-attention-with-external-attention/)tention-with-external-attention/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Customize a Model with Azure OpenAI Service: https://learn.microsoft.com/en-gb/[azure/cognitive-services/openai/how-to/fine-tuning?pivots=pro](https://learn.microsoft.com/en-gb/azure/cognitive-services/openai/how-to/fine-tuning?pivots=programming-language-studio&openai-cli-data-preparation-tool)openai-cli-data-preparation-tool'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moody’s and Microsoft Develop Enhanced Risk, Data, Analytics, Research and
    Collaboration Solut[ions Powered by Generative AI: https://ir.moodys.com/press-releases/news-details/2023/Moodys-and-Microsoft-Develop-Enhanced-Risk-Data-Analytics-Research-and-Collaboration-Solutions-Power](https://ir.moodys.com/press-releases/news-details/2023/Moodys-and-Microsoft-Develop-Enhanced-Risk-Data-Analytics-Research-and-Collaboration-Solutions-Powered-by-Generative-AI/default.aspx)ed-by-Generative-AI/default.aspx'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Increasing Accu[racy of Pediatric Visit Notes: https:/](https://openai.com/index/summer-health/)/openai.com/index/summer-health/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coca-Cola Invites Digital Artists to ‘Create Real [Magic’ Using New AI Platform:
    https://www.coca-colacompany.com/media-center/coca-cola-invites-digital-artists-to-create-](https://www.coca-colacompany.com/media-center/coca-cola-invites-digital-artists-to-create-real-magic-using-new-ai-platform)real-magic-using-new-ai-platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IVECO Group Uses Azure OpenAI Servi[ce to Transform Manufacturing: https://customers.microsoft.com/en-us/story/1706380538888475836-iveco-group-azure-op](https://customers.microsoft.com/en-us/story/1706380538888475836-iveco-group-azure-openai-service-manufacturing-italy)enai-service-manufacturing-italy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ir[onclad and OpenAI Partnership: ht](https://openai.com/index/ironclad/)tps://openai.com/index/ironclad/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inworl[d AI and OpenAI Collaboration: http](https://openai.com/index/inworld-ai/)s://openai.com/index/inworld-ai/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan Academy and OpenAI Partnership: https://openai.com/index/khan-academy/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our communities on Discord and Reddit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have questions about the book or want to contribute to discussions on Generative
    AI and LLMs? Join our Discord server at [https://packt.link/I1tSU](Chapter_10.xhtml)
    and our Reddit channel at [https://packt.link/jwAmA](Chapter_10.xhtml) to connect,
    share, and collaborate with like-minded enthusiasts.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Discord.png) ![](img/QR_Code757615820155951000.png)'
  prefs: []
  type: TYPE_IMG
