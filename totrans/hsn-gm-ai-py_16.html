<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">3D Worlds</h1>
                </header>
            
            <article>
                
<p>We are almost nearing the end of our journey into what <strong>artificial general intelligence</strong> (<strong>AGI</strong>) is and how <strong>deep reinforcement learning</strong> (<strong>DRL</strong>) can be used to help us get there. While it is still questionable whether DRL is indeed the right path to AGI, it is what appears to be our current best option. However, the reason we are questioning DRL is because of its ability or inability to master diverse 3D spaces or worlds, the same 3D spaces we humans and all animals have mastered but something we find very difficult to train RL agents on. In fact, it is the belief of many an AGI researcher that solving the 3D state-space problem could go a long way to solving true general artificial intelligence. We will look at why that is the case in this chapter.</p>
<p>For this chapter, we are going to look at why 3D worlds pose such a unique problem to DRL agents and the ways we can train them to interpret state. We will look at how typical 3D agents use vision to interpret state and we will look to the type of deep learning networks derived from that. Then we look to a practical example of using 3D vision in an environment and what options we have for processing state. Next, sticking with Unity, we look at the Obstacle Tower Challenge, an AI challenge with a $100,000 prize, and what implementation was used to win the prize. Moving to the end of the chapter, we will look at another 3D environment called Habitat and how it can be used for developing agents.</p>
<p>Here is a summary of the main points we will discuss this chapter:</p>
<ul>
<li>Reasoning on 3D worlds</li>
<li>Training a visual agent</li>
<li>Generalizing 3D vision</li>
<li>Challenging the Unity Obstacle Tower Challenge</li>
<li>Exploring habitat—embodied agents by FAIR</li>
</ul>
<p>The examples in this chapter can take an especially long time to train, so please either be patient or perhaps just choose to do one. This not only saves you time but reduces energy consumption. In the next chapter, we explore why 3D worlds are so special.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reasoning on 3D worlds</h1>
                </header>
            
            <article>
                
<p>So, why are 3D worlds so important, or are at least believed to be so? Well, it all has to come down to state interpretation, or what we in DRL like to call state representation. A lot of work is being done on better representation of state for RL and other problems. The theory is that being able to represent just key or converged points of state allow us to simplify the problem dramatically. We have looked at doing just that using various techniques over several chapters. Recall how we discretized the state representation of a continuous observation space into a discrete space using a grid mesh. This technique is how we solved more difficult continuous space problems with the tools we had at the time. Over the course of several chapters since then, we saw how we could input that continuous space directly into our deep learning network. That included the ability to directly input an image as the game state, a screenshot, using convolutional neural networks. However, 3D worlds, ones that represent the real world, pose a unique challenge to representing state.</p>
<p>So what is the difficulty in representing the state space in a 3D environment? Could we not just give the agent sensors, as we did in other environments? Well, yes and no. The problem is that giving the agent sensors is putting our bias on what the agent needs to use in order to interpret the problem. For example, we could give the agent a sensor that told it the distance of an object directly in front of it, as well as to its left and right. While that would likely be enough information for any driving agent, would it work for an agent that needed to climb stairs? Not likely. Instead, we would likely need to give the height of the stairs as another sensor input, which means our preferred method of introducing state to an agent for a 3D world is using vision or an image of the environment. The reason for this, of course, is to remove any bias on our part (us humans) and we can best do that by just feeding the environment state as an image directly to the agent.</p>
<p>We have already seen how we could input game state using an image of the playing area when we looked at playing Atari games. However, those game environments were all 2D, meaning the state space was essentially flattened or converged. The word <strong>converged</strong> works here because this becomes the problem when tackling 3D environments and the real world. In 3D space, one vantage point could potentially yield to multiple states, and likewise, multiple state spaces can be observed by one single vantage point. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can see how this works in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-872 image-border" src="assets/3615c006-7587-4155-9acf-ad804d03949f.png" style="width:47.00em;height:35.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Examples of agent state in a 3D world</div>
<p>In the diagram, we can see the agent, the blue dot in the center of Visual Hallway environment in Unity with the ML-Agents toolkit. We will review an example of this environment shortly, so don't worry about reviewing it just yet. You can see from the diagram how the agent is observing different observations of state from the same physical position using an agent camera. An agent camera is the vision we give to the agent to observe the world.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>From this camera, the agent ingests the state as a visual observation that is fed as an image into a deep learning network. This image is broken up with 2D convolutional neural network layers into features, which the agent learns. The problem is that we are using 2D filters to try and digest 3D information. In <a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 7</a>, <em>Going Deeper with DDQN</em>, we explored using CNNs to ingest the image state from Atari games and, as we have seen, this works very well.</p>
<div class="packt_tip">You will need the ML-Agents toolkit installed and should have opened the <strong>UnitySDK</strong> test project. If you need assistance with this, return to <a href="ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml">Chapter 11</a>, <em>Exploiting ML-Agents, </em>and follow some of the exercises there first.</div>
<p><span>Unity does the same thing for its agent camera setup, and in the next exercise we will see how the following looks:</span></p>
<ol>
<li>Locate the folder at <kbd>ml-agents/ml-agents/mlagents/trainers</kbd> located in the ML-Agents repository. If you need help pulling the repository, follow the previous information tip given.</li>
<li>From this folder, locate and open the <kbd>models.py</kbd> file in a text or Python IDE. ML-Agents is written in TensorFlow, which may be intimidating at first, but the code follows many of the same principles as PyTorch.</li>
<li>Around line 250 a <kbd>create_visual_observation_encoder</kbd> function from the <kbd>LearningModel</kbd> base class is created. This is the base class model that ML-Agents, the PPO, and SAC implementations use.</li>
</ol>
<div class="packt_infobox">ML-Agents was originally developed in Keras and then matured to TensorFlow in order to improve performance. Since that time, PyTorch has seen a huge surge in popularity for academic researchers, as well as builders. At the time of writing, PyTorch is the fastest growing DL framework. It remains to be seen if Unity will also follow suit and convert the code to PyTorch, or just upgrade it to TensorFlow 2.0.</div>
<ol start="4">
<li>The <kbd>create_visual_observation_encoder</kbd> function is the base function for encoding state, and the full definition of the function (minus comments) is shown here:</li>
</ol>
<pre style="padding-left: 60px">def create_visual_observation_encoder(<br/>        self,<br/>        image_input: tf.Tensor,<br/>        h_size: int,<br/>        activation: ActivationFunction,<br/>        num_layers: int,<br/>        scope: str,<br/>        reuse: bool,<br/>    ) -&gt; tf.Tensor:        <br/>        with tf.variable_scope(scope):<br/>            conv1 = tf.layers.conv2d(<br/>                image_input,<br/>                16,<br/>                kernel_size=[8, 8],<br/>                strides=[4, 4],<br/>                activation=tf.nn.elu,<br/>                reuse=reuse,<br/>                name="conv_1",<br/>            )<br/>            conv2 = tf.layers.conv2d(<br/>                conv1,<br/>                32,<br/>                kernel_size=[4, 4],<br/>                strides=[2, 2],<br/>                activation=tf.nn.elu,<br/>                reuse=reuse,<br/>                name="conv_2",<br/>            )<br/>            hidden = c_layers.flatten(conv2)<br/><br/>        with tf.variable_scope(scope + "/" + "flat_encoding"):<br/>            hidden_flat = self.create_vector_observation_encoder(<br/>                hidden, h_size, activation, num_layers, scope, reuse<br/>            )<br/>        return hidden_flat</pre>
<ol start="5">
<li>While the code is in TensorFlow, there are a few obvious indicators of common terms, such as layers and conv2d. With that information, you can see that this encoder uses two CNN layers: one with a kernel size of 8 x 8, a stride of 4 x 4, and 16 filters; followed by a second layer that uses a kernel size of 4 x 4, a stride of 2 x 2, and 32 filters.  </li>
</ol>
<div class="packt_tip">Notice again the use of no pooling layers. This is because spatial information is lost when we use pooling between CNN layers. However, depending on the depth of the network, a single pooling layer near the top can be beneficial.   </div>
<p class="mce-root"/>
<ol start="6">
<li>Notice the return from the function is a hidden flat layer denoted by <kbd>hidden_flat</kbd>. Recall that our CNN layers are being used to learn state that is then fed into our learning network as the following diagram shows:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-669 image-border" src="assets/f158292d-be01-4052-a899-7c7353056078.png" style="width:35.00em;height:24.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example network diagram</div>
<ol start="7">
<li>The preceding diagram is a simplified network diagram showing that the CNN layers flatten as they feed into the hidden middle layer. Flattening is converting that convolutional 2D data into a one-dimensional vector and then feeding that into the rest of the network.</li>
<li>We can see how the image source is defined by opening up the Unity editor to the <strong>ML-Agents UnitySDK</strong> project to the <strong>VisualHallway</strong> scene located in the <kbd>Assets/ML-Agents/Examples/Hallway/Scenes</kbd> folder.</li>
<li>Expand the first <strong>VisualSymbolFinderArea</strong> and select the <strong>Agent</strong> object in the <strong>Hierarchy</strong> window. Then, in the <strong>Inspector</strong> window, locate and double-click on the <strong>Brain</strong> to bring it up in the following window:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-670 image-border" src="assets/e8ab9385-dd56-41f1-9f1c-f8e16e985c52.png" style="width:38.00em;height:34.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Inspecting the VisualHallwayLearning Brain</span></div>
<p>The important thing to note here is that the agent is set up to accept an image of size 84 x 84 pixels. That means the agent camera is sampled down to an image size matching the same pixel area. A relatively small pixel area for this environment works because of the lack of detail in the scene. If the detail increased, we would likely also need to increase the resolution of the input image. </p>
<p>In the next section, we look at training the agent visually using the ML-Agents toolkit.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a visual agent</h1>
                </header>
            
            <article>
                
<p>Unity develops a 2D and 3D gaming engine/platform that has become the most popular platform for building games. Most of these games are the 3D variety, hence the specialized interest by Unity in mastering the task of agents that can tackle more 3D natural worlds. It naturally follows then that Unity has invested substantially into this problem and has/is working with DeepMind to develop this further. How this collaboration turns out remains to be seen, but one thing is for certain is that Unity will be our go-to platform for exploring 3D agent training.</p>
<p>In the next exercise, we are going to jump back into Unity and look at how we can train an agent in a visual 3D environment. Unity is arguably the best place to set up and build these type of environments as we have seen in the earlier chapters. Open the Unity editor and follow these steps:</p>
<ol>
<li>Open the <strong>VisualHallway</strong> scene located in the <kbd>Assets/ML-Agents/Examples/Hallway/Scenes</kbd> folder.</li>
<li>Locate the <strong>Academy</strong> object in the scene hierarchy window and set the <strong>Control</strong> option to enabled on the <strong>Hallway Academy</strong> component <strong>Brains</strong> section and as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-873 image-border" src="assets/8d787f4a-007b-4d0f-9960-e166f925ad35.png" style="width:31.42em;height:24.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Setting the academy to control the learning brain</div>
<ol start="3">
<li>This sets the Academy to control the Brain for the agent.  </li>
<li>Next, select all the <strong>VisualSymbolFinderArea</strong> objects from <strong>(1)</strong> to <strong>(7)</strong> and then make sure to enable them all by clicking the object's <strong>Active</strong> option in the Inspector window, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-672 image-border" src="assets/e657e8c6-2c23-473a-94c7-2d98d97914e8.png" style="width:66.58em;height:22.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Enabling all the sub-environments in the scene</div>
<ol start="5">
<li>This enables all the sub environment areas and allows us to run an additional seven agents when training. As we have seen when using actor-critic methods, being able to sample more efficiently from the environment has many advantages. Almost all the example ML-Agents environments provide for multiple sub-training environments. These multiple environments are considered separate environments but allow for the brain to be trained synchronously with multiple agents.</li>
<li>Save the scene and the project file from the <strong>File</strong> menu.</li>
<li>Open a new Python or Anaconda shell and set the virtual environment to use the one you set up earlier for ML-Agents. If you need help, refer to <a href="ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml">Chapter 11</a>, <em>Exploiting ML-Agents</em>.</li>
<li>Navigate to the Unity <kbd>ml-agents</kbd> folder and execute the following command to start training:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=vishall_1 --train</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="9">
<li>This will start the Python trainer, and after a few seconds, will prompt you to click Play in the editor. After you do that, the agents in all the environments will begin training and you will be able to visualize this in the editor. An example of how this looks in the command shell is shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-673 image-border" src="assets/48f2e1f1-d14a-4bc4-a4b5-7ec0a648f834.png" style="width:101.58em;height:53.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Running the ML-Agents trainer</div>
<p>Now that we have reviewed how to train an agent in Unity with ML-Agents, we can move on to explore some other undocumented training options in the next section.</p>
<div class="packt_tip">If you encounter problems training the Hallway environment, you can always try one of the other various environments. It is not uncommon for a few of the environments to become broken because of releases or version conflicts.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generalizing 3D vision</h1>
                </header>
            
            <article>
                
<p>As previously mentioned in <a href="ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml">Chapter 11</a>, <em>Exploiting ML-Agents</em>, we saw how the team at Unity is one of the leaders in training agents for 3D worlds. After all, they do have a strong vested interest in providing an AI platform that developers can just plug into and build intelligent agents. Except, the very agents that fit this broad type of application are now considered the first step to AGI because if Unity can successfully build a universal agent to play any game, it will have effectively built a first-level AGI.</p>
<p>The problem with defining AGI is trying to understand how broad or general an intelligence has to be as well as how we quantify the agent's understanding of that environment and possible ability to transfer knowledge to other tasks. We really won't know how best to define what that is until someone has the confidence to stand up and claim to have developed an AGI. A big part of that claim will depend on how well an agent can generalize environmental state and a big part of that will be generalizing 3D vision itself.</p>
<p>Unity has an undocumented way to alter the type of visual encoder you can use in training on an environment <span>(at least at time of writing)</span>. </p>
<p>In the next exercise, we look at how the hyperparameter can be added to the configuration and set for different visual encoders by following these steps:</p>
<ol>
<li>Locate the <kbd>trainer_config.yaml</kbd> configuration file located in the <kbd>mlagents/ml-agents/config</kbd> folder and open it <span>in an </span>IDE or text editor.</li>
</ol>
<div class="packt_infobox"><strong>YAML</strong> is an acronym that stands for for <strong>YAML ain't markup language</strong>. The format of the ML-Agents configuration markup files is quite similar to Windows INI configuration files of old.</div>
<ol start="2">
<li>This file defines the configuration for the various learning brains. Locate the section for the <kbd>VisualHallwayLearning</kbd> brain as shown here:</li>
</ol>
<pre style="padding-left: 60px">VisualHallwayLearning:<br/>    use_recurrent: true<br/>    sequence_length: 64<br/>    num_layers: 1<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64</pre>
<ol start="3">
<li>These hyperparameters are additional to a set of base values set in a default brain configuration at the top of the config file and shown as follows:</li>
</ol>
<pre style="padding-left: 60px">idefault:<br/>    trainer: ppo<br/>    batch_size: 1024<br/>    beta: 5.0e-3<br/>    buffer_size: 10240<br/>    epsilon: 0.2<br/>    hidden_units: 128<br/>    lambd: 0.95<br/>    learning_rate: 3.0e-4<br/>    learning_rate_schedule: linear<br/>    max_steps: 5.0e4<br/>    memory_size: 256<br/>    normalize: false<br/>    num_epoch: 3<br/>    num_layers: 2<br/>    time_horizon: 64<br/>    sequence_length: 64<br/>    summary_freq: 1000<br/>    use_recurrent: false<br/>    <strong>vis_encode_type: simple</strong><br/>    reward_signals:<br/>        extrinsic:<br/>            strength: 1.0<br/>            gamma: 0.99</pre>
<ol start="4">
<li>The hyperparameter of interest for us is the <kbd>vis_encode_type</kbd> value set to simple highlighted in the preceding code example. ML-Agents supports two additional types of visual encoding by changing that option like so:</li>
</ol>
<ul>
<li><kbd>vis_enc_type</kbd>: Hyperparameter to set type of visual encoding:</li>
<li style="padding-left: 60px"><kbd>simple</kbd>: This is the default and the version we already looked at.</li>
<li style="padding-left: 60px"><kbd>nature_cnn</kbd>: This defines a CNN architecture proposed by a paper in Nature, we will look at this closer shortly.</li>
<li style="padding-left: 60px"><kbd>resnet</kbd>: ResNet is a published CNN architecture that has been shown to be very effective at image classification.</li>
</ul>
<ol start="5">
<li>We will change the default value in our <kbd>VisualHallwayLearning</kbd> brain by adding a new line to the end of the brain's configuration:</li>
</ol>
<pre style="padding-left: 60px">VisualHallwayLearning:<br/>    use_recurrent: true<br/>    sequence_length: 64<br/>    num_layers: 1<br/>    hidden_units: 128<br/>    memory_size: 256<br/>    beta: 1.0e-2<br/>    num_epoch: 3<br/>    buffer_size: 1024<br/>    batch_size: 64<br/>    max_steps: 5.0e5<br/>    summary_freq: 1000<br/>    time_horizon: 64<br/>    <strong>vis_enc_type: nature_cnn --or-- resnet</strong></pre>
<ol start="6">
<li>Now that we know how to set these, let's see what they look like by opening the <kbd>models.py</kbd> code like we did earlier from the <kbd>ml-agents/trainers</kbd> folder. Scroll down past the <kbd>create_visual_observation_encoder</kbd> function to the <kbd>create_nature_cnn_observation_encoder</kbd> function shown here:</li>
</ol>
<pre style="padding-left: 60px">def create_nature_cnn_visual_observation_encoder(<br/>        self,<br/>        image_input: tf.Tensor,<br/>        h_size: int,<br/>        activation: ActivationFunction,<br/>        num_layers: int,<br/>        scope: str,<br/>        reuse: bool,<br/>    ) -&gt; tf.Tensor:        <br/>        with tf.variable_scope(scope):<br/>            conv1 = tf.layers.conv2d(<br/>                image_input,<br/>                32,<br/>                kernel_size=[8, 8],<br/>                strides=[4, 4],<br/>                activation=tf.nn.elu,<br/>                reuse=reuse,<br/>                name="conv_1",<br/>            )<br/>            conv2 = tf.layers.conv2d(<br/>                conv1,<br/>                64,<br/>                kernel_size=[4, 4],<br/>                strides=[2, 2],<br/>                activation=tf.nn.elu,<br/>                reuse=reuse,<br/>                name="conv_2",<br/>            )<br/>            conv3 = tf.layers.conv2d(<br/>                conv2,<br/>                64,<br/>                kernel_size=[3, 3],<br/>                strides=[1, 1],<br/>                activation=tf.nn.elu,<br/>                reuse=reuse,<br/>                name="conv_3",<br/>            )<br/>            hidden = c_layers.flatten(conv3)<br/><br/>        with tf.variable_scope(scope + "/" + "flat_encoding"):<br/>            hidden_flat = self.create_vector_observation_encoder(<br/>                hidden, h_size, activation, num_layers, scope, reuse<br/>            )<br/>        return hidden_flat</pre>
<ol start="7">
<li>The main difference with this implementation is the use of a third layer called <kbd>conv3</kbd>. We can see this third layer has a kernel size of 3 x 3, a stride of 1 x 1 and 64 filters. With a smaller kernel and stride size, we can see this new layer is being used to extract finer features. How useful that is depends on the environment.</li>
<li>Next, we want to look at the third visual encoding implementation listed just after the last function. The next function is <kbd>create_resent_visual_observation_encoder</kbd> and is shown as follows:</li>
</ol>
<pre style="padding-left: 60px"> def create_resnet_visual_observation_encoder(<br/>        self,<br/>        image_input: tf.Tensor,<br/>        h_size: int,<br/>        activation: ActivationFunction,<br/>        num_layers: int,<br/>        scope: str,<br/>        reuse: bool,<br/>    ) -&gt; tf.Tensor:       <br/>        n_channels = [16, 32, 32] <br/>        n_blocks = 2 <br/>        with tf.variable_scope(scope):<br/>            hidden = image_input<br/>            for i, ch in enumerate(n_channels):<br/>                hidden = tf.layers.conv2d(<br/>                    hidden,<br/>                    ch,<br/>                    kernel_size=[3, 3],<br/>                    strides=[1, 1],<br/>                    reuse=reuse,<br/>                    name="layer%dconv_1" % i,<br/>                )<br/>                hidden = tf.layers.max_pooling2d(<br/>                    hidden, pool_size=[3, 3], strides=[2, 2], padding="same"<br/>                )                <br/>                for j in range(n_blocks):<br/>                    block_input = hidden<br/>                    hidden = tf.nn.relu(hidden)<br/>                    hidden = tf.layers.conv2d(<br/>                        hidden,<br/>                        ch,<br/>                        kernel_size=[3, 3],<br/>                        strides=[1, 1],<br/>                        padding="same",<br/>                        reuse=reuse,<br/>                        name="layer%d_%d_conv1" % (i, j),<br/>                    )<br/>                    hidden = tf.nn.relu(hidden)<br/>                    hidden = tf.layers.conv2d(<br/>                        hidden,<br/>                        ch,<br/>                        kernel_size=[3, 3],<br/>                        strides=[1, 1],<br/>                        padding="same",<br/>                        reuse=reuse,<br/>                        name="layer%d_%d_conv2" % (i, j),<br/>                    )<br/>                    hidden = tf.add(block_input, hidden)<br/>            hidden = tf.nn.relu(hidden)<br/>            hidden = c_layers.flatten(hidden)<br/><br/>        with tf.variable_scope(scope + "/" + "flat_encoding"):<br/>            hidden_flat = self.create_vector_observation_encoder(<br/>                hidden, h_size, activation, num_layers, scope, reuse<br/>            )<br/>        return hidden_flat</pre>
<ol start="9">
<li>You can now go back and update the <kbd>vis_enc_type</kbd> hyperparameter in the config file and retrain the visual agent. Note which encoder is more successful if you have time to run both versions.</li>
</ol>
<p>We have seen what variations of visual encoders that ML-Agents supports and the team at Unity has also included a relatively new variant called ResNet. ResNet is an important achievement, and thus far, has shown to be useful for training agents in some visual environments. Therefore, in the next section, we will spend some extra time looking at ResNet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ResNet for visual observation encoding</h1>
                </header>
            
            <article>
                
<p>Convolutional layers have been used in various configurations for performing image classification and recognition tasks successfully for some time now. The problem we encounter with using straight 2D CNNs is we are essentially flattening state representations, but generally not in a good way. This means that we are taking a visual observation of a 3D space and flattening it to a 2D image that we then try and extract important features from. This results in an agent thinking it is in the same state if it recognizes the same visual features from potentially different locations in the same 3D environment. This creates confusion in the agent and you can visualize this by watching an agent just wander in circles.   </p>
<p>The same type of agent confusion can often be seen happening due to vanishing or exploding gradients. We haven't encountered this problem very frequently because our networks have been quite shallow. However, in order to improve network performance, we often deepen the network by adding additional layers. In fact, in some vision classification networks, there could be 100 layers or more of convolution trying to extract all manner of features. By adding this many additional layers, we introduce the opportunity for vanishing gradients. A vanishing gradient is a term we use for a gradient that becomes so small as to appear to vanish, or really have no effect on training/learning. Remember that our gradient calculation requires a total loss that is then transferred back through the network. The more layers the loss needs to push back through the network, the smaller it becomes. This is a major issue in the deep CNN networks that we use for image classification and interpretation. </p>
<p>ResNet or residual CNN networks were introduced as a way of allowing for deeper encoding structures without suffering vanishing gradients. Residual networks are so named because they carry forth a residual identity called an <strong>identity shortcut connection</strong>. The following diagram, sourced from the <em>Deep Residual Learning for Image Recognition</em> paper, shows the basic components in a residual block:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-874 image-border" src="assets/88c43406-c183-43a5-9841-a062dce3403f.png" style="width:22.75em;height:21.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">A residual block</div>
<p>The intuition from the authors in the paper is that stacked layers shouldn't degrade network performance just because they are stacked. Instead, by pushing the output of the last layer to the layer ahead, we are effectively able to isolate training to individual layers. We refer to this as an <strong>identity</strong> because the size of the output from the last layer will likely not match the input of the next layer, since we are bypassing the middle layer. Instead, we multiply the output of the last layer with an identity input tensor in order to match the output to the input.</p>
<p>Let's jump back to the ResNet encoder implementation back in ML-Agents and see how this is done in the next exercise:</p>
<ol>
<li>Open the <kbd>models.py</kbd> file located in the <kbd>mlagents/ml-agents/trainers</kbd> folder. </li>
<li>Scroll down to the <kbd>create_resnet_visual_observation_encoder</kbd> function again. Look at the first two lines that define some variables for building up the residual network as shown here:</li>
</ol>
<pre style="padding-left: 60px">n_channels = [16, 32, 32] # channel for each stack<br/>n_blocks = 2 # number of residual blocks</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Next, scroll down a little more to where we enumerate the number of channels listed to build up each of the input layers. The code is shown as follows:</li>
</ol>
<pre style="padding-left: 60px">for i, ch in enumerate(n_channels):<br/>    hidden = tf.layers.conv2d(<br/>        hidden, <br/>        ch, <br/>        kernel_size=[3, 3], <br/>        strides=[1, 1], <br/>        reuse=reuse,    <br/>        name="layer%dconv_1" % i,)<br/>        hidden = tf.layers.max_pooling2d(<br/>            hidden, pool_size=[3, 3], strides=[2, 2], padding="same")</pre>
<ol start="4">
<li>The <kbd>n_channels</kbd> variable represents the number of channels or filters used in each of the input convolution layers. Thus, we are creating three groups of residual layers with an input layer and blocks in between. The blocks are used to isolate the training to each of the layers.</li>
<li>Keep scrolling down, and we can see where the blocks are constructed between the layers with the following code:</li>
</ol>
<pre style="padding-left: 60px">for j in range(n_blocks):<br/>    block_input = hidden<br/>    hidden = tf.nn.relu(hidden)<br/>    hidden = tf.layers.conv2d(<br/>        hidden,<br/>        ch,<br/>        kernel_size=[3, 3],<br/>        strides=[1, 1],<br/>        padding="same",<br/>        reuse=reuse,<br/>        name="layer%d_%d_conv1" % (i, j),)<br/>    hidden = tf.nn.relu(hidden)<br/>    hidden = tf.layers.conv2d(<br/>        hidden,<br/>        ch,<br/>        kernel_size=[3, 3],<br/>        strides=[1, 1],<br/>        padding="same",<br/>        reuse=reuse,<br/>        name="layer%d_%d_conv2" % (i, j),)<br/>    hidden = tf.add(block_input, hidden)</pre>
<ol start="6">
<li>This code creates a network structure similar to what is shown in the following diagram:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-675 image-border" src="assets/078574c1-5e9c-4d9d-8858-1ef548849664.png" style="width:20.17em;height:26.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Diagram of the ResNet architecture in ML-Agents </div>
<ol start="7">
<li>In essence, we still only have three distinct convolutional layers extracting features, but each of those layers can now be trained independently. Furthermore, we can likely increase the depth of this network several times and expect an increase in visual encoding performance.</li>
<li>Go back and, if you have not already done so, train a visual agent with residual networks for visual observation encoding.  </li>
</ol>
<p>What you will likely find if you went back and trained another visual agent with residual networks is the agent performs marginally better, but they still can get confused. Again, this is more of a problem with the visual encoding system than the DRL itself. However, it is believed that once we can tackle visual encoding of visual environments, real AGI will certainly be a lot closer.</p>
<p>In the next section, we look at a special environment that the team at Unity put together (<span>with the help of Google DeepMind</span>) in order to challenge DRL researchers, which is the very problem of visual encoding 3D worlds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenging the Unity Obstacle Tower Challenge</h1>
                </header>
            
            <article>
                
<p>In late 2018, Unity, with the help of DeepMind, began development of a challenge designed to task researchers in the most challenging areas of DRL. The challenge was developed with Unity as a Gym interface environment and featured a game using a 3D first-person perspective. The 3D perspective is a type of game interface made famous with the likes of games such as Tomb Raider and Resident Evil, to name just a couple of examples. An example of the game interface is shown in the following screenshot:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-676 image-border" src="assets/9134c7f0-ca58-413f-bc3d-fa8cd1de16be.png" style="width:42.67em;height:25.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Example the obstacle tower challenge</span></div>
<p>The Obstacle Tower Challenge is not only in 3D, but the patterns and materials in the rooms and on the walls change over the levels. This makes vision generalization even more difficult. Furthermore, the challenge poses multiple concurrent steps to complete tasks. That is, each level requires the character to find a door and open it. On more advancing levels, the doors require a special key to be activated or acquired, which makes this almost a multi-task RL problem—not a problem we have considered solving previously. Fortunately, as we demonstrated using ML-Agents Curiosity Learning, multi-step RL can be accomplished, provided the tasks are linearly connected. This means there is no branching or tasks that require decisions.</p>
<p>Multi-task reinforcement learning is quickly advancing in research but it is still a very complicated topic. The current preferred method to solve MTRL is called <strong>meta reinforcement learning</strong>. We will cover Meta Reinforcement Learning in <a href="a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml">Chapter 14</a>, <em>From DRL to AGI</em>, where we will talk about the next evolutions of DRL in the coming months and/or years.</p>
<p>For the next exercise, we are going to closely review the work of the winner of the Unity Obstacle Tower Challenge, Alex Nichol. Alex won the $100,000 challenge by entering a modified PPO agent that was pre-trained on classified images and human recorded demonstrations (behavioural cloning). He essentially won by better generalizing the agent's observations of state using a number of engineered solutions. </p>
<p>Open up your Anaconda prompt and follow the next example:</p>
<ol>
<li>It is recommended that you create a new virtual environment before installing any new code and environments. This can easily be done with Anaconda using the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda create -n obtower python=3.6</strong><br/><strong>conda activate obstower</strong></pre>
<ol start="2">
<li>First, you will need to download and install the Unity Obstacle Tower Challenge from this repository (<a href="https://github.com/Unity-Technologies/obstacle-tower-env">https://github.com/Unity-Technologies/obstacle-tower-env</a>) or just use the following commands from a new virtual environment:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone git@github.com:Unity-Technologies/obstacle-tower-env.git
<span class="pl-c1">cd</span> obstacle-tower-env
pip install -e <span class="pl-c1">.</span></strong></pre>
<ol start="3">
<li>Running the OTC environment is quite simple and can be done with this simple block of code that just performs random actions in the environment:</li>
</ol>
<pre style="padding-left: 60px">from obstacle_tower_env import ObstacleTowerEnv, ObstacleTowerEvaluation<br/>def run_episode(env):<br/>    done = False<br/>    episode_return = 0.0<br/>    <br/>    while not done:<br/>        action = env.action_space.sample()<br/>        obs, reward, done, info = env.step(action)<br/>        episode_return += reward<br/>    return episode_return<br/><br/>if __name__ == '__main__':    <br/>    eval_seeds = [1001, 1002, 1003, 1004, 1005]    <br/>    env = ObstacleTowerEnv('./ObstacleTower/obstacletower')    <br/>    env = ObstacleTowerEvaluation(env, eval_seeds)    <br/>    while not env.evaluation_complete:<br/>        episode_rew = run_episode(env)    <br/>    print(env.results)<br/>    env.close()</pre>
<ol start="4">
<li>The code to run the OTC environment should be quite familiar by now, but does have one item to note. The agent cycles through episodes or lives, but the agent only has a certain number of lives. This environment simulates a real game, and hence, the agent only has a limited number of tries and time to complete the challenge.</li>
<li>Next, pull down the repository from Alex Nichol (<kbd>unixpickle</kbd>) here: <a href="https://github.com/unixpickle/obs-tower2.git">https://github.com/unixpickle/obs-tower2.git</a>, or check the <kbd>Chapter_13/obs-tower2</kbd> source folder.</li>
<li>Navigate to the folder and run the following command to install the required dependencies:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install -e .</strong></pre>
<ol start="7">
<li>After that, you need to configure some environment variables to the following:</li>
</ol>
<pre style="padding-left: 60px"> `OBS_TOWER_PATH` - the path to the obstacle tower binary.<br/> `OBS_TOWER_RECORDINGS` - the path to a directory where demonstrations are stored.<br/> `OBS_TOWER_IMAGE_LABELS` - the path to the directory of labeled images.</pre>
<ol start="8">
<li>How you set these environment variables will depend on your OS and at what level you want them set. For Windows users, you can set the environment variable using the <strong>System Environment Variables</strong> setup panel as shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-677 image-border" src="assets/75adffed-d9da-4814-ab62-c582c01ed82c.png" style="width:37.83em;height:33.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Setting the environment variables (Windows)</div>
<p>Now with everything set up, it is time to move on to pre-training the agent. We will cover that training in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pre-training the agent</h1>
                </header>
            
            <article>
                
<p>We have already covered a number of ways to manage training performance often caused by low rewards or rewards sparsity. This covered using a technique called behavioural cloning, whereby a human demonstrates a set of actions leading to a reward and those actions are then fed back into the agent as a pre-trained policy. The winning implementation here used a combination of behavioural cloning with pre-trained image classification.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We will continue from where we left off in the last exercise and learn what steps we need to perform in order to pre-train a classifier first:</p>
<ol>
<li>Firstly, we need to capture images of the environment in order to pre-train a classifier. This requires you to run the <kbd>record.py</kbd> script located at the <kbd>obs_tower2/recorder/record.py</kbd> folder. Make sure when running this script that your environment variables are configured correctly.</li>
</ol>
<div class="packt_tip">The documentation or <kbd>README.md</kbd> on the repository is good but is only really intended for advanced users who are very interested in replicating results. If you do encounter issues in this walkthrough, refer back to that documentation.</div>
<ol start="2">
<li>Running the script will launch the Unity OTC and allow you as a player to interact with the game. As you play the game, the <kbd>record.py</kbd> script will record your moves as images after every episode. You will need to play several games in order to have enough training data. Alternatively, Alex has provided a number of recordings online at this location: <a href="http://obstower.aqnichol.com/">http://obstower.aqnichol.com/</a>.</li>
</ol>
<div class="packt_infobox"><strong>Note:</strong> <br/>
The recordings and labels are both in tar files with the recordings weighing in at 25 GB.  </div>
<ol start="3">
<li>Next, we need to label the recorded images in order to assist the agent in classification. Locate and run the <kbd>main.py</kbd> script located in the <kbd>obs_tower2/labeler/</kbd> folder. This will launch a web application. As long as you have your paths set correctly, you can now open a browser and go to <kbd>http://127.0.0.1:5000</kbd> (localhost, port <kbd>5000</kbd>).</li>
<li>You will now be prompted to label images by using the web interface. For each image, classify the state as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-678 image-border" src="assets/4863d210-ccdd-417c-a78f-d796b60ecab9.png" style="width:33.08em;height:32.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Labeling image data for classification</div>
<ol start="5">
<li>Alex notes in his original documentation that he could label 20-40 images per second after some practice. Again, if you want to avoid this step, just download the tar files containing his example recordings and labels.</li>
<li>Next, you will need to run that classifier with the training input images and labels you either just generated or downloaded. Run the classified by executing the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd obs_tower2/scripts</strong><br/><strong>python run_classifier.py</strong></pre>
<ol start="7">
<li>After the classification is done, the results will be output to a <kbd>save_classifier.pk1</kbd> file periodically. The whole process may take several hours to train completely.</li>
</ol>
<ol start="8">
<li>With the pre-classifier built, we can move to behavioral cloning using the human sample playing. This means you will used the saved and pre-labelled sessions as inputs for later agent training. You can start the process by running the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>python run_clone.py</strong></pre>
<ol start="9">
<li>Running this script generates periodic output to a <kbd>save_clone.pkl</kbd> file and the whole script can take a day or more to run. When the script is complete, copy the output to a <kbd>save_prior.pkl</kbd> file like so:</li>
</ol>
<pre style="padding-left: 60px"><strong>cp save_clone.pkl save_prior.pkl</strong></pre>
<p>This creates a prior set of recordings or memories we will use to train the agent in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Prierarchy – implicit hierarchies</h1>
                </header>
            
            <article>
                
<p>Alex used the notion of hierarchical reinforcement learning in order to tackle the problem of multi-task agent learning that OTC requires you to solve. HRL is another method outside Meta-RL that has been used to successfully solve multi-task problems. Prierarchy-RL refines this by building a prior hierarchy that allows an action or action-state to be defined by entropy or uncertainty. High entropy or highly uncertain actions become high level or top-based actions. This is someone abstract in concept, so let's look at a code example to see how this comes together:</p>
<ol>
<li>The base agent used to win the challenge was PPO; following is a full source listing of that agent and a refresher to PPO:</li>
</ol>
<pre style="padding-left: 60px">import itertools<br/><br/>import numpy as np<br/>import torch<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from .util import atomic_save<br/><br/>class PPO:    <br/>    def __init__(self, model, epsilon=0.2, gamma=0.99, lam=0.95, lr=1e-4, ent_reg=0.001):<br/>        self.model = model<br/>        self.epsilon = epsilon<br/>        self.gamma = gamma<br/>        self.lam = lam<br/>        self.optimizer = optim.Adam(model.parameters(), lr=lr)<br/>        self.ent_reg = ent_reg<br/><br/>    def outer_loop(self, roller, save_path='save.pkl', **kwargs):        <br/>        for i in itertools.count():<br/>            terms, last_terms = self.inner_loop(roller.rollout(), **kwargs)<br/>            self.print_outer_loop(i, terms, last_terms)<br/>            atomic_save(self.model.state_dict(), save_path)<br/><br/>    def print_outer_loop(self, i, terms, last_terms):<br/>        print('step %d: clipped=%f entropy=%f explained=%f' %<br/>              (i, last_terms['clip_frac'], terms['entropy'], terms['explained']))<br/><br/>    def inner_loop(self, rollout, num_steps=12, batch_size=None):<br/>        if batch_size is None:<br/>            batch_size = rollout.num_steps * rollout.batch_size<br/>        advs = rollout.advantages(self.gamma, self.lam)<br/>        targets = advs + rollout.value_predictions()[:-1]<br/>        advs = (advs - np.mean(advs)) / (1e-8 + np.std(advs))<br/>        actions = rollout.actions()<br/>        log_probs = rollout.log_probs()<br/>        firstterms = None<br/>        lastterms = None<br/>        for entries in rollout.batches(batch_size, num_steps):<br/>            def choose(values):<br/>                return self.model.tensor(np.array([values[t, b] for t, b in entries]))<br/>            terms = self.terms(choose(rollout.states),<br/>                               choose(rollout.obses),<br/>                               choose(advs),<br/>                               choose(targets),<br/>                               choose(actions),<br/>                               choose(log_probs))<br/>            self.optimizer.zero_grad()<br/>            terms['loss'].backward()<br/>            self.optimizer.step()<br/>            lastterms = {k: v.item() for k, v in terms.items() if k != 'model_outs'}<br/>            if firstterms is None:<br/>                firstterms = lastterms<br/>            del terms<br/>        return firstterms, lastterms<br/><br/>    def terms(self, states, obses, advs, targets, actions, log_probs):<br/>        model_outs = self.model(states, obses)<br/><br/>        vf_loss = torch.mean(torch.pow(model_outs['critic'] - targets, 2))<br/>        variance = torch.var(targets)<br/>        explained = 1 - vf_loss / variance<br/><br/>        new_log_probs = -F.cross_entropy(model_outs['actor'], actions.long(), reduction='none')<br/>        ratio = torch.exp(new_log_probs - log_probs)<br/>        clip_ratio = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon)<br/>        pi_loss = -torch.mean(torch.min(ratio * advs, clip_ratio * advs))<br/>        clip_frac = torch.mean(torch.gt(ratio * advs, clip_ratio * advs).float())<br/><br/>        all_probs = torch.log_softmax(model_outs['actor'], dim=-1)<br/>        neg_entropy = torch.mean(torch.sum(torch.exp(all_probs) * all_probs, dim=-1))<br/>        ent_loss = self.ent_reg * neg_entropy<br/><br/>        return {<br/>            'explained': explained,<br/>            'clip_frac': clip_frac,<br/>            'entropy': -neg_entropy,<br/>            'vf_loss': vf_loss,<br/>            'pi_loss': pi_loss,<br/>            'ent_loss': ent_loss,<br/>            'loss': vf_loss + pi_loss + ent_loss,<br/>            'model_outs': model_outs,<br/>        }</pre>
<ol start="2">
<li>Familiarize yourself with the differences between this implementation and what we covered for PPO. Our example was simplified for explanation purposes but follows the same patterns.</li>
<li>Pay particular attention to the code in <kbd>inner_loop</kbd> and understand how this works:</li>
</ol>
<pre style="padding-left: 60px">def inner_loop(self, rollout, num_steps=12, batch_size=None):</pre>
<ol start="4">
<li>Open the <kbd>prierarchy.py</kbd> file located in the root <kbd>obs_tower2</kbd> folder and as shown here:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>import torch<br/>import torch.nn.functional as F<br/><br/>from .ppo import PPO<br/><br/>class Prierarchy(PPO):  <br/>    def __init__(self, prior, *args, kl_coeff=0, **kwargs):<br/>        super().__init__(*args, **kwargs)<br/>        self.prior = prior<br/>        self.kl_coeff = kl_coeff<br/><br/>    def print_outer_loop(self, i, terms, last_terms):<br/>        print('step %d: clipped=%f entropy=%f explained=%f kl=%f' %<br/>              (i, last_terms['clip_frac'], last_terms['entropy'], terms['explained'],<br/>               terms['kl']))<br/><br/>    def inner_loop(self, rollout, num_steps=12, batch_size=None):<br/>        if batch_size is None:<br/>            batch_size = rollout.num_steps * rollout.batch_size<br/>        prior_rollout = self.prior.run_for_rollout(rollout)<br/>        prior_logits = prior_rollout.logits()<br/>        rollout = self.add_rewards(rollout, prior_rollout)<br/>        advs = rollout.advantages(self.gamma, self.lam)<br/>        targets = advs + rollout.value_predictions()[:-1]<br/>        actions = rollout.actions()<br/>        log_probs = rollout.log_probs()<br/>        firstterms = None<br/>        lastterms = None<br/>        for entries in rollout.batches(batch_size, num_steps):<br/>            def choose(values):<br/>                return self.model.tensor(np.array([values[t, b] for t, b in entries]))<br/>            terms = self.extended_terms(choose(prior_logits),<br/>                                        choose(rollout.states),<br/>                                        choose(rollout.obses),<br/>                                        choose(advs),<br/>                                        choose(targets),<br/>                                        choose(actions),<br/>                                        choose(log_probs))<br/>            self.optimizer.zero_grad()<br/>            terms['loss'].backward()<br/>            self.optimizer.step()<br/>            lastterms = {k: v.item() for k, v in terms.items() if k != 'model_outs'}<br/>            if firstterms is None:<br/>                firstterms = lastterms<br/>            del terms<br/>        return firstterms, lastterms<br/><br/>    def extended_terms(self, prior_logits, states, obses, advs, targets, actions, log_probs):<br/>        super_out = self.terms(states, obses, advs, targets, actions, log_probs)<br/>        log_prior = F.log_softmax(prior_logits, dim=-1)<br/>        log_posterior = F.log_softmax(super_out['model_outs']['actor'], dim=-1)<br/>        kl = torch.mean(torch.sum(torch.exp(log_posterior) * (log_posterior - log_prior), dim=-1))<br/>        kl_loss = kl * self.ent_reg<br/>        super_out['kl'] = kl<br/>        super_out['kl_loss'] = kl_loss<br/>        super_out['loss'] = super_out['vf_loss'] + super_out['pi_loss'] + kl_loss<br/>        return super_out<br/><br/>    def add_rewards(self, rollout, prior_rollout):<br/>        rollout = rollout.copy()<br/>        rollout.rews = rollout.rews.copy()<br/><br/>        def log_probs(r):<br/>            return F.log_softmax(torch.from_numpy(np.array([m['actor'] for m in r.model_outs])),<br/>                                 dim=-1)<br/><br/>        q = log_probs(prior_rollout)<br/>        p = log_probs(rollout)<br/>        kls = torch.sum(torch.exp(p) * (p - q), dim=-1).numpy()<br/><br/>        rollout.rews -= kls[:-1] * self.kl_coeff<br/><br/>        return rollout</pre>
<ol start="5">
<li>What we see here is the <kbd>Pierarchy</kbd> class, an extension to <kbd>PPO</kbd>, which works by extending the <kbd>inner_loop</kbd> function. Simply, this code refines the KL-Divergence calculation that allowed us to secure that spot on the hill without falling off. Recall this was our discussion of the clipped objective function.</li>
<li>Notice the use of the <kbd>prior</kbd> policy or the policy that was generated based on the pre-training and behavioral cloning done earlier. This prior policy defines if actions are high or low in uncertainty. That way, an agent can actually use the prior hierarchy or prierarchy to select a series of high and then lower entropy/uncertain actions.  The following diagram illustrates how this effectively works:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-679 image-border" src="assets/f367f2af-2a1b-45e6-973f-b8dd7cf7a3bc.png" style="width:26.83em;height:17.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Agent selecting action based on entropy hierarchy</div>
<ol start="7">
<li>Thus, instead of deciding when and if to explore, the agent decides random actions based on their hierarchy or uncertainty. This means that higher-level actions can be reduced in uncertainty quickly because each successive action has less uncertainty.  </li>
</ol>
<div class="packt_infobox">A helpful example when trying to understand Priercarchy is the movie <em>Groundhog Day</em>, starring Bill Murray. In the movie, the character continually cycles through the same day, attempting by trial and error to find the optimum path to break out of the path. In the movie, we can see the character try thousands, perhaps millions, of different combinations, but we see this done in hierarchical steps. We first see the character wildly going about his day never accomplishing anything, until he learns through past hierarchical actions what are the best possible rewards. He learns that by improving on himself, his time in eternity becomes more pleasant. In the end, we see the character try to live their best life, only to discover they solved the game and can move on to the next day.</div>
<ol start="8">
<li>You can train the agent by running the following command on the first 10 levels:</li>
</ol>
<pre style="padding-left: 60px"><strong>cp save_prior.pkl save.pkl</strong><br/><strong>python run_tail.py --min 0 --max 1 --path save.pkl</strong></pre>
<ol start="9">
<li>Then, to train the agent on floors greater than 10, you can use the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>cp save_prior.pkl save_tail.pkl</strong><br/><strong>python run_tail.py --min 10 --max 15 --path save_tail.pkl</strong></pre>
<p>Every 10 levels in the OTC, the game theme changes. This means the wall color and textures will change as well as the tasks that need to get completed. As we mentioned earlier, this visual change, combined with 3D, will make the Unity OTC one of the most difficult and benchmark challenges to beat when we first get smart/bold and/or brave enough to tackle AGI. AGI and the road to more general intelligence with DRL will be our focus for <a href="a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml">Chapter 14</a>, <em>From DRL to AGI</em>.</p>
<p>In the next section, we look at 3D world Habitat by Facebook, which is more difficult but equally fun.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring Habitat – embodied agents by FAIR</h1>
                </header>
            
            <article>
                
<p>Habitat is a relatively new entry by Facebook AI Research for a new form of embodied agents. This platform represents the ability to represent full 3D worlds displayed from real-world complex scenes. The environment is intended for AI research of robots and robotic-like applications that DRL will likely power in the coming years. To be fair though, pun intended, this environment is implemented to train all forms of AI on this type of environment. The current Habitat repository only features some simple examples and implementation of PPO.</p>
<p>The Habitat platform comes in two pieces: the Habitat Sim and Habitat API. The simulation environment is a full 3D powered world that can render at thousands of frames per second, which is powered by photogrammetry RGBD data. RGBD is essentially RGB color data plus depth. Therefore, any image taken will have a color value and depth. This allows the data to be mapped in 3D as a hyper-realistic representation of the real environment. You can explore what one of these environments look like by using Habitat itself in your browser by following the next quick exercise:</p>
<ol>
<li>Navigate your browser to <a href="https://aihabitat.org/demo/">https://aihabitat.org/demo/.</a></li>
</ol>
<div class="mce-root packt_tip">Habitat will currently only run in Chrome or on your desktop.</div>
<ol start="2">
<li>It may take some time to load the app so be patient. When the app is loaded, you will see something like the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-680 image-border" src="assets/ebbec784-c61d-4fa2-aa67-1cca633cf1d8.png" style="width:158.67em;height:72.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example of Habitat running in the browser</div>
<ol start="3">
<li>Use the WASD keys to move around in the environment.  </li>
</ol>
<p>Habitat supports importing from the following three vendors: <a href="https://niessner.github.io/Matterport/">MatterPort3D</a><span>,</span> <a href="http://gibsonenv.stanford.edu/database/">Gibson</a><span>, and</span> <a href="https://github.com/facebookresearch/Replica-Dataset">Replica</a>, who produce tools and utilities to capture RGBD data and have libraries of this data. Now that we understand what Habitat is, we will set it up in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Habitat</h1>
                </header>
            
            <article>
                
<p>At the time of writing, Habitat was still a new product, but the documentation worked well to painlessly install and run an agent for training. In our next exercise, we walk through parts of that documentation to install and run a training agent in Habitat:</p>
<ol>
<li>Open an Anaconda command prompt and navigate to a clean folder. Use the following commands to download and install the Habitat:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>git clone --branch stable git@github.com:facebookresearch/habitat-sim.git <br/></span><span class="pl-c1">cd</span><span> habitat-sim</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="2">
<li>Then, create a new virtual environment and install the required dependencies with the following:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>conda create -n habitat python=3.6 cmake=3.14.0 <br/>conda activate habitat <br/>pip install -r requirements.txt</span></strong></pre>
<ol start="3">
<li>Next, we need to build the Habitat Sim with the following:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>python setup.py install</span></strong></pre>
<ol start="4">
<li>Download the test scenes from the following link: <a href="http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip">http://dl.fbaipublicfiles.com/habitat/habitat-test-scenes.zip</a>.</li>
<li>Unzip the scene files into a familiar path, one that you can link to later. These files are sets of RGBD data that represent the scenes.</li>
</ol>
<div class="packt_infobox">RGBD image capture is not new, and traditionally, it has been expensive since it requires moving a camera equipped with a special sensor around a room. Thankfully, most modern cell phones also feature this depth sensor. This depth sensor is often used to build augmented reality applications now. Perhaps in a few years, agents themselves will be trained to capture these types of images using just a simple cell phone.</div>
<ol start="6">
<li>After everything is installed, we can test the Habitat installation by running the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>python examples/example.py --scene /path/to/data/scene_datasets/habitat-test-scenes/skokloster-castle.glb</span></strong></pre>
<ol start="7">
<li>That will launch the Sim in non-interactive fashion and play some random moves. If you want to see or interact with the environment, you will need to download and install the interactive plugin found in the repository documentation.</li>
</ol>
<p>After the Sim is installed, we can move on to installing the API and training an agent in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training in Habitat</h1>
                </header>
            
            <article>
                
<p>At the time of writing, Habitat was quite new but showed amazing potential, especially for training agents. This means the environment currently only has a simple and PPO agent implementation in which you can quickly train agents. Of course, since Habitat uses PyTorch, you could probably implement one of the other algorithms we have covered. In the next exercise, we finish off by looking at the PPO implementation in Habitat and how to run it:</p>
<ol>
<li>Download and install the Habitat API with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>git clone --branch stable git@github.com:facebookresearch/habitat-api.git <br/></span><span class="pl-c1">cd</span><span> habitat-api <br/></span></strong><span class="pl-c1"><strong><span>pip install -r requirements.txt </span></strong><br/><strong><span>python setup.py develop --all</span></strong><br/></span></pre>
<ol start="2">
<li>At this point, you can use the API in a number of ways. We will first look at a basic code example you could write to run the Sim:</li>
</ol>
<pre style="padding-left: 60px">import habitat<br/><br/># Load embodied AI task (PointNav) and a pre-specified virtual robot<br/>env = habitat.Env(<br/>    config=habitat.get_config("configs/tasks/pointnav.yaml")<br/>)<br/><br/>observations = env.reset()<br/><br/># Step through environment with random actions<br/>while not env.episode_over:<br/>    observations = env.step(env.action_space.sample())</pre>
<ol start="3">
<li>As you can see, the Sim allows us to program an agent using the same familiar Gym style interface we are used to.  </li>
<li>Next, we need to install the Habitat Baselines package. This package is the RL portion and currently provides an example of PPO. The package is named Baselines after the OpenAI testing package of the same name.</li>
<li>Install the Habitat Baselines package using the following commands:</li>
</ol>
<pre style="padding-left: 60px"># be sure to cd to the habitat_baselines folder<br/><strong><span>pip install -r requirements.txt <br/>python setup.py develop --all</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>After the installation, you can run the <kbd>run.py</kbd> script in order to train an agent with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>python -u habitat_baselines/run.py --exp-config habitat_baselines/config/pointnav/ppo_pointnav.yaml --run-type train</span></strong></pre>
<ol start="7">
<li>Then, you can test this agent with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>python -u habitat_baselines/run.py --exp-config habitat_baselines/config/pointnav/ppo_pointnav.yaml --run-type </span><span class="pl-c1">eval</span></strong></pre>
<p>Habitat is a fairly recent development and opens the door to training agents/robots in real-world environments. While Unity and ML-Agents are great platforms for training agents in 3D game environments, they still do not compare to the complexity of the real world. In the real world, objects are rarely perfect and are often very complex, which makes these environments especially difficult to generalize, and therefore, train on. In the next section, we finish the chapter with our typical exercises.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>As we progressed through this book, the exercises have morphed from learning exercises to almost research efforts, and that is the case in this chapter. Therefore, the exercises in this chapter are meant for the hardcore RL enthusiast and may not be for everyone:</p>
<ol>
<li>Tune the hyperparameters for one of the sample visual environments in the ML-Agents toolkit.</li>
<li>Modify the visual observation standard encoder found in the ML-Agents toolkit to include additional layers or different kernel filter settings.</li>
<li>Train an agent with <kbd>nature_cnn</kbd> or <kbd>resnet</kbd> visual encoder networks and compare their performance with earlier examples using the base visual encoder.</li>
<li>Modify the <kbd>resnet</kbd> visual encoder to accommodate many more layers or other variations of filter/kernel size. </li>
<li>Download, install, and play the Unity Obstacle Tower Challenge and see how far you can get in the game. As you play, think of yourself as an agent and reflect on what actions you are taking and how they reflect your current task trajectory.</li>
<li>Build your own implementation of an algorithm to test against the Unity OTC. Completing this challenge will be especially rewarding if you beat the results of the previous winner. This challenge is still somewhat open and anyone claiming to do higher than level 20 will probably make a big impact on DRL in the future.</li>
</ol>
<ol start="7">
<li>Replace the PPO base example in the Habitat Baselines module with an implementation of Rainbow DQN. How does the performance compare?</li>
<li>Implement a different visual encoder for the Habitat Baselines framework. Perhaps use the previous examples of <kbd>nature_cnn</kbd> or <kbd>resnet</kbd>.</li>
<li>Compete in the Habitat Challenge. This is a challenge that requires an agent to complete a navigation task through a series of waypoints. It's certainly not as difficult as the OTC, but the visual environment is far more complex.</li>
<li>Habitat is intended more for sensor development instead of visual development. See if you are able to combine visual observation encoding with other sensor input as a type of combined visual and sensor observation input.</li>
</ol>
<p>The exercises in this chapter are intended to be entirely optional; please choose to do these only if you have a reason to do so. They likely will require additional time as this is a very complex area to develop in.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored the concept of 3D worlds for not only games but the real world. The real world, and to a greater extent 3D worlds, are the next great frontier in DRL research. We looked at why 3D creates nuances for DRL that we haven't quite figured out how best to solve. Then, we looked at using 2D visual observation encoders but tuned for 3D spaces, with variations in the Nature CNN and ResNet or residual networks. After that, we looked at the Unity Obstacle Tower Challenge, which challenged developers to build an agent capable of solving the 3D multi-task environment.</p>
<p>From there, we looked at the winning entries use of Prierarchy; a form of HRL in order to manage multiple task spaces. We also looked at the code in detail to see how this reflected in the winners modified PPO implementation. Lastly, we finished the chapter by looking at Habitat; an advanced AI environment that uses RGBD and depth based color data, to render real-world environments in 3D.</p>
<p>We are almost done with our journey, and in the next and final chapter, we will look at how DRL is moving toward artificial general intelligence, or what we refer to as AGI.</p>


            </article>

            
        </section>
    </body></html>