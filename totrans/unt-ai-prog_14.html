<html><head></head><body>
		<div id="_idContainer151">
			<h1 id="_idParaDest-156"><em class="italic"><a id="_idTextAnchor587"/>Chapter 11</em>: <a id="_idTextAnchor588"/><a id="_idTextAnchor589"/><a id="_idTextAnchor590"/><a id="_idTextAnchor591"/>Machine Learning in Unity</h1>
			<p>Machine learning<a id="_idIndexMarker547"/> is the hottest buzzword in <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>). Nowadays, everything contains (or claims to contain) some machine learning-powered AI<a id="_idIndexMarker548"/> that is supposed to improve our life: calendars, to-do apps, photo management software, every smartphone, and much more. However, even if the phrase <em class="italic">machine learning</em> is just a marketing gimmick most of the time, it is without question that machine learning has improved significantly in recent years. Most importantly, though, there are now plenty of tools that allow everybody to implement a learning algorithm without any previous academic-level AI knowledge.</p>
			<p>At the moment, machine learning is not used in game development (except for applications for procedural content generation). There are many reasons for that. The main reason, though, is that a designer can't control the output of a machine learning agent, and in game design, uncontrollable outcomes often correlate to not-fun games. For this reason, game AI developers prefer more predictable and straightforward techniques, such as behavior trees.</p>
			<p>On the other hand, being able to use machine learning algorithms in Unity is useful for non-gaming purposes, such as simulations, AI research, and <em class="italic">some serious</em> gaming applications. Whatever the reason, Unity provides a complete toolkit for machine learning that spares us the complication of interfacing the game engine with an external machine learning framework.</p>
			<p>In this chapter, we will look at the following topics:</p>
			<ul>
				<li>An introduction to the Unity Machine Learning Agents Toolkit </li>
				<li>Setting up the Unity Machine Learning Agents Toolkit</li>
				<li>Seeing how to run a simple example</li>
			</ul>
			<p>Machine learning is an extensive topic; therefore, we do not expect to cover every single aspect of it. Instead, look at the toolkit documentation and the additional resources linked at the end of this chapter for further reference<a id="_idTextAnchor592"/>.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor593"/>Technical requirements</h1>
			<p>For this chapter, you need Unity3D 2022, Python 3.7, PyTorch, and the ML-Agents Toolkit installed on your system. Don't worry if you don't; we will go over the installation steps. You can find the example project described in this chapter in the <strong class="source-inline">Chapter 11</strong> folder in the book's repository: <a href="https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11">https://github.com/PacktPublishing/Unity-Artificial-Intelligence-Programming-Fifth-Edition/tree/main/Chapter11</a></p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor594"/>The Unity Machine Learning Agents Toolkit</h1>
			<p>The <strong class="bold">Unity Machine Learning Agents Toolkit</strong> (<strong class="bold">ML-Agents Toolkit</strong>) is a collection of software and plugins that help developers write autonomous game agents powered by machine learning algorithms. You can explore and download the source code at the GitHub repository at <a href="https://github.com/Unity-Technologies/ml-agents">https://github.com/Unity-Technologies/ml-agents</a>.</p>
			<p>The ML-Agents Toolkit<a id="_idIndexMarker549"/> is based on the reinforcement learning algorithm. Simplistically, reinforcement learning is the algorithmic equivalent of training a dog. For example, if you want to teach a dog some trick, you give him a command, and then, when the dog does what you expect, you reward him. The reward tells your dog that it responded correctly to the command, and therefore, the next time it hears the same command, it will do the same thing to get a new reward.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In reinforcement learning, you can also punish your agent when doing the wrong things, but in the dog-training example, I can assure you that punishment is entirely unnecessary. Just give them rewards!</p>
			<p>For an AI agent trained with reinforcement learning, we perform a similar cycle:</p>
			<ol>
				<li>When an agent acts, the action influences the world (such as changing the Agent's position, moving an object around, collecting a coin, gaining score points, and so on).</li>
				<li>The algorithm then sends the new world state back to the Agent with a reward (or punishment).</li>
				<li>When the Agent decides its following action, it will choose the action that maximizes the expected reward (or minimizes the expected punishment).</li>
			</ol>
			<p>For this reason, it is clear that training a reinforcement learning agent requires several simulations of the scenario in which the Agent acts, receives a reward, updates its decision-making values, performs another action, and so on. This work is offloaded from Unity to Torch via PyTorch. <strong class="bold">Torch</strong> is a<a id="_idIndexMarker550"/> popular open source machine learning library used, among others, by tech giants such as Facebook and IBM.</p>
			<p>Refer to the <em class="italic">Further reading</em> section at the end of the chapter for more information on reinforcement learning.</p>
			<p>Let's now see how to install the toolk<a id="_idTextAnchor595"/><a id="_idTextAnchor596"/><a id="_idTextAnchor597"/>it.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor598"/>Installing the ML-Agents Toolkit</h1>
			<p>As a first step, we <a id="_idIndexMarker551"/>need to download the toolkit. We can do this by cloning the repository with the following command:</p>
			<p class="source-code">git clone --branch release_19 https://github.com/Unity-Technologies/ml-agents.git</p>
			<p>This command creates an <strong class="source-inline">ml-agents</strong> folder in your current folder. The ML-Agents Toolkit is composed of two main components:</p>
			<ul>
				<li>A Python package containing the Python interface for Unity and PyTorch's trainers (stored in the <strong class="source-inline">ml-agents</strong> folder)</li>
				<li>A Python package containing the interface with <a id="_idIndexMarker552"/>OpenAI Gym (<a href="https://gym.openai.com/">https://gym.openai.com/</a>), a toolkit for training reinforcement learning agents (stored in the <strong class="source-inline">gym-unity</strong> folder).<p class="callout-heading">Information</p><p class="callout"><strong class="bold">Git</strong> is the most <a id="_idIndexMarker553"/>famous version-control application in the world. It is used to store your source code, keep track of different versions, collaborate with other people, and much more. If you are not already using Git, you should really check it out. You can download<a id="_idIndexMarker554"/> it from <a href="https://git-scm.com/">https://git-scm.com/</a>.</p></li>
			</ul>
			<p>Now, it is time to install the required depende<a id="_idTextAnchor599"/>ncies. </p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor600"/>Installing Python and PyTorch on Windows</h2>
			<p>The suggested<a id="_idIndexMarker555"/> version of Python for the ML-Agents Toolkit is <a id="_idIndexMarker556"/>version 3.7. You can install it in many ways, the faster of which is by searching in<a id="_idIndexMarker557"/> Microsoft Store for Python 3.7 (or follow this link: <a href="https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p">https://www.microsoft.com/en-us/p/python-37/9nj46sx7x90p</a>).</p>
			<p>On Windows, you need<a id="_idIndexMarker558"/> to manually install PyTorch before installing<a id="_idIndexMarker559"/> the <strong class="source-inline">mlagents</strong> package. To do that, you can simply run this command in a terminal:</p>
			<p class="source-code">pip3 install torch~=1.7.1 -f https://download.pytorch.org/whl/torch_stable.html</p>
			<p class="callout-heading">Information</p>
			<p class="callout">If you have any difficulties <a id="_idIndexMarker560"/>installing PyTorch, you can refer to the official installation guide at <a href="https://pytorch.org/get-started/locally/">https://pytorch.org/get-started/locally/</a>.</p>
			<p>After this step, you should be able to follow the same installation steps for macOS and Unix-<a id="_idTextAnchor601"/><a id="_idTextAnchor602"/>like systems.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor603"/>Installing Python and PyTorch on macOS and Unix-like systems</h2>
			<p>To install <a id="_idIndexMarker561"/>the ML-Agents Toolkit on macOS or Linux, you need first <a id="_idIndexMarker562"/>to install Python 3.6 or Python 3.7 (at the<a id="_idIndexMarker563"/> moment, the ML-Agents Toolkit recommends only these two<a id="_idIndexMarker564"/> Python versions).</p>
			<p>Then, you can run the following command:</p>
			<p class="source-code">python -m pip install mlagents==0.28.0</p>
			<p>On macOS and Linux, this command installs automatically the correct version of PyTorch.</p>
			<p>After the installation, if everything is correct, you should be able to run the <strong class="source-inline">mlagents-learn --help</strong> command without any errors from any place in the system.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Pip3 is automatically installed with any Python 3.x distribution. If, for some reason, you don't have <strong class="source-inline">pip3</strong> installed, try following <a id="_idIndexMarker565"/>the official guide: <a href="https://pip.pypa.io/en/latest/installing/">https://pip.pypa.io/en/l<span id="_idTextAnchor604"/><span id="_idTextAnchor605"/>atest/installing/</a>.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor606"/>Using the ML-Agents Toolkit – a basic example</h1>
			<p>Now that<a id="_idIndexMarker566"/> everything is installed, we can start using the ML-Agents Toolkit. First, let's explain the basic architecture of an ML-Agents scene.</p>
			<p>An ML-Agents scene is called a <strong class="bold">learning environment</strong>. The<a id="_idIndexMarker567"/> learning environment is a standard Unity scene and contains two main elements: </p>
			<ul>
				<li><strong class="bold">The agent</strong>: Obviously, the Agent is the central object in the ML-Agents Toolkit. An agent<a id="_idIndexMarker568"/> is an object that performs an action, receives information from the environment, and can receive rewards for actions. To create an Agent, you need to subclass the <strong class="source-inline">Agent</strong> class and write the behavior for the agent. For instance, if the Agent is a car, we need to write how the car is controlled by the input and how we can reward and penalize the car (for example, we can reward the vehicle for going above a certain speed and punish it when it goes off-road). A learning environment can have as many agents as you like.</li>
				<li><strong class="bold">The academy</strong>: This component <a id="_idIndexMarker569"/>is a singleton (therefore, it doesn't need to be explicitly instantiated in the scene in a game object) that orchestrates the agents in the scene and is responsible for their training and decision making. Each sequence of actions and data collection is called an <strong class="bold">episode</strong>. An episode<a id="_idIndexMarker570"/> usually starts from a default starting configuration and ends when the Agent performs a maximum number of steps, reaches a goal, or fails to reach a goal. In particular, for every episode, the academy<a id="_idIndexMarker571"/> does the following operations:<ul><li>Invokes <strong class="source-inline">OnEpisodeBegin()</strong> for each <strong class="source-inline">Agent</strong> in the scene.</li><li>Invokes <strong class="source-inline">CollectObservations(VectorSensor sensor)</strong> for each <strong class="source-inline">Agent</strong> in the scene. This function is used to collect information on the environment so that each <strong class="source-inline">Agent</strong> can update its internal model.</li><li>Invokes <strong class="source-inline">OnActionReceived()</strong> for every <strong class="source-inline">Agent</strong> in the scene. This function executes the action chosen by each <strong class="source-inline">Agent</strong> and collects the rewards (or penalty).</li><li>If an agent completes its episode, the academy calls <strong class="source-inline">OnEpisodeBegin()</strong> for the Agent. This function is responsible for resetting the Agent in the starting configuration.</li></ul></li>
			</ul>
			<p>To start using<a id="_idIndexMarker572"/> the ML-Agents Toolkit, we need to do the following:</p>
			<ol>
				<li value="1">Open Unity and create an empty project.</li>
				<li>Go to <strong class="bold">Windows</strong> | <strong class="bold">Package Manager</strong>.</li>
				<li>In the top-left menu, select <strong class="bold">Unity Registry</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B17981_11_1.jpg" alt="Figure 11.1 – The Unity Package Manager&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – The Unity Package Manager</p>
			<ol>
				<li value="4">Look for <a id="_idIndexMarker573"/>the <strong class="bold">ML Agents</strong> package and install it:</li>
			</ol>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B17981_11_2.jpg" alt="Figure 11.2 – The ML Agents package in Package Manager&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – The ML Agents package in Package Manager</p>
			<p>We need to<a id="_idIndexMarker574"/> make sure that we are using the correct runtime.</p>
			<ol>
				<li value="5">To do so, go to <strong class="bold">Edit</strong> | <strong class="bold">Project Settings</strong> | <strong class="bold">Player</strong>, and for each platform (PC, Mac, Android, and so on), go into <strong class="bold">Other Settings</strong> and make sure that <strong class="bold">Api Compatibility Level</strong> is set to <strong class="bold">.NET Framework</strong>. If not, adjust these settings to be as we need them and then save, as follows:</li>
			</ol>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B17981_11_3.jpg" alt="Figure 11.3 – The Project Settings window with the correct settings&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – The Project Settings window w<a id="_idTextAnchor607"/><a id="_idTextAnchor608"/>ith the correct settings</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor609"/>Creating the scene</h2>
			<p>Creating a learning<a id="_idIndexMarker575"/> environment is easy. Let's create a simple 3D scene with a plane, a sphere, and a cube, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B17981_11_4.jpg" alt="Figure 11.4 – The basic demo scene&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – The basic demo scene</p>
			<p>We put the cube at the plane's center and add a <strong class="source-inline">Rigidbody</strong> component to the sphere. This scene aims to train a<a id="_idIndexMarker576"/> rolling ball to reach the target (the cube) withou<a id="_idTextAnchor610"/><a id="_idTextAnchor611"/>t falling from the plane.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor612"/>Implementing the code</h2>
			<p>Now, we need to implement the code<a id="_idIndexMarker577"/> that will describe the Agent's behavior and the ML-Agent's academy. The agent's behavior script describes how the Agents perform actions in the simulation, the reward the Agent receives, and how we reset it to start a new simulation:</p>
			<ol>
				<li value="1">Select the sphere. Let's add to it a new script, called <strong class="source-inline">SphereAgent</strong>, with the following content:<p class="source-code">using System.Collections.G<a id="_idTextAnchor613"/><a id="_idTextAnchor614"/>eneric;</p><p class="source-code">using UnityEngine;</p><p class="source-code">using Unity.MLAgents;</p><p class="source-code">using Unity.MLAgents.Sensors;</p><p class="source-code">using <a id="_idTextAnchor615"/><a id="_idTextAnchor616"/>Unity.MLAgents.Actuators;</p><p class="source-code">public c<a id="_idTextAnchor617"/><a id="_idTextAnchor618"/>lass SphereAgent : Agent {</p><p class="source-code">    Rigidbody rBody;</p><p class="source-code">    public Transform Target;</p><p class="source-code">    public float forceMultiplier = 10;</p><p class="source-code">    void Start () {</p><p class="source-code">        rBody = GetComponent&lt;Rigidbody&gt;();</p><p class="source-code">    }</p><p class="source-code"><a id="_idTextAnchor619"/><a id="_idTextAnchor620"/></p><p class="source-code">    override public void OnEpisodeBegin() {</p><p class="source-code">        if (transform.position.y &lt; -1.0) {</p><p class="source-code">            // The agent fell</p><p class="source-code">            transform.position = Vector3.zero;</p><p class="source-code">            rBody.angularVelocity = Vector3.zero;</p><p class="source-code">            rBody.velocity = Vector3.zero;</p><p class="source-code">        } else {</p><p class="source-code">            // Move the target to a new spot</p><p class="source-code">            Target.position = new Vector3(Random.value *</p><p class="source-code">              8 - 4, 0.5f, Random.value * 8 - 4);</p><p class="source-code">        }</p><p class="source-code">    }</p><p class="source-code">}</p></li>
			</ol>
			<p>This is the base agent for our demo. <strong class="source-inline">OnEpisodeBegin</strong> is a function called by the system every time we want to reset the training scene. In our example, we check whether the sphere fell from the plane and bring it back to zero; otherwise, we move it to a random location.</p>
			<ol>
				<li value="2">We need to<a id="_idIndexMarker578"/> add the <strong class="source-inline">CollectObservations</strong> method to <strong class="source-inline">SphereAgent</strong>. The agent uses this method to get information from the game world and then use<a id="_idTextAnchor621"/><a id="_idTextAnchor622"/><a id="_idTextAnchor623"/>s it to perform a decision:<p class="source-code">    override public void CollectObservations(</p><p class="source-code">      VectorSensor sensor) {</p><p class="source-code">        // Calculate relative position</p><p class="source-code">        Vector3 relativePosition = </p><p class="source-code">          Target.position - transform.position;</p><p class="source-code">        // Relative position</p><p class="source-code">        sensor.AddObservation(relativePosition.x/5);</p><p class="source-code">        sensor.AddObservation(relativePosition.z / 5);</p><p class="source-code">        // Distance to edges of platform</p><p class="source-code">        sensor.AddObservation(</p><p class="source-code">         (transform.position.x + 5) / 5);</p><p class="source-code">        sensor.AddObservation(</p><p class="source-code">          (transform.position.x - 5) / 5);</p><p class="source-code">        sensor.AddObservation(</p><p class="source-code">          (transform.position.z + 5) / 5);</p><p class="source-code">        sensor.AddObservation(</p><p class="source-code">          (transform.position.z - 5) / 5);</p><p class="source-code">        // agent velocity</p><p class="source-code">        sensor.AddObservation(rBody.velocity.x / 5);</p><p class="source-code">        sensor.AddObservation(rBody.velocity.z / 5);    }</p></li>
			</ol>
			<p>In this example, we are interested in the following:</p>
			<ul>
				<li>The relative position of the sphere agent from the cube (the target). We are only interested in the <strong class="source-inline">x</strong> and <strong class="source-inline">z</strong> values because the sphere only moves on the plane (note that we normalize the values by dividing by <strong class="source-inline">5</strong>, which is half the default plane size).</li>
				<li>The distance<a id="_idIndexMarker579"/> from the plane's edges.</li>
				<li>The sphere's velocity.</li>
			</ul>
			<ol>
				<li value="3">We need to implement the <strong class="source-inline">OnActionReceived</strong> method. This method is called whenever the Agent needs to act. The method takes a single parameter of the <strong class="source-inline">ActionBuffer</strong> type. The <strong class="source-inline">ActionBuffer</strong> object contains a description of the control inputs for the sphere. In our case, we only need two continuous actions, corresponding to the force applied along the <em class="italic">x</em> and <em class="italic">z</em> axes of the game. </li>
				<li>We also need to define the rewards. As said before, we reward the Agent with one point by calling <strong class="source-inline">SetReward</strong> when we reach the target. If the Agent falls off the plane, we end the episode with zero points by calling <strong class="source-inline">EndEpisode</strong>. The final version o<a id="_idTextAnchor624"/><a id="_idTextAnchor625"/>f the code<a id="_idIndexMarker580"/> is the follow<a id="_idTextAnchor626"/><a id="_idTextAnchor627"/>ing:<p class="source-code">    public override void OnActionReceived(</p><p class="source-code">      ActionBuffers actionBuffers) {</p><p class="source-code">        // Actions, size = 2</p><p class="source-code">        Vector3 controlSignal = Vector3.zero;</p><p class="source-code">        controlSignal.x = </p><p class="source-code">          actionBuffers.ContinuousActions[0];</p><p class="source-code">        controlSignal.z = </p><p class="source-code">          actionBuffers.ContinuousActions[1];</p><p class="source-code">        rBody.AddForce(controlSignal * forceMultiplier);</p><p class="source-code">        // Rewards</p><p class="source-code">        float distanceToTarget = Vector3.Distance(this.</p><p class="source-code">          transform.localPosition, Target.localPosition);</p><p class="source-code">        // Reached target</p><p class="source-code">        if (distanceTo<a id="_idTextAnchor628"/><a id="_idTextAnchor629"/>Target &lt; 1.42f) {</p><p class="source-code">            SetReward(1.0f);</p><p class="source-code">            EndEpisode();</p><p class="source-code">        }</p><p class="source-code">        // Fell off platform</p><p class="source-code">        else if (this.transform.localPosition.y &lt; 0) {</p><p class="source-code">            EndEpisode();</p><p class="source-code">        }</p><p class="source-code">    }}</p></li>
			</ol>
			<p>Now, it is time to<a id="_idIndexMarker581"/> connect this <strong class="source-inline">Sph<a id="_idTextAnchor630"/><a id="_idTextAnchor631"/>ereAgent</strong> script to our sphere.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor632"/>Adding the final touches</h2>
			<p>Now, we need to connect all the <a id="_idIndexMarker582"/>pieces to make the demo work:</p>
			<ol>
				<li value="1">First, we attach the <strong class="source-inline">SphereAgent</strong> script to the sphere.</li>
				<li>We drag and drop the cube into the <strong class="bold">Target</strong> field of the <strong class="bold">Sphere Agent</strong> component.</li>
				<li>We add a <strong class="bold">Decision Requester</strong> component by clicking on <strong class="bold">Add Component</strong>. We can leave the default settings. </li>
				<li>In the <strong class="bold">Behavior Parameters</strong> script, we set up <strong class="source-inline">MovingSphere</strong>.</li>
				<li>We set the <strong class="bold">Vector Observation</strong> | <strong class="bold">Space Size</strong> value to <strong class="source-inline">8</strong>, corresponding to the number of observations we added in the <strong class="source-inline">CollectObservations</strong> method. </li>
				<li>Finally, we set the <strong class="bold">Actions</strong> | <strong class="bold">Continuous Actions</strong> value to <strong class="source-inline">2</strong>. At this point, the sphere scripts should look like the following screenshot: </li>
			</ol>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B17981_11_5.jpg" alt="Figure 11.5 – The Inspector view for the sphere agent&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – The Inspector view for the sphere agent</p>
			<p>It is now to test our environment.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor633"/>Testing the learning environment</h1>
			<p>Before we start learning, we want to <a id="_idIndexMarker583"/>test the environment by controlling the Agents with manual input. It is very useful to debug the learning environment without wasting hours of the training process.</p>
			<p>Fortunately, the ML-Agents Toolkit makes it very handy to control an agent with live input. We only need two steps:</p>
			<ol>
				<li value="1">We add the <strong class="source-inline">Heuristic</strong> method to the <strong class="source-inline">SphereAgent</strong> component. This function allows us to manually <a id="_idIndexMarker584"/>specify the values of the <strong class="source-inline">ActionBuffer</strong> objects. In our case, we want to add the two continuous actions to the input axes of the controller:<p class="source-code">    public override void Heuristic(</p><p class="source-code">      in ActionBuffers actionsOut) {</p><p class="source-code">        var continuousActionsOut = </p><p class="source-code">          actionsOut.ContinuousActions;</p><p class="source-code">        continuousActionsOut[0] = </p><p class="source-code">          Input.GetAxis("Horizontal");</p><p class="source-code">        continuousActionsOut[1] = </p><p class="source-code">          Input.GetAxis("Vertical");</p><p class="source-code">    }</p></li>
				<li>Now,<a id="_idTextAnchor634"/><a id="_idTextAnchor635"/> we go to the <a id="_idIndexMarker585"/>Inspector and set the <strong class="bold">Behavior Type</strong> parameter to <strong class="bold">Heuristic Only</strong>:</li>
			</ol>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B17981_11_6.jpg" alt="Figure 11.6 – The Behavior Type configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – The Behavior Type configuration</p>
			<p>At this point, you can press <strong class="bold">Play</strong> in Unity, and you should be able to control the sphere using the arrow key (or a gaming controller stick). You can test the environment by checking the episode's behavior. If you reach the target cube, it should disappear and spawn in another random location. If you fall from the plane, you should respawn on the plane.</p>
			<p>If everything looks fine, it is time t<a id="_idTextAnchor636"/><a id="_idTextAnchor637"/>o train the Agent automatically.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor638"/>Training an agent</h1>
			<p>Before we can start training, we<a id="_idIndexMarker586"/> need to write a training configuration file. Open your terminal and go into any empty folder. Then, create a <strong class="source-inline">sphere.yaml</strong> file with the following code:</p>
			<p class="source-code">behaviors:</p>
			<p class="source-code">  MovingSphere:</p>
			<p class="source-code">    trainer_type: ppo</p>
			<p class="source-code">    hyperparameters:</p>
			<p class="source-code">      batch_size: 10</p>
			<p class="source-code">      buffer_size: 100</p>
			<p class="source-code">      learning_rate: 3.0e-4</p>
			<p class="source-code">      beta: 5.0e-4</p>
			<p class="source-code">      epsilon: 0.2</p>
			<p class="source-code">      lambd: 0.99</p>
			<p class="source-code">      num_epoch: 3</p>
			<p class="source-code">      learning_rate_schedule: linear</p>
			<p class="source-code">      beta_schedule: constant</p>
			<p class="source-code">      epsilon_schedule: linear</p>
			<p class="source-code">    network_settings:</p>
			<p class="source-code">      normalize: false</p>
			<p class="source-code">      hidden_units: 128</p>
			<p class="source-code">      num_layers: 2</p>
			<p class="source-code">    reward_signals:</p>
			<p class="source-code">      extrinsic:</p>
			<p class="source-code">        gamma: 0.99</p>
			<p class="source-code">        strength: 1.0</p>
			<p class="source-code">    max_steps: 500000</p>
			<p class="source-code">    time_horizon: 64</p>
			<p class="source-code">    summary_freq: 10000</p>
			<p>Then, we need to be sure to <a id="_idIndexMarker587"/>change the <strong class="bold">Behavior Type</strong> parameter in the sphere object to <strong class="bold">Default</strong>. </p>
			<p>Now, from the same folder, we should be able to run the following command:</p>
			<p class="source-code">mlagents-learn sphere.yaml --run-id=myMovingSphere</p>
			<p><strong class="source-inline">run-id</strong> is a unique ID for your running session. If everything is going according to plan, you should see the <strong class="bold">Start training by pressing the Play button in the Unity Editor</strong> message on the terminal window at some point. Now, you can do as the message says and press <strong class="bold">Play</strong> in Unity.</p>
			<p>After the training is complete, you will find the trained model in the <strong class="source-inline">results/myMovingSphere/MovingSphere.onnx</strong> file (the <strong class="source-inline">results</strong> folder inside the folder, in which you run the <strong class="source-inline">mlagents-learn</strong> command).</p>
			<p>Copy this file inside your Unity project and then put this in the <strong class="bold">Model</strong> placeholder in the <strong class="bold">Behavior Parameters</strong> component of the sphere:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B17981_11_7.jpg" alt="Figure 11.7 – The trained MovingSphere model inside the behavior parameters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – The trained MovingSphere model inside the behavior parameters</p>
			<p>Now, if you press <strong class="bold">Play</strong>, the Sphere will move autonomously according to the training model. It is not something big and complex, but it is<a id="_idTextAnchor639"/><a id="_idTextAnchor640"/><a id="_idTextAnchor641"/> automated learning nevertheless.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor642"/>Summary</h1>
			<p>In this chapter, we barely scratched the surface of machine learning and how to use it for training Unity agents. We learned how to install Unity's official ML-Agents Toolkit, set up a learning environment, and trained the model. However, this is just a basic introduction to the ML-Agents Toolkit, and many unexplored directions are waiting for you. I encourage you to look at the ML-Agents official repository; it includes many interesting demo projects.</p>
			<p>In the next chapter, we will wrap everything up by developing an <a id="_idTextAnchor643"/>AI age<a id="_idTextAnchor644"/>nt into a more complex game demo.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor645"/>Further reading</h1>
			<ul>
				<li>For more information, I encourage you to check the in-depth documentation for ML-Agents in the official repository (<a href="https://github.com/Unity-Technologies/ml-agents/tree/master/docs">https://github.com/Unity-Technologies/ml-agents/tree/master/docs</a>).</li>
				<li>For a more in-depth (but still very accessible) introduction to reinforcement learning, there is a good article on freeCodeCamp (<a href="https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419">https://medium.freecodecamp.org/an-introduction-to-reinforcement-learning-4339519de419</a>).</li>
				<li>If you are willing to go even deeper into reinforcement learning, a perfect next step is <em class="italic">Deep Reinforcement Learning Hands-On,  Second Edition</em>, <em class="italic">Maxim</em> <em class="italic">Lapan</em>, <em class="italic">Packt Publishing</em>. </li>
			</ul>
		</div>
	</body></html>