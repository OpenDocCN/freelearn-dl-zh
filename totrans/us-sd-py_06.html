<html><head></head><body>
		<div id="_idContainer041">
			<h1 id="_idParaDest-74" class="chapter-number"><a id="_idTextAnchor117"/>6</h1>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor118"/>Using Stable Diffusion Models</h1>
			<p>When we start using Stable Diffusion models, we will immediately encounter different kinds of model files and will need to know how to convert a model file to the <span class="No-Break">desired format.</span></p>
			<p>In this chapter, we are going to get more familiar with Stable Diffusion model files, covering how to load models from the Hugging Face repository using model IDs. We’ll also provide sample code to load <strong class="source-inline">safetensors</strong> and <strong class="source-inline">.ckpt</strong> model files shared by the open <span class="No-Break">source community.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Loading the <span class="No-Break">Diffusers model</span></li>
				<li>Loading model checkpoints from safetensors and <span class="No-Break">ckpt files</span></li>
				<li>Using CKPT and safetensors files <span class="No-Break">with Diffusers</span></li>
				<li>Model <span class="No-Break">safety checker</span></li>
				<li>Converting checkpoint model files to the <span class="No-Break">Diffusers format</span></li>
				<li>Using Stable <span class="No-Break">Diffusion XL</span></li>
			</ul>
			<p>By the end of this chapter, you will have learned about the Stable Diffusion model file types and how to convert and load model files to a format that can be loaded <span class="No-Break">with Diffusers.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor119"/>Technical requirements</h1>
			<p>Before you start, make sure you have the <strong class="source-inline">safetensors</strong> <span class="No-Break">package installed:</span></p>
			<pre class="source-code">
pip install safetensors</pre>
			<p>The <strong class="source-inline">safetensors</strong> Python package offers a simple and efficient way to access, store, and share <span class="No-Break">tensors securely.</span></p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor120"/>Loading the Diffusers model</h1>
			<p>Instead of downloading model<a id="_idIndexMarker180"/> files manually, the Hugging Face Diffusers package provides a convenient way to access open source model files from a string-type model ID <span class="No-Break">like this:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.float16
)</pre>
			<p>When the preceding code is executed, if Diffusers can’t find the model files that are denoted by the model ID, the package will automatically reach out to the Hugging Face repository to download the model files and store them in a cache folder for <span class="No-Break">next time.</span></p>
			<p>By default, the cache files will be stored in the <span class="No-Break">following places:</span></p>
			<p><span class="No-Break">Windows:</span></p>
			<p><span class="No-Break"><strong class="source-inline">C:\Users\user_name\.cache\huggingface\hub</strong></span></p>
			<p><span class="No-Break">Linux:</span></p>
			<p><strong class="source-inline">\</strong><span class="No-Break"><strong class="source-inline">home\user_name\.cache\huggingface\hub</strong></span></p>
			<p>Using the default cache path is fine in the beginning, however, if your system driver is less than 512 GB, you will soon find those model files are eating up storage space. To avoid running out of storage, we may need to plan the model storage in advance. Diffusers provides a parameter for us to specify a custom path for storing the cached <span class="No-Break">weight files.</span></p>
			<p>The following is the preceding sample code with one more <span class="No-Break">parameter, </span><span class="No-Break"><strong class="source-inline">cache_dir</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.float16,
    cache_dir = r"D:\my_model_folder"
)</pre>
			<p>By specifying this <strong class="source-inline">cache_dir</strong> parameter, all auto-downloaded model and configuration files will be stored in the new location instead of eating up the system <span class="No-Break">disk drive.</span></p>
			<p>You might also notice that the sample code specifies a <strong class="source-inline">torch_dtytpe</strong> parameter to tell Diffusers to use <strong class="source-inline">torch.float16</strong>. By default, PyTorch uses <strong class="source-inline">torch.float32</strong> for matrix multiplications. For model inference, or in other words, at the stage of using Stable Diffusion to generate images, we can use the <strong class="source-inline">float16</strong> type to not only increase the speed by about 100% but also save GPU memory with almost <span class="No-Break">unnoticeable </span><span class="No-Break">difference.</span></p>
			<p>Usually, using models from Hugging Face is easy and safe. Hugging Face implements a safety checker to ensure the uploaded model files do not contain any malicious code that may harm <span class="No-Break">your </span><span class="No-Break"><a id="_idIndexMarker181"/></span><span class="No-Break">computer.</span></p>
			<p>Nevertheless, we can still use manually downloaded model files with Diffusers. Next, we are going to load various model files from the <span class="No-Break">local disk<a id="_idTextAnchor121"/>.</span></p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor122"/>Loading model checkpoints from safetensors and ckpt files</h1>
			<p>The <a id="_idIndexMarker182"/>complete model files are also <a id="_idIndexMarker183"/>called <strong class="bold">checkpoint</strong> data. If you read an article or document talking about downloading a checkpoint, they are talking about a Stable Diffusion <span class="No-Break">model file.</span></p>
			<p>There are many types of checkpoints, such as <strong class="source-inline">.ckpt</strong> files, <strong class="source-inline">safetensors</strong> files, and <span class="No-Break"><strong class="source-inline">diffusers</strong></span><span class="No-Break"> files:</span></p>
			<ul>
				<li><strong class="source-inline">.ckpt</strong> is the most basic <a id="_idIndexMarker184"/>file format and is compatible with most Stable Diffusion models. However, they are also the most vulnerable to <span class="No-Break">malicious attacks.</span></li>
				<li><strong class="source-inline">safetensors</strong> is a newer file format that is designed to be more secure than <strong class="source-inline">.ckpt</strong> files. The <strong class="source-inline">safetensors</strong> format is better in terms of security, speed, and usability compared with <strong class="source-inline">.ckpt</strong>. Safetensors has several features to prevent <span class="No-Break">code execution:</span><ul><li><strong class="bold">Restricted data types</strong>: Only specific data types, such as integers and tensors, are allowed to be stored. This eliminates the possibility of including code within the <span class="No-Break">saved data.</span></li><li><strong class="bold">Hashing</strong>: Each chunk of data is hashed, and the hash is stored alongside the data. Any modification to the data would change the hash, making it <span class="No-Break">instantly detectable.</span></li><li><strong class="bold">Isolation</strong>: Data is stored in an isolated environment, preventing interaction with other programs, and protecting your system from <span class="No-Break">potential exploits.</span></li></ul></li>
				<li>Diffusers files<a id="_idIndexMarker185"/> are the latest file format specifically crafted for seamless integration with the <strong class="source-inline">Diffusers</strong> library. This format boasts top-notch security features and ensures compatibility with all Stable Diffusion models. Unlike traditional compression into a single file, the Diffusers format takes the form of a folder that encompasses both weights and configuration files. Moreover, the model files contained within these folders adhere to the <span class="No-Break"><strong class="source-inline">safetensors</strong></span><span class="No-Break"> format.</span></li>
			</ul>
			<p>When we use the Diffusers auto download function, Diffusers will store the files in the <span class="No-Break">Diffusers format.</span></p>
			<p>Next, we are going to load up a Stable Diffusion model in <strong class="source-inline">ckpt</strong> or <span class="No-Break"><strong class="source-inline">safetensors</strong></span><span class="No-Break"> fo<a id="_idTextAnchor123"/>rmat.</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor124"/>Using ckpt and safetensors files with Diffusers</h1>
			<p>The<a id="_idIndexMarker186"/> Diffusers community is<a id="_idIndexMarker187"/> actively enhancing the functionality. At<a id="_idIndexMarker188"/> the time of <a id="_idIndexMarker189"/>writing, we can easily load <strong class="source-inline">.ckpt</strong> or <strong class="source-inline">safetensors</strong> checkpoint files using the <span class="No-Break"><strong class="source-inline">Diffusers</strong></span><span class="No-Break"> package.</span></p>
			<p>The following code can be used to load and use a <strong class="source-inline">safetensors</strong> or <strong class="source-inline">.ckpt</strong> <span class="No-Break">checkpoint file.</span></p>
			<p>Load the <span class="No-Break"><strong class="source-inline">safetensors</strong></span><span class="No-Break"> model:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
model_path = r"model/path/path/model_name.safetensors"
pipe = StableDiffusionPipeline.from_single_file(
    model_path,
    torch_dtype = torch.float16
)</pre>
			<p>Load the <strong class="source-inline">.ckpt</strong> model with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
model_path = r"model/path/path/model_name.ckpt"
pipe = StableDiffusionPipeline.from_single_file(
    model_path,
    torch_dtype = torch.float16
)</pre>
			<p>You are not <a id="_idIndexMarker190"/>reading the wrong code; we can load both <strong class="source-inline">safetensors</strong> and <strong class="source-inline">.ckpt</strong> model<a id="_idIndexMarker191"/> files <a id="_idIndexMarker192"/>with the <a id="_idIndexMarker193"/>same function – <strong class="source-inline">from_single_file</strong>. Next, let’s take a look at the <span class="No-Break">sa<a id="_idTextAnchor125"/>fety checker.</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor126"/>Turning off the model safety checker</h1>
			<p>By default, the Diffusers pipeline <a id="_idIndexMarker194"/>will check the output result with a safety checker model to ensure the generated result does not include any NSFW, violent, or unsafe content. In certain cases, the safety checker may trigger false alarms and produce empty images (completely black images). There are several GitHub issue discussions about the safety checker [11]. In the test stage, we can temporarily turn off the <span class="No-Break">safety checker.</span></p>
			<p>To turn off the safety checker when loading the model using the model ID, run the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype    = torch.float16,
    safety_checker = None # or load_safety_checker = False
)</pre>
			<p>Note that the <a id="_idIndexMarker195"/>parameter to turn off the safety checker is different when we are loading the model from a <strong class="source-inline">safetensors</strong> or <strong class="source-inline">.ckpt</strong> file. Instead of using <strong class="source-inline">safety_checker</strong>, we should use <strong class="source-inline">load_safety_checker</strong> as shown in the following <span class="No-Break">sample code:</span></p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
model_path = r"model/path/path/model_name.ckpt"
pipe = StableDiffusionPipeline.from_single_file(
    model_path,
    torch_dtype = torch.float16,
    load_safety_checker = False
)</pre>
			<p>You should be able to use <strong class="source-inline">load_safety_checker = False</strong> in the <strong class="source-inline">from_pretrained</strong> function to disable the <span class="No-Break">safety checker.</span></p>
			<p>The safety checker is an<a id="_idIndexMarker196"/> open source machine learning model from CompVis – Computer Vision and Learning LMU Munich (<a href="https://github.com/CompVis">https://github.com/CompVis</a>), built based on CLIP [9][10], called <strong class="bold">Stable Diffusion Safety </strong><span class="No-Break"><strong class="bold">Checker</strong></span><span class="No-Break"> [3].</span></p>
			<p>While we can load a model in a single file, in <a id="_idIndexMarker197"/>some cases, we will need to convert a <strong class="source-inline">.ckpt</strong> or <strong class="source-inline">safetensors</strong> model file to the Diffusers folder structure. Next, let’s see how we can convert model files to the <a id="_idTextAnchor127"/><span class="No-Break">Diffusers format.</span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor128"/>Converting the checkpoint model file to the Diffusers format</h1>
			<p>Loading<a id="_idIndexMarker198"/> checkpoint model data from <a id="_idIndexMarker199"/>a <strong class="source-inline">.ckpt</strong> or <strong class="source-inline">safetensors</strong> file is slow compared with the Diffusers format because every time we load a <strong class="source-inline">.ckpt</strong> or <strong class="source-inline">safetensors</strong> file, Diffusers will unpack and convert the file to the Diffusers format. To save the conversion every time we load a model file, we may consider converting checkpoint files to the <span class="No-Break">Diffusers format.</span></p>
			<p>We can use the following code to convert a <strong class="source-inline">.ckpt</strong> file to the <span class="No-Break">Diffusers format:</span></p>
			<pre class="source-code">
ckpt_checkpoint_path = r"D:\temp\anythingV3_fp16.ckpt"
target_part = r"D:\temp\anythingV3_fp16"
pipe = download_from_original_stable_diffusion_ckpt(
    ckpt_checkpoint_path,
    from_safetensors = False,
    device = "cuda:0"
)
pipe.save_pretrained(target_part)</pre>
			<p>To convert a <strong class="source-inline">safetensors</strong> file to the Diffusers format, simply change the <strong class="source-inline">from_safetensors</strong> parameter to <strong class="source-inline">True</strong> as shown in the following <span class="No-Break">sample code:</span></p>
			<pre class="source-code">
from diffusers.pipelines.stable_diffusion.convert_from_ckpt import \
    download_from_original_stable_diffusion_ckpt
safetensors_checkpoint_path = \
    r"D:\temp\deliberate_v2.safetensors"
target_part = r"D:\temp\deliberate_v2"
pipe = download_from_original_stable_diffusion_ckpt(
    safetensors_checkpoint_path, 
    from_safetensors  = True,
    device = "cuda:0"
)
pipe.save_pretrained(target_part)</pre>
			<p>If you <a id="_idIndexMarker200"/>have tried asking a search <a id="_idIndexMarker201"/>engine to find a solution to do the conversion, from some corners of the internet, you may see a solution that uses a script called <strong class="source-inline">convert_original_stable_diffusion_to_diffusers.py</strong>. The script is located in the Diffusers GitHub repository: <a href="https://github.com/huggingface/diffusers/tree/main/scripts">https://github.com/huggingface/diffusers/tree/main/scripts</a>. The script works well. If you look at the code of the script, the script uses the same code <span class="No-Break">presented previously.</span></p>
			<p>To use the converted model file, simply use the <strong class="source-inline">from_pretrained</strong> function to load the <strong class="source-inline">local</strong> folder (instead of the model ID) <span class="No-Break">this time:</span></p>
			<pre class="source-code">
# load local diffusers model files using from_pretrained function
import torch
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    r"D:\temp\deliberate_v2",
    torch_dtype = torch.float16,
    safety_checker = None
).to("cuda:0")
image = pipe("a cute puppy").images[0]
image</pre>
			<p>You <a id="_idIndexMarker202"/>should see a cute puppy image <a id="_idIndexMarker203"/>generated by the preceding code. Next, let’s load Stab<a id="_idTextAnchor129"/>le Diffusion <span class="No-Break">XL models.</span></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor130"/>Using Stable Diffusion XL</h1>
			<p><strong class="bold">Stable Diffusion XL</strong> (<strong class="bold">SDXL</strong>) is a <a id="_idIndexMarker204"/>model from Stability AI. Slightly different compared to previous models, SDXL is designed to be a two-stage model. We will need the base model to generate an image and can leverage a second, refiner model to refine an image, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>. The refiner model <span class="No-Break">is optional</span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B21263_06_01.jpg" alt="Figure 6.1: SDXL, a two-model pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1: SDXL, a two-model pipeline</p>
			<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em> shows<a id="_idIndexMarker205"/> that to generate images of the best quality from the SDXL model, we will need to use the base model to generate a raw image, output as a 128x128 latent, and then use the refiner model to <span class="No-Break">enhance it.</span></p>
			<p>Before trying out the SDXL model, please ensure you have at least 15 GB of VRAM, otherwise, you may see a <strong class="source-inline">CUDA out of memory</strong> error right before the refiner model outputs the image. You can also use the optimization methods from <a href="B21263_05.xhtml#_idTextAnchor097"><span class="No-Break"><em class="italic">Chapter </em></span><span class="No-Break"><em class="italic">5</em></span></a>,<em class="italic"> </em>to build a custom pipeline to move the model out of VRAM <span class="No-Break">whenever possible.</span></p>
			<p>Here are the steps to load up an <span class="No-Break">SDXL model:</span></p>
			<ol>
				<li>Download the base model <strong class="source-inline">safetensors</strong> file [6]. You don’t need to download all files from the model repository. At the time of writing this, the checkpoint name <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">sd_xl_base_1.0.safetensors</strong></span><span class="No-Break">.</span></li>
				<li>Download the refiner model <strong class="source-inline">safetensors</strong> file [7]. We can also let the Diffusers pipeline download the <strong class="source-inline">safetensors</strong> file for us by providing the <span class="No-Break">model ID.</span></li>
				<li>Next, we will<a id="_idIndexMarker206"/> initialize the base and refiner models from the <span class="No-Break"><strong class="source-inline">safetensors</strong></span><span class="No-Break"> files:</span><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import (</pre><pre class="source-code">
    StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline)</pre><pre class="source-code">
# load base model</pre><pre class="source-code">
base_model_checkpoint_path = \ </pre><pre class="source-code">
    r"path/to/sd_xl_base_1.0.safetensors"</pre><pre class="source-code">
base_pipe = StableDiffusionXLPipeline.from_single_file(</pre><pre class="source-code">
    base_model_checkpoint_path,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    use_safetensors = True</pre><pre class="source-code">
)</pre><pre class="source-code">
# load refiner model</pre><pre class="source-code">
refiner_model_checkpoint_path = \</pre><pre class="source-code">
    r"path/to/sd_xl_refiner_1.0.safetensors"</pre><pre class="source-code">
refiner_pipe = \</pre><pre class="source-code">
    StableDiffusionXLImg2ImgPipeline.from_single_file(</pre><pre class="source-code">
    refiner_model_checkpoint_path,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    use_safetensors = True</pre><pre class="source-code">
)</pre><p class="list-inset">Or, we<a id="_idIndexMarker207"/> can initialize the base and refiner models using <span class="No-Break">model ID:</span></p><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import (</pre><pre class="source-code">
    StableDiffusionXLPipeline,</pre><pre class="source-code">
    StableDiffusionXLImg2ImgPipeline</pre><pre class="source-code">
)</pre><pre class="source-code">
# load base model</pre><pre class="source-code">
base_model_id = "stabilityai/stable-diffusion-xl-base-1.0"</pre><pre class="source-code">
base_pipe = StableDiffusionXLPipeline.from_pretrained(</pre><pre class="source-code">
    base_model_id,</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
)</pre><pre class="source-code">
# load refiner model</pre><pre class="source-code">
refiner_model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"</pre><pre class="source-code">
refiner_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(</pre><pre class="source-code">
    refiner_model_id,</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
)</pre></li>
				<li>Let’s <a id="_idIndexMarker208"/>generate the base image in latent space (the 4x128x128 middle <span class="No-Break">layer latent):</span><pre class="source-code">
# move model to cuda and generate base image latent</pre><pre class="source-code">
from diffusers import EulerDiscreteScheduler</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
analog photograph of a cat in a spacesuit taken inside the cockpit of a stealth fighter jet,</pre><pre class="source-code">
Fujifilm, Kodak Portra 400, vintage photography</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = """</pre><pre class="source-code">
paint, watermark, 3D render, illustration, drawing,worst quality, low quality</pre><pre class="source-code">
"""</pre><pre class="source-code">
base_pipe.to("cuda")</pre><pre class="source-code">
base_pipe.scheduler = EulerDiscreteScheduler.from_config(</pre><pre class="source-code">
    base_pipe.scheduler.config)</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    base_latents = base_pipe(</pre><pre class="source-code">
        prompt = prompt,</pre><pre class="source-code">
        negative_prompt = neg_prompt,</pre><pre class="source-code">
        output_type = "latent"</pre><pre class="source-code">
    ).images[0]</pre><pre class="source-code">
base_pipe.to("cpu")</pre><pre class="source-code">
torch.cuda.empty_cache()</pre><p class="list-inset">Note that at the<a id="_idIndexMarker209"/> end of the preceding code, we moved <strong class="source-inline">base_pipe</strong> out of VRAM by using <strong class="source-inline">base_pipe.to("cpu")</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">torch.cuda.empty_cache()</strong></span><span class="No-Break">.</span></p></li>
				<li>Load the refiner model to VRAM and use the base image in latent space to generate the <span class="No-Break">final image:</span><pre class="source-code">
# refine the image</pre><pre class="source-code">
refiner_pipe.to("cuda")</pre><pre class="source-code">
refiner_pipe.scheduler = EulerDiscreteScheduler.from_config(</pre><pre class="source-code">
    refiner_pipe.scheduler.config)</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    image = refiner_pipe(</pre><pre class="source-code">
        prompt = prompt,</pre><pre class="source-code">
        negative_prompt = neg_prompt,</pre><pre class="source-code">
        image = [base_latents]</pre><pre class="source-code">
    ).images[0]</pre><pre class="source-code">
refiner_pipe.to("cpu")</pre><pre class="source-code">
torch.cuda.empty_cache()</pre><pre class="source-code">
image</pre><p class="list-inset">The result<a id="_idIndexMarker210"/> will be similar to the one shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B21263_06_02.jpg" alt="Figure 6.2: Image generated by SDXL – a cat in a spacesuit"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2: Image generated by SDXL – a cat in a spacesuit</p>
			<p>The <a id="_idIndexMarker211"/>detail and quality are way better than the one rendered by Stable Diffusion 1.5. While this model is relatively new at the time of writing, in the near future, more mixed checkpoint models<a id="_idTextAnchor131"/> and Low-Rank Adapters (LoRAs) will <span class="No-Break">be available.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor132"/>Summary</h1>
			<p>This chapter mainly focused on the usage of the Stable Diffusion model. We can utilize a model from Hugging Face by using its model ID. Additionally, widely distributed open source models are available on community websites such as CIVITAI [4], where you can download numerous model resources. These model files are typically in the <strong class="source-inline">.ckpt</strong> or <strong class="source-inline">safetensors</strong> <span class="No-Break">file format.</span></p>
			<p>The chapter covered the distinction between these model files and using checkpoint model files directly from the <strong class="source-inline">Diffusers</strong> package. Furthermore, it offered a solution to convert standalone model checkpoint files to the Diffusers format for faster <span class="No-Break">model loading.</span></p>
			<p>Lastly, this chapter also covered how to load and us<a id="_idTextAnchor133"/>e SDXL’s <span class="No-Break">two-model pipelines.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor134"/>References</h1>
			<ol>
				<li>Hugging Face Load <span class="No-Break">safetensors: </span><a href="https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors"><span class="No-Break">https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors</span></a><a href="https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors&#13;"/></li>
				<li>pickle — Python object <span class="No-Break">serialization: </span><a href="https://docs.python.org/3/library/pickle.html"><span class="No-Break">https://docs.python.org/3/library/pickle.html</span></a><a href="https://docs.python.org/3/library/pickle.html&#13;"/></li>
				<li>Stable Diffusion Safety <span class="No-Break">Checker: </span><a href="https://huggingface.co/CompVis/stable-diffusion-safety-checker&#13;"><span class="No-Break">https://huggingface.co/CompVis/stable-diffusion-safety-checker</span></a></li>
				<li><span class="No-Break">civitai: </span><a href="https://www.civitai.com&#13;"><span class="No-Break">https://www.civitai.com</span></a></li>
				<li><span class="No-Break">stability.ai: </span><a href="https://stability.ai/"><span class="No-Break">https://stability.ai/</span></a></li>
				<li><span class="No-Break">stable-diffusion-xl-base-1.0: </span><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#13;"><span class="No-Break">https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0</span></a></li>
				<li><span class="No-Break">stable-diffusion-xl-refiner-1.0: </span><a href="https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0&#13;"><span class="No-Break">https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0</span></a></li>
				<li>safetensors GitHub <span class="No-Break">repository: </span><a href="https://github.com/huggingface/safetensors"><span class="No-Break">https://github.com/huggingface/safetensors</span></a></li>
				<li>Alec Radford et al, Learning Transferable Visual Models From Natural Language <span class="No-Break">Supervision: </span><a href="https://arxiv.org/abs/2103.00020"><span class="No-Break">https://arxiv.org/abs/2103.00020</span></a></li>
				<li>OpenAI CLIP GitHub <span class="No-Break">repository: </span><a href="https://github.com/openai/CLIP"><span class="No-Break">https://github.com/openai/CLIP</span></a></li>
				<li>Issues with safety checker: <a href="https://github.com/huggingface/diffusers/issues/845"><span class="No-Break">https://github.com/huggingface/diffusers/issues/845</span></a><span class="No-Break">, </span><a href="https://github.com/huggingface/diffusers/issues/3422"><span class="No-Break">https://github.com/huggingface/diffusers/issues/3422</span></a></li>
			</ol>
		</div>
	

		<div id="_idContainer042" class="Content">
			<h1 id="_idParaDest-85" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor135"/>Part 2 – Improving Diffusers with Custom Features</h1>
			<p>In Part 1, we explored the fundamental concepts and techniques behind diffusers, setting the stage for their application in various domains. Now, it’s time to take our understanding to the next level by delving into advanced customization options that can significantly enhance the capabilities of these <span class="No-Break">powerful models.</span></p>
			<p>The chapters in this section are designed to equip you with the knowledge and skills necessary to optimize and extend your diffusers, unlocking new possibilities for creative expression and problem-solving. From refining performance and managing VRAM usage to leveraging community-driven resources and exploring innovative techniques such as textual inversion, we’ll cover a range of topics that will help you push the boundaries of what’s possible <span class="No-Break">with diffusers.</span></p>
			<p>Through the following chapters, you’ll learn how to overcome limitations, tap into the collective wisdom of the community, and unlock new features that will elevate your work with diffusers. Whether you’re seeking to improve efficiency, explore new artistic avenues, or simply stay at the forefront of innovation, the custom features and techniques presented in this part of the book will provide you with the tools and inspiration you need <span class="No-Break">to succeed.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21263_07.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a><em class="italic">, Optimizing Performance and VRAM Usage</em></li>
				<li><a href="B21263_08.xhtml#_idTextAnchor153"><em class="italic">Chapter 8</em></a><em class="italic">, Using Community-Shared LoRAs</em></li>
				<li><a href="B21263_09.xhtml#_idTextAnchor178"><em class="italic">Chapter 9</em></a><em class="italic">, Using Textual Inversion</em></li>
				<li><a href="B21263_10.xhtml#_idTextAnchor197"><em class="italic">Chapter 10</em></a><em class="italic">, Unlocking 77 Token Limitations and Enabling Prompt Weighting</em></li>
				<li><a href="B21263_11.xhtml#_idTextAnchor214"><em class="italic">Chapter 11</em></a><em class="italic">, Image Restore and Super-Resolution</em></li>
				<li><a href="B21263_12.xhtml#_idTextAnchor240"><em class="italic">Chapter 12</em></a><em class="italic">, Scheduled Prompt Parsing</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer043">
			</div>
		</div>
		<div>
			<div id="_idContainer044" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer045" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer046">
			</div>
		</div>
		<div>
			<div id="_idContainer047" class="Basic-Graphics-Frame">
			</div>
		</div>
	</body></html>