<html><head></head><body>
		<div><h1 id="_idParaDest-74" class="chapter-number"><a id="_idTextAnchor117"/>6</h1>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor118"/>Using Stable Diffusion Models</h1>
			<p>When we start using Stable Diffusion models, we will immediately encounter different kinds of model files and will need to know how to convert a model file to the desired format.</p>
			<p>In this chapter, we are going to get more familiar with Stable Diffusion model files, covering how to load models from the Hugging Face repository using model IDs. We’ll also provide sample code to load <code>safetensors</code> and <code>.ckpt</code> model files shared by the open source community.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Loading the Diffusers model</li>
				<li>Loading model checkpoints from safetensors and ckpt files</li>
				<li>Using CKPT and safetensors files with Diffusers</li>
				<li>Model safety checker</li>
				<li>Converting checkpoint model files to the Diffusers format</li>
				<li>Using Stable Diffusion XL</li>
			</ul>
			<p>By the end of this chapter, you will have learned about the Stable Diffusion model file types and how to convert and load model files to a format that can be loaded with Diffusers.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor119"/>Technical requirements</h1>
			<p>Before you start, make sure you have the <code>safetensors</code> package installed:</p>
			<pre class="source-code">
pip install safetensors</pre>
			<p>The <code>safetensors</code> Python package offers a simple and efficient way to access, store, and share tensors securely.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor120"/>Loading the Diffusers model</h1>
			<p>Instead of downloading model<a id="_idIndexMarker180"/> files manually, the Hugging Face Diffusers package provides a convenient way to access open source model files from a string-type model ID like this:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.float16
)</pre>
			<p>When the preceding code is executed, if Diffusers can’t find the model files that are denoted by the model ID, the package will automatically reach out to the Hugging Face repository to download the model files and store them in a cache folder for next time.</p>
			<p>By default, the cache files will be stored in the following places:</p>
			<p>Windows:</p>
			<p><code>C:\Users\user_name\.cache\huggingface\hub</code></p>
			<p>Linux:</p>
			<p><code>\</code><code>home\user_name\.cache\huggingface\hub</code></p>
			<p>Using the default cache path is fine in the beginning, however, if your system driver is less than 512 GB, you will soon find those model files are eating up storage space. To avoid running out of storage, we may need to plan the model storage in advance. Diffusers provides a parameter for us to specify a custom path for storing the cached weight files.</p>
			<p>The following is the preceding sample code with one more parameter, <code>cache_dir</code>:</p>
			<pre class="source-code">
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.float16,
    cache_dir = r"D:\my_model_folder"
)</pre>
			<p>By specifying this <code>cache_dir</code> parameter, all auto-downloaded model and configuration files will be stored in the new location instead of eating up the system disk drive.</p>
			<p>You might also notice that the sample code specifies a <code>torch_dtytpe</code> parameter to tell Diffusers to use <code>torch.float16</code>. By default, PyTorch uses <code>torch.float32</code> for matrix multiplications. For model inference, or in other words, at the stage of using Stable Diffusion to generate images, we can use the <code>float16</code> type to not only increase the speed by about 100% but also save GPU memory with almost unnoticeable difference.</p>
			<p>Usually, using models from Hugging Face is easy and safe. Hugging Face implements a safety checker to ensure the uploaded model files do not contain any malicious code that may harm your <a id="_idIndexMarker181"/>computer.</p>
			<p>Nevertheless, we can still use manually downloaded model files with Diffusers. Next, we are going to load various model files from the local disk<a id="_idTextAnchor121"/>.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor122"/>Loading model checkpoints from safetensors and ckpt files</h1>
			<p>The <a id="_idIndexMarker182"/>complete model files are also <a id="_idIndexMarker183"/>called <strong class="bold">checkpoint</strong> data. If you read an article or document talking about downloading a checkpoint, they are talking about a Stable Diffusion model file.</p>
			<p>There are many types of checkpoints, such as <code>.ckpt</code> files, <code>safetensors</code> files, and <code>diffusers</code> files:</p>
			<ul>
				<li><code>.ckpt</code> is the most basic <a id="_idIndexMarker184"/>file format and is compatible with most Stable Diffusion models. However, they are also the most vulnerable to malicious attacks.</li>
				<li><code>safetensors</code> is a newer file format that is designed to be more secure than <code>.ckpt</code> files. The <code>safetensors</code> format is better in terms of security, speed, and usability compared with <code>.ckpt</code>. Safetensors has several features to prevent code execution:<ul><li><strong class="bold">Restricted data types</strong>: Only specific data types, such as integers and tensors, are allowed to be stored. This eliminates the possibility of including code within the saved data.</li><li><strong class="bold">Hashing</strong>: Each chunk of data is hashed, and the hash is stored alongside the data. Any modification to the data would change the hash, making it instantly detectable.</li><li><strong class="bold">Isolation</strong>: Data is stored in an isolated environment, preventing interaction with other programs, and protecting your system from potential exploits.</li></ul></li>
				<li>Diffusers files<a id="_idIndexMarker185"/> are the latest file format specifically crafted for seamless integration with the <code>Diffusers</code> library. This format boasts top-notch security features and ensures compatibility with all Stable Diffusion models. Unlike traditional compression into a single file, the Diffusers format takes the form of a folder that encompasses both weights and configuration files. Moreover, the model files contained within these folders adhere to the <code>safetensors</code> format.</li>
			</ul>
			<p>When we use the Diffusers auto download function, Diffusers will store the files in the Diffusers format.</p>
			<p>Next, we are going to load up a Stable Diffusion model in <code>ckpt</code> or <code>safetensors</code> fo<a id="_idTextAnchor123"/>rmat.</p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor124"/>Using ckpt and safetensors files with Diffusers</h1>
			<p>The<a id="_idIndexMarker186"/> Diffusers community is<a id="_idIndexMarker187"/> actively enhancing the functionality. At<a id="_idIndexMarker188"/> the time of <a id="_idIndexMarker189"/>writing, we can easily load <code>.ckpt</code> or <code>safetensors</code> checkpoint files using the <code>Diffusers</code> package.</p>
			<p>The following code can be used to load and use a <code>safetensors</code> or <code>.ckpt</code> checkpoint file.</p>
			<p>Load the <code>safetensors</code> model:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
model_path = r"model/path/path/model_name.safetensors"
pipe = StableDiffusionPipeline.from_single_file(
    model_path,
    torch_dtype = torch.float16
)</pre>
			<p>Load the <code>.ckpt</code> model with the following code:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
model_path = r"model/path/path/model_name.ckpt"
pipe = StableDiffusionPipeline.from_single_file(
    model_path,
    torch_dtype = torch.float16
)</pre>
			<p>You are not <a id="_idIndexMarker190"/>reading the wrong code; we can load both <code>safetensors</code> and <code>.ckpt</code> model<a id="_idIndexMarker191"/> files <a id="_idIndexMarker192"/>with the <a id="_idIndexMarker193"/>same function – <code>from_single_file</code>. Next, let’s take a look at the sa<a id="_idTextAnchor125"/>fety checker.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor126"/>Turning off the model safety checker</h1>
			<p>By default, the Diffusers pipeline <a id="_idIndexMarker194"/>will check the output result with a safety checker model to ensure the generated result does not include any NSFW, violent, or unsafe content. In certain cases, the safety checker may trigger false alarms and produce empty images (completely black images). There are several GitHub issue discussions about the safety checker [11]. In the test stage, we can temporarily turn off the safety checker.</p>
			<p>To turn off the safety checker when loading the model using the model ID, run the following code:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype    = torch.float16,
    safety_checker = None # or load_safety_checker = False
)</pre>
			<p>Note that the <a id="_idIndexMarker195"/>parameter to turn off the safety checker is different when we are loading the model from a <code>safetensors</code> or <code>.ckpt</code> file. Instead of using <code>safety_checker</code>, we should use <code>load_safety_checker</code> as shown in the following sample code:</p>
			<pre class="source-code">
import torch
from diffusers import StableDiffusionPipeline
model_path = r"model/path/path/model_name.ckpt"
pipe = StableDiffusionPipeline.from_single_file(
    model_path,
    torch_dtype = torch.float16,
    load_safety_checker = False
)</pre>
			<p>You should be able to use <code>load_safety_checker = False</code> in the <code>from_pretrained</code> function to disable the safety checker.</p>
			<p>The safety checker is an<a id="_idIndexMarker196"/> open source machine learning model from CompVis – Computer Vision and Learning LMU Munich (<a href="https://github.com/CompVis">https://github.com/CompVis</a>), built based on CLIP [9][10], called <strong class="bold">Stable Diffusion Safety </strong><strong class="bold">Checker</strong> [3].</p>
			<p>While we can load a model in a single file, in <a id="_idIndexMarker197"/>some cases, we will need to convert a <code>.ckpt</code> or <code>safetensors</code> model file to the Diffusers folder structure. Next, let’s see how we can convert model files to the <a id="_idTextAnchor127"/>Diffusers format.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor128"/>Converting the checkpoint model file to the Diffusers format</h1>
			<p>Loading<a id="_idIndexMarker198"/> checkpoint model data from <a id="_idIndexMarker199"/>a <code>.ckpt</code> or <code>safetensors</code> file is slow compared with the Diffusers format because every time we load a <code>.ckpt</code> or <code>safetensors</code> file, Diffusers will unpack and convert the file to the Diffusers format. To save the conversion every time we load a model file, we may consider converting checkpoint files to the Diffusers format.</p>
			<p>We can use the following code to convert a <code>.ckpt</code> file to the Diffusers format:</p>
			<pre class="source-code">
ckpt_checkpoint_path = r"D:\temp\anythingV3_fp16.ckpt"
target_part = r"D:\temp\anythingV3_fp16"
pipe = download_from_original_stable_diffusion_ckpt(
    ckpt_checkpoint_path,
    from_safetensors = False,
    device = "cuda:0"
)
pipe.save_pretrained(target_part)</pre>
			<p>To convert a <code>safetensors</code> file to the Diffusers format, simply change the <code>from_safetensors</code> parameter to <code>True</code> as shown in the following sample code:</p>
			<pre class="source-code">
from diffusers.pipelines.stable_diffusion.convert_from_ckpt import \
    download_from_original_stable_diffusion_ckpt
safetensors_checkpoint_path = \
    r"D:\temp\deliberate_v2.safetensors"
target_part = r"D:\temp\deliberate_v2"
pipe = download_from_original_stable_diffusion_ckpt(
    safetensors_checkpoint_path, 
    from_safetensors  = True,
    device = "cuda:0"
)
pipe.save_pretrained(target_part)</pre>
			<p>If you <a id="_idIndexMarker200"/>have tried asking a search <a id="_idIndexMarker201"/>engine to find a solution to do the conversion, from some corners of the internet, you may see a solution that uses a script called <code>convert_original_stable_diffusion_to_diffusers.py</code>. The script is located in the Diffusers GitHub repository: <a href="https://github.com/huggingface/diffusers/tree/main/scripts">https://github.com/huggingface/diffusers/tree/main/scripts</a>. The script works well. If you look at the code of the script, the script uses the same code presented previously.</p>
			<p>To use the converted model file, simply use the <code>from_pretrained</code> function to load the <code>local</code> folder (instead of the model ID) this time:</p>
			<pre class="source-code">
# load local diffusers model files using from_pretrained function
import torch
from diffusers import StableDiffusionPipeline
pipe = StableDiffusionPipeline.from_pretrained(
    r"D:\temp\deliberate_v2",
    torch_dtype = torch.float16,
    safety_checker = None
).to("cuda:0")
image = pipe("a cute puppy").images[0]
image</pre>
			<p>You <a id="_idIndexMarker202"/>should see a cute puppy image <a id="_idIndexMarker203"/>generated by the preceding code. Next, let’s load Stab<a id="_idTextAnchor129"/>le Diffusion XL models.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor130"/>Using Stable Diffusion XL</h1>
			<p><strong class="bold">Stable Diffusion XL</strong> (<strong class="bold">SDXL</strong>) is a <a id="_idIndexMarker204"/>model from Stability AI. Slightly different compared to previous models, SDXL is designed to be a two-stage model. We will need the base model to generate an image and can leverage a second, refiner model to refine an image, as shown in <em class="italic">Figure 6</em><em class="italic">.1</em>. The refiner model is optional:</p>
			<div><div><img src="img/B21263_06_01.jpg" alt="Figure 6.1: SDXL, a two-model pipeline"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1: SDXL, a two-model pipeline</p>
			<p><em class="italic">Figure 6</em><em class="italic">.1</em> shows<a id="_idIndexMarker205"/> that to generate images of the best quality from the SDXL model, we will need to use the base model to generate a raw image, output as a 128x128 latent, and then use the refiner model to enhance it.</p>
			<p>Before trying out the SDXL model, please ensure you have at least 15 GB of VRAM, otherwise, you may see a <code>CUDA out of memory</code> error right before the refiner model outputs the image. You can also use the optimization methods from <a href="B21263_05.xhtml#_idTextAnchor097"><em class="italic">Chapter </em><em class="italic">5</em></a>,<em class="italic"> </em>to build a custom pipeline to move the model out of VRAM whenever possible.</p>
			<p>Here are the steps to load up an SDXL model:</p>
			<ol>
				<li>Download the base model <code>safetensors</code> file [6]. You don’t need to download all files from the model repository. At the time of writing this, the checkpoint name is <code>sd_xl_base_1.0.safetensors</code>.</li>
				<li>Download the refiner model <code>safetensors</code> file [7]. We can also let the Diffusers pipeline download the <code>safetensors</code> file for us by providing the model ID.</li>
				<li>Next, we will<a id="_idIndexMarker206"/> initialize the base and refiner models from the <code>safetensors</code> files:<pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import (</pre><pre class="source-code">
    StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline)</pre><pre class="source-code">
# load base model</pre><pre class="source-code">
base_model_checkpoint_path = \ </pre><pre class="source-code">
    r"path/to/sd_xl_base_1.0.safetensors"</pre><pre class="source-code">
base_pipe = StableDiffusionXLPipeline.from_single_file(</pre><pre class="source-code">
    base_model_checkpoint_path,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    use_safetensors = True</pre><pre class="source-code">
)</pre><pre class="source-code">
# load refiner model</pre><pre class="source-code">
refiner_model_checkpoint_path = \</pre><pre class="source-code">
    r"path/to/sd_xl_refiner_1.0.safetensors"</pre><pre class="source-code">
refiner_pipe = \</pre><pre class="source-code">
    StableDiffusionXLImg2ImgPipeline.from_single_file(</pre><pre class="source-code">
    refiner_model_checkpoint_path,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    use_safetensors = True</pre><pre class="source-code">
)</pre><p class="list-inset">Or, we<a id="_idIndexMarker207"/> can initialize the base and refiner models using model ID:</p><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import (</pre><pre class="source-code">
    StableDiffusionXLPipeline,</pre><pre class="source-code">
    StableDiffusionXLImg2ImgPipeline</pre><pre class="source-code">
)</pre><pre class="source-code">
# load base model</pre><pre class="source-code">
base_model_id = "stabilityai/stable-diffusion-xl-base-1.0"</pre><pre class="source-code">
base_pipe = StableDiffusionXLPipeline.from_pretrained(</pre><pre class="source-code">
    base_model_id,</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
)</pre><pre class="source-code">
# load refiner model</pre><pre class="source-code">
refiner_model_id = "stabilityai/stable-diffusion-xl-refiner-1.0"</pre><pre class="source-code">
refiner_pipe = StableDiffusionXLImg2ImgPipeline.from_pretrained(</pre><pre class="source-code">
    refiner_model_id,</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
)</pre></li>
				<li>Let’s <a id="_idIndexMarker208"/>generate the base image in latent space (the 4x128x128 middle layer latent):<pre class="source-code">
# move model to cuda and generate base image latent</pre><pre class="source-code">
from diffusers import EulerDiscreteScheduler</pre><pre class="source-code">
prompt = """</pre><pre class="source-code">
analog photograph of a cat in a spacesuit taken inside the cockpit of a stealth fighter jet,</pre><pre class="source-code">
Fujifilm, Kodak Portra 400, vintage photography</pre><pre class="source-code">
"""</pre><pre class="source-code">
neg_prompt = """</pre><pre class="source-code">
paint, watermark, 3D render, illustration, drawing,worst quality, low quality</pre><pre class="source-code">
"""</pre><pre class="source-code">
base_pipe.to("cuda")</pre><pre class="source-code">
base_pipe.scheduler = EulerDiscreteScheduler.from_config(</pre><pre class="source-code">
    base_pipe.scheduler.config)</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    base_latents = base_pipe(</pre><pre class="source-code">
        prompt = prompt,</pre><pre class="source-code">
        negative_prompt = neg_prompt,</pre><pre class="source-code">
        output_type = "latent"</pre><pre class="source-code">
    ).images[0]</pre><pre class="source-code">
base_pipe.to("cpu")</pre><pre class="source-code">
torch.cuda.empty_cache()</pre><p class="list-inset">Note that at the<a id="_idIndexMarker209"/> end of the preceding code, we moved <code>base_pipe</code> out of VRAM by using <code>base_pipe.to("cpu")</code> and <code>torch.cuda.empty_cache()</code>.</p></li>
				<li>Load the refiner model to VRAM and use the base image in latent space to generate the final image:<pre class="source-code">
# refine the image</pre><pre class="source-code">
refiner_pipe.to("cuda")</pre><pre class="source-code">
refiner_pipe.scheduler = EulerDiscreteScheduler.from_config(</pre><pre class="source-code">
    refiner_pipe.scheduler.config)</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    image = refiner_pipe(</pre><pre class="source-code">
        prompt = prompt,</pre><pre class="source-code">
        negative_prompt = neg_prompt,</pre><pre class="source-code">
        image = [base_latents]</pre><pre class="source-code">
    ).images[0]</pre><pre class="source-code">
refiner_pipe.to("cpu")</pre><pre class="source-code">
torch.cuda.empty_cache()</pre><pre class="source-code">
image</pre><p class="list-inset">The result<a id="_idIndexMarker210"/> will be similar to the one shown in <em class="italic">Figure 6</em><em class="italic">.2</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_06_02.jpg" alt="Figure 6.2: Image generated by SDXL – a cat in a spacesuit"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2: Image generated by SDXL – a cat in a spacesuit</p>
			<p>The <a id="_idIndexMarker211"/>detail and quality are way better than the one rendered by Stable Diffusion 1.5. While this model is relatively new at the time of writing, in the near future, more mixed checkpoint models<a id="_idTextAnchor131"/> and Low-Rank Adapters (LoRAs) will be available.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor132"/>Summary</h1>
			<p>This chapter mainly focused on the usage of the Stable Diffusion model. We can utilize a model from Hugging Face by using its model ID. Additionally, widely distributed open source models are available on community websites such as CIVITAI [4], where you can download numerous model resources. These model files are typically in the <code>.ckpt</code> or <code>safetensors</code> file format.</p>
			<p>The chapter covered the distinction between these model files and using checkpoint model files directly from the <code>Diffusers</code> package. Furthermore, it offered a solution to convert standalone model checkpoint files to the Diffusers format for faster model loading.</p>
			<p>Lastly, this chapter also covered how to load and us<a id="_idTextAnchor133"/>e SDXL’s two-model pipelines.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor134"/>References</h1>
			<ol>
				<li>Hugging Face Load safetensors: <a href="https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors">https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors</a><a href="https://huggingface.co/docs/diffusers/using-diffusers/using_safetensors&#13;"/></li>
				<li>pickle — Python object serialization: <a href="https://docs.python.org/3/library/pickle.html">https://docs.python.org/3/library/pickle.html</a><a href="https://docs.python.org/3/library/pickle.html&#13;"/></li>
				<li>Stable Diffusion Safety Checker: <a href="https://huggingface.co/CompVis/stable-diffusion-safety-checker&#13;">https://huggingface.co/CompVis/stable-diffusion-safety-checker</a></li>
				<li>civitai: <a href="https://www.civitai.com&#13;">https://www.civitai.com</a></li>
				<li>stability.ai: <a href="https://stability.ai/">https://stability.ai/</a></li>
				<li>stable-diffusion-xl-base-1.0: <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0&#13;">https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0</a></li>
				<li>stable-diffusion-xl-refiner-1.0: <a href="https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0&#13;">https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0</a></li>
				<li>safetensors GitHub repository: <a href="https://github.com/huggingface/safetensors">https://github.com/huggingface/safetensors</a></li>
				<li>Alec Radford et al, Learning Transferable Visual Models From Natural Language Supervision: <a href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</a></li>
				<li>OpenAI CLIP GitHub repository: <a href="https://github.com/openai/CLIP">https://github.com/openai/CLIP</a></li>
				<li>Issues with safety checker: <a href="https://github.com/huggingface/diffusers/issues/845">https://github.com/huggingface/diffusers/issues/845</a>, <a href="https://github.com/huggingface/diffusers/issues/3422">https://github.com/huggingface/diffusers/issues/3422</a></li>
			</ol>
		</div>
	

		<div><h1 id="_idParaDest-85" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor135"/>Part 2 – Improving Diffusers with Custom Features</h1>
			<p>In Part 1, we explored the fundamental concepts and techniques behind diffusers, setting the stage for their application in various domains. Now, it’s time to take our understanding to the next level by delving into advanced customization options that can significantly enhance the capabilities of these powerful models.</p>
			<p>The chapters in this section are designed to equip you with the knowledge and skills necessary to optimize and extend your diffusers, unlocking new possibilities for creative expression and problem-solving. From refining performance and managing VRAM usage to leveraging community-driven resources and exploring innovative techniques such as textual inversion, we’ll cover a range of topics that will help you push the boundaries of what’s possible with diffusers.</p>
			<p>Through the following chapters, you’ll learn how to overcome limitations, tap into the collective wisdom of the community, and unlock new features that will elevate your work with diffusers. Whether you’re seeking to improve efficiency, explore new artistic avenues, or simply stay at the forefront of innovation, the custom features and techniques presented in this part of the book will provide you with the tools and inspiration you need to succeed.</p>
			<p>This part contains the following chapters:</p>
			<ul>
				<li><a href="B21263_07.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a><em class="italic">, Optimizing Performance and VRAM Usage</em></li>
				<li><a href="B21263_08.xhtml#_idTextAnchor153"><em class="italic">Chapter 8</em></a><em class="italic">, Using Community-Shared LoRAs</em></li>
				<li><a href="B21263_09.xhtml#_idTextAnchor178"><em class="italic">Chapter 9</em></a><em class="italic">, Using Textual Inversion</em></li>
				<li><a href="B21263_10.xhtml#_idTextAnchor197"><em class="italic">Chapter 10</em></a><em class="italic">, Unlocking 77 Token Limitations and Enabling Prompt Weighting</em></li>
				<li><a href="B21263_11.xhtml#_idTextAnchor214"><em class="italic">Chapter 11</em></a><em class="italic">, Image Restore and Super-Resolution</em></li>
				<li><a href="B21263_12.xhtml#_idTextAnchor240"><em class="italic">Chapter 12</em></a><em class="italic">, Scheduled Prompt Parsing</em></li>
			</ul>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
		<div><div></div>
		</div>
	</body></html>