["```py\nconda activate deepfakes\n```", "```py\n    import os\n    from argparse import ArgumentParser\n    import json_tricks\n    import torch\n    import cv2\n    import numpy as np\n    from tqdm import tqdm\n    import face_alignment\n    from face_alignment.detection.sfd import FaceDetector\n    from face_alignment import FaceAlignment, LandmarksType\n    from lib.bisenet import BiSeNet\n    from lib.models import OriginalEncoder, OriginalDecoder\n    ```", "```py\n    If __name__ == \"__main__\":\n      \"\"\" Process images, replacing the face with another as trained\n          Example CLI:\n          ------------\n          python C7-convert.py \"C:/media_files/\"\n      \"\"\"\n    ```", "```py\n      parser = ArgumentParser()\n      parser.add_argument(\"path\",\n        help=\"folder of images to convert\")\n      parser.add_argument(\"--model-path\",\n        default=\"model/\",\n        help=\"folder which has the trained model\")\n      parser.add_argument(\"--cpu\",\n        action=\"store_true\",\n        help=\"Force CPU usage\")\n      parser.add_argument(\"--swap\",\n        action=\"store_true\",\n        help=\"Convert to the first face instead of the second\")\n      parser.add_argument(\"--json-path\",\n       default=\"$path/face_images/face_alignments.json\",\n       help=\"path to the json data from the extract\")\n      parser.add_argument(\"--export-path\",\n        default=\"$path/convert/\",\n        help=\"folder to put images (swaps $path with input path)\")\n    ```", "```py\n  Opt = parser.parse_args()\n  opt.export_path = opt.export_path.replace(\"$path\", opt.path)\n  opt.json_path = opt.json_path.replace(\"$path\", opt.path)\n  main(opt)\n```", "```py\n    def main(opt):\n    ```", "```py\n    if not os.path.exists(opt.export_path):\n            os.mkdir(opt.export_path)\n    ```", "```py\n    device = \"cuda\" if torch.cuda.is_available() and not opt.cpu else \"cpu\"\n    ```", "```py\n    encoder = OriginalEncoder()\n    decoder = OriginalDecoder()\n    ```", "```py\n    encoder.load_state_dict(torch.load(\n      os.path.join( opt.model_path, \"encoder.pth\")).state_dict())\n    if not opt.swap:\n      decoder.load_state_dict(torch.load(\n       os.path.join(opt.model_path, \"decoderb.pth\")).state_dict())\n    else:\n      decoder.load_state_dict(torch.load(\n       os.path.join(opt.model_path, \"decodera.pth\")).state_dict())\n    ```", "```py\n    If device == \"cuda\":\n      encoder = encoder.cuda()\n      decoder = decoder.cuda()\n    ```", "```py\n    with open(os.path.join(json_path), \"r\", encoding=\"utf-8\") as alignment_file:\n      alignment_data = json_tricks.loads( alignment_file.read(), encoding=\"utf-8\")\n      alignment_keys = list(alignment_data.keys())\n    ```", "```py\n    list_of_images_in_dir = [file for file in os.listdir(opt.path)\n     if os.path.isfile(os.path.join(opt.path, file))\n     and file in alignment_keys]\n    ```", "```py\n    for file in tqdm(list_of_images_in_dir):\n      filename, extension = os.path.splitext(file)\n      image_bgr = cv2.imread(os.path.join(opt.path, file))\n      image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n      width, height, channels = image_bgr.shape\n      output_image = image_rgb\n    ```", "```py\n    for idx, face in enumerate(alignment_data[file]['faces']):\n      aligned_face = cv2.warpAffine(image_rgb, face[\"warp_matrix\"][:2], (256, 256))\n      aligned_face_tensor = torch.tensor(aligned_face/255,\n        dtype=torch.float32).permute(2, 0, 1)\n      aligned_face_tensor_small = torch.nn.functional.interpolate(\n        aligned_face_tensor.unsqueeze(0), size=(64,64),\n        mode='bilinear', align_corners=False)\n    if device == \"cuda\":\n      aligned_face_tensor_small = aligned_face_tensor_small.cuda()\n    ```", "```py\n    with torch.no_grad():\n      output_face_tensor = decoder( encoder(\n        aligned_face_tensor_small ))\n    ```", "```py\n    output_face_tensor = torch.nn.functional.interpolate(\n      output_face_tensor, size=(256,256))\n    mask_img = cv2.imread(os.path.join(extract_path,\n      f\"face_mask_{filename}_{idx}.png\"), 0)\n    mask_tensor = torch.where(torch.tensor(mask_img) >\n      200, 1, 0)\n    output_face_tensor = (output_face_tensor.cpu() *\n      mask_tensor) + ( aligned_face_tensor.cpu() * (1 –\n      mask_tensor))\n    ```", "```py\n    output_face = (output_face_tensor[0].permute(1,2,0).numpy() *\n      255).astype(np.uint8)\n    output_image = cv2.warpAffine(output_face,\n      face[\"warp_matrix\"][:2], (height, width), output_image,\n      borderMode = cv2.BORDER_TRANSPARENT,\n      flags = cv2.WARP_INVERSE_MAP)\n    ```", "```py\n    output_image = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)\n      cv2.imwrite(os.path.join(opt.export_path,\n        f\"{filename}.png\"), output_image)\n    ```", "```py\n    ffmpeg -i {path_to_convert}\\%05d.png Output.mp4\n    ```", "```py\n    ffmpeg -framerate {framerate} -i {path_to_convert}\\%05d.png\n      Output.mp4\n    ```", "```py\nffmpeg -i {OriginalVideo.mp4}\n```", "```py\nStream #0:0(eng): Video: hevc (Main) (hvc1 / 0x31637668), yuvj420p(pc, bt470bg/bt470bg/smpte170m), 1920x1080, 13661 kb/s, SAR 1:1 DAR 16:9, 59.32 fps, 59.94 tbr, 90k tbn, 90k tbc (default)\n```", "```py\n    ffmpeg -i {path_to_convert}\\%05d.png -i {OriginalVideo.mp4}\n      -map 0:v:0 -map 1:a:0 Output.mp4\n    ```"]