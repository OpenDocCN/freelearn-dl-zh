["```py\n    poetry poe create-sagemaker-role \n    ```", "```py\n    poetry poe create-sagemaker-execution-role \n    ```", "```py\ndef create_endpoint(endpoint_type=EndpointType.INFERENCE_COMPONENT_BASED):\n    llm_image = get_huggingface_llm_image_uri(\"huggingface\", version=None)\n    resource_manager = ResourceManager()\n    deployment_service = DeploymentService(resource_manager=resource_manager)\n    SagemakerHuggingfaceStrategy(deployment_service).deploy(\n        role_arn=settings.ARN_ROLE,\n        llm_image=llm_image,\n        config=hugging_face_deploy_config,\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE,\n        endpoint_config_name=settings.SAGEMAKER_ENDPOINT_CONFIG_INFERENCE,\n        gpu_instance_type=settings.GPU_INSTANCE_TYPE,\n        resources=model_resource_config,\n        endpoint_type=endpoint_type,\n    ) \n```", "```py\nclass ResourceManager:\n    def __init__(self) -> None:\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        ) \n```", "```py\n def endpoint_config_exists(self, endpoint_config_name: str) -> bool:\n        try:\n            self.sagemaker_client.describe_endpoint_config(EndpointConfigName=endpoint_config_name)\n            logger.info(f\"Endpoint configuration '{endpoint_config_name}' exists.\")\n            return True\n        except ClientError:\n            logger.info(f\"Endpoint configuration '{endpoint_config_name}' does not exist.\")\n            return False \n```", "```py\ndef endpoint_exists(self, endpoint_name: str) -> bool:\n        try:\n            self.sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n            logger.info(f\"Endpoint '{endpoint_name}' exists.\")\n            return True\n        except self.sagemaker_client.exceptions.ResourceNotFoundException:\n            logger.info(f\"Endpoint '{endpoint_name}' does not exist.\")\n            return False \n```", "```py\nclass DeploymentService:\n    def __init__(self, resource_manager):\n        self.sagemaker_client = boto3.client(\n            \"sagemaker\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        )\n        self.resource_manager = resource_manager \n```", "```py\ndef deploy(\n    self,\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    endpoint_config_name: str,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    try:\n        if self.resource_manager.endpoint_config_exists(endpoint_config_name=endpoint_config_name):\n            logger.info(f\"Endpoint configuration {endpoint_config_name} exists. Using existing configuration...\")\n        else:\n            logger.info(f\"Endpoint configuration{endpoint_config_name} does not exist.\")\n        self.prepare_and_deploy_model(\n            role_arn=role_arn,\n            llm_image=llm_image,\n            config=config,\n            endpoint_name=endpoint_name,\n            update_endpoint=False,\n            resources=resources,\n            endpoint_type=endpoint_type,\n            gpu_instance_type=gpu_instance_type,\n        )\n        logger.info(f\"Successfully deployed/updated model to endpoint {endpoint_name}.\")\n    except Exception as e:\n        logger.error(f\"Failed to deploy model to SageMaker: {e}\")\n        raise \n```", "```py\n@staticmethod\ndef prepare_and_deploy_model(\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    update_endpoint: bool,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    huggingface_model = HuggingFaceModel(\n        role=role_arn,\n        image_uri=llm_image,\n        env=config,\n        transformers_version=\"4.6\",\n        pytorch_version=\"1.13\",\n        py_version=\"py310\",\n    )\n    huggingface_model.deploy(\n        instance_type=gpu_instance_type,\n        initial_instance_count=1,\n        endpoint_name=endpoint_name,\n        update_endpoint=update_endpoint,\n        resources=resources,\n        tags=[{\"Key\": \"task\", \"Value\": \"model_task\"}],\n        endpoint_type=endpoint_type,\n    ) \n```", "```py\nclass SagemakerHuggingfaceStrategy(DeploymentStrategy):\ndef __init__(self, deployment_service):\n    self.deployment_service = deployment_service \n```", "```py\ndef deploy(\n    self,\n    role_arn: str,\n    llm_image: str,\n    config: dict,\n    endpoint_name: str,\n    endpoint_config_name: str,\n    gpu_instance_type: str,\n    resources: Optional[dict] = None,\n    endpoint_type: enum.Enum = EndpointType.MODEL_BASED,\n) -> None:\n    logger.info(\"Starting deployment using Sagemaker Huggingface Strategy...\")\n    logger.info(\n        f\"Deployment parameters: nb of replicas: {settings.COPIES}, nb of gpus:{settings.GPUS}, instance_type:{settings.GPU_INSTANCE_TYPE}\"\n    ) \n```", "```py\ntry:\n    self.deployment_service.deploy(\n        role_arn=role_arn,\n        llm_image=llm_image,\n        config=config,\n        endpoint_name=endpoint_name,\n        endpoint_config_name=endpoint_config_name,\n        gpu_instance_type=gpu_instance_type,\n        resources=resources,\n        endpoint_type=endpoint_type,\n    )\n    logger.info(\"Deployment completed successfully.\")\nexcept Exception as e:\n    logger.error(f\"Error during deployment: {e}\")\n    raise \n```", "```py\nfrom sagemaker.compute_resource_requirements.resource_requirements import ResourceRequirements\n    model_resource_config = ResourceRequirements(\n    requests={\n        \"copies\": settings.COPIES,\n        \"num_accelerators\": settings.GPUS\n        \"num_cpus\": settings.CPUS,\n        \"memory\": 5 * 1024\n    },\n) \nResourceRequirements is configured with four key parameters:\n```", "```py\nhugging_face_deploy_config = {\n    \"HF_MODEL_ID\": settings.HF_MODEL_ID,\n    \"SM_NUM_GPUS\": json.dumps(settings.SM_NUM_GPUS),  # Number of GPU used per replica\n    \"MAX_INPUT_LENGTH\": json.dumps(settings.MAX_INPUT_LENGTH),  # Max length of input text\n    \"MAX_TOTAL_TOKENS\": json.dumps(settings.MAX_TOTAL_TOKENS),  # Max length of the generation (including input text)\n    \"MAX_BATCH_TOTAL_TOKENS\": json.dumps(settings.MAX_BATCH_TOTAL_TOKENS),\n    \"HUGGING_FACE_HUB_TOKEN\": settings.HUGGINGFACE_ACCESS_TOKEN,\n    \"MAX_BATCH_PREFILL_TOKENS\": \"10000\",\n    \"HF_MODEL_QUANTIZE\": \"bitsandbytes\",\n} \n```", "```py\ncreate_endpoint(endpoint_type=EndpointType.MODEL_BASED) \n```", "```py\npoetry poe deploy-inference-endpoint \n```", "```py\ntext = \"Write me a post about AWS SageMaker inference endpoints.\"\nllm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE\n    )\nAnswer = InferenceExecutor(llm, text).execute() \n```", "```py\nclass LLMInferenceSagemakerEndpoint(Inference):\n    def __init__(\n        self,\n        endpoint_name: str,\n        default_payload: Optional[Dict[str, Any]] = None,\n        inference_component_name: Optional[str] = None,\n    ) -> None:\n        super().__init__()\n        self.client = boto3.client(\n            \"sagemaker-runtime\",\n            region_name=settings.AWS_REGION,\n            aws_access_key_id=settings.AWS_ACCESS_KEY,\n            aws_secret_access_key=settings.AWS_SECRET_KEY,\n        )\n        self.endpoint_name = endpoint_name\n        self.payload = default_payload if default_payload else self._default_payload()\n        self.inference_component_name = inference_component_name \n```", "```py\ndef _default_payload(self) -> Dict[str, Any]:\n    return {\n        \"inputs\": \"\",\n        \"parameters\": {\n            \"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n            \"top_p\": settings.TOP_P_INFERENCE,\n            \"temperature\": settings.TEMPERATURE_INFERENCE,\n            \"return_full_text\": False,\n        },\n    } \n```", "```py\ndef set_payload(self, inputs: str, parameters: Optional[Dict[str, Any]] = None) -> None:\n    self.payload[\"inputs\"] = inputs\n    if parameters:\n        self.payload[\"parameters\"].update(parameters) \n```", "```py\ndef inference(self) -> Dict[str, Any]:\n    try:\n        logger.info(\"Inference request sent.\")\n        invoke_args = {\n            \"EndpointName\": self.endpoint_name,\n            \"ContentType\": \"application/json\",\n            \"Body\": json.dumps(self.payload),\n        }\n        if self.inference_component_name not in [\"None\", None]:\n            invoke_args[\"InferenceComponentName\"] = self.inference_component_name\n        response = self.client.invoke_endpoint(**invoke_args)\n        response_body = response[\"Body\"].read().decode(\"utf8\")\n        return json.loads(response_body)\n    except Exception:\n        logger.exception(\"SageMaker inference failed.\")\n        raise \n```", "```py\nclass InferenceExecutor:\n    def __init__(\n        self,\n        llm: Inference,\n        query: str,\n        context: str | None = None,\n        prompt: str | None = None,\n    ) -> None:\n        self.llm = llm\n        self.query = query\n        self.context = context if context else \"\"\n        if prompt is None:\n            self.prompt = \"\"\"\n    You are a content creator. Write what the user asked you to while using the provided context as the primary source of information for the content.\nUser query: {query}\nContext: {context}\n            \"\"\"\n        else:\n            self.prompt = prompt \n```", "```py\ndef execute(self) -> str:\n    self.llm.set_payload(\n        inputs=self.prompt.format(query=self.query, context=self.context),\n        parameters={\n            \"max_new_tokens\": settings.MAX_NEW_TOKENS_INFERENCE,\n            \"repetition_penalty\": 1.1,\n            \"temperature\": settings.TEMPERATURE_INFERENCE,\n        },\n    )\n    answer = self.llm.inference()[0][\"generated_text\"]\n    return answer \n```", "```py\npoetry run python -m llm_engineering.model.inference.test \n```", "```py\npoetry poe test-sagemaker-endpoint \n```", "```py\nfrom fastapi import FastAPI\napp = FastAPI() \n```", "```py\nclass QueryRequest(BaseModel):\n    query: str\nclass QueryResponse(BaseModel):\n    answer: str \n```", "```py\ndef call_llm_service(query: str, context: str | None) -> str:\n    llm = LLMInferenceSagemakerEndpoint(\n        endpoint_name=settings.SAGEMAKER_ENDPOINT_INFERENCE, inference_component_name=None\n    )\n    answer = InferenceExecutor(llm, query, context).execute()\n    return answer \n```", "```py\ndef rag(query: str) -> str:\n    retriever = ContextRetriever(mock=False)\n    documents = retriever.search(query, k=3 * 3)\n    context = EmbeddedChunk.to_context(documents)\n    answer = call_llm_service(query, context)\n    return answer \n```", "```py\n@app.post(\"/rag\", response_model=QueryResponse)\nasync def rag_endpoint(request: QueryRequest):\n    try:\n        answer = rag(query=request.query)\n        return {\"answer\": answer}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e)) from e \n```", "```py\nuvicorn tools.ml_service:app --host 0.0.0.0 --port 8000 --reload \n```", "```py\npoetry poe run-inference-ml-service \n```", "```py\ncurl -X POST 'http://127.0.0.1:8000/rag' -H 'Content-Type: application/json' -d '{\\\"query\\\": \\\"your_query \\\"}' \n```", "```py\npoetry poe call-inference-ml-service \n```", "```py\npoetry poe delete-inference-endpoint \n```", "```py\nmodel_resource_config = ResourceRequirements(\n    requests={\n        \"copies\": 4,  # Number of replicas.\n        \"num_accelerators\": 4, # Number of GPUs required.\n        \"num_cpus\": 8, # Number of CPU cores required.\n        \"memory\": 5 * 1024,  # Minimum memory required in Mb (required)\n    },\n) \n```"]