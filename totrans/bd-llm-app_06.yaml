- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Conversational Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this chapter, we embark on the hands-on section of this book, with our
    first concrete implementation of LLM-powered applications. Throughout this chapter,
    we will cover a step-by-step implementation of a conversational application, using
    LangChain and its components, building on the knowledge you’ve gained from the
    previous chapters. By the end of this chapter, you will be able to set up your
    own conversational application project with just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following key topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the schema of a simple chatbot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding the memory component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding non-parametric knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding tools and making the chatbot “agentic”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing the front-end with Streamlit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the tasks in this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Hugging Face account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI account and user access token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7.1 or a later version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Python packages – make sure to have the following Python packages installed:
    `langchain`, `python-dotenv`, `huggingface_hub, streamlit`, `openai`, `pypdf`,
    `tiktoken`, `faiss-cpu`, and `google-search-results.` They can be easily installed
    via `pip install` in your terminal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll find the code for this chapter in the book’s GitHub repository at [https://github.com/PacktPublishing/Building-LLM-Powered-Applications](Chapter_06.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with conversational applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A conversational application is a type of software that can interact with users
    using natural language. It can be used for various purposes, such as providing
    information, assistance, entertainment, or transactions. Generally speaking, a
    conversational application can use different modes of communication, such as text,
    voice, graphics, or even touch. A conversational application can also use different
    platforms, such as messaging apps, websites, mobile devices, or smart speakers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, conversational applications are being taken to the next level thanks
    to LLMs. Let’s look at some of the benefits that they provide:'
  prefs: []
  type: TYPE_NORMAL
- en: Not only do LLMs provide a new level of natural language interactions, but they
    can also enable applications to perform reasoning based on the best responses,
    given users’ preferences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we saw in previous chapters, LLMs can leverage their parametric knowledge,
    but are also enriched with non-parametric knowledge, thanks to embeddings and
    plug-ins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, LLMs are also able to keep track of the conversation thanks to different
    types of memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows what the architecture of a conversational bot might
    look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a computer program  Description automatically generated](img/B21714_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Sample architecture of a conversational bot'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we will build from scratch a text conversational application
    that is able to help users plan their vacations. We will call this app GlobeBotter.
    We will add incremental layers of complexity to make the app as enjoyable as possible
    for the end user.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s start with the basics behind a conversational app architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a plain vanilla bot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start with, let’s initialize our LLM and set the schema for our bot. The
    schema refers to the type of messages the bot is able to receive. In our case,
    we will have three types of messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**System message**:The instructions we give the bot so that it behaves as a
    travel assistant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI Message**:The message generated by the LLM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human Message**: The user’s query'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start with a simple configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then save and print the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the model was pretty good at generating an itinerary in Rome
    with only one piece of information from our side, the number of days.
  prefs: []
  type: TYPE_NORMAL
- en: However, we might want to keep interacting with the bot, so that we can further
    optimize the itinerary, providing more information about our preferences and habits.
    To achieve that, we need to add memory to our bot.
  prefs: []
  type: TYPE_NORMAL
- en: Adding memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’re creating a conversational bot with relatively short messages, in this
    scenario, a `ConversationBufferMemory` could be suitable. To make the configuration
    easier, let’s also initialize a `ConversationChain` to combine the LLM and the
    memory components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first initialize our memory and chain (I’m keeping `verbose = True` so
    that you can see the bot keeping track of previous messages):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now let’s have some interactions with our bot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we provide the following input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the corresponding output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the chain, it is keeping track of the previous interactions.
    Let’s challenge it and ask something related to the previous context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output that we receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The bot was able to understand that our request was related to its previous
    answer. We can also retrieve the message history with the `memory.load_memory_variables()`
    method (you can see the full output in the GitHub repository). Here is a snippet
    of the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Rather than running the conversation.run method at every interaction, I’ve
    coded a `while` cycle to make it interactive. The following is a snapshot of the
    whole conversation (you can find it in the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a truncated sample from the output (you can find the whole
    output in the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, now the AI assistant is capable of keeping track of the whole
    conversation. In the next section, we are going to add yet another layer of complexity:
    an external knowledge base.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding non-parametric knowledge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine that you also want your GlobeBotter to have access to exclusive documentation
    about itineraries that are not part of its parametric knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: To do so, we can either embed the documentation in a VectorDB or directly use
    a retriever to do the job. In this case, we will use a vector-store-backed retriever
    using a particular chain, `ConversationalRetrievalChain.` This type of chain leverages
    a retriever over the provided knowledge base that has the chat history, which
    can be passed as a parameter using the desired type of memory previously seen.
  prefs: []
  type: TYPE_NORMAL
- en: With this goal in mind, we will use a sample Italy travel guide PDF downloaded
    from [https://www.minube.net/guides/italy](https://www.minube.net/guides/italy).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Python code shows how to initialize all the ingredients we need,
    which are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document Loader**:Since the document is in PDF format, we will use `PyPDFLoader`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text splitter**:We will use a `RecursiveCharacterTextSplitter`, which splits
    text by recursively looking at characters to find one that works.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector store**:We will use the `FAISS` VectorDB.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory**:We will use a `ConversationBufferMemory`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLMs**:We will use the `gpt-3.5-turbo` model for conversations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddings**:We will use the `text-embedding-ada-002`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now interact with the chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output (I’m reporting a truncated version. You can see
    the whole output in the book’s GitHub repository):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that, by default, the `ConversationalRetrievalChain` uses a prompt template
    called `CONDENSE_QUESTION_PROMPT`, which merges the last user’s query with the
    chat history, so that it results as just one query to the retriever. If you want
    to pass a custom prompt, you can do so using the `condense_question_prompt` parameter
    in the `ConversationalRetrievalChain.from_llm` module.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the bot was able to provide an answer based on the documentation,
    we still have a limitation. In fact, with such a configuration, our GlobeBotter
    will only look at the provided documentation, but what if we want it to also use
    its parametric knowledge? For example, we might want the bot to be able to understand
    whether it could integrate with the provided documentation or simply answer *freely*.
    To do so, we need to make our GlobeBotter *agentic*, meaning that we want to leverage
    the LLM’s reasoning capabilities to orchestrate and invoke the available tools
    without a fixed order, but rather following the best approach given the user’s
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we will use two main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '`create_retriever_tool`: This method creates a custom tool that acts as a retriever
    for an agent. It will need a database to retrieve from, a name, and a short description,
    so that the model can understand when to use it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`create_conversational_retrieval_agent`: This method initializes a conversational
    agent that is configured to work with retrievers and chat models. It will need
    an LLM, a list of tools (in our case, the retriever), and a memory key to keep
    track of the previous chat history.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code illustrates how to initialize the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now let’s see the thought process of the agent with two different questions
    (I will report only the chain of thoughts and truncate the output, but you can
    find the whole code in the GitHub repo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now try with a question not related to the document:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output that we receive:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, when I asked the agent something about Italy, it immediately
    invoked the provided document, while this was not done in the last question.
  prefs: []
  type: TYPE_NORMAL
- en: The last thing we want to add to our GlobeBotter is the capability to navigate
    the web, since, as travelers, we want to have up-to-date information about the
    country we are traveling to. Let’s implement it with LangChain’s tools.
  prefs: []
  type: TYPE_NORMAL
- en: Adding external tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tool we are going to add here is the Google SerpApi tool, so that our bot
    will be able to navigate the internet.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: SerpApi is a real-time API designed to access Google search results. It simplifies
    the process of data scraping by handling complexities such as managing proxies,
    solving CAPTCHAs, and parsing structured data from search engine results pages.
  prefs: []
  type: TYPE_NORMAL
- en: LangChain offers a pre-built tool that wraps SerpApi to make it easier to integrate
    it within your agents. To enable SerpApi, you need to sign in at [https://serpapi.com/users/sign_up](https://serpapi.com/users/sign_up),
    then go to the dashboard under the tab **API key**.
  prefs: []
  type: TYPE_NORMAL
- en: Since we don’t want our GlobeBotter to be focused only on the web, we will add
    the SerpApi tool to the previous one, so that the agent will be able to pick the
    most useful tool to answer the question – or use no tool if not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s initialize our tools and agent (you learned about this and other LangChain
    components in *Chapter 5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, now let’s test it with three different questions (here, again, the output
    has been truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: “What can I visit in India in 3 days?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, the model doesn’t need external knowledge to answer the question,
    hence it is responding without invoking any tool.
  prefs: []
  type: TYPE_NORMAL
- en: “What is the weather currently in Delhi?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note how the agent is invoking the search tool; this is due to the reasoning
    capability of the underlying gpt-3.5-turbo model, which captures the user’s intent
    and dynamically understands which tool to use to accomplish the request.
  prefs: []
  type: TYPE_NORMAL
- en: “I’m traveling to Italy. Can you give me some suggestions for the main attractions
    to visit?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note how the agent is invoking the document retriever to provide the preceding
    output.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, our GlobeBotter is now able to provide up-to-date information, as well
    as retrieving specific knowledge from curated documentation. The next step will
    be that of building a front-end. We will do so by building a web app using Streamlit.
  prefs: []
  type: TYPE_NORMAL
- en: Developing the front-end with Streamlit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streamlit is a Python library that allows you to create and share web apps.
    It is designed to be easy and fast to use, without requiring any front-end experience
    or knowledge. You can write your app in pure Python, using simple commands to
    add widgets, charts, tables, and other elements.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to its native capabilities, in July 2023, Streamlit announced an
    initial integration and its future plans with LangChain. At the core of this initial
    integration, there is the ambition of making it easier to build a GUI for conversational
    applications, as well as showing all the steps LangChain’s agents take before
    producing the final response.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, the main module that Streamlit introduced is the Streamlit
    callback handler. This module provides a class called `StreamlitCallbackHandler`
    that implements the `BaseCallbackHandler` interface from LangChain. This class
    can handle various events that occur during the execution of a LangChain pipeline,
    such as tool start, tool end, tool error, LLM token, agent action, agent finish,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: The class can also create and update Streamlit elements, such as containers,
    expanders, text, progress bars, etc., to display the output of the pipeline in
    a user-friendly way. You can use the Streamlit callback handler to create Streamlit
    apps that showcase the capabilities of LangChain and interact with the user through
    natural language. For example, you can create an app that takes a user prompt
    and runs it through an agent that uses different tools and models to generate
    a response. You can use the Streamlit callback handler to show the agent’s thought
    process and the results of each tool in real time.
  prefs: []
  type: TYPE_NORMAL
- en: To start building your application, you need to create a `.py` file to run in
    your terminal via `streamlit run file.py`. In our case, the file will be named
    `globebotter.py`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the main building blocks of the application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting the configuration of the webpage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initializing the LangChain backbone components we need. The code is the same
    as the one in the previous section, so I will share here only the initialization
    code, without all the preliminary steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Setting the input box for the user with a placeholder question:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Setting Streamlit’s session states. Session state is a way to share variables
    between reruns, for each user session. In addition to the ability to store and
    persist state, Streamlit also exposes the ability to manipulate state using callbacks.
    Session state also persists across apps inside a multipage app. You can use the
    session state API to initialize, read, update, and delete variables in the session
    state. In the case of our GlobeBotter, we want two main states: `messages` and
    `memory`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Making sure to display the whole conversation. To do so, I created a for loop
    that iterates over the list of messages stored in `st.session_state["messages"].`
    For each message, it creates a Streamlit element called `st.chat_message` that
    displays a chat message in a nice format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Configuring the AI assistant to respond when given a user’s query. In this
    first example, we will keep the whole chain visible and printed to the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, adding a button to clear the history of the conversation and start
    from scratch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final product looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Front-end of GlobeBotter with Streamlit'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the expander, we can see that the agent used the `Search` tool (provided
    with the SerpApi). We can also expand `chat_history` or `intermediate_steps` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21714_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Example of Streamlit expander'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we can also decide to only show the output rather than the whole
    chain of thoughts, by specifying in the code to return only `response['output']`.
    You can see the whole code in the book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we wrap up, let’s discuss how you can give your users a streaming experience
    while interacting with your chatbot. You can leverage the `BaseCallbackHandler`
    class to create a custom callback handler in your Streamlit app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The `StreamHandler` is designed to capture and display streaming data, such
    as text or other content, in a designated container. Then, you can use it as follows
    in your Streamlit app, making sure to set `streaming=True` while initializing
    your OpenAI LLM.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: You can refer to the original code on LangChain’s GitHub repo at [https://github.com/langchain-ai/streamlit-agent/blob/main/streamlit_agent/basic_streaming.py](https://github.com/langchain-ai/streamlit-agent/blob/main/streamlit_agent/basic_streaming.py).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we approached the end-to-end implementation of a conversational
    application, leveraging LangChain’s modules and progressively adding layers of
    complexity. We started with a plain vanilla chatbot with no memory, then moved
    on to more complex systems with the ability to keep traces of past interactions.
    We’ve also seen how to add non-parametric knowledge to our application with external
    tools, making it more “agentic” so that it is able to determine which tool to
    use, depending on the user’s query. Finally, we introduced Streamlit as the front-end
    framework to build the web app for our GlobeBotter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on a more specific domain where LLMs add
    value and demonstrate emerging behaviors, that is, recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Example of a context-aware chatbot. [https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/2_%E2%AD%90_context_aware_chatbot.py](https://github.com/shashankdeshpande/langchain-chatbot/blob/master/pages/2_%E2%AD%90_context_aware_chatbot.py)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge base for the AI travel assistant. [https://www.minube.net/guides/italy](https://www.minube.net/guides/italy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LangChain repository. [https://github.com/langchain-ai](https://github.com/langchain-ai
    )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/llm](https://packt.link/llm )'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code214329708533108046.png)'
  prefs: []
  type: TYPE_IMG
