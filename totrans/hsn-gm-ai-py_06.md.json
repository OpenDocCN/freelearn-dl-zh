["```py\nimport gym\nimport math\nfrom copy import deepcopy\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nenv = gym.make('MountainCar-v0')\nQ_table = np.zeros((20,20,3))\nalpha=0.3\nbuckets=[20, 20]\ngamma=0.99\nrewards=[]\nepisodes = 3000\n\ndef to_discrete_states(observation):\n interval=[0 for i in range(len(observation))]\n max_range=[1.2,0.07] \n\n for i in range(len(observation)):\n  data = observation[i]\n  inter = int(math.floor((data + max_range[i])/(2*max_range[i]/buckets[i])))\n if inter>=buckets[i]:\n   interval[i]=buckets[i]-1\n  elif inter<0:\n   interval[i]=0\n  else:\n   interval[i]=inter\n return interval\n\ndef expect_epsilon(t):\n  return min(0.015, 1.0 - math.log10((t+1)/220.))\n\ndef expect_alpha(t):\n  return min(0.1, 1.0 - math.log10((t+1)/125.))\n\ndef get_action(observation,t):\n if np.random.random()<max(0.001, expect_epsilon(t)):\n  return env.action_space.sample()\n interval = to_discrete_states(observation) \n return np.argmax(np.array(Q_table[tuple(interval)]))\n\ndef update_SARSA(observation,reward,action,ini_obs,next_action,t): \n interval = to_discrete_states(observation)\n Q_next = Q_table[tuple(interval)][next_action]\n ini_interval = to_discrete_states(ini_obs)\n Q_table[tuple(ini_interval)][action]+=max(0.4, expect_alpha(t))*(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])\n\nfor episode in range(episodes):\n  observation = env.reset() \n  t=0\n  done=False\n  while (done==False):\n    env.render()\n    print(observation)\n    action = get_action(observation,episode)\n    obs_next, reward, done, info = env.step(action)\n    next_action = get_action(obs_next,episode)\n    update_SARSA(obs_next,reward,action,observation,next_action,episode) \n    observation=obs_next\n    action = next_action\n    t+=1\n  rewards.append(t+1)   \n\nplt.plot(rewards)\nplt.show()\n```", "```py\nenv = gym.make('MountainCar-v0')\nQ_table = np.zeros((20,20,3))\nalpha=0.3\nbuckets=[20, 20]\ngamma=0.99\nrewards=[]\nepisodes = 3000\n```", "```py\nobservation = env.reset() \nt=0\ndone=False\nwhile (done==False):\n env.render()\n print(observation)\n action = get_action(observation,episode)\n obs_next, reward, done, info = env.step(action)\n next_action = get_action(obs_next,episode)\n update_SARSA(obs_next,reward,action,observation,next_action,episode)\n  observation=obs_next\n  action = next_action\n  t+=1\nrewards.append(t+1) \n```", "```py\naction = get_action(observation,episode)\nobs_next, reward, done, info = env.step(action)\nnext_action = get_action(obs_next,episode)\n```", "```py\nupdate_SARSA(obs_next,reward,action,observation,next_action,episode)\n```", "```py\ndef update_SARSA(observation,reward,action,ini_obs,next_action,t):  \n  interval = to_discrete_states(observation) \n  Q_next = Q_table[tuple(interval)][next_action]\n  ini_interval = to_discrete_states(ini_obs)\n  Q_table[tuple(ini_interval)][action]+=max(0.4, expect_alpha(t))*(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])\n```", "```py\ndef to_discrete_states(observation):\n interval=[0 for i in range(len(observation))]\n max_range=[1.2,0.07] \n for i in range(len(observation)):\n   data = observation[i]\n   inter = int(math.floor((data + max_range[i])/(2*max_range[i]/buckets[i])))\n   if inter>=buckets[i]:\n     interval[i]=buckets[i]-1\n   elif inter<0:\n     interval[i]=0\n   else:\n     interval[i]=inter\n return interval\n```", "```py\ntuple(interval)\n```", "```py\ndef expect_epsilon(t):\n  return min(0.015, 1.0 - math.log10((t+1)/220.))\n\ndef expect_alpha(t):\n  return min(0.1, 1.0 - math.log10((t+1)/125.))\n```", "```py\ndef get_action(observation,t):  \n  if np.random.random()<max(0.001, expect_epsilon(t)):\n    return env.action_space.sample()\n  interval = to_discrete_states(observation) \n  return np.argmax(np.array(Q_table[tuple(interval)]))\n```", "```py\nQ_table[tuple(ini_interval)][action]+=max(0.4, expect_alpha(t))*(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])\n```", "```py\nenv = gym.make('CartPole-v0')\n```", "```py\nQ_table = np.zeros((20,20,20,20,3))\n```", "```py\nbuckets=[20, 20, 20, 20]\n```", "```py\ndef to_discrete_states(observation):\n interval=[0 for i in range(len(observation))]\n max_range=[2.4,999999, 41.8,999999]\n```", "```py\nQ_table[:,:,:,:,action]+=lr*td_error*(eligibility[:,:,:,:,action])\n```", "```py\nimport gym\nimport math\nfrom copy import deepcopy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nenv = gym.make('MountainCar-v0')\nQ_table = np.zeros((65,65,3))\nalpha=0.3\nbuckets=[65, 65]\ngamma=0.99\nrewards=[]\nepisodes=2000\nlambdaa=0.8\n\ndef to_discrete_states(observation):\n interval=[0 for i in range(len(observation))]\n max_range=[1.2,0.07] \n for i in range(len(observation)):\n  data = observation[i]\n  inter = int(math.floor((data + max_range[i])/(2*max_range[i]/buckets[i])))\n  if inter>=buckets[i]:\n   interval[i]=buckets[i]-1\n  elif inter<0:\n   interval[i]=0\n  else:\n   interval[i]=inter\n return interval\n\ndef expect_epsilon(t):\n  return min(0.015, 1.0 - math.log10((t+1)/220.))\n\ndef get_action(observation,t):\n if np.random.random()<max(0.001, expect_epsilon(t)):\n  return env.action_space.sample()\n interval = to_discrete_states(observation)\n return np.argmax(np.array(Q_table[tuple(interval)]))\n\ndef expect_alpha(t):\n  return min(0.1, 1.0 - math.log10((t+1)/125.))\n\ndef updateQ_SARSA(observation,reward,action,ini_obs,next_action,t,eligibility):\n interval = to_discrete_states(observation)\n Q_next = Q_table[tuple(interval)][next_action]\n ini_interval = to_discrete_states(ini_obs)\n lr=max(0.4, expect_alpha(t))\n td_error=(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])\n Q_table[:,:,action]+=lr*td_error*(eligibility[:,:,action])\nfor episode in range(episodes):\n  observation = env.reset()\n  t=0\n  eligibility = np.zeros((65,65,3))\n  done=False\n  while (done==False):\n    env.render()\n    action = get_action(observation,episode)\n    next_obs, reward, done, info = env.step(action)\n    interval = to_discrete_states(observation)\n    eligibility *= lambdaa * gamma\n    eligibility[tuple(interval)][action]+=1\n\n    next_action = get_action(next_obs,episode)\n    updateQ_SARSA(next_obs,reward,action,observation,next_action,episode,eligibility)\n    observation=next_obs\n    action = next_action\n    t+=1\n  rewards.append(t+1)\n\nplt.plot(rewards)\nplt.show()\n```", "```py\nenv = gym.make('MountainCar-v0')\nQ_table = np.zeros((65,65,3))\n```", "```py\nenv.render()\naction = get_action(observation,episode)        next_obs, reward, done, info = env.step(action)\ninterval = to_discrete_states(observation)\neligibility *= lambdaa * gamma\neligibility[tuple(interval)][action]+=1 \nnext_action = get_action(next_obs,episode)\nupdateQ_SARSA(next_obs,reward,action,observation,next_action,episode,eligibility)\nobservation=next_obs\naction = next_action\nt+=1\n```", "```py\ndef updateQ_SARSA(observation,reward,action,ini_obs,next_action,t,eligibility):\n interval = to_discrete_states(observation)\n Q_next = Q_table[tuple(interval)][next_action]\n ini_interval = to_discrete_states(ini_obs)\n lr=max(0.4, expect_alpha(t))\n td_error=(reward + gamma*(Q_next) - Q_table[tuple(ini_interval)][action])\n Q_table[:,:,action]+=lr*td_error*(eligibility[:,:,action])\n```", "```py\n conda install swig\n```", "```py\npip install box2d-py\n\n```", "```py\npip install gym[all]\n```", "```py\nenv = gym.make('LunarLander-v2')\nQ_table = np.zeros((5,5,5,5,5,5,5,5,4))\n```", "```py\nbuckets=[5,5,5,5,5,5,5,5]\n```", "```py\nmax_range=[100,100,100,100,100,100,100,100] \n```", "```py\nQ_table[:,:,:,:,:,:,:,:,action]+=lr*td_error*(eligibility[:,:,:,:,:,:,:,:,action])\n```"]