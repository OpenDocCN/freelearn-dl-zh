- en: <st c="0">9</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2">Evaluating RAG Quantitatively and with Visualizations</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="55">Evaluation plays a crucial role in building and maintaining</st>
    **<st c="116">retrieval-augmented generation</st>** <st c="146">(</st>**<st c="148">RAG</st>**<st
    c="151">) pipelines.</st> <st c="165">While you build the pipeline, you can use
    evaluation to identify areas for improvement, optimize the system’s performance,
    and systematically measure the impact of improvements.</st> <st c="343">When your
    RAG system is deployed, evaluation can help ensure the effectiveness, reliability,
    and performance of</st> <st c="455">the system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="466">In this chapter, we will cover the</st> <st c="502">following topics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="519">Evaluating when building a</st> <st c="547">RAG application</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="562">Evaluating a RAG application</st> <st c="592">after deployment</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="608">Standardized</st> <st c="622">evaluation frameworks</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="643">Ground truth</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="656">Code lab 9.1 –</st> <st c="672">ragas</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="677">Additional evaluation techniques for</st> <st c="715">RAG systems</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="726">Let’s start by talking about how evaluation can help during the
    initial stages of building your</st> <st c="823">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="834">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="857">The code for this chapter is placed in the following GitHub</st>
    <st c="918">repository:</st> [<st c="930">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_09</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_09)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1027">Evaluate as you build</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="1049">Evaluation plays a crucial role throughout the development process
    of a RAG pipeline.</st> <st c="1136">By continuously evaluating your system as
    you build it, you can identify areas that need improvement, optimize the system’s
    performance, and systematically measure the impact of any modifications or enhancements</st>
    <st c="1348">you make.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1357">Evaluation is</st> <st c="1372">essential for understanding the
    trade-offs and limitations of different approaches within the RAG pipeline.</st>
    <st c="1480">RAG pipelines often involve various technical choices, such as the
    vector store, the retrieval algorithm, and the language generation model.</st>
    <st c="1621">Each of these components can have a significant impact on the overall
    performance of the system.</st> <st c="1718">By systematically evaluating different
    combinations of these components, you can gain valuable insights into which approaches
    yield the best results for your specific tasks</st> <st c="1891">and domain.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1902">For instance, you might experiment with different embedding models,
    such as local open source models that you can download for free or cloud service
    APIs that charge each time you convert text to an embedding.</st> <st c="2113">You
    may need to understand whether the cloud API service is better than the free model,
    and if so, whether it is good enough to offset the additional cost.</st> <st c="2269">Similarly,
    you can evaluate the performance of various language generation models, such as
    ChatGPT, Llama,</st> <st c="2376">and Claude.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2387">This iterative evaluation process helps you make informed decisions
    about the most suitable architecture and components for your RAG pipeline.</st>
    <st c="2531">By considering factors such as efficiency, scalability, and generalization
    ability, you can fine-tune your system to achieve optimal performance while minimizing
    computational costs and ensuring robustness across</st> <st c="2744">different
    scenarios.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2764">Evaluation is essential for understanding the trade-offs and limitations
    of different approaches within the RAG pipeline.</st> <st c="2887">But evaluation
    can also be useful after deployment, which we will talk</st> <st c="2958">about
    next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2969">Evaluate after you deploy</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="2995">Once your RAG</st> <st c="3010">system is deployed, evaluation
    remains a crucial aspect of ensuring its ongoing effectiveness, reliability, and
    performance.</st> <st c="3135">Continuous monitoring and assessment of your deployed
    RAG pipeline are essential for maintaining its quality and identifying any potential
    issues or degradation</st> <st c="3296">over time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3306">There are numerous reasons why a RAG system’s performance might
    decline after deployment.</st> <st c="3397">For example, the data used for retrieval
    may become outdated or irrelevant as new information emerges.</st> <st c="3500">The
    language generation model may struggle to adapt to evolving user queries or changes
    in the target domain.</st> <st c="3610">Additionally, the underlying infrastructure,
    such as hardware or software components, may experience performance issues</st>
    <st c="3730">or failures.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3742">Imagine a situation where you are at a financial wealth management
    company that has a RAG-based application that helps users understand the most
    likely factors to impact their financial portfolio.</st> <st c="3940">Your data
    sources might include all of the analyses published by major financial firms in
    the past five years covering all the financial assets represented by</st> <st
    c="4099">your clientele.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4114">In financial markets, though, major (macro) events around the world
    can have a dramatic impact</st> <st c="4210">on those portfolios that have not
    been captured in the past five years of data.</st> <st c="4290">Major catastrophes,
    political instability, or even a regional event for some stocks can set a whole
    new trajectory for their performance.</st> <st c="4428">For your RAG application,
    this represents shifts in the value that the data can provide to your end user,
    and over time, that value can decrease rapidly without proper updates.</st> <st
    c="4605">Users may start asking questions about those specific events that the
    RAG application will not be able to handle, such as</st> *<st c="4727">“What impact
    will the Category 5 hurricane that just occurred have on my portfolio in the next
    year?”</st>* <st c="4828">But with continual updates and monitoring, and particularly
    with more recent reports about the impacts of the hurricane, these issues will
    likely be</st> <st c="4978">well addressed.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4993">To mitigate these risks, it is crucial to continuously monitor
    your RAG system, particularly at common failure points.</st> <st c="5113">By continuously
    evaluating these critical components of your RAG pipeline, you can proactively
    identify and address any degradation in performance.</st> <st c="5261">This may
    involve updating the retrieval corpus with fresh and relevant data, fine-tuning
    the language generation model on new data, or optimizing the system’s infrastructure
    to handle increased load or address</st> <st c="5471">performance bottlenecks.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5495">Furthermore, it is essential to establish a feedback loop that
    allows users to report any issues or provide suggestions for improvement.</st>
    <st c="5633">By actively soliciting and incorporating user feedback, you can continuously
    refine and enhance your RAG system to better meet the needs of its users.</st>
    <st c="5784">This can also include monitoring aspects such as user interface usage,
    response times, and the relevance and usefulness of the generated outputs from
    the user’s perspective.</st> <st c="5958">Conducting user surveys, analyzing user
    interaction logs, and monitoring user satisfaction metrics can provide valuable
    insights into how well your RAG system is meeting its intended purpose.</st> <st
    c="6150">How you utilize this information depends heavily on what type of RAG
    application you have developed, but in general, these are the most common areas
    monitored for continual improvement of deployed</st> <st c="6347">RAG applications.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6364">By regularly</st> <st c="6378">evaluating your deployed RAG system,
    you can ensure its long-term effectiveness, reliability, and performance.</st>
    <st c="6489">Continuous monitoring, proactive issue detection, and a commitment
    to ongoing improvement are key to maintaining a high-quality RAG pipeline that
    delivers value to its users</st> <st c="6663">over time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6673">Evaluation helps you get better</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="6705">Why is evaluation so important?</st> <st c="6738">Put simply, if
    you don’t measure where you are at, and then measure again after you have made
    improvements, it will be difficult to understand</st> <st c="6881">how or what
    improved (or hurt) the performance of your</st> <st c="6936">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6947">It is also difficult to understand what is going wrong when something
    does go wrong without something objective to compare against.</st> <st c="7080">Was
    it your retrieval mechanism?</st> <st c="7113">Was it the prompt?</st> <st c="7132">Is
    it your LLM responses?</st> <st c="7158">These are questions a good evaluation
    system can</st> <st c="7207">help answer.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7219">Evaluation provides a systematic and objective way to measure the
    performance of your pipeline, identify areas for enhancement, and track the impact
    of any changes or improvements you make.</st> <st c="7410">Without a robust evaluation
    framework, it becomes challenging to understand how your RAG system is progressing
    and where it needs</st> <st c="7540">further refinement.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7559">By embracing evaluation as an integral part of your development
    process, you can continuously refine and optimize your RAG pipeline, ensuring
    that it delivers the best possible results and meets the evolving needs of</st>
    <st c="7777">its users.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7787">Early in the RAG system development process, you have to start
    making decisions about what technical components you are going to consider.</st>
    <st c="7927">At this point, you haven’t even installed</st> <st c="7969">anything,
    so you can’t evaluate your code yet, but you can still use</st> **<st c="8038">standardized
    evaluation frameworks</st>** <st c="8072">to narrow down what you are considering.</st>
    <st c="8114">Let’s discuss these standardized evaluation frameworks for some of
    the most common RAG</st> <st c="8201">system elements.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8217">Standardized evaluation frameworks</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="8252">Key technical components of your RAG system include the embedding
    model that makes</st> <st c="8336">your embeddings, the vector store, the vector
    search, and the LLM.</st> <st c="8403">When you look at the different options
    for each technical component, there are a number of standardized metrics that
    are available for each that help you compare them against each other.</st> <st
    c="8590">Here are some common metrics for</st> <st c="8623">each category.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8637">Embedding model benchmarks</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8664">The</st> **<st c="8669">Massive Text Embedding Benchmark</st>**
    <st c="8701">(</st>**<st c="8703">MTEB</st>**<st c="8707">) Retrieval Leaderboard
    evaluates</st> <st c="8742">the performance of embedding models on various retrieval
    tasks across different datasets.</st> <st c="8832">The MTEB leaderboard ranks
    models based on their average performance across many embedding and</st> <st c="8927">retrieval-related
    tasks.</st> <st c="8952">You can visit the leaderboard using this</st> <st c="8993">link:</st>
    [<st c="8999">https://huggingface.co/spaces/mteb/leaderboard</st>](https://huggingface.co/spaces/mteb/leaderboard)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9045">When visiting this web page, click on the</st> **<st c="9088">Retrieval</st>**
    <st c="9097">and</st> **<st c="9102">Retrieval w/Instructions</st>** <st c="9126">tabs
    for</st> <st c="9136">retrieval-specific embedding ratings.</st> <st c="9174">To
    evaluate each of the models on the leaderboard, the model’s outputs are tested
    using a number of datasets that cover a wide range of domains, such as</st> <st
    c="9327">the following:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9341">Argument</st> <st c="9351">retrieval (</st>`<st c="9362">ArguAna</st>`<st
    c="9370">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9372">Climate fact</st> <st c="9385">retrieval (</st>`<st c="9396">ClimateFEVER</st>`<st
    c="9409">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9411">Duplicate question</st> <st c="9430">retrieval (</st>`<st c="9441">CQADupstackRetrieval</st>`<st
    c="9462">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9464">Entity</st> <st c="9471">retrieval (</st>`<st c="9482">DBPedia</st>`<st
    c="9490">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9492">Fact extraction and</st> <st c="9512">verification (</st>`<st c="9526">FEVER</st>`<st
    c="9532">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9534">Financial</st> <st c="9544">question-answering (</st>`<st c="9564">FiQA2018</st>`<st
    c="9573">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9575">Multi-hop</st> <st c="9585">question-answering (</st>`<st c="9605">HotpotQA</st>`<st
    c="9614">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9616">Passage and document</st> <st c="9637">ranking (</st>`<st c="9646">MSMARCO</st>`<st
    c="9654">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9656">Fact-checking (</st>`<st c="9671">NFCorpus</st>`<st c="9680">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9682">Open-domain</st> <st c="9694">question-answering (</st>`<st c="9714">NQ</st>`<st
    c="9717">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9719">Duplicate-question</st> <st c="9738">detection (</st>`<st c="9749">QuoraRetrieval</st>`<st
    c="9764">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9766">Scientific document</st> <st c="9786">retrieval (</st>`<st c="9797">SCIDOCS</st>`<st
    c="9805">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9807">Scientific claim</st> <st c="9824">verification (</st>`<st c="9838">SciFact</st>`<st
    c="9846">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9848">Argument</st> <st c="9857">retrieval (</st>`<st c="9868">Touche2020</st>`<st
    c="9879">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9881">COVID-19-related information</st> <st c="9910">retrieval (</st>`<st
    c="9921">TRECCOVID</st>`<st c="9931">)</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="9933">The leaderboard</st> <st c="9949">ranks embedding models based
    on</st> <st c="9981">their average performance across these tasks, providing a
    comprehensive view of their strengths and weaknesses.</st> <st c="10093">You can
    also click on any metric to order the board by that metric.</st> <st c="10161">So,
    for example, if you are interested in a metric that is more focused on financial
    question-answering, look at what model scored top marks on the</st> <st c="10309">FiQA2018
    dataset.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="10326">Vector store and vector search benchmarks</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**<st c="10368">ANN-Benchmarks</st>** <st c="10383">is a</st> <st c="10389">benchmarking
    tool that evaluates the performance</st> <st c="10438">of</st> **<st c="10441">approximate
    nearest neighbor</st>** <st c="10469">(</st>**<st c="10471">ANN</st>**<st c="10474">)
    algorithms, which we discussed thoroughly in</st> [*<st c="10522">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="10531">. ANN-Benchmarks</st> <st c="10548">assesses the</st> <st c="10561">search</st>
    <st c="10568">accuracy, speed, and memory usage of different</st> <st c="10615">vector
    search tools on various datasets, including</st> <st c="10666">the vector search
    tools we mentioned in</st> [*<st c="10706">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="10715">—</st>**<st c="10717">Facebook AI Similarity Search</st>** <st c="10746">(</st>**<st
    c="10748">FAISS</st>**<st c="10753">),</st> **<st c="10757">Approximate Nearest
    Neighbors Oh Yeah</st>** <st c="10794">(</st>**<st c="10796">ANNOY</st>**<st c="10801">),
    and</st> **<st c="10809">Hierarchical Navigable Small</st>** **<st c="10838">Worlds</st>**
    <st c="10844">(</st>**<st c="10846">HNSW</st>**<st c="10850">).</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="10853">Benchmarking IR</st>** <st c="10869">(</st>**<st c="10871">BEIR</st>**<st
    c="10875">) is another</st> <st c="10889">useful</st> <st c="10896">resource for
    evaluating vector stores and search algorithms.</st> <st c="10957">It provides
    a heterogeneous benchmark for zero-shot evaluation of information retrieval models
    across diverse domains, including question-answering, fact-checking, and entity
    retrieval.</st> <st c="11143">We will further discuss what</st> *<st c="11172">zero-shot</st>*
    <st c="11181">means in</st> [*<st c="11191">Chapter 13</st>*](B22475_13.xhtml#_idTextAnchor256)<st
    c="11201">, but basically, it means questions/user queries that do not have any
    examples included with them, which is a common situation in RAG.</st> <st c="11336">BEIR
    offers a standardized evaluation framework and includes popular datasets such
    as</st> <st c="11422">the following:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="11436">MSMARCO</st>`<st c="11444">: A large-scale dataset derived from
    real-world queries and answers for evaluating deep learning models in search</st>
    <st c="11559">and question-answering</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11581">HotpotQA</st>`<st c="11590">: A question-answering dataset that
    features natural, multi-hop questions, with strong supervision for supporting
    facts and enabling more explainable</st> <st c="11741">question-answering systems</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="11767">CQADupStack</st>`<st c="11779">: A benchmark dataset for</st>
    **<st c="11806">community question-answering</st>** <st c="11834">(</st>**<st
    c="11836">cQA</st>**<st c="11839">) research, taken</st> <st c="11858">from 12
    Stack Exchange subforums and annotated with duplicate</st> <st c="11920">question
    information</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="11940">These</st> <st c="11947">datasets, along</st> <st c="11963">with
    others in the</st> <st c="11982">BEIR benchmark, cover a diverse range of domains
    and</st> <st c="12035">information retrieval tasks, allowing you to assess the
    performance of your retrieval system in different contexts and compare it against</st>
    <st c="12173">state-of-the-art methods.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12198">LLM benchmarks</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="12213">The Artificial Analysis LLM Performance Leaderboard is a comprehensive
    resource for evaluating</st> <st c="12309">both open source and proprietary language
    models, such as</st> <st c="12367">ChatGPT, Claude, and Llama.</st> <st c="12395">It</st>
    <st c="12398">assesses the models’ performance</st> <st c="12431">on a wide range
    of tasks.</st> <st c="12457">For quality comparisons, it uses a number</st> <st
    c="12499">of sub-leaderboards:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="12519">General ability</st>**<st c="12535">:</st> <st c="12538">Chatbot
    Arena</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="12551">Reasoning</st>** **<st c="12562">and knowledge</st>**<st c="12575">:</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="12577">Massive Multitask Language</st>** **<st c="12604">Understanding</st>**
    <st c="12617">(</st>**<st c="12619">MMLU</st>**<st c="12623">)</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="12625">Multi-turn Benchmark</st>** <st c="12645">(</st>**<st c="12647">MT
    Bench</st>**<st c="12655">)</st>'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="12657">They also track speed and price and provide analysis to allow
    you to compare a balance of each of these areas.</st> <st c="12768">By ranking
    the models based on their performance across these tasks, the leaderboard provides
    a holistic view of</st> <st c="12881">their capabilities.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12900">It can be found</st> <st c="12917">here:</st> [<st c="12923">https://artificialanalysis.ai/</st>](https://artificialanalysis.ai/)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12953">In addition to the general LLM leaderboard, there are specialized
    leaderboards that focus on specific aspects of LLM performance.</st> <st c="13084">The
    Artificial Analysis LLM Performance Leaderboard</st> <st c="13136">evaluates the
    technical aspects of LLMs, such as inference speed, memory consumption, and scalability.</st>
    <st c="13239">It</st> <st c="13242">includes metrics such as throughput (tokens
    processed per second), latency (time to generate a response), memory footprint,
    and scaling efficiency.</st> <st c="13390">These metrics help you understand the
    computational requirements and performance characteristics of</st> <st c="13490">different
    LLMs.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="13505">The Open LLM Leaderboard tracks the performance of open source
    language models on various</st> <st c="13596">natural language understanding and
    generation tasks.</st> <st c="13649">It includes benchmarks such as the</st> **<st
    c="13684">AI2 Reasoning Challenge</st>** <st c="13707">(</st>**<st c="13709">ARC</st>**<st
    c="13712">) for complex scientific reasoning, HellaSwag for common-sense reasoning,
    MMLU for domain-specific performance, TruthfulQA for generating truthful and informative
    responses, WinoGrande</st> <st c="13898">for common-sense reasoning through pronoun
    disambiguation, and</st> **<st c="13961">Grade School Math 8K</st>** <st c="13981">(</st>**<st
    c="13983">GSM8K</st>**<st c="13988">) for mathematical</st> <st c="14008">reasoning
    abilities.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14028">Final thoughts on standardized evaluation frameworks</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="14081">Using standardized evaluation frameworks and benchmarks offers
    a valuable starting point for</st> <st c="14175">comparing the performance of
    different components in your RAG pipeline.</st> <st c="14247">They cover a wide
    range of tasks and domains, allowing you to assess the strengths and weaknesses
    of various approaches.</st> <st c="14368">By considering the results on these
    benchmarks, along with other factors such as computational efficiency and ease
    of integration, you can narrow down your options and make better-informed decisions
    when selecting the most suitable components for your specific</st> <st c="14629">RAG
    application.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="14645">However, it’s important to note that while these standardized
    evaluation metrics are helpful for initial component selection, they may not fully
    capture the performance of your specific RAG pipeline with your unique inputs
    and outputs.</st> <st c="14882">To truly understand how well your RAG system performs
    in your particular use case, you need to set up your own evaluation framework
    tailored to your specific requirements.</st> <st c="15054">This customized evaluation</st>
    <st c="15081">system will provide the most accurate and relevant insights into
    the performance of your</st> <st c="15170">RAG pipeline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15183">Next, we need to talk about one of the most important and often
    overlooked aspects of RAG evaluation, your</st> <st c="15291">ground-truth data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15309">What is the ground truth?</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="15335">Simply put, ground-truth data is data that represents the ideal
    responses you would expect if your RAG system was operating at</st> <st c="15463">peak
    performance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15480">As a practical example, if you had a RAG system focused on allowing
    someone to ask questions about</st> <st c="15580">the latest cancer research in
    veterinarian medicine for dogs, with your data source being all the latest research
    papers on the subject that have been submitted to PubMed, your ground truth would
    likely be questions and answers that could be asked and answered of that data.</st>
    <st c="15855">You would want to use realistic questions that your target audience
    would really ask, and the answers should be what you consider to be the ideal
    answer expected from the LLM.</st> <st c="16031">This could be somewhat objective,
    but nonetheless, having a set of ground-truth data to compare against the input
    and output of your RAG system is a critical way to help compare the impact of
    changes you make and ultimately make the system</st> <st c="16271">more effective.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16286">How to use the ground truth?</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16315">Ground-truth data serves as a benchmark to measure the performance
    of RAG systems.</st> <st c="16399">By comparing</st> <st c="16412">the output
    generated by the RAG system to the ground truth, you can assess how well the system
    retrieves relevant information and generates accurate and coherent responses.</st>
    <st c="16585">The ground truth helps quantify the effectiveness of different RAG
    approaches and identify areas</st> <st c="16682">for improvement.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="16698">Generating the ground truth</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="16726">Creating</st> <st c="16736">ground-truth data manually can be
    time-consuming.</st> <st c="16786">If your company already has a dataset of ideal
    responses for specific queries or prompts, that can be a valuable resource.</st>
    <st c="16909">However, if such data is not readily available, there are alternative
    methods to obtain the ground truth that we will look</st> <st c="17032">into next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17042">Human annotation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="17059">You can</st> <st c="17068">employ human annotators to manually
    create ideal responses for a set of queries or prompts.</st> <st c="17160">This
    ensures high-quality ground-truth data but can be costly and time-consuming, especially
    for</st> <st c="17257">large-scale evaluations.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17281">Expert knowledge</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="17298">In some</st> <st c="17307">domains, you may have access to</st>
    **<st c="17339">subject-matter experts</st>** <st c="17361">(</st>**<st c="17363">SMEs</st>**<st
    c="17367">) who can provide ground-truth responses based on their</st> <st c="17424">expertise.</st>
    <st c="17435">This can be particularly useful for specialized or technical domains
    where accurate information</st> <st c="17531">is crucial.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17542">One common</st> <st c="17554">approach to help with this method
    is called</st> **<st c="17598">rule-based generation</st>**<st c="17619">. With
    rule-based generation, for specific domains or tasks, you can define a set of
    rules or templates to generate the synthetic ground truth and utilize your SMEs
    to fill in the template.</st> <st c="17809">By leveraging domain knowledge and
    predefined patterns, you can create responses that align with the expected format</st>
    <st c="17926">and content.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17938">For example, if you are building a customer support chatbot to
    support mobile phones, you may have a template such as this:</st> `<st c="18063">To
    resolve [issue], you can try [solution]</st>`<st c="18105">. Your SMEs can fill
    in various issue-solution approaches where an issue might be</st> *<st c="18187">battery
    drain</st>* <st c="18200">and the solution</st> *<st c="18218">reducing screen
    brightness and closing background apps</st>*<st c="18272">. This would be fed
    to the template (what we call hydrating) and the final output would be something
    such as this:</st> `<st c="18387">To resolve [battery drain], you can try [reducing
    screen brightness and closing</st>` `<st c="18467">background apps]</st>`<st c="18483">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18484">Crowdsourcing</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18498">Platforms</st> <st c="18509">such as Amazon Mechanical Turk and
    Figure Eight allow you to outsource the task of creating ground-truth data to
    a large pool of workers.</st> <st c="18647">By providing clear instructions and
    quality control measures, you can obtain a diverse set</st> <st c="18738">of responses.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18751">Synthetic ground truth</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18774">In cases where obtaining real ground-truth data is challenging
    or infeasible, generating the synthetic</st> <st c="18878">ground truth can be
    a viable</st> <st c="18907">alternative.</st> <st c="18920">The synthetic ground
    truth involves using existing LLMs or techniques to automatically generate plausible
    responses.</st> <st c="19037">Here are a</st> <st c="19048">few approaches:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="19063">Fine-tuned language models</st>**<st c="19090">: You can fine-tune
    LLMs on a smaller dataset of</st> <st c="19140">high-quality responses.</st> <st
    c="19164">By</st> <st c="19167">providing the model with examples of ideal responses,
    it can learn to generate similar responses for new queries or prompts.</st> <st
    c="19292">The generated responses can serve as a synthetic</st> <st c="19341">ground
    truth.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="19354">Retrieval-based methods</st>**<st c="19378">: If you have a
    large corpus of high-quality text data, you</st> <st c="19439">can use retrieval-based
    methods to find</st> <st c="19479">relevant passages or sentences that closely
    match the query or prompt.</st> <st c="19550">These retrieved passages can be
    used as a proxy for</st> <st c="19602">ground-truth responses.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="19625">Obtaining the ground truth can be a challenging step in building
    your RAG system, but once you have obtained it, you will have a strong foundation
    for effective RAG evaluation.</st> <st c="19803">In the next section, we have
    a code lab where we generate synthetic ground-truth data and then integrate a
    useful evaluation platform into our RAG system that will tell us how the hybrid
    search we used in the previous chapter impacted</st> <st c="20038">our results.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20050">Code lab 9.1 – ragas</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**<st c="20071">Retrieval-augmented generation assessment</st>** <st c="20113">(</st>**<st
    c="20115">ragas</st>**<st c="20120">) is an evaluation platform designed</st>
    <st c="20158">specifically for RAG.</st> <st c="20180">In this code lab, we will
    step through the implementation of ragas in your code, generating a synthetic
    ground truth, and then establishing a comprehensive set of metrics that you can
    integrate into your RAG system.</st> <st c="20396">But evaluation systems are
    meant to evaluate something, right?</st> <st c="20459">What will we evaluate in
    our</st> <st c="20488">code lab?</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20497">If you remember in</st> [*<st c="20517">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="20526">, we introduced a new search method for our retrieval stage called</st>
    **<st c="20593">hybrid search</st>**<st c="20606">. In this</st> <st c="20616">code
    lab, we will both implement the original dense vector semantic-based search and
    then use ragas to evaluate the impact of using the hybrid search method.</st>
    <st c="20774">This will give you a real-world working example of how a comprehensive
    evaluation system can be implemented in your</st> <st c="20890">own code!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20899">Before we dive</st> <st c="20915">into how to use ragas, it is
    important to note that it is a highly evolving project.</st> <st c="21000">New
    features and API changes are happening often with new releases, so be sure to
    refer to the documentation website when walking through code</st> <st c="21143">examples:</st>
    [<st c="21153">https://docs.ragas.io/</st>](https://docs.ragas.io/)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21175">This code lab picks up right where we left off in the last chapter
    when we added</st> `<st c="21257">EnsembleRetriever</st>` <st c="21274">from LangChain
    (</st>*<st c="21291">Code</st>* *<st c="21297">lab 8.3</st>*<st c="21304">):</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21307">Let’s start with some new packages</st> <st c="21343">to install:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="21867">Next, we need to add several imports related to what we</st> <st
    c="21924">just installed:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**`<st c="23571">from ragas.testset.generator import TestsetGenerator</st>`<st
    c="23624">: The</st> `<st c="23631">TestsetGenerator</st>` <st c="23647">class
    is used to generate synthetic ground-truth datasets for evaluating RAG pipelines.</st>
    <st c="23736">It takes a set of documents and generates question-answer pairs
    along with the corresponding contexts.</st> <st c="23839">One key aspect of</st>
    `<st c="23857">TestsetGenerator</st>` <st c="23873">is that it allows the customization
    of the test data distribution by specifying the</st> <st c="23958">proportions
    of different question types (e.g., simple, multi-context, or reasoning) using
    the</st> `<st c="24052">distributions</st>` <st c="24065">parameter.</st> <st
    c="24077">It supports generating test sets using both LangChain and LlamaIndex</st>
    <st c="24146">document loaders.</st>**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**`<st c="24163">from ragas.testset.evolutions import simple, reasoning, multi_context</st>`<st
    c="24233">: These imports represent different types of question evolutions used
    in the test dataset generation process.</st> <st c="24344">These evolutions help
    create a diverse and comprehensive test dataset that covers various types of questions
    encountered in</st> <st c="24468">real-world scenarios:</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`<st c="24807">from ragas.metrics import…()</st>`<st c="24836">: This</st>
    `<st c="24844">import</st>` <st c="24850">statement brings in various evaluation
    metrics provided by the ragas library.</st> <st c="24929">The metrics imported
    include</st> `<st c="24958">answer_relevancy</st>`<st c="24974">,</st> `<st c="24976">faithfulness</st>`<st
    c="24988">,</st> `<st c="24990">context_recall</st>`<st c="25004">,</st> `<st
    c="25006">context_precision</st>`<st c="25023">,</st> `<st c="25025">answer_correctness</st>`<st
    c="25043">, and</st> `<st c="25049">answer_similarity</st>`<st c="25066">. There
    are currently two more component-wise metrics (context relevancy and context entity
    recall) that we can import, but to reduce the complexity of this, we will skip
    over them here.</st> <st c="25253">We will talk about additional metrics you can
    use toward the end of the code lab.</st> <st c="25335">These metrics assess different
    aspects of the RAG pipeline’s performance that relate to the retrieval and generation
    and, overall, all the end-to-end stages of the</st> <st c="25499">active pipeline.</st>**'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="25515">Overall, these</st> <st c="25531">imports from the ragas library
    provide a comprehensive set of tools for generating synthetic test datasets, evaluating
    RAG pipelines using various metrics, and analyzing the</st> <st c="25705">performance
    results.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25725">Setting up LLMs/embedding models</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="25758">Now, we are going to upgrade how we handle our LLM and embedding
    services.</st> <st c="25834">With ragas, we</st> <st c="25849">are introducing
    more complexity to the number of LLMs that we use; we want to better</st> <st
    c="25934">manage that by setting our initializations upfront for both the embedding
    service and the LLM services.</st> <st c="26038">Let’s look at</st> <st c="26052">the
    code:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: <st c="26514">Note that while we still only use one embedding service, we now
    have two different LLMs to call.</st> <st c="26612">The main goal of this, though,
    is to establish the</st> *<st c="26663">primary</st>* <st c="26670">LLM that we
    want to use directly for LLMs (</st>`<st c="26714">llm</st>`<st c="26718">), and
    then two additional LLMs that are designated for the evaluation process (</st>`<st
    c="26799">generator_llm</st>` <st c="26813">and</st> `<st c="26818">critic_llm</st>`<st
    c="26828">).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26831">We have the benefit of having a more advanced LLM available, ChatGPT-4o-mini,
    which we can use as the critic LLM, which, in theory, means it can be more effective
    at evaluating input that we feed to it.</st> <st c="27035">This may not always
    be the case, or you may have an LLM</st> <st c="27091">that you fine-tune specifically
    for the task of evaluation.</st> <st c="27151">Either way, breaking these LLMs
    out into specialized designations shows you how different LLMs can be used for
    different</st> <st c="27272">purposes within a RAG system.</st> <st c="27302">You
    can remove the following line from the previous code that was initializing the
    LLM object that we were</st> <st c="27409">originally using:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27484">Next, we will add a new RAG chain for running the similarity search
    (which is what we were originally running with just the</st> <st c="27609">dense
    embeddings):</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27769">To make things clearer, we will update the hybrid RAG chain with</st>
    <st c="27835">this name:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: <st c="27986">Note the change of the variable that is bolded, which used to
    be</st> `<st c="28052">rag_chain_with_source</st>`<st c="28073">. It is now called</st>
    `<st c="28092">rag_chain_hybrid</st>`<st c="28108">, representing the hybrid</st>
    <st c="28134">search aspect.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28148">Now we are going to update our original code for submitting a
    user query, only this time, we are going to use both the similarity and hybrid</st>
    <st c="28290">search versions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28306">Create the</st> <st c="28318">similarity version:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: <st c="28889">Now, create</st> <st c="28902">the</st> <st c="28906">hybrid version:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: <st c="29465">The</st> <st c="29470">primary difference between these two sets
    of code is that they show the use of the different</st> <st c="29563">RAG chains
    we have created,</st> `<st c="29591">rag_chain_similarity</st>` <st c="29611">and</st>
    `<st c="29616">rag_chain_hybrid</st>`<st c="29632">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="29633">First, let’s take a look at the output from the</st> <st c="29682">similarity
    search:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: <st c="30477">Next is the output from the</st> <st c="30506">hybrid search:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: <st c="31464">Saying which is better may be subjective, but if you look back
    at the code from</st> [*<st c="31545">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="31554">, the retrieval mechanism for each of these chains returns a different
    set of data for the</st> <st c="31645">LLM to use as the basis for answering the
    user query.</st> <st c="31699">You can see these differences reflected</st> <st
    c="31739">in the preceding responses, where each has slightly different information
    and highlights slightly different aspects of</st> <st c="31858">that information.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31875">So far, we have set up our RAG system to have the ability to use
    two different RAG chains, one focused on using just the similarity/dense search
    and the other using the hybrid search.</st> <st c="32060">This sets the foundation
    for applying ragas to our code to establish a more objective approach to evaluating
    the results we are getting from either of</st> <st c="32211">these chains.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32224">Generating the synthetic ground truth</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="32262">As we mentioned in the previous section, ground truth is a key
    element for us to conduct</st> <st c="32352">this evaluation analysis.</st> <st
    c="32378">But we have no ground truth—oh no!</st> <st c="32413">No problem, we
    can use ragas to generate synthetic data for</st> <st c="32473">this purpose.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32486">WARNING</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32494">The ragas library uses your LLM API extensively.</st> <st c="32544">The
    analysis that ragas will provide is LLM-assisted evaluation, meaning every time
    a ground-truth example is generated or evaluated, an LLM is called (sometimes
    multiple times for one metric) and an API charge is incurred.</st> <st c="32768">If
    you generate 100 ground-truth examples, which includes the generation of both
    questions and answers, and then run six different evaluation metrics, the number
    of LLM API calls you make multiplies substantially, well into the thousands of
    calls.</st> <st c="33016">It is recommended to use it sparingly until you have
    a good grasp on how often the calls are being made.</st> <st c="33121">These are
    cost-incurring API calls and they have the potential to run up your LLM API bills!</st>
    <st c="33214">At the time of this writing, I was incurring a cost of about $2
    to $2.50 every time I ran through the entire code lab with just 10 ground examples
    and 6 metrics.</st> <st c="33376">If you have a larger dataset or set</st> `<st
    c="33412">test_size</st>` <st c="33421">for your</st> `<st c="33431">testset</st>`
    <st c="33438">generator to generate more than 10 examples, the costs will</st>
    <st c="33499">increase substantially.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33522">We start</st> <st c="33532">with creating an instance of our generator
    that we will use to generate our</st> <st c="33608">ground-truth dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: <st c="34099">As you can see in this code, we are using both</st> `<st c="34147">generator_llm</st>`
    <st c="34160">and</st> `<st c="34165">critic_llm</st>`<st c="34175">, as well
    as</st> `<st c="34188">embedding_function</st>`<st c="34206">. As the previous</st>*<st
    c="34223">WARNING</st>* <st c="34231">box states, be careful about this!</st>
    <st c="34267">That is three different APIs that can generate substantial costs
    if you are not careful with the settings in this code.</st> <st c="34387">In this
    code, we also take our splits of data generated earlier in the code and preprocess
    them to work more effectively with ragas.</st> <st c="34520">Each</st> <st c="34525">chunk
    in splits is assumed to be a string representing a portion of a document.</st>
    <st c="34605">The</st> `<st c="34609">Document</st>` <st c="34617">class is from
    the LangChain library and is a convenient way to represent a document with</st>
    <st c="34707">its content.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="34719">testset</st>` <st c="34727">uses the</st> `<st c="34737">generator_with_langchain_docs</st>`
    <st c="34766">function from our generator object to generate a synthetic test.</st>
    <st c="34832">This function takes the documents list as input.</st> <st c="34881">The</st>
    `<st c="34885">test_size</st>` <st c="34894">parameter sets the desired number
    of test questions to be generated (in this case, 10).</st> <st c="34983">The</st>
    `<st c="34987">distributions</st>` <st c="35000">parameter defines the distribution
    of question types, with simple questions comprising 50% of the dataset, reasoning
    questions 25%, and multi-context questions 25%, in this example.</st> <st c="35183">We
    then convert</st> `<st c="35199">testset</st>` <st c="35206">into a pandas DataFrame,
    which we can use to view the results, and save it as a file.</st> <st c="35293">Given
    the costs we just mentioned, saving the data at this point to a CSV that can persist
    in your file directory offers the added convenience of only having to run this</st>
    <st c="35463">code once!</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35473">Now let’s pull our saved dataset back up and look</st> <st c="35524">at
    it!</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: <st c="35689">The output should look something like what you see here in</st>
    *<st c="35749">Figure 9</st>**<st c="35757">.1</st>*<st c="35759">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – DataFrame showing synthesized ground-truth data](img/B22475_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="36615">Figure 9.1 – DataFrame showing synthesized ground-truth data</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36675">In this dataset, you see questions and answers (</st>`<st c="36724">ground_truth</st>`<st
    c="36737">) that have been generated by the</st> `<st c="36772">generator_llm</st>`
    <st c="36785">instance you initialized earlier.</st> <st c="36820">You now have
    your ground truth!</st> <st c="36852">The LLM will attempt to generate 10 different
    question-and-answer pairs for our ground truth, but in some cases, a failure will
    occur that limits this generation.</st> <st c="37015">This will result</st> <st
    c="37032">in fewer ground-truth examples than you had set in the</st> `<st c="37087">test_size</st>`
    <st c="37096">variable.</st> <st c="37107">In this case, the generation resulted
    in 7 examples, rather than 10\.</st> <st c="37176">Overall, you will likely want
    to generate more than 10 examples for a thorough test of your RAG system.</st>
    <st c="37280">We are going to accept seven examples for this simple example though,
    primarily to keep your API</st> <st c="37377">costs down!</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="37388">Next, let’s prepare the</st> <st c="37413">similarity dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: <st c="37670">Here, we are performing some more data conversion to make formats
    compatible with other parts of our code (for ragas input in this case).</st> <st
    c="37809">We convert the</st> `<st c="37824">saved_testset_df</st>` <st c="37840">DataFrame
    into a dictionary format using the</st> `<st c="37886">to_dict()</st>` <st c="37895">method
    with</st> `<st c="37908">orient='list'</st>`<st c="37921">, after converting all
    columns to the string type using</st> `<st c="37977">astype(str)</st>`<st c="37988">.
    The resulting</st> `<st c="38004">saved_testing_data</st>` <st c="38022">dictionary
    is then used to create a</st> `<st c="38059">Dataset</st>` <st c="38066">object
    called</st> `<st c="38081">saved_testing_dataset</st>` <st c="38102">using the</st>
    `<st c="38113">from_dict()</st>` <st c="38124">method from the</st> `<st c="38141">datasets</st>`
    <st c="38149">library.</st> <st c="38159">We create a new dataset called</st>
    `<st c="38190">saved_testing_dataset_sm</st>` <st c="38214">representing a smaller
    section of the data containing just the columns</st> <st c="38286">we need.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38294">In this case, we remove the</st> `<st c="38323">evolution_type</st>`
    <st c="38337">and</st> `<st c="38342">episode_done</st>` <st c="38354">columns
    using the</st> `<st c="38373">remove_columns()</st>` <st c="38389">method.</st>
    <st c="38398">Let’s take a look by adding this code in a</st> <st c="38441">separate
    cell:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: <st c="38480">The output should look</st> <st c="38504">like this:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: <st c="38603">If you</st> <st c="38611">have more ground-truth examples, the</st>
    `<st c="38648">num_rows</st>` <st c="38656">variable will reflect that, but the
    rest should be the same.</st> <st c="38718">The</st> `<st c="38722">Dataset</st>`
    <st c="38729">object indicates the “features” we have, representing the columns
    we passed into it, and then this indicates that we have seven rows</st> <st c="38863">of
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38871">Next, we will set up a function to run the RAG chains we pass
    to it, and then add some additional formatting that enables it to work</st> <st
    c="39005">with ragas:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: <st c="39275">This block defines a</st> `<st c="39297">generate_answer()</st>`
    <st c="39314">function that takes a question, the</st> `<st c="39351">ground_truth</st>`
    <st c="39363">data, and</st> `<st c="39374">rag_chain</st>` <st c="39383">as inputs.</st>
    <st c="39395">This function is flexible in that it accepts either of the chains
    that we provide to it, which will come in handy when we want to generate an analysis
    of both the similarity and hybrid chains.</st> <st c="39588">The first step in
    this function is to invoke the</st> `<st c="39637">rag_chain</st>` <st c="39646">input
    that has been passed to it with the given question</st> <st c="39704">and retrieve
    the result.</st> <st c="39729">The second step is to return a dictionary containing
    the question, the final answer from the result, the contexts extracted from the
    result, and the</st> <st c="39878">ground truth.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39891">Now we are ready to prep our datasets more to work</st> <st c="39943">with
    ragas:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: <st c="40338">In this code, we create two new datasets,</st> `<st c="40381">testing_dataset_similarity</st>`
    <st c="40407">and</st> `<st c="40412">testing_dataset_hybrid</st>`<st c="40434">,
    by applying the</st> `<st c="40452">generate_answer()</st>` <st c="40469">function
    to each row of</st> `<st c="40494">saved_testing_dataset_sm</st>` <st c="40518">for
    each of our RAG chains (similarity and hybrid) using the</st> `<st c="40580">map()</st>`
    <st c="40585">method.</st> `<st c="40594">rag_chain_similarity</st>` <st c="40614">and</st>
    `<st c="40619">rag_chain_hybrid</st>` <st c="40635">are used as the</st> `<st
    c="40652">rag_chain</st>` <st c="40661">argument in the respective dataset creations.</st>
    <st c="40708">The original columns of</st> `<st c="40732">saved_testing_dataset_sm</st>`
    <st c="40756">are removed</st> <st c="40769">using</st> `<st c="40775">remove_columns=saved_testing_dataset_sm.column_names</st>`<st
    c="40827">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40828">And finally, let’s run ragas on the two datasets.</st> <st c="40879">Here
    is the code for applying ragas to our similarity</st> <st c="40933">RAG chain:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: <st c="41164">Here, we</st> <st c="41174">apply ragas to evaluate</st> `<st
    c="41198">testing_dataset_similarity</st>` <st c="41224">using the</st> `<st c="41235">evaluate()</st>`
    <st c="41245">function from the ragas library.</st> <st c="41279">The evaluation
    is performed using the specified metrics, which include</st> `<st c="41350">faithfulness</st>`<st
    c="41362">,</st> `<st c="41364">answer_relevancy</st>`<st c="41380">,</st> `<st
    c="41382">context_precision</st>`<st c="41399">,</st> `<st c="41401">context_recall</st>`<st
    c="41415">,</st> `<st c="41417">answer_correctness</st>`<st c="41435">, and</st>
    `<st c="41441">answer_similarity</st>`<st c="41458">. The evaluation results are
    stored in the</st> `<st c="41501">score_similarity</st>` <st c="41517">variable,
    which is then converted to a pandas DataFrame,</st> `<st c="41575">similarity_df</st>`<st
    c="41588">, using the</st> `<st c="41600">to_pandas()</st>` <st c="41611">method.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41619">We will do the same with the</st> <st c="41649">hybrid dataset:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: <st c="41869">Once</st> <st c="41875">you have reached this point, the use of
    ragas is done!</st> <st c="41930">We have now performed a full evaluation of our
    two chains using ragas, and within these two DataFrames,</st> `<st c="42034">similarity_df</st>`
    <st c="42047">and</st> `<st c="42052">hybrid_df</st>`<st c="42061">, we have all
    of our metrics data.</st> <st c="42096">All we have left to do is analyze the
    data</st> <st c="42139">ragas provided.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42154">Analyzing the ragas results</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="42182">We will spend the rest of this code lab formatting the data so
    that we can first save and persist it (because again, this can be a more expensive
    part of our RAG system).</st> <st c="42354">The rest of this</st> <st c="42371">code
    can be reused in the future to pull the data from the</st> `<st c="42430">.csv</st>`
    <st c="42434">files if you save them, preventing you from having to re-run this
    potentially expensive</st> <st c="42523">evaluation process.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42542">Let’s start with setting up some important variables and then
    saving the data we’ve collected to</st> `<st c="42640">csv</st>` <st c="42643">files:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: <st c="43350">In this</st> <st c="43359">code, we first define a</st> `<st c="43383">key_columns</st>`
    <st c="43394">list containing the names of the key columns to be used for comparison.</st>
    <st c="43467">We then calculate the mean scores for each key column in</st> `<st
    c="43524">similarity_df</st>` <st c="43537">and</st> `<st c="43542">hybrid_df</st>`
    <st c="43551">using the</st> `<st c="43562">mean()</st>` <st c="43568">method
    and store them in</st> `<st c="43594">similarity_means</st>` <st c="43610">and</st>
    `<st c="43615">hybrid_means</st>`<st c="43627">, respectively.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43642">Next, we create a new DataFrame called</st> `<st c="43682">comparison_df</st>`
    <st c="43695">that compares the mean scores of the similarity run and the hybrid
    run.</st> <st c="43768">The</st> `<st c="43772">Difference</st>` <st c="43782">column
    is added to</st> `<st c="43802">comparison_df</st>`<st c="43815">, calculated
    as the difference between the mean scores of the similarity run and the hybrid
    run.</st> <st c="43912">And finally, we save the</st> `<st c="43937">similarity_df</st>`<st
    c="43950">,</st> `<st c="43952">hybrid_df</st>`<st c="43961">, and</st> `<st c="43967">comparison_df</st>`
    <st c="43980">DataFrames as</st> `<st c="43995">.csv</st>` <st c="43999">files.</st>
    <st c="44007">We will save the files again, and we can work from these files in
    the future without having to go back and re-generate everything again.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44144">Also, keep in mind that this is just one way to conduct the analysis.</st>
    <st c="44214">This is where you will want to get creative and adjust this code
    to conduct an analysis that focuses on the aspects you find important in your
    specific RAG system.</st> <st c="44378">For example, you may be focused solely
    on improving your retrieval mechanisms.</st> <st c="44457">Or you could be applying
    this to data that is streaming from a deployed environment, in which case you
    likely have no ground truth and will want to focus on the metrics that can work
    without a ground truth (see the</st> *<st c="44672">ragas founder insights</st>*
    <st c="44694">section later in this chapter for more information about</st> <st
    c="44752">that concept).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44766">Moving</st> <st c="44774">on with this analysis, though, we now
    want to pull the files we saved back up to complete our analysis, and then print
    out our analysis of each of the stages of our RAG system across the two</st> <st
    c="44965">different chains:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: <st c="45570">This section of the code will generate a set of metrics that we
    will examine further as follows.</st> <st c="45668">We first load DataFrames from
    the CSV files we generated in the previous code block.</st> <st c="45753">We then
    apply an analysis that consolidates everything into</st> <st c="45813">easier-to-read
    scores.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45835">We continue on, using variables we defined in the previous code
    block to help generate plots</st> <st c="45929">with</st> `<st c="45934">matplotlib</st>`<st
    c="45944">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: <st c="46229">Here, we</st> <st c="46239">are creating subplots for each category
    with</st> <st c="46284">increased spacing.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="46302">Next, we will iterate over each of those categories and plot the</st>
    <st c="46368">corresponding metrics:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: <st c="47334">Most</st> <st c="47340">of this code is focused on formatting
    our visualizations, including plotting bars for both the similarity and hybrid
    runs, as well as adding values to those bars.</st> <st c="47503">We give the bars
    some color and even add some hashing to improve accessibility for the</st> <st
    c="47590">visually impaired.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47608">We have just a few more improvements to make to</st> <st c="47657">the
    visualization:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: <st c="47902">In this code, we add labels and the title to our visualization.</st>
    <st c="47967">We also adjust the spacing between the subplots and increase the
    top margin.</st> <st c="48044">Then, finally, we</st> `<st c="48062">use plt.show()</st>`
    <st c="48076">to display the visualization within the</st> <st c="48117">notebook
    interface.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48136">Overall, the code in this section will generate a text-based analysis
    that shows you results</st> <st c="48230">from both chains, and then it generates
    a visualization in the form of bar charts comparing the results.</st> <st c="48335">While
    the code will generate all of this together, we are going to break this up and
    discuss each part of the output as it relates to the main stages of our</st> <st
    c="48492">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '<st c="48503">As we discussed in previous chapters, RAG has two primary stages
    of action when it is engaged: retrieval and generation.</st> <st c="48625">When
    evaluating a RAG system, you can break down your evaluation by those two categories
    as well.</st> <st c="48723">Let’s first talk about</st> <st c="48746">evaluating
    retrieval.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48767">Retrieval evaluation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="48788">Ragas</st> <st c="48795">provides metrics</st> <st c="48812">for
    evaluating each stage of the RAG pipeline in</st> <st c="48861">isolation.</st>
    <st c="48872">For retrieval, ragas has two metrics, called</st> **<st c="48917">context
    precision</st>** <st c="48934">and</st> **<st c="48939">context recall</st>**<st
    c="48953">. You</st> <st c="48959">can see this here in this part of the output</st>
    <st c="49004">and charts:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: <st c="49178">You can see the chart for the retrieval metrics in</st> *<st c="49230">Figure
    9</st>**<st c="49238">.2</st>*<st c="49240">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Chart showing retrieval performance comparison between similarity
    search and hybrid search](img/B22475_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="49254">Figure 9.2 – Chart showing retrieval performance comparison between
    similarity search and hybrid search</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49357">Retrieval</st> <st c="49368">evaluation is focused on assessing
    the accuracy and relevance of the documents that were retrieved.</st> <st c="49468">We
    do this with ragas using these two metrics, as described on the ragas</st> <st
    c="49541">documentation website:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="49632">context_precision</st>` <st c="49649">is a metric that evaluates
    whether all of the ground-truth-relevant items present in the contexts are ranked
    higher or not.</st> <st c="49774">Ideally, all the relevant chunks must appear
    at the top ranks.</st> <st c="49837">This metric is computed using the question,</st>
    `<st c="49881">ground_truth</st>`<st c="49893">, and</st> `<st c="49899">contexts</st>`<st
    c="49907">, with values ranging between</st> `<st c="49937">0</st>` <st c="49938">and</st>
    `<st c="49943">1</st>`<st c="49944">, where higher scores indicate</st> <st c="49975">better
    precision.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="50088">context_recall</st>` <st c="50102">measures the extent to which
    the retrieved context aligns with the annotated answer, treated as the ground
    truth.</st> <st c="50217">It is computed based on the ground truth and the retrieved
    context, and the values range between</st> `<st c="50314">0</st>` <st c="50315">and</st>
    `<st c="50320">1</st>`<st c="50321">, with higher values indicating</st> <st c="50353">better
    performance.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="50372">If you come from a traditional data science or information retrieval
    background, you may recognize the terms</st> *<st c="50482">precision</st>* <st
    c="50491">and</st> *<st c="50496">recall</st>* <st c="50502">and be wondering
    whether there is any relation to those terms.</st> <st c="50566">The context precision
    and context recall metrics used in ragas are conceptually similar to those traditional
    precision and</st> <st c="50689">recall metrics.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50704">In traditional terms, precision measures the proportion of retrieved
    items that are relevant, while recall measures the proportion of relevant items
    that</st> <st c="50859">are retrieved.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50873">Similarly, context precision evaluates the relevance of the retrieved
    context by assessing whether</st> <st c="50973">the ground-truth-relevant items
    are ranked higher, while context recall measures the extent to which the retrieved
    context covers the relevant information required to answer</st> <st c="51147">the
    question.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51160">However, there are some key differences</st> <st c="51201">to
    note.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51209">Traditional precision and recall are typically computed based
    on a binary relevance judgment (relevant or not relevant) for each item, whereas
    context precision and recall in ragas consider the ranking and alignment of the
    retrieved context with respect to the ground-truth answer.</st> <st c="51492">Additionally,
    context precision and recall are specifically designed to evaluate the retrieval
    performance in the context of question-answering tasks, taking into account the
    specific requirements of retrieving relevant information to answer a</st> <st
    c="51736">given question.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51751">When looking at the results from our analysis, we need to keep
    in mind that we are using a small dataset for our ground truth.</st> <st c="51879">In
    fact, the original dataset our entire RAG system is based on is small and that
    could also impact our results.</st> <st c="51992">Therefore, I wouldn’t read into
    the numbers you are seeing here too much.</st> <st c="52066">But what this does
    show you is how you can use ragas to run an analysis and then provide a very informative
    representation of what is happening in the retrieval stage of our RAG system.</st>
    <st c="52252">This code lab is primarily to demonstrate real-world challenges
    you will likely encounter when building a RAG system, where you have to consider
    different metrics in the context of your specific use case, the trade-offs between
    those different metrics, and having to decide which approach fits your needs in
    the most</st> <st c="52569">effective way.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52583">Next, we will review a similar analysis in the generation stage
    of our</st> <st c="52655">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="52666">Generation evaluation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="52688">As</st> <st c="52692">mentioned, ragas provides metrics for evaluating
    each stage of the RAG pipeline in isolation.</st> <st c="52786">For the generation
    stage, ragas has two metrics, called</st> `<st c="52842">faithfulness</st>` <st
    c="52854">and</st> `<st c="52859">answer relevancy</st>`<st c="52875">, as you
    see here in this part of the output and the</st> <st c="52928">following chart:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: <st c="53081">The generation metrics can be seen in the chart in</st> *<st c="53133">Figure
    9</st>**<st c="53141">.3</st>*<st c="53143">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Chart showing generation performance comparison between similarity
    search and hybrid search](img/B22475_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="53175">Figure 9.3 – Chart showing generation performance comparison between
    similarity search and hybrid search</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53279">Generation evaluation measures the appropriateness of the response
    generated by the system when the context is provided.</st> <st c="53401">We do
    this with ragas using the following two metrics, as described in the</st> <st
    c="53476">ragas documentation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="53496">faithfullness</st>`<st c="53510">: How factually accurate is
    the generated answer?</st> <st c="53561">This measures the factual consistency
    of the generated answer against the given context.</st> <st c="53650">It is calculated
    from the answer and retrieved context.</st> <st c="53706">The answer is scaled
    to a (</st>`<st c="53733">0-1</st>`<st c="53737">) range, with a higher score</st>
    <st c="53767">being better.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="53780">answer_relevancy</st>`<st c="53797">: How relevant is the generated
    answer to the question?</st> <st c="53854">Answer relevancy focuses on assessing
    how pertinent the generated answer is to the given prompt.</st> <st c="53951">A
    lower score is assigned to answers that are incomplete or contain redundant information
    and higher scores indicate better relevancy.</st> <st c="54086">This metric is
    computed using the question, the context, and</st> <st c="54147">the answer.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="54158">Again, I want</st> <st c="54173">to reiterate that we are using
    a small dataset for our ground truth and dataset, which likely makes these results
    less reliable.</st> <st c="54302">But you can see here how these results form
    the foundation for providing a very informative representation of what is happening
    in the generation stage of our</st> <st c="54461">RAG system.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54472">This leads us to our next set of metrics, the end-to-end evaluation
    metrics, which we</st> <st c="54559">discuss next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54572">End-to-end evaluation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="54594">Beyond providing the metrics for evaluating each stage of the
    RAG pipeline in isolation, ragas</st> <st c="54690">provides metrics for the entire
    RAG system, called end-to-end</st> <st c="54752">evaluation.</st> <st c="54764">For
    the generation stage, ragas has two</st> <st c="54804">metrics, called</st> **<st
    c="54820">answer correctness</st>** <st c="54838">and</st> **<st c="54843">answer
    similarity</st>**<st c="54860">, as you see here in the last part of the output</st>
    <st c="54909">and charts:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: <st c="55075">The chart in</st> *<st c="55089">Figure 9</st>**<st c="55097">.4</st>*
    <st c="55099">shows the visualization for</st> <st c="55128">these results:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Chart showing end-to-end performance comparison between similarity
    search and hybrid search](img/B22475_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="55186">Figure 9.4 – Chart showing end-to-end performance comparison between
    similarity search and hybrid search</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55290">End-to-end</st> <st c="55302">metrics are for evaluating the end-to-end
    performance of a pipeline, gauging the overall experience of using the pipeline.</st>
    <st c="55425">Combining these metrics provides a comprehensive evaluation of the
    RAG pipeline.</st> <st c="55506">We do this with ragas using the following two
    metrics, as described in the</st> <st c="55581">ragas documentation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="55601">answer_correctness</st>`<st c="55620">: Gauges the accuracy
    of the generated answer when compared to the ground truth.</st> <st c="55702">The
    assessment of answer correctness involves gauging the accuracy of the generated
    answer when compared to the ground truth.</st> <st c="55828">This evaluation relies
    on the ground truth and the answer, with scores ranging from 0 to 1\.</st> <st
    c="55920">A higher score indicates a closer alignment between the generated answer
    and the ground truth, signifying</st> <st c="56026">better correctness.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="56045">answer_similarity</st>`<st c="56063">: Assesses the semantic
    resemblance between the generated answer and the ground truth.</st> <st c="56151">The
    concept of answer semantic similarity pertains to the assessment of the semantic
    resemblance between the generated answer and the ground truth.</st> <st c="56299">This
    evaluation is based on the ground truth and the answer, with values falling within
    the range of</st> `<st c="56400">0</st>` <st c="56401">to</st> `<st c="56405">1</st>`<st
    c="56406">. A higher score signifies a better alignment between the generated
    answer and the</st> <st c="56489">ground truth.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="56502">Evaluating</st> <st c="56514">the end-to-end performance of a
    pipeline is also crucial, as it directly affects the user experience and helps
    to ensure a</st> <st c="56637">comprehensive evaluation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56662">To keep this code lab simple, we left out a couple more metrics
    you might also consider in your analysis.</st> <st c="56769">Let’s talk about
    those</st> <st c="56792">metrics next.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56805">Other component-wise evaluation</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="56837">Component-wise evaluation involves evaluating individual components
    of the pipeline, such as the</st> <st c="56935">retrieval and generation stages,
    to gain insights into their effectiveness and identify areas for improvement.</st>
    <st c="57046">We already shared two metrics for each of these stages, but here
    are a couple more that are available in the</st> <st c="57155">ragas platform:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="57338">(0-1)</st>`<st c="57343">, with higher values indicating</st>
    <st c="57375">better relevancy.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<st c="57531">ground_truth</st>` <st c="57543">data and</st> `<st c="57553">contexts</st>`
    <st c="57561">data relative to the number of entities present in the</st> `<st
    c="57617">ground_truth</st>` <st c="57629">data alone.</st> <st c="57642">Simply
    put, it is a measure of what fraction of entities are recalled from</st> `<st
    c="57717">ground_truth</st>` <st c="57729">data.</st> <st c="57736">This metric
    is particularly useful in fact-based use cases such as a tourism help desk and
    historical Q&A.</st> <st c="57843">This metric can help evaluate the retrieval
    mechanism for entities, based on comparison with entities present in</st> `<st
    c="57956">ground_truth</st>` <st c="57968">data, because in cases where entities
    matter, we need the contexts that</st> <st c="58041">cover them.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="58052">Aspect critique</st>**<st c="58068">: Aspect critique is designed
    to assess submissions based on predefined aspects such as harmlessness and correctness.</st>
    <st c="58187">Additionally, users have the flexibility to define their own aspects
    for evaluating submissions according to their specific criteria.</st> <st c="58321">The
    output of aspect critiques is binary, indicating whether the submission aligns
    with the defined aspect or not.</st> <st c="58436">This evaluation is performed
    using the “</st>*<st c="58476">answer</st>*<st c="58483">”</st> <st c="58486">as
    input.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="58495">These additional component-wise evaluation metrics offer further
    granularity in assessing the retrieved context and the generated answers.</st>
    <st c="58635">To finish off this code lab, we are going</st> <st c="58677">to
    bring in some insights that one of the founders of ragas provided directly to
    help you with your</st> <st c="58777">RAG evaluation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58792">Founder’s perspective</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="58814">In preparation for this chapter, we had a chance to talk with
    one of the founders of ragas, Shahul Es, to gain additional insights into the
    platform and how you can better utilize it for RAG development and evaluation.</st>
    <st c="59034">Ragas is a young platform, but as you have seen in the code lab,
    it already has a solid foundation of metrics that you can implement to evaluate
    your RAG system.</st> <st c="59196">But this also means ragas has a lot of room
    for growth, this platform that is built specifically for RAG implementations will
    continue to evolve.</st> <st c="59342">Shahul provided some helpful tips and insights
    that we will summarize and share with you here.</st> <st c="59437">We share notes
    from that discussion in the</st> <st c="59480">following section.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="59498">Ragas founder insights</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="59521">The following</st> <st c="59536">is a list of notes taken from
    a discussion with ragas co-founder Shahul Es, discussing how ragas can be used
    for</st> <st c="59649">RAG evaluation:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="59664">Synthetic data generation</st>**<st c="59690">: The first roadblock
    people typically face with RAG evaluation is not having enough testing ground-truth
    data.</st> <st c="59803">Ragas’ main focus is to create an algorithm that could
    create a test dataset that covers a wide variety of question types, resulting
    in their synthetic data generation capabilities.</st> <st c="59985">Once you have
    used ragas to synthesize your ground truth, it is helpful to vet the ground truth
    generated and pick out any questions that</st> <st c="60123">don’t belong.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="60136">Feedback metrics</st>**<st c="60153">: Something that is currently
    being emphasized in their development is incorporating various feedback loops
    into the evaluation from both performance and user feedback, where there are explicit
    metrics (something went wrong) and implicit metrics (levels of satisfaction, thumbs
    up/down, and similar mechanisms).</st> <st c="60466">Any kind of interaction with
    the user can potentially be implicit.</st> <st c="60533">Implicit feedback can
    be noisy (from a data standpoint), but can still be useful if</st> <st c="60617">used
    properly.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="60631">Reference and reference-free metrics</st>**<st c="60668">:
    Shahul categorized the metrics into reference metrics and reference-free metrics,
    where reference means it requires a ground truth to process.</st> <st c="60814">The
    ragas team place emphasis on building reference-free metrics in their work, which
    you can read more about in the ragas paper (</st>[<st c="60944">https://arxiv.org/abs/2309.15217</st>](https://arxiv.org/abs/2309.15217)<st
    c="60977">).</st> <st c="60981">For many fields, where the ground truth is difficult
    to collect, this is an important point, as this makes at least</st> <st c="61097">some
    of the evaluation still possible.</st> <st c="61136">Faithfulness and answer relevance
    were reference-free metrics</st> <st c="61198">Shahul mentioned.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**<st c="61215">Deployment evaluation</st>**<st c="61237">: Reference-free
    evaluation metrics are also ideal for deployment evaluation, where you are less
    likely to have a ground</st> <st c="61359">truth available.</st>'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="61375">These are some key insights, and it will be exciting to see where
    ragas development goes in the future to help us all continually improve our RAG
    systems.</st> <st c="61531">You can find the latest ragas documentation</st> <st
    c="61575">here:</st> [<st c="61581">https://docs.ragas.io/en/stable/</st>](https://docs.ragas.io/en/stable/)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61613">That concludes our evaluation code lab using ragas.</st> <st c="61666">But
    ragas is not the only evaluation tool that is used for RAG evaluation; there are
    many more!</st> <st c="61762">Next, we will discuss some other approaches you</st>
    <st c="61810">can consider.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="61823">Additional evaluation techniques</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="61856">Ragas is just one of many evaluation tools and techniques available
    to evaluate your RAG system.</st> <st c="61954">This is not an exhaustive list,
    but in the following subsections, we will discuss some of the more popular techniques
    you can use to evaluate the performance of your RAG system, once you have obtained
    or generated</st> <st c="62168">ground-truth data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62186">Bilingual Evaluation Understudy (BLEU)</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62225">BLEU measures the overlap of n-grams between the generated response
    and the ground-truth</st> <st c="62315">response.</st> <st c="62325">It provides
    a score indicating the similarity between the two.</st> <st c="62388">In the context
    of RAG, BLEU can be used to evaluate the quality of the generated answers by comparing
    them to the ground-truth answers.</st> <st c="62524">By calculating the n-gram
    overlap, BLEU assesses how closely the generated answers match the reference answers
    in terms of word choice and phrasing.</st> <st c="62673">However, it’s important
    to note that BLEU is more focused on surface-level similarity and may not capture
    the semantic meaning or relevance of the</st> <st c="62820">generated answers.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="62838">Recall-Oriented Understudy for Gisting Evaluation (ROUGE)</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="62896">ROUGE assesses the quality of the generated response by comparing
    it to the ground truth</st> <st c="62986">in terms of recall.</st> <st c="63006">It
    measures how much of the ground truth is captured in the generated response.</st>
    <st c="63086">For RAG evaluation, ROUGE can be used to evaluate the coverage and
    completeness of the generated answers.</st> <st c="63192">By calculating the recall
    between the generated answers and the ground-truth answers, ROUGE assesses how
    well the generated answers capture the key information and details present in
    the reference answers.</st> <st c="63398">ROUGE is particularly useful when the
    ground-truth answers are longer or more detailed, as it focuses on the overlap
    of information rather than exact</st> <st c="63548">word matches.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="63561">Semantic similarity</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="63581">Metrics</st> <st c="63590">such as cosine similarity or</st> **<st
    c="63619">semantic textual similarity</st>** <st c="63646">(</st>**<st c="63648">STS</st>**<st
    c="63651">) can be used to evaluate the semantic relevance between the generated
    response and the ground truth.</st> <st c="63754">These metrics capture the meaning
    and context beyond exact word matches.</st> <st c="63827">In RAG evaluation, semantic
    similarity metrics can be used to assess the semantic coherence and relevance
    of the generated answers.</st> <st c="63959">By comparing the semantic representations
    of the generated answers and the ground-truth answers, these metrics evaluate
    how well the generated answers capture the underlying meaning and context of the
    reference answers.</st> <st c="64179">Semantic similarity metrics are particularly
    useful when the generated answers may use different words or phrasing but still
    convey the same meaning as the</st> <st c="64335">ground truth.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="64348">Human evaluation</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="64365">While automated metrics provide a quantitative assessment, human
    evaluation remains important</st> <st c="64460">for assessing the coherence, fluency,
    and overall quality of the generated responses compared to the ground truth.</st>
    <st c="64575">In the context of RAG, human evaluation involves having human raters
    assess the generated answers based on various criteria.</st> <st c="64700">These
    criteria may include relevance to the question, factual correctness, clarity of
    the answer, and overall coherence.</st> <st c="64821">Human evaluators can provide
    qualitative feedback and insights that automated metrics may not capture, such
    as the appropriateness of the answer tone, the presence of any inconsistencies
    or contradictions, and the overall user experience.</st> <st c="65060">Human evaluation
    can complement automated metrics by providing a more comprehensive and nuanced
    assessment of the RAG</st> <st c="65178">system’s performance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65199">When evaluating a RAG system, it’s often beneficial to use a combination
    of these evaluation techniques to obtain a holistic view of the system’s performance.</st>
    <st c="65359">Each technique has its strengths and limitations, and using multiple
    metrics can provide a more robust and comprehensive evaluation.</st> <st c="65492">Additionally,
    it’s important to consider the specific requirements and goals of your RAG application
    when selecting the appropriate evaluation techniques.</st> <st c="65647">Some
    applications may prioritize factual correctness, while others may focus more on
    the fluency and coherence of the generated answers.</st> <st c="65784">By aligning
    the evaluation techniques with your specific needs, you can effectively assess
    the performance of your RAG system and identify areas</st> <st c="65929">for improvement.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="65945">Summary</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="65953">In this chapter, we explored the key role that evaluation plays
    in building and maintaining RAG pipelines.</st> <st c="66061">We discussed how
    evaluation helps developers identify areas for improvement, optimize system performance,
    and measure the impact of modifications throughout the development process.</st>
    <st c="66243">We also highlighted the importance of evaluating the system after
    deployment to ensure ongoing effectiveness, reliability,</st> <st c="66366">and
    performance.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66382">We introduced standardized evaluation frameworks for various components
    of a RAG pipeline, such as embedding models, vector stores, vector search, and
    LLMs.</st> <st c="66540">These frameworks provide valuable benchmarks for comparing
    the performance of different models and components.</st> <st c="66651">We emphasized
    the significance of ground-truth data in RAG evaluation and discussed methods
    for obtaining or generating the ground truth, including human annotation, expert
    knowledge, crowdsourcing, and synthetic</st> <st c="66864">ground-truth generation.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="66888">The chapter included a hands-on code lab where we integrated the
    ragas evaluation platform into our RAG system.</st> <st c="67001">We generated
    synthetic ground-truth data and established a comprehensive set of metrics to
    evaluate the impact of using hybrid search compared to the original dense vector
    semantic-based search.</st> <st c="67196">We explored the different stages of
    RAG evaluation, including retrieval evaluation, generation evaluation, and end-to-end
    evaluation, and analyzed the results obtained from our evaluation.</st> <st c="67385">The
    code lab provided a real-world example of implementing a comprehensive evaluation
    system in a RAG pipeline, demonstrating how developers can leverage evaluation
    metrics to gain insights and make data-driven decisions to improve their RAG pipelines.</st>
    <st c="67638">We were also able to share key insights from one of the founders
    of ragas to help your RAG evaluation efforts</st> <st c="67748">even further.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67761">In the next chapter, we will start our discussion about how to
    utilize LangChain with the key components of RAG systems in the most</st> <st
    c="67894">effective way.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="67908">References</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`<st c="67919">MSMARCO</st>`<st c="67927">:</st> [<st c="67930">https://microsoft.github.io/msmarco/</st>](https://microsoft.github.io/msmarco/)'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="67966">HotpotQA</st>`<st c="67975">:</st> [<st c="67978">https://hotpotqa.github.io/</st>](https://hotpotqa.github.io/)'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="68005">CQADupStack</st>`<st c="68017">:</st> [<st c="68020">http://nlp.cis.unimelb.edu.au/resources/cqadupstack/</st>](http://nlp.cis.unimelb.edu.au/resources/cqadupstack/)'
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="68072">Chatbot</st>** **<st c="68081">Arena</st>**<st c="68086">:</st>
    [<st c="68089">https://chat.lmsys.org/?leaderboard</st>](https://chat.lmsys.org/?leaderboard)'
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="68124">MMLU</st>**<st c="68129">:</st> [<st c="68132">https://arxiv.org/abs/2009.03300</st>](https://arxiv.org/abs/2009.03300)'
  prefs: []
  type: TYPE_NORMAL
- en: '**<st c="68164">MT</st>** **<st c="68168">Bench</st>**<st c="68173">:</st>
    [<st c="68176">https://arxiv.org/pdf/2402.14762</st>](https://arxiv.org/pdf/2402.14762)**'
  prefs: []
  type: TYPE_NORMAL
