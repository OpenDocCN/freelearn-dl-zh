<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">ES-HyperNEAT and the Retina Problem</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">In this chapter, you will learn about the ES-HyperNEAT extension of the HyperNEAT method, which we discussed in the previous chapter. As you learned in the previous chapter, the HyperNEAT method allows the encoding of larger-scale <strong>artificial neural network</strong> (<strong>ANN</strong>) topologies, which is essential for working in areas where the input data has a large number of dimensions, such as computer vision. However, despite all its power, the HyperNEAT method has a significant drawback—the configuration of the ANN substrate should be designed beforehand by a human architect. The ES-HyperNEAT method was invented to address this issue by introducing the concept of evolvable-substrate, which</span><span class="s1"> allows us to produce the appropriate configuration of the substrate </span><span>automatically </span><span>during evolution.</span></p>
<p>After familiarizing yourself with the <span>basics of the </span><span>ES-HyperNEAT method, you will have a chance to apply this knowledge to solve the modular retina problem. During this task, we will show you how to choose an appropriate initial substrate configuration that helps the evolutionary process to discover the modular structures. Also, we will discuss the source code of the modular retina problem solver along with the test environment, which can be used to evaluate the fitness of each detector ANN.</span></p>
<p class="p1"><span class="s1">Through this chapter, you will gain hands-on experience with applying the ES-HyperNEAT method using the MultiNEAT Python library.</span></p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Manual versus evolution-based configuration of the topography of neural nodes</li>
<li class="mce-root">Quadtree information extraction and ES-HyperNEAT basics</li>
<li class="mce-root">The modular left and right retina experiment</li>
<li>Discussion of the experiment results</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The following technical requirements should be met to execute the experiments described in this chapter:</span></p>
<ul class="ul1">
<li class="li3"><span class="s1">Windows 8/10, macOS 10.13 or newer, or modern Linux</span></li>
<li class="li3"><span class="s1">Anaconda Distribution version 2019.03 or newer</span></li>
</ul>
<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter8">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter8</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Manual versus evolution-based configuration of the topography of neural nodes</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">The HyperNEAT method, which we discussed in <a href="21fb699f-605d-4156-aa5c-5ba501dc09cf.xhtml" target="_blank">Chapter 7</a>, <em>Hypercube-Based NEAT for Visual Discrimination</em>, allows us to use neuroevolution methods for a broad class of problems that require the use of large-scale ANN structures to find a solution. This class of problem spreads across multiple practical domains, including visual pattern recognition. The main distinguishing feature of all these problems is the high dimensionality of the input/output data.</span></p>
<p class="p1"><span class="s1">In the previous chapter, you learned how to define the configuration of the substrate of the discriminator ANN to solve a visual discrimination task. You also learned that it is crucial to use an appropriate substrate configuration that is aligned with the geometric features of the search space of the target problem. With the HyperNEAT method, you, as an architect, need to define the substrate configuration beforehand, using only your understanding of the spatial geometry of the problem. However, it is not always possible to learn about all the geometric regularities hidden behind a specific problem space.</span></p>
<p class="p1"><span class="s1">If you design the substrate manually, you create an unintentional constraint on the pattern of weights drawn over it by the connective<strong> </strong><span><strong>Compositional Pattern Producing Networks</strong> (</span><strong>CPPNs</strong>). By placing nodes at specific locations in the substrate, you interfere with the ability of the CPPN to discover the geometric regularities of the natural world. The CPPN should produce a connectivity pattern that is perfectly aligned with the structure of the substrate that you provided, and connections are </span><span>only</span><span> </span><span>possible between the nodes of this structure. This limitation leads to unnecessary approximation errors, which taints the results when you use an evolved CPPN to create the topology of the solution-solver ANN (the phenotype).</span></p>
<p class="p1"><span class="s1">However, why are the limitations that are introduced with manual substrate configuration inflicted in the first place? Would it be better if the CPPN could elaborate on the connectivity patterns between the nodes of the substrate that are automatically positioned <span>in the right</span> <span>locations </span>in the substrate? It seems that evolving connectivity patterns in the substrate provides valuable implicit hints that help us to estimate the nodes' positions for the next epoch of the evolution. The method of substrate configuration evolution during the CPPN training got a name: <strong>Evolvable-Substrate</strong>.</span></p>
<p class="p1"><span class="s1">The implicit data allowing us to estimate the position of the next node is the amount of the information encoded by the connectivity pattern in the specific substrate area. The areas with a uniform distribution of connection weights encode a small amount of information, thereby requiring only a few substrate nodes in those areas. At the same time, substrate areas with large gradients of connection weights are informationally intensive and can benefit from additional nodes placed within those areas. When you place an additional node in such areas of the substrate, you allow the CPPN to represent the much more granular encoding of the natural world. Thus, the placement of the nodes and the connectivity pattern can be mandated by the distribution of the connection weights while the CPPN produces the connection weights during the evolution.</span></p>
<p class="p1"><span class="s1">HyperNEAT represents each connection between two nodes of the substrate as a point in the four-dimensional hypercube. The evolvable-substrate HyperNEAT algorithm extends HyperNEAT by automatically placing fewer hyperpoints in the areas of the hypercube with lower variation in the connection weights. Thus, ES-HyperNEAT uses information density as the primary guiding principle when determining the topology of the substrate during the evolution.</span></p>
<p>In the next section, we discuss the particulars of the <span>ES-HyperNEAT algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Quadtree information extraction and ES-HyperNEAT basics</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">For the effective calculation of the information density within the connectivity patterns of the substrate, we need to use an appropriate data structure. We need to employ a data structure that allows an effective search through the two-dimensional substrate space at different levels of granularity. In computer science, there is a data structure that perfectly fits these requirements. This structure is the <strong>quadtree</strong>.</span></p>
<p class="p3"><span class="s1">The quadtree is a data structure that allows us to organize an effective search through two-dimensional space by splitting any area of interest into four subareas. Each of these subareas consequently becomes a leaf of a tree, with the root node representing the initial region.</span></p>
<p class="p3"><span class="s1">ES-HyperNEAT employs the quadtree data structure to iteratively look for the new connections and nodes in the substrate, starting from the input and the output nodes predefined by the data scientist. Using a quadtree to search for new connections and nodes is much more computationally effective than searching in the four-dimensional space of the hypercube.</span></p>
<p class="p3"><span class="s1">The scheme of information extraction using the quadtree structure is shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-791 image-border" src="assets/e7cca6a3-0382-4424-b28f-3b759c53f33e.png" style="width:27.75em;height:36.75em;"/></p>
<div class="p1 packt_figref CDPAlignCenter CDPAlign"><span class="s1">The scheme of information extraction</span></div>
<p class="p1"><span class="s1">The information extraction method depicted in the diagram has two major parts:</span></p>
<ol class="ol1">
<li class="li1"><span class="s1">The <strong>division and initialization</strong> stage is presented in the top part of the diagram. At this stage, the quadtree is created by recursively dividing the initial substrate area, which spans from (<em>-1</em>, <em>-1</em>) to (<em>1, 1</em>). The division stops when the desired depth of the quadtree is reached. Now we have several subspaces that are fitted into the substrate, determining the initial substrate resolution (<em>r</em>). Next, for every node of the quadtree with a center at (<img class="fm-editor-equation" src="assets/7b9cf86d-a9b4-429d-bc7e-faa039aa599c.png" style="width:1.17em;height:0.92em;"/>,<img class="fm-editor-equation" src="assets/058e9c7f-9dc4-42db-9c59-311c42c52554.png" style="width:0.92em;height:0.92em;"/>), we query the CPPN to find a connection weight (<em>w</em>) between this node and a specific input or output neuron at coordinates (<em>a, b</em>). When we have calculated the connection weights for the<em> k</em> leaf nodes in the subtree of the quadtree, <em>p</em>, we are ready to calculate the information variance of the node, <em>p</em>, in the quadtree as follows:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><span class="s1"> <img class="fm-editor-equation" src="assets/4c372061-3f19-40ba-90fe-8c7d3773a415.png" style="width:11.33em;height:4.42em;"/></span></p>
<p style="padding-left: 60px"><span class="s1"><br/></span><span class="s1"><img class="fm-editor-equation" src="assets/806e14e4-960c-4327-bd4c-d6c048112dd3.png" style="width:0.83em;height:0.92em;"/> is the mean connection weight among <em>k</em> leaf nodes and <img class="fm-editor-equation" src="assets/7567b3d8-2e17-4436-9d1b-8bc18199c6ee.png" style="width:1.33em;height:1.00em;"/> is a connection weight to each leaf node.</span></p>
<p style="padding-left: 60px"><span class="s1">We can use this estimated variance value as a heuristic indicator of the information density in the specific subarea of the substrate. The higher this value, the higher the information density. The variance can be used to manage the information density in the specific subarea of the substrate by introducing the <strong>division threshold</strong> constant. If the variance is greater than the division threshold, then the division stage is repeated until the desired information density is reached.</span></p>
<p style="padding-left: 60px"><span class="s1">At this stage, we create an indicative structure that allows the CPPN to decide where to make connections within the given substrate. The next stage of the processing places all necessary connections using the created quadtree structure.</span></p>
<ol start="2">
<li>The<strong> pruning and extraction</strong> stage is represented in the bottom part of the diagram. In this stage, we use the populated quadtree structure from the previous stage to find the regions with high variance and make sure that more connections are expressed among the nodes of these regions. We traverse the quadtree depth-first and stop the traversal at the node that has a variance value that's smaller than the given<strong> variance threshold </strong>(<img src="assets/00f974a1-7ad2-4378-a76b-079f334dc42b.png" style="width:1.08em;height:1.33em;"/>) or when the current node has no children (that is, has zero variance). For every quadtree node found by the depth-first search, we express the connection between the center of the node (<em>x</em>, <em>y</em>) and each parent node that is already determined. The parent node can either be determined by an architect (input/output nodes) or be found in the previous runs of the information extraction method, that is, from hidden nodes already created by the ES-HyperNEAT method. When this stage completes, the substrate configuration will have more nodes in the informationally intensive substrate regions and fewer nodes in the regions encoding a small amount of information.</li>
</ol>
<p class="p1"><span class="s1">In the following section, we will discuss how to use the ES-HyperNEAT algorithm we've just described to find a solution for the modular retina problem.</span></p>
<div class="p1 packt_infobox"><span class="s1">For more details about the ES-HyperNEAT algorithm, refer to <a href="f59c6396-55e5-4495-95c0-7af9a42c2f20.xhtml" target="_blank">Chapter 1</a>, <em>Overview of Neuroevolution Methods</em>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modular retina problem basics</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The hierarchical modular structures are an essential part of the complex biological organisms and play an indispensable role in their evolution. The modularity enhances the evolvability, allowing the recombination of various modules during the evolution process. The evolved hierarchy of modular components bootstraps the evolution process, allowing operations over a collection of complex structures rather than basic genes. After that, the neuroevolutionary process does not need to spend time to evolve similar functionality from scratch again. Instead, the ready-to-use modular components can be used as building blocks to produce very complex neural networks.</span></p>
<p class="p3"><span class="s1">In this chapter, we will implement a solution to the retina problem using the ES-HyperNEAT algorithm. The retina problem is about the simultaneous identification of valid 2x2 patterns on the left and the right side of an artificial retina that has a resolution of 4x2. Thus, the detector ANN must decide if the patterns presented on the left and the right side of the retina are valid for the corresponding side of the retina (left or right).</span></p>
<p class="p3"><span class="s1">In the retina problem, the left and the right problem components are perfectly separated into different functional units. At the same time, some components can be present on each side of the retina, while others are unique to a specific part of the retina. Thus, to produce a successful detector ANN, the neuroevolution process needs to discover the modular structures separately for the left and the right detection zones.</span></p>
<p class="p3"><span class="s1">The retina problem scheme is shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-792 image-border" src="assets/6015ae45-08dd-4dcd-afcf-3af1767e3ab2.png" style="width:30.75em;height:20.08em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The retina problem scheme</span></div>
<p class="p1"><span class="s1">As you can see in the preceding diagram, the artificial retina is represented as a 2D grid with a resolution of 4x2 pixels. The values of the two-dimensional array representing the patterns drawn on the retina constitute the inputs of the detector ANN. The filled pixels in the array have a value of <kbd>1.0</kbd> and the empty pixels have a value of <kbd>0.0</kbd>. With the given resolution, it is possible to draw 16 different 2x2 patterns for the left and the right parts of the retina. Thus, we have eight valid patterns for the left side and eight valid patterns for the right side of the retina. Some of the patterns <span>mentioned </span>are valid for both sides of the retina.</span></p>
<p class="p1"><span class="s1">The scheme of decision-making by the detector ANN in the retina problem domain is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-793 image-border" src="assets/4a0213e3-11fc-47ec-a951-e49296ae65fc.png" style="width:21.25em;height:14.92em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The scheme of decision making by the detector ANN</span></div>
<p class="p1"><span class="s1">The detector ANN has eight inputs to accept input data patterns from both sides of the retina and two output nodes. Each of the output nodes produces a value that can be used to classify the pattern's validity at each side of the retina. The first output node is assigned to the left and the second node to the right side of the retina correspondingly. The activation value of the output node that is greater than or equal to <kbd>0.5</kbd> classifies the pattern for the related side of the retina as valid. If the activation value is less than <kbd>0.5</kbd>, the pattern is considered not valid. To even further simplify the detection, we apply rounding to the values of the output nodes according to the rounding scheme shown in the diagram. Thus, each output node of the detector ANN serves as a binary classifier for the related part of the retina that produces a value of <kbd>0.0</kbd> or <kbd>1.0</kbd> to mark the input pattern as invalid or valid correspondingly.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Objective function definition</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">The task of the detector ANN is to correctly classify the inputs from the left and right sides of the retina as valid or not by producing a vector of the binary outputs with values of <kbd>0.0</kbd> or <kbd>1.0</kbd>. The output vector has a length of 2, which is equal to number of the output nodes.</span></p>
<p class="p1"><span class="s1">We can define the detection error as the Euclidean distance between the vector with ground truth values and the vector with ANN output values, as given by the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/264a3ace-8838-4bf8-a668-371867d9bc28.png" style="width:10.83em;height:4.33em;"/></p>
<p class="p1"><span class="s1"><img class="fm-editor-equation" src="assets/9132f8c9-c333-422f-a337-a3652a762eae.png" style="width:1.00em;height:1.08em;"/> is the squared detection error for one trial, <img class="fm-editor-equation" src="assets/75051026-cfcc-4719-9751-778e01da36aa.png" style="width:0.83em;height:0.92em;"/> is the vector with detector ANN outputs, and <img class="fm-editor-equation" src="assets/a1fbc606-d1ef-4b6e-9a30-29a33ff88b84.png" style="width:0.67em;height:1.42em;"/> is the vector with the ground truth values.</span></p>
<p class="p1"><span class="s1">At each generation of the evolution, we evaluate each detector ANN (phenotype) against all 256 </span><span>possible</span><span> </span><span>combinations of 4x4 retina patterns, which are produced by combining 16 different 2x2 patterns for each side of the retina. Thus, to get a final detection error value for the particular detector ANN, we calculate the sum of 256 error values obtained for each configuration of the retina patterns, as indicated by the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4c0370c0-7e99-48b6-a17e-607e740979a1.png" style="width:6.17em;height:4.42em;"/></p>
<p><img class="fm-editor-equation" src="assets/653fbe3d-5f27-475f-b9c4-f46b68d9ce66.png" style="width:0.58em;height:1.00em;"/> <span class="s1">is the sum of all errors obtained during 256 trials and <img class="fm-editor-equation" src="assets/debe4e93-4cab-486d-bc3e-8161c06bf4af.png" style="width:0.92em;height:1.25em;"/><span class="Apple-converted-space"> </span>is the squared detection error for a particular trial.</span></p>
<p class="p1"><span class="s1">The fitness function can be defined as the inverse of the sum of the errors obtained from all 256 trials against all possible retina patterns, as shown in the following formula:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/da755433-17e7-417d-8e5d-0cb93ed122fe.png" style="width:7.00em;height:2.83em;"/></p>
<p class="p1"><span class="s1">We add <kbd>1.0</kbd> to the sum of errors (<img class="fm-editor-equation" src="assets/bd661974-d4b9-45c8-b734-3585e3f849ef.png" style="width:0.58em;height:1.00em;"/>) in the denominator to avoid dividing by 0 in cases when all trials produce no error. Thus, according to the fitness function formula, the maximum value of the fitness score in our experiment is <kbd>1000.0</kbd>, which we will use as a fitness threshold value later.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modular retina experiment setup</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">In this section, we discuss the details of an experiment aimed at creating a successful solver of the modular retina problem. In our experiment, we use this problem as a benchmark to test the ability of the ES-HyperNEAT method to discover modular topologies in the phenotype ANN.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The initial substrate configuration</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">As described earlier in the chapter, the retina has dimensions of 4x2, with two <span>2x2 </span>areas, one on the left side and one on the right side. The particulars of the retina geometry must be represented in the geometry of the initial substrate configuration. In our experiment, we use a three-dimensional substrate, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-794 image-border" src="assets/8a75c354-2a52-42a3-ba74-c43d033e7e8f.png" style="width:29.08em;height:15.58em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The initial substrate configuration</span></div>
<p class="p1"><span class="s1">As you can see in the diagram, the input nodes are placed within the XZ plane, which is orthogonal to the XY plane. They are presented in two groups, with four nodes to describe the left and right sides of the retina. The two output and bias nodes are located within the XY plane, which divides the Z-plane in half with the input nodes. The evolution of the substrate creates new hidden nodes in the same XY plane where the output nodes are located. The evolved connective CPPN draws the connectivity patterns between all nodes within the substrate. Our ultimate goal is to evolve the CPPN and the substrate configuration, which produces an appropriate modular graph of the detector ANN. This graph should include two modules, each representing an appropriate configuration for the binary classifier, which we discussed earlier. Let's now look at the test environment for the modular retina problem.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Test environment for the modular retina problem</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">First, we need to create a test environment that can be used to evaluate the results of the neuroevolution process that aims to create a successful detector ANN. The test environment should create a dataset that consists of all possible patterns of pixels on the retina. Also, it should provide functions to evaluate the detector ANN against each pattern in the dataset. Thus, the test environment can be divided into two main parts:</span></p>
<ul>
<li><span class="s1">The data structure to hold visual patterns for the left-hand, the right-hand, or both sides of the retina</span></li>
<li><span class="s1">The test environment storing the dataset and providing functions for detector ANN evaluation<br/></span></li>
</ul>
<p class="p1"><span class="s1">In the following sections, we provide a detailed description of each part.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The visual object definition</h1>
                </header>
            
            <article>
                
<p class="p2"><span class="s2">Each of the allowed configurations of pixels in the specific part of the retina space can be represented as a separate visual object. The Python class encapsulating the related functionality is named <kbd>VisualObject</kbd> and is defined in the <kbd>retina_experiment.py</kbd> file. It has the following constructor:</span></p>
<pre>    def __init__(self, configuration, side, size=2):<br/>        self.size = size<br/>        self.side = side<br/>        self.configuration = configuration<br/>        self.data = np.zeros((size, size))<br/>        <br/>        # Parse configuration<br/>        lines = self.configuration.splitlines()<br/>        for r, line in enumerate(lines):<br/>            chars = line.split(" ")<br/>            for c, ch in enumerate(chars):<br/>                if ch == 'o':<br/>                    # pixel is ON<br/>                    self.data[r, c] = 1.0<br/>                else:<br/>                    # pixel is OFF<br/>                    self.data[r, c] = 0.0</pre>
<p class="p1"><span class="s1">The constructor receives the configuration of a particular visual object as a string, along with a valid location for this object in the retina space. After that, it assigns received parameters to the internal fields and creates a two-dimensional data array holding the states of the pixels in the visual object.</span></p>
<p class="p1"><span class="s1">The pixels' states are obtained by parsing the visual object configuration string as follows:</span></p>
<pre>        # Parse configuration<br/>        lines = self.configuration.splitlines()<br/>        for r, line in enumerate(lines):<br/>            chars = line.split(" ")<br/>            for c, ch in enumerate(chars):<br/>                if ch == 'o':<br/>                    # pixel is ON<br/>                    self.data[r, c] = 1.0<br/>                else:<br/>                    # pixel is OFF<br/>                    self.data[r, c] = 0.0</pre>
<p class="p1"><span class="s1">The visual object configuration string has four characters, excluding the line break, which define the state of the corresponding pixel in the visual object. If the symbol at a specific position in the configuration line is <kbd>o</kbd>, then the pixel at the corresponding position of the visual object is set to the ON state, and the value <kbd>1.0</kbd> is saved to the data array at this position.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The retina environment definition</h1>
                </header>
            
            <article>
                
<p class="p2"><span class="s2">The retina environment creates and stores the dataset consisting of all possible visual objects and provides functions for evaluating the fitness of the detector ANN. It has the following main implementation parts.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The function to create a dataset with all the possible visual objects</h1>
                </header>
            
            <article>
                
<p>In this function, we create the visual objects for the dataset as follows:</p>
<pre style="padding-left: 30px">    def create_data_set(self):<br/>        # set left side objects<br/>        self.visual_objects.append(VisualObject(". .\n. .", <br/>                                                side=Side.BOTH))<br/>        self.visual_objects.append(VisualObject(". .\n. o", <br/>                                                side=Side.BOTH))<br/>        self.visual_objects.append(VisualObject(". o\n. o", <br/>                                                side=Side.LEFT))<br/>        self.visual_objects.append(VisualObject(". o\n. .", <br/>                                                side=Side.BOTH))<br/>        self.visual_objects.append(VisualObject(". o\no o", <br/>                                                side=Side.LEFT))<br/>        self.visual_objects.append(VisualObject(". .\no .", <br/>                                                side=Side.BOTH))<br/>        self.visual_objects.append(VisualObject("o o\n. o", <br/>                                                side=Side.LEFT))<br/>        self.visual_objects.append(VisualObject("o .\n. .", <br/>                                                side=Side.BOTH))</pre>
<p>The preceding code creates visual objects for the left side of the retina. The visual objects for the right side can be created in a similar way:</p>
<pre style="padding-left: 30px">       # set right side objects<br/>       self.visual_objects.append(VisualObject(". .\n. .", <br/>                                               side=Side.BOTH))<br/>       self.visual_objects.append(VisualObject("o .\n. .", <br/>                                               side=Side.BOTH))<br/>       self.visual_objects.append(VisualObject("o .\no .", <br/>                                               side=Side.RIGHT))<br/>       self.visual_objects.append(VisualObject(". .\no .", <br/>                                               side=Side.BOTH))<br/>       self.visual_objects.append(VisualObject("o o\no .", <br/>                                               side=Side.RIGHT))<br/>       self.visual_objects.append(VisualObject(". o\n. .", <br/>                                               side=Side.BOTH))<br/>       self.visual_objects.append(VisualObject("o .\no o", <br/>                                               side=Side.RIGHT))<br/>       self.visual_objects.append(VisualObject(". .\n. o", <br/>                                               side=Side.BOTH))</pre>
<p class="p1"><span class="s1">The created objects appended to the list of the visual objects are defined as a dataset for evaluating the fitness of the detector ANN produced by the neuroevolution process from the substrate.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The function to evaluate the detector ANN against two specific visual objects</h1>
                </header>
            
            <article>
                
<p><span class="s1">This function evaluates the performance of the detector ANN against two given visual objects—one for each side of the retina space. For the complete source code, please refer to the <kbd>def _evaluate(self, net, left, right, depth, debug=False)</kbd> function defined at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter8/retina_environment.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter8/retina_environment.py</a>.</span></p>
<p>The source code of the function has the following essential parts:</p>
<ol>
<li>First, we prepare the inputs for the detector ANN in the order that they are defined in for the substrate configuration:</li>
</ol>
<pre style="padding-left: 30px">        inputs = left.get_data() + right.get_data()<br/>        inputs.append(0.5) # the bias<br/><br/>        net.Input(inputs)</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">The <kbd>inputs</kbd> array starts with the left-side data and continues with the right-side data. After that, the bias value is appended to the end of the <kbd>inputs</kbd> array and the array data is supplied as input to the detector ANN.</span></p>
<ol start="2">
<li>After a specific number of activations of the detector ANN, the outputs are obtained and rounded:</li>
</ol>
<pre style="padding-left: 30px">        outputs = net.Output()<br/>        outputs[0] = 1.0 if outputs[0] &gt;= 0.5 else 0.0<br/>        outputs[1] = 1.0 if outputs[1] &gt;= 0.5 else 0.0</pre>
<ol start="3">
<li>Next, we need to calculate squared detection error, which is the Euclidean distance between the outputs vector and the vector with the ground-truth values. Thus, we first create the vector with ground-truth values as follows:</li>
</ol>
<pre style="padding-left: 30px">        left_target = 1.0 if left.side == Side.LEFT or \<br/>                             left.side == Side.BOTH else 0.0<br/>        right_target = 1.0 if right.side == Side.RIGHT or \<br/>                              right.side  == Side.BOTH else 0.0<br/>        targets = [left_target, right_target]</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">The corresponding ground-truth value is set to <kbd>1.0</kbd> if the visual object is valid for a given side of the retina, or both sides. Otherwise, it is set to <kbd>0.0</kbd> to indicate an incorrect visual object position.</span></p>
<ol start="4">
<li class="p1">Finally, the squared detection error is calculated as follows:</li>
</ol>
<pre style="padding-left: 60px">    error = (outputs[0]-targets[0]) * (outputs[0]-targets[0]) + \<br/>            (outputs[1]-targets[1]) * (outputs[1]-targets[1])</pre>
<p class="p1"><span class="s1">The function returns the detection error and the outputs from the detector ANN. In the next section, we will discuss the retina experiment runner implementation. </span></p>
<div class="p1 packt_infobox"><span class="s1">For complete implementation details, refer to the <kbd>retina_environment.py</kbd> file at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter8/retina_environment.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter8/retina_environment.py</a>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Experiment runner</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">To solve the modular retina problem, we need to use a Python library that provides an implementation of the ES-HyperNEAT algorithm. If you've read the previous chapter, you are already familiar with the MultiNEAT Python library, which also has an implementation of the ES-HyperNEAT algorithm. Thus, we can use this library to create a retina experiment runner implementation.</span></p>
<p class="p1"><span class="s1">Let's discuss the essential components of the implementation.</span></p>
<div class="p1 packt_infobox"><span class="s1">For full implementation details, refer to the <kbd>retina_experiment.py</kbd> file at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter8/retina_experiment.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter8/retina_experiment.py</a>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The experiment runner function</h1>
                </header>
            
            <article>
                
<p class="p2"><span class="s2">The <kbd>run_experiment</kbd> function runs the experiment using the provided hyperparameters and an initialized test environment to evaluate the discovered detector ANNs against the possible retina configurations. The function implementation has the following significant parts:</span></p>
<ol class="ol1">
<li class="li1"><span class="s1">First is the initialization of the population of the initial CPPN genomes:</span></li>
</ol>
<pre style="padding-left: 60px">    seed = 1569777981<br/>    # Create substrate<br/>    substrate = create_substrate()<br/>    # Create CPPN genome and population<br/>    g = NEAT.Genome(0,<br/>             substrate.GetMinCPPNInputs(),<br/>             2, # hidden units<br/>             substrate.GetMinCPPNOutputs(),<br/>             False,<br/>             NEAT.ActivationFunction.TANH,<br/>             NEAT.ActivationFunction.SIGNED_GAUSS, # hidden <br/>             1, # hidden layers seed<br/>             params, <br/>             1) # one hidden layer<br/>    pop = NEAT.Population(g, params, True, 1.0, seed)<br/>    pop.RNG.Seed(seed)</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">At first, the preceding code sets the random seed value to the one that we found to be useful for generating successful solutions by sequentially running many experiment trials. After that, we create the substrate configuration that is suitable for the retina experiment, taking into account the geometry of the retina space.</span></p>
<p class="p1" style="padding-left: 60px"><span class="s1">Next, we create the initial CPPN genome using the substrate configuration we already have. The CPPN genome needs to have a number of input and output nodes that is compatible with the substrate configuration. Also, we seed the initial CPPN genome with two hidden nodes with a Gaussian activation function to boost the neuroevolution process in the right direction. The Gaussian hidden nodes start the neuroevolution search with a bias toward producing particular detector ANN topologies. With these hidden nodes, we introduce to the connectivity patterns of the substrate the principle of symmetry, which is precisely what we are expecting to achieve in the topology of the successful detector ANN. For the retina problem, we need to discover a symmetrical detector ANN configuration incorporating the two symmetrical classifier modules.</span></p>
<ol start="2">
<li>Next, we prepare the intermediary variables to hold the experiment execution results, along with the statistics collector. After that, we run the evolution loop for a set number of generations:</li>
</ol>
<pre style="padding-left: 60px">    start_time = time.time()<br/>    best_genome_ser = None<br/>    best_ever_goal_fitness = 0<br/>    best_id = -1<br/>    solution_found = False<br/><br/>    stats = Statistics()<br/>    ...</pre>
<ol start="3">
<li>Inside the evolution loop, we get the list of genomes belonging to the current population and evaluate it against the test environment as follows:</li>
</ol>
<pre style="padding-left: 60px">        # get list of current genomes<br/>        genomes = NEAT.GetGenomeList(pop)<br/><br/>        # evaluate genomes<br/>        genome, fitness, errors = eval_genomes(genomes, <br/>                         rt_environment=rt_environment, <br/>                         substrate=substrate, <br/>                         params=params)<br/>        stats.post_evaluate(max_fitness=fitness, errors=errors)<br/>        solution_found = fitness &gt;= FITNESS_THRESHOLD</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">The <kbd>eval_genomes</kbd> function returns a tuple that has the following components: the best-fit genome, the highest fitness score among all evaluated genomes, and the list of detection errors for each evaluated genome. We save the appropriate parameters into a statistics collector and evaluate the obtained fitness score against the search termination criterion, which is defined as a <kbd>FITNESS_THRESHOLD</kbd> constant with a value of <kbd>1000.0</kbd>. The evolutionary search terminates successfully if the best fitness score in population is greater than or equal to the <kbd>FITNESS_THRESHOLD</kbd> value.</span></p>
<ol start="4">
<li>If the successful solution was found, or the current best fitness score of the population is higher than the maximum fitness score ever achieved, we save the best CPPN genome and current fitness score as follows:</li>
</ol>
<pre style="padding-left: 60px">        if solution_found or best_ever_goal_fitness &lt; fitness:<br/>            # dump to pickle to freeze the genome state<br/>            best_genome_ser = pickle.dumps(genome)<br/>            best_ever_goal_fitness = fitness<br/>            best_id = genome.GetID()</pre>
<ol start="5">
<li>After that, if the value of the <kbd>solution_found</kbd> variable was set to <kbd>True</kbd>, we terminate the evolution loop:</li>
</ol>
<pre style="padding-left: 60px">        if solution_found:<br/>            print('Solution found at generation: %d, best fitness: %f, species count: %d' % (generation, fitness, len(pop.Species)))<br/>            break</pre>
<ol start="6">
<li>If the evolution failed to produce a successful solution, we print the statistics for the current generation and move to the next epoch:</li>
</ol>
<pre style="padding-left: 60px">        # advance to the next generation<br/>        pop.Epoch()<br/><br/>        # print statistics<br/>        gen_elapsed_time = time.time() - gen_time<br/>        print("Best fitness: %f, genome ID: %d" % <br/>               (fitness, best_id))<br/>        print("Species count: %d" % len(pop.Species))<br/>        print("Generation elapsed time: %.3f sec" % <br/>               (gen_elapsed_time))<br/>        print("Best fitness ever: %f, genome ID: %d" % <br/>               (best_ever_goal_fitness, best_id))</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">The rest of the experiment runner code reports the results of the experiment in different formats.</span></p>
<ol start="7">
<li>We report the experiment results in textual and visual formats using the statistics collected in the evolution loop. Furthermore, visualizations are also saved into the local filesystem in the SVG vector format:</li>
</ol>
<pre style="padding-left: 60px">    print("\nBest ever fitness: %f, genome ID: %d" % <br/>             (best_ever_goal_fitness, best_id))<br/>    print("\nTrial elapsed time: %.3f sec" % (elapsed_time))<br/>    print("Random seed:", seed)</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">The first three lines of the code print general statistics about experiment execution, such as the highest fitness score achieved, the time elapsed for experiment execution, and the random generator seed value.</span></p>
<ol start="8">
<li>The next part of the code is about visualizing the experiment results, which is the most informative part, and you should pay great attention to it. We start with visualizing the CPPN network that we create from the best genome found during the evolution:</li>
</ol>
<pre style="padding-left: 60px">    if save_results or show_results:        <br/>        # Draw CPPN network graph<br/>        net = NEAT.NeuralNetwork()<br/>        best_genome.BuildPhenotype(net)<br/>        visualize.draw_net(net, view=False, node_names=None, <br/>                           filename="cppn_graph.svg", <br/>                           directory=trial_out_dir, fmt='svg')<br/>        print("\nCPPN nodes: %d, connections: %d" % <br/>                     (len(net.neurons), len(net.connections)))</pre>
<ol start="9">
<li>After that, we visualize the detector ANN topology that is created using the best CPPN genome and the retina substrate:</li>
</ol>
<pre style="padding-left: 60px">        net = NEAT.NeuralNetwork()<br/>        best_genome.BuildESHyperNEATPhenotype(net, substrate, <br/>                                              params)<br/>        visualize.draw_net(net, view=False, node_names=None, <br/>                           filename="substrate_graph.svg", <br/>                           directory=trial_out_dir, fmt='svg')<br/>        print("\nSubstrate nodes: %d, connections: %d" % <br/>                 (len(net.neurons), <br/>               len(net.connections)))<br/>        inputs = net.NumInputs()<br/>        outputs = net.NumOutputs()<br/>        hidden = len(net.neurons) - net.NumInputs() - \<br/>                 net.NumOutputs()<br/>        print("\n\tinputs: %d, outputs: %d, hidden: %d" % <br/>                (inputs, outputs, hidden))</pre>
<ol start="10">
<li>Also, we print the results of the evaluation of the detector ANN created by the preceding code against a full dataset and two randomly selected visual objects:</li>
</ol>
<pre style="padding-left: 60px">        # Test against random retina configuration<br/>        l_index = random.randint(0, 15)<br/>        r_index = random.randint(0, 15)<br/>        left = rt_environment.visual_objects[l_index]<br/>        right = rt_environment.visual_objects[r_index]<br/>        err, outputs = rt_environment._evaluate(net, left, <br/>                                                right, 3)<br/>        print("Test evaluation error: %f" % err)<br/>        print("Left flag: %f, pattern: %s" % (outputs[0], left))<br/>        print("Right flag: %f, pattern: %s" % (outputs[1], right))<br/><br/>        # Test against all visual objects<br/>        fitness, avg_error, total_count, false_detections = \<br/>                     rt_environment.evaluate_net(net, debug=True)<br/>        print("Test evaluation against full data set [%d], fitness: %f, average error: %f, false detections: %f" % (total_count, fitness, avg_error, false_detections))</pre>
<p>Finally, we render the statistics data collected during the experiment as follows:</p>
<pre style="padding-left: 60px">        # Visualize statistics<br/>        visualize.plot_stats(stats, ylog=False, view=show_results, <br/>              filename=os.path.join(trial_out_dir,            ‘avg_fitness.svg’))</pre>
<p class="p1"><span class="s1">All visualization plots mentioned here can be found after <span>execution of the </span>experiment in the <kbd>trial_out_dir</kbd> directory of the local filesystem. Now, let's discuss how the substrate builder function is implemented.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The substrate builder function</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The ES-HyperNEAT method runs the neuroevolution process, which includes the evolution of the CPPN genomes along with the evolution of the substrate configuration. However, even though the substrate is evolving during evolution, it is incredibly beneficial to start with an appropriate initial substrate configuration. This configuration should correspond to the geometry of the problem space.</span></p>
<p class="p3"><span class="s1">For the retina experiment, the appropriate substrate configuration is created as follows:</span></p>
<ol>
<li class="p1"><span class="s1">First, we create the configuration of the input layer of the substrate. </span>As you may remember from the <em>The initial substrate configuration</em> section, the eight nodes of the input layer are placed within the XZ plane, which is orthogonal to the XY plane. Furthermore, to reflect the geometry of the retina space, the left object's nodes need to be placed on the left side, and the right object's nodes on the right side of the plane correspondingly. The bias node should be located at the center of the input nodes plane. Thus, the input layer is created as follows:</li>
</ol>
<pre style="padding-left: 90px">    # The input layer<br/>    x_space = np.linspace(-1.0, 1.0, num=4)<br/>    inputs = [<br/>        # the left side<br/>        (x_space[0], 0.0, 1.0), (x_space[1], 0.0, 1.0), <br/>        (x_space[0], 0.0, -1.0), (x_space[1], 0.0, -1.0),<br/>        # the right side<br/>        (x_space[2], 0.0, 1.0), (x_space[3], 0.0, 1.0), <br/>        (x_space[2], 0.0, -1.0), (x_space[3], 0.0, -1.0), <br/>        (0,0,0) # the bias<br/>        ]</pre>
<p class="p1" style="padding-left: 60px"><span class="s1">The two output nodes are located within the XY plane, which is orthogonal to the inputs plane. This substrate configuration allows natural substrate evolution by placing the discovered hidden nodes within the XY plane.</span></p>
<ol start="2">
<li>The output layer is created as follows:</li>
</ol>
<pre style="padding-left: 60px">        # The output layer<br/>        outputs = [(-1.0, 1.0, 0.0), (1.0, 1.0, 0.0)]</pre>
<ol start="3">
<li>Next, we define the general substrate configuration parameters as follows:</li>
</ol>
<pre style="padding-left: 90px">    # Allow connections: input-to-hidden, hidden-to-output, <br/>    # and  hidden-to- hidden<br/>    substrate.m_allow_input_hidden_links = True<br/>    substrate.m_allow_hidden_output_links = True<br/>    substrate.m_allow_hidden_hidden_links = True<br/><br/>    substrate.m_allow_input_output_links = False<br/>    substrate.m_allow_output_hidden_links = False<br/>    substrate.m_allow_output_output_links = False<br/>    substrate.m_allow_looped_hidden_links = False<br/>    substrate.m_allow_looped_output_links = False<br/><br/>    substrate.m_hidden_nodes_activation = \<br/>           NEAT.ActivationFunction.SIGNED_SIGMOID<br/>    substrate.m_output_nodes_activation = \<br/>           NEAT.ActivationFunction.UNSIGNED_SIGMOID<br/><br/>    # send connection length to the CPPN as a parameter<br/>    substrate.m_with_distance = True <br/>    substrate.m_max_weight_and_bias = 8.0</pre>
<p class="p1"><span class="s1">We allow the substrate to have </span><span>connections</span><span> </span><span>from input-to-hidden, hidden-to-hidden, and hidden-to-output nodes. We specify that hidden nodes should use the signed sigmoid activation function, while output nodes should use the unsigned sigmoid activation function. We choose the unsigned sigmoid activation for the output nodes in order to have detector ANN output values in the range <kbd>[0,1]</kbd>.</span></p>
<p class="mce-root">In the next section, we discuss the implementation of the functions to evaluate the <span>fitness of the </span>solutions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fitness evaluation</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The neuroevolution process requires a means to evaluate the fitness of the genome population at each generation of evolution. The fitness evaluation in our experiment consists of two parts, which we discuss here.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The eval_genomes function</h1>
                </header>
            
            <article>
                
<p><span class="s1">This function evaluates the fitness of the overall population. It has the following definition:</span></p>
<pre>def eval_genomes(genomes, substrate, rt_environment, params):<br/>    best_genome = None<br/>    max_fitness = 0<br/>    errors = []<br/>    for genome in genomes:<br/>        fitness, error, total_count, false_detetctions = eval_individual(<br/>                               genome, substrate, rt_environment, params)<br/>        genome.SetFitness(fitness)<br/>        errors.append(error)<br/><br/>        if fitness &gt; max_fitness:<br/>            max_fitness = fitness<br/>            best_genome = genome<br/>    <br/>    return best_genome, max_fitness, errors</pre>
<p class="p1"><span class="s1">The <kbd>eval_genomes</kbd> function takes the list of CPPN genomes from the current population, the substrate configuration, the initialized test environment, and the ES-HyperNEAT hyperparameters </span><span>as parameters</span><span>.</span></p>
<p class="p1"><span class="s1">At the beginning of the code, we create an intermediary object to collect the evaluation results of each specific genome:</span></p>
<pre>    best_genome = None<br/>    max_fitness = 0<br/>    errors = []</pre>
<p class="p1"><span class="s1">After that, we start the loop that iterates over all genomes and evaluates each genome against a given test environment:</span></p>
<pre>    for genome in genomes:<br/>        fitness, error, total_count, false_detetctions = eval_individual(<br/>                               genome, substrate, rt_environment, params)<br/>        genome.SetFitness(fitness)<br/>        errors.append(error)<br/><br/>        if fitness &gt; max_fitness:<br/>            max_fitness = fitness<br/>            best_genome = genome</pre>
<p class="p1"><span class="s1">Finally, the function returns the evaluation results as a tuple that includes the best genome, the highest fitness score, and the list of all detection errors for each evaluated genome.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The eval_individual function</h1>
                </header>
            
            <article>
                
<p>This function evaluates the fitness of each individual genome and has the following definition:</p>
<pre>def eval_individual(genome, substrate, rt_environment, params):<br/>    # Create ANN from provided CPPN genome and substrate<br/>    net = NEAT.NeuralNetwork()<br/>    genome.BuildESHyperNEATPhenotype(net, substrate, params)<br/><br/>    fitness, dist, total_count, false_detetctions = \<br/>       rt_environment.evaluate_net(net, max_fitness=MAX_FITNESS)<br/>    return fitness, dist, total_count, false_detetctions</pre>
<p class="p1"><span class="s1">It takes the CPPN genome to be evaluated, the substrate configuration, the test environment, and the ES-HyperNEAT hyperparameters </span><span>as parameters</span><span>. Using the provided parameters, we create the neural network configuration of the detector ANN and evaluate it against the given test environment. The function then returns the evaluation result.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Modular retina experiment</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">Now we are ready to start experimenting against the test environment that simulates the modular retina problem space. In the next subsections, you will learn how to select appropriate hyperparameters and how to set up the environment and run the experiment. After that, we discuss the experiment results.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter selection</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">The hyperparameters are defined as a <kbd>Parameters</kbd> Python class, and the MultiNEAT library refers to it for the necessary configuration options. In the source code of the experiment runner script, we define a specialized function called <kbd>create_hyperparameters</kbd>, which encapsulates the logic of the hyperparameter initialization. Hereafter, we describe the most critical hyperparameters and the reasons for choosing these specific values:</span></p>
<ol>
<li><span class="s1">We decided to use a medium size for the CPPN genome population. This is done to intensify the evolution by providing a large space of options for the solution search from the beginning. The size of the population is defined as follows:</span></li>
</ol>
<pre style="padding-left: 60px">    params.PopulationSize = 300</pre>
<ol start="2">
<li>Next, we define the number of species to be kept during evolution in the range<kbd>[5,15]</kbd><span>and set the species stagnation to <kbd>100</kbd> generations. This configuration allows us to have healthy diversity among species and keep them alive for long enough to produce the solution we are looking for:</span></li>
</ol>
<pre style="padding-left: 60px">    params.SpeciesMaxStagnation = 100<br/>    params.MinSpecies = 5<br/>    params.MaxSpecies = 15</pre>
<ol start="3">
<li>We are interested in producing an extra-compact configuration of CPPN genomes. Thus, we have very small values of probabilities that control how often new nodes and connections will be introduced into the genome:</li>
</ol>
<pre style="padding-left: 60px">    params.MutateAddLinkProb = 0.03<br/>    params.MutateAddNeuronProb = 0.03</pre>
<ol start="4">
<li>The ES-HyperNEAT method is an extension of the HyperNEAT method. Thus, during the evolution, it changes the types of activation functions in the hidden and output nodes. In this experiment, to produce appropriate substrate configurations, we are interested in the following activation types, selected with equal probability:</li>
</ol>
<pre style="padding-left: 60px">    params.ActivationFunction_SignedGauss_Prob = 1.0<br/>    params.ActivationFunction_SignedStep_Prob = 1.0<br/>    params.ActivationFunction_Linear_Prob = 1.0<br/>    params.ActivationFunction_SignedSine_Prob = 1.0<br/>    params.ActivationFunction_SignedSigmoid_Prob = 1.0</pre>
<ol start="5">
<li>Finally, we define the ES-HyperNEAT specific hyperparameters, which control how the substrate evolves. The following hyperparameters control the dynamics of the creation of nodes and connections within the substrate during evolution:</li>
</ol>
<pre style="padding-left: 60px">    params.DivisionThreshold = 0.5<br/>    params.VarianceThreshold = 0.03</pre>
<p class="p1"><span class="s1"><kbd>params.DivisionThreshold</kbd> controls how many new nodes and connections are introduced into the substrate at each generation of evolution. <kbd>params.VarianceThreshold</kbd> </span>determines how many nodes and connections are allowed to remain in the substrate after the pruning and extraction phase. <span class="s1">See the <em>Quadtree information extraction and ES-HyperNEAT basics</em> </span><span>section</span><span> </span><span>for more details about these thresholds.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working environment setup</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">In this experiment, we use the MultiNEAT Python library, which provides the implementation of the ES-HyperNEAT algorithm. Thus, we need to create an appropriate Python environment, which includes the MultiNEAT Python library and all necessary dependencies. This can be done using Anaconda by executing the following commands on the command line:</span></p>
<pre><strong>$ conda create --name rt_multineat python=3.5</strong><br/><strong>$ conda activate vd_multineat</strong><br/><strong>$ conda install -c conda-forge multineat </strong><br/><strong>$ conda install matplotlib</strong><br/><strong>$ conda install -c anaconda seaborn</strong><br/><strong>$ conda install graphviz</strong><br/><strong>$ conda install python-graphviz</strong></pre>
<p class="p1"><span class="s1">These commands create and activate the <kbd>rt_multineat</kbd> virtual environment with Python 3.5. After that, they install the MultiNEAT Python library with the latest version, along with dependencies that are used by our code for result visualization.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running the modular retina experiment</h1>
                </header>
            
            <article>
                
<p class="p1"><span class="s1">At this stage, we already have the experiment runner script fully defined in the <kbd>retina_experiment.py</kbd> Python script. You can start the experiment by cloning the corresponding Git repository and running the script with the following commands:</span></p>
<pre><strong>$ git clone https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python.git</strong><br/><strong>$ cd Hands-on-Neuroevolution-with-Python/Chapter8</strong><br/><strong>$ python retina_experiment.py -t 1 -g 1000</strong></pre>
<div class="p1 packt_infobox"><span class="s1">Do not forget to activate the appropriate virtual environment with the following command:<br/>
<br/></span> <strong><kbd><span class="s1">conda activate rt_multineat</span></kbd></strong></div>
<p class="p3"><span class="s1">The preceding command starts one trial of the experiment for 1,000 generations of evolution. After a particular number of generations, the successful solution should be found, and you will see the following output in the console:</span></p>
<pre><strong>****** Generation: 949 ******</strong><br/><br/><strong>Solution found at generation: 949, best fitness: 1000.000000, species count: 6</strong><br/><br/><strong>Best ever fitness: 1000.000000, genome ID: 284698</strong><br/><br/><strong>Trial elapsed time: 1332.576 sec</strong><br/><strong>Random seed: 1569777981</strong><br/><br/><strong>CPPN nodes: 21, connections: 22</strong><br/><br/><strong>Substrate nodes: 15, connections: 28</strong></pre>
<p class="p1"><span class="s1">As you can see in the output, the successful solution was found in generation <kbd>949</kbd>. It was produced by a CPPN genome with 21 nodes and 22 connections among them. At the same time, the substrate that determines the topology of the detector ANN has 15 nodes and 28 connections between them. The successful solution was produced using random seed value <kbd>1569777981</kbd>. Using other random seed values may fail to produce successful solutions, or it will require many more generations of evolution.</span></p>
<p class="p1"><span class="s1">Next, it is interesting to look at the plot of the average fitness and error during the evolution:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-837 image-border" src="assets/cbf8d9e1-8200-4f88-816c-a7fa49166931.png" style="width:39.75em;height:29.67em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The average fitness and error per generation</span></div>
<p class="p1"><span class="s1">You can see in the preceding plot that, during most of the evolution generations, the fitness score was very small (about 20), but suddenly, the successful CPPN genome was found, which produced an immediate evolutionary leap just in one generation.</span></p>
<p class="p1"><span class="s1">The configuration of the successful CPPN genome is shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-796 image-border" src="assets/3f032f44-2073-4c2d-b375-9b60f090b918.png" style="width:38.33em;height:46.33em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The CPPN phenotype graph of the successful genome</span></div>
<p class="p1"><span class="s1">The diagram is extremely interesting because, as you can see, the configuration of the successful CPPN genome does not use all the available inputs (the gray squares) to produce outputs. Moreover, even more confounding is that it uses only the <em>x</em> coordinate of the input (node #0) and the <em>y</em> coordinate of the hidden (node #3) substrate nodes when deciding about exposing a connection between these substrate nodes. At the same time, both the <em>x</em> and <em>y</em> coordinates of the substrate output nodes are involved in the decision-making process (nodes #4 and #5).</span></p>
<p class="p1"><span class="s1">When you look at the initial substrate configuration, which we presented earlier, you will see that the peculiarities we've mentioned are fully substantiated by the substrate topology. We placed the input nodes within the XZ plane. Thus, the <em>y</em> coordinate is not critical for them at all. At the same time, the hidden nodes located within the XY plane, with the <em>y</em> coordinate determining the distance from the inputs plane. Finally, the output nodes are also located within the XY plane. Their <em>x</em> coordinate determines the side of the retina to which each output node relates. Thus, for the output nodes, it is natural that both the <em>x</em> and <em>y</em> coordinates are included.</span></p>
<p class="p1"><span class="s1">In the CPPN phenotype plot, the input nodes are marked with squares, the output nodes are filled circles, the bias node is a diamond, and the hidden nodes are empty circles.</span></p>
<p class="p1"><span class="s1">Two output nodes in the CPPN diagram has the following meaning: </span></p>
<ul class="ul1">
<li class="li1"><span class="s1">The first node (8) provides the weight of the connection.</span></li>
<li class="li1"><span class="s1">The second node (9) determines whether the connection is expressed or not.</span></li>
</ul>
<p class="p1"><span class="s1">The CPPN's input nodes are defined as the following:</span></p>
<ul class="ul1">
<li class="li1"><span class="s1">The first two nodes (0 and 1) set the point coordinates (<em>x</em>, <em>z</em>) in the input layer of the substrate.</span></li>
<li class="li1"><span class="s1">The next two nodes (2 and 3) set the point coordinates (<em>x</em>, <em>y</em>) in the hidden layer of the substrate.</span></li>
<li class="li1"><span class="s1">The next two nodes (4 and 5) set the point coordinates (<em>x</em>, <em>y</em>) in the output layer of the substrate.</span></li>
<li class="li1"><span class="s1">The last node (6) sets the Euclidean distance of the point in the input layer from the origin of the coordinates.</span></li>
</ul>
<p class="p1"><span class="s1">However, you can see the most exciting part of the experiment results in the following diagram. It represents the configuration of the successful detector ANN:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-797 image-border" src="assets/ed3cd3e3-ba05-4ea1-9434-257132dfc46b.png" style="width:43.67em;height:25.83em;"/></p>
<div class="p1 CDPAlignCenter CDPAlign packt_figref"><span class="s1">The configuration of the detector ANN</span></div>
<p class="p1"><span class="s1">As in the previous plot, we mark the input nodes with squares, the output nodes with filled circles, the bias node as a diamond, and the hidden nodes as empty circles.</span></p>
<p class="p1"><span class="s1">As you can see, we have two clearly separated modular structures on the left and right sides of the graph. Each module is connected to the corresponding inputs from the left (nodes #0, #1, #2, and #3) and the right (nodes #4, #5, #6, and #7) sides of the retina. Both modules have the same number of hidden nodes, which are connected to the corresponding output nodes: node #9 for the left side and node #10 for the right side of the retina. Also, you can see that connectivity patterns in the left and right modules are similar. The hidden node, #11, on the left has similar connection patterns to node #14 on the right, and the same can be said for nodes #12 and #13.</span></p>
<p class="p1"><span class="s1">It is just amazing how the stochastic evolutionary process was able to discover such a simple and elegant solution. With the results of this experiment, we fully confirmed our hypothesis that the retina problem can be solved by the creation of modular detector ANN topologies.</span></p>
<div class="packt_infobox">More details about modular retina<span> problem can be found in the original paper at </span><a href="http://eplex.cs.ucf.edu/papers/risi_alife12.pdf">http://eplex.cs.ucf.edu/papers/risi_alife12.pdf</a>.<a href="http://eplex.cs.ucf.edu/papers/risi_alife12.pdf"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<ol class="ol1">
<li class="li3"><span class="s1">Try to run an experiment with different values of the random seed generator that can be changed in line 101 of the <kbd>retina_experiment.py</kbd> script. See if you can find successful solutions with other values.</span></li>
<li class="li3"><span class="s1">Try to increase the initial population size to 1,000 by adjusting the value of the <kbd>params.PopulationSize</kbd> <span>hyperparameter. </span>How did this affect the performance of the algorithm? </span></li>
<li class="li3"><span class="s1">Try to change the number of activation function types used during the evolution by setting the probability of its selection <span>to 0</span>. It's especially interesting to see what happens when you exclude the <kbd>ActivationFunction_SignedGauss_Prob</kbd> and <kbd>ActivationFunction_SignedStep_Prob</kbd> activation types <span>from selection</span>.</span></li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="p3"><span class="s1">In this chapter, we learned about the neuroevolution method that allows the substrate configuration to evolve during the process of finding the solution to the problem. This approach frees the human designer from the burden of creating a suitable substrate configuration to the smallest details, allowing us to define only the primary outlines. The algorithm will automatically learn the remaining details of the substrate configuration during the evolution.</span></p>
<p class="p3"><span class="s1">Also, you learned about the modular ANN structures that can be used to solve various problems, including the modular retina problem. Modular ANN topologies are a very powerful concept that allows the reuse of the successful phenotype ANN module multiple times to build a complex hierarchical topology. </span><span class="s1">Furthermore, you have had the chance to hone your skills with the Python programming language by implementing the corresponding solution using the MultiNEAT Python library.</span></p>
<p class="p3"><span class="s1">In the next chapter, we will discuss the fascinating concept of coevolution and how it can be used to simultaneously coevolve the solver and the objective function that is used for optimization. We will discuss the method of solution and fitness evolution, and you will learn how to apply it to the modified maze-solving experiment.</span></p>


            </article>

            
        </section>
    </body></html>