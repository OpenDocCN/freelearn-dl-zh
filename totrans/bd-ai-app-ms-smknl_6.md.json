["```py\nfrom openai import OpenAI\nfrom typing import Tuple\nimport numpy as np\nfrom numpy.linalg import norm\nimport os\n```", "```py\ndef get_embedding(text: str) -> Tuple[float]:\n    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n    response = client.embeddings.create(\n        input=text,\n        model=\"text-embedding-3-small\"\n    )\n    return response.data[0].embedding\n```", "```py\ndef similarity(A: np.array, B: np.array) -> float:\n    # compute cosine similarity\n    cosine = np.dot(A,B)/(norm(A)*norm(B))\n    return cosine\n```", "```py\nif __name__ == \"__main__\":\n    load_dotenv()\n    king = get_embedding(\"The king has been crowned\")\n    queen = get_embedding(\"The queen has been crowned\")\n    linkedin = get_embedding(\"LinkedIn is a social media platform for professionals\")\n    print(similarity(king, queen))\n    print(similarity(king, linkedin))\n    print(similarity(queen, linkedin))\n```", "```py\n0.8684853246664367\n0.028215574794606412\n0.046607036099519175\n```", "```py\n    for i in range(0, 10):\n        print(king[i])\n```", "```py\n-0.009829566814005375\n-0.009656181558966637\n0.024287164211273193\n0.01408415473997593\n-0.03662413731217384\n-0.0040411921218037605\n-0.00032176158856600523\n0.046813808381557465\n-0.03235621005296707\n-0.04099876061081886\n```", "```py\nimport asyncio\nimport semantic_kernel as sk\nfrom semantic_kernel.connectors.ai.open_ai import OpenAITextEmbedding, OpenAIChatCompletion, OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.functions import KernelArguments, KernelFunction\nfrom semantic_kernel.prompt_template import PromptTemplateConfig\nfrom semantic_kernel.utils.settings import openai_settings_from_dot_env\nfrom semantic_kernel.memory.volatile_memory_store import VolatileMemoryStore\nfrom semantic_kernel.memory.semantic_text_memory import SemanticTextMemory\nfrom semantic_kernel.core_plugins.text_memory_plugin import TextMemoryPlugin\n```", "```py\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Memory;\nusing Microsoft.SemanticKernel.Plugins.Memory;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\n#pragma warning disable SKEXP0003, SKEXP0011, SKEXP0052\n```", "```py\ndef create_kernel() -> tuple[sk.Kernel, OpenAITextEmbedding]:\n    api_key, org_id =  openai_settings_from_dot_env()\n    kernel = sk.Kernel()\n    gpt = OpenAIChatCompletion(ai_model_id=\"gpt-4-turbo-preview\", api_key=api_key, org_id=org_id, service_id=\"gpt4\")\n    emb = OpenAITextEmbedding(ai_model_id=\"text-embedding-ada-002\", api_key=api_key, org_id=org_id, service_id=\"emb\")\n    kernel.add_service(emb)\n    kernel.add_service(gpt)\n    return kernel, emb\nasync def main():\n    kernel, emb = create_kernel()\n    memory = SemanticTextMemory(storage=VolatileMemoryStore(), embeddings_generator=emb)\n    kernel.add_plugin(TextMemoryPlugin(memory), \"TextMemoryPlugin\")\n```", "```py\nvar builder = Kernel.CreateBuilder();\nbuilder.AddOpenAIChatCompletion(\"gpt-4-turbo-preview\", apiKey, orgId);\nvar kernel = builder.Build();\nvar memoryBuilder = new MemoryBuilder();\nmemoryBuilder.WithMemoryStore(new VolatileMemoryStore());\nmemoryBuilder.WithOpenAITextEmbeddingGeneration(\"text-embedding-3-small\", apiKey);\nvar memory = memoryBuilder.Build();\n```", "```py\n async def add_to_memory(memory: SemanticTextMemory, id: str, text: str):\n    await memory.save_information(collection=\"generic\", id=id, text=text)\n```", "```py\nconst string MemoryCollectionName = \"default\";\nawait memory.SaveInformationAsync(MemoryCollectionName, id: \"1\", text: \"My favorite city is Paris\");\nawait memory.SaveInformationAsync(MemoryCollectionName, id: \"2\", text: \"My favorite activity is visiting museums\");\n```", "```py\nasync def tour(kernel: sk.Kernel) -> KernelFunction:\n    prompt = \"\"\"\n    Information about me, from previous conversations:\n    - {{$city}} {{recall $city}}\n    - {{$activity}} {{recall $activity}}\n    \"\"\".strip()\n    execution_settings = kernel.get_service(\"gpt4\").instantiate_prompt_execution_settings(service_id=\"gpt4\")\n    execution_settings.max_tokens = 4000\n    execution_settings.temperature = 0.8\n    prompt_template_config = PromptTemplateConfig(template=prompt, execution_settings=execution_settings)\n    chat_func = kernel.add_function(\n        function_name=\"chat_with_memory\",\n        plugin_name=\"TextMemoryPlugin\",\n        prompt_template_config=prompt_template_config,\n    )\n    return chat_func\n```", "```py\nkernel.ImportPluginFromObject(new TextMemoryPlugin(memory));\nconst string prompt = @\"\nInformation about me, from previous conversations:\n- {{$city}} {{recall $city}}\n- {{$activity}} {{recall $activity}}\nGenerate a personalized tour of activities for me to do when I have a free day in my favorite city. I just want to do my favorite activity.\n\";\nvar f = kernel.CreateFunctionFromPrompt(prompt, new OpenAIPromptExecutionSettings { MaxTokens = 2000, Temperature = 0.8 });\nvar context = new KernelArguments();\ncontext[\"city\"] = \"What is my favorite city?\";\ncontext[\"activity\"] = \"What is my favorite activity?\";\ncontext[TextMemoryPlugin.CollectionParam] = MemoryCollectionName;\n```", "```py\n    await add_to_memory(memory, id=\"1\", text=\"My favorite city is Paris\")\n    await add_to_memory(memory, id=\"2\", text=\"My favorite activity is visiting museums\")\n    f = await tour(kernel)\n    args = KernelArguments()\n    args[\"city\"] = \"My favorite city is Paris\"\n    args[\"activity\"] = \"My favorite activity is visiting museums\"\n    answer = await kernel.invoke(f, arguments=args)\n    print(answer)\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "```py\nawait memory.SaveInformationAsync(MemoryCollectionName, id: \"1\", text: \"My favorite city is Paris\");\nawait memory.SaveInformationAsync(MemoryCollectionName, id: \"2\", text: \"My favorite activity is visiting museums\");\nvar result = await f.InvokeAsync(kernel, context);\nConsole.WriteLine(result);\n```", "```py\nGiven your love for Paris and visiting museums, here's a personalized itinerary for a fulfilling day exploring some of the most iconic and enriching museums in the City of Light:\n**Morning: Musée du Louvre**\nStart your day at the Louvre, the world's largest art museum and a historic monument in Paris. Home to thousands of works of art, including the Mona Lisa and the Venus de Milo, the Louvre offers an unparalleled experience for art lovers. Arrive early to beat the crowds and spend your morning marveling at the masterpieces from across the world. Don't forget to walk through the Tuileries Garden nearby for a peaceful stroll.\n**Afternoon: Musée d'Orsay**\nNext, head to the Musée d'Orsay, located on the left bank of the Seine. Housed in the former Gare d'Orsay, a Beaux-Arts railway station, the museum holds the largest collection of Impressionist and Post-Impressionist masterpieces in the world. Spend your afternoon admiring works by Monet, Van Gogh, Renoir, and many others.\n**Late Afternoon: Musée de l'Orangerie**\nConclude your day of museum visits at the Musée de l'Orangerie, located in the corner of the Tuileries Gardens. This gallery is famous for housing eight large Water Lilies murals by Claude Monet, displayed in two oval rooms offering a breathtaking panorama of Monet's garden-inspired masterpieces. The museum also contains works by Cézanne, Matisse, Picasso, and Rousseau, among others.\n**Evening: Seine River Walk and Dinner**\nAfter an enriching day of art, take a leisurely walk along the Seine River. The riverside offers a picturesque view of Paris as the city lights begin to sparkle. For dinner, choose one of the numerous bistros or restaurants along the river or in the nearby neighborhoods to enjoy classic French cuisine, reflecting on the beautiful artworks and memories created throughout the day.\n```", "```py\nimport asyncio\nimport semantic_kernel as sk\nfrom semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion, OpenAIChatPromptExecutionSettings\nfrom semantic_kernel.functions import KernelFunction\nfrom semantic_kernel.prompt_template import PromptTemplateConfig, InputVariable\nfrom semantic_kernel.core_plugins import ConversationSummaryPlugin\nfrom semantic_kernel.contents.chat_history import ChatHistory\nfrom semantic_kernel.utils.settings import openai_settings_from_dot_env\n```", "```py\nusing Microsoft.SemanticKernel;\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\nusing Microsoft.SemanticKernel.Plugins.Core;\n#pragma warning disable SKEXP0003, SKEXP0011, SKEXP0052, SKEXP0050\n```", "```py\ndef create_kernel() -> sk.Kernel:\n    api_key, org_id =  openai_settings_from_dot_env()\n    kernel = sk.Kernel()\n    gpt = OpenAIChatCompletion(ai_model_id=\"gpt-4-turbo-preview\", api_key=api_key, org_id=org_id, service_id=\"gpt4\")\n    kernel.add_service(gpt)\n    # The following execution settings are used for the ConversationSummaryPlugin\n    execution_settings = OpenAIChatPromptExecutionSettings(\n        service_id=\"gpt4\", max_tokens=ConversationSummaryPlugin._max_tokens, temperature=0.1, top_p=0.5)\n    prompt_template_config = PromptTemplateConfig(\n        template=ConversationSummaryPlugin._summarize_conversation_prompt_template,\n        description=\"Given a section of a conversation transcript, summarize it\",\n        execution_settings=execution_settings,\n    )\n    # Import the ConversationSummaryPlugin\n    kernel.add_plugin(\n        ConversationSummaryPlugin(kernel=kernel, prompt_template_config=prompt_template_config),\n        plugin_name=\"ConversationSummaryPlugin\",\n    )\n    return kernel\n```", "```py\nvar (apiKey, orgId) = Settings.LoadFromFile();\nvar builder = Kernel.CreateBuilder();\nbuilder.AddOpenAIChatCompletion(\"gpt-4-turbo-preview\", apiKey, orgId);\nvar kernel = builder.Build();\nkernel.ImportPluginFromObject(new ConversationSummaryPlugin());\n```", "```py\nasync def create_chat_function(kernel: sk.Kernel) -> KernelFunction:\n    # Create the prompt\n    prompt = \"\"\"\n    User: {{$request}}\n    Assistant:  \"\"\"\n    # These execution settings are tied to the chat function, created below.\n    execution_settings = kernel.get_service(\"gpt4\").instantiate_prompt_execution_settings(service_id=\"gpt4\")\n    chat_prompt_template_config = PromptTemplateConfig(\n        template=prompt,\n        description=\"Chat with the assistant\",\n        execution_settings=execution_settings,\n        input_variables=[\n            InputVariable(name=\"request\", description=\"The user input\", is_required=True),\n            InputVariable(name=\"history\", description=\"The history of the conversation\", is_required=True),\n        ],\n    )\n    # Create the function\n    chat_function = kernel.add_function(\n        prompt=prompt,\n        plugin_name=\"Summarize_Conversation\",\n        function_name=\"Chat\",\n        description=\"Chat with the assistant\",\n        prompt_template_config=chat_prompt_template_config,)\n    return chat_function\n```", "```py\nconst string prompt = @\"\nChat history:\n{{$history}}\nUser: {{$userInput}}\nAssistant:\";\nvar executionSettings = new OpenAIPromptExecutionSettings {MaxTokens = 2000,Temperature = 0.8,};\nvar chatFunction = kernel.CreateFunctionFromPrompt(prompt, executionSettings);\nvar history = \"\";\nvar arguments = new KernelArguments();\narguments[\"history\"] = history;\n```", "```py\nasync def main():\n    kernel = create_kernel()\n    history = ChatHistory()\n    chat_function = await create_chat_function(kernel)\n    while True:\n        try:\n            request = input(\"User:> \")\n        except KeyboardInterrupt:\n            print(\"\\n\\nExiting chat...\")\n            return False\n        except EOFError:\n            print(\"\\n\\nExiting chat...\")\n            return False\n        if request == \"exit\":\n            print(\"\\n\\nExiting chat...\")\n            return False\n        result = await kernel.invoke(\n            chat_function,\n            request=request,\n            history=history,\n        )\n        # Add the request to the history\n        history.add_user_message(request)\n        history.add_assistant_message(str(result))\n        print(f\"Assistant:> {result}\")\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```", "```py\nvar chatting = true;\nwhile (chatting) {\n    Console.Write(\"User: \");\n    var input = Console.ReadLine();\n    if (input == null) {break;}\n    input = input.Trim();\n    if (input == \"exit\") {break;}\n    arguments[\"userInput\"] = input;\n    var answer = await chatFunction.InvokeAsync(kernel, arguments);\n    var result = $\"\\nUser: {input}\\nAssistant: {answer}\\n\";\n    history += result;\n    arguments[\"history\"] = history;\n    // Show the bot response\n    Console.WriteLine(result);\n}\n```", "```py\nprompt = \"\"\"\nChat history:\n{{ConversationSummaryPlugin.SummarizeConversation $history}}\nUser: {{$userInput}}\nAssistant:\n\"\"\"\n```", "```py\nconst string prompt = @\"\nChat history:\n{{ConversationSummaryPlugin.SummarizeConversation $history}}\nUser: {{$userInput}}\nChatBot:\";\n```", "```py\nUser:> What is the largest city by population in Western Europe?\nAssistant:> The largest city by population in Western Europe is London, United Kingdom.\n```", "```py\nUser:> Are there any famous people who lived there?\nAssistant:> Yes, London has been home to many famous people throughout history. Some notable individuals include:\n1\\. **William Shakespeare** - The renowned playwright and poet lived in London for most of his career.\n2\\. **Charles Dickens** - The famous novelist, known for works like \"Oliver Twist\" and \"A Christmas Carol,\" spent much of his life in London.\n3\\. **Winston Churchill** - The iconic British Prime Minister during World War II was born in Woodstock but lived and worked in London for much of his life.\n```", "```py\nUser:> What is a famous play from the first one set in that city?\nAssistant:> A famous play from William Shakespeare that is set in London is \"Henry V.\" This historical play, part of Shakespeare's series on the English monarchs, includes scenes that are set in London, notably the English court. It portrays the events before and after the Battle of Agincourt during the Hundred Years' War, with significant portions reflecting on the life and times in London during the early 15th century.\n```", "```py\nUser:> exit|\nExiting chat...\n```"]