<html><head></head><body>
<div><h1 class="chapterNumber">10</h1>
<h1 class="chapterTitle" id="_idParaDest-136">Building Multimodal Applications with LLMs</h1>
<p class="normal">In this chapter, we are going beyond LLMs, to introduce the concept of multimodality while building agents. We will see the logic behind the combination of foundation models in different AI domains – language, images, and audio – into one single agent that can adapt to a variety of tasks. By the end of this chapter, you will be able to build your own multimodal agent, providing it with the tools and LLMs needed to perform various AI tasks.</p>
<p class="normal">Throughout this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">Introduction to multimodality and <strong class="keyWord">large multimodal models</strong> (<strong class="keyWord">LMMs</strong>)</li>
<li class="bulletList">Examples of emerging LMMs</li>
<li class="bulletList">How to build a multimodal agent with single-modal LLMs using LangChain</li>
</ul>
<h1 class="heading-1" id="_idParaDest-137">Technical requirements</h1>
<p class="normal">To complete the tasks in this chapter, you will need the following:</p>
<ul>
<li class="bulletList">A Hugging Face account and user access token.</li>
<li class="bulletList">An OpenAI account and user access token.</li>
<li class="bulletList">Python 3.7.1 or later version.</li>
<li class="bulletList">Python packages. Make sure to have the following Python packages installed: <code class="inlineCode">langchain</code>, <code class="inlineCode">python-dotenv</code>, <code class="inlineCode">huggingface_hub</code>, <code class="inlineCode">streamlit</code>, <code class="inlineCode">pytube</code>, <code class="inlineCode">openai</code>, and <code class="inlineCode">youtube_search</code>. Those can be easily installed via <code class="inlineCode">pip install</code> in your terminal.</li>
</ul>
<p class="normal">You can find all the code and examples in the book’s GitHub repository at <a href="Chapter_10.xhtml">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<h1 class="heading-1" id="_idParaDest-138">Why multimodality?</h1>
<p class="normal">In the context of Generative AI, multimodality<a id="_idIndexMarker712"/> refers to a model’s capability of processing data in various formats. For example, a multimodal model can communicate with humans via text, speech, images, or even videos, making the interaction extremely smooth and “human-like.”</p>
<p class="normal">In <em class="chapterRef">Chapter 1</em>, we defined <strong class="keyWord">large foundation models</strong> (<strong class="keyWord">LFMs</strong>) as a type of pre-trained generative AI model<a id="_idIndexMarker713"/> that offers immense versatility by being adaptable for various specific tasks. LLMs, on the other hand, are a subset of foundation models that are able to process one type of data: natural language. Even though LLMs have proven to be not only excellent text understanders and generators but also reasoning engines to power applications and copilots, it soon became clear that we could aim at even more powerful applications.</p>
<p class="normal">The dream is to have intelligent systems that are capable of handling multiple data formats – text, images, audio, video, etc – always powered by the reasoning engine, which makes them able to plan and execute actions with an agentic approach. Such an AI system would<a id="_idIndexMarker714"/> be a further milestone toward the reaching of <strong class="keyWord">artificial general intelligence</strong> (<strong class="keyWord">AGI</strong>).</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">AGI is a hypothetical type of <strong class="keyWord">artificial intelligence</strong> (<strong class="keyWord">AI</strong>) that can perform any intellectual<a id="_idIndexMarker715"/> task that a human can. AGI would have a general cognitive ability, similar to human intelligence, and be able to learn from experience, reason, plan, communicate, and solve problems across different domains. An AGI system would also be able to “perceive” the world as we do, meaning that it could process data in different formats, from text to images to sounds. Hence, AGI implies multimodality.</p>
<p class="normal">Creating AGI is a primary goal of some AI research and a common topic in science fiction. However, there is no consensus on how to achieve AGI, what criteria to use to measure it, or when it might be possible. Some researchers argue that AGI could be achieved in years or decades, while others maintain that it might take a century or longer, or that it might never be achieved.</p>
</div>
<p class="normal">However, AGI is not seen as the ultimate milestone in AI development. In fact, in recent months another definition has emerged in the context of AI – that is, Strong AI or Super AI, referring to an AI system<a id="_idIndexMarker716"/> that is more capable than a human.</p>
<p class="normal">At the time of writing this book (February 2024), LMMs such as GPT-4 Turbo with Vision are a reality. However, those are not the only ways to reach multimodality. In this chapter, we are going to examine how to merge multiple AI systems to reach a multimodal AI assistant. The idea is that if we combine single-modal models, one for each data format we want to process, and then use an LLM as the brain of our agent to let it interact in dynamic ways with those models (that will be its tools), we can still achieve this goal. The following diagram shows the structure of a multimodal application that integrates various single-modal tools to perform a task – in this case, describing a picture aloud. The application uses image analysis to examine the picture, text generation to create some text that describes what it observes in the picture, and text-to-speech to convey this text to the user through speech. </p>
<p class="normal">The LLM acts as the “reasoning engine” of the application, invoking<a id="_idIndexMarker717"/> the proper tools needed to accomplish the user’s query.</p>
<figure class="mediaobject"><img alt="A person talking to a speech bubble  Description automatically generated" src="img/B21714_10_01.png"/></figure>
<p class="packt_figref">Figure 10.1: Illustration of multimodal application with single-modal tools</p>
<p class="normal">In the upcoming section, we are going to explore various approaches to building multimodal applications, all based on the idea of combining existing single-modal tools or models.</p>
<h1 class="heading-1" id="_idParaDest-139">Building a multimodal agent with LangChain</h1>
<p class="normal">So far, we’ve covered the main <a id="_idIndexMarker718"/>aspects of multimodality<a id="_idIndexMarker719"/> and how to achieve it with modern LFMs. As we saw throughout Part 2 of this book, LangChain offers a variety of components that we leveraged massively, such as chains, agents, tools, and so on. As a result, we already have all the ingredients we need to start building<a id="_idIndexMarker720"/> our multimodal<a id="_idIndexMarker721"/> agent.</p>
<p class="normal">However, in this chapter, we will adopt three approaches to tackle the problem:</p>
<ul>
<li class="bulletList"><strong class="keyWord">The agentic, out-of-the-box approach</strong>: Here we will leverage the Azure Cognitive Services toolkit, which offers native integrations toward a set of AI models that can be consumed via API, and that covers various domains such as image, audio, OCR, etc.</li>
<li class="bulletList"><strong class="keyWord">The agentic, custom approach</strong>: Here, we are going to select single models and tools (including defining custom tools) and concatenate them into a single agent that can leverage all of them.</li>
<li class="bulletList"><strong class="keyWord">The hard-coded approach</strong>: Here, we are going to build<a id="_idIndexMarker722"/> separate chains and combine them into a sequential chain.</li>
</ul>
<p class="normal">In the upcoming sections, we will cover all these approaches with concrete examples.</p>
<h1 class="heading-1" id="_idParaDest-140">Option 1: Using an out-of-the-box toolkit for Azure AI Services</h1>
<p class="normal">Formerly known as Azure Cognitive Services, Azure AI Services <a id="_idIndexMarker723"/>are a set of cloud-based APIs<a id="_idIndexMarker724"/> and AI services developed by Microsoft that enable developers and data scientists to add cognitive capabilities to their apps. AI Services are meant to provide every developer with AI models to be integrated with programming languages such as Python, C#, or JavaScript.</p>
<p class="normal">Azure AI Services cover various domains of AI, including speech, natural language, vision, and decision-making. All those services come with models that can be consumed via API, and you can decide to:</p>
<ul>
<li class="bulletList">Leverage powerful pre-built models available as they are and ready to use.</li>
<li class="bulletList">Customize those pre-built models with custom data so that they are tailored to your use case.</li>
</ul>
<p class="normal">Hence, considered all together, Azure AI Services can achieve the goal of multimodality, if properly orchestrated by an LLM as a reasoning engine, which is exactly the framework LangChain built.</p>
<h2 class="heading-2" id="_idParaDest-141">Getting Started with AzureCognitiveServicesToolkit</h2>
<p class="normal">In fact, LangChain has a native<a id="_idIndexMarker725"/> integration with Azure AI Services called <strong class="keyWord">AzureCognitiveServicesToolkit</strong>, which can be passed as a parameter to an agent and leverage the multimodal capabilities of those models.</p>
<p class="normal">The toolkit makes it easier to incorporate Azure AI services’ capabilities – such as image analysis, form recognition, speech-to-text, and text-to-speech – within your application. It can be used within an agent, which is then empowered to use the AI services to enhance its functionality and provide richer responses.</p>
<p class="normal">Currently, the integration supports the following tools:</p>
<ul>
<li class="bulletList"><strong class="keyWord">AzureCogsImageAnalysisTool</strong>: Used to analyze and extract<a id="_idIndexMarker726"/> metadata from images.</li>
<li class="bulletList"><strong class="keyWord">AzureCogsSpeech2TextTool</strong>: Used to convert speech<a id="_idIndexMarker727"/> to text.</li>
<li class="bulletList"><strong class="keyWord">AzureCogsText2SpeechTool</strong>: Used to synthetize text<a id="_idIndexMarker728"/> to speech with neural voices.</li>
<li class="bulletList"><strong class="keyWord">AzureCogsFormRecognizerTool</strong>: Used to<a id="_idIndexMarker729"/> perform <strong class="keyWord">optical character recognition</strong> (<strong class="keyWord">OCR</strong>).</li>
</ul>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">OCR is a technology <a id="_idIndexMarker730"/>that converts different types of documents, such as scanned paper documents, PDFs, or images captured by a digital camera, into editable and searchable data. OCR can save time, cost, and resources by automating data entry and storage processes. It can also enable access to and editing of the original content of historical, legal, or other types of documents.</p>
</div>
<p class="normal">For example, if you ask an agent what you can make with some ingredients, and provide an image of eggs and flour, the agent can use the Azure AI Services Image Analysis tool to extract the caption, objects, and tags from the image, and then use the provided LLM to suggest some recipes based on the ingredients. To implement this, let’s first set up our toolkit.</p>
<h3 class="heading-3" id="_idParaDest-142">Setting up the toolkit</h3>
<p class="normal">To get started<a id="_idIndexMarker731"/> with the toolkit, you can follow these steps:</p>
<ol>
<li class="numberedList" value="1">You first need to create a multi-service<a id="_idIndexMarker732"/> instance of Azure AI Services in Azure following the instructions at <a href="https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&amp;pivots=azportal">https://learn.microsoft.com/en-us/azure/ai-services/multi-service-resource?tabs=windows&amp;pivots=azportal</a>.</li>
<li class="numberedList">A multi-service resource allows you to access multiple AI services with a single key and endpoint to be passed to LangChain as environmental variables. You can find your keys and endpoint under the <strong class="keyWord">Keys and Endpoint</strong> tab in your resource panel:</li>
</ol>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_10_02.png"/></figure>
<p class="packt_figref">Figure 10.2: Screenshot of a multi-service instance of Azure AI Services</p>
<ol>
<li class="numberedList" value="3">Once the resource is set, we can start building our LegalAgent. To do so, the first thing we need to do is set the AI services environmental variables in order to configure the toolkit. To do so, I’ve saved the following variables in my <code class="inlineCode">.env</code> file:
        <pre class="programlisting code-one"><code class="hljs-code">AZURE_COGS_KEY = "your-api-key"
AZURE_COGS_ENDPOINT = "your-endpoint
AZURE_COGS_REGION = "your-region"
</code></pre>
</li>
<li class="numberedList">Then, you can load them as always alongside the other environmental variables:
        <pre class="programlisting code-one"><code class="hljs-code">import os
from dotenv import load_dotenv
load_dotenv()
azure_cogs_key = os.environ["AZURE_COGS_KEY"]
azure_cogs_endpoint = os.environ["AZURE_COGS_ENDPOINT"]
azure_cogs_region = os.environ["AZURE_COGS_REGION"]
openai_api_key = os.environ['OPENAI_API_KEY']
</code></pre>
</li>
<li class="numberedList">Now, we can configure our toolkit<a id="_idIndexMarker733"/> and also see which tools we have, alongside their description:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain.agents.agent_toolkits import AzureCognitiveServicesToolkit
toolkit = AzureCognitiveServicesToolkit()
[(tool.name, tool.description) for tool in toolkit.get_tools()]
</code></pre>
</li>
</ol>
<p class="normal-one">The following is the corresponding output:</p>
<pre class="programlisting con-one"><code class="hljs-con">[('azure_cognitive_services_form_recognizer',
  'A wrapper around Azure Cognitive Services Form Recognizer. Useful for when you need to extract text, tables, and key-value pairs from documents. Input should be a url to a document.'),
 ('azure_cognitive_services_speech2text',
  'A wrapper around Azure Cognitive Services Speech2Text. Useful for when you need to transcribe audio to text. Input should be a url to an audio file.'),
 ('azure_cognitive_services_text2speech',
  'A wrapper around Azure Cognitive Services Text2Speech. Useful for when you need to convert text to speech. '),
 ('azure_cognitive_services_image_analysis',
  'A wrapper around Azure Cognitive Services Image Analysis. Useful for when you need to analyze images. Input should be a url to an image.')]
</code></pre>
<ol>
<li class="numberedList" value="6">Now, it’s time to initialize our agent. For this purpose, we will use a <code class="inlineCode">STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION</code> agent that, as we saw in previous chapters, also allows for multi-tools input, since we will also add<a id="_idIndexMarker734"/> further tools in the <em class="italic">Leveraging multiple tools</em> section:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain.agents import initialize_agent, AgentType
from langchain import OpenAI
llm = OpenAI()
Model = ChatOpenAI()
agent = initialize_agent(
    tools=toolkit.get_tools(),
    llm=llm,
    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
)
</code></pre>
</li>
</ol>
<p class="normal">Now we have all the ingredients to start testing our agent.</p>
<h3 class="heading-3" id="_idParaDest-143">Leveraging a single tool</h3>
<p class="normal">To start easy, let’s simply ask the agent<a id="_idIndexMarker735"/> to describe the following picture, which will only require the <code class="inlineCode">image_analysis</code> tool to be accomplished:</p>
<figure class="mediaobject"><img alt="A person holding a slingshot  Description automatically generated" src="img/B21714_10_03.png"/></figure>
<p class="packt_figref">Figure 10.3: Sample picture of a slingshot (source: <a href="https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg">https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg</a>)</p>
<p class="normal">Let’s pass the URL of this image<a id="_idIndexMarker736"/> as input to our model, as per the description of the <code class="inlineCode">azure_cognitive_services_image_analysis</code> tool:</p>
<pre class="programlisting code"><code class="hljs-code">description = agent.run("what shows the following image?:"
"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg")
print(description)
</code></pre>
<p class="normal">We then get the following output:</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
Action:
```
{
  "action": "azure_cognitive_services_image_analysis",
  "action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"
}
```
Observation: Caption: a person holding a slingshot
Tags: person, tool, nail, hand, holding, needle
Thought: I know what the image is.
Action:
```
{
  "action": "Final Answer",
  "action_input": "The image is of a person holding a slingshot."
}
```
&gt; Finished chain.
The image is of a person holding a slingshot.
</code></pre>
<p class="normal">As you can see, the agent<a id="_idIndexMarker737"/> was able to retrieve the proper tool to address the user’s question. In this case, the question was very simple, so I want to challenge the same tool with a trickier question.</p>
<p class="normal">The goal is to replicate the GPT-4 capabilities in its common-sense reasoning while working with images, as the following illustration from GPT-4’s earliest experiments shows:</p>
<figure class="mediaobject"><img alt="A close up of a cell phone  Description automatically generated" src="img/B21714_10_04.png"/></figure>
<p class="packt_figref">Figure 10.4: Example of visual capabilities and common sense reasoning of GPT-4 (source: <a href="https://openai.com/research/gpt-4">https://openai.com/research/gpt-4</a>)</p>
<p class="normal">So let’s ask our model something<a id="_idIndexMarker738"/> more challenging. Let’s ask it to reason about the consequences of letting the slingshot go:</p>
<pre class="programlisting code"><code class="hljs-code">agent.run("what happens if the person lets the slingshot go?:"
"https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg")
</code></pre>
<p class="normal">We then obtain the following output:</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
Action:
```
{
  "action": "azure_cognitive_services_image_analysis",
  "action_input": "https://www.stylo24.it/wp-content/uploads/2020/03/fionda.jpg"
}
```
Observation: Caption: a person holding a slingshot
Tags: person, tool, nail, hand, holding, needle
Thought: I know what to respond
Action:
```
{
  "action": "Final Answer",
  "action_input": "If the person lets the slingshot go, it will fly through the air."
}
```
&gt; Finished chain.
'If the person lets go of the slingshot, the object being launched by it would be released and propelled forward by the tension of the stretched rubber bands.'
</code></pre>
<p class="normal">It might seem an easy question, but the agent’s answers imply an extremely refined common sense reasoning: thanks to the metadata extracted from the image leveraging the <code class="inlineCode">image_analysis</code> tool, the LLM was able to set up reasoning in terms of consequences given an action (the person letting the slingshot go).</p>
<p class="normal">In this example, the agent only leveraged<a id="_idIndexMarker739"/> one tool. But what happens if we ask it something that requires at least two tools?</p>
<h3 class="heading-3" id="_idParaDest-144">Leveraging multiple tools</h3>
<p class="normal">Let’s say, for example, that we want<a id="_idIndexMarker740"/> the model to read a story aloud to us based on a picture.</p>
<figure class="mediaobject"><img alt="Meet Côtelette, the Kyrgyz stray dog that followed cyclists all season ..." src="img/B21714_10_05.png"/></figure>
<p class="packt_figref">Figure 10.5: Example of an input image for a story-telling agent (source: <a href="https://i.redd.it/diawvlriobq11.jpg">https://i.redd.it/diawvlriobq11.jpg</a>)</p>
<p class="normal">Let’s ask the agent<a id="_idIndexMarker741"/> to do so:</p>
<pre class="programlisting code"><code class="hljs-code">agent.run("Tell me a story related to the following picture and read the story aloud to me: https://i.redd.it/diawvlriobq11.jpg")
</code></pre>
<p class="normal">We obtain the following output:</p>
<pre class="programlisting con"><code class="hljs-con">Thought:The human requested a story related to the picture. I will use the Azure Cognitive Services Image Analysis tool to analyze the picture and come up with a story.
Action:
```
{
  "action": "azure_cognitive_services_image_analysis",
  "action_input": {
    "query": "https://i.redd.it/diawvlriobq11.jpg"
  }
}
```
Observation: Caption: a dog standing on a snowy hill
Objects: dog
Tags: outdoor, animal, mammal, snow, freezing, dog breed, dog, mountain, covered, winter
Thought:Based on the Azure Cognitive Services Image Analysis tool's analysis of the picture, I will come up with a story involving a dog in a snowy winter wonderland.
Action:
```
{
  "action": "azure_cognitive_services_text2speech",
  "action_input": {
    "query": "Once upon a time, in a snowy winter wonderland, there lived a brave little dog named Snowy. Snowy loved to explore the vast snowy hills and mountains surrounding his home. One day, Snowy stumbled upon a group of lost animals who were stranded in the snow. They were cold, hungry, and scared. Without hesitation, Snowy stepped up to help. He led the group to a nearby shelter and found food and warmth for everyone. Snowy's bravery and kindness earned him a reputation as a hero in the winter wonderland. From then on, he became known as Snowy the Savior of the Snow."
  }
}
```
Observation: C:\Users\vaalt\AppData\Local\Temp\tmp_cecwqht.wav
Thought:The story involving a dog in a snowy winter wonderland seems to have satisfied the human's request for a story related to the picture. I will finalize the response by providing a closing statement.
Action:
```
{
  "action": "Final Answer",
  "action_input": "I hope you enjoyed the story of Snowy the Savior of the Snow, the brave little dog who helped rescue a group of lost animals in the snowy winter wonderland. Thank you for the opportunity to share this tale with you."
}
```
</code></pre>
<p class="normal">As you can see, the agent was able to invoke two tools to accomplish the request:</p>
<ol>
<li class="numberedList" value="1">It first started with the <code class="inlineCode">image_analysis</code> tool to generate the image caption used to produce the story.</li>
<li class="numberedList">Then, it invoked the <code class="inlineCode">text2speech</code> tool to read it aloud to the user.</li>
</ol>
<p class="normal">The agent saved the audio file<a id="_idIndexMarker742"/> in a temporary file, and you can listen to it directly by clicking on the URL. Alternatively, you can save the output as a Python variable and execute it as follows:</p>
<pre class="programlisting code"><code class="hljs-code">from IPython import display
audio = agent.run("Tell me a story related to the following picture and read the story aloud to me: https://i.redd.it/diawvlriobq11.jpg")
display.display(audio)
</code></pre>
<p class="normal">Finally, we can also modify the default prompt that comes with the agent type, to make it more customized with respect to our specific use case. To do so, we first need to inspect the template and then decide which part we can modify. To inspect the template, you can run the following command:</p>
<pre class="programlisting code"><code class="hljs-code">print(agent.agent.llm_chain.prompt.messages[0].prompt.template)
</code></pre>
<p class="normal">Here is our output:</p>
<pre class="programlisting con"><code class="hljs-con">Respond to the human as helpfully and accurately as possible. You have access to the following tools:
{tools}
Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).
Valid "action" values: "Final Answer" or youtube_search, CustomeYTTranscribe
Provide only ONE action per $JSON_BLOB, as shown:
```
{{
  "action": $TOOL_NAME,
  "action_input": $INPUT
}}
```
Follow this format:
Question: input question to answer
Thought: consider previous and subsequent steps
Action:
```
$JSON_BLOB
...
```
Begin! Reminder to ALWAYS respond with a valid json blob of a single action. Use tools if necessary. Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation:.
Thought:
</code></pre>
<p class="normal">Let’s modify the prefix of the prompt and pass it as <code class="inlineCode">kwargs</code> to our agent:</p>
<pre class="programlisting con"><code class="hljs-con">PREFIX = """
You are a story teller for children. 
You read aloud stories based on pictures that the user pass you.
 You always start your story with a welcome message targeting children, with the goal of make them laugh.
 You can use multiple tools to answer the question.
 ALWAYS use the tools.
 You have access to the following tools:"""
agent = initialize_agent(toolkit.get_tools(), model, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True,
                         agent_kwargs={
                            'prefix':PREFIX})
</code></pre>
<p class="normal">As you can see, now the agent <a id="_idIndexMarker743"/>acts more similar to a storyteller with a specific style. You can customize your prompt as you wish, always keeping in mind that each pre-built agent has its own prompt template, hence it is always recommended to first inspect it before customizing it.</p>
<p class="normal">Now that we have explored the out-of-the-box capabilities of the toolkit, let’s build an end-to-end application.</p>
<h3 class="heading-3" id="_idParaDest-145">Building an end-to-end application for invoice analysis</h3>
<p class="normal">Analyzing invoices might require<a id="_idIndexMarker744"/> a lot of manual work<a id="_idIndexMarker745"/> if not assisted by digital processes. To address this, we will build an AI assistant that is able to analyze invoices for us and tell us any<a id="_idIndexMarker746"/> relevant information aloud. We will call this application <strong class="keyWord">CoPenny</strong>.</p>
<p class="normal">With CoPenny, individuals and enterprises could reduce the time of invoice analysis, as well as build toward document process automation and, more generally, digital process automation.</p>
<div><p class="normal"><strong class="keyWord">Definition</strong></p>
<p class="normal">Document process automation is a strategy that uses technology to streamline and automate various document-related tasks and processes within an organization. It involves the use of software tools, including document capture, data extraction, workflow automation, and integration with other systems. For example, document process automation can help you extract, validate, and analyze data from invoices, receipts, forms, and other types of documents. Document process automation can save you time and money, improve accuracy and efficiency, and provide valuable insights and reports from your document data.</p>
<p class="normal"><strong class="keyWord">Digital process automation</strong> (<strong class="keyWord">DPA</strong>) is a broader term that refers<a id="_idIndexMarker747"/> to automating any business process with digital technology. DPA can help you connect your apps, data, and services and boost your team’s productivity with cloud flows. DPA can also help you create more sophisticated and intuitive customer experiences, collaborate across your organization, and innovate with AI and ML.</p>
</div>
<p class="normal">To start building our application, we can follow these steps:</p>
<ol>
<li class="numberedList" value="1">Using <code class="inlineCode">AzureCognitiveServicesToolkit</code>, we will leverage the <code class="inlineCode">azure_cognitive_services_form_recognizer</code> and <code class="inlineCode">azure_cognitive_services_text2speech</code> tools, so we can limit the agent’s “powers” only to those two:
        <pre class="programlisting code-one"><code class="hljs-code">toolkit = AzureCognitiveServicesToolkit().get_tools()
#those tools are at the first and third position in the list
tools = [toolkit[0], toolkit[2]]
tools
</code></pre>
</li>
</ol>
<p class="normal-one">The following is<a id="_idIndexMarker748"/> the corresponding<a id="_idIndexMarker749"/> output:</p>
<pre class="programlisting con-one"><code class="hljs-con">[AzureCogsFormRecognizerTool(name='azure_cognitive_services_form_recognizer', description='A wrapper around Azure Cognitive Services Form Recognizer. Useful for when you need to extract text, tables, and key-value pairs from documents. Input should be a url to a document.', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, azure_cogs_key='', azure_cogs_endpoint='', doc_analysis_client=&lt;azure.ai.formrecognizer._document_analysis_client.DocumentAnalysisClient object at 0x000001FEA6B80AC0&gt;), AzureCogsText2SpeechTool(name='azure_cognitive_services_text2speech', description='A wrapper around Azure Cognitive Services Text2Speech. Useful for when you need to convert text to speech. ', args_schema=None, return_direct=False, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, handle_tool_error=False, azure_cogs_key='', azure_cogs_region='', speech_language='en-US', speech_config=&lt;azure.cognitiveservices.speech.SpeechConfig object at 0x000001FEAF932CE0&gt;)]
</code></pre>
<ol>
<li class="numberedList" value="2">Let’s now initialize the agent with the default prompt and see the results. For this purpose, we will use a sample invoice as a template with which to query the agent:</li>
</ol>
<figure class="mediaobject"><img alt="A close-up of a receipt  Description automatically generated" src="img/B21714_10_06.png"/></figure>
<p class="packt_figref">Figure 10.6: Sample template of a generic invoice (source: https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg)</p>
<ol>
<li class="numberedList" value="3">Let’s start by asking the model to tell us all the men’s <strong class="keyWord">stock-keeping units</strong> (<strong class="keyWord">SKUs</strong>) on the invoice:
        <pre class="programlisting code-one"><code class="hljs-code">agent.run("what are all men's skus?"
"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg")
</code></pre>
</li>
</ol>
<p class="normal-one">We then get the following<a id="_idIndexMarker750"/> output (showing a truncated<a id="_idIndexMarker751"/> output; you can find the whole output in the book’s GitHub repository):</p>
<pre class="programlisting con-one"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
Action:
```
{
  "action": "azure_cognitive_services_form_recognizer",
  "action_input": {
    "query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"
  }
}
```
Observation: Content: PURCHASE ORDER TEMPLATE […]
&gt; Finished chain.
"The men's skus are B222 and D444."
</code></pre>
<ol>
<li class="numberedList" value="4">We can also ask for multiple information (women’s SKUs, shipping address, and delivery dates) as follows (note that the delivery date is not specified, as we want our agent not to hallucinate):
        <pre class="programlisting code-one"><code class="hljs-code">agent.run("give me the following information about the invoice: women's SKUs, shipping address and delivery date."
"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg")
</code></pre>
</li>
</ol>
<p class="normal-one">This gives us the following output:</p>
<pre class="programlisting con-one"><code class="hljs-con">"The women's SKUs are A111 Women's Tall - M. The shipping address is Company Name 123 Main Street Hamilton, OH 44416 (321) 456-7890. The delivery date is not mentioned in the invoice."
</code></pre>
<ol>
<li class="numberedList" value="5">Finally, let’s also leverage the text2speech tool to produce the audio of the response:
        <pre class="programlisting code-one"><code class="hljs-code">agent.run("extract women's SKUs in the following invoice, then read it aloud:"
"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg")
</code></pre>
</li>
</ol>
<p class="normal-one">As per the previous example, you can listen to the audio by clicking on the URL in the chain, or using Python’s <code class="inlineCode">Display</code> function if you save it as a variable.</p>
<ol>
<li class="numberedList" value="6">Now, we want our agent<a id="_idIndexMarker752"/> to be better tailored<a id="_idIndexMarker753"/> toward our goal. To do so, let’s customize the prompt giving specific instructions. In particular, we want the agent to produce the audio output without the user explicitly asking for it:
        <pre class="programlisting code-one"><code class="hljs-code">PREFIX = """
You are an AI assistant that help users to interact with invoices.
You extract information from invoices and read it aloud to users.
You can use multiple tools to answer the question.
Always divide your response in 2 steps:
1. Extracting the information from the invoice upon user's request
2. Converting the transcript of the previous point into an audio file
ALWAYS use the tools.
ALWAYS return an audio file using the proper tool.
You have access to the following tools:
"""
agent = initialize_agent(tools, model, agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose = True,
                         agent_kwargs={
                            'prefix':PREFIX})
</code></pre>
</li>
<li class="numberedList">Let’s run the agent:
        <pre class="programlisting code-one"><code class="hljs-code">agent.run("what are women's SKUs in the following invoice?:"
"https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg")
</code></pre>
</li>
</ol>
<p class="normal-one">This yields<a id="_idIndexMarker754"/> the following<a id="_idIndexMarker755"/> output:</p>
<pre class="programlisting con-one"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
I will need to use the azure_cognitive_services_form_recognizer tool to extract the information from the invoice.
Action:
```
{
  "action": "azure_cognitive_services_form_recognizer",
  "action_input": {
    "query": "https://www.whiteelysee.fr/design/wp-content/uploads/2022/01/custom-t-shirt-order-form-template-free.jpg"
  }
}
```
Observation: Content: PURCHASE ORDER TEMPLATE […]
Observation: C:\Users\vaalt\AppData\Local\Temp\tmpx1n4obf3.wav
Thought:Now that I have provided the answer, I will wait for further inquiries.
</code></pre>
<p class="normal">As you can see, now the agent saved the output into an audio file, even when the user didn’t ask explicitly for it.</p>
<p class="normal"><code class="inlineCode">AzureCognitiveServicesToolkit</code> is a powerful integration that allows for native consumption of Azure AI Services. However, there are some pitfalls of this approach, including the limited<a id="_idIndexMarker756"/> number of AI <a id="_idIndexMarker757"/>services. In the next section, we are going to explore yet another option to achieve multimodality, with a more flexible approach while still keeping an agentic strategy.</p>
<h1 class="heading-1" id="_idParaDest-146">Option 2: Combining single tools into one agent</h1>
<p class="normal">In this leg of our journey<a id="_idIndexMarker758"/> toward multimodality, we will leverage different tools as plug-ins to our <code class="inlineCode">STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION</code> agent. Our goal is to build a copilot agent that will help us generate reviews about YouTube videos, as well as post those reviews on our social media with a nice description and related picture. In all of that, we want to make little or no effort, so we need our agent to perform the following steps:</p>
<ol>
<li class="numberedList" value="1">Search and transcribe a YouTube video based on our input.</li>
<li class="numberedList">Based on the transcription, generate a review with a length and style defined by the user query.</li>
<li class="numberedList">Generate an image related to the video and the review.</li>
</ol>
<p class="normal">We will call our copilot <strong class="keyWord">GPTuber</strong>. In the following<a id="_idIndexMarker759"/> subsections, we will examine each tool and then put them all together.</p>
<h2 class="heading-2" id="_idParaDest-147">YouTube tools and Whisper</h2>
<p class="normal">The first step of our agent will be to search and transcribe the YouTube video based on our input. To do so, there are two tools we need to leverage:</p>
<ul>
<li class="bulletList"><strong class="keyWord">YouTubeSearchTool</strong>: An out-of-the-box tool<a id="_idIndexMarker760"/> offered by LangChain and adapted from <a href="https://github.com/venuv/langchain_yt_tools">https://github.com/venuv/langchain_yt_tools</a>. You can import and try the tool by running the following code, specifying the topic of the video and the number of videos you want the tool to return:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain.tools import YouTubeSearchTool
tool = YouTubeSearchTool()
result = tool.run("Avatar: The Way of Water,1")
result:
</code></pre>
</li>
</ul>
<p class="normal-one">Here is the output:</p>
<pre class="programlisting con-one"><code class="hljs-con">"['/watch?v=d9MyW72ELq0&amp;pp=ygUYQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVy']"
</code></pre>
<p class="normal-one">The tool returns<a id="_idIndexMarker761"/> the URL of the video. To watch it, you can add it to <a href="https://youtube.com">https://youtube.com domain</a>.</p>
<ul>
<li class="bulletList"><strong class="keyWord">CustomYTTranscribeTool</strong>: This is a custom tool<a id="_idIndexMarker762"/> that I’ve adapted from <a href="https://github.com/venuv/langchain_yt_tools">https://github.com/venuv/langchain_yt_tools</a>. It consists of transcribing the audio file retrieved from the previous tool using a speech-to-text model. In our case, we will<a id="_idIndexMarker763"/> be leveraging OpenAI’s <strong class="keyWord">Whisper</strong>.</li>
</ul>
<p class="normal-one">Whisper is a transformer-based model introduced by OpenAI in September 2022. It works as follows:</p>
<ol class="romanList level-2" style="list-style-type: lower-roman;">
<li class="romanList level-2" value="1">It splits the input audio into 30-second chunks, converting them into spectrograms (visual representations of sound frequencies).</li>
<li class="romanList level-2">It then passes them to an encoder.</li>
<li class="romanList level-2">The encoder then produces a sequence of hidden states that capture the information in the audio.</li>
<li class="romanList level-2">A decoder then predicts the corresponding text caption, using special tokens to indicate the task (such as language identification, speech transcription, or speech translation) and the output language.</li>
<li class="romanList level-2">The decoder can also generate timestamps for each word or phrase in the caption.</li>
</ol>
<p class="normal-one">Unlike most OpenAI models, Whisper is open-source.</p>
<p class="normal-one">Since this model takes as input only files and not URLs, within the custom tool, there is a function defined as <code class="inlineCode">yt_get</code> (you can find it in the GitHub repository) that, starting from the video URL, downloads it into a <code class="inlineCode">.mp4</code> file. Once downloaded, you can try Whisper with the following lines of code:</p>
<pre class="programlisting code-one"><code class="hljs-code">import openai
audio_file = open("Avatar The Way of Water  Official Trailer.mp4", 'rb')
result = openai.Audio.transcribe("whisper-1", audio_file)
audio_file.close()
print(result.text)
</code></pre>
<p class="normal-one">Here is the corresponding output:</p>
<pre class="programlisting con-one"><code class="hljs-con">♪ Dad, I know you think I'm crazy. But I feel her. I hear her heartbeat. She's so close. ♪ So what does her heartbeat sound like? ♪ Mighty. ♪ We cannot let you bring your war here. Outcast, that's all I see. I see you. ♪ The way of water connects all things. Before your birth. And after your death. This is our home! I need you with me. And I need you to be strong. ♪ Strongheart. ♪
</code></pre>
<p class="normal-one">By embedding Whisper<a id="_idIndexMarker764"/> in this custom tool, we can transcribe the output of the first tool into a transcript that will serve as input to the next tool. You can see the code and logic behind this embedding and the whole tool in this book’s GitHub repository at <a href="Chapter_10.xhtml">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>, which is a modified version from <a href="https://github.com/venuv/langchain_yt_tools">https://github.com/venuv/langchain_yt_tools</a>.</p>
<p class="normal">Since we already have two tools, we can start building our tools list<a id="_idIndexMarker765"/> and initializing<a id="_idIndexMarker766"/> our agent, using the following code:</p>
<pre class="programlisting code"><code class="hljs-code">llm = OpenAI(temperature=0)
tools = []
tools.append(YouTubeSearchTool())
tools.append(CustomYTTranscribeTool())
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
agent.run("search a video trailer of Avatar: the way of water. Return only 1 video. transcribe the youtube video and return the transcription."
</code></pre>
<p class="normal">The following is the corresponding output:</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
I need to find a specific video and transcribe it.
Action: youtube_search
Action Input: "Avatar: the way of water,1"
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUYQXZhdGFyOiB0aGUgd2F5IG9mIHdhdGVy']
Thought:I found the video I was looking for, now I need to transcribe it.
Action: CustomeYTTranscribe
Action Input: […]
Observation: ♪ Dad, I know you think I'm crazy. […]
Thought:I have the transcription of the video trailer for Avatar: the way of water.
Final Answer: The transcription of the video trailer for Avatar: the way of water is: "♪ Dad, I know you think I'm crazy. […]
&gt; Finished chain.
</code></pre>
<p class="normal">Great! We were able to generate the transcription of this video. The next step will be to generate a review alongside a picture. While the review can be written directly from the LLM and passed as a parameter to the model (so we don’t need another tool), the image generation will need an additional tool. For this purpose, we are going to use OpenAI’s DALL·E.</p>
<h2 class="heading-2" id="_idParaDest-148">DALL·E and text generation</h2>
<p class="normal">Introduced by OpenAI<a id="_idIndexMarker767"/> in January 2021, DALL·E is a transformer-based model that can create images from text descriptions. It is based on GPT-3, which is also used for natural language processing tasks. It is trained on a large dataset of text-image pairs from the web and uses a vocabulary of tokens for both text and image concepts. DALL·E can produce multiple images for the same text, showing different interpretations and variations.</p>
<p class="normal">LangChain offers native integration with DALL·E, which you can use as a tool by running the following code (always setting the environmental variable of your <code class="inlineCode">OPENAI_API_KEY</code> from the <code class="inlineCode">.env</code> file):</p>
<pre class="programlisting code"><code class="hljs-code">from langchain.agents import load_tools
from langchain.agents import initialize_agent
tools = load_tools(['dalle-image-generator'])
agent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
agent.run("Create an image of a halloween night. Return only the image url.")
</code></pre>
<p class="normal">Here is the corresponding output:</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
I need to use an image generator to create an image of a halloween night.
Action: Dall-E Image Generator
Action Input: "An image of a spooky halloween night with a full moon, bats flying in the sky, and a haunted house in the background."
Observation: [link_to_the_blob]
Thought:I have successfully generated an image of a halloween night.
Final Answer: The image url is [link_to_the_blob]
&gt; Finished chain.
</code></pre>
<p class="normal">The following is the image<a id="_idIndexMarker768"/> that was generated, as requested:</p>
<figure class="mediaobject"><img alt="A house with bats flying in the sky  Description automatically generated" src="img/B21714_10_07.png"/></figure>
<p class="packt_figref">Figure 10.7: Image generated by DALL·E upon the user’s input</p>
<p class="normal">Great! Now let’s also<a id="_idIndexMarker769"/> see whether our agent is capable of generating a review of a video based on the transcription:</p>
<pre class="programlisting code"><code class="hljs-code">agent.run("search a video trailer of Avatar: the way of water. Return only 1 video. transcribe the youtube video and return a review of the trailer.")
</code></pre>
<p class="normal">We get the<a id="_idIndexMarker770"/> following<a id="_idIndexMarker771"/> output:</p>
<pre class="programlisting con"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
I need to use both youtube_search and CustomeYTTranscribe tools to answer this question.
Action: youtube_search
Action Input: "Avatar: the way of water,1"
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUYQXZhdGFyOiB0aGUgd2F5IG9mIHdhdGVy']
Thought:I found the video I need to transcribe.
Action: CustomeYTTranscribe
Action Input: […]
Observation: ♪ Dad, I know you think I'm crazy. […]
Thought:I need to write a review of the trailer.
Action: Write a review
Action Input: "Avatar: The Way of Water trailer is visually stunning and captivating. The trailer gives a glimpse into the world of Pandora and the connection between the Na'vi and the way of water. The music and sound effects add to the overall immersive experience. The trailer leaves the audience wanting more and eagerly anticipating the release of the movie."
Observation: Write a review is not a valid tool, try one of [youtube_search, CustomeYTTranscribe].
Thought:I need to use a different tool to write a review.
Action: Write a review
Action Input: N/A
Observation: Write a review is not a valid tool, try one of [youtube_search, CustomeYTTranscribe].
...
Thought:I can write the review manually.
Final Answer: Avatar: The Way of Water trailer is visually stunning and captivating. The trailer gives a glimpse into the world of Pandora and the connection between the Na'vi and the way of water. The music and sound effects add to the overall immersive experience. The trailer leaves the audience wanting more and eagerly anticipating the release of the movie.
&gt; Finished chain.
</code></pre>
<p class="normal">Note how the agent was initially looking for a tool to make a review, to then realize that there is no tool yet that can do it manually thanks to its parametric knowledge. This is a great example of how LLMs are reasoning engines and endowed with common sense reasoning. As always, you can find<a id="_idIndexMarker772"/> the entire chain of thoughts<a id="_idIndexMarker773"/> in the book’s repository.</p>
<p class="normal">The next step will be to put it all together and see whether the agent is capable of orchestrating all the tools, with some assistance in terms of prompt engineering.</p>
<h2 class="heading-2" id="_idParaDest-149">Putting it all together</h2>
<p class="normal">Now that we have all the<a id="_idIndexMarker774"/> ingredients, we need to put them together into one single agent. To do so, we can follow these steps:</p>
<ol>
<li class="numberedList" value="1">First, we need to add the DALL·E tool to the list of tools:
        <pre class="programlisting code-one"><code class="hljs-code">tools = []
tools.append(YouTubeSearchTool())
tools.append(CustomYTTranscribeTool())
tools.append(load_tools(['dalle-image-generator'])[0])
[tool.name for tool in tools]
</code></pre>
</li>
</ol>
<p class="normal-one">This gives us the following output:</p>
<pre class="programlisting con-one"><code class="hljs-con">['youtube_search', 'CustomeYTTranscribe', 'Dall-E Image Generator']
</code></pre>
<ol>
<li class="numberedList" value="2">The next step will be to test the agent with the default prompt, and then try to refine the instructions with some prompt engineering. Let’s start with a pre-configured agent (you can find all the steps in the GitHub repository):
        <pre class="programlisting code-one"><code class="hljs-code">agent = initialize_agent(tools, model, AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)
agent.run("search a video trailer of Avatar: the way of water. Return only 1 video. transcribe the youtube video and return a review of the trailer. Generate an image based on the video transcription")
</code></pre>
</li>
</ol>
<p class="normal-one">This gives us the following output:</p>
<pre class="programlisting con-one"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
I need to search for a video trailer of "Avatar: The Way of Water" and transcribe it to generate a review. Then, I can use the transcription to generate an image based on the video content.
Action: youtube_search
Action Input: "Avatar: The Way of Water trailer,1"
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUgQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXI%3D']
Thought:I found a video trailer of "Avatar: The Way of Water" with the given search query. Now I need to transcribe the video and generate a review.
Action: CustomeYTTranscribe
Action Input: '/watch?v=d9MyW72ELq0&amp;pp=ygUgQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXI%3D'[YTTRANSCIBE***], […]
Observation: ♪ Dad, I know you think I'm crazy. […]
Thought:I have transcribed the video and now I can generate a review based on the transcription. Additionally, I can use the transcription to generate an image.
Action: Dall-E Image Generator
Action Input: "Review of Avatar: The Way of Water trailer: The trailer for Avatar: The Way of Water is captivating and mysterious. It starts with a dialogue between a father and his son, discussing a connection to someone or something. The visuals are stunning, showcasing breathtaking landscapes and vibrant underwater scenes. The trailer hints at an impending war and the protagonist's role as an outcast. The theme of water and its connection to all things is also emphasized. Overall, the trailer leaves the viewer excited and intrigued about the upcoming movie."
Observation: [image_url]
Thought:I have generated a review of the "Avatar: The Way of Water" trailer based on the transcription of the video. Additionally, I have generated an image based on the review. Now I can provide the final answer.
Final Answer: The "Avatar: The Way of Water" trailer is captivating and mysterious, featuring stunning visuals of landscapes and underwater scenes. It hints at an impending war and explores the theme of water and its connection to all things. The trailer leaves viewers excited and intrigued about the upcoming movie.
&gt; Finished chain.
</code></pre>
<p class="normal-one">The following is the accompanying visual output:</p>
<p class="packt_figref"><img alt="A person with dreadlocks and green eyes  Description automatically generated" src="img/B21714_10_08.png"/></p>
<p class="packt_figref">Figure 10.8: Image generated by DALL·E based on the trailer review</p>
<p class="normal-one">Well, even without<a id="_idIndexMarker775"/> any prompt engineering, the agent was able to orchestrate the tools and return the desired results!</p>
<ol>
<li class="numberedList" value="3">Now, let’s try to make it more tailored toward our purpose. Similar to the CoPenny application, we don’t want the user to specify every time to generate a review alongside an image. So let’s modify the default prompt as follows:
        <pre class="programlisting code-one"><code class="hljs-code">PREFIX = """
You are an expert reviewer of movie trailer.
You adapt the style of the review depending on the channel the user want to use, namely Instagram, LinkedIn, Facebook.
You can use multiple tools to answer the question.
ALWAYS search for the youtube video related to the trailer. Search ONLY 1 video.
ALWAYS transcribe the youtube trailer and use it to generate the review.
ALWAYS generate an image alongside the review, based on the transcription of the trailer.
ALWAYS use all the available tools for the various steps.
You have access to the following tools:
"""
agent = initialize_agent(tools, model, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose = True,
                         agent_kwargs={
                            'prefix':PREFIX})
agent.run("Generate a review of the trailer of Avatar: The Way of Water. I want to publish it on Instagram.")
</code></pre>
</li>
</ol>
<p class="normal-one">The output obtained<a id="_idIndexMarker776"/> is as follows:</p>
<pre class="programlisting con-one"><code class="hljs-con">&gt; Entering new AgentExecutor chain...
To generate a review for Instagram, I will need to search for the trailer of "Avatar: The Way of Water" on YouTube and transcribe it. I will also need to generate an image based on the transcription. Let's start by searching for the YouTube video.
Action: youtube_search
Action Input: "Avatar: The Way of Water trailer", 1
Observation: ['/watch?v=d9MyW72ELq0&amp;pp=ygUhQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXIi']
Thought:I have found a YouTube video of the "Avatar: The Way of Water" trailer. Now I will transcribe it and generate an image based on the transcription.
Action: CustomeYTTranscribe
Action Input: '/watch?v=d9MyW72ELq0&amp;pp=ygUhQXZhdGFyOiBUaGUgV2F5IG9mIFdhdGVyIHRyYWlsZXIi'[YTTRANSCIBE***],[…]
Observation: ♪ Dad, I know you think I'm crazy.[…]
Action: Dall-E Image Generator
Action Input: "A scene from the movie 'Avatar: The Way of Water' with the text 'The Way of Water connects all things. This is our home!'"
Observation: [image_url]
Thought:I have generated an image for the Instagram review of the trailer of "Avatar: The Way of Water". Now I can write the review.
Final Answer: "Avatar: The Way of Water" is an upcoming movie that promises to take us on a breathtaking journey. The trailer captivated me with its stunning visuals and powerful storytelling. The tagline "The Way of Water connects all things. This is our home!" resonated with me, highlighting the movie's theme of unity and the importance of preserving our planet. I can't wait to dive into this immersive cinematic experience. #AvatarTheWayOfWater #MovieReview #ComingSoon
&gt; Finished chain.
</code></pre>
<p class="normal-one">This is accompanied<a id="_idIndexMarker777"/> by the following visual output:</p>
<figure class="mediaobject"><img alt="A mountain with a lake and trees  Description automatically generated with medium confidence" src="img/B21714_10_09.png"/></figure>
<p class="packt_figref">Figure 10.9: Image generated by DALL·E based on a trailer review</p>
<p class="normal">Wow! Not only was the agent<a id="_idIndexMarker778"/> able to use all the tools with the proper scope but it also adapted the style to the type of channel we want to share our review on – in this case, Instagram.</p>
<h1 class="heading-1" id="_idParaDest-150">Option 3: Hard-coded approach with a sequential chain</h1>
<p class="normal">The third and last option<a id="_idIndexMarker779"/> offers yet another way of implementing<a id="_idIndexMarker780"/> a multimodal application, which performs the following tasks:</p>
<ul>
<li class="bulletList">Generates a story based on a topic given by the user.</li>
<li class="bulletList">Generates a social media post to promote the story.</li>
<li class="bulletList">Generates an image to go along with the social media post.</li>
</ul>
<p class="normal">We will call<a id="_idIndexMarker781"/> this application <strong class="keyWord">StoryScribe</strong>.</p>
<p class="normal">To implement this, we will build<a id="_idIndexMarker782"/> separate LangChain chains<a id="_idIndexMarker783"/> for those single tasks, and then combine them into a <code class="inlineCode">SequentialChain</code>. As we saw in <em class="chapterRef">Chapter 1</em>, this is a type of chain that allows you to execute multiple chains in a sequence. You can specify the order of the chains and how they pass their outputs to the next chain. So, we first need to create individual chains, then combine them and run as a unique chain. Let’s follow these steps:</p>
<ol>
<li class="numberedList" value="1">We’ll start by initializing the story generator chain:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain.chains import SequentialChain, LLMChain
from langchain.prompts import PromptTemplate
story_template = """You are a storyteller. Given a topic, a genre and a target audience, you generate a story.
Topic: {topic}
Genre: {genre}
Audience: {audience}
Story: This is a story about the above topic, with the above genre and for the above audience:"""
story_prompt_template = PromptTemplate(input_variables=["topic", "genre", "audience"], template=story_template)
story_chain = LLMChain(llm=llm, prompt=story_prompt_template, output_key="story")
result = story_chain({'topic': 'friendship story','genre':'adventure', 'audience': 'young adults'})
print(result['story'])
</code></pre>
</li>
</ol>
<p class="normal-one">This gives us the following output:</p>
<pre class="programlisting con-one"><code class="hljs-con">John and Sarah had been best friends since they were kids. They had grown up together, shared secrets, and been through thick and thin.[…]
</code></pre>
<ol>
<li class="numberedList" value="2">Note that I’ve set the <code class="inlineCode">output_key= "story"</code> parameter so that it can be easily linked as output to the next chain, which will be the social post generator:
        <pre class="programlisting code-one"><code class="hljs-code">template = """You are an influencer that, given a story, generate a social media post to promote the story.
The style should reflect the type of social media used.
Story:
{story}
Social media: {social}
Review from a New York Times play critic of the above play:"""
prompt_template = PromptTemplate(input_variables=["story", "social"], template=template)
social_chain = LLMChain(llm=llm, prompt=prompt_template, output_key='post')
post = social_chain({'story': result['story'], 'social': 'Instagram'})
print(post['post'])
</code></pre>
</li>
</ol>
<p class="normal-one">The following output is then obtained:</p>
<pre class="programlisting con-one"><code class="hljs-con">"John and Sarah's journey of discovery and friendship is a must-see! From the magical world they explore to the obstacles they overcome, this play is sure to leave you with a newfound appreciation for the power of friendship. #FriendshipGoals #AdventureAwaits #MagicalWorlds"
</code></pre>
<p class="normal-one">Here, I used the output<a id="_idIndexMarker784"/> of <code class="inlineCode">story_chain</code> as input<a id="_idIndexMarker785"/> to <code class="inlineCode">social_chain</code>. When we combine all the chains together, this step will be automatically performed by the sequential chain.</p>
<ol>
<li class="numberedList" value="3">Finally, let’s initialize an image generator chain:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain.utilities.dalle_image_generator import DallEAPIWrapper
from langchain.llms import OpenAI
template = """Generate a detailed prompt to generate an image based on the following social media post:
Social media post:
{post}
"""
prompt = PromptTemplate(
    input_variables=["post"],
    template=template,
)
image_chain = LLMChain(llm=llm, prompt=prompt, output_key='image')
</code></pre>
</li>
</ol>
<p class="normal-one">Note that the output<a id="_idIndexMarker786"/> of the chain will be the prompt<a id="_idIndexMarker787"/> to pass to the DALL·E model.</p>
<ol>
<li class="numberedList" value="4">In order to generate the image, we need to use the <code class="inlineCode">DallEAPIWrapper()</code> module available in LangChain:
        <pre class="programlisting code-one"><code class="hljs-code">from langchain.utilities.dalle_image_generator import DallEAPIWrapper
image_url = DallEAPIWrapper().run(image_chain.run("a cartoon-style cat playing piano"))
import cv2
from skimage import io
image = io.imread(image_url)
cv2.imshow('image', image)
cv2.waitKey(0)  
cv2.destroyAllWindows()
</code></pre>
</li>
</ol>
<p class="normal-one">This generates the following output:</p>
<figure class="mediaobject"><img alt="A child giving a flower to a child  Description automatically generated" src="img/B21714_10_10.png"/></figure>
<p class="packt_figref">Figure 10.10: Picture generated by DALL·E given a social media post</p>
<ol>
<li class="numberedList" value="5">The final step will<a id="_idIndexMarker788"/> be to put it all together<a id="_idIndexMarker789"/> into a sequential chain:
        <pre class="programlisting code-one"><code class="hljs-code">overall_chain = SequentialChain(input_variables = ['topic', 'genre', 'audience', 'social'],
                chains=[story_chain, social_chain, image_chain],
                output_variables = ['post', 'image'], verbose=True)
overall_chain({'topic': 'friendship story','genre':'adventure', 'audience': 'young adults', 'social': 'Instagram'}, return_only_outputs=True)
</code></pre>
</li>
</ol>
<p class="normal-one">Here is our output:</p>
<pre class="programlisting con-one"><code class="hljs-con">{'post': '\n\n"John and Sarah\'s journey of discovery and friendship is a must-see! […],
'image': '\nPrompt:\n\nCreate a digital drawing of John and Sarah standing side-by-side,[…]'}
</code></pre>
<p class="normal">Since we passed the <code class="inlineCode">output_variables = ['post, 'image']</code> parameter to the chain, those will be the two outputs of the chain. With <code class="inlineCode">SequentialChain</code>, we have the flexibility to decide as many output variables as we want, so that we can construct our output as we please.</p>
<p class="normal">Overall, there are several ways<a id="_idIndexMarker790"/> to reach multimodality<a id="_idIndexMarker791"/> within your application, and LangChain offers many components that make it easier. Now, let’s compare these approaches.</p>
<h1 class="heading-1" id="_idParaDest-151">Comparing the three options</h1>
<p class="normal">We examined three options<a id="_idIndexMarker792"/> to achieve<a id="_idIndexMarker793"/> this result: options 1 and 2 follow the “agentic” approach, using, respectively, pre-built toolkit and single tools<a id="_idIndexMarker794"/> combined; option 3, on the other<a id="_idIndexMarker795"/> hand, follows a hard-coded<a id="_idIndexMarker796"/> approach, letting<a id="_idIndexMarker797"/> the developer decide the order of actions to be done.</p>
<p class="normal"> All three come with pros and cons, so let’s wrap up some final considerations:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Flexibility vs control</strong>: The agentic approach lets the LLM decide which actions to take and in which order. This implies greater flexibility for the end user since there are no constraints in terms of queries that can be done. On the other hand, having no control over the agent’s chain of thoughts could lead to mistakes that would need several tests of prompt engineering. Plus, as LLMs are non-deterministic, it is also hard to recreate mistakes to retrieve the wrong thought process. Under this point of view, the hard-coded approach is safer, since the developer has full control over the order of execution of the actions.</li>
<li class="bulletList"><strong class="keyWord">Evaluations</strong>: The agentic approach leverages the tools to generate the final answer so that we don’t have to bother to plan these actions. However, if the final output doesn’t satisfy us, it might be cumbersome to understand what is the main source of the error: it might be a wrong plan, rather than a tool that is not doing its job correctly, or maybe a wrong prompt overall. On the other hand, with the hard-coded approach, each chain has its own model that can be tested separately, so that it is easier to identify the step of the process where the main error has occurred.</li>
<li class="bulletList"><strong class="keyWord">Maintenance</strong>: With the agentic approach, there is one component to maintain: the agent itself. We have in fact one prompt, one agent, and one LLM, while the toolkit or list of tools is pre-built and we don’t need to maintain them. On the other hand, with the hard-coded approach, for each chain, we need a separate prompt, model, and testing activities.</li>
</ul>
<p class="normal">To conclude, there is no golden rule to decide which approach to follow: it’s up to the developer to decide depending on the relative weight of the above parameters. As a general rule of thumb, the first step<a id="_idIndexMarker798"/> should be to define<a id="_idIndexMarker799"/> the problem to solve and then evaluate the complexity of each<a id="_idIndexMarker800"/> approach with respect to that problem. If, for example, it is a task that can be entirely<a id="_idIndexMarker801"/> addressed with the Cognitive Services toolkit without even doing prompt<a id="_idIndexMarker802"/> engineering, that could<a id="_idIndexMarker803"/> be the easiest way to proceed; on the other hand, if it requires a lot of control over the single components as well as on the sequence of execution, a hard-coded approach is preferable.</p>
<p class="normal">In the next section, we are going to build a sample front-end using Streamlit, built on top of StoryScribe.</p>
<h1 class="heading-1" id="_idParaDest-152">Developing the front-end with Streamlit</h1>
<p class="normal">Now that we have seen the logic<a id="_idIndexMarker804"/> behind an LLM-powered<a id="_idIndexMarker805"/> StoryScribe, it is time to give our application a GUI. To do so, we will once again leverage Streamlit. As always, you can find the whole Python code in the GitHub book repository at <a href="Chapter_10.xhtml">https://github.com/PacktPublishing/Building-LLM-Powered-Applications</a>.</p>
<p class="normal">As per the previous sections, you need to create a <code class="inlineCode">.py</code> file to run in your terminal via <code class="inlineCode">streamlit run file.py</code>. In our case, the file will be named <code class="inlineCode">storyscribe.py</code>.</p>
<p class="normal">The following are the main steps to set up the front-end:</p>
<ol>
<li class="numberedList" value="1">Configuring the application webpage:
        <pre class="programlisting code-one"><code class="hljs-code">st.set_page_config(page_title="StoryScribe", page_icon="<img alt="" role="presentation" src="img/Book.png"/>")
st.header('<img alt="" role="presentation" src="img/Book.png"/> Welcome to StoryScribe, your story generator and promoter!')
load_dotenv()
openai_api_key = os.environ['OPENAI_API_KEY']
</code></pre>
</li>
<li class="numberedList">Initialize the dynamic variables to be used within the placeholders of the prompts:
        <pre class="programlisting code-one"><code class="hljs-code">topic = st.sidebar.text_input("What is topic?", 'A dog running on the beach')
genre = st.sidebar.text_input("What is the genre?", 'Drama')
audience = st.sidebar.text_input("What is your audience?", 'Young adult')
social = st.sidebar.text_input("What is your social?", 'Instagram')
</code></pre>
</li>
<li class="numberedList">Initialize all the chains<a id="_idIndexMarker806"/> and the overall <a id="_idIndexMarker807"/>chain (I will omit here all the prompt templates; you can find them in the GitHub repository of the book):
        <pre class="programlisting code-one"><code class="hljs-code">story_chain = LLMChain(llm=llm, prompt=story_prompt_template, output_key="story")
social_chain = LLMChain(llm=llm, prompt=social_prompt_template, output_key='post')
image_chain = LLMChain(llm=llm, prompt=prompt, output_key='image')
overall_chain = SequentialChain(input_variables = ['topic', 'genre', 'audience', 'social'],
                chains=[story_chain, social_chain, image_chain],
                output_variables = ['story','post', 'image'], verbose=True)
</code></pre>
</li>
<li class="numberedList">Run the overall chain and print the results:
        <pre class="programlisting code-one"><code class="hljs-code">if st.button('Create your post!'):
    result = overall_chain({'topic': topic,'genre':genre, 'audience': audience, 'social': social}, return_only_outputs=True)
    image_url = DallEAPIWrapper().run(result['image'])
    st.subheader('Story')
    st.write(result['story'])
    st.subheader('Social Media Post')
    st.write(result['post'])
    st.image(image_url)
</code></pre>
</li>
</ol>
<p class="normal">In this case, I’ve set the <code class="inlineCode">output_variables = ['story','post', 'image']</code> parameter so that we will have also the story<a id="_idIndexMarker808"/> itself as output. The final result looks<a id="_idIndexMarker809"/> like the following:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated" src="img/B21714_10_11.png"/></figure>
<p class="packt_figref">Figure 10.11: Front-end of StoryScribe showing the story output</p>
<p class="normal">The following picture is the resulting Instagram post:</p>
<figure class="mediaobject"><img alt="A screenshot of a painting  Description automatically generated" src="img/B21714_10_12.png"/></figure>
<p class="packt_figref">Figure 10.12: Front-end of StoryScribe showing the social media post along with the generated image</p>
<p class="normal">With just a few lines<a id="_idIndexMarker810"/> of code, we were able to set up a simple front-end<a id="_idIndexMarker811"/> for StoryScribe with multimodal capabilities.</p>
<h1 class="heading-1" id="_idParaDest-153">Summary</h1>
<p class="normal">In this chapter, we introduced the concept of multimodality and how to achieve it even without multimodal models. We explored three different ways of achieving the objective of a multimodal application: an agentic approach with a pre-built toolkit, an agentic approach with the combination of single tools, and a hard-coded approach with chained models.</p>
<p class="normal">We delved into the concrete implementation of three applications with the above methods, examining the pros and cons of each approach. We saw, for example, how an agentic approach gives higher flexibility to the end user at the price of less control of the backend plan of action.</p>
<p class="normal">Finally, we implemented a front-end with Streamlit to build a consumable application with the hard-coded approach.</p>
<p class="normal">With this chapter, we conclude Part 2 of the book, where we examined hands-on scenarios and built LLMs-powered applications. In the next chapter, we will focus on how to customize your LLMs even more with the process of fine-tuning, leveraging open-source models, and using custom data for this purpose.</p>
<h1 class="heading-1" id="_idParaDest-154">References</h1>
<ul>
<li class="bulletList">Source code for YouTube tools: <a href="https://github.com/venuv/langchain_yt_tools">https://github.com/venuv/langchain_yt_tools</a></li>
<li class="bulletList">LangChain YouTube tool: <a href="https://python.langchain.com/docs/integrations/tools/youtube">https://python.langchain.com/docs/integrations/tools/youtube</a></li>
<li class="bulletList">LangChain AzureCognitiveServicesToolkit: <a href="https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services">https://python.langchain.com/docs/integrations/toolkits/azure_cognitive_services</a></li>
</ul>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the author and other readers:</p>
<p class="normal"><a href="https://packt.link/llm ">https://packt.link/llm</a></p>
<p class="normal"><img alt="" role="presentation" src="img/QR_Code214329708533108046.png"/></p>
</div>
</body></html>