["```py\n    import torch\n    ```", "```py\n    from diffusers.models import AutoencoderKL\n    ```", "```py\n    vae_model = AutoencoderKL.from_pretrained(\n    ```", "```py\n        \"stabilityai/stable-diffusion-xl-base-1.0\",\n    ```", "```py\n        subfolder = \"vae\"\n    ```", "```py\n    ).to(\"cuda:0\")\n    ```", "```py\n    from diffusers.utils import load_image\n    ```", "```py\n    from diffusers.image_processor import VaeImageProcessor\n    ```", "```py\n    image = load_image(\"/path/to/cat.png\")\n    ```", "```py\n    image_processor = VaeImageProcessor()\n    ```", "```py\n    prep_image = image_processor.preprocess(image)\n    ```", "```py\n    prep_image = prep_image.to(\"cuda:0\")\n    ```", "```py\n    with torch.no_grad():\n    ```", "```py\n        image_latent = vae_model.encode(prep_image\n    ```", "```py\n            ).latent_dist.sample()\n    ```", "```py\n    image_latent.shape\n    ```", "```py\n    with torch.no_grad():\n    ```", "```py\n        decode_image = vae_model.decode(\n    ```", "```py\n            image_latent,\n    ```", "```py\n            return_dict = False\n    ```", "```py\n        )[0]\n    ```", "```py\n    image = image_processor.postprocess(image = decode_image)[0]\n    ```", "```py\n    image\n    ```", "```py\ninput_prompt = \"a running dog\"\nfrom transformers import CLIPTokenizer,CLIPTextModel\nimport torch\n# initialize tokenizer 1\nclip_tokenizer = CLIPTokenizer.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    subfolder = \"tokenizer\",\n    dtype = torch.float16\n)\ninput_tokens = clip_tokenizer(\n    input_prompt,\n    return_tensors=\"pt\"\n)[\"input_ids\"]\nprint(input_tokens)\nclip_tokenizer_2 = CLIPTokenizer.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    subfolder = \"tokenizer_2\",\n    dtype = torch.float16\n)\ninput_tokens_2 = clip_tokenizer_2(\n    input_prompt,\n    return_tensors=\"pt\"\n)[\"input_ids\"]\nprint(input_tokens_2)\n```", "```py\ntensor([[49406,   320,  2761,  1929, 49407]])\ntensor([[49406,   320,  2761,  1929, 49407]])\n```", "```py\nclip_text_encoder = CLIPTextModel.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    subfolder = \"text_encoder\",\n    torch_dtype =torch.float16\n).to(\"cuda\")\n# encode token ids to embeddings\nwith torch.no_grad():\n    prompt_embeds = clip_text_encoder(\n        input_tokens.to(\"cuda\")\n    )[0]\nprint(prompt_embeds.shape)\n```", "```py\ntorch.Size([1, 5, 768])\n```", "```py\nclip_text_encoder_2 = CLIPTextModel.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    subfolder = \"text_encoder_2\",\n    torch_dtype =torch.float16\n).to(\"cuda\")\n# encode token ids to embeddings\nwith torch.no_grad():\n    prompt_embeds_2 = clip_text_encoder_2(input_tokens.to(\"cuda\"))[0]\nprint(prompt_embeds_2.shape)\n```", "```py\ntorch.Size([1, 5, 1280])\n```", "```py\nfrom transformers import CLIPTextModelWithProjection\nclip_text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(\n    \"stabilityai/stable-diffusion-xl-base-1.0\",\n    subfolder = \"text_encoder_2\",\n    torch_dtype =torch.float16\n).to(\"cuda\")\n# encode token ids to embeddings\nwith torch.no_grad():\n    pool_embed = clip_text_encoder_2(input_tokens.to(\"cuda\"))[0]\nprint(pool_embed.shape)\n```", "```py\nimport torch\nfrom diffusers import StableDiffusionXLPipeline\nbase_pipe = StableDiffusionXLPipeline.from_pretrained(\n    \"RunDiffusion/RunDiffusion-XL-Beta\",\n    torch_dtype = torch.float16\n)\nbase_pipe.watermark = None\n```", "```py\nprompt = \"realistic photo of astronaut cat in fighter cockpit, detailed, 8k\"\nsdxl_pipe.to(\"cuda\")\nimage = sdxl_pipe(\n    prompt = prompt,\n    width = 768,\n    height = 1024,\n    generator = torch.Generator(\"cuda\").manual_seed(1)\n).images[0]\nsdxl_pipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nimage\n```", "```py\nfrom diffusers.image_processor import VaeImageProcessor\nimg_processor = VaeImageProcessor()\n# get the size of the image\n(width, height) = image.size\n# upscale image\nimage_x = img_processor.resize(\n    image = image,\n    width = int(width * 1.5),\n    height = int(height * 1.5)\n)\nimage_x\n```", "```py\nfrom diffusers import StableDiffusionXLImg2ImgPipeline\nimg2img_pipe = StableDiffusionXLImg2ImgPipeline(\n    vae = sdxl_pipe.vae,\n    text_encoder = sdxl_pipe.text_encoder,\n    text_encoder_2 = sdxl_pipe.text_encoder_2,\n    tokenizer = sdxl_pipe.tokenizer,\n    tokenizer_2 = sdxl_pipe.tokenizer_2,\n    unet = sdxl_pipe.unet,\n    scheduler = sdxl_pipe.scheduler,\n    add_watermarker = None\n)\nimg2img_pipe.watermark = None\n```", "```py\nimg2img_pipe.to(\"cuda\")\nrefine_image_2x = img2img_pipe(\n    image = image_x,\n    prompt = prompt,\n    strength = 0.3,\n    num_inference_steps = 30,\n    guidance_scale = 4.0\n).images[0]\nimg2img_pipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nrefine_image_2x\n```", "```py\nsdxl_pipe.load_lora_weights(\"path/to/lora.safetensors\")\nsdxl_pipe.fuse_lora(lora_scale = 0.5)\n```", "```py\nsdxl_pipe.load_lora_weights(\"path/to/lora1.safetensors\")\nsdxl_pipe.fuse_lora(lora_scale = 0.5)\nsdxl_pipe.load_lora_weights(\"path/to/lora2.safetensors\")\nsdxl_pipe.fuse_lora(lora_scale = 0.5)\n```", "```py\nsdxl_pipe.load_lora_weights(\"path/to/lora2.safetensors\")\nsdxl_pipe.fuse_lora(lora_scale = -0.5)\n```", "```py\nsdxl_pipe.load_lora_weights(\"path/to/lora1.safetensors\",\n    adapter_name=\"lora1\")\nsdxl_pipe.load_lora_weights(\"path/to/lora2.safetensors\", ,\n    adapter_name=\"lora2\")\n```", "```py\nsdxl_pipe.set_adapters([\"lora1\", \"lora2\"], adapter_weights=[0.5, 1.0])\n```", "```py\nsdxl_pipe.disable_lora()\n```", "```py\nsdxl_pipe.set_adapters([\"lora1\", \"lora2\"], adapter_weights=[0.0, 1.0])\n```", "```py\npip install -U diffusers\n```", "```py\nfrom diffusers import DiffusionPipeline\nimport torch\npipe = DiffusionPipeline.from_pretrained(\n    \"RunDiffusion/RunDiffusion-XL-Beta\",\n    torch_dtype = torch.float16,\n    use_safetensors = True,\n    variant = \"fp16\",\n    custom_pipeline = \"lpw_stable_diffusion_xl\",\n)\nprompt = \"\"\"\nglamour photography, (full body:1.5) photo of young man,\nwhite blank background,\nwear sweater, with scarf,\nwear jean pant,\nwear nike run shoes,\nwear sun glass,\nwear leather shoes,\nholding a umbrella in hand\n\"\"\" * 2\nprompt = prompt + \" a (cute cat:1.5) aside\"\nneg_prompt = \"\"\"\n(worst quality:1.5),(low quality:1.5), paint, cg, spots, bad hands,\nthree hands, noise, blur\n\"\"\"\npipe.to(\"cuda\")\nimage = pipe(\n    prompt = prompt,\n    negative_prompt = neg_prompt,\n    width = 832,\n    height = 1216,\n    generator = torch.Generator(\"cuda\").manual_seed(7)\n).images[0]\npipe.to(\"cpu\")\ntorch.cuda.empty_cache()\nimage\n```"]