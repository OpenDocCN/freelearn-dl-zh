<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04" class="calibre1"/>Chapter 4. Tagging Words and Tokens</h1></div></div></div><p class="calibre9">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Interesting phrase detection</li><li class="listitem">Foreground- or background-driven interesting phrase detection</li><li class="listitem">Hidden Markov Models (HMM) – part-of-speech</li><li class="listitem">N-best word tagging</li><li class="listitem">Confidence-based tagging</li><li class="listitem">Training word tagging</li><li class="listitem">Word-tagging evaluation</li><li class="listitem">Conditional random fields (CRF) for word/token tagging</li><li class="listitem">Modifying CRFs</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch04lvl1sec45" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre9">Words and tokens are the focus of this chapter. The more common extraction technologies, such as named entity recognition, are actually encoded into the concepts presented here, but this will have to wait until <a class="calibre1" title="Chapter 5. Finding Spans in Text – Chunking" href="part0061_split_000.html#page">Chapter 5</a>, <em class="calibre10">Finding Spans in Text – Chunking</em>. We will start easy with finding interesting sets of tokens. Then, we will move on to HMM and finish with one of the most complex components of LingPipe—CRF. As usual, we show you how to evaluate tagging and train your own taggers.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec46" class="calibre1"/>Interesting phrase detection</h1></div></div></div><p class="calibre9">Imagine that a program can take a <a id="id358" class="calibre1"/>bunch of text data and automatically find the interesting parts, where "interesting" means that the word or phrase occurs more often than expected. It has a very nice property—no training data is needed, and it works for any language that we have tokens for. You have seen this most often in tag clouds such as the one in the following figure:</p><div><img src="img/00010.jpeg" alt="Interesting phrase detection" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre9">The preceding figure shows a tag cloud generated for the <a class="calibre1" href="http://lingpipe.com">lingpipe.com</a> home page. However, be aware that tag clouds<a id="id359" class="calibre1"/> are considered to be the "mullets of the Internet" as noted by Jeffery Zeldman in <a class="calibre1" href="http://www.zeldman.com/daily/0405d.shtml">http://www.zeldman.com/daily/0405d.shtml</a>, so you will be on shaky ground if you deploy such a feature on a website.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec102" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">To get the interesting phrases <a id="id360" class="calibre1"/>from a small dataset with tweets about Disney, perform the following steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">Fire up the command line and type:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter4.InterestingPhrases</strong>
</pre></div></li><li class="listitem" value="2">The program should respond with something like:<div><pre class="programlisting">
<strong class="calibre2">Score 42768.0 : Crayola Color </strong>
<strong class="calibre2">Score 42768.0 : Bing Rewards </strong>
<strong class="calibre2">Score 42768.0 : PassPorter Moms </strong>
<strong class="calibre2">Score 42768.0 : PRINCESS BATMAN </strong>
<strong class="calibre2">Score 42768.0 : Vinylmation NIB </strong>
<strong class="calibre2">Score 42768.0 : York City </strong>
<strong class="calibre2">Score 42768.0 : eternal damnation </strong>
<strong class="calibre2">Score 42768.0 : ncipes azules </strong>
<strong class="calibre2">Score 42768.0 : diventare realt </strong>
<strong class="calibre2">Score 42768.0 : possono diventare </strong>
<strong class="calibre2">….</strong>
<strong class="calibre2">Score 42768.0 : Pictures Releases </strong>
<strong class="calibre2">Score 42768.0 : SPACE MOUNTAIN </strong>
<strong class="calibre2">Score 42768.0 : DEVANT MOI </strong>
<strong class="calibre2">Score 42768.0 : QUOI DEVANT </strong>
<strong class="calibre2">Score 42768.0 : Lindsay Lohan </strong>
<strong class="calibre2">Score 42768.0 : EPISODE VII </strong>
<strong class="calibre2">Score 42768.0 : STAR WARS </strong>
<strong class="calibre2">Score 42768.0 : Indiana Jones </strong>
<strong class="calibre2">Score 42768.0 : Steve Jobs </strong>
<strong class="calibre2">Score 42768.0 : Smash Mouth</strong>
</pre></div></li><li class="listitem" value="3">You can also supply a <code class="email">.csv</code> file in our standard format as an argument to see different data.</li></ol><div></div><p class="calibre9">The output tends to be<a id="id361" class="calibre1"/> tantalizingly useless. Tantalizingly useless means that some useful phrases show up, but with a bunch of less interesting phrases that you will never want in your summary of what is interesting in the data. On the interesting side, we can see <code class="email">Crayola Color</code>, <code class="email">Lindsey Lohan</code>, <code class="email">Episode VII</code>, and so on. On the junk side, we can see <code class="email">ncipes azules</code>, <code class="email">pictures releases</code>, and so on. There are lots of ways to address the junk output—the obvious first step will be to use a language ID classifier to throw out non-English.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec103" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre9">Here, we will go through the source in its entirety, broken by explanatory text:</p><div><pre class="programlisting">package com.lingpipe.cookbook.chapter4;

import java.io.FileReader;
import java.io.IOException;
import java.util.List;
import java.util.SortedSet;
import au.com.bytecode.opencsv.CSVReader;
import com.aliasi.lm.TokenizedLM;
import com.aliasi.tokenizer.IndoEuropeanTokenizerFactory;
import com.aliasi.util.ScoredObject;

public class InterestingPhrases {
  static int TEXT_INDEX = 3;
  public static void main(String[] args) throws IOException {
    String inputCsv = args.length &gt; 0 ? args[0] : "data/disney.csv";</pre></div><p class="calibre9">Here, we see the path, imports, and the <code class="email">main()</code> method. Our ternary operator that supplies a default file name or reads from the command line is the last line:</p><div><pre class="programlisting">List&lt;String[]&gt; lines = Util.readCsv(new File(inputCsv));
int ngramSize = 3;
TokenizedLM languageModel = new TokenizedLM(IndoEuropeanTokenizerFactory.INSTANCE, ngramSize);</pre></div><p class="calibre9">After collecting the input data, the first interesting code constructs a tokenized language model which differs in significant ways from the character language models used in <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>. A tokenized language model<a id="id362" class="calibre1"/> operates over tokens created by <code class="email">TokenizerFactory</code>, and the <code class="email">ngram</code> parameter dictates the number of tokens used instead of the number of characters. A subtlety of <code class="email">TokenizedLM</code> is that it can also use character language models to make predictions for tokens it has not seen before. See the <em class="calibre10">Foreground- or background-driven interesting phrase detection</em> recipe to<a id="id363" class="calibre1"/> understand how this works in practice; don't use the preceding constructor unless there are no unknown tokens when estimating. Also, the relevant Javadoc provides more details on this. In the following code snippet, the language model is trained:</p><div><pre class="programlisting">for (String [] line: lines) {
  languageModel.train(line[TEXT_INDEX]);
}</pre></div><p class="calibre9">The next relevant step is the creation of collocations:</p><div><pre class="programlisting">int phraseLength = 2;
int minCount = 2;
int maxReturned = 100;
SortedSet&lt;ScoredObject&lt;String[]&gt;&gt; collocations = languageModel.collocationSet(phraseLength, minCount, maxReturned);</pre></div><p class="calibre9">The parameterization controls the phrase length in tokens; it also sets a minimum count of how often the phrase can be seen and how many phrases to return. We can look at phrases of length 3 as we have a language model that stores 3 grams. Next, we will visit the results:</p><div><pre class="programlisting">for (ScoredObject&lt;String[]&gt; scoredTokens : collocations) {
  double score = scoredTokens.score();
  StringBuilder sb = new StringBuilder();
  for (String token : scoredTokens.getObject()) {
    sb.append(token + " ");
  }
  System.out.printf("Score %.1f : ", score);
  System.out.println(sb);
}</pre></div><p class="calibre9">The <code class="email">SortedSet&lt;ScoredObject&lt;String[]&gt;&gt;</code> collocation<a id="id364" class="calibre1"/> is sorted from a high score to a low score. The intuition behind the score is that a higher score is given when the tokens are seen together more than one would expect, given their singleton frequency in the training data. In other words, phrases are scored depending on how much they vary from the independence assumption based on the tokens. See the <a id="id365" class="calibre1"/>Javadoc at <a class="calibre1" href="http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html">http://alias-i.com/lingpipe/docs/api/com/aliasi/lm/TokenizedLM.html</a> for the exact definitions—an interesting exercise will be to create your own score and compare it with what is <a id="id366" class="calibre1"/>done in LingPipe.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec104" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre9">Given that this code is close to being usable on a website, it is worth discussing tuning. Tuning <a id="id367" class="calibre1"/>is the process of looking at system output and making changes based on the mistakes the system makes. Some changes that we would immediately consider include:</p><div><ul class="itemizedlist"><li class="listitem">A language ID classifier that will be handy to filter out non-English texts</li><li class="listitem">Some thought around how to better tokenize the data</li><li class="listitem">Varying token lengths to include 3 grams and unigrams in the summary</li><li class="listitem">Using named entity recognition to highlight proper nouns</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec47" class="calibre1"/>Foreground- or background-driven interesting phrase detection</h1></div></div></div><p class="calibre9">Like the previous recipe, this recipe finds interesting phrases, but it uses another language model to determine what is interesting. Amazon's statistically improbable phrases (<strong class="calibre2">SIP</strong>)<a id="id368" class="calibre1"/> work this way. You can get a clear view from their <a id="id369" class="calibre1"/>website at <a class="calibre1" href="http://www.amazon.com/gp/search-inside/sipshelp.html">http://www.amazon.com/gp/search-inside/sipshelp.html</a>:</p><div><blockquote class="blockquote1"><p class="calibre19"><em class="calibre10">"Amazon.com's Statistically Improbable Phrases, or "SIPs", are the most distinctive phrases in the text of books in the Search Inside!™ program. To identify SIPs, our computers scan the text of all books in the Search Inside! program. If they find a phrase that occurs a large number of times in a particular book relative to all Search Inside! books, that phrase is a SIP in that book.</em></p><p class="calibre19">SIPs are not necessarily improbable within a particular book, but they are improbable relative to all books in Search Inside!."</p></blockquote></div><p class="calibre9">The foreground model<a id="id370" class="calibre1"/> will be the book being processed, and the background model will be all the other books in Amazon's Search Inside!™ program. While Amazon has probably introduced tweaks that differ, it is the same basic idea.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec105" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre9">There are a few sources of data worth looking at to get interesting phrases with two separate language models. The key is you want the background model to function as the source of expected word/phrase distributions that will help highlight interesting phrases in the foreground model. Some examples include:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Time-separated Twitter data</strong>: The examples of<a id="id371" class="calibre1"/> time-separated Twitter data are as follows:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre2">Background model</strong>: This <a id="id372" class="calibre1"/>refers to a year worth of tweets about Disney World up to yesterday.</li><li class="listitem"><strong class="calibre2">Foreground model</strong>: Tweets <a id="id373" class="calibre1"/>for today.</li><li class="listitem"><strong class="calibre2">Interesting phrases</strong>: What's<a id="id374" class="calibre1"/> new in Disney World today on Twitter.</li></ul></div></li><li class="listitem"><strong class="calibre2">Topic-separated Twitter data</strong>: The examples of topic-separated Twitter data are as follows:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre2">Background model</strong>: Tweets<a id="id375" class="calibre1"/> about Disneyland</li><li class="listitem"><strong class="calibre2">Foreground model</strong>: Tweets<a id="id376" class="calibre1"/> about Disney World</li><li class="listitem"><strong class="calibre2">Interesting phrases</strong>: What <a id="id377" class="calibre1"/>is said about Disney World that is not said about Disneyland</li></ul></div></li><li class="listitem"><strong class="calibre2">Books on very similar topics</strong>: The<a id="id378" class="calibre1"/> examples of books on similar topics are as follows:<div><ul class="itemizedlist1"><li class="listitem"><strong class="calibre2">Background model</strong>: A pile<a id="id379" class="calibre1"/> of early sci-fi novels</li><li class="listitem"><strong class="calibre2">Foreground model</strong>: Jules<a id="id380" class="calibre1"/> Verne's <em class="calibre10">War of the Worlds</em></li><li class="listitem"><strong class="calibre2">Interesting phrases</strong>: The <a id="id381" class="calibre1"/>unique phrases and concepts of "War of the Worlds"</li></ul></div></li></ul></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec106" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">Here are the steps to run a foreground or background model <a id="id382" class="calibre1"/>on tweets<a id="id383" class="calibre1"/> about Disneyland versus tweets about Disney World:</p><div><ol class="orderedlist"><li class="listitem" value="1">In the command line, type:<div><pre class="programlisting">
<strong class="calibre2">java -cp  lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar:lib/opencsv-2.4.jar com.lingpipe.cookbook.chapter4.InterestingPhrasesForegroundBackground</strong>
</pre></div></li><li class="listitem" value="2">The output will look something like:<div><pre class="programlisting">
<strong class="calibre2">Score 989.621859 : [sleeping, beauty]</strong>
<strong class="calibre2">Score 989.621859 : [california, adventure]</strong>
<strong class="calibre2">Score 521.568529 : [winter, dreams]</strong>
<strong class="calibre2">Score 367.309361 : [disneyland, resort]</strong>
<strong class="calibre2">Score 339.429700 : [talking, about]</strong>
<strong class="calibre2">Score 256.473825 : [disneyland, during]</strong>
</pre></div></li><li class="listitem" value="3">The foreground model consists of tweets for the search term, <code class="email">disneyland</code>, and the background model consists of tweets for the search term, <code class="email">disneyworld</code>.</li><li class="listitem" value="4">The top tied results are for unique features of California-based Disneyland, namely, the name of the castle, Sleeping Beauty's Castle, and a theme park built in the parking lot of Disneyland, California Adventure.</li><li class="listitem" value="5">The next bigram is for <em class="calibre10">Winter Dreams</em>, which refers to a premier for a film.</li><li class="listitem" value="6">Overall, not a bad output to distinguish between the tweets of the two resorts.</li></ol><div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec107" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre9">The code is in <code class="email">src/com/lingpipe/cookbook/chapter4/InterestingPhrasesForegroundBackground.java</code>. The exposition starts after we loaded the raw <code class="email">.csv</code> data for the foreground and background models:</p><div><pre class="programlisting">TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
tokenizerFactory = new LowerCaseTokenizerFactory(tokenizerFactory);
int minLength = 5;
tokenizerFactory = new LengthFilterTokenizerFactoryPreserveToken(tokenizerFactory, minLength);</pre></div><pre>Chapter 2</a>, <em class="calibre10">Finding and Working with Words</em>, but the third one is a customized factory that bears some examination. The intent behind the <code class="email">LengthFilterTokenizerFactoryPreserveToken</code> class is to filter short tokens but at the same time not lose adjacency information. The goal is to take the phrase, "Disney is my favorite resort", and produce tokens (<code class="email">disney</code>, <code class="email">_234</code>, <code class="email">_235</code>, <code class="email">favorite</code>, <code class="email">resort</code>), because we don't want short words in our interesting phrases—they tend to sneak past simple statistical models and mess up the output. Please refer to <code class="email">src/come/lingpipe/cookbook/chapter4/LengthFilterTokenizerFactoryPreserveToken.java</code> for the source of the third tokenizer. Also, refer to <a class="calibre1" title="Chapter 2. Finding and Working with Words" href="part0027_split_000.html#page">Chapter 2</a>, <em class="calibre10">Finding and Working with Words</em> for exposition. Next is the background model:</pre><div><pre class="programlisting">int nGramOrder = 3;
TokenizedLM backgroundLanguageModel = new TokenizedLM(tokenizerFactory, nGramOrder);
for (String [] line: backgroundData) {
  backgroundLanguageModel.train(line[Util.TEXT_OFFSET]);
}</pre></div><p class="calibre9">What is being built here is the model that is used to judge the novelty of phrases in the foreground model. Then, we will create and train the foreground model:</p><div><pre class="programlisting">TokenizedLM foregroundLanguageModel = new TokenizedLM(tokenizerFactory,nGramOrder);
for (String [] line: foregroundData) {
  foregroundLanguageModel.train(line[Util.TEXT_OFFSET]);
}</pre></div><p class="calibre9">Next, we will access the <code class="email">newTermSet()</code> method from the foreground model. The parameters and <code class="email">phraseSize</code> determine how long the token sequences are; <code class="email">minCount</code> specifies a minimum number of instances of the phrase to be considered, and <code class="email">maxReturned</code> controls how many results to return:</p><div><pre class="programlisting">int phraseSize = 2;
int minCount = 3;
int maxReturned = 100;
SortedSet&lt;ScoredObject&lt;String[]&gt;&gt; suprisinglyNewPhrases
    = foregroundLanguageModel.newTermSet(phraseSize, minCount, maxReturned,backgroundLanguageModel);
for (ScoredObject&lt;String[]&gt; scoredTokens : suprisinglyNewPhrases) {
    double score = scoredTokens.score();
    String[] tokens = scoredTokens.getObject();
    System.out.printf("Score %f : ", score);
    System.out.println(java.util.Arrays.asList(tokens));
}</pre></div><p class="calibre9">The <a id="id386" class="calibre1"/>preceding <code class="email">for</code> loop prints out the phrases in order of the most surprising phrase to the least surprising one.</p><p class="calibre9">The details of what is going on here are beyond the scope of the recipe, but the Javadoc again starts us down the road to enlightenment.</p><p class="calibre9">The<a id="id387" class="calibre1"/> exact scoring used is the z-score, as defined in <code class="email">BinomialDistribution.z(double,int,int)</code>, with the success probability defined by the n-grams probability estimate in the background model, the number of successes being the count of the n-gram in this model, and the number of trials being the total count in this model.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch04lvl2sec108" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre9">This recipe is the first place where we have faced unknown tokens, which can have very bad properties if not handled correctly. It is easy to see why this is a<a id="id388" class="calibre1"/> problem with a maximum likelihood of a token-based language model, which is a fancy name for a language model that provides an estimate of some unseen tokens by multiplying the likelihoods of each token. Each likelihood is the number of times the token was seen in training divided by the number of tokens seen in data. For example, consider training on the following data from <em class="calibre10">A Connecticut Yankee in King Arthur's Court</em>:</p><div><blockquote class="blockquote1"><p class="calibre19"><em class="calibre10">"The ungentle laws and customs touched upon in this tale are historical, and the episodes which are used to illustrate them are also historical."</em></p></blockquote></div><p class="calibre9">This is very little training data, but it is sufficient for the point being made. Consider how we will get an estimate for the phrase, "The ungentle inlaws" using our language model. There are 24 words with "The" occurring once; we will assign a probability of 1/24 to this. We will assign a probability of 1/24 to "ungentle" as well. If we stop here, we can say that the likelihood of "The ungentle" is 1/24 * 1/24. However, the next word is "inlaws", which does not exist in the training data. If this token is assigned a value of 0/24, it will make the likelihood of the entire string 0 (1/24 * 1/24 * 0/20). This means that whenever there is an unseen token for which the estimate is likely going to be zero, this is generally an<a id="id389" class="calibre1"/> unhelpful property.</p><p class="calibre9">The standard response to this issue is to substitute and approximate the value to stand in for data that has not been seen in training. There are a few approaches to solving this problem:</p><div><ul class="itemizedlist"><li class="listitem">Provide a low but non-zero estimate for unknown tokens. This is a very common approach.</li><li class="listitem">Use character language models with the unknown token. There are provisions for this in the class—refer to the Javadoc.</li><li class="listitem">There are lots of other approaches and substantial research literature. Good search terms are "back off" and "smoothing".</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec48" class="calibre1"/>Hidden Markov Models (HMM) – part-of-speech</h1></div></div></div><p class="calibre9">This recipe brings <a id="id390" class="calibre1"/>in the first hard-core linguistic capability of LingPipe; it refers to the grammatical category for words or <a id="id391" class="calibre1"/>
<strong class="calibre2">part-of-speech</strong> (<strong class="calibre2">POS</strong>). What are the verbs, nouns, adjectives, and so on in text?</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec109" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">Let's jump right in and drag ourselves back to those awkward middle-school years in English class or our equivalent:</p><div><ol class="orderedlist"><li class="listitem" value="1">As always, head over to your friendly command prompt and type the following:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter9.PosTagger </strong>
</pre></div></li><li class="listitem" value="2">The system will respond with a prompt to which we will add a Jorge Luis Borges quote:<div><pre class="programlisting">
<strong class="calibre2">INPUT&gt; Reality is not always probable, or likely.</strong>
</pre></div></li><li class="listitem" value="3">The system will respond delightfully to this quote with:<div><pre class="programlisting">
<strong class="calibre2">Reality_nn is_bez not_* always_rb probable_jj ,_, or_cc likely_jj ._. </strong>
</pre></div></li></ol><div></div><p class="calibre9">Appended to each token is <code class="email">_</code> with a part-of-speech tag; <code class="email">nn</code> is noun, <code class="email">rb</code> is adverb, and so on. The complete tag set and description of the <a id="id392" class="calibre1"/>corpus of the tagger can be found at <a class="calibre1" href="http://en.wikipedia.org/wiki/Brown_Corpus">http://en.wikipedia.org/wiki/Brown_Corpus</a>. Play around with it a bit. POS tagger was one of the first breakthrough machine-learning applications in NLP back in the '90s. You can expect this one to perform at better than 90-percent accuracy, although it might suffer a bit on Twitter data given that that the underlying corpus was collected in 1961.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec110" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre9">As appropriate for a recipe<a id="id393" class="calibre1"/> book, we are not revealing the fundamentals of how part-of-speech taggers are built. There is Javadoc, the Web, and the research literature to help you understand the underlying technology—in the recipe for training an HMM, there is a brief discussion of the underlying HMM. This is about how to use the API as presented:</p><div><pre class="programlisting">public static void main(String[] args) throws ClassNotFoundException, IOException {
  TokenizerFactory tokFactory = IndoEuropeanTokenizerFactory.INSTANCE;
  String hmmModelPath = args.length &gt; 0 ? args[0] : "models/pos-en-general-brown.HiddenMarkovModel";
  HiddenMarkovModel hmm = (HiddenMarkovModel) AbstractExternalizable.readObject(new File(hmmModelPath));
  HmmDecoder decoder = new HmmDecoder(hmm);
  BufferedReader bufReader = new BufferedReader(new InputStreamReader(System.in));
  while (true) {
    System.out.print("\n\nINPUT&gt; ");
    System.out.flush();
    String input = bufReader.readLine();
    Tokenizer tokenizer = tokFactory.tokenizer(input.toCharArray(),0,input.length());
    String[] tokens = tokenizer.tokenize();
    List&lt;String&gt; tokenList = Arrays.asList(tokens);
    firstBest(tokenList,decoder);
  }
}</pre></div><p class="calibre9">The code starts by setting up <code class="email">TokenizerFactory</code>, which makes sense because we need to know what the words that are going to get the parts of speech are. The next line reads in a previously trained part-of-speech tagger as <code class="email">HiddenMarkovModel</code>. We will not go into too much detail; you just need to know that an HMM assigns a part-of-speech tag for token <em class="calibre10">n</em> as a function of the tag assignments that preceded it.</p><p class="calibre9">The fact that these tags are not directly observed in data makes the Markov model hidden. Usually, one or two tokens back are looked at. There is a lot going on with HMMs that is worth understanding.</p><p class="calibre9">The next line with HmmDecoder decoder wraps the HMM in code to tag provided tokens. Our standard interactive <code class="email">while</code> loop is up next with all the interesting bits happening in the firstBest(tokenList,decoder) method at the end. The method is as follows:</p><div><pre class="programlisting">static void firstBest(List&lt;String&gt; tokenList, HmmDecoder decoder) {
  Tagging&lt;String&gt; tagging = decoder.tag(tokenList);
    System.out.println("\nFIRST BEST");
    for (int i = 0; i &lt; tagging.size(); ++i){
      System.out.print(tagging.token(i) + "_" + tagging.tag(i) + " ");
    }
  System.out.println();
}</pre></div><p class="calibre9">Note <a id="id394" class="calibre1"/>the <code class="email">decoder.tag(tokenList)</code> call that produces a <code class="email">Tagging&lt;String&gt;</code> tagging. Tagging does not have an iterator or useful encapsulation of the tag/token pair, so the information is accessed by incrementing an index i.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec49" class="calibre1"/>N-best word tagging</h1></div></div></div><p class="calibre9">The certainty-driven nature of<a id="id395" class="calibre1"/> Computer Science is not reflected in the vagaries of linguistics where reasonable PhDs can agree or disagree at least until Chomsky's henchmen show up. This recipe uses the same HMM trained in the preceding recipe but provides a ranked list of possible tags for each word.</p><p class="calibre9">Where might this be helpful? Imaging a search engine that searched for words and a tag—not necessarily part-of-speech. The search engine can index the word and the top <em class="calibre10">n</em>-best tags that will allow a match into a non-first best tag. This can help increase recall.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec111" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">
<code class="email">N</code>-best analyses push the sophistication boundaries of NLP developers. What used to be a singleton is now a ranked list, but it is where the next level of performance occurs. Let's get started by performing the following steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">Put away your copy of <em class="calibre10">Syntactic Structures</em> face down and type out the following:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.NbestPosTagger </strong>
</pre></div></li><li class="listitem" value="2">Then, enter the following:<div><pre class="programlisting">
<strong class="calibre2">INPUT&gt; Colorless green ideas sleep furiously.</strong>
</pre></div></li><li class="listitem" value="3">It yields the following output:<div><pre class="programlisting">
<strong class="calibre2">N BEST</strong>
<strong class="calibre2">#   JointLogProb         Analysis</strong>
<strong class="calibre2">0     -91.141  Colorless_jj   green_jj   ideas_nns  sleep_vb   furiously_rb   ._.    </strong>
<strong class="calibre2">1     -93.916  Colorless_jj   green_nn   ideas_nns  sleep_vb   furiously_rb   ._.    </strong>
<strong class="calibre2">2     -95.494  Colorless_jj   green_jj   ideas_nns  sleep_rb   furiously_rb   ._.    </strong>
<strong class="calibre2">3     -96.266  Colorless_jj   green_jj   ideas_nns  sleep_nn   furiously_rb   ._.    </strong>
<strong class="calibre2">4     -98.268  Colorless_jj   green_nn   ideas_nns  sleep_rb   furiously_rb   ._.</strong>
</pre></div></li></ol><div></div><p class="calibre9">The output lists from most likely to least likely the estimate of the entire sequence of tokens given the estimates from the HMM. Remember that the joint probabilities are log 2 based. To compare joint probabilities subtract -93.9 from -91.1 for a difference of 2.8. So, the tagger thinks that option 1 is 2 ^ 2.8 = 7 times less likely to occur than option 0. The source of this difference is in assigning green to noun rather than adjective.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec112" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre9">The code to load the <a id="id396" class="calibre1"/>model and command I/O is the same as that of the previous recipe. The difference is in the method used to get and display the tagging:</p><div><pre class="programlisting">static void nBest(List&lt;String&gt; tokenList, HmmDecoder decoder, int maxNBest) {
  System.out.println("\nN BEST");
  System.out.println("#   JointLogProb         Analysis");
  Iterator&lt;ScoredTagging&lt;String&gt;&gt; nBestIt = decoder.tagNBest(tokenList,maxNBest);
  for (int n = 0; nBestIt.hasNext(); ++n) {
    ScoredTagging&lt;String&gt; scoredTagging = nBestIt.next();
    System.out.printf(n + "   %9.3f  ",scoredTagging.score());
    for (int i = 0; i &lt; tokenList.size(); ++i){
      System.out.print(scoredTagging.token(i) + "_" + pad(scoredTagging.tag(i),5));
    }
    System.out.println();
  }</pre></div><p class="calibre9">There is nothing much to it other than working out the formatting issues as the taggings are being iterated over.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec50" class="calibre1"/>Confidence-based tagging</h1></div></div></div><p class="calibre9">There is another view into the <a id="id397" class="calibre1"/>tagging probabilities; this reflects the probability assignments at the level of word. The code reflects the underlying <code class="email">TagLattice</code> and offers insights into whether the tagger is confident or not.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec113" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">This recipe will focus the probability estimates on the individual token. Perform the following steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">Type in the following on the command line or IDE equivalent:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.ConfidenceBasedTagger</strong>
</pre></div></li><li class="listitem" value="2">Then, enter the following:<div><pre class="programlisting">
<strong class="calibre2">INPUT&gt; Colorless green ideas sleep furiously.</strong>
</pre></div></li><li class="listitem" value="3">It yields the following output:<div><pre class="programlisting">
<strong class="calibre2">CONFIDENCE</strong>
<strong class="calibre2">#   Token          (Prob:Tag)*</strong>
<strong class="calibre2">0   Colorless           0.991:jj       0.006:np$      0.002:np  </strong>
<strong class="calibre2">1   green               0.788:jj       0.208:nn       0.002:nns </strong>
<strong class="calibre2">2   ideas               1.000:nns      0.000:rb       0.000:jj  </strong>
<strong class="calibre2">3   sleep               0.821:vb       0.101:rb       0.070:nn  </strong>
<strong class="calibre2">4   furiously           1.000:rb       0.000:ql       0.000:jjr </strong>
<strong class="calibre2">5   .                   1.000:.        0.000:np       0.000:nn  </strong>
</pre></div></li></ol><div></div><p class="calibre9">This view of the data distributes the joint probabilities of the tag and word. We can see that there is <code class="email">.208</code> chance that <code class="email">green</code> should be tagged as <code class="email">nn</code> or a singular noun, but the correct analysis is still <code class="email">.788</code> with adjective <code class="email">jj</code>.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec114" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre9">We are still using the same old HMM from the <em class="calibre10">Hidden Markov Models (HMM) – part-of-speech</em> recipe but using different parts of it. The code to read in the model is exactly the same, with a major difference in how we report results. <code class="email">src/com/lingpipe/cookbook/chapter4/ConfidenceBasedTagger.java</code> the method:</p><div><pre class="programlisting">static void confidence(List&lt;String&gt; tokenList, HmmDecoder decoder) {
  System.out.println("\nCONFIDENCE");
  System.out.println("#   Token          (Prob:Tag)*");
  TagLattice&lt;String&gt; lattice = decoder.tagMarginal(tokenList);

  for (int tokenIndex = 0; tokenIndex &lt; tokenList.size(); ++tokenIndex) {
    ConditionalClassification tagScores = lattice.tokenClassification(tokenIndex);
    System.out.print(pad(Integer.toString(tokenIndex),4));
    System.out.print(pad(tokenList.get(tokenIndex),15));

    for (int i = 0; i &lt; 3; ++i) {
      double conditionalProb = tagScores.score(i);
      String tag = tagScores.category(i);
      System.out.printf(" %9.3f:" + pad(tag,4),conditionalProb);

    }
    System.out.println();
  }
}</pre></div><p class="calibre9">The method <a id="id398" class="calibre1"/>demonstrates the underlying lattice of tokens to the probabilities explicitly, which is at the heart of the HMM. Change the termination condition on the <code class="email">for</code> loop to see more or fewer tags.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec51" class="calibre1"/>Training word tagging</h1></div></div></div><p class="calibre9">Word tagging gets much <a id="id399" class="calibre1"/>more interesting when you can create your own models. The realm of annotating part-of-speech tagging corpora is a bit too much for a mere recipe book—annotation of the part-of-speech data is very difficult because it requires considerable linguistic knowledge to do well. This recipe will directly address the machine-learning component of the HMM-based sentence detector.</p><p class="calibre9">As this is a recipe book, we will minimally explain what an HMM is. The token language models that we have been working with do their previous context calculations on some number of words/tokens that precede the current word being estimated. HMMs take into account some length of the previous tags while calculating estimates for the current token's tag. This allows for seemingly disparate neighbors such as <code class="email">of</code> and <code class="email">in</code> to be similar, because they are both prepositions.</p><p class="calibre9">In the <em class="calibre10">Sentence detection</em> recipe, from <a class="calibre1" title="Chapter 5. Finding Spans in Text – Chunking" href="part0061_split_000.html#page">Chapter 5</a>, <em class="calibre10">Finding Spans in Text – Chunking</em>, a useful but not very flexible sentence detector is based on the <code class="email">HeuristicSentenceModel</code> in LingPipe. Rather than mucking about with modifying/extending the <code class="email">HeuristicSentenceModel</code>, we will build a machine-learning-based sentence system with the data that we annotate.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec115" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">The steps here describe <a id="id400" class="calibre1"/>how to run the program in <code class="email">src/com/lingpipe/cookbook/chapter4/HMMTrainer.java</code>:</p><div><ol class="orderedlist"><li class="listitem" value="1">Either create a new corpus of the sentence-annotated data or use the following default data, which is in <code class="email">data/connecticut_yankee_EOS.txt</code>. If you are rolling your own data, simply edit some text with <code class="email">[' and ']</code> to mark the sentence boundaries. Our example looks like the following:<div><pre class="programlisting">[The ungentle laws and customs touched upon in this tale are
historical, and the episodes which are used to illustrate them
are also historical.] [It is not pretended that these laws and
customs existed in England in the sixth century; no, it is only
pretended that inasmuch as they existed in the English and other
civilizations of far later times, it is safe to consider that it is
no libel upon the sixth century to suppose them to have been in
practice in that day also.] [One is quite justified in inferring
that whatever one of these laws or customs was lacking in that
remote time, its place was competently filled by a worse one.]</pre></div></li><li class="listitem" value="2">Go to the command prompt and run the program with the following command:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar com.lingpipe.cookbook.chapter4.HmmTrainer</strong>
</pre></div></li><li class="listitem" value="3">It will give the following output:<div><pre class="programlisting">
<strong class="calibre2">Training The/BOS ungentle/WORD laws/WORD and/WORD customs/WORD touched/WORD…</strong>
<strong class="calibre2">done training, token count: 123</strong>
<strong class="calibre2">Enter text followed by new line</strong>
<strong class="calibre2">&gt; The cat in the hat. The dog in a bog.</strong>
<strong class="calibre2">The/BOS cat/WORD in/WORD the/WORD hat/WORD ./EOS The/BOS dog/WORD in/WORD a/WORD bog/WORD ./EOS</strong>
</pre></div></li><li class="listitem" value="4">The output is a tokenized text with one of the three tags: <code class="email">BOS</code> for the beginning of a sentence, <code class="email">EOS</code> for the end of a sentence, and <code class="email">WORD</code> for all other tokens.</li></ol><div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec116" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre9">Like many span-based<a id="id401" class="calibre1"/> markups, the <code class="email">span</code> annotation is translated into a token-level annotation, as shown earlier in the recipe's output. So, the first order of business is to collect the annotated text, set up <code class="email">TokenizerFactory</code>, and then call a parsing subroutine to add to <code class="email">List&lt;Tagging&lt;String&gt;&gt;</code>:</p><div><pre class="programlisting">public static void main(String[] args) throws IOException {
  String inputFile = args.length &gt; 0 ? args[0] : "data/connecticut_yankee_EOS.txt";
  char[] text = Files.readCharsFromFile(new File(inputFile), Strings.UTF8);
  TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
  List&lt;Tagging&lt;String&gt;&gt; taggingList = new ArrayList&lt;Tagging&lt;String&gt;&gt;();
  addTagging(tokenizerFactory,taggingList,text);</pre></div><p class="calibre9">The subroutine to parse the preceding format works by first tokenizing the text with <code class="email">IndoEuropeanTokenizer</code>, which has the desirable property of treating the <code class="email">[' and ']</code> sentence delimiters as separate tokens. It does not check whether the sentence delimiters are well formed—a more robust solution will be needed to do this. The tricky bit is that we want to ignore this markup in the resulting token stream, but we want to use it to have the token following <code class="email">[' be a BOS and the token preceding ']</code> be <code class="email">EOS</code>. Other tokens are just <code class="email">WORD</code>. The subroutine builds a parallel <code class="email">Lists&lt;String&gt;</code> instance for tags and tokens, which is then used to create <code class="email">Tagging&lt;String&gt;</code> and is added to <code class="email">taggingList</code>. The tokenization recipes in <a class="calibre1" title="Chapter 2. Finding and Working with Words" href="part0027_split_000.html#page">Chapter 2</a>, <em class="calibre10">Finding and Working with Words</em>, cover what is going on with the tokenizer. Have a look at the following code snippet:</p><div><pre class="programlisting">static void addTagging(TokenizerFactory tokenizerFactory, List&lt;Tagging&lt;String&gt;&gt; taggingList, char[] text) {
  Tokenizer tokenizer = tokenizerFactory.tokenizer(text, 0, text.length);
  List&lt;String&gt; tokens = new ArrayList&lt;String&gt;();
  List&lt;String&gt; tags = new ArrayList&lt;String&gt;();
  boolean bosFound = false;
  for (String token : tokenizer.tokenize()) {
    if (token.equals("[")) {
      bosFound = true;
    }
    else if (token.equals("]")) {
      tags.set(tags.size() - 1,"EOS");
    }
    else {
      tokens.add(token);
      if (bosFound) {
        tags.add("BOS");
        bosFound = false;
      }
      else {
        tags.add("WORD");
      }
    }
  }
  if (tokens.size() &gt; 0) {
    taggingList.add(new Tagging&lt;String&gt;(tokens,tags));
  }
}</pre></div><p class="calibre9">There is a subtlety with <a id="id402" class="calibre1"/>the preceding code. The training data is treated as a single tagging—this will emulate what the input will look like when we use the sentence detector on novel data. If more than one document/chapter/paragraph is being used for training, then we will call this subroutine for each block of text.</p><p class="calibre9">Returning to the <code class="email">main()</code> method, we will set up <code class="email">ListCorpus</code> and add the tagging to the training side of the corpus, one tagging at a time. There is an <code class="email">addTest()</code> method as well, but this recipe is not concerned with evaluation; if it was, we would most likely use <code class="email">XValidatingCorpus</code> anyway:</p><div><pre class="programlisting">ListCorpus&lt;Tagging&lt;String&gt;&gt; corpus = new ListCorpus&lt;Tagging&lt;String&gt;&gt; ();
for (Tagging&lt;String&gt; tagging : taggingList) {
  System.out.println("Training " + tagging);
  corpus.addTrain(tagging);
}</pre></div><p class="calibre9">Next, we will create <code class="email">HmmCharLmEstimator</code>, which is our HMM. Note that there are constructors that allow for customized parameters that affect performance—see the Javadoc. Next, the estimator is trained against the corpus, and <code class="email">HmmDecoder</code> is created, which will actually tag tokens, as shown in the following code snippet:</p><div><pre class="programlisting">HmmCharLmEstimator estimator = new HmmCharLmEstimator();
corpus.visitTrain(estimator);
System.out.println("done training, token count: " + estimator.numTrainingTokens());
HmmDecoder decoder = new HmmDecoder(estimator);</pre></div><pre>Note that there is no requirement that the training tokenizer be the same as the production tokenizer, but one must be careful to not tokenize in a radically different way; otherwise, the HMM will not be seeing the tokens it was trained with. The back-off model will then be used, which will likely degrade performance. Have a look at the following code snippet:</pre><div><pre class="programlisting">BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.print("Enter text followed by new line\n&gt;");
  String evalText = reader.readLine();
  Tokenizer tokenizer = tokenizerFactory.tokenizer(evalText.toCharArray(),0,evalText.length());
  List&lt;String&gt; evalTokens = Arrays.asList(tokenizer.tokenize());
  Tagging&lt;String&gt; evalTagging = decoder.tag(evalTokens);
  System.out.println(evalTagging);
}</pre></div><p class="calibre9">That's it! To truly wrap this as a proper sentence detector, we will need to map back to the character offsets in the original text, but this is covered in <a class="calibre1" title="Chapter 5. Finding Spans in Text – Chunking" href="part0061_split_000.html#page">Chapter 5</a>, <em class="calibre10">Finding Spans in Text – Chunking</em>. This is sufficient to show how to work with HMMs. A more full-featured approach will make sure that each BOS has a matching EOS and the other way around. The HMM has no such requirement.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec117" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre9">We have a small and easy-to-use corpus of the part-of-speech tags; this allows us to show how training the HMM for a very different problem works out to be the same thing. It is like our <em class="calibre10">How to classify a sentiment – simple version</em> recipe, in <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>; the only difference between the language ID and sentiment is the training data. We will start with a hard-coded corpus for simplicity—it is in <code class="email">src/com/lingpipe/cookbook/chapter4/TinyPosCorus.java</code>:</p><div><pre class="programlisting">public class TinyPosCorpus extends Corpus&lt;ObjectHandler&lt;Tagging&lt;String&gt;&gt;&gt; {

  public void visitTrain(ObjectHandler&lt;Tagging&lt;String&gt;&gt; handler) {
    for (String[][] wordsTags : WORDS_TAGSS) {
      String[] words = wordsTags[0];
      String[] tags = wordsTags[1];
      Tagging&lt;String&gt; tagging = new Tagging&lt;String&gt;(Arrays.asList(words),Arrays.asList(tags));
      handler.handle(tagging);
    }
  }

  public void visitTest(ObjectHandler&lt;Tagging&lt;String&gt;&gt; handler) {
    /* no op */
  }

  static final String[][][] WORDS_TAGSS = new String[][][] {
    { { "John", "ran", "." },{ "PN", "IV", "EOS" } },
    { { "Mary", "ran", "." },{ "PN", "IV", "EOS" } },
    { { "John", "jumped", "!" },{ "PN", "IV", "EOS" } },
    { { "The", "dog", "jumped", "!" },{ "DET", "N", "IV", "EOS" } },
    { { "The", "dog", "sat", "." },{ "DET", "N", "IV", "EOS" } },
    { { "Mary", "sat", "!" },{ "PN", "IV", "EOS" } },
    { { "Mary", "likes", "John", "." },{ "PN", "TV", "PN", "EOS" } },
    { { "The", "dog", "likes", "Mary", "." }, { "DET", "N", "TV", "PN", "EOS" } },
    { { "John", "likes", "the", "dog", "." }, { "PN", "TV", "DET", "N", "EOS" } },
    { { "The", "dog", "ran", "." },{ "DET", "N", "IV", "EOS", } },
    { { "The", "dog", "ran", "." },{ "DET", "N", "IV", "EOS", } }
  };</pre></div><p class="calibre9">The corpus manually <a id="id404" class="calibre1"/>creates tokens as well as the tags for the tokens in the static <code class="email">WORDS_TAGS</code> and creates <code class="email">Tagging&lt;String&gt;</code> for each sentence; <code class="email">Tagging&lt;String&gt;</code> consists of two aligned <code class="email">List&lt;String&gt;</code> instances in this case. The taggings are then sent to the <code class="email">handle()</code> method for the <code class="email">Corpus</code> superclass. Swapping in this corpus looks like the following:</p><div><pre class="programlisting">/*
List&lt;Tagging&lt;String&gt;&gt; taggingList = new ArrayList&lt;Tagging&lt;String&gt;&gt;();
addTagging(tokenizerFactory,taggingList,text);
ListCorpus&lt;Tagging&lt;String&gt;&gt; corpus = new ListCorpus&lt;Tagging&lt;String&gt;&gt; ();
for (Tagging&lt;String&gt; tagging : taggingList) {
  System.out.println("Training " + tagging);
  corpus.addTrain(tagging);
}
*/

Corpus&lt;ObjectHandler&lt;Tagging&lt;String&gt;&gt;&gt; corpus = new TinyPosCorpus();
HmmCharLmEstimator estimator = new HmmCharLmEstimator();
corpus.visitTrain(estimator);</pre></div><p class="calibre9">We just <a id="id405" class="calibre1"/>commented out the code that loads the corpus with sentence detection and features in <code class="email">TinyPosCorpus</code> in its place. It doesn't need data to be added so we will just train the HMM with it. To avoid confusion we have created a separate class <code class="email">HmmTrainerPos.java</code>. Running it results in the following:</p><div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar </strong>
<strong class="calibre2">done training, token count: 42</strong>
<strong class="calibre2">Enter text followed by new line</strong>
<strong class="calibre2">&gt; The cat in the hat is back.</strong>
<strong class="calibre2">The/DET cat/N in/TV the/DET hat/N is/TV back/PN ./EOS</strong>
</pre></div><p class="calibre9">The only mistake is that <code class="email">in</code> is a transitive verb <code class="email">TV</code>. The training data is very small so mistakes are to be expected. Like the difference in language ID and sentiment classification in <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>, the HMM is used to learn a very different phenomenon just by changing what the training data is.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec52" class="calibre1"/>Word-tagging evaluation</h1></div></div></div><p class="calibre9">Word tagging evaluation<a id="id406" class="calibre1"/> drives developments in downstream technologies such as named entity detection, which, in turn, drives high-end applications such as coreference. You will notice that much of the evaluation resembles the evaluation from our classifiers except that each tag is evaluated like its own classifier category.</p><p class="calibre9">This recipe should serve to get you started on evaluation, but be aware that there is a very good tutorial on tagging evaluation<a id="id407" class="calibre1"/> on our website at <a class="calibre1" href="http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/posTags/read-me.html</a>; this recipe goes into greater detail on how to best understand tagger performance.</p><p class="calibre9">This recipe is short and easy to use, so you have no excuses to not evaluate your tagger.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec118" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre9">The following is the class<a id="id408" class="calibre1"/> source for our evaluator located at <code class="email">src/com/lingpipe/cookbook/chapter4/TagEvaluator.java</code>:</p><div><pre class="programlisting">public class TagEvaluator {
  public static void main(String[] args) throws ClassNotFoundException, IOException {
    HmmDecoder decoder = null;
    boolean storeTokens = true;
    TaggerEvaluator&lt;String&gt; evaluator = new TaggerEvaluator&lt;String&gt;(decoder,storeTokens);
    Corpus&lt;ObjectHandler&lt;Tagging&lt;String&gt;&gt;&gt; smallCorpus = new TinyPosCorpus();
    int numFolds = 10;
    XValidatingObjectCorpus&lt;Tagging&lt;String&gt;&gt; xValCorpus = new XValidatingObjectCorpus&lt;Tagging&lt;String&gt;&gt;(numFolds);
    smallCorpus.visitCorpus(xValCorpus);
    for (int i = 0; i &lt; numFolds; ++i) {
      xValCorpus.setFold(i);
      HmmCharLmEstimator estimator = new HmmCharLmEstimator();
      xValCorpus.visitTrain(estimator);
      System.out.println("done training " + estimator.numTrainingTokens());
      decoder = new HmmDecoder(estimator);
      evaluator.setTagger(decoder);
      xValCorpus.visitTest(evaluator);
    }
    BaseClassifierEvaluator&lt;String&gt; classifierEval = evaluator.tokenEval();
    System.out.println(classifierEval);
  }
}</pre></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec119" class="calibre1"/>How to do it…</h2></div></div></div><p class="calibre9">We will call out the interesting bits of the preceding code:</p><div><ol class="orderedlist"><li class="listitem" value="1">First off, we will set up <code class="email">TaggerEvaluator</code> with a null <code class="email">HmmDecoder</code> and <code class="email">boolean</code> that controls whether the tokens are stored or not. The <code class="email">HmmDecoder</code> object will be set in the cross-validation code later in the code:<div><pre class="programlisting">HmmDecoder decoder = null;
boolean storeTokens = true;
TaggerEvaluator&lt;String&gt; evaluator = new TaggerEvaluator&lt;String&gt;(decoder,storeTokens);</pre></div></li><li class="listitem" value="2">Next, we will load <code class="email">TinyPosCorpus</code> from the previous recipe and use it to populate <code class="email">XValididatingObjectCorpus</code>—a pretty neat trick that allows for easy conversion between corpus types. Note that we pick 10 folds—the corpus only has 11 training examples, so we want to maximize the amount of training data per fold. See the <em class="calibre10">How to train and evaluate with cross validation</em> recipe in <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>, if you are new to this concept. Have a look at the following code snippet:<div><pre class="programlisting">Corpus&lt;ObjectHandler&lt;Tagging&lt;String&gt;&gt;&gt; smallCorpus = new TinyPosCorpus();
int numFolds = 10;
XValidatingObjectCorpus&lt;Tagging&lt;String&gt;&gt; xValCorpus = new XValidatingObjectCorpus&lt;Tagging&lt;String&gt;&gt;(numFolds);
smallCorpus.visitCorpus(xValCorpus);</pre></div></li><li class="listitem" value="3">The following <a id="id409" class="calibre1"/>code snippet is a <code class="email">for()</code> loop that iterates over the number of folds. The first half of the loop handles training:<div><pre class="programlisting">for (int i = 0; i &lt; numFolds; ++i) {
  xValCorpus.setFold(i);
  HmmCharLmEstimator estimator = new HmmCharLmEstimator();
  xValCorpus.visitTrain(estimator);
  System.out.println("done training " + estimator.numTrainingTokens());</pre></div></li><li class="listitem" value="4">The rest of the loop first creates a decoder for the HMM, sets the evaluator to use this decoder, and then applies the appropriately configured evaluator to the test portion of the corpus:<div><pre class="programlisting">decoder = new HmmDecoder(estimator);
evaluator.setTagger(decoder);
xValCorpus.visitTest(evaluator);</pre></div></li><li class="listitem" value="5">The last lines apply after all folds of the corpus have been used for training and testing. Notice that the evaluator is <code class="email">BaseClassifierEvaluator</code>! It reports on each tag as a category:<div><pre class="programlisting">BaseClassifierEvaluator&lt;String&gt; classifierEval = evaluator.tokenEval();
System.out.println(classifierEval);</pre></div></li><li class="listitem" value="6">Brace yourself for the torrent of evaluation. The following is a small bit of it, namely, the confusion matrix that you should be familiar with from <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>:<div><pre class="programlisting">Confusion Matrix
reference \ response
  ,DET,PN,N,IV,TV,EOS
  DET,4,2,0,0,0,0
  PN,0,7,0,1,0,0
  N,0,0,4,1,1,0
  IV,0,0,0,8,0,0
  TV,0,1,0,0,2,0
  EOS,0,0,0,0,0,11</pre></div></li></ol><div></div><p class="calibre9">That's it. You have an <a id="id410" class="calibre1"/>evaluation setup that is strongly related to the classifier evaluation from <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec120" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre9">There are evaluation classes for the n-best word tagging, that<a id="id411" class="calibre1"/> is, <code class="email">NBestTaggerEvaluator</code> and <code class="email">MarginalTaggerEvaluator</code>, for the confidence<a id="id412" class="calibre1"/> ranked. Again, look at the more detailed tutorial on part-of-speech tagging for a quite thorough presentation on evaluation metrics and some example software to help tune the HMM.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec53" class="calibre1"/>Conditional random fields (CRF) for word/token tagging</h1></div></div></div><p class="calibre9">
<strong class="calibre2">Conditional random fields</strong> (<strong class="calibre2">CRF</strong>) are an extension of the <em class="calibre10">Logistic regression</em> recipe in <a class="calibre1" title="Chapter 3. Advanced Classifiers" href="part0036_split_000.html#page">Chapter 3</a>, <em class="calibre10">Advanced Classifiers</em>, but are applied to word tagging. At the end of <a class="calibre1" title="Chapter 1. Simple Classifiers" href="part0014_split_000.html#page">Chapter 1</a>, <em class="calibre10">Simple Classifiers</em>, we discussed various ways to encode a problem into a classification problem. CRFs<a id="id413" class="calibre1"/> treat the sequence tagging problem as finding the best category where each category (C) is one of the C*T tag (T) assignments to tokens.</p><p class="calibre9">For example, if we have the tokens <code class="email">The</code> and <code class="email">rain</code> and tag <code class="email">d</code> for determiner and <code class="email">n</code> for noun, then the set of categories for the CRF classifier are:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre2">Category 1</strong>: <code class="email">d d</code></li><li class="listitem"><strong class="calibre2">Category 2</strong>: <code class="email">n d</code></li><li class="listitem"><strong class="calibre2">Category 3</strong>: <code class="email">n n</code></li><li class="listitem"><strong class="calibre2">Category 4</strong>: <code class="email">d d</code></li></ul></div><p class="calibre9">Various optimizations <a id="id414" class="calibre1"/>are applied to keep this combinatoric nightmare computable, but this is the general idea. Crazy, but it works.</p><p class="calibre9">Additionally, CRFs allow random features to be used in training in the exact same way that logistic regression does for classification. Additionally, it has data structures optimized for HMM style observations against context. Its use for part-of-speech tagging is not very exciting, because our current HMMs are pretty close to state of the art. Where CRFs really make a difference is in use cases like named entity detection which are covered in <a class="calibre1" title="Chapter 5. Finding Spans in Text – Chunking" href="part0061_split_000.html#page">Chapter 5</a>, <em class="calibre10">Finding Spans in Text – Chunking</em>, but we wanted to address the pure CRF implementation before complicating the presentation with a chunking interface.</p><p class="calibre9">There is an excellent detailed tutorial on <a id="id415" class="calibre1"/>CRFs at <a class="calibre1" href="http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html">http://alias-i.com/lingpipe/demos/tutorial/crf/read-me.html</a>; this recipe follows this tutorial fairly closely. You will find more information and proper references there.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec121" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">All of the <a id="id416" class="calibre1"/>technologies we have been presenting up to now were invented in the previous millennium; this is a technology from the new millennium. Perform the following steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">In the command line, type:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.CRFTagger</strong>
</pre></div></li><li class="listitem" value="2">The console continues with the convergence results that should be familiar from the <em class="calibre10">Logistic regression</em> recipe of <a class="calibre1" title="Chapter 3. Advanced Classifiers" href="part0036_split_000.html#page">Chapter 3</a>, <em class="calibre10">Advanced Classifiers</em>, and we will get the standard command prompt: <div><pre class="programlisting">
<strong class="calibre2">Enter text followed by new line</strong>
<strong class="calibre2">&gt;The rain in Spain falls mainly on the plain.</strong>
</pre></div></li><li class="listitem" value="3">In response to this, we will get some fairly confused output:<div><pre class="programlisting">
<strong class="calibre2">The/DET rain/N in/TV Spain/PN falls/IV mainly/EOS on/DET the/N plain/IV ./EOS</strong>
</pre></div></li><li class="listitem" value="4">This is an awful output, but the CRF has been trained on 11 sentences. So, let's not be too harsh—particularly since this technology mostly reigns supreme for word tagging and span tagging when given sufficient training data to do its work.</li></ol><div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec122" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre9">Like logistic regression, there are many configuration-related tasks that we need to perform to get this class up and running. This recipe will address the CRF-specific aspects of the code and refer to <em class="calibre10">the Logistic regression</em> recipe of <a class="calibre1" title="Chapter 3. Advanced Classifiers" href="part0036_split_000.html#page">Chapter 3</a>, <em class="calibre10">Advanced Classifiers</em> for the logistic-regression aspects of the configuration.</p><p class="calibre9">Starting at the top of the <code class="email">main()</code> method, we will get our corpus, which was discussed in the earlier three recipes:</p><div><pre class="programlisting">Corpus&lt;ObjectHandler&lt;Tagging&lt;String&gt;&gt;&gt; corpus = new TinyPosCorpus();</pre></div><p class="calibre9">Next up is the feature extractor, which is the actual input to the CRF trainer. The only reason it is final is that an anonymous inner class will access it to demonstrate how feature extraction works in the next recipe:</p><div><pre class="programlisting">final ChainCrfFeatureExtractor&lt;String&gt; featureExtractor
  = new SimpleCrfFeatureExtractor();</pre></div><p class="calibre9">We will address how this class works later in the recipe.</p><p class="calibre9">The next block of <a id="id417" class="calibre1"/>configuration is for the underlying logistic-regression algorithm. Refer to the <em class="calibre10">logistic regression</em> recipe in <a class="calibre1" title="Chapter 3. Advanced Classifiers" href="part0036_split_000.html#page">Chapter 3</a>, <em class="calibre10">Advanced Classifiers</em>, for more information on this. Have a look at the following code snippet:</p><div><pre class="programlisting">boolean addIntercept = true;
int minFeatureCount = 1;
boolean cacheFeatures = false;
boolean allowUnseenTransitions = true;
double priorVariance = 4.0;
boolean uninformativeIntercept = true;
RegressionPrior prior = RegressionPrior.gaussian(priorVariance, uninformativeIntercept);
int priorBlockSize = 3;
double initialLearningRate = 0.05;
double learningRateDecay = 0.995;
AnnealingSchedule annealingSchedule = AnnealingSchedule.exponential(initialLearningRate,
  learningRateDecay);
double minImprovement = 0.00001;
int minEpochs = 2;
int maxEpochs = 2000;
Reporter reporter = Reporters.stdOut().setLevel(LogLevel.INFO);</pre></div><p class="calibre9">Next up, the CRF is trained with:</p><div><pre class="programlisting">System.out.println("\nEstimating");
ChainCrf&lt;String&gt; crf = ChainCrf.estimate(corpus,featureExtractor,addIntercept,minFeatureCount,cacheFeatures,allowUnseenTransitions,prior,priorBlockSize,annealingSchedule,minImprovement,minEpochs,maxEpochs,reporter);</pre></div><p class="calibre9">The rest of the code just uses the standard I/O loop. Refer to <a class="calibre1" title="Chapter 2. Finding and Working with Words" href="part0027_split_000.html#page">Chapter 2</a>, <em class="calibre10">Finding and Working with Words</em>, for how the <code class="email">tokenizerFactory</code> works:</p><div><pre class="programlisting">TokenizerFactory tokenizerFactory = IndoEuropeanTokenizerFactory.INSTANCE;
BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
while (true) {
  System.out.print("Enter text followed by new line\n&gt;");
  System.out.flush();
  String text = reader.readLine();
  Tokenizer tokenizer = tokenizerFactory.tokenizer(text.toCharArray(),0,text.length());
  List&lt;String&gt; evalTokens = Arrays.asList(tokenizer.tokenize());
  Tagging&lt;String&gt; evalTagging = crf.tag(evalTokens);
  System.out.println(evalTagging);</pre></div><div><div><div><div><h3 class="title2"><a id="ch04lvl3sec14" class="calibre1"/>SimpleCrfFeatureExtractor</h3></div></div></div><p class="calibre9">Now, we will get to the <a id="id418" class="calibre1"/>feature extractor. The provided implementation closely mimics the features of a standard HMM. The class at <code class="email">com/lingpipe/cookbook/chapter4/SimpleCrfFeatureExtractor.java</code> starts with:</p><div><pre class="programlisting">public class SimpleCrfFeatureExtractor implements ChainCrfFeatureExtractor&lt;String&gt; {
  public ChainCrfFeatures&lt;String&gt; extract(List&lt;String&gt; tokens, List&lt;String&gt; tags) {
    return new SimpleChainCrfFeatures(tokens,tags);
  }</pre></div><p class="calibre9">The <code class="email">ChainCrfFeatureExtractor</code> interface<a id="id419" class="calibre1"/> requires an <code class="email">extract()</code> method with the tokens and associated tags that get converted into <code class="email">ChainCrfFeatures&lt;String&gt;</code> in this case. This is handled by an inner class below <code class="email">SimpleChainCrfFeatures</code>; this inner class extends <code class="email">ChainCrfFeatures</code> and provides implementations of the abstract methods, <code class="email">nodeFeatures()</code> and <code class="email">edgeFeatures()</code>:</p><div><pre class="programlisting">static class SimpleChainCrfFeatures extends ChainCrfFeatures&lt;String&gt; {</pre></div><p class="calibre9">The following constructor access passes the tokens and tags to the super class, which will do the bookkeeping to support looking up <code class="email">tags</code> and <code class="email">tokens</code>:</p><div><pre class="programlisting">public SimpleChainCrfFeatures(List&lt;String&gt; tokens, List&lt;String&gt; tags) {
  super(tokens,tags);
}</pre></div><p class="calibre9">The node features are computed as follows:</p><div><pre class="programlisting">public Map&lt;String,Double&gt; nodeFeatures(int n) {
  ObjectToDoubleMap&lt;String&gt; features = new ObjectToDoubleMap&lt;String&gt;();
  features.increment("TOK_" + token(n),1.0);
  return features;
}</pre></div><p class="calibre9">The tokens are <a id="id420" class="calibre1"/>indexed by their position in the sentence. The node feature for the word/token in position <code class="email">n</code> is the <code class="email">String</code> value returned by the base class method <code class="email">token(n)</code> from <code class="email">ChainCrfFeatures</code> with the prefix <code class="email">TOK_</code>. The value here is <code class="email">1.0</code>. Feature values can be usefully adjusted to values other than 1.0, which is handy for more sophisticated approaches to CRFs, such as using the confidence estimates of other classifiers. Take a look at the following recipe for an example of this.</p><p class="calibre9">Like HMMs, there are features that are dependent on other positions in the input—these are called <strong class="calibre2">edge features</strong>. The edge features take two arguments: one for the position that features are being generated for <code class="email">n</code> and <code class="email">k</code>, which will apply to all other positions in the sentence:</p><div><pre class="programlisting">public Map&lt;String,Double&gt; edgeFeatures(int n, int k) {
  ObjectToDoubleMap&lt;String&gt; features = new ObjectToDoubleMap&lt;String&gt;();
  features.increment("TAG_" + tag(k),1.0);
  return features;
}</pre></div><p class="calibre9">The next recipe will address how to modify feature extraction.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec123" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre9">There is an extensive research literature referenced in the Javadoc and a much more exhaustive tutorial on the LingPipe website.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch04lvl1sec54" class="calibre1"/>Modifying CRFs</h1></div></div></div><p class="calibre9">The power and <a id="id421" class="calibre1"/>appeal of CRFs comes from rich feature extraction—proceed with an evaluation harness that provides feedback on your explorations. This recipe will detail how to create more complex features.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch04lvl2sec124" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre9">We will not train and run a CRF; instead, we will print out the features. Substitute this feature extractor for the one in the previous recipe to see them at work. Perform the following steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">Go to a command line and type:<div><pre class="programlisting">
<strong class="calibre2">java -cp lingpipe-cookbook.1.0.jar:lib/lingpipe-4.1.0.jar: com.lingpipe.cookbook.chapter4.ModifiedCrfFeatureExtractor</strong>
</pre></div></li><li class="listitem" value="2">The feature extractor class outputs for each token in the training data the truth tagging that is being used to learn:<div><pre class="programlisting">
<strong class="calibre2">-------------------</strong>
<strong class="calibre2">Tagging:  John/PN</strong>
</pre></div></li><li class="listitem" value="3">This reflects the training tagging for the token <code class="email">John</code> as determined by <code class="email">src/com/lingpipe/cookbook/chapter4/TinyPosCorpus.java</code>.</li><li class="listitem" value="4">The node features follow the top-three POS tags from our Brown corpus HMM tagger and the <code class="email">TOK_John</code> feature:<div><pre class="programlisting">Node Feats:{nps=2.0251355582754984E-4, np=0.9994337160349874, nn=2.994165140854113E-4, TOK_John=1.0}</pre></div></li><li class="listitem" value="5">Next, the edge features are displayed for the other tokens in the sentence, "John ran":<div><pre class="programlisting">Edge Feats:{TOKEN_SHAPE_LET-CAP=1.0, TAG_PN=1.0}
Edge Feats:{TAG_IV=1.0, TOKEN_SHAPE_LET-CAP=1.0}
Edge Feats:{TOKEN_SHAPE_LET-CAP=1.0, TAG_EOS=1.0}</pre></div></li><li class="listitem" value="6">The rest of the output are the features for the remaining tokens in the sentence and then the remaining sentences in <code class="email">TinyPosCorpus</code>.</li></ol><div></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch04lvl2sec125" class="calibre1"/>How it works…</h2></div></div></div><p class="calibre9">Our feature <a id="id422" class="calibre1"/>extraction code occurs in <code class="email">src/com/lingpipe/cookbook/chapter4/ModifiedCrfFeatureExtractor.java</code>. We will start with the <code class="email">main()</code> method that loads a corpus, runs the contents past the feature extractor, and prints it out:</p><div><pre class="programlisting">public static void main(String[] args) throws IOException, ClassNotFoundException {

  Corpus &lt;ObjectHandler&lt;Tagging&lt;String&gt;&gt;&gt; corpus = new TinyPosCorpus();
  final ChainCrfFeatureExtractor&lt;String&gt; featureExtractor = new ModifiedCrfFeatureExtractor();</pre></div><p class="calibre9">We will tee up <code class="email">TinyPosCorpus</code> from the previous recipe as our corpus, and then, we will create a feature extractor from the containing class. The use of <code class="email">final</code> is required by referencing the variable in the anonymous inner class that follows.</p><p class="calibre9">Apologies for the anonymous inner class, but it is just the easiest way to access what is stored in the corpus for various reasons such as copying and printing. In this case, we are just generating and printing the features found for the training data:</p><div><pre class="programlisting">corpus.visitCorpus(new ObjectHandler&lt;Tagging&lt;String&gt;&gt;() {
  @Override
  public void handle(Tagging&lt;String&gt; tagging) {
    ChainCrfFeatures&lt;String&gt; features = featureExtractor.extract(tagging.tokens(), tagging.tags());</pre></div><p class="calibre9">The corpus contains <code class="email">Tagging</code> objects, and they, in turn, contain a <code class="email">List&lt;String&gt;</code> of tokens and tags. Then, this information is used to create a <code class="email">ChainCrfFeatures&lt;String&gt;</code> object by applying the <code class="email">featureExtractor.extract()</code> method to the tokens and tags. This will involve substantial computation, as will be shown.</p><p class="calibre9">Next, we will do reporting of the training data with tokens and the expected tagging:</p><div><pre class="programlisting">for (int i = 0; i &lt; tagging.size(); ++i) {
  System.out.println("---------");
  System.out.println("Tagging:  " + tagging.token(i) + "/" + tagging.tag(i));</pre></div><p class="calibre9">Then, we will follow with the features that will be used to inform the CRF model in attempting to produce the preceding tagging for nodes:</p><div><pre class="programlisting">System.out.println("Node Feats:" + features.nodeFeatures(i));</pre></div><p class="calibre9">Then, the edge features are produced by the following iteration of relative positions to the source node <code class="email">i</code>:</p><div><pre class="programlisting">for (int j = 0; j &lt; tagging.size(); ++j) {
  System.out.println("Edge Feats:" 
        + features.edgeFeatures(i, j));
}</pre></div><p class="calibre9">This is it to print out the features. Now, we will address how the feature extractor is constructed. We assume that you are familiar with the previous recipe. First, the constructor that brings in the Brown corpus POS tagger:</p><div><pre class="programlisting">HmmDecoder mDecoder;

public ModifiedCrfFeatureExtractor() throws IOException, ClassNotFoundException {
  File hmmFile = new File("models/pos-en-general-" + "brown.HiddenMarkovModel");
  HiddenMarkovModel hmm = (HiddenMarkovModel)AbstractExternalizable.readObject(hmmFile);
  mDecoder = new HmmDecoder(hmm);
}</pre></div><p class="calibre9">The constructor <a id="id423" class="calibre1"/>brings in some external resources for feature generation, namely, a POS tagger trained on the Brown corpus. Why involve another POS tagger for a POS tagger? We will call the role of the Brown POS tagger a "feature tagger" to distinguish it from the tagger we are trying to build. A few reasons to use the feature tagger are:</p><div><ul class="itemizedlist"><li class="listitem">We are using a stupidly small corpus for training, and a more robust generalized POS feature tagger will help things out. <code class="email">TinyPosCorpus</code> is too small for even this benefit, but with a bit more data, the fact that there is a feature <code class="email">at</code> that unifies <code class="email">the</code>, <code class="email">a</code>, and <code class="email">some</code> will help the CRF recognize that <code class="email">some dog</code> is <code class="email">'DET'</code> <code class="email">'N'</code>, even though it has never seen <code class="email">some</code> in training.</li><li class="listitem">We have had to work with tag sets that are not aligned with POS feature taggers. The CRF can use these observations in the foreign tag set to better reason about the desired tagging. The simplest case is that <code class="email">at</code>, from the Brown corpus tag set, maps cleanly onto <code class="email">DET</code> in this tag set.</li><li class="listitem">There can be performance improvements to run multiple taggers that are either trained on different data or use different technologies to tag. The CRF can then, hopefully, recognize contexts where one tagger outperforms others and use this information to guide the analysis. Back in the day, our MUC-6 system featured 3 POS taggers that voted for the best output. Letting the CRF sort it out will be a superior approach.</li></ul></div><p class="calibre9">The guts of feature extraction are accessed with the <code class="email">extract</code> method:</p><div><pre class="programlisting">public ChainCrfFeatures&lt;String&gt; extract(List&lt;String&gt; tokens, List&lt;String&gt; tags) {
  return new ModChainCrfFeatures(tokens,tags);
}</pre></div><p class="calibre9">
<code class="email">ModChainCrfFeatures</code> is created as an inner class just to keep the proliferation of classes to a minimum, and the enclosing class is very lightweight:</p><div><pre class="programlisting">class ModChainCrfFeatures extends ChainCrfFeatures&lt;String&gt; {
  
  TagLattice&lt;String&gt; mBrownTaggingLattice;
  
  public ModChainCrfFeatures(List&lt;String&gt; tokens, List&lt;String&gt; tags) {
    super(tokens,tags);
    mBrownTaggingLattice = mDecoder.tagMarginal(tokens);
  }</pre></div><p class="calibre9">The preceding <a id="id424" class="calibre1"/>constructor hands off the tokens and tags to the super class, which handles bookkeeping of this data. Then, the "feature tagger" is applied to the tokens, and the resulting output is assigned to the member variable, <code class="email">mBrownTaggingLattice</code>. The code will access the tagging, one token at a time, so it must be computed now.</p><p class="calibre9">The feature creation step happens with two methods: <code class="email">nodeFeatures</code> and <code class="email">edgeFeatures</code>. We will start with a simple enhancement of <code class="email">edgeFeatures</code> from the previous recipe:</p><div><pre class="programlisting">public Map&lt;String,? extends Number&gt; edgeFeatures(int n, int k) {
  ObjectToDoubleMap&lt;String&gt; features = newObjectToDoubleMap&lt;String&gt;();
  features.set("TAG_" + tag(k), 1.0d);
  String category = IndoEuropeanTokenCategorizer.CATEGORIZER.categorize(token(n));
  features.set("TOKEN_SHAPE_" + category,1.0d);
  return features;
}</pre></div><p class="calibre9">The code adds a<a id="id425" class="calibre1"/> token-shaped feature that generalizes <code class="email">12</code> and <code class="email">34</code> into <code class="email">2-DIG</code> and many other generalizations. To CRF, the similarity of <code class="email">12</code> and <code class="email">34</code> as two-digit numbers is non-existent unless feature extraction says otherwise. Refer to the Javadoc for the complete categorizer output.</p><div><div><div><div><h3 class="title2"><a id="ch04lvl3sec15" class="calibre1"/>Candidate-edge features</h3></div></div></div><p class="calibre9">CRFs allow random <a id="id426" class="calibre1"/>features to be applied, so the question is what features make sense to use. Edge features are used in conjunction with node features, so another issue is whether a feature should be applied to edges or nodes. Edge features will be used to reason about relationships between the current word/token to those around it. Some possible edge features are:</p><div><ul class="itemizedlist"><li class="listitem">The token shape (all caps, starts with a number, and so on) of the previous token as done earlier.</li><li class="listitem">Recognition of iambic pentameter that requires a correct ordering of stressed and unstressed syllables. This will require a syllable-stress tokenizer as well.</li><li class="listitem">It often happens that text contains one or more languages—this is called code switching. It is a common occurrence in tweets. A reasonable edge feature will be the language of surrounding tokens; this language will better model that the next word is likely to be of the same language as the previous word.</li></ul></div></div><div><div><div><div><h3 class="title2"><a id="ch04lvl3sec16" class="calibre1"/>Node features</h3></div></div></div><p class="calibre9">The node features tend to be where the action is <a id="id427" class="calibre1"/>in CRFs, and they can get very rich. The <em class="calibre10">Named entity recognition using CRFs with better features</em> recipe in <a class="calibre1" title="Chapter 5. Finding Spans in Text – Chunking" href="part0061_split_000.html#page">Chapter 5</a>, <em class="calibre10">Finding Spans in Text – Chunking</em>, is an example. We will add part-of-speech tags in this recipe to the token feature of the previous recipe:</p><div><pre class="programlisting">public Map&lt;String,? extends Number&gt; nodeFeatures(int n) {
  ObjectToDoubleMap&lt;String&gt; features = new ObjectToDoubleMap&lt;String&gt;();
  features.set("TOK_" + token(n), 1);
  ConditionalClassification tagScores = mBrownTaggingLattice.tokenClassification(n);
  for (int i = 0; i &lt; 3; ++ i) {
    double conditionalProb = tagScores.score(i);
    String tag = tagScores.category(i);
    features.increment(tag, conditionalProb);
  }
  return features;
}</pre></div><p class="calibre9">Then as in the previous recipe the token feature is added with:</p><div><pre class="programlisting">features.set("TOK_" + token(n), 1); </pre></div><p class="calibre9">This results in the <a id="id428" class="calibre1"/>token string being prepended with <code class="email">TOK_</code> and a count of <code class="email">1</code>. Note that while <code class="email">tag(n)</code> is available in training, it doesn't make sense to use this information, as that is what the CRF is trying to predict.</p><p class="calibre9">Next, the top-three tags are extracted from the POS feature tagger and added with the associated conditional probability. CRFs will be able to work with the varying weights productively.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch04lvl2sec126" class="calibre1"/>There's more…</h2></div></div></div><p class="calibre9">When generating new features, some thought about the sparseness of data is worth considering. If dates were likely to be an important feature for the CRF, it would probably not be a good idea to do the standard Computer Science thing and convert the date to milliseconds since Jan 1, 1970 GMT. The reason is that the <code class="email">MILLI_1000000000</code> feature will be treated as completely different from <code class="email">MILLI_1000000001</code>. There are a few reasons:</p><div><ul class="itemizedlist"><li class="listitem">The underlying classifier does not know that the two values are nearly the same</li><li class="listitem">The classifier does not know that the <code class="email">MILLI_</code> prefix is the same—the common prefix is only there for human convenience</li><li class="listitem">The feature is unlikely to occur in training more than once and will likely be pruned by a minimum feature count</li></ul></div><p class="calibre9">Instead of normalizing dates to milliseconds, consider an abstraction over the dates that will likely have many instances in training data, such as the <code class="email">has_date</code> feature that ignores the actual date but notes the existence of the date. If the date is important, then compute all the important information about the date. If it is a day of the week, then map to days of the week. If temporal order matters, then map to coarser measurement that is likely to have many measurements. Generally speaking, CRFs and the underlying logistic regression classifier are robust against ineffective features, so feel free to be creative—you are unlikely to make accuracy worse by adding features.</p></div></div></body></html>