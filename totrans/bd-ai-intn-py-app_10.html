<html><head></head><body>

		<p>&#13;
			<h1 class="chapter-number" id="_idParaDest-172"><a id="_idTextAnchor214"/>10</h1>&#13;
			<h1 id="_idParaDest-173"><a id="_idTextAnchor215"/>Refining the Semantic Data Model to Improve Accuracy</h1>&#13;
			<p>To effectively use vector search for semantic long-term memory in an intelligent application, you must optimize the semantic data model to the application’s needs. As the semantic data model uses vector embedding models and vector search, you must optimize the contents of the embedded data and the way the data is retrieved.</p>&#13;
			<p>Refining the semantic data model can lead to significant improvements in retrieval accuracy and overall application performance. In <strong class="bold">retrieval-augmented generation</strong> (<strong class="bold">RAG</strong>) applications, an effective semantic data model serves as the foundation for a robust retrieval system, which directly informs the quality of the generated outputs. The rest of the chapter examines different ways in which you can refine the semantic data model and retrieval.</p>&#13;
			<p>This chapter will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Experimenting with different embedding models</li>&#13;
				<li>Fine-tuning embedding models</li>&#13;
				<li>Including metadata in the embedded content to maximize semantic relevance</li>&#13;
				<li>Various techniques to optimize RAG use cases, including query mutation, formatting ingested data, and advanced retrieval systems</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-174"><a id="_idTextAnchor216"/>Technical requirements</h1>&#13;
			<p>You will need the following technical requirements to run the code in this chapter:</p>&#13;
			<ul>&#13;
				<li>A programming environment with Python 3.x installed</li>&#13;
				<li>A programming environment capable of running the open source embedding model <code>gte-base-en-v1.5</code> locally</li>&#13;
				<li>An OpenAI API key. To create an API key, refer to the OpenAI documentation at <a href="https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key">https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key</a></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-175"><a id="_idTextAnchor217"/>Embeddings</h1>&#13;
			<p><strong class="bold">Vector embeddings</strong> are the foundation of the semantic data model, serving as the machine-interpretable representation of ideas and relationships. Embeddings are mathematical representations of objects as points in a multi-dimensional space. They act as the glue that connects the various semantic pieces of data in an intelligent application. The distance between vectors correlates to semantic similarity. You can use this semantic similarity score to retrieve related information that would otherwise be difficult to connect. This concept holds true regardless of the specific use case, be it RAG, recommendation systems, anomaly detection, or others.</p>&#13;
			<p>Having an embedding model better tailored to a use case can improve accuracy and performance. Experimenting with different embedding models and fine-tuning them on domain-specific data can help identify the best fit for a particular use case, further enhancing their effectiveness.</p>&#13;
			<h2 id="_idParaDest-176"><a id="_idTextAnchor218"/>Experimenting with different embedding models</h2>&#13;
			<p>When building intelligent applications, you can experiment with different pre-trained embedding models. Different models have varying accuracy, cost, and efficiency. Their performance can vary significantly depending on the specific application and data. By experimenting with multiple models, developers can identify the best fit for their use case.</p>&#13;
			<p><em class="italic">Table 10.1</em> lists some popular embedding models as of writing in spring 2024 that are taken from the Hugging Face <strong class="bold">Massive Test Embedding Benchmark</strong> (<strong class="bold">MTEB</strong>) Leaderboard:<a class="_idFootnoteLink _idGenColorInherit" href="B22495_10.xhtml#footnote-002">1</a></p>&#13;
			<div>&#13;
				<p class="Footnote"><a class="_idFootnoteAnchor _idGenColorInherit" href="B22495_10.xhtml#footnote-002-backlink">1</a>	The information from the MTEB Leaderboard was taken on April 30, 2024. (<a href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a>)</p>&#13;
			</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table001-6">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<thead>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Model name</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Developer</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Is it open </strong><strong class="bold">source?</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Embedding </strong><strong class="bold">length</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Average </strong><strong class="bold">score</strong><a class="_idFootnoteLink _idGenColorInherit" href="B22495_10.xhtml#footnote-001">2</a></p>&#13;
							<p>&#13;
								<p class="Footnote"><a class="_idFootnoteAnchor _idGenColorInherit" href="B22495_10.xhtml#footnote-001-backlink">2</a>	This score is calculated as an average of a variety of benchmarks. For more information about the evaluation metrics used in the benchmark, refer to the <em class="italic">MTEB: Massive Text Embedding Benchmark</em> research paper (<a href="https://arxiv.org/abs/2210.07316">https://arxiv.org/abs/2210.07316</a>).</p>&#13;
							</p>&#13;
						</td>&#13;
					</tr>&#13;
				</thead>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><code>text-embedding-3-large</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>OpenAI</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>No</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>3072</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>64.59</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><code>cohere-embed-english-v3.0</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Cohere</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>No</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>1024</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>64.47</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><code>gte-base-en-v1.5</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Alibaba</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Yes</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>768</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>64.11</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><code>sentence-t5-large</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Sentence Transformers</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Yes</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>768</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>57.06</p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.1: Selected embedding models</p>&#13;
			<p>To properly compare different embedding models, you must have a consistent evaluation framework. This involves defining a set of relevant evaluation datasets and metrics. Use the same evaluation sets and metrics across all models for fair comparison. The evaluation datasets should be representative of the relevant application domain. An evaluation framework will help you iterate and refine the evaluation process over time, incorporating learnings from initial experiments to progressively improve the application.</p>&#13;
			<p>The following are useful evaluation metrics for using embedding models for information retrieval. The metrics are taken from <strong class="bold">Ragas</strong>, a framework for RAG evaluation:</p>&#13;
			<ul>&#13;
				<li><strong class="bold">Context precision</strong>: Evaluates whether the retrieved results contain ground-truth facts that you would want to answer the input query. Relevant items present in the contexts are ranked highly in the retrieved results.</li>&#13;
				<li><strong class="bold">Context entities recall</strong>: Evaluates what fraction of the entities from a set of ground truths are preset in the retrieved information.</li>&#13;
			</ul>&#13;
			<p>Ragas supports other RAG evaluation metrics as well, which you can learn more about in the Ragas documentation (https://docs.ragas.io/en/stable/).</p>&#13;
			<p>The following code example uses Ragas and LangChain to evaluate how different embedding models perform on the context entities recall metric.</p>&#13;
			<p>First, install the required dependencies in the terminal:</p>&#13;
			<pre class="console">&#13;
pip3 install ragas==0.1.13 datasets==2.20.0 langchain==0.2.12 openai==1.39.0 faiss-cpu==1.8.0.post1</pre>			<p>The following code evaluates how the OpenAI <code>text-embedding-ada-002</code> and <code>text-embedding-3-large</code> embedding models perform on the Ragas context entities recall evaluation for a sample dataset:</p>&#13;
			<pre class="console">&#13;
from ragas.metrics import context_entity_recall&#13;
from ragas import evaluate, RunConfig&#13;
from datasets import load_dataset, Dataset&#13;
from langchain_openai import ChatOpenAI, OpenAIEmbeddings&#13;
from langchain_text_splitters import RecursiveCharacterTextSplitter&#13;
from langchain_community.vectorstores import FAISS&#13;
import os&#13;
from typing import List&#13;
# Add your OpenAI API key to the environment variables&#13;
openai_api_key = os.getenv("OPENAI_API_KEY")&#13;
# Load sample dataset.&#13;
dataset = load_dataset("explodinggradients/amnesty_qa", split="eval")&#13;
sample_size = 100&#13;
# Get sample questions from the sample dataset.&#13;
sample_questions = dataset['question'][:sample_size]&#13;
# Get sample context information from the sample dataset.&#13;
sample_contexts = [item for row in dataset["contexts"]&#13;
                   [:sample_size] for item in row]&#13;
sample_ground_truths = [item for row in dataset["ground_truths"]&#13;
                   [:sample_size] for item in row]&#13;
# Break sample context into chunks to use with vector search.&#13;
text_splitter = RecursiveCharacterTextSplitter(&#13;
    chunk_size=400, chunk_overlap=100, add_start_index=True&#13;
)&#13;
chunks: List[str] = []&#13;
for context in sample_contexts:&#13;
    split_chunks = text_splitter.split_text(context)&#13;
    chunks.extend(split_chunks)&#13;
# Embedding models that we are evaluating.&#13;
openai_embedding_models = ["text-embedding-ada-002", "text-embedding-3-large"]&#13;
# Ragas evaluation config to use in all evaluations.&#13;
ragas_run_config = RunConfig(max_workers=4, max_wait=180)&#13;
# #Evaluate each embedding model&#13;
for embedding_model in openai_embedding_models:&#13;
    # Create an in-memory vector store for the evaluation.&#13;
    db = FAISS.from_texts(&#13;
        chunks, OpenAIEmbeddings(openai_api_key=openai_api_key, model=embedding_model))&#13;
    # Get retrieved context using similarity search.&#13;
    retrieval_contexts: List[str] = []&#13;
    for question in sample_questions:&#13;
        search_results = db.similarity_search(question)&#13;
        retrieval_contexts.append(list(map(&#13;
            lambda result: result.page_content, search_results)))&#13;
    # Run evaluation for context relevancy of retrieved information.&#13;
    result = evaluate(&#13;
        dataset=Dataset.from_dict({&#13;
            "question": sample_questions,&#13;
            "contexts": retrieval_contexts,&#13;
            "ground_truth": sample_ground_truths&#13;
        }),&#13;
        metrics=[context_entity_recall],&#13;
        run_config=ragas_run_config,&#13;
        raise_exceptions=False,&#13;
        llm=ChatOpenAI(openai_api_key=openai_api_key, model_name="gpt-4o-mini")&#13;
    )&#13;
    # Print out results&#13;
    print(f"Results for embedding model '{embedding_model}':")&#13;
    print(result)</pre>			<p>This code outputs results resembling the following to the terminal:</p>&#13;
			<pre class="console">&#13;
Results for embedding model 'text-embedding-ada-002': {'context_entity_recall': 0.5687}&#13;
Results for embedding model 'text-embedding-3-large': {'context_entity_recall': 0.5973}</pre>			<p>As you can see from these results, <code>text-embedding-3-large</code> yields higher context entity recall on this evaluation. The context relevancy score is normalized between <code>0</code> and <code>1</code>, inclusive.</p>&#13;
			<p>When creating evaluations for your own application, consider using sample data that’s relevant to your use case for a better comparison. Also, you will likely want to include a representative sample of at least 100 examples.</p>&#13;
			<h2 id="_idParaDest-177"><a id="_idTextAnchor219"/>Fine-tuning embedding models</h2>&#13;
			<p>In addition to experimenting with different pre-trained models, you can fine-tune a pre-trained embedding model to optimize it for your use case.</p>&#13;
			<p>Fine-tuning an embedding model can be beneficial in the following scenarios:</p>&#13;
			<ul>&#13;
				<li><strong class="bold">Domain-specific data</strong>: If the application deals with domain-specific data that might not be well captured using an off-the-shelf model, such as legal documents or medical records with specialized terminology, fine-tuning can help the model better understand and represent the domain-specific concepts.</li>&#13;
				<li><strong class="bold">Avoiding undesirable matches</strong>: In cases where there are seemingly similar concepts that should be differentiated, fine-tuning can help the model distinguish between them. For example, you could fine-tune the model to differentiate between <em class="italic">Apple the company</em> and <em class="italic">apple </em><em class="italic">the fruit</em>.</li>&#13;
			</ul>&#13;
			<p>However, off-the-shelf embedding models are often highly performant for many tasks, especially when combined with the metadata enrichment and RAG optimizations discussed later in this chapter.</p>&#13;
			<p>The available options for fine-tuning an embedding model can vary depending on the model and how it is hosted. Managed model hosting providers might only expose certain methods for their models, whereas using an open source model can provide more flexibility. The <strong class="bold">SentenceTransformers </strong>(<a href="https://sbert.net/">https://sbert.net/</a>) framework is designed for using and fine-tuning open-source embedding models.</p>&#13;
			<p>Generally, fine-tuning involves providing similar pairs of sentences, optionally including a magnitude of similarity. Alternatively, anchor, positive, and negative examples can be provided to guide the fine-tuning process. <em class="italic">Table 10.2</em> provides an overview of anchor, positive, and negative examples, that are used in the subsequent code example:</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table002-3">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<thead>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Type</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Definition</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Example</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
				</thead>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Anchor</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>The reference text that serves as the starting point for identifying similar and dissimilar examples.</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
"I love eating apples."</pre>						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Positive</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Text that should be represented as similar to the anchor example.</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
"Apples are my favorite fruit."</pre>						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Negative</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Text that should be represented as dissimilar or different from the anchor example.</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
"Apple is a tech company."</pre>						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.2: Methods for fine-tuning embedding models</p>&#13;
			<p>Here’s a brief code example of using the <code>SentenceTransformers</code> and <code>PyTorch</code> libraries to fine-tune the open source embedding model <code>gte-base-en-v1.5</code>.</p>&#13;
			<p>First, install the dependencies in the terminal:</p>&#13;
			<pre class="console">&#13;
pip3 install sentence-transformers==3.0.1 torch==2.2.2</pre>			<p>Then run the following code:</p>&#13;
			<pre class="console">&#13;
from sentence_transformers import SentenceTransformer, InputExample, losses, util&#13;
from torch.utils.data import DataLoader&#13;
# Load embedding model&#13;
model = SentenceTransformer("Alibaba-NLP/gte-base-en-v1.5", trust_remote_code=True)&#13;
# Function to print similarity score&#13;
def get_similarity_score():&#13;
    sentence1 = "I love the taste of fresh apples."&#13;
    sentence2 = "Apples are rich in vitamins and fiber."&#13;
    embedding1 = model.encode(sentence1)&#13;
    embedding2 = model.encode(sentence2)&#13;
    cosine_score = util.cos_sim(embedding1, embedding2)&#13;
    score_number = cosine_score.item()&#13;
    print(f"Cosine similarity between '{sentence1}' and '{sentence2}': {score_number:.4f}")&#13;
    return cosine_score&#13;
# Print similarity score before training&#13;
print("Before training:")&#13;
similarity_before = get_similarity_score()&#13;
train_examples = [&#13;
    InputExample(texts=["I love eating apples.", "Apples are my favorite fruit", "Apple is a tech company"]),&#13;
    InputExample(texts=["Chocolate is a sweet treat loved by many.", "I can't resist a good piece of chocolate.", "Chocolate Rain was one of the most popular songs on YouTube from 2007."]),&#13;
    InputExample(texts=["Ice cream is a refreshing dessert.", "I love trying different ice cream flavors.", "The rapper and actor Ice Cube was wearing a cream colored suit to the VMAs."]),&#13;
    InputExample(texts=["Salad is a healthy meal option.", "I love a fresh, crisp salad with various vegetables.", "Salad Fingers is a surreal web series created by David Firth."]),&#13;
]&#13;
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)&#13;
train_loss = losses.TripletLoss(model=model)&#13;
# fine tune&#13;
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=10)&#13;
print("After training:")&#13;
similarity_after = get_similarity_score()&#13;
similarity_difference = similarity_after - similarity_before&#13;
print(f"Change in similarity score: {similarity_difference.item():4f}")</pre>			<p>This code outputs a result resembling the following to the terminal:</p>&#13;
			<pre class="console">&#13;
Before training:&#13;
Cosine similarity between 'I love the taste of fresh apples.' and 'Apples are rich in vitamins and fiber.': 0.4402&#13;
[10/10 00:05, Epoch 10/10]&#13;
After training:&#13;
Cosine similarity between 'I love the taste of fresh apples.' and 'Apples are rich in vitamins and fiber.': 0.4407&#13;
Change in similarity score: 0.000540</pre>			<p>As you can see from this example, just the small fine-tuning that was performed, increased the vector similarity between the related sentences.</p>&#13;
			<p>If you would like to learn more about fine-tuning embedding models, a great place to start is the <em class="italic">Train and Fine-Tune Sentence Transformers Models Hugging Face</em> blog post by Omar Espejel (<a href="https://huggingface.co/blog/how-to-train-sentence-transformers">https://huggingface.co/blog/how-to-train-sentence-transformers</a>). This blog post includes a more detailed look at fine-tuning an embedding model using a similar approach to the one in the preceding code example.</p>&#13;
			<p>The following section discusses how you can further enhance the semantic data model after you have chosen the right data model by embedding relevant metadata in the text.</p>&#13;
			<h1 id="_idParaDest-178"><a id="_idTextAnchor220"/>Embedding metadata</h1>&#13;
			<p>Including <strong class="bold">metadata</strong> in embedded content can significantly improve the quality of retrieval results by adding greater semantic meaning to it. Metadata creates a richer and more meaningful semantic representation of content. Metadata can include descriptors such as the type of content, tags, titles, and summaries. The following table contains some useful examples of metadata to include in embedded content:</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table003-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<thead>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Type</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Example(s)</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
				</thead>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Content type</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Article, recipe, product review, etc.</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Tags</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>“dinner”, “Italian”, “vegetarian”</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Title of document</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Roasted Garlic and Tomato Pasta</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Summary of document</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce</p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.3: Useful types of embedded metadata</p>&#13;
			<p>You can also include metadata types that are specific to your application. For example, consider creating a RAG chatbot where users ask natural language questions and get generated answers on cooking and recipes.</p>&#13;
			<p>You have the following recipe for <em class="italic">Roast Garlic and Tomato Pasta</em> to include in your recipe database:</p>&#13;
			<pre class="source-code">&#13;
# Roasted Garlic and Tomato Pasta&#13;
## Ingredients&#13;
- 8 oz pasta&#13;
- 1 head garlic&#13;
- 1 pint cherry tomatoes&#13;
- 1/4 cup olive oil&#13;
- 1/2 cup fresh basil, chopped&#13;
- Salt and pepper&#13;
## Instructions&#13;
1. Preheat oven to 400°F (200°C).&#13;
2. Cut the top off the garlic head, drizzle with olive oil, wrap in foil, and roast for 30 minutes.&#13;
3. Roast cherry tomatoes with olive oil, salt, and pepper for 20 minutes until blistered.&#13;
4. Cook pasta according to package instructions. Mix pasta with roasted garlic (squeezed out), tomatoes, and olive oil.&#13;
5. Stir in basil, season with salt and pepper, and serve.&#13;
Yield: 4 servings</pre>			<p>When creating a vector embedding for the recipe, you could include the following metadata before the recipe text:</p>&#13;
			<pre class="source-code">&#13;
---&#13;
contentType: recipe&#13;
recipeTitle: Roasted Garlic and Tomato Pasta&#13;
keyIngredients: pasta, garlic, tomatoes, olive oil, basil&#13;
servings: 4&#13;
tags: [dinner, Italian, vegetarian]&#13;
summary: A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce&#13;
---&#13;
# Roasted Garlic and Tomato Pasta&#13;
## Ingredients&#13;
- 8 oz pasta&#13;
...other ingredients&#13;
## Instructions&#13;
1. Preheat your oven to 400°F (200°C).&#13;
   ...other instructions&#13;
Yield: 4 servings</pre>			<p>By including this metadata with the embedded text, you imbue the text with greater semantic meaning making it more likely that user queries will capture the correct content. This makes relevant user queries have greater cosine similarity scores with the text.</p>&#13;
			<p>The following table shows the cosine similarity scores between various queries and the text with and without metadata using the <code>BAAI/bge-large-en-v1.5</code> embedding model:</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table004-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<thead>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Query text</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Text without metadata </strong><strong class="bold">similarity score</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Text with metadata </strong><strong class="bold">similarity score</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Metadata similarity </strong><strong class="bold">score improvement</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
				</thead>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
I have tomatoes, basil and pasta in my fridge. What to make?</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.7141546</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.7306514</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.016496778</code></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
simple vegetarian pasta with roasted vegetables</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.71199816</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.76754296</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.055544794</code></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
vegetarian italian pasta dinner</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.60327804</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.6559261</code></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><code>0.052648067</code></p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.4: Comparing cosine similarity of vectors of text with and without embedded metadata</p>&#13;
			<p>As you can see in <em class="italic">Table 10.4</em>, the text with prepended metadata has a higher cosine similarity for a diverse set of relevant queries. This means that the relevant content is more likely to be surfaced and used in the RAG chatbot.</p>&#13;
			<h2 id="_idParaDest-179"><a id="_idTextAnchor221"/>Formatting metadata</h2>&#13;
			<p>When including metadata, you must consider how it is structured to optimize processing and interpretation. You should use a machine-readable format that is easy to parse and manipulate, such as YAML (<a href="https://yaml.org/spec/1.2.2/">https://yaml.org/spec/1.2.2/</a>), JSON (<a href="https://www.json.org/json-en.html">https://www.json.org/json-en.html</a>), or TOML (<a href="https://toml.io/">https://toml.io/</a>).</p>&#13;
			<p><strong class="bold">YAML</strong> is generally more token-efficient compared to other data formats such as <strong class="bold">JSON</strong>. This means that using YAML saves on the compute cost of processing extra tokens and also represents the same idea with fewer <em class="italic">distraction</em> tokens that could dilute the LLM’s ability to interpret the input and produce a high-quality output. YAML also has widespread adoption, so embedding models and LLMs can effectively work with it.</p>&#13;
			<p>The following table demonstrates the comparative token density for the same data represented in YAML and JSON using the GPT-4 tokenizer (<a href="https://platform.openai.com/tokenizer">https://platform.openai.com/tokenizer</a>):</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table005-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Format</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Content</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Token </strong><strong class="bold">count</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>YAML</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
contentType: recipe</pre>							<pre class="source-code">&#13;
recipeTitle: Roasted Garlic and Tomato Pasta</pre>							<pre class="source-code">&#13;
keyIngredients: pasta, garlic, tomatoes, olive oil, basil</pre>							<pre class="source-code">&#13;
servings: 4</pre>							<pre class="source-code">&#13;
tags: [dinner, Italian, vegetarian]</pre>							<pre class="source-code">&#13;
summary: A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>60</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>JSON</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
{</pre>							<pre class="source-code">&#13;
  " contentType": "recipe",</pre>							<pre class="source-code">&#13;
  " recipeTitle": "Roasted Garlic and Tomato Pasta",</pre>							<pre class="source-code">&#13;
  " keyIngredients": "pasta, garlic, tomatoes, olive oil, basil",</pre>							<pre class="source-code">&#13;
  " servings": 4,</pre>							<pre class="source-code">&#13;
  " tags": [</pre>							<pre class="source-code">&#13;
    "dinner",</pre>							<pre class="source-code">&#13;
    "Italian",</pre>							<pre class="source-code">&#13;
    "vegetarian"</pre>							<pre class="source-code">&#13;
  ],</pre>							<pre class="source-code">&#13;
  " summary": "A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce"</pre>							<pre class="source-code">&#13;
}</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>89</p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.5: Comparing token length of the same content in YAML and JSON</p>&#13;
			<p>As you can see in <em class="italic">Table 10.5</em>, YAML uses approximately two-thirds of tokens compared to JSON. The exact difference in token usage depends on the data and formatting. YAML generally proves to be a more efficient metadata format than JSON.</p>&#13;
			<p>If including metadata alongside additional text, consider including it as <em class="italic">front matter</em> (<a href="https://jekyllrb.com/docs/front-matter/">https://jekyllrb.com/docs/front-matter/</a>). <code>---</code> before and after the metadata.</p>&#13;
			<p>Here is an example of front matter preceding Markdown (<a href="https://commonmark.org/help/">https://commonmark.org/help/</a>) text:</p>&#13;
			<pre class="source-code">&#13;
---&#13;
foo: bar&#13;
letters:&#13;
  - a&#13;
  - b&#13;
  - c&#13;
---&#13;
# Title&#13;
Some **body** text!</pre>			<p>The front matter specification originates from the Jekyll static site builder (<a href="https://jekyllrb.com/docs/">https://jekyllrb.com/docs/</a>). It has since become widely adopted across various domains. Given its popularity, language models and embedding models should be able to understand its semantic context as metadata for the rest of the text. Additionally, libraries are available to easily manipulate front matter in relation to the main text content, such as the <code>python-frontmatter</code> in Python.</p>&#13;
			<p>The following code example shows how to add front matter to Markdown and print out the results.</p>&#13;
			<p>First, install the <code>python-frontmatter</code> package in the terminal:</p>&#13;
			<pre class="console">&#13;
pip3 install python-frontmatter==1.1.0</pre>			<p>Add front matter to text using the <code>python-frontmatter</code> library:</p>&#13;
			<pre class="source-code">&#13;
import frontmatter&#13;
# Define the text content&#13;
text = """# Roasted Garlic and Tomato Pasta&#13;
## Ingredients&#13;
- 8 oz pasta&#13;
- 1 head garlic&#13;
- 1 pint cherry tomatoes&#13;
- 1/4 cup olive oil&#13;
- 1/2 cup fresh basil, chopped&#13;
- Salt and pepper&#13;
## Instructions&#13;
1. Preheat oven to 400°F (200°C).&#13;
2. Cut the top off the garlic head, drizzle with olive oil, wrap in foil, and roast for 30 minutes.&#13;
3. Roast cherry tomatoes with olive oil, salt, and pepper for 20 minutes until blistered.&#13;
4. Cook pasta according to package instructions. Mix pasta with roasted garlic (squeezed out), tomatoes, and olive oil.&#13;
5. Stir in basil, season with salt and pepper, and serve.&#13;
Yield: 4 servings&#13;
"""&#13;
# Define the dictionary to be added as frontmatter&#13;
metadata = {&#13;
    "contentType": "recipe",&#13;
    "recipeTitle": "Roasted Garlic and Tomato Pasta",&#13;
    "keyIngredients": ["pasta", "garlic", "tomatoes", "olive oil", "basil"],&#13;
    "servings": 4,&#13;
    "tags": ["dinner", "Italian", "vegetarian"],&#13;
    "summary": "A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce"&#13;
}&#13;
# Create a frontmatter object with the metadata and content&#13;
post = frontmatter.Post(text, **metadata)&#13;
print("Text with front matter:")&#13;
print(frontmatter.dumps(post))&#13;
print("\n------\n")&#13;
print("You can also extract the front matter as a dict:")&#13;
print(post.metadata)</pre>			<p>This outputs the following text with front matter to the terminal:</p>&#13;
			<pre class="console">&#13;
---&#13;
contentType: recipe&#13;
keyIngredients: ["pasta", "garlic", "tomatoes", "olive oil", "basil"]recipeTitle: Roasted Garlic and Tomato Pasta&#13;
servings: 4&#13;
summary: A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce&#13;
tags: ["dinner", "Italian", "vegetarian"]&#13;
---&#13;
# Roasted Garlic and Tomato Pasta&#13;
## Ingredients&#13;
- 8 oz pasta&#13;
...other ingredients&#13;
## Instructions&#13;
1. Preheat your oven to 400°F (200°C).&#13;
...other instructions&#13;
Yield: 4 servings&#13;
------&#13;
You can also extract the front matter as a dict:&#13;
{'contentType': 'recipe', 'recipeTitle': 'Roasted Garlic and Tomato Pasta', 'keyIngredients': ['pasta', 'garlic', 'tomatoes', 'olive oil', 'basil'], 'servings': 4, 'tags': ['dinner', 'Italian', 'vegetarian'], 'summary': 'A simple pasta dish featuring roasted garlic and cherry tomatoes in a light sauce'}</pre>			<p>The preceding example demonstrates the usefulness of adding front matter as a metadata format in your semantic retrieval.</p>&#13;
			<h2 id="_idParaDest-180"><a id="_idTextAnchor222"/>Including static metadata</h2>&#13;
			<p>For certain types of content or sources, it can be beneficial to include static metadata that is the same across all documents. This is a computationally cheap and an easy way to consistently include metadata across documents.</p>&#13;
			<p>For a cookbook chatbot, you could include the cookbook source in the metadata. For example:</p>&#13;
			<pre class="source-code">&#13;
contentType: recipe&#13;
source: The MongoDB Cooking School Cookbook</pre>			<p>This ensures that every document of a particular type or from a specific source contains a consistent base level of metadata. You can then layer on additional dynamic metadata that is unique to each specific document, as discussed in the following sections. Including static metadata is a low-effort way to provide additional semantic context to your documents, aiding in retrieval and interpretation.</p>&#13;
			<h2 id="_idParaDest-181"><a id="_idTextAnchor223"/>Extracting metadata programmatically</h2>&#13;
			<p>You can extract metadata from content using traditional software development techniques that do not rely on AI models.</p>&#13;
			<p>One approach is to extract headers in a document, which can be done with <strong class="bold">regular expressions</strong> (<strong class="bold">regex</strong>) to match header patterns or by parsing the document’s <strong class="bold">abstract syntax tree</strong> (<strong class="bold">AST</strong>) to identify header elements. Extracting and including headings as metadata can be useful because headings frequently summarize or provide high-level information about the content in that section, thus aiding in understanding the semantic context and improving retrieval relevance.</p>&#13;
			<p>Extracting the headers from a Markdown document could create a document with metadata resembling the following:</p>&#13;
			<pre class="source-code">&#13;
---&#13;
headers:&#13;
  - text: Vegetable Stir-Fry&#13;
    level: h1&#13;
  - text: Ingredients&#13;
    level: h2&#13;
  - text: Vegetable Preparation&#13;
    level: h3&#13;
  - text: Instructions&#13;
    level: h2&#13;
  - text: Cooking the Stir-Fry&#13;
    level: h3&#13;
  - text: Serving&#13;
    level: h3&#13;
---&#13;
# Vegetable Stir-Fry&#13;
A quick and easy stir-fry with fresh veggies and a savory sauce.&#13;
## Ingredients&#13;
- 2 cups mixed vegetables (e.g., broccoli, carrots, bell peppers)...other ingredients&#13;
### Vegetable Preparation&#13;
- Wash and chop the vegetables into bite-sized pieces.&#13;
...other preparation&#13;
## Instructions&#13;
### Cooking the Stir-Fry&#13;
1. Heat the sesame oil in a large skillet or wok over high heat.&#13;
...other instructions&#13;
### Serving&#13;
- Serve hot over steamed rice or noodles.&#13;
...other instructions&#13;
Serves 4</pre>			<h2 id="_idParaDest-182"><a id="_idTextAnchor224"/>Generating metadata with LLMs</h2>&#13;
			<p>You can use LLMs to generate metadata for your content. Some potential use cases for using LLMs to generate metadata include:</p>&#13;
			<ul>&#13;
				<li>Summarizing the text</li>&#13;
				<li>Extracting key phrases or terms from the text</li>&#13;
				<li>Classifying the text into categories</li>&#13;
				<li>Identifying the sentiment of the text</li>&#13;
				<li>Recognizing named entities</li>&#13;
			</ul>&#13;
			<p>When selecting an LLM for metadata generation, you may be able to use smaller (and therefore faster and cheaper) language models compared to those used for other components of your intelligent application.</p>&#13;
			<p>You can also use traditional <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) techniques to provide additional metadata. For example, <strong class="bold">calculating n-grams</strong> can surface the most frequently occurring terms or phrases in the text. Other NLP approaches such as <strong class="bold">part-of-speech tagging</strong> and <strong class="bold">keyword tagging</strong> can also provide useful metadata. These approaches typically use small AI models.</p>&#13;
			<p>You can use the Python NLP libraries, such as <code>NLTK</code> or <code>spaCy</code>, to extract metadata. While using these libraries is generally more compute efficient than using an LLM, they generally require fine-tuning, so it’s not worthwhile to use them unless your application is running at a scale where the compute requirements of an LLM are cost or resource prohibitive.</p>&#13;
			<p>The following code uses the OpenAI GPT-4o mini LLM to extract the metadata. It also uses Pydantic to format the response as JSON.</p>&#13;
			<p>First, install the dependencies in the terminal:</p>&#13;
			<pre class="console">&#13;
pip3 install openai==1.39.0 pydantic==2.8.2</pre>			<p>Then, execute the code:</p>&#13;
			<pre class="console">&#13;
import os&#13;
from openai import OpenAI&#13;
from pydantic import BaseModel&#13;
import json&#13;
# Create client to call model&#13;
api_key = os.environ["OPENAI_API_KEY"]&#13;
client = OpenAI(&#13;
    api_key=api_key,&#13;
)&#13;
# Format response structure&#13;
class TopicsResult(BaseModel):&#13;
    topics: list[str]&#13;
function_definition = {&#13;
    "name": "get_topics",&#13;
    "description": "Extract the key topics from the text",&#13;
    "parameters": json.loads(TopicsResult.schema_json())&#13;
}&#13;
response = client.chat.completions.create(&#13;
    model="gpt-4o-mini",&#13;
    functions=[function_definition],&#13;
    function_call={ "name": function_definition["name"] },&#13;
    messages=[&#13;
        {&#13;
            "role": "system",&#13;
            "content": "Extract key topics from the following text. Include no more than 3 key terms. Format response as a JSON object.",&#13;
        },&#13;
        {&#13;
            "role": "user",&#13;
            "content": "Eggs, like milk, form a typical food, inasmuch as they contain all the elements, in the right proportion, necessary for the support of the body. Their highly concentrated, nutritive value renders it necessary to use them in combination with other foods rich in starch (bread, potatoes, etc.). In order that the stomach may have enough to act upon, a certain amount of bulk must be furnished."&#13;
        }&#13;
    ],&#13;
)&#13;
# Get model results as a dict&#13;
content = TopicsResult.model_validate(json.loads(response.choices[0].message.function_call.arguments))&#13;
print(f"Topics: {content.topics}")</pre>			<p>This code produces an output resembling the following to the terminal:</p>&#13;
			<pre class="console">&#13;
Topics: ['eggs', 'milk', 'nutritive value']</pre>			<p>As you saw here, LLMs allow you to perform many forms of NLP tasks with prompt engineering and minimal technical overhead.</p>&#13;
			<h2 id="_idParaDest-183"><a id="_idTextAnchor225"/>Including metadata with query embedding and ingested content embeddings</h2>&#13;
			<p>In addition to including metadata with the content that you ingest into a vector store, you can also include metadata along with the content that you use in your search query. By structuring the metadata similarly on both the query and the retrieved content, you increase the likelihood of a relevant match using vector similarity search.</p>&#13;
			<p>You can extract metadata for the query using the same strategies as those for extracting metadata from the data sources as discussed previously in this chapter.</p>&#13;
			<p>For example, say you’re querying the cookbook chatbot mentioned previously. Given the user query <code>apple pie recipe</code>, you might want to use the following query for vector search:</p>&#13;
			<pre class="source-code">&#13;
---&#13;
contentType: recipe&#13;
keyIngredients: apples, sugar, butter&#13;
tags: [dessert, pie]&#13;
---&#13;
apple pie recipe</pre>			<p>A query such as the above will make it more likely to match a recipe with similarly structured embedded metadata like the following:</p>&#13;
			<pre class="source-code">&#13;
---&#13;
contentType: recipe&#13;
recipeTitle: Classic Apple Pie&#13;
keyIngredients: apples, pie crust, sugar, cinnamon, butter&#13;
servings: 8&#13;
tags: [dessert, baking, American, fruit]&#13;
summary: A classic apple pie with a flaky crust and a sweet, cinnamon-spiced apple filling.&#13;
---&#13;
# Classic Apple Pie&#13;
## Ingredients&#13;
- 1 premade pie crust&#13;
- 1 can apple pie filling&#13;
- 1 teaspoon ground cinnamon&#13;
- 1 egg, beaten (for egg wash)&#13;
## Instructions&#13;
1. Preheat oven to 425°F (220°C). Place the premade crust in a 9-inch pie plate.&#13;
2. Pour the apple pie filling into the crust and sprinkle with cinnamon.&#13;
3. Cover with the top crust, trim and crimp edges, and cut slits for steam. Brush with egg wash.&#13;
4. Bake for 15 minutes, reduce temperature to 350°F (175°C), and bake for another 30-35 minutes until golden brown. Cool before serving.&#13;
Yield: 8 servings</pre>			<p>Including structured metadata in the query can act as a kind of <em class="italic">semantic filter</em> to get more accurate search results. The following section examines other techniques for improving the accuracy of the data model in RAG applications.</p>&#13;
			<h1 id="_idParaDest-184"><a id="_idTextAnchor226"/>Optimizing retrieval-augmented generation</h1>&#13;
			<p>Beyond optimizing the semantic data model itself through vector embedding model choice and metadata enrichment, there are ways to further refine and improve RAG applications. This section covers strategies for optimizing different components and stages of the RAG pipeline.</p>&#13;
			<p>Key areas of optimization include query handling, formatting of ingested data, retrieval system configuration, and application-level guardrails. Effectively optimizing these aspects can lead to significant boosts in the accuracy, relevance, and overall performance of RAG applications.</p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">This section covers more advanced techniques than the ones discussed in <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in </em><em class="italic">AI Applications.</em></p>&#13;
			<h2 id="_idParaDest-185"><a id="_idTextAnchor227"/>Query mutation</h2>&#13;
			<p>In the naive RAG approach, you use direct user input to create the embedding used in vector search, perhaps augmented with metadata as discussed earlier in the chapter. However, you can drive better search performance by mutating the user input using an LLM.</p>&#13;
			<p>Several popular techniques for query mutation include:</p>&#13;
			<ul>&#13;
				<li><code>My daughter is allergic to nuts. My son is allergic to dairy. What is a vegetarian dinner I can make for them?</code> the LLM-generated step-back search query could be <em class="italic">Vegetarian dinner recipe without dairy </em><em class="italic">or nuts</em>.</p></li>&#13;
				<li><code>sirloin steak recipe</code>, the LLM-generated HyDE search query could be <em class="italic">Preheat your grill or grill pan to high heat. Pat the sirloin steaks dry and season generously with salt and pepper. Drizzle with olive oil and use your hands to coat the steaks evenly. Place the steaks on the hot grill and cook for 4-5 minutes per side for medium-rare, flipping only once. Use an instant-read thermometer to check for doneness (135°F for medium-rare). Transfer the steaks to a cutting board and let rest for 5 minutes before slicing against the grain. Serve the juicy sirloin steaks with your favorite sides like roasted potatoes, grilled vegetables, or a </em><em class="italic">fresh salad.</em></p></li>&#13;
				<li><code>vegan dinner party menu</code>, the LLM-generated multiple search queries could be <em class="italic">Vegan appetizer</em>, <em class="italic">Vegan dinner main course</em>, and <em class="italic">Vegan desert</em>.</p></li>&#13;
			</ul>&#13;
			<p>All of these techniques can be optimized for your application’s domain. You can even combine them or have an LLM select what is the most appropriate technique for a given user query.</p>&#13;
			<p>However, introducing another point of AI in the application also presents challenges. The query mutation may not always work as expected, potentially degrading performance in some cases. Additionally, it introduces another component to evaluate and incurs the cost of additional AI usage. Any LLM-query mutations should be thoroughly evaluated to mitigate unexpected outcomes.</p>&#13;
			<h2 id="_idParaDest-186"><a id="_idTextAnchor228"/>Extracting query metadata for pre-filtering</h2>&#13;
			<p>In addition to performing semantic filtering as discussed in the <em class="italic">Embedding metadata</em> section, you can also programmatically filter on metadata before performing vector search. This lets you reduce the number of embeddings that you are searching over to only examine the subset of total embeddings relevant to a given query.</p>&#13;
			<p>It is important to select a vector database that contains metadata filtering capabilities suitable to your application needs. Metadata filtering capabilities vary greatly by vector database. For example, MongoDB Atlas Vector Search supports a variety of pre-filter options in the <code>$vectorSearch</code> aggregation pipeline stage. (<a href="https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter">https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-stage/#atlas-vector-search-pre-filter</a>). In <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in AI Applications</em>, you learned how to set up these pre-filter options with Atlas Vector Search Index.</p>&#13;
			<p>You can use an LLM to extract metadata from a query to use as a filter, like how you extract metadata from ingested content, as discussed in the <em class="italic">Embedding metadata</em> section. Alternatively, you could use heuristics to determine filter criteria.</p>&#13;
			<p>For instance, say you are building a cooking chatbot that performs RAG over a vector database of recipes and general cooking information such as the popular spices in certain cuisines. You could add a metadata filter that only looks at the recipe items in the vector database if a user query contains the word <code>recipe</code>. You can also create so-called <em class="italic">smart</em> filters that use AI models such as LLMs to determine which subsets of the data to include.</p>&#13;
			<p>Here is a code example of an LLM function that determines what, if any, filter to apply to a search query. It also uses Pydantic to format the response as JSON.</p>&#13;
			<p>The following Python code extracts the topic from a query using the OpenAI LLM GPT-4o mini. It also uses Pydantic to format the response as JSON. You can then use the extracted topic as a pre-filter, as described in <a href="B22495_08.xhtml#_idTextAnchor180"><em class="italic">Chapter 8</em></a>, <em class="italic">Implementing Vector Search in </em><em class="italic">AI Applications</em>.</p>&#13;
			<p>First, install the required dependencies in your terminal:</p>&#13;
			<pre class="console">&#13;
pip3 install openai==1.39.0 pydantic==2.8.2</pre>			<p>Then, run the following code:</p>&#13;
			<pre class="source-code">&#13;
from openai import OpenAI&#13;
from pydantic import BaseModel&#13;
import json&#13;
from typing import Literal, Optional&#13;
# Create client to call model&#13;
api_key = os.environ["OPENAI_API_KEY"]&#13;
client = OpenAI(&#13;
    api_key=api_key,&#13;
)&#13;
# Create classifier&#13;
class ContentTopic(BaseModel):&#13;
    topic: Optional[Literal[&#13;
        "nutritional_information",&#13;
        "equipment",&#13;
        "cooking_technique",&#13;
        "recipe"&#13;
    ]]&#13;
function_definition = {&#13;
    "name": "classify_topic",&#13;
    "description": "Extract the key topics from the query",&#13;
    "parameters": json.loads(ContentTopic.schema_json())&#13;
}&#13;
# The topic classifier uses few-shot examples to optimize the classification task.&#13;
def get_topic(query: str):&#13;
    response = client.chat.completions.create(&#13;
    model="gpt-4o-mini",&#13;
    functions=[function_definition],&#13;
    function_call={ "name": function_definition["name"] },&#13;
    temperature=0,&#13;
    messages=[&#13;
        {&#13;
            "role": "system",&#13;
            "content": """Extract the topic of the following user query about cooking.&#13;
Only use the topics present in the content topic classifier function.&#13;
If you cannot tell the query topic or it is not about cooking, respond `null`. Output JSON.&#13;
You MUST choose one of the given content topic types.&#13;
Example 1:&#13;
User:  "How many grams of sugar are in a banana?"&#13;
Assistant: '{"topic": "nutritional_information"}'&#13;
Example 2:&#13;
User: "What are the ingredients for a classic margarita?"&#13;
Assistant: '{"topic": "recipe"}'&#13;
Example 3:&#13;
User: "What kind of knife is best for chopping vegetables?"&#13;
Assistant: '{"topic": "equipment"}'&#13;
Example 4:&#13;
User: "What is a quick recipe for chicken stir-fry?"&#13;
Assistant: '{"topic": "recipe"}'&#13;
Example 5:&#13;
User: Who is the best soccer player ever?&#13;
Assistant: '{"topic": null}'&#13;
Example 6:&#13;
User: Explain gravity to me&#13;
Assistant: '{"topic": null}'""",&#13;
          },&#13;
          {&#13;
              "role": "user",&#13;
              "content": query&#13;
          }&#13;
      ],&#13;
    )&#13;
    content = ContentTopic.model_validate(json.loads(response.choices[0].message.function_call.arguments))&#13;
    return content.topic&#13;
## Test the classifier&#13;
queries = [&#13;
    "what's a recipe for vegetarian spaghetti?",&#13;
    "what is the best way to poach an egg?",&#13;
    "What blender setting should I use to make a fruit smoothie?",&#13;
    "Can you give me a recipe for chocolate chip cookies?",&#13;
    "Why is the sky blue?"&#13;
]&#13;
for query in queries:&#13;
    print(f"Query: {query}")&#13;
    print(f"Topic: {get_topic(query)}")&#13;
    print("---")&#13;
    class ContentTopic(BaseModel):&#13;
    topic: Optional[Literal[&#13;
        "nutritional_information",&#13;
        "equipment",&#13;
        "cooking_technique",&#13;
        "recipe"&#13;
    ]]</pre>			<p>This outputs the following to your terminal:</p>&#13;
			<pre class="console">&#13;
Query: what's a recipe for vegetarian spaghetti?&#13;
Topic: recipe&#13;
---&#13;
Query: what is the best way to poach an egg?&#13;
Topic: cooking_technique&#13;
---&#13;
Query: What blender setting should I use to make a fruit smoothie?&#13;
Topic: equipment&#13;
---&#13;
Query: Can you give me a recipe for chocolate chip cookies?&#13;
Topic: recipe&#13;
---&#13;
Query: Why is the sky blue?&#13;
Topic: None&#13;
---</pre>			<p>By combining metadata filtering with vector search, your RAG application can search more efficiently and accurately. This approach narrows down the search space to the most contextually appropriate data, leading to more precise and useful results.</p>&#13;
			<h2 id="_idParaDest-187"><a id="_idTextAnchor229"/>Formatting ingested data</h2>&#13;
			<p>When ingesting data to create embeddings, you must consider the format that the data is in. Standardizing the data format as much as possible can lead to more consistent results.</p>&#13;
			<p>For longer-form text data, such as technical documentation or reports, you should format the ingested and embedded data in a consistent format that includes appropriate semantic meaning in a token-dense format. Markdown is a good choice because it has high information density per token compared to XML-based formats such as HTML or PDFs.</p>&#13;
			<p>For instance, see the total GPT-4 tokenizer token count for the following content represented in plain text, Markdown, and HTML:</p>&#13;
			<table class="No-Table-Style _idGenTablePara-1" id="table006-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Format</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Content</strong></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Token </strong><strong class="bold">count</strong></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Plain Text</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
Simple Vegan Soup</pre>							<pre class="source-code">&#13;
Ingredients</pre>							<pre class="source-code">&#13;
- 1 can diced tomatoes</pre>							<pre class="source-code">&#13;
- 1 cup vegetable broth</pre>							<pre class="source-code">&#13;
- 1 cup mixed frozen vegetables</pre>							<pre class="source-code">&#13;
 Instructions</pre>							<pre class="source-code">&#13;
1. In a medium pot, combine the diced tomatoes, vegetable broth, and mixed frozen vegetables.</pre>							<pre class="source-code">&#13;
2. Bring to a boil, then reduce heat and simmer for 10-15 minutes, or until the vegetables are heated through. Serve hot.</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>81</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>Markdown</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
# Simple Vegan Soup</pre>							<pre class="source-code">&#13;
## Ingredients</pre>							<pre class="source-code">&#13;
- 1 can diced tomatoes</pre>							<pre class="source-code">&#13;
- 1 cup vegetable broth</pre>							<pre class="source-code">&#13;
- 1 cup mixed frozen vegetables</pre>							<pre class="source-code">&#13;
## Instructions</pre>							<pre class="source-code">&#13;
1. In a medium pot, combine the diced tomatoes, vegetable broth, and mixed frozen vegetables.</pre>							<pre class="source-code">&#13;
2. Bring to a boil, then reduce heat and simmer for 10-15 minutes, or until the vegetables are heated through. Serve hot.</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>83</p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p>HTML</p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<pre class="source-code">&#13;
&lt;h1 id="simple-vegan-soup"&gt;Simple Vegan Soup&lt;/h1&gt;</pre>							<pre class="source-code">&#13;
&lt;h2 id="ingredients"&gt;Ingredients&lt;/h2&gt;</pre>							<pre class="source-code">&#13;
&lt;ul&gt;</pre>							<pre class="source-code">&#13;
&lt;li&gt;1 can diced tomatoes&lt;/li&gt;</pre>							<pre class="source-code">&#13;
&lt;li&gt;1 cup vegetable broth&lt;/li&gt;</pre>							<pre class="source-code">&#13;
&lt;li&gt;1 cup mixed frozen vegetables&lt;/li&gt;</pre>							<pre class="source-code">&#13;
&lt;/ul&gt;</pre>							<pre class="source-code">&#13;
&lt;h2 id="instructions"&gt;Instructions&lt;/h2&gt;</pre>							<pre class="source-code">&#13;
&lt;ol&gt;</pre>							<pre class="source-code">&#13;
&lt;li&gt;In a medium pot, combine the diced tomatoes, vegetable broth, and mixed frozen vegetables.&lt;/li&gt;</pre>							<pre class="source-code">&#13;
&lt;li&gt;Bring to a boil, then reduce heat and simmer for 10-15 minutes, or until the vegetables are heated through. Serve hot.&lt;/li&gt;</pre>							<pre class="source-code">&#13;
&lt;/ol&gt;</pre>						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>138</p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.6: Token count of different text formats</p>&#13;
			<p>How you format ingested data can have a meaningful impact on retrieval quality and resource consumption. Generally, plain text or Markdown are effective formats for most text-based use cases.</p>&#13;
			<h2 id="_idParaDest-188"><a id="_idTextAnchor230"/>Advanced retrieval systems</h2>&#13;
			<p>A variety of advanced retrieval systems have emerged that go beyond simply retrieving the nearest match to the query.</p>&#13;
			<p>All of the following retrieval architectures are experimental as of writing in August 2024. When developing your intelligent application, you should probably start with standard vector search retrieval. Optimize standard vector search retrieval before using techniques such as filtering and adding semantic metadata before you experiment with these advanced retrieval systems.</p>&#13;
			<p>Advanced retrieval systems include:</p>&#13;
			<ul>&#13;
				<li><strong class="bold">Summary retrieval</strong>: Extract a summary from each document and store that summary in the vector search index. Retrieve the content of the whole document when the embedded version of the summary is matched.</li>&#13;
				<li><strong class="bold">Knowledge graph retrieval</strong>: During data ingestion, create a knowledge graph of relations between documents in the vector store. These relationships can be created using an LLM. During retrieval, perform an initial semantic search.</li>&#13;
				<li><strong class="bold">Router retrieval</strong>: Use a classifier to determine where a user query should be routed to between different data stores.</li>&#13;
			</ul>&#13;
			<p>LlamaIndex has done an excellent job of staying on top of the latest research in advanced retrieval systems. To learn more about the various advanced retrieval patterns that LlamaIndex supports, refer to the LlamaIndex Query Engine documentation (<a href="https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/">https://docs.llamaindex.ai/en/stable/examples/query_engine/knowledge_graph_rag_query_engine/</a>).</p>&#13;
			<h1 id="_idParaDest-189"><a id="_idTextAnchor231"/>Summary</h1>&#13;
			<p>In this chapter, you explored various techniques for refining your semantic data model to improve retrieval accuracy for vector search and RAG. You learned how to improve your data model used in information retrieval and RAG. By fine-tuning embeddings, you can adjust pre-trained models to improve the accuracy and relevance of search results. With embedded metadata, you can improve the vector search quality. Finally, RAG optimization ensures that the retrieval process fetches the most relevant information.</p>&#13;
			<p>In the next chapter, you will examine ways to address common issues in AI application development.</p>&#13;
		</div>&#13;
	</body></html>