<html><head></head><body>
<div id="_idContainer346" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-117"><a id="_idTextAnchor209" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-118" class="calibre4"><a id="_idTextAnchor210" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Text Classification Reimagined: Delving Deep into Deep Learning Language Models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">In this chapter, we delve</span><a id="_idIndexMarker541" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.4.1"> into the realm of </span><strong class="bold"><span class="kobospan" id="kobo.5.1">deep learning</span></strong><span class="kobospan" id="kobo.6.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.7.1">DL</span></strong><span class="kobospan" id="kobo.8.1">) and its application in </span><strong class="bold"><span class="kobospan" id="kobo.9.1">natural language processing</span></strong><span class="kobospan" id="kobo.10.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.11.1">NLP</span></strong><span class="kobospan" id="kobo.12.1">), specifically focusing</span><a id="_idIndexMarker542" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.13.1"> on the groundbreaking</span><a id="_idIndexMarker543" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.14.1"> transformer-based models such as </span><strong class="bold"><span class="kobospan" id="kobo.15.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="kobospan" id="kobo.16.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.17.1">BERT</span></strong><span class="kobospan" id="kobo.18.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.19.1">generative pretrained transformer</span></strong><span class="kobospan" id="kobo.20.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.21.1">GPT</span></strong><span class="kobospan" id="kobo.22.1">). </span><span class="kobospan" id="kobo.22.2">We begin by introducing</span><a id="_idIndexMarker544" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.23.1"> the fundamentals of DL, elucidating its powerful capability to learn intricate patterns from large amounts of data, making it the cornerstone of state-of-the-art </span><span><span class="kobospan" id="kobo.24.1">NLP systems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.25.1">Following this, we delve into transformers, a novel architecture that has revolutionized NLP by offering a more effective method</span><a id="_idIndexMarker545" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.26.1"> of handling sequence data compared to traditional </span><strong class="bold"><span class="kobospan" id="kobo.27.1">recurrent neural networks</span></strong><span class="kobospan" id="kobo.28.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.29.1">RNNs</span></strong><span class="kobospan" id="kobo.30.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.31.1">convolutional neural networks</span></strong><span class="kobospan" id="kobo.32.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.33.1">CNNs</span></strong><span class="kobospan" id="kobo.34.1">). </span><span class="kobospan" id="kobo.34.2">We unpack the transformer’s unique</span><a id="_idIndexMarker546" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.35.1"> characteristics, including its attention mechanisms, which allow it to focus on different parts of the input sequence to better understand </span><span><span class="kobospan" id="kobo.36.1">the context.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.37.1">Then, we turn our attention to BERT and GPT, transformer-based language models that leverage these strengths to create highly nuanced language representations. </span><span class="kobospan" id="kobo.37.2">We provide a detailed breakdown of the BERT architecture, discussing its innovative use of bidirectional training to generate contextually rich word embeddings. </span><span class="kobospan" id="kobo.37.3">We will demystify the inner workings of BERT and explore its pretraining process, which leverages a large corpus of text to learn </span><span><span class="kobospan" id="kobo.38.1">language semantics.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.39.1">Finally, we discuss how BERT can be fine-tuned for specific tasks, such as text classification. </span><span class="kobospan" id="kobo.39.2">We walk you through the steps, from data preprocessing and model configuration to training and evaluation, providing a hands-on understanding of how to leverage BERT’s power for </span><span><span class="kobospan" id="kobo.40.1">text classification.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.41.1">This chapter provides a thorough exploration of DL in NLP, moving from foundational concepts to practical applications, equipping you with the knowledge to harness the capabilities of BERT and transformer models for your text </span><span><span class="kobospan" id="kobo.42.1">classification tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.43.1">The following topics are covered in </span><span><span class="kobospan" id="kobo.44.1">this cha</span><a id="_idTextAnchor211" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.45.1">pter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.46.1">Understanding deep </span><span><span class="kobospan" id="kobo.47.1">learning b</span><a id="_idTextAnchor212" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.48.1">asics</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.49.1">The architecture of different </span><span><span class="kobospan" id="kobo.50.1">neural networks</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.51.1">Transformers</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.52.1">Language models</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.53.1">The challenges of training </span><span><span class="kobospan" id="kobo.54.1">neural networks</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.55.1">BERT</span></span></li>
<li class="calibre15"><span><span class="kobospan" id="kobo.56.1">GPT</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.57.1">How to use language models </span><span><span class="kobospan" id="kobo.58.1">for classification</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.59.1">NLP-ML system </span><span><span class="kobospan" id="kobo.60.1">design ex</span><a id="_idTextAnchor213" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.61.1">ample</span></span></li>
</ul>
<h1 id="_idParaDest-119" class="calibre4"><a id="_idTextAnchor214" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.62.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.63.1">To successfully navigate through this chapter, certain technical prerequisites are necessary, </span><span><span class="kobospan" id="kobo.64.1">as foll</span><a id="_idTextAnchor215" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.65.1">ows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.66.1">Programming knowledge</span></strong><span class="kobospan" id="kobo.67.1">: A strong understanding of Python is essential, as it’s the primary language used for most DL and </span><span><span class="kobospan" id="kobo.68.1">NLP librar</span><a id="_idTextAnchor216" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.69.1">ies.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.70.1">Machine learning fundamentals</span></strong><span class="kobospan" id="kobo.71.1">: A good grasp of basic ML concepts such as training/testing data, overfitting, underfitting, accuracy, precision, recall, and F1 score will </span><span><span class="kobospan" id="kobo.72.1">be valu</span><a id="_idTextAnchor217" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.73.1">able.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.74.1">DL basics</span></strong><span class="kobospan" id="kobo.75.1">: Familiarity with </span><strong class="bold"><span class="kobospan" id="kobo.76.1">DL</span></strong><span class="kobospan" id="kobo.77.1"> concepts and architectures, including neural networks, backpropagation, activation functions, and loss functions, will be essential. </span><span class="kobospan" id="kobo.77.2">Knowledge of RNNs and CNNs would be advantageous but not strictly necessary as we will focus more on </span><span><span class="kobospan" id="kobo.78.1">transformer architect</span><a id="_idTextAnchor218" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.79.1">ures.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.80.1">NLP basics</span></strong><span class="kobospan" id="kobo.81.1">: Some understanding of basic NLP concepts such as tokenization, stemming, lemmatization, and word embeddings (such as </span><strong class="bold"><span class="kobospan" id="kobo.82.1">Word2Vec </span></strong><span class="kobospan" id="kobo.83.1">or </span><strong class="bold"><span class="kobospan" id="kobo.84.1">GloVe</span></strong><span class="kobospan" id="kobo.85.1">) would </span><span><span class="kobospan" id="kobo.86.1">be benefi</span><a id="_idTextAnchor219" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.87.1">cial.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.88.1">Libraries and frameworks</span></strong><span class="kobospan" id="kobo.89.1">: Experience with libraries such as </span><strong class="bold"><span class="kobospan" id="kobo.90.1">TensorFlow </span></strong><span class="kobospan" id="kobo.91.1">and </span><strong class="bold"><span class="kobospan" id="kobo.92.1">PyTorch</span></strong><span class="kobospan" id="kobo.93.1"> for building and training neural models is crucial. </span><span class="kobospan" id="kobo.93.2">Familiarity with NLP libraries such as </span><strong class="bold"><span class="kobospan" id="kobo.94.1">NLTK </span></strong><span class="kobospan" id="kobo.95.1">or </span><strong class="bold"><span class="kobospan" id="kobo.96.1">SpaCy</span></strong><span class="kobospan" id="kobo.97.1"> can also be beneficial. </span><span class="kobospan" id="kobo.97.2">For working with BERT specifically, knowledge of the </span><strong class="source-inline1"><span class="kobospan" id="kobo.98.1">transformers</span></strong><span class="kobospan" id="kobo.99.1"> library from </span><strong class="bold"><span class="kobospan" id="kobo.100.1">Hugging Face</span></strong><span class="kobospan" id="kobo.101.1"> would be </span><span><span class="kobospan" id="kobo.102.1">very hel</span><a id="_idTextAnchor220" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.103.1">pful.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.104.1">Hardware requirements</span></strong><span class="kobospan" id="kobo.105.1">: DL models, especially transformer-based models such as BERT, are computationally intensive and typically require a modern </span><strong class="bold"><span class="kobospan" id="kobo.106.1">graphics processing unit</span></strong><span class="kobospan" id="kobo.107.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.108.1">GPU)</span></strong><span class="kobospan" id="kobo.109.1"> to train in a reasonable amount of time. </span><span class="kobospan" id="kobo.109.2">Access to a high-performance computer or cloud-based solutions with GPU capabilities is </span><span><span class="kobospan" id="kobo.110.1">highly recomme</span><a id="_idTextAnchor221" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.111.1">nded.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.112.1">Mathematics</span></strong><span class="kobospan" id="kobo.113.1">: A good understanding of linear algebra, calculus, and probability is helpful for understanding the inner workings of these models, but most of the chapter can be understood without in-depth </span><span><span class="kobospan" id="kobo.114.1">mathematical knowledge.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.115.1">These prerequisites are intended to equip you with the necessary background to understand and implement the concepts discussed in the chapter. </span><span class="kobospan" id="kobo.115.2">With these in place, you should be well-prepared to delve into the fascinating world of DL for text classification </span><span><span class="kobospan" id="kobo.116.1">using </span><a id="_idTextAnchor222" class="calibre5 pcalibre1 pcalibre"/></span><span><strong class="bold"><span class="kobospan" id="kobo.117.1">BERT</span></strong></span><span><span class="kobospan" id="kobo.118.1">.</span></span></p>
<h1 id="_idParaDest-120" class="calibre4"><a id="_idTextAnchor223" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.119.1">Understanding deep learning basics</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.120.1">In this part, we explain what</span><a id="_idIndexMarker547" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.121.1"> neural network and deep neural networks are, what is the motivation for using them, and the different types (architectures) of deep </span><span><span class="kobospan" id="kobo.122.1">learning mo</span><a id="_idTextAnchor224" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.123.1">dels.</span></span></p>
<h2 id="_idParaDest-121" class="calibre7"><a id="_idTextAnchor225" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.124.1">What is a neural network?</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.125.1">Neural networks</span><a id="_idIndexMarker548" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.126.1"> are a subfield of </span><strong class="bold"><span class="kobospan" id="kobo.127.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.128.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.129.1">AI</span></strong><span class="kobospan" id="kobo.130.1">) and ML that focuses on algorithms</span><a id="_idIndexMarker549" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.131.1"> inspired by the structure and function of the brain. </span><span class="kobospan" id="kobo.131.2">It is also known as “deep” learning because these neural networks often consist of many repetitive layers, creating a </span><span><span class="kobospan" id="kobo.132.1">deep architecture.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.133.1">These DL models</span><a id="_idIndexMarker550" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.134.1"> are capable of “learning” from large volumes of complex, high-dimensional, and unstructured data. </span><span class="kobospan" id="kobo.134.2">The term “learning” refers to the ability of the model to automatically learn and improve from experience without being explicitly programmed to do so for any one particular task of the tasks </span><span><span class="kobospan" id="kobo.135.1">it learns.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.136.1">DL can be supervised, semi-supervised, or unsupervised. </span><span class="kobospan" id="kobo.136.2">It’s used in numerous applications, including NLP, speech recognition, image recognition, and even playing games. </span><span class="kobospan" id="kobo.136.3">The models can identify patterns and make data-driven predictions </span><span><span class="kobospan" id="kobo.137.1">or decisions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.138.1">One of the critical advantages</span><a id="_idIndexMarker551" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.139.1"> of DL is its ability to process and model data of various types, including text, images, sound, and more. </span><span class="kobospan" id="kobo.139.2">This versatility has led to a vast range of applications, from self-driving cars to sophisticated web search algorithms and highly responsive speech </span><span><span class="kobospan" id="kobo.140.1">recognition systems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.141.1">It’s worth noting that DL, despite its high potential, also requires significant computational power and large amounts of high-quality data to train effectively, which can be </span><span><span class="kobospan" id="kobo.142.1">a challenge.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.143.1">In essence, DL is a powerful</span><a id="_idIndexMarker552" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.144.1"> and transformative technology that is at the forefront of many of today’s </span><span><span class="kobospan" id="kobo.145.1">technological ad</span><a id="_idTextAnchor226" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.146.1">vancements.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.147.1">The motivation for using neural networks</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.148.1">Neural networks are used for a variety of reasons in the field of ML and artificial intelligence. </span><span class="kobospan" id="kobo.148.2">Here are some</span><a id="_idIndexMarker553" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.149.1"> of the </span><span><span class="kobospan" id="kobo.150.1">key </span><a id="_idTextAnchor227" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.151.1">motivations:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.152.1">Nonlinearity</span></strong><span class="kobospan" id="kobo.153.1">: Neural networks, with their intricate structure and use of activation functions, can capture nonlinear relationships in data. </span><span class="kobospan" id="kobo.153.2">Many real-world phenomena are nonlinear in nature, and neural networks offer a way to model </span><span><span class="kobospan" id="kobo.154.1">these c</span><a id="_idTextAnchor228" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.155.1">omplexities.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.156.1">Universal approximation theorem</span></strong><span class="kobospan" id="kobo.157.1">: This theorem states that a neural network with enough hidden units can approximate virtually any function with a high degree of accuracy. </span><span class="kobospan" id="kobo.157.2">This makes them highly flexible and adaptable to a wide ran</span><a id="_idTextAnchor229" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.158.1">ge </span><span><span class="kobospan" id="kobo.159.1">of tasks.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.160.1">Ability to handle high dimensional data</span></strong><span class="kobospan" id="kobo.161.1">: Neural networks can handle data with a large number of features or dimensions effectively, which makes them useful for tasks such as image or speech recognition, where data is </span><span><span class="kobospan" id="kobo.162.1">highly </span><a id="_idTextAnchor230" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.163.1">dimensional.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.164.1">Pattern recognition and prediction</span></strong><span class="kobospan" id="kobo.165.1">: Neural networks excel at identifying patterns and trends within large datasets, making them especially useful for prediction tasks, such as forecasting sales or predicting stock </span><span><span class="kobospan" id="kobo.166.1">ma</span><a id="_idTextAnchor231" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.167.1">rket trends.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.168.1">Parallel processing</span></strong><span class="kobospan" id="kobo.169.1">: Neural networks’ architecture allows them to perform many operations simultaneously, making them highly efficient when implemented on </span><span><span class="kobospan" id="kobo.170.1">mode</span><a id="_idTextAnchor232" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.171.1">rn hardware.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.172.1">Learning from data</span></strong><span class="kobospan" id="kobo.173.1">: Neural networks can improve their performance as they are exposed to more data. </span><span class="kobospan" id="kobo.173.2">This ability to learn from data makes them highly effective for tasks where large amounts of data </span><span><span class="kobospan" id="kobo.174.1">ar</span><a id="_idTextAnchor233" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.175.1">e available.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.176.1">Robustness</span></strong><span class="kobospan" id="kobo.177.1">: Neural networks can handle noise in the input data and are robust to small variations</span><a id="_idIndexMarker554" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.178.1"> in </span><span><span class="kobospan" id="kobo.179.1">the input.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.180.1">Additionally, neural networks are extensively used in NLP tasks due to several reasons. </span><span class="kobospan" id="kobo.180.2">Here are some of the </span><span><span class="kobospan" id="kobo.181.1">primary</span><a id="_idTextAnchor234" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.182.1"> motivations:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.183.1">Handling sequential data</span></strong><span class="kobospan" id="kobo.184.1">: Natural language is inherently sequential (words follow one another to make coherent sentences). </span><span class="kobospan" id="kobo.184.2">RNNs and their advanced</span><a id="_idIndexMarker555" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.185.1"> versions, such as </span><strong class="bold"><span class="kobospan" id="kobo.186.1">long short-term memory</span></strong><span class="kobospan" id="kobo.187.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.188.1">LSTM</span></strong><span class="kobospan" id="kobo.189.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.190.1">gated recurrent units</span></strong><span class="kobospan" id="kobo.191.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.192.1">GRUs</span></strong><span class="kobospan" id="kobo.193.1">), are types of neural networks</span><a id="_idIndexMarker556" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.194.1"> that are capable of processing sequential data by maintaining a form of internal state or memory about the previous steps i</span><a id="_idTextAnchor235" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.195.1">n </span><span><span class="kobospan" id="kobo.196.1">the sequence.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.197.1">Context understanding</span></strong><span class="kobospan" id="kobo.198.1">: Neural networks, especially recurrent types, are capable of understanding the context in a sentence by taking into account the surrounding words or even previous sentences, which is crucia</span><a id="_idTextAnchor236" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.199.1">l in </span><span><span class="kobospan" id="kobo.200.1">NLP tasks.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.201.1">Semantic hashing</span></strong><span class="kobospan" id="kobo.202.1">: Neural networks, through the use of word embeddings (such as Word2Vec and GloVe), can encode words in a way that preserves their semantic meaning. </span><span class="kobospan" id="kobo.202.2">Words with similar meanings are placed closer together in the vector space, which is highly valuable for </span><a id="_idTextAnchor237" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.203.1">many </span><span><span class="kobospan" id="kobo.204.1">NLP tasks.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.205.1">End-to-end learning</span></strong><span class="kobospan" id="kobo.206.1">: Neural networks can learn directly from raw data. </span><span class="kobospan" id="kobo.206.2">For example, in image classification, a neural network can learn features from the pixel level without needing any manual feature extraction steps. </span><span class="kobospan" id="kobo.206.3">This is a significant advantage, as the feature extraction process can be time-consuming and require </span><span><span class="kobospan" id="kobo.207.1">domain expertise.</span></span><p class="calibre6"><span class="kobospan" id="kobo.208.1">Similarly, neural networks can learn to perform NLP tasks from raw text data without the need for manual feature extraction. </span><span class="kobospan" id="kobo.208.2">This is a big advantage in NLP, where creating hand-engineered features can be challenging </span><span><span class="kobospan" id="kobo.209.1">and </span><a id="_idTextAnchor238" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.210.1">time-consuming.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.211.1">Performance</span></strong><span class="kobospan" id="kobo.212.1">: Neural networks, especially with the advent of transformer-based architectures such as BERT, GPT, and so on., have been shown to achieve state-of-the-art results in many NLP tasks, including but not limited to machine translation, text summarization, sentiment analysis, and </span><span><span class="kobospan" id="kobo.213.1">ques</span><a id="_idTextAnchor239" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.214.1">tion answering.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.215.1">Handling large vocabularies</span></strong><span class="kobospan" id="kobo.216.1">: Neural networks can effectively handle large vocabularies and continuous text streams, which is typical in man</span><a id="_idTextAnchor240" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.217.1">y </span><span><strong class="bold"><span class="kobospan" id="kobo.218.1">NLP</span></strong></span><span><span class="kobospan" id="kobo.219.1"> problems.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.220.1">Learning hierarchical features</span></strong><span class="kobospan" id="kobo.221.1">: Deep neural networks can learn hierarchical representations. </span><span class="kobospan" id="kobo.221.2">In the context of NLP, lower layers often learn to represent simple things such as n-grams, whereas higher layers can represent complex concepts</span><a id="_idIndexMarker557" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.222.1"> such </span><span><span class="kobospan" id="kobo.223.1">as sentiment.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.224.1">Despite these advantages, it’s worth noting that neural networks also have their challenges, including their “black box” nature, which makes their decision-making process difficult to interpret, and their need for large amounts of data and computational resources for training. </span><span class="kobospan" id="kobo.224.2">However, the benefits they provide in terms of performance and their ability to learn from raw text data and model complex relationships make them a go-to choice for</span><a id="_idTextAnchor241" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.225.1"> many </span><span><span class="kobospan" id="kobo.226.1">NLP tasks.</span></span></p>
<h2 id="_idParaDest-122" class="calibre7"><a id="_idTextAnchor242" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.227.1">The basic design of a neural network</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.228.1">A neural network consists</span><a id="_idIndexMarker558" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.229.1"> of multiple layers of interconnected nodes, or “neurons,” each of which performs a simple computation on the data it receives, passing its output to the neurons of the next layer. </span><span class="kobospan" id="kobo.229.2">Each connection between neurons has an associated weight that is adjusted during the </span><span><span class="kobospan" id="kobo.230.1">learning process.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.231.1">The architecture of a basic neural network consists of three types of layers, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.232.1">Figure 6</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.233.1">.1</span></em></span><span><span class="kobospan" id="kobo.234.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer327">
<span class="kobospan" id="kobo.235.1"><img alt="Figure 6.1 – Basic architecture of neural networks" src="image/B18949_06_001.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.236.1">Figure 6.1 – Basic architecture of neural networks</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.237.1">In the following list, we explain each layer of the mode</span><a id="_idTextAnchor243" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.238.1">l in </span><span><span class="kobospan" id="kobo.239.1">more detail:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.240.1">Input layer</span></strong><span class="kobospan" id="kobo.241.1">: This is where the network</span><a id="_idIndexMarker559" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.242.1"> receives its input. </span><span class="kobospan" id="kobo.242.2">If the network is designed to process an image with dimensions of 28x28 pixels, for instance, there would be 784 neurons in the input layer, each representing the va</span><a id="_idTextAnchor244" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.243.1">lue of </span><span><span class="kobospan" id="kobo.244.1">one pixel.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.245.1">Hidden layer(s)</span></strong><span class="kobospan" id="kobo.246.1">: These are the layers </span><a id="_idIndexMarker560" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.247.1">between the input and output layers. </span><span class="kobospan" id="kobo.247.2">Each neuron in a hidden layer takes the outputs of the neurons from the previous layer, multiplies each of these by the weight of the respective connection, and sums these values up. </span><span class="kobospan" id="kobo.247.3">This sum is then passed through an “activation function” to introduce nonlinearity into the model, which helps the network learn complex patterns. </span><span class="kobospan" id="kobo.247.4">There can be any number of hidden layers in a neural network, and a network with</span><a id="_idIndexMarker561" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.248.1"> many hidden layers</span><a id="_idIndexMarker562" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.249.1"> is often referred to as a “d</span><a id="_idTextAnchor245" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.250.1">eep” </span><span><span class="kobospan" id="kobo.251.1">neural network.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.252.1">Output layer</span></strong><span class="kobospan" id="kobo.253.1">: This is the final layer</span><a id="_idIndexMarker563" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.254.1"> in the network. </span><span class="kobospan" id="kobo.254.2">The neurons in this layer produce the final output of the network. </span><span class="kobospan" id="kobo.254.3">For a classification problem, for instance, you might design the network to have one output neuron for each class in the problem, with each neuron outputting a value indicating the probability that the input belongs to its </span><span><span class="kobospan" id="kobo.255.1">respective class.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.256.1">The neurons in the network are interconnected. </span><span class="kobospan" id="kobo.256.2">The weights of these connections, which are initially set to random values, represent what the network has learned once it has been trained </span><span><span class="kobospan" id="kobo.257.1">on data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.258.1">During the training process, an algorithm such as backpropagation is used to adjust the weights of the connections in the network in response to the difference between the network’s output and the desired output. </span><span class="kobospan" id="kobo.258.2">This process is repeated many times, and the network gradually improves its performance on the </span><span><span class="kobospan" id="kobo.259.1">training data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.260.1">To provide a simple visual idea, imagine</span><a id="_idIndexMarker564" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.261.1"> three sets of circles (representing neurons) arranged in columns (representing layers). </span><span class="kobospan" id="kobo.261.2">The first column is the input layer, the last column is the output layer and any columns in between are the hidden layers. </span><span class="kobospan" id="kobo.261.3">Then, imagine lines connecting every circle in each column to every circle in the next column, representing the weighted connections between neurons. </span><span class="kobospan" id="kobo.261.4">That’s a basic visual representation of a </span><span><span class="kobospan" id="kobo.262.1">neural network.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.263.1">In the next part, we are going to describe the common terms relate</span><a id="_idTextAnchor246" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.264.1">d to </span><span><span class="kobospan" id="kobo.265.1">neural networks.</span></span></p>
<h2 id="_idParaDest-123" class="calibre7"><a id="_idTextAnchor247" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.266.1">Neural network common terms</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.267.1">In the following subsections, we'll look at some of the most commonly used terms in the conte</span><a id="_idTextAnchor248" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.268.1">xt of </span><span><span class="kobospan" id="kobo.269.1">neural networks.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.270.1">Neuron (or node)</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.271.1">This is the basic unit of computation</span><a id="_idIndexMarker565" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.272.1"> in a neural network; typically, a simple computation involves inputs, weights, a bias, and an activation function. </span><span class="kobospan" id="kobo.272.2">A neuron, also known as a node or unit, is a fundamental element</span><a id="_idIndexMarker566" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.273.1"> in a neural network. </span><span class="kobospan" id="kobo.273.2">It receives input from some other nodes or from an external source if the neuron is in the input layer. </span><span class="kobospan" id="kobo.273.3">The neuron then computes an output based on </span><span><span class="kobospan" id="kobo.274.1">this input.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.275.1">Each input has an associated weight (</span><em class="italic"><span class="kobospan" id="kobo.276.1">w</span></em><span class="kobospan" id="kobo.277.1">), which is assigned based on its relative importance to other inputs. </span><span class="kobospan" id="kobo.277.2">The neuron applies a weight to the inputs, sums them up, and then applies an activation function to the sum plus a bias </span><span><span class="kobospan" id="kobo.278.1">value (</span></span><span><em class="italic"><span class="kobospan" id="kobo.279.1">b</span></em></span><span><span class="kobospan" id="kobo.280.1">).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.281.1">Here’s a</span><a id="_idTextAnchor249" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.282.1">step-by-step breakdown:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.283.1">Weighted sum</span></strong><span class="kobospan" id="kobo.284.1">: Each input (</span><em class="italic"><span class="kobospan" id="kobo.285.1">x</span></em><span class="kobospan" id="kobo.286.1">) to the neuron</span><a id="_idIndexMarker567" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.287.1"> is multiplied by a corresponding weight (</span><em class="italic"><span class="kobospan" id="kobo.288.1">w</span></em><span class="kobospan" id="kobo.289.1">). </span><span class="kobospan" id="kobo.289.2">These weighted inputs are then summed together with a bias term (</span><em class="italic"><span class="kobospan" id="kobo.290.1">b</span></em><span class="kobospan" id="kobo.291.1">). </span><span class="kobospan" id="kobo.291.2">The bias term allows for the activation function to be shifted to the left or the right, helping the neuron model a wider range of patterns. </span><span class="kobospan" id="kobo.291.3">Mathematically, this step can be represented </span><span><span class="kobospan" id="kobo.292.1">as follows:</span></span><p class="calibre6"><span class="kobospan" id="kobo.293.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/297.png" class="calibre296"/></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.294.1">Activation function</span></strong><span class="kobospan" id="kobo.295.1">: The result of the weighted</span><a id="_idIndexMarker568" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.296.1"> sum is then passed through an activation function. </span><span class="kobospan" id="kobo.296.2">The purpose of the activation function is to introduce nonlinearity into the output of a neuron. </span><span class="kobospan" id="kobo.296.3">This nonlinearity allows the network to learn from errors and make adjustments, which is essential when it comes to performing complex tasks such as language translation or image recognition. </span><span class="kobospan" id="kobo.296.4">Common choices</span><a id="_idIndexMarker569" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.297.1"> for activation functions include</span><a id="_idIndexMarker570" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.298.1"> the sigmoid function, hyperbolic </span><strong class="bold"><span class="kobospan" id="kobo.299.1">tangent</span></strong><span class="kobospan" id="kobo.300.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.301.1">tanh</span></strong><span class="kobospan" id="kobo.302.1">), and </span><strong class="bold"><span class="kobospan" id="kobo.303.1">rectified linear unit</span></strong><span class="kobospan" id="kobo.304.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.305.1">ReLU</span></strong><span class="kobospan" id="kobo.306.1">), </span><span><span class="kobospan" id="kobo.307.1">among others.</span></span><p class="calibre6"><span class="kobospan" id="kobo.308.1">The output of the neuron is the result of the activation function. </span><span class="kobospan" id="kobo.308.2">It serves as the input to the neurons in the next layer of </span><span><span class="kobospan" id="kobo.309.1">the network.</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.310.1">The weights and bias in the neuron</span><a id="_idIndexMarker571" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.311.1"> are learnable parameters. </span><span class="kobospan" id="kobo.311.2">In other words, their values are learned over time as the neura</span><a id="_idTextAnchor250" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.312.1">l network is trained </span><span><span class="kobospan" id="kobo.313.1">on data:</span></span></p><ul class="calibre17"><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.314.1">Weights</span></strong><span class="kobospan" id="kobo.315.1">: The strength or amplitude</span><a id="_idIndexMarker572" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.316.1"> of the connection between two neurons. </span><span class="kobospan" id="kobo.316.2">During the training phase, the neural network learns the correct weights that better map inputs to outputs. </span><span class="kobospan" id="kobo.316.3">Weight is used in t</span><a id="_idTextAnchor251" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.317.1">he neuron, as </span><span><span class="kobospan" id="kobo.318.1">explained previously.</span></span></li><li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.319.1">Bias</span></strong><span class="kobospan" id="kobo.320.1">: An additional parameter in the neuron</span><a id="_idIndexMarker573" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.321.1"> that allows for the activation function to be shifted to the left or right, which can be critical for successful lear</span><a id="_idTextAnchor252" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.322.1">ning (also used in </span><span><span class="kobospan" id="kobo.323.1">the neuron).</span></span></li></ul></li>
</ol>
<h3 class="calibre8"><span class="kobospan" id="kobo.324.1">Activation function</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.325.1">The function (in each neuron) that determines</span><a id="_idIndexMarker574" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.326.1"> the output a neuron</span><a id="_idIndexMarker575" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.327.1"> should produce given its input is called an activation function. </span><span class="kobospan" id="kobo.327.2">Common examples include sigmoid, ReLU </span><span><span class="kobospan" id="kobo.328.1">and tanh.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.329.1">Here are some of the most common types of </span><span><span class="kobospan" id="kobo.330.1">activation functions:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.331.1">Sigmoid function</span></strong><span class="kobospan" id="kobo.332.1">: This is where we’re essentially </span><a id="_idIndexMarker576" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.333.1">classifying the input</span><a id="_idIndexMarker577" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.334.1"> as either 0 or 1. </span><span class="kobospan" id="kobo.334.2">The sigmoid function takes a real-valued input and squashes it to range between 0 and 1. </span><span class="kobospan" id="kobo.334.3">It’s often used in the output layer of a binary </span><span><span class="kobospan" id="kobo.335.1">classification network:</span></span><p class="calibre6"><span class="kobospan" id="kobo.336.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/298.png" class="calibre297"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.337.1">However, it has two major drawbacks: </span><strong class="bold"><span class="kobospan" id="kobo.338.1">the vanishing gradients problem</span></strong><span class="kobospan" id="kobo.339.1"> (gradients are very small</span><a id="_idIndexMarker578" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.340.1"> for large positive or negative inputs, which can slow down learning during</span><a id="_idIndexMarker579" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.341.1"> backpropagation) and the </span><strong class="bold"><span class="kobospan" id="kobo.342.1">outputs are </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.343.1">not zero-centered</span></strong></span><span><span class="kobospan" id="kobo.344.1">.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.345.1">Hyperbolic tanh function</span></strong><span class="kobospan" id="kobo.346.1">: The tanh function also takes</span><a id="_idIndexMarker580" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.347.1"> a real-valued input</span><a id="_idIndexMarker581" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.348.1"> and squashes it to range between -1 and 1. </span><span class="kobospan" id="kobo.348.2">Unlike the sigmoid function, its output is zero-centered because its range is symmetric around </span><span><span class="kobospan" id="kobo.349.1">the origin:</span></span><p class="calibre6"><span class="kobospan" id="kobo.350.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/299.png" class="calibre298"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.351.1">It also suffers from the vanishing gradients</span><a id="_idIndexMarker582" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.352.1"> problem, as does the </span><span><span class="kobospan" id="kobo.353.1">sigmoid function.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.354.1">ReLU function</span></strong><span class="kobospan" id="kobo.355.1">: The ReLU function has become</span><a id="_idIndexMarker583" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.356.1"> very popular in recent</span><a id="_idIndexMarker584" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.357.1"> years. </span><span class="kobospan" id="kobo.357.2">It computes the function </span><span><span class="kobospan" id="kobo.358.1">as follows:</span></span><p class="calibre6"><span class="kobospan" id="kobo.359.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/300.png" class="calibre299"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.360.1">In other words, the activation is simply the input if the input is positive; otherwise, </span><span><span class="kobospan" id="kobo.361.1">it’s zero.</span></span></p><p class="calibre6"><span class="kobospan" id="kobo.362.1">It doesn’t activate all the neurons at the same time, meaning that the neurons will only be deactivated if the output of the linear transformation is less than 0. </span><span class="kobospan" id="kobo.362.2">This makes the network sparse and efficient. </span><span class="kobospan" id="kobo.362.3">However, ReLU units can be fragile during training and can “die” (they stop learning completely) if a large gradient flows </span><span><span class="kobospan" id="kobo.363.1">through them.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.364.1">Leaky ReLU</span></strong><span class="kobospan" id="kobo.365.1">: Leaky ReLU is a variant of ReLU</span><a id="_idIndexMarker585" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.366.1"> that addresses</span><a id="_idIndexMarker586" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.367.1"> the “dying ReLU” problem. </span><span class="kobospan" id="kobo.367.2">Instead of defining the function as </span><em class="italic"><span class="kobospan" id="kobo.368.1">0</span></em><span class="kobospan" id="kobo.369.1"> for negative </span><em class="italic"><span class="kobospan" id="kobo.370.1">x</span></em><span class="kobospan" id="kobo.371.1">, we define it as a small linear component </span><span><span class="kobospan" id="kobo.372.1">of </span></span><span><em class="italic"><span class="kobospan" id="kobo.373.1">x</span></em></span><span><span class="kobospan" id="kobo.374.1">:</span></span><p class="calibre6"><span class="kobospan" id="kobo.375.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;0.01&lt;/mn&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/301.png" class="calibre300"/></span></p><p class="calibre6"><span class="kobospan" id="kobo.376.1">This allows the function to “leak” some information when the input is negative and helps to mitigate</span><a id="_idIndexMarker587" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.377.1"> the dying </span><span><span class="kobospan" id="kobo.378.1">ReLU problem.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.379.1">Exponential linear unit (ELU)</span></strong><span class="kobospan" id="kobo.380.1">: ELU is also a variant of ReLU</span><a id="_idIndexMarker588" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.381.1"> that modifies the function</span><a id="_idIndexMarker589" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.382.1"> to be a non-zero value for negative </span><em class="italic"><span class="kobospan" id="kobo.383.1">x</span></em><span class="kobospan" id="kobo.384.1">, which can help the </span><span><span class="kobospan" id="kobo.385.1">learning process:</span></span><p class="calibre6"><span><span class="kobospan" id="kobo.386.1">f</span></span><span><span class="kobospan" id="kobo.387.1">(</span></span><span><span class="kobospan" id="kobo.388.1">x</span></span><span><span class="kobospan" id="kobo.389.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.390.1">=</span></span><span> </span><span><span class="kobospan" id="kobo.391.1">x</span></span><span> </span><span><span class="kobospan" id="kobo.392.1">i</span></span><span><span class="kobospan" id="kobo.393.1">f</span></span><span> </span><span><span class="kobospan" id="kobo.394.1">x</span></span><span> </span><span><span class="kobospan" id="kobo.395.1">&gt;</span></span><span> </span><span><span class="kobospan" id="kobo.396.1">0</span></span><span><span class="kobospan" id="kobo.397.1">,</span></span><span> </span><span><span class="kobospan" id="kobo.398.1">e</span></span><span><span class="kobospan" id="kobo.399.1">l</span></span><span><span class="kobospan" id="kobo.400.1">s</span></span><span><span class="kobospan" id="kobo.401.1">e</span></span><span> </span></p><p class="calibre6"><span><span class="kobospan" id="kobo.402.1">α</span></span><span><span class="kobospan" id="kobo.403.1">(</span></span><span><span class="kobospan" id="kobo.404.1">e</span></span><span><span class="kobospan" id="kobo.405.1">x</span></span><span><span class="kobospan" id="kobo.406.1">p</span></span><span><span class="kobospan" id="kobo.407.1">(</span></span><span><span class="kobospan" id="kobo.408.1">x</span></span><span><span class="kobospan" id="kobo.409.1">)</span></span><span> </span><span><span class="kobospan" id="kobo.410.1">−</span></span><span> </span><span><span><span class="kobospan" id="kobo.411.1">1</span></span></span><span><span><span class="kobospan" id="kobo.412.1">)</span></span></span></p><p class="calibre6"><span class="kobospan" id="kobo.413.1">Here alpha (</span><em class="italic"><span class="kobospan" id="kobo.414.1">α</span></em><span class="kobospan" id="kobo.415.1">) is a constant that defines function smoothness when inputs are negative. </span><span class="kobospan" id="kobo.415.2">ELU tends to converge cost to zero faster and produce more accurate results. </span><span class="kobospan" id="kobo.415.3">However, it can be slower to compute because of the use of the </span><span><span class="kobospan" id="kobo.416.1">exponential operation.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.417.1">Softmax function</span></strong><span class="kobospan" id="kobo.418.1">: The softmax function is often used </span><a id="_idIndexMarker590" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.419.1">in the output layer</span><a id="_idIndexMarker591" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.420.1"> of a classifier where we’re trying to assign the input to one of several classes. </span><span class="kobospan" id="kobo.420.2">It gives the probability that any given input belongs to each of the </span><span><span class="kobospan" id="kobo.421.1">possible classes:</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.422.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/msup&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;/msup&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/302.png" class="calibre301"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.423.1">The denominator normalizes the probabilities, so they all sum up to 1 across all classes. </span><span class="kobospan" id="kobo.423.2">The softmax function is also used in multinomial </span><span><span class="kobospan" id="kobo.424.1">logistical regression.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.425.1">Each of these activation functions</span><a id="_idIndexMarker592" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.426.1"> has pros and cons, and the choice of activation function can depend on the specif</span><a id="_idTextAnchor253" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.427.1">ic application and context of the problem </span><span><span class="kobospan" id="kobo.428.1">at hand.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.429.1">Layer</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.430.1">A set of neurons that process</span><a id="_idIndexMarker593" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.431.1"> signals at the same level of abstraction. </span><span class="kobospan" id="kobo.431.2">The first layer is the input layer, the last layer</span><a id="_idIndexMarker594" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.432.1"> is the output layer,</span><a id="_idTextAnchor254" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.433.1"> and all layers in between are called </span><span><span class="kobospan" id="kobo.434.1">hidden layers.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.435.1">Epoch</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.436.1">In the context of</span><a id="_idIndexMarker595" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.437.1"> training a neural</span><a id="_idIndexMarker596" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.438.1"> network, an epoch is a term used to denote one complete pass through the entire training dataset. </span><span class="kobospan" id="kobo.438.2">During an epoch, the neural network’s weights are updated in an attempt to minimize the </span><span><span class="kobospan" id="kobo.439.1">loss function.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.440.1">The number of epochs hyperparameter sets how many times the deep learning algorithm processes the entire training dataset. </span><span class="kobospan" id="kobo.440.2">Too many epochs can cause overfitting, where the model performs well on training data but poorly on new data. </span><span class="kobospan" id="kobo.440.3">Conversely, training for too few epochs may mean the model is underfitting—it could improve with </span><span><span class="kobospan" id="kobo.441.1">further training.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.442.1">It’s also important to note that the concept of an epoch is more relevant in the batch and mini-batch variants of gradient descent. </span><span class="kobospan" id="kobo.442.2">In stochastic gradient descent, the model’s weights are updated after seeing each individual exampl</span><a id="_idTextAnchor255" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.443.1">e, so the concept of an epoch</span><a id="_idIndexMarker597" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.444.1"> is </span><span><span class="kobospan" id="kobo.445.1">less straightforward.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.446.1">Batch size</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.447.1">The number of</span><a id="_idIndexMarker598" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.448.1"> training instances</span><a id="_idIndexMarker599" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.449.1"> used in one iteration. </span><span class="kobospan" id="kobo.449.2">Batch size refers to the number of training examples used in </span><span><span class="kobospan" id="kobo.450.1">one iteration.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.451.1">When you start training a neural network, you have a cou</span><a id="_idTextAnchor256" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.452.1">ple of options for how you feed your data into </span><span><span class="kobospan" id="kobo.453.1">the model:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.454.1">Batch gradient descent</span></strong><span class="kobospan" id="kobo.455.1">: Here, the entire training dataset is used to compute the gradient of the loss function for each iteration of the optimizer (as with gradient descent). </span><span class="kobospan" id="kobo.455.2">In this case, the batch size is equal</span><a id="_idTextAnchor257" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.456.1"> to the total number of examples in the </span><span><span class="kobospan" id="kobo.457.1">training dataset.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.458.1">Stochastic gradient descent (SGD)</span></strong><span class="kobospan" id="kobo.459.1">: SGD uses a single example at each iteration </span><a id="_idTextAnchor258" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.460.1">of the optimizer. </span><span class="kobospan" id="kobo.460.2">Therefore, the batch size for SGD </span><span><span class="kobospan" id="kobo.461.1">is </span></span><span><em class="italic"><span class="kobospan" id="kobo.462.1">1</span></em></span><span><span class="kobospan" id="kobo.463.1">.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.464.1">Mini-batch gradient descent</span></strong><span class="kobospan" id="kobo.465.1">: This is a compromise between batch gradient descent and SGD. </span><span class="kobospan" id="kobo.465.2">In mini-batch gradient descent, the batch size is usually between 10 and 1,000 and is chosen depending on the computational resources </span><span><span class="kobospan" id="kobo.466.1">you have.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.467.1">The batch size can significantly</span><a id="_idIndexMarker600" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.468.1"> impact the learning process. </span><span class="kobospan" id="kobo.468.2">Larger batch sizes result in faster progress in training but don’t always converge as fast. </span><span class="kobospan" id="kobo.468.3">Smaller batch sizes update the model frequently but the progress in training </span><span><span class="kobospan" id="kobo.469.1">is slower.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.470.1">Moreover, smaller batch sizes have a regularizing effect and can help the model generalize better, leading to better performance on unseen data. </span><span class="kobospan" id="kobo.470.2">However, using a batch size that is too small can lead to unstable training, less accurate estimates of the gradient, and, ultimately, a model with </span><span><span class="kobospan" id="kobo.471.1">worse performance.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.472.1">Choosing the right batch size</span><a id="_idIndexMarker601" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.473.1"> is a matter of trial and error and depends on th</span><a id="_idTextAnchor259" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.474.1">e specific problem and the computational resources </span><span><span class="kobospan" id="kobo.475.1">at hand:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.476.1">Iterations</span></strong><span class="kobospan" id="kobo.477.1">: The number of batches of data the algorithm has</span><a id="_idTextAnchor260" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.478.1"> seen (or the number of passes it has made on </span><span><span class="kobospan" id="kobo.479.1">the dataset).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.480.1">Learning rate</span></strong><span class="kobospan" id="kobo.481.1">: A hyperparameter that controls the speed of convergence of the learning algorithm by ad</span><a id="_idTextAnchor261" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.482.1">justing the weight update rate based on the </span><span><span class="kobospan" id="kobo.483.1">loss gradient.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.484.1">Loss function (cost function)</span></strong><span class="kobospan" id="kobo.485.1">: The loss function evaluates the neural network’s performance on the dataset. </span><span class="kobospan" id="kobo.485.2">Higher deviations between predictions and actual results result in a larger output from the loss function. </span><span class="kobospan" id="kobo.485.3">The goal is to minimize</span><a id="_idIndexMarker602" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.486.1"> this </span><a id="_idTextAnchor262" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.487.1">output, which will give the model more </span><span><span class="kobospan" id="kobo.488.1">accurate predictions.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.489.1">Backpropagation</span></strong><span class="kobospan" id="kobo.490.1">: The primary algorithm for performing gradient descent on neural networks. </span><span class="kobospan" id="kobo.490.2">It calculates the gradient of the loss function at the output layer and distributes it back through the layers of the network, updat</span><a id="_idTextAnchor263" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.491.1">ing the weights and biases in a way that minimizes </span><span><span class="kobospan" id="kobo.492.1">the loss.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.493.1">Overfitting</span></strong><span class="kobospan" id="kobo.494.1">: A situation where a model learns</span><a id="_idIndexMarker603" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.495.1"> the detail and noise in the training da</span><a id="_idTextAnchor264" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.496.1">ta to the extent that it performs poorly on new, </span><span><span class="kobospan" id="kobo.497.1">unseen data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.498.1">Underfitting</span></strong><span class="kobospan" id="kobo.499.1">: A situation where a model is too simple to learn the underlying structure of the d</span><a id="_idTextAnchor265" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.500.1">ata and, thus, performs poorly on both training and </span><span><span class="kobospan" id="kobo.501.1">new data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.502.1">Regularization</span></strong><span class="kobospan" id="kobo.503.1">: A technique used to prevent overfitting by adding a penalty term to the loss fun</span><a id="_idTextAnchor266" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.504.1">ction, which, in turn, constrains the weights of </span><span><span class="kobospan" id="kobo.505.1">the network.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.506.1">Dropout</span></strong><span class="kobospan" id="kobo.507.1">: A regularization technique where randomly selected neurons are</span><a id="_idTextAnchor267" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.508.1"> ignored during training, which helps to </span><span><span class="kobospan" id="kobo.509.1">prevent overfitting.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.510.1">CNN</span></strong><span class="kobospan" id="kobo.511.1">: A type of neural netwo</span><a id="_idTextAnchor268" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.512.1">rk well-suited to image processing and computer </span><span><span class="kobospan" id="kobo.513.1">vision tasks.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.514.1">RNN</span></strong><span class="kobospan" id="kobo.515.1">: A type of neural network designed to recognize patterns</span><a id="_idIndexMarker604" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.516.1"> in sequences of data, such as time </span><a id="_idIndexMarker605" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.517.1">series </span><span><span class="kobospan" id="kobo.518.1">or text.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.519.1">Let’s</span><a id="_idTextAnchor269" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.520.1"> move on to the architecture of different neural </span><span><span class="kobospan" id="kobo.521.1">networks next.</span></span></p>
<h1 id="_idParaDest-124" class="calibre4"><a id="_idTextAnchor270" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.522.1">The architecture of different neural networks</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.523.1">Neural networks come in various</span><a id="_idIndexMarker606" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.524.1"> types, each with a specific architecture suited to a different kind of task. </span><span class="kobospan" id="kobo.524.2">The following list</span><a id="_idTextAnchor271" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.525.1"> contains general descriptions of some of the most </span><span><span class="kobospan" id="kobo.526.1">common types:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.527.1">Feedforward neural network (FNN)</span></strong><span class="kobospan" id="kobo.528.1">: This is the most straightforward</span><a id="_idIndexMarker607" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.529.1"> type of neural</span><a id="_idIndexMarker608" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.530.1"> network. </span><span class="kobospan" id="kobo.530.2">Information in this network moves in one direction only, from the input layer through any hidden layers to the output layer. </span><span class="kobospan" id="kobo.530.3">There are no cycles or loops in the network; it’s a straight</span><a id="_idTextAnchor272" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.531.1">, “</span><span><span class="kobospan" id="kobo.532.1">feedforward” path.</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer334">
<span class="kobospan" id="kobo.533.1"><img alt="Figure 6.2 – Feedforward neural network" src="image/B18949_06_002.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.534.1">Figure 6.2 – Feedforward neural network</span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.535.1">Multilayer perceptron (MLP)</span></strong><span class="kobospan" id="kobo.536.1">: An MLP is a type of feedforward </span><a id="_idIndexMarker609" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.537.1">network that has at</span><a id="_idIndexMarker610" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.538.1"> least one hidden layer in addition to its input and output layers. </span><span class="kobospan" id="kobo.538.2">The layers are fully connected, meaning each neuron in a layer connects with every neuron in the next layer. </span><span class="kobospan" id="kobo.538.3">MLPs can model complex patterns and are widely used for tasks such as image recognition, classification, speech recognition, and other types of machine learning tasks. </span><span class="kobospan" id="kobo.538.4">The MLP is a feedforward network with layers of neurons arranged sequentially. </span><span class="kobospan" id="kobo.538.5">Information flows from the input layer through hidden layers</span><a id="_idIndexMarker611" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.539.1"> to the o</span><a id="_idTextAnchor273" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.540.1">utput layer</span><a id="_idIndexMarker612" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.541.1"> in </span><span><span class="kobospan" id="kobo.542.1">one direction:</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer335">
<span class="kobospan" id="kobo.543.1"><img alt="Figure 6.3 – Multilayer perceptron" src="image/B18949_06_003.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.544.1">Figure 6.3 – Multilayer perceptron</span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.545.1">CNN</span></strong><span class="kobospan" id="kobo.546.1">: A CNN is particularly well-suited</span><a id="_idIndexMarker613" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.547.1"> to tasks involving</span><a id="_idIndexMarker614" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.548.1"> spatial data, such as images. </span><span class="kobospan" id="kobo.548.2">Its architecture includes three main types of layers: convolutional layers, pooling layers, and fully connected layers. </span><span class="kobospan" id="kobo.548.3">The convolutional layers apply a series of filters to the input, which allows the network to automatically and adaptively learn spatial hierarchies of features. </span><span class="kobospan" id="kobo.548.4">Pooling layers decrease the spatial size of the representation, thereby reducing parameters and computation in the network to control overfitting and decrease the computation cost in the following layers. </span><span class="kobospan" id="kobo.548.5">Fully connected layers get the output of the pooling layer and conduct high-level</span><a id="_idTextAnchor274" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.549.1"> reasoning on </span><span><span class="kobospan" id="kobo.550.1">the output.</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer336">
<span class="kobospan" id="kobo.551.1"><img alt="Figure 6.4 – ﻿Convolutional neural network" src="image/B18949_06_004.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.552.1">Figure 6.4 – Convolutional neural network</span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.553.1">Recurrent neural network (RNN)</span></strong><span class="kobospan" id="kobo.554.1">: Unlike feedforward networks, RNNs have connections</span><a id="_idIndexMarker615" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.555.1"> that form directed </span><a id="_idIndexMarker616" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.556.1">cycles. </span><span class="kobospan" id="kobo.556.2">This architecture allows them to use information from their previous outputs as inputs, making them ideal for tasks involving sequential data, such</span><a id="_idIndexMarker617" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.557.1"> as time series prediction or NLP. </span><span class="kobospan" id="kobo.557.2">A significant variation of RNNs is the LSTM network, which uses special units in addition to standard units. </span><span class="kobospan" id="kobo.557.3">RNN units include a "memory cell" that can maintain information in memory for long periods of time, a feature that is particularly useful for tasks that require learning from long-distance dependencies in the data, such as handwri</span><a id="_idTextAnchor275" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.558.1">ting or </span><span><span class="kobospan" id="kobo.559.1">speech recognition.</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer337">
<span class="kobospan" id="kobo.560.1"><img alt="Figure 6.5 – Recurrent neural network" src="image/B18949_06_005.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.561.1">Figure 6.5 – Recurrent neural network</span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.562.1">Autoencoder (AE)</span></strong><span class="kobospan" id="kobo.563.1">: An AE is a type</span><a id="_idIndexMarker618" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.564.1"> of neural network</span><a id="_idIndexMarker619" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.565.1"> used to learn the efficient coding of input data. </span><span class="kobospan" id="kobo.565.2">It has a symmetrical architecture and is designed to apply backpropagation, setting the target values to be equal to the inputs. </span><span class="kobospan" id="kobo.565.3">Autoencoders are typically used for feature extraction, learning representations of data, and dimensionality reduction. </span><span class="kobospan" id="kobo.565.4">They’re also used in generative models, noise removal, </span><a id="_idTextAnchor276" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.566.1">and </span><span><span class="kobospan" id="kobo.567.1">recommendation systems.</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer338">
<span class="kobospan" id="kobo.568.1"><img alt="Figure 6.6 – Autoencoder architecture" src="image/B18949_06_006.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.569.1">Figure 6.6 – Autoencoder architecture</span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.570.1">Generative adversarial network (GAN)</span></strong><span class="kobospan" id="kobo.571.1">: A GAN consists of two</span><a id="_idIndexMarker620" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.572.1"> parts, a generator</span><a id="_idIndexMarker621" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.573.1"> and a discriminator, which are both neural networks. </span><span class="kobospan" id="kobo.573.2">The generator creates data instances that aim to come from the same distribution as the training dataset. </span><span class="kobospan" id="kobo.573.3">The discriminator’s goal is to distinguish between instances from the true distribution and instances from the generator. </span><span class="kobospan" id="kobo.573.4">The generator and the discriminator are trained together, with the goal that the generator produces better instances as training progresses, whereas the discriminator becomes better at distinguishing true instances from </span><span><span class="kobospan" id="kobo.574.1">generated ones.</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer339">
<span class="kobospan" id="kobo.575.1"><img alt="Figure 6.7 – Generative adversarial network in computer vision" src="image/B18949_06_007.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.576.1">Figure 6.7 – Generative adversarial network in computer vision</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.577.1">These are just a few examples of neural network architectures, and many variations and combinations exist. </span><span class="kobospan" id="kobo.577.2">The architecture you choose for a task</span><a id="_idTextAnchor277" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.578.1"> will depend on the specific requirements and constraints of </span><span><span class="kobospan" id="kobo.579.1">your task.</span></span></p>
<h1 id="_idParaDest-125" class="calibre4"><a id="_idTextAnchor278" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.580.1">The challenges of training neural networks</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.581.1">Training neural networks is a complex task and comes with challenges during the training, such as local minima and vanishing/exploding gradients, as well as computational costs and int</span><a id="_idTextAnchor279" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.582.1">erpretability. </span><span class="kobospan" id="kobo.582.2">All challenges</span><a id="_idIndexMarker622" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.583.1"> are explained in detail in the </span><span><span class="kobospan" id="kobo.584.1">following points:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.585.1">Local minima</span></strong><span class="kobospan" id="kobo.586.1">: The objective of training a neural network is to find the set of weights that minimizes the loss function. </span><span class="kobospan" id="kobo.586.2">This is a high-dimensional optimization problem, and there are many points (sets of weights) where the loss function has local minima. </span><span class="kobospan" id="kobo.586.3">A suboptimal local minimum is a point where the loss is lower than for the nearby points but higher than the global minimum, which is the overall lowest possible loss. </span><span class="kobospan" id="kobo.586.4">The training process can get stuck in such suboptimal local minima. </span><span class="kobospan" id="kobo.586.5">It’s important to remember that the local minima problem exists even in convex loss functions d</span><a id="_idTextAnchor280" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.587.1">ue to the discrete representation that is a part of </span><span><span class="kobospan" id="kobo.588.1">digital computation.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.589.1">Vanishing/exploding gradients</span></strong><span class="kobospan" id="kobo.590.1">: This is a difficulty encountered, especially when training deep neural networks. </span><span class="kobospan" id="kobo.590.2">The gradients of the loss function may become very small (vanish) or very large (explode) in deeper layers of the network during the backpropagation process. </span><span class="kobospan" id="kobo.590.3">Vanishing gradients make it hard for the network to learn from the data because the weight updates become very small. </span><span class="kobospan" id="kobo.590.4">Exploding gradients can cause the training process to fail because weig</span><a id="_idTextAnchor281" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.591.1">ht updates become too large, and the loss becomes undefined (</span><span><span class="kobospan" id="kobo.592.1">e.g., NaN).</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.593.1">Overfitting</span></strong><span class="kobospan" id="kobo.594.1">: One of the common problems in training machine learning models is when our model is too complex, and we train it too much. </span><span class="kobospan" id="kobo.594.2">In this case, the model learns even the noises in the training data a</span><a id="_idTextAnchor282" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.595.1">nd works very well on training data but poorly on the unseen </span><span><span class="kobospan" id="kobo.596.1">test data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.597.1">Underfitting</span></strong><span class="kobospan" id="kobo.598.1">: Conversely, underfitting occurs when the model is too simple and can’t capture the underlying structure of the data. </span><span class="kobospan" id="kobo.598.2">Both overfitting and underfitting can be mitigated by using proper model complexi</span><a id="_idTextAnchor283" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.599.1">ty, regularization techniques, and a sufficient amount of </span><span><span class="kobospan" id="kobo.600.1">training data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.601.1">Computational resources</span></strong><span class="kobospan" id="kobo.602.1">: Training neural</span><a id="_idIndexMarker623" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.603.1"> networks, particularly deep networks, requires significant computational resources (CPU/GPU power and memory). </span><span class="kobospan" id="kobo.603.2">They also often require a large amount of training data </span><a id="_idTextAnchor284" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.604.1">to perform well, which can be a problem when such data are </span><span><span class="kobospan" id="kobo.605.1">not available.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.606.1">Lack of interpretability</span></strong><span class="kobospan" id="kobo.607.1">: While not strictly a training issue, the lack of interpretability of neural networks is a significant problem. </span><span class="kobospan" id="kobo.607.2">They are often referred to as “black boxes” because it i</span><a id="_idTextAnchor285" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.608.1">s challenging</span><a id="_idIndexMarker624" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.609.1"> to understand why they are making the predictions </span><span><span class="kobospan" id="kobo.610.1">they do.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.611.1">Difficulty in selecting appropriate architecture and hyperparameters</span></strong><span class="kobospan" id="kobo.612.1">: There are many types of neural network architectures to choose from (such as CNN and RNN), and each has a set of hyperparameters that need to be tuned (such as learning rate, batch size, number of layers, and number of units per layer). </span><span class="kobospan" id="kobo.612.2">Selecting the best architecture and tuning these hyperpar</span><a id="_idTextAnchor286" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.613.1">ameters for a given problem can be a challenging and </span><span><span class="kobospan" id="kobo.614.1">time-consuming task.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.615.1">Data preprocessing</span></strong><span class="kobospan" id="kobo.616.1">: Neural networks often require the input data to be in a specific format. </span><span class="kobospan" id="kobo.616.2">For instance, data might need to be normalized, categorical variables might need to be one-hot encoded, and missing values might need to be imputed. </span><span class="kobospan" id="kobo.616.3">This preprocessing can be a complex and </span><span><span class="kobospan" id="kobo.617.1">time-consuming step.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.618.1">These challenges make training neural networks</span><a id="_idIndexMarker625" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.619.1"> a non-trivial task, often requiring a combin</span><a id="_idTextAnchor287" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.620.1">ation of technical expertise, computational resources, and trial </span><span><span class="kobospan" id="kobo.621.1">and error.</span></span></p>
<h1 id="_idParaDest-126" class="calibre4"><a id="_idTextAnchor288" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.622.1">Language models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.623.1">A language model is a statistical</span><a id="_idIndexMarker626" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.624.1"> model in NLP that is designed to learn and understand the structure of human language. </span><span class="kobospan" id="kobo.624.2">More specifically, it is a probabilistic model that is trained to estimate the likelihood of words when provided with a given word scenario. </span><span class="kobospan" id="kobo.624.3">For instance, a language model could be trained to predict the next word in a sentence, given the </span><span><span class="kobospan" id="kobo.625.1">previous words.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.626.1">Language models are fundamental to many NLP tasks. </span><span class="kobospan" id="kobo.626.2">They are used in machine translation, speech recognition, part-of-speech tagging, and named entity recognition, among other things. </span><span class="kobospan" id="kobo.626.3">More recently, they have been used to create conversational AI models such as chatbots and personal assistants and to generate </span><span><span class="kobospan" id="kobo.627.1">human-like text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.628.1">Traditional language models were often based on explicitly statistical methods, such as n-gram models, which consider </span><a id="_idIndexMarker627" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.629.1">only the previous n words when predicting the next word, or </span><strong class="bold"><span class="kobospan" id="kobo.630.1">hidden Markov </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.631.1">models</span></strong></span><span><span class="kobospan" id="kobo.632.1"> (</span></span><span><strong class="bold"><span class="kobospan" id="kobo.633.1">HMMs</span></strong></span><span><span class="kobospan" id="kobo.634.1">).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.635.1">More recently, neural networks have become popular for creating language models, leading to the rise of neural language models. </span><span class="kobospan" id="kobo.635.2">These models use the power of neural networks to consider the context of each word when making predictions, resulting in higher accuracy and fluency. </span><span class="kobospan" id="kobo.635.3">Examples of neural language models include RNNs, the transformer model, and various transformer-based architectures such as BERT </span><span><span class="kobospan" id="kobo.636.1">and GPT.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.637.1">Language models are essential for understanding, generating, and interpreting human language in a computational setting, and they play a vital role in many </span><a id="_idTextAnchor289" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.638.1">applications </span><span><span class="kobospan" id="kobo.639.1">of NLP.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.640.1">Here are several motivations</span><a id="_idIndexMarker628" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.641.1"> for using </span><span><span class="kobospan" id="kobo.642.1">language models:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.643.1">Machine translation</span></strong><span class="kobospan" id="kobo.644.1">: Language models are a crucial component in systems that translate text from one language to another. </span><span class="kobospan" id="kobo.644.2">They can assess the fluency o</span><a id="_idTextAnchor290" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.645.1">f translated sentences and help choose between multiple </span><span><span class="kobospan" id="kobo.646.1">possible translations.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.647.1">Speech recognition</span></strong><span class="kobospan" id="kobo.648.1">: Language models are used in speech recognition systems to help distinguish between words and phrases that sound similar. </span><span class="kobospan" id="kobo.648.2">By predicting what word is like</span><a id="_idTextAnchor291" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.649.1">ly to come next in a sentence, they can improve the accuracy </span><span><span class="kobospan" id="kobo.650.1">of transcription.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.651.1">Information retrieval</span></strong><span class="kobospan" id="kobo.652.1">: When you search for something on the internet, language models help to determine what documents are relevant to your query. </span><span class="kobospan" id="kobo.652.2">They can under</span><a id="_idTextAnchor292" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.653.1">stand the semantic similarity between your search terms and </span><span><span class="kobospan" id="kobo.654.1">potential results.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.655.1">Text generation</span></strong><span class="kobospan" id="kobo.656.1">: Language models can generate human-like text, which is useful in various applications such as chatbots, writing assistants, and content creation tools. </span><span class="kobospan" id="kobo.656.2">For example, a chatb</span><a id="_idTextAnchor293" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.657.1">ot can use a language model to generate appropriate responses to </span><span><span class="kobospan" id="kobo.658.1">user queries.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.659.1">Sentiment analysis</span></strong><span class="kobospan" id="kobo.660.1">: By understanding the structure of language, language models can help determine whether the sentiment of a piece of text is positive, negative, or neutral. </span><span class="kobospan" id="kobo.660.2">This is useful in </span><a id="_idTextAnchor294" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.661.1">areas such as social media monitoring, product reviews, and </span><span><span class="kobospan" id="kobo.662.1">customer feedback.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.663.1">Grammar checking</span></strong><span class="kobospan" id="kobo.664.1">: Language models can predict what word should come next </span><a id="_idTextAnchor295" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.665.1">in a sentence, which can help identify grammatical errors or </span><span><span class="kobospan" id="kobo.666.1">awkward phrasing.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.667.1">Named entity recognition</span></strong><span class="kobospan" id="kobo.668.1">: Language models can help identify named entities in text, such as people, organizations, locations, and more. </span><span class="kobospan" id="kobo.668.2">This can b</span><a id="_idTextAnchor296" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.669.1">e useful for tasks such as information extraction and </span><span><span class="kobospan" id="kobo.670.1">automated summarization.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.671.1">Understanding context</span></strong><span class="kobospan" id="kobo.672.1">: Language models, especially recent models based on </span><strong class="bold"><span class="kobospan" id="kobo.673.1">DL</span></strong><span class="kobospan" id="kobo.674.1">, such as transformers, are excellent at understanding the context of words and sentences. </span><span class="kobospan" id="kobo.674.2">This capability is vital for many </span><strong class="bold"><span class="kobospan" id="kobo.675.1">NLP</span></strong><span class="kobospan" id="kobo.676.1"> tasks, such as question answering, summarization, and </span><span><span class="kobospan" id="kobo.677.1">dialogue</span></span><span><a id="_idIndexMarker629" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.678.1"> systems.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.679.1">All these motivations stem from a central theme: language models help machines understand and generate human language more effectively, which is crucial for many applications in today’s </span><span><span class="kobospan" id="kobo.680.1">data-driven world.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.681.1">In the following section, we introduce the different types of learning and </span><a id="_idTextAnchor297" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.682.1">then explain how one can use self-supervised learning to train </span><span><span class="kobospan" id="kobo.683.1">language models.</span></span></p>
<h2 id="_idParaDest-127" class="calibre7"><a id="_idTextAnchor298" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.684.1">Semi-supervised learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.685.1">Semi-supervised learning</span><a id="_idIndexMarker630" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.686.1"> is a type of ML</span><a id="_idIndexMarker631" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.687.1"> approach that utilizes both labeled and unlabeled data for training. </span><span class="kobospan" id="kobo.687.2">It is particularly useful when you have a small amount of labeled data and a large amount of unlabeled data. </span><span class="kobospan" id="kobo.687.3">The strategy here is to use the labeled data to train an initial model and then use this model to predict labels for the unlabeled data. </span><span class="kobospan" id="kobo.687.4">The model is th</span><a id="_idTextAnchor299" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.688.1">en retrained using the newly labeled data, improving its accuracy in </span><span><span class="kobospan" id="kobo.689.1">the process.</span></span></p>
<h2 id="_idParaDest-128" class="calibre7"><a id="_idTextAnchor300" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.690.1">Unsupervised learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.691.1">Unsupervised learning, on</span><a id="_idIndexMarker632" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.692.1"> the other</span><a id="_idIndexMarker633" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.693.1"> hand, involves training models entirely on unlabeled data. </span><span class="kobospan" id="kobo.693.2">The goal here is to find underlying patterns or structures in the data. </span><span class="kobospan" id="kobo.693.3">Unsupervised learning includes techniques such as clustering (where the aim is to group similar instances together) and dimensionality red</span><a id="_idTextAnchor301" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.694.1">uction (where the aim is to simplify the data without losing too </span><span><span class="kobospan" id="kobo.695.1">much information).</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.696.1">Using self-supervised learning to train language models</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.697.1">Self-supervised learning</span><a id="_idIndexMarker634" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.698.1"> is a form of unsupervised learning</span><a id="_idIndexMarker635" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.699.1"> where the data provides the supervision. </span><span class="kobospan" id="kobo.699.2">In other words, the model learns to predict certain parts of the input data from other parts of the same input data. </span><span class="kobospan" id="kobo.699.3">It does not require explicit labels provided by humans, hence the </span><span><span class="kobospan" id="kobo.700.1">term “self-supervised.”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.701.1">In the context of language models, self-supervision is typically implemented by predicting parts of a sentence when given other parts. </span><span class="kobospan" id="kobo.701.2">For example, given the sentence “The cat is on the __,” the model</span><a id="_idIndexMarker636" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.702.1"> would be trained</span><a id="_idIndexMarker637" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.703.1"> to predict the missing word (“mat,” in </span><span><span class="kobospan" id="kobo.704.1">this case).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.705.1">Let’s look at a c</span><a id="_idTextAnchor302" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.706.1">ouple of popular self-supervised learning strategies for training language </span><span><span class="kobospan" id="kobo.707.1">models next.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.708.1">Masked language modeling (MLM)</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.709.1">This strategy, used in the training</span><a id="_idIndexMarker638" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.710.1"> of BERT, randomly masks some percentage of the input tokens and tasks the model with predicting the masked words based on the context provided by the unmasked words. </span><span class="kobospan" id="kobo.710.2">For instance, in the sentence “The cat is on the mat,” we could mask “cat,” and the model’s job would be to predict this word. </span><span class="kobospan" id="kobo.710.3">Please note that more than one word can also </span><span><span class="kobospan" id="kobo.711.1">be masked.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.712.1">Mathematically, the objective of an MLM is to maximize the </span><span><span class="kobospan" id="kobo.713.1">following likelihood:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.714.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munder&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munder&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;log&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;⁡&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;|&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;;&lt;/mml:mo&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/303.png" class="calibre302"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.715.1">where </span><em class="italic"><span class="kobospan" id="kobo.716.1">w</span></em><span class="subscript"><span class="kobospan1" id="kobo.717.1">_i</span></span><span class="kobospan" id="kobo.718.1"> i</span><a id="_idTextAnchor303" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.719.1">s a masked word, </span><em class="italic"><span class="kobospan" id="kobo.720.1">w</span></em><span class="subscript"><span class="kobospan1" id="kobo.721.1">_{-i}</span></span><span class="kobospan" id="kobo.722.1"> are the non-masked words, and </span><em class="italic"><span class="kobospan" id="kobo.723.1">θ</span></em><span class="kobospan" id="kobo.724.1"> represents the </span><span><span class="kobospan" id="kobo.725.1">model parameters.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.726.1">Autoregressive language modeling</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.727.1">In autoregressive language modeling, which is used</span><a id="_idIndexMarker639" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.728.1"> in models such as GPT, the model predicts the next word in a sentence given all the preceding words. </span><span class="kobospan" id="kobo.728.2">It’s trained to maximize the likelihood of a word given its previous words in </span><span><span class="kobospan" id="kobo.729.1">the sentence.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.730.1">The objective of an autoregressive language model is </span><span><span class="kobospan" id="kobo.731.1">to maximize</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.732.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;L&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munder&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;mrow&gt;&lt;mi&gt;log&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;θ&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/304.png" class="calibre303"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.733.1">where </span><em class="italic"><span class="kobospan" id="kobo.734.1">w_</span></em><span class="subscript"><span class="kobospan1" id="kobo.735.1">i</span></span><span class="kobospan" id="kobo.736.1"> is the current word, </span><span class="kobospan" id="kobo.737.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;.&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/305.png" class="calibre304"/></span><span class="kobospan" id="kobo.738.1"> are the previous words, and </span><em class="italic"><span class="kobospan" id="kobo.739.1">θ</span></em><span class="kobospan" id="kobo.740.1"> represents the </span><span><span class="kobospan" id="kobo.741.1">model parameters.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.742.1">These strategies enable language</span><a id="_idIndexMarker640" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.743.1"> models to obtain a rich understanding of language syntax and semantics directly from raw text without the need for explicit labels. </span><span class="kobospan" id="kobo.743.2">The models can then be fine-tuned for various tasks such as text classification, sentiment analysis, and more</span><a id="_idTextAnchor304" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.744.1">, leveraging the language understanding gained from the self-supervised </span><span><span class="kobospan" id="kobo.745.1">pretraining phase.</span></span></p>
<h2 id="_idParaDest-129" class="calibre7"><a id="_idTextAnchor305" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.746.1">Transfer learning</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.747.1">Transfer learning is an ML</span><a id="_idIndexMarker641" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.748.1"> technique</span><a id="_idIndexMarker642" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.749.1"> where a pretrained model is reused as the starting point for a different but related problem. </span><span class="kobospan" id="kobo.749.2">Compared to traditional ML approaches, where you start with initializing your model with random weights, transfer learning has the advantage of kick-starting the learning process from patterns that have been learned from a related task, which can both speed up the training process and improve the performance of the model, especially when you have limited labeled </span><span><span class="kobospan" id="kobo.750.1">training data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.751.1">In transfer learning, a model is typically trained on a large-scale task, and then parts of the model are used as a starting point for another task. </span><span class="kobospan" id="kobo.751.2">The large-scale task is often chosen to be broad enough that the learned representations are useful for many different tasks. </span><span class="kobospan" id="kobo.751.3">This process works particularly well when the input data for both tasks are of the same type and the tasks </span><span><span class="kobospan" id="kobo.752.1">are related.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.753.1">There are several ways to apply transfer learning, and the best approach can depend on how much data you ha</span><a id="_idTextAnchor306" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.754.1">ve for your task and how similar your task is to the original task the model was </span><span><span class="kobospan" id="kobo.755.1">trained on.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.756.1">Feature extraction</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.757.1">The pretrained model acts</span><a id="_idIndexMarker643" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.758.1"> as a feature extractor. </span><span class="kobospan" id="kobo.758.2">You remove the last layer or several layers of the model, leaving the rest of the network intact. </span><span class="kobospan" id="kobo.758.3">Then, you pass your data through this truncated mode</span><a id="_idTextAnchor307" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.759.1">l and use the output as input to a new, smaller model that is trained for your </span><span><span class="kobospan" id="kobo.760.1">specific task.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.761.1">Fine-tuning</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.762.1">You use the pretrained model as a starting</span><a id="_idIndexMarker644" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.763.1"> point and update all or some of the model’s parameters for your new task. </span><span class="kobospan" id="kobo.763.2">In other words, you continue the training where it left off, allowing the model to adjust from generic feature extraction to features more specific to your task. </span><span class="kobospan" id="kobo.763.3">Often, a lower learning rate is used during fine-tuning to avoid overwriting the prelearned features entirely </span><span><span class="kobospan" id="kobo.764.1">during training.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.765.1">Transfer learning is a powerful technique that can be used to improve the performance of ML models. </span><span class="kobospan" id="kobo.765.2">It is particularly useful for tasks where there are limited labeled data available. </span><span class="kobospan" id="kobo.765.3">It is commonly used in DL applications. </span><span class="kobospan" id="kobo.765.4">For instance, it’s almost a standard in image classification problems where pretrained models on ImageNet, a large-scale annotated image dataset (ResNet, VGG, Inception, and so on), are used as the starting point. </span><span class="kobospan" id="kobo.765.5">The features learned by these models are generic for image classification and can be fine-tuned on a specific image classification task with a smaller amount </span><span><span class="kobospan" id="kobo.766.1">of data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.767.1">Here are some examples</span><a id="_idIndexMarker645" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.768.1"> of how transfer learning can </span><span><span class="kobospan" id="kobo.769.1">be used:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.770.1">A model trained to classify images of cats and dogs can be used to fine-tune a model to classify images of other animals, such as birds </span><span><span class="kobospan" id="kobo.771.1">or fish</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.772.1">A model trained to translate text from English to Spanish can be used to fine-tune a model to translate text from Spanish </span><span><span class="kobospan" id="kobo.773.1">to French</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.774.1">A model trained to predict the price of a house can be used to fine-tune a model to predict the price of </span><span><span class="kobospan" id="kobo.775.1">a car</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.776.1">Similarly, in natural language processing, large pretrained models, such as BERT or GPT, are often used as the starting </span><a id="_idIndexMarker646" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.777.1">point for a wide range of tasks. </span><span class="kobospan" id="kobo.777.2">These models are pretrained on a large corpus of text and learn a rich representation of language that can be fine-tuned f</span><a id="_idTextAnchor308" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.778.1">or specific tasks such as text classification, sentiment analysis, question answering, </span><span><span class="kobospan" id="kobo.779.1">and more.</span></span></p>
<h1 id="_idParaDest-130" class="calibre4"><a id="_idTextAnchor309" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.780.1">Understanding transformers</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.781.1">Transformers are a type</span><a id="_idIndexMarker647" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.782.1"> of neural network architecture that was introduced in a paper called </span><em class="italic"><span class="kobospan" id="kobo.783.1">Attention is All You Need</span></em><span class="kobospan" id="kobo.784.1"> by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. </span><span class="kobospan" id="kobo.784.2">Gomez, Łukasz Kaiser, and Illia Polosukhin (</span><em class="italic"><span class="kobospan" id="kobo.785.1">Advances in neural information processing systems 30</span></em><span class="kobospan" id="kobo.786.1"> (2017),  Harvard). </span><span class="kobospan" id="kobo.786.2">They have been very influential in the field of NLP and have formed the basis for state-of-the-art models such as BERT </span><span><span class="kobospan" id="kobo.787.1">and GPT.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.788.1">The key innovation in transformers is the self-attention mechanism, which allows the model to weigh the relevance of each word in the input when producing an output, thereby considering the context of each word. </span><span class="kobospan" id="kobo.788.2">This is unlike previous models such as RNNs or RNNs, which process the input seq</span><a id="_idTextAnchor310" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.789.1">uentially and, therefore, have a harder time capturing the long-range dependencies</span><a id="_idIndexMarker648" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.790.1">between words.</span></span></p>
<h2 id="_idParaDest-131" class="calibre7"><a id="_idTextAnchor311" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.791.1">Architecture of transformers</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.792.1">A transformer is composed</span><a id="_idIndexMarker649" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.793.1"> of an encoder and a decoder, both of which are made up of several identical layers, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.794.1">Figure 6</span></em></span><em class="italic"><span class="kobospan" id="kobo.795.1">.8</span></em><span class="kobospan" id="kobo.796.1">. </span><span class="kobospan" id="kobo.796.2">Each layer in the encoder contains two sub-layers: a self-attention mechanism and a position-wise fully connected feedforward network. </span><span class="kobospan" id="kobo.796.3">A residual connection is employed around each of the two sub-layers, followed by </span><span><span class="kobospan" id="kobo.797.1">layer normalization:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer343">
<span class="kobospan" id="kobo.798.1"><img alt="Figure 6.8 – Self-attention mechanism" src="image/B18949_06_008.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.799.1">Figure 6.8 – Self-attention mechanism</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.800.1">Similarly, each layer in the decoder has three sub-layers. </span><span class="kobospan" id="kobo.800.2">The first is a self-attention layer, the second is a cross-attention layer that attends to the output of the encoder stack, and the third is a position-wise fully connected feedforward network. </span><span class="kobospan" id="kobo.800.3">Like the encoder, each of these sub-layers has a residual connection around it, followed by layer normalization. </span><span class="kobospan" id="kobo.800.4">Please</span><a id="_idIndexMarker650" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.801.1"> note that in the </span><a id="_idTextAnchor312" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.802.1">figure, just one head is being shown, and we can have multiple heads working in parallel (</span><span><em class="italic"><span class="kobospan" id="kobo.803.1">N</span></em></span><span><span class="kobospan" id="kobo.804.1"> heads).</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.805.1">Self-attention mechanism</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.806.1">The self-attention</span><a id="_idIndexMarker651" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.807.1"> mechanism, or scaled dot-product attention, calculates</span><a id="_idIndexMarker652" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.808.1"> the relevance</span><a id="_idIndexMarker653" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.809.1"> of each word in the sequence to the current word being processed. </span><span class="kobospan" id="kobo.809.2">The input to the self-attention layer is a sequence of word embeddings, each of which is split into a </span><strong class="bold"><span class="kobospan" id="kobo.810.1">query</span></strong><span class="kobospan" id="kobo.811.1"> (</span><em class="italic"><span class="kobospan" id="kobo.812.1">Q</span></em><span class="kobospan" id="kobo.813.1">), a </span><strong class="bold"><span class="kobospan" id="kobo.814.1">key</span></strong><span class="kobospan" id="kobo.815.1"> (</span><em class="italic"><span class="kobospan" id="kobo.816.1">K</span></em><span class="kobospan" id="kobo.817.1">), and a </span><strong class="bold"><span class="kobospan" id="kobo.818.1">value</span></strong><span class="kobospan" id="kobo.819.1"> (</span><em class="italic"><span class="kobospan" id="kobo.820.1">V</span></em><span class="kobospan" id="kobo.821.1">) using separately learned </span><span><span class="kobospan" id="kobo.822.1">linear transformations.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.823.1">The attention score for each word is then calculated </span><span><span class="kobospan" id="kobo.824.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.825.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;Q&lt;/mi&gt;&lt;mi&gt;K&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;T&lt;/mi&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;_&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;k&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/306.png" class="calibre305"/></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.826.1">Where </span><em class="italic"><span class="kobospan" id="kobo.827.1">d_k</span></em><span class="kobospan" id="kobo.828.1"> is the dimensionality of the queries and keys, which is used to scale the dot product to prevent it from growing too large. </span><span class="kobospan" id="kobo.828.2">The softmax operation ensures that the attention scores are normalized and sum to 1. </span><span class="kobospan" id="kobo.828.3">These scores represent the weight given to each word’s value when producing the output for the </span><span><span class="kobospan" id="kobo.829.1">current word.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.830.1">The output of the self-attention layer is a new sequence of vectors, where the output for eac</span><a id="_idTextAnchor313" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.831.1">h word is a weighted sum of all the input values, with the weights determined by the </span><span><span class="kobospan" id="kobo.832.1">attention scores.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.833.1">Positional encoding</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.834.1">Since the self-attention mechanism</span><a id="_idIndexMarker654" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.835.1"> does not take into</span><a id="_idIndexMarker655" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.836.1"> account the position of the words in the sequence, the transformer adds a positional encoding to the input embeddings at the bottom of the encoder and decoder stacks. </span><span class="kobospan" id="kobo.836.2">This encoding is a fixed function of the position and allows the model to learn to use the order of </span><span><span class="kobospan" id="kobo.837.1">the words.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.838.1">In the original transformer paper, positional encoding is a sinusoidal function o</span><a id="_idTextAnchor314" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.839.1">f the position and the dimension, although learned positional encodings</span><a id="_idIndexMarker656" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.840.1"> have also been</span><a id="_idIndexMarker657" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.841.1">used effectively.</span></span></p>
<h2 id="_idParaDest-132" class="calibre7"><a id="_idTextAnchor315" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.842.1">Applications of transformers</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.843.1">Since their introduction, transformers</span><a id="_idIndexMarker658" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.844.1"> have been used to achieve state-of-the-art results on a wide range of NLP tasks, including machine translation, text summarization, sentiment analysis, and more. </span><span class="kobospan" id="kobo.844.2">They have also been adapted for other domains, such as computer vision and </span><span><span class="kobospan" id="kobo.845.1">reinforcement learning.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.846.1">The introduction of transformers has led to a shift in the NLP field towards pretraining large transformer models on a large corpus of text and then fine-tuning them on specific tasks, which is an effect</span><a id="_idTextAnchor316" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.847.1">ive form of transfer learning. </span><span class="kobospan" id="kobo.847.2">This approach has been used in models such as BERT, GPT-2, GPT-3, </span><span><span class="kobospan" id="kobo.848.1">and GPT-4.</span></span></p>
<h1 id="_idParaDest-133" class="calibre4"><a id="_idTextAnchor317" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.849.1">Learning more about large language models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.850.1">Large language models</span><a id="_idIndexMarker659" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.851.1"> are a class of ML models that have been trained on a broad range of </span><span><span class="kobospan" id="kobo.852.1">internet text.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.853.1">The term “large” in “large language models” refers to the number of parameters that these models have. </span><span class="kobospan" id="kobo.853.2">For example, GPT-3 has 175 billion parameters. </span><span class="kobospan" id="kobo.853.3">These models are trained using self-supervised learning on a large corpus of text, which means they predict the next word in a sentence (such as GPT) or a word based on surrounding words (such as BERT, which is also trained to predict whether a pair of sentences is sequential). </span><span class="kobospan" id="kobo.853.4">Because they are exposed to such a large amount of text, these models learn grammar, facts about the world, reasoning abilities, and also biases in the data they’re </span><span><span class="kobospan" id="kobo.854.1">trained on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.855.1">These models are transformer-based, meaning they leverage the transformer architecture, which uses self-attention mechanisms to weigh the importance of words in input data. </span><span class="kobospan" id="kobo.855.2">This architecture allows these models to process long-range dependencies in text, making them very effective for a wide range of </span><span><span class="kobospan" id="kobo.856.1">NLP tasks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.857.1">Large language models can be fine-tuned on specific tasks to achieve high performance. </span><span class="kobospan" id="kobo.857.2">Fine-tuning involves additional training on a smaller, task-specific dataset and allows the model to adapt its general language understanding abilities to the specifics of the task. </span><span class="kobospan" id="kobo.857.3">This approach has been used to achieve state-of-the-art results on many </span><span><span class="kobospan" id="kobo.858.1">NLP benchmarks.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.859.1">While large language models have demonstrated impressive abilities, they also raise important challenges. </span><span class="kobospan" id="kobo.859.2">For example, because they’re trained on internet text, they can reproduce and amplify biases present in the data. </span><span class="kobospan" id="kobo.859.3">They can also generate outputs that are harmful or misleading. </span><span class="kobospan" id="kobo.859.4">Additionally, due to their size, these models require significant computational resources to train and deploy, which raises issues around cost and </span><span><span class="kobospan" id="kobo.860.1">environmental impact.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.861.1">Despite these challenges, large</span><a id="_idIndexMarker660" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.862.1"> language models represent a significant advance in the field of AI and are a powerful tool for a wide</span><a id="_idTextAnchor318" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.863.1"> range of applications, including translation, summarization, content creation, question answering, </span><span><span class="kobospan" id="kobo.864.1">and more.</span></span></p>
<h1 id="_idParaDest-134" class="calibre4"><a id="_idTextAnchor319" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.865.1">The challenges of training language models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.866.1">Training large language mo</span><a id="_idTextAnchor320" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.867.1">dels</span><a id="_idIndexMarker661" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.868.1"> is a complex and resource-intensive task that poses several challenges. </span><span class="kobospan" id="kobo.868.2">Here are some of the </span><span><span class="kobospan" id="kobo.869.1">key issues:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.870.1">Computational resources</span></strong><span class="kobospan" id="kobo.871.1">: The training of large language models requires substantial computational resources. </span><span class="kobospan" id="kobo.871.2">These models have billions of parameters that need to be updated during training, which involves performing a large amount of computation over an extensive dataset. </span><span class="kobospan" id="kobo.871.3">This computation is usually carrie</span><a id="_idTextAnchor321" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.872.1">d out on high-performance GPUs or </span><strong class="bold"><span class="kobospan" id="kobo.873.1">tensor processing units</span></strong><span class="kobospan" id="kobo.874.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.875.1">TPUs</span></strong><span class="kobospan" id="kobo.876.1">), and the costs associated can </span><span><span class="kobospan" id="kobo.877.1">be prohibitive.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.878.1">Memory limitations</span></strong><span class="kobospan" id="kobo.879.1">: As the size of the model increases, the amount of memory required to store the model parameters, intermediate activations, and gradients during training also increases. </span><span class="kobospan" id="kobo.879.2">This can lead to memory issues on even the most advanced hardware. </span><span class="kobospan" id="kobo.879.3">Techniques such as model parallelism, gradient checkp</span><a id="_idTextAnchor322" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.880.1">ointing, and offloading can be used to mitigate these issues, but they add complexity to the </span><span><span class="kobospan" id="kobo.881.1">training process.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.882.1">Dataset size and quality</span></strong><span class="kobospan" id="kobo.883.1">: Large language models are trained on extensive text corpora. </span><span class="kobospan" id="kobo.883.2">Finding, cleaning, and structurally organizing such massive datasets can be challenging. </span><span class="kobospan" id="kobo.883.3">Moreover, the quality of the dataset directly impacts the performance of the model. </span><span class="kobospan" id="kobo.883.4">Since these models l</span><a id="_idTextAnchor323" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.884.1">earn from the data they’re trained on, biases or errors in the data can lead to a biased or </span><span><span class="kobospan" id="kobo.885.1">error-prone model.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.886.1">Overfitting</span></strong><span class="kobospan" id="kobo.887.1">: While large models have a high capacity to learn complex patterns, they can also be overfitted to the training data, especially when the amount of available data is limited compared to the size of the model. </span><span class="kobospan" id="kobo.887.2">Overfitting leads to poor generalization of unseen data. </span><span class="kobospan" id="kobo.887.3">Re</span><a id="_idTextAnchor324" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.888.1">gularization techniques, such as weight decay, dropout, and early stopping, can be used to </span><span><span class="kobospan" id="kobo.889.1">combat overfitting.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.890.1">Training stability</span></strong><span class="kobospan" id="kobo.891.1">: As models get larger, stably training them becomes more difficult. </span><span class="kobospan" id="kobo.891.2">The challenges incl</span><a id="_idTextAnchor325" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.892.1">ude managing learning rates and batch sizes and dealing with issues such as vanishing or </span><span><span class="kobospan" id="kobo.893.1">exploding gradients.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.894.1">Evaluation and fine-tuning</span></strong><span class="kobospan" id="kobo.895.1">: Evaluating the performance of these models can also be challenging due to their size. </span><span class="kobospan" id="kobo.895.2">Moreover, fine-tuning these models on a specific task </span><a id="_idTextAnchor326" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.896.1">can be tricky, as it can lead to “catastrophic forgetting,” where the model forgets the </span><span><span class="kobospan" id="kobo.897.1">pretraining knowledge.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.898.1">Ethical and safety concerns</span></strong><span class="kobospan" id="kobo.899.1">: Large language models can generate content that is harmful or inappropriate. </span><span class="kobospan" id="kobo.899.2">They can also propagate and amplify biases present in the training data. </span><span class="kobospan" id="kobo.899.3">These issues necessitate the development of robust methods to control</span><a id="_idIndexMarker662" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.900.1"> the behavior of the model, both during training and </span><span><span class="kobospan" id="kobo.901.1">at runtime.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.902.1">Despite these challenges, progress continues in the field of large language models. </span><span class="kobospan" id="kobo.902.2">Researchers are </span><a id="_idTextAnchor327" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.903.1">developing new strategies to mitigate these issues and to train large models more effectively </span><span><span class="kobospan" id="kobo.904.1">and responsibly.</span></span></p>
<h2 id="_idParaDest-135" class="calibre7"><span class="kobospan" id="kobo.905.1">Specific designs of langua</span><a id="_idTextAnchor328" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.906.1">ge models</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.907.1">Here, we are going to explain two popular architectures of language models, BERT and GPT, </span><span><span class="kobospan" id="kobo.908.1">in detail.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.909.1">BERT</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.910.1">BERT, which we mentioned</span><a id="_idIndexMarker663" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.911.1"> already and will </span><a id="_idIndexMarker664" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.912.1">now expand on, is a transformer-based ML technique for NLP tasks. </span><span class="kobospan" id="kobo.912.2">It was developed by Google and introduced in a paper by Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova titled </span><em class="italic"><span class="kobospan" id="kobo.913.1">Bert: Pre-training of deep bidirectional transformers for language understanding</span></em><span class="kobospan" id="kobo.914.1">, arXiv preprint </span><span><span class="kobospan" id="kobo.915.1">arXiv:1810.04805 (2018).</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.916.1">BERT is designed to pretrain deep bidirectional representations from the unlabeled text by joint conditioning on both left and right contexts in all layers. </span><span class="kobospan" id="kobo.916.2">This is in contrast to previous methods, such as GPT and ELMo, which pretrain text representations from only the left context or from left and right contexts separately.</span><a id="_idTextAnchor329" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.917.1"> This bi-directionality allows BERT to understand the context and the semantic meaning of a word </span><span><span class="kobospan" id="kobo.918.1">more accurately.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.919.1">BERT’s design</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.920.1">BERT is based on the transformer</span><a id="_idIndexMarker665" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.921.1"> model architecture, which is shown in </span><span><em class="italic"><span class="kobospan" id="kobo.922.1">Figure 6</span></em></span><em class="italic"><span class="kobospan" id="kobo.923.1">.8</span></em><span class="kobospan" id="kobo.924.1">, originally introduced by Vaswani et al. </span><span class="kobospan" id="kobo.924.2">in the paper </span><em class="italic"><span class="kobospan" id="kobo.925.1">Attention is All You Need</span></em><span class="kobospan" id="kobo.926.1">. </span><span class="kobospan" id="kobo.926.2">The model architecture consists of stacked self-attention and point-wise fully </span><span><span class="kobospan" id="kobo.927.1">connected layers.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.928.1">BERT comes in two sizes: </span><strong class="bold"><span class="kobospan" id="kobo.929.1">BERT Base</span></strong><span class="kobospan" id="kobo.930.1"> and </span><strong class="bold"><span class="kobospan" id="kobo.931.1">BERT Large</span></strong><span class="kobospan" id="kobo.932.1">. </span><span class="kobospan" id="kobo.932.2">BERT Base is composed of 12 transformer</span><a id="_idIndexMarker666" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.933.1"> layers, each</span><a id="_idIndexMarker667" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.934.1"> with 12 self-attention heads, and a total of 110 million parameters. </span><span class="kobospan" id="kobo.934.2">BERT Large is much bigger and has 24 transformer layers, each with 16 self-attention heads, for a total of 340 </span><span><span class="kobospan" id="kobo.935.1">million parameters.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.936.1">BERT’s training process involves two steps: </span><strong class="bold"><span class="kobospan" id="kobo.937.1">pretraining</span></strong> <span><span class="kobospan" id="kobo.938.1">and </span></span><span><strong class="bold"><span class="kobospan" id="kobo.939.1">fine-tuning</span></strong></span><span><span class="kobospan" id="kobo.940.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.941.1">The very first step in trai</span><a id="_idTextAnchor330" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.942.1">ning or using a language model is to create or load its dictionary. </span><span class="kobospan" id="kobo.942.2">We usually use a tokenizer to achieve </span><span><span class="kobospan" id="kobo.943.1">this goal.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.944.1">Tokenizer</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.945.1">In order to use the language</span><a id="_idIndexMarker668" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.946.1"> models efficiently, we need to use a tokenizer that converts the input</span><a id="_idIndexMarker669" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.947.1"> text into a limited number</span><a id="_idIndexMarker670" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.948.1"> of tokens. </span><span class="kobospan" id="kobo.948.2">Subword tokenization algorithms, such as </span><strong class="bold"><span class="kobospan" id="kobo.949.1">byte pair encoding</span></strong><span class="kobospan" id="kobo.950.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.951.1">BPE</span></strong><span class="kobospan" id="kobo.952.1">), </span><strong class="bold"><span class="kobospan" id="kobo.953.1">unigram language model</span></strong><span class="kobospan" id="kobo.954.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.955.1">ULM</span></strong><span class="kobospan" id="kobo.956.1">), and </span><strong class="bold"><span class="kobospan" id="kobo.957.1">WordPiece</span></strong><span class="kobospan" id="kobo.958.1">, split words into smaller</span><a id="_idIndexMarker671" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.959.1"> subword units. </span><span class="kobospan" id="kobo.959.2">This is useful for handling out-of-vocabulary words and allows the model to learn meaningful representations for subword parts that often carry </span><span><span class="kobospan" id="kobo.960.1">semantic meaning.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.961.1">The BERT tokenizer is a critical component of the BERT model, performing the initial preprocessing of text data necessary for input into the model. </span><span class="kobospan" id="kobo.961.2">BERT uses WordPiece tokenization, a subword tokenization algorithm that breaks words into smaller parts, allowing BERT</span><a id="_idIndexMarker672" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.962.1"> to handle out-of-vocabulary words, reduce the size of the vocabulary, and deal with the richness and diversity </span><span><span class="kobospan" id="kobo.963.1">of languages.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.964.1">Here’s a detailed breakdown of how the BERT </span><span><span class="kobospan" id="kobo.965.1">tokenizer works:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.966.1">Basic tokenization</span></strong><span class="kobospan" id="kobo.967.1">: First, the BERT tokenizer performs basic tokenization, breaking text into individual words by splitting on whitespace and punctuation. </span><span class="kobospan" id="kobo.967.2">This is similar to what you might find in other </span><span><span class="kobospan" id="kobo.968.1">tokenization methods.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.969.1">WordPiece tokenization</span></strong><span class="kobospan" id="kobo.970.1">: After basic tokenization, the BERT tokenizer applies WordPiece tokenization. </span><span class="kobospan" id="kobo.970.2">This step breaks words into smaller subword units or “WordPieces.” </span><span class="kobospan" id="kobo.970.3">If a word isn’t in the BERT vocabulary, the tokenizer will iteratively break the word down into smaller sub words until it finds a match in the vocabulary or until it has to resort to </span><span><span class="kobospan" id="kobo.971.1">character-level representation.</span></span><p class="calibre6"><span class="kobospan" id="kobo.972.1">For example, the word “unhappiness” might be broken down into two WordPieces: “un” and “##happiness”. </span><span class="kobospan" id="kobo.972.2">The “##” symbol is used to denote sub-words that are part of a larger word and not a whole word on </span><span><span class="kobospan" id="kobo.973.1">their own.</span></span></p></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.974.1">Special tokens addition</span></strong><span class="kobospan" id="kobo.975.1">: The </span><strong class="bold"><span class="kobospan" id="kobo.976.1">BERT</span></strong><span class="kobospan" id="kobo.977.1"> tokenizer then adds special tokens necessary for specific </span><strong class="bold"><span class="kobospan" id="kobo.978.1">BERT</span></strong><span class="kobospan" id="kobo.979.1"> functionalities. </span><span class="kobospan" id="kobo.979.2">The [</span><strong class="source-inline1"><span class="kobospan" id="kobo.980.1">CLS</span></strong><span class="kobospan" id="kobo.981.1">] token is appended at the beginning of each sentence, serving as an aggregate representation for classification tasks. </span><span class="kobospan" id="kobo.981.2">The [</span><strong class="source-inline1"><span class="kobospan" id="kobo.982.1">SEP</span></strong><span class="kobospan" id="kobo.983.1">] token is added at the end of each sentence to signify sentence boundaries. </span><span class="kobospan" id="kobo.983.2">If two sentences are inputted (for tasks that require sentence pairs), they are separated by this [</span><span><strong class="source-inline1"><span class="kobospan" id="kobo.984.1">SEP</span></strong></span><span><span class="kobospan" id="kobo.985.1">] token.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.986.1">Token to ID conversion</span></strong><span class="kobospan" id="kobo.987.1">: Finally, each token is mapped to an integer ID corresponding to its index in the </span><strong class="bold"><span class="kobospan" id="kobo.988.1">BERT</span></strong><span class="kobospan" id="kobo.989.1"> vocabulary. </span><span class="kobospan" id="kobo.989.2">These IDs are what the </span><strong class="bold"><span class="kobospan" id="kobo.990.1">BERT</span></strong><span class="kobospan" id="kobo.991.1"> model actually uses </span><span><span class="kobospan" id="kobo.992.1">as input.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.993.1">So, in summary, the BERT tokenizer works by first tokenizing the text into words, then further breaking these words down into WordPieces (if necessary), adding special tokens, and finally converting these tokens into IDs. </span><span class="kobospan" id="kobo.993.2">This process allows the model to understand and generate</span><a id="_idIndexMarker673" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.994.1"> meaningful re</span><a id="_idTextAnchor331" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.995.1">presentations for a wide variety of words and sub-words, contributing to BERT’s powerful performance on various </span><span><span class="kobospan" id="kobo.996.1">NLP tasks.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.997.1">Pretraining</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.998.1">During pretraining, </span><strong class="bold"><span class="kobospan" id="kobo.999.1">BERT</span></strong><span class="kobospan" id="kobo.1000.1"> was trained on a large</span><a id="_idIndexMarker674" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1001.1"> corpus of text (the entire English Wikipedia and BooksCorpus are used in the original paper). </span><span class="kobospan" id="kobo.1001.2">The model was trained to predict masked words in a sentence (masked language model) and to distinguish whether two sentences come in order in the text (next sentence prediction), as </span><span><span class="kobospan" id="kobo.1002.1">explained here:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1003.1">Masked language model</span></strong><span class="kobospan" id="kobo.1004.1">: In this task, 15% of the words in a sentence are replaced by a [</span><strong class="source-inline1"><span class="kobospan" id="kobo.1005.1">MASK</span></strong><span class="kobospan" id="kobo.1006.1">] token, and the model is trained to predict the original word from the context provided by the </span><span><span class="kobospan" id="kobo.1007.1">non-masked words.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1008.1">Next sentence prediction</span></strong><span class="kobospan" id="kobo.1009.1">: When the model is</span><a id="_idTextAnchor332" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1010.1"> given a pair of two sentences, it is also trained to predict whether sentence </span><em class="italic"><span class="kobospan" id="kobo.1011.1">B</span></em><span class="kobospan" id="kobo.1012.1"> is the next sentence following </span><span><span class="kobospan" id="kobo.1013.1">sentence </span></span><span><em class="italic"><span class="kobospan" id="kobo.1014.1">A</span></em></span><span><span class="kobospan" id="kobo.1015.1">.</span></span></li>
</ul>
<h4 class="calibre135"><span class="kobospan" id="kobo.1016.1">Fine-tuning</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1017.1">After pretraining, BERT</span><a id="_idIndexMarker675" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1018.1"> can be fine-tuned on a specific task with a significantly smaller amount of training data. </span><span class="kobospan" id="kobo.1018.2">Fine-tuning involves adding an additional output layer to BERT and training the entire model end-to-end on the specific task. </span><span class="kobospan" id="kobo.1018.3">This approach has been shown to achieve state-of-the-art results on a wide range of NLP tasks, including question answering, named entity recognition, sentiment analysis, </span><span><span class="kobospan" id="kobo.1019.1">and more.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1020.1">BERT’s design</span><a id="_idIndexMarker676" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1021.1"> and its pretraining/fine-tuning approach revolutionized the field of NL</span><a id="_idTextAnchor333" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1022.1">P and have led to a shift toward training large models on a broad range of data and then fine-tuning them on </span><span><span class="kobospan" id="kobo.1023.1">specific tasks.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1024.1">How to fine-tune BERT for text classification</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1025.1">As mentioned, BERT has been pretrained</span><a id="_idIndexMarker677" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1026.1"> on a large corpus of text data, and the learned</span><a id="_idIndexMarker678" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1027.1"> representations can be fine-tuned for </span><a id="_idTextAnchor334" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1028.1">specific tasks, including text classification. </span><span class="kobospan" id="kobo.1028.2">Here is a step-by-step process on how to fine-tune BERT for </span><span><span class="kobospan" id="kobo.1029.1">text classification:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1030.1">Preprocessing input data</span></strong><span class="kobospan" id="kobo.1031.1">: BERT requires a specific format for input data. </span><span class="kobospan" id="kobo.1031.2">The sentences need to be tokenized into sub-words using BERT’s own tokenizer, and special tokens such as [CLS] (classification) and [SEP] (separation) need to be added. </span><span class="kobospan" id="kobo.1031.3">The [CLS] token is added at the beginning of each example and is used as the aggregate sequence representation for classification tasks. </span><span class="kobospan" id="kobo.1031.4">The [SEP] token is added at t</span><a id="_idTextAnchor335" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1032.1">he end of each sentence to denote sentence boundaries. </span><span class="kobospan" id="kobo.1032.2">All sequences are then padded to a fixed length to form a </span><span><span class="kobospan" id="kobo.1033.1">uniform input.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1034.1">Loading the pretrained BERT model</span></strong><span class="kobospan" id="kobo.1035.1">: BERT has several pretrained models, and the right one should be chosen based on the task at hand. </span><span class="kobospan" id="kobo.1035.2">The models differ in terms of the size of the model and the language of the pretrain</span><a id="_idTextAnchor336" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1036.1">ing data. </span><span class="kobospan" id="kobo.1036.2">Once the pretrained BERT model is loaded, it can be used to create contextualized word embeddings for the </span><span><span class="kobospan" id="kobo.1037.1">input data.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1038.1">Adding a classification layer</span></strong><span class="kobospan" id="kobo.1039.1">: A classification layer, also known as the classification head, is added</span><a id="_idIndexMarker679" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1040.1"> on top of the pretrained BERT model. </span><span class="kobospan" id="kobo.1040.2">This layer will be trained to make predictions for the text classification task. </span><span class="kobospan" id="kobo.1040.3">Usually, this layer is a fully connected neural network layer th</span><a id="_idTextAnchor337" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1041.1">at takes the representation corresponding to the [CLS] token as input and outputs the probability distribution over </span><span><span class="kobospan" id="kobo.1042.1">the classes.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1043.1">Fine-tuning the model</span></strong><span class="kobospan" id="kobo.1044.1">: Fine-tuning involves training the model on the specific task (in this case, text classification) using the labeled data. </span><span class="kobospan" id="kobo.1044.2">This process can be done in multiple ways. </span><span class="kobospan" id="kobo.1044.3">The more common approach is to update the weights of the pretrained BERT model and the newly added classification layer to minimize a loss function, typically the cross-entropy loss for classification tasks. </span><span class="kobospan" id="kobo.1044.4">It is important to use a lower learning rate during fine-tuning, as larger rates can destabilize the prelearned weights. </span><span class="kobospan" id="kobo.1044.5">Additionally, the number of recommended epochs is two to four, so the model learns the task but does not overfit. </span><span class="kobospan" id="kobo.1044.6">The benefit of this approach is that the model weights will be adj</span><a id="_idTextAnchor338" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1045.1">usted to perform well on specific tasks. </span><span class="kobospan" id="kobo.1045.2">Alternatively, we can freeze BERT layers and just update the classifier </span><span><span class="kobospan" id="kobo.1046.1">layer weights.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1047.1">Evaluating the model</span></strong><span class="kobospan" id="kobo.1048.1">: Once the model has been fine-tuned, it can be evaluated on a validation set to assess its performance. </span><span class="kobospan" id="kobo.1048.2">This involves calculating metrics such as accuracy, precision, recall,</span><a id="_idTextAnchor339" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1049.1"> and F1 score. </span><span class="kobospan" id="kobo.1049.2">During the training and evaluation task, similar to other ML and DL models, we can perform </span><span><span class="kobospan" id="kobo.1050.1">hyperparameter tuning.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.1051.1">Applying the model</span></strong><span class="kobospan" id="kobo.1052.1">: The fine-tuned model can now be used to make predictions on new, unseen text data. </span><span class="kobospan" id="kobo.1052.2">As with the training data, this new data also need to be preprocessed into the format that </span><span><span class="kobospan" id="kobo.1053.1">BERT expects.</span></span></li>
</ol>
<p class="callout-heading"><span class="kobospan" id="kobo.1054.1">Important note</span></p>
<p class="callout"><span class="kobospan" id="kobo.1055.1">Note that working with </span><strong class="bold"><span class="kobospan" id="kobo.1056.1">BERT</span></strong><span class="kobospan" id="kobo.1057.1"> requires considerable computational resources, as the model has a large number of parameters. </span><span class="kobospan" id="kobo.1057.2">A GPU is typically recommended for fine-tuning and applying BERT models. </span><span class="kobospan" id="kobo.1057.3">There are some models that are lighter than BERT with slightly lower performance, such as DistilBERT, that we can use in the case of being constrained by the computation or memory resources. </span><span class="kobospan" id="kobo.1057.4">Additionally, BERT is able to process 512 tokens, which limits the length of our input text. </span><span class="kobospan" id="kobo.1057.5">If we want to process longer text, Longformer or BigBird are good choices. </span><span class="kobospan" id="kobo.1057.6">What we explained here works for similar language models such as RoBERTa, XLNet, and </span><span><span class="kobospan" id="kobo.1058.1">so on.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1059.1">In summary, fine-tuning</span><a id="_idIndexMarker680" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1060.1"> BERT</span><a id="_idIndexMarker681" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1061.1"> for text classification involves preprocessing the input data, loading the pretrained BERT model, adding a classification layer, fine-tuning the model on the labeled data, and then evaluating</span><a id="_idIndexMarker682" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1062.1"> and applying</span><a id="_idIndexMarker683" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.1063.1">the model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1064.1">We will demonstrate the preceding paradigm of fine-tuning </span><a id="_idTextAnchor340" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1065.1">BERT and then apply it at the end of this chapter. </span><span class="kobospan" id="kobo.1065.2">You will have the opportunity to employ it firsthand and adjust it to </span><span><span class="kobospan" id="kobo.1066.1">your needs.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1067.1">GPT-3</span></h3>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.1068.1">GPT-3</span></strong><span class="kobospan" id="kobo.1069.1">, short for </span><strong class="bold"><span class="kobospan" id="kobo.1070.1">generative</span></strong><strong class="bold"><a id="_idIndexMarker684" class="calibre5 pcalibre1 pcalibre"/></strong><strong class="bold"><span class="kobospan" id="kobo.1071.1"> pretrained transformer 3</span></strong><span class="kobospan" id="kobo.1072.1">, is an</span><a id="_idIndexMarker685" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1073.1"> autoregressive language model developed by OpenAI that uses DL techniques to generate human-like text. </span><span class="kobospan" id="kobo.1073.2">It is the third version of the GPT series. </span><a id="_idTextAnchor341" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1074.1">The GPT versions that followed it, GPT-3.5 and GPT-4, will be covered in the next chapter, as we will expand on large </span><span><span class="kobospan" id="kobo.1075.1">language models.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1076.1">Design and architecture of GPT-3</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1077.1">GPT-3 extends the transformer</span><a id="_idIndexMarker686" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1078.1"> model architecture</span><a id="_idIndexMarker687" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1079.1"> used by its predecessors. </span><span class="kobospan" id="kobo.1079.2">The architecture is based on a transformer model that uses layers of transformer blocks, where each block is composed of self-attention and feedforward neural </span><span><span class="kobospan" id="kobo.1080.1">network layers.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1081.1">GPT-3 is massive compared to the previous versions. </span><span class="kobospan" id="kobo.1081.2">It consists of 175 billion ML parameters. </span><span class="kobospan" id="kobo.1081.3">These parameters are learned during the training phase, where the model learns to predict the next word in a sequence </span><span><span class="kobospan" id="kobo.1082.1">of words.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1083.1">GPT-3’s transformer model is designed to process sequences of data (in this case, sequences of words or tokens in text), making it well-suited for language tasks. </span><span class="kobospan" id="kobo.1083.2">It processes input data sequentially from left to right and generates predictions for the next item in the sequence. </span><span class="kobospan" id="kobo.1083.3">This is the difference between BERT and GPT, where, in BERT, words from both sides are used </span><a id="_idTextAnchor342" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1084.1">to predict masked words, but in GPT, just the previous words are used for prediction, which makes it a good choice for </span><span><span class="kobospan" id="kobo.1085.1">generative tasks.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1086.1">Pretraining </span><a id="_idTextAnchor343" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1087.1">and fine-tuning</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1088.1">Similar to BERT and</span><a id="_idIndexMarker688" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1089.1"> other transformer-based</span><a id="_idIndexMarker689" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1090.1"> models, GPT-3 also involves a two-step process: </span><strong class="bold"><span class="kobospan" id="kobo.1091.1">pretraining</span></strong> <span><span class="kobospan" id="kobo.1092.1">and </span></span><span><strong class="bold"><span class="kobospan" id="kobo.1093.1">fine-tuning</span></strong></span><span><span class="kobospan" id="kobo.1094.1">.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1095.1">Pretraining</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1096.1">In this phase, GPT-3 is trained</span><a id="_idIndexMarker690" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1097.1"> on a large corpus of text data. </span><span class="kobospan" id="kobo.1097.2">It learns to predict the next word in a sentence. </span><span class="kobospan" id="kobo.1097.3">However, un</span><a id="_idTextAnchor344" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1098.1">like BERT, which uses a bidirectional context for prediction, GPT-3 only uses the left context (i.e., the previous words in </span><span><span class="kobospan" id="kobo.1099.1">the sentence).</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1100.1">Fine-tuning</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1101.1">After the pretraining </span><a id="_idIndexMarker691" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1102.1">phase, GPT-3 can be fine-tuned on a specific task using a smaller amount of </span><a id="_idTextAnchor345" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1103.1">task-specific training data. </span><span class="kobospan" id="kobo.1103.2">This could be any NLP task, such as text completion, translation, summarization, question answering, and </span><span><span class="kobospan" id="kobo.1104.1">so on.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1105.1">Zero-shot, one-shot, and few-shot learning</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1106.1">One of the impressive features</span><a id="_idIndexMarker692" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1107.1"> of GPT-3 is its </span><a id="_idIndexMarker693" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1108.1">capability to perform few-shot learning. </span><span class="kobospan" id="kobo.1108.2">When given a task and a few examples of that task, GPT-3 can often learn to perform the </span><span><span class="kobospan" id="kobo.1109.1">task accurately.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1110.1">In the zero-shot setting, the model</span><a id="_idIndexMarker694" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1111.1"> is given a task wit</span><a id="_idTextAnchor346" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1112.1">hout any prior examples. </span><span class="kobospan" id="kobo.1112.2">In the one-shot setting, it’s given one example, and in the few-shot setting, it’s given a few examples to </span><span><span class="kobospan" id="kobo.1113.1">learn from.</span></span></p>
<h1 id="_idParaDest-136" class="calibre4"><a id="_idTextAnchor347" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1114.1">Challenges of using GPT-3</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1115.1">Despite its impressive</span><a id="_idIndexMarker695" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1116.1"> capabilities, GPT-3 also presents some challenges. </span><span class="kobospan" id="kobo.1116.2">Due to its large size, it requires substantial computational resources to train. </span><span class="kobospan" id="kobo.1116.3">It can sometimes generate incorrect or nonsensical responses, and it can reflect biases present in the training d</span><a id="_idTextAnchor348" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1117.1">ata. </span><span class="kobospan" id="kobo.1117.2">It also struggles with tasks that require a deep understanding of the world or common sense reasoning beyond what can be learned </span><span><span class="kobospan" id="kobo.1118.1">from text.</span></span></p>
<h2 id="_idParaDest-137" class="calibre7"><a id="_idTextAnchor349" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1119.1">Reviewing our use case – ML/DL system design for NLP classification in a Jupyter Notebook</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1120.1">In this section, we are going to work</span><a id="_idIndexMarker696" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1121.1"> on a real-world problem and see how we can use an NLP pipeline to solve it. </span><span class="kobospan" id="kobo.1121.2">The code for this pa</span><a id="_idTextAnchor350" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1122.1">rt is shared as a Google Colab notebook </span><span><span class="kobospan" id="kobo.1123.1">at </span></span><a href="https://colab.research.google.com/drive/1HVD2fvxHup6OsPi2mKxNS_nfCRZ0iGCw?usp=sharing" class="calibre5 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.1124.1">Ch6_Text_Classification_DL.ipynb</span></span></a><span><span class="kobospan" id="kobo.1125.1">.</span></span></p>
<h2 id="_idParaDest-138" class="calibre7"><a id="_idTextAnchor351" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1126.1">The business objective</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1127.1">In this scenario, we are</span><a id="_idIndexMarker697" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1128.1"> in the healthca</span><a id="_idTextAnchor352" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1129.1">re sector. </span><span class="kobospan" id="kobo.1129.2">Our objective is to develop a general medical knowledge engine that is very up to date with recent findings in the world </span><span><span class="kobospan" id="kobo.1130.1">of healthcare.</span></span></p>
<h2 id="_idParaDest-139" class="calibre7"><a id="_idTextAnchor353" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1131.1">The technical objective</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1132.1">The CTO derives several technical objectives</span><a id="_idIndexMarker698" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1133.1"> from the business objective. </span><span class="kobospan" id="kobo.1133.2">One objective is for the ML team: given the growing collection of conclusions that correspond to medica</span><a id="_idTextAnchor354" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1134.1">l publications, identify the ones that represent advice. </span><span class="kobospan" id="kobo.1134.2">This will allow us to identify the medical advice that stems from the </span><span><span class="kobospan" id="kobo.1135.1">underlying research.</span></span></p>
<h2 id="_idParaDest-140" class="calibre7"><a id="_idTextAnchor355" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1136.1">The pipeline</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.1137.1">Let’s review the parts</span><a id="_idIndexMarker699" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1138.1"> of the pipeline, as depicted in </span><span><em class="italic"><span class="kobospan" id="kobo.1139.1">Figure 6</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.1140.1">.9</span></em></span><span><span class="kobospan" id="kobo.1141.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer345">
<span class="kobospan" id="kobo.1142.1"><img alt="Figure 6.9 – The structure of a typical exploration and model pipeline﻿" src="image/B18949_06_009.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.1143.1">Figure 6.9 – The structure of a typical exploration and model pipeline</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1144.1">Notice how this design is different from the design we saw in </span><span><em class="italic"><span class="kobospan" id="kobo.1145.1">Figure 5</span></em></span><em class="italic"><span class="kobospan" id="kobo.1146.1">.2</span></em><span class="kobospan" id="kobo.1147.1">. </span><span class="kobospan" id="kobo.1147.2">There, the exploration and evaluation parts leverage the same feature engineering technique that is later used by the ML models. </span><span class="kobospan" id="kobo.1147.3">Here, with LMs, feature engineering is not a part of the preparation for the modeling. </span><span class="kobospan" id="kobo.1147.4">The pretrained model, and particularly the tokenizer, performs feature engineering, which yields very different and less interpretable features than the binary, BoW, or </span><span><span class="kobospan" id="kobo.1148.1">TF-IDF features.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.1149.1">Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.1150.1">Code parts: From “Settings” through “Generating Results of the Traditional </span><span><span class="kobospan" id="kobo.1151.1">ML Models.</span><a id="_idTextAnchor356" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1152.1">”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1153.1">These parts are identical in their nature </span><a id="_idIndexMarker700" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1154.1">to the analog parts discussed in </span><a href="B18949_05_split_000.xhtml#_idTextAnchor130" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.1155.1">Chapter 5</span></em></span></a><span class="kobospan" id="kobo.1156.1">. </span><span class="kobospan" id="kobo.1156.2">The only differences relate to the differences in </span><span><span class="kobospan" id="kobo.1157.1">the data.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1158.1">Deep learning</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1159.1">In this part of the code, we employ</span><a id="_idIndexMarker701" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1160.1"> a deep learning</span><a id="_idIndexMarker702" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.1161.1">language model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1162.1">When looking to apply transfer learning via LMs and fine-tuning them per our objective and data, there are several stacks to choose from. </span><span class="kobospan" id="kobo.1162.2">The ones that stand out the most are Google’s TensorFlow, and Meta’s PyTorch. </span><span class="kobospan" id="kobo.1162.3">A package called </span><strong class="bold"><span class="kobospan" id="kobo.1163.1">Transformers</span></strong><span class="kobospan" id="kobo.1164.1"> was built as a wrapper</span><a id="_idIndexMarker703" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1165.1"> around these stacks to allow for a simpler implementation of the code. </span><span class="kobospan" id="kobo.1165.2">In this example, we leverage the simplicity and richness of </span><span><span class="kobospan" id="kobo.1166.1">transformers models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1167.1">It is worth highlighting the company that built and supports the Transformers package: Hugging Face. </span><span class="kobospan" id="kobo.1167.2">Hugging Face took it upon themselves to create an entire ecosystem around the collection and sharing of free, open source DL models, which includes the many components that accommodate for implementing these models. </span><span class="kobospan" id="kobo.1167.3">The most actionable tool is the Transformers package, which is a Python package dedicated to picking, importing, training, and employing a large and growing set of </span><span><span class="kobospan" id="kobo.1168.1">DL models.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1169.1">The</span><a id="_idTextAnchor357" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1170.1"> code we are reviewing</span><a id="_idIndexMarker704" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1171.1"> here provides more than just</span><a id="_idIndexMarker705" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1172.1"> an example of ML/DL system design in the real world; it also showcases Hugging </span><span><span class="kobospan" id="kobo.1173.1">F</span><a id="_idTextAnchor358" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1174.1">ace’s Transformers.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1175.1">Formatting the data</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1176.1">Here, we set the data up in a format</span><a id="_idIndexMarker706" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1177.1"> that suits the Transformers library. </span><span class="kobospan" id="kobo.1177.2">The column names must be </span><span><span class="kobospan" id="kobo.1178.1">very specific.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1179.1">Evaluation metric</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1180.1">We decided which metric</span><a id="_idIndexMarker707" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1181.1"> we wished to optimize and plugged it into the training process. </span><span class="kobospan" id="kobo.1181.2">For this prob</span><a id="_idTextAnchor359" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1182.1">lem of binary classification, we optimized for accuracy and evaluated our result</span><a id="_idIndexMarker708" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1183.1"> in comparison to the dataset’s baseline accuracy, also known as </span><span><span class="kobospan" id="kobo.1184.1">the prior.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.1185.1">Trainer object</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.1186.1">This is the core object</span><a id="_idIndexMarker709" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1187.1"> for training</span><a id="_idIndexMarker710" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1188.1"> the LM in Transformers. </span><span class="kobospan" id="kobo.1188.2">It holds a set of predefined configurations. </span><span class="kobospan" id="kobo.1188.3">Some of the key training configurations are </span><span><span class="kobospan" id="kobo.1189.1">the following:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.1190.1">The neural net’s mathematical</span><a id="_idIndexMarker711" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1191.1"> learning hyperparameters, such </span><span><span class="kobospan" id="kobo.1192.1">the following:</span></span><ul class="calibre17"><li class="calibre15"><span class="kobospan" id="kobo.1193.1">The </span><span><span class="kobospan" id="kobo.1194.1">learning rate</span></span></li><li class="calibre15"><span class="kobospan" id="kobo.1195.1">The gradient </span><span><span class="kobospan" id="kobo.1196.1">decent settings</span></span></li></ul></li>
<li class="calibre15"><span class="kobospan" id="kobo.1197.1">The </span><a id="_idTextAnchor360" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1198.1">number of </span><span><span class="kobospan" id="kobo.1199.1">training epochs</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1200.1">The computation </span><span><span class="kobospan" id="kobo.1201.1">hardware usage</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.1202.1">Logging setting for capturing</span><a id="_idIndexMarker712" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1203.1"> the progression of the objective metric throughout the</span><a id="_idIndexMarker713" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.1204.1">training process</span></span></li>
</ul>
<h4 class="calibre135"><span class="kobospan" id="kobo.1205.1">Fine-tuning the neural network parameters</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1206.1">The fundamental concept</span><a id="_idIndexMarker714" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1207.1"> around fine-tuning LMs</span><a id="_idIndexMarker715" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1208.1"> is transfer learning. </span><span class="kobospan" id="kobo.1208.2">Neural networks lend themselves so well to transfer learning because one can simply strip any number of layers from the end of the structure and replace them with untrained layers that would be trained based on the underlying problem. </span><span class="kobospan" id="kobo.1208.3">The rest of the layers that weren’t removed and aren’t trained continue to operate exactly in the same way they did when the LM was originally trained (when it was originally built). </span><span class="kobospan" id="kobo.1208.4">If we replace the last layer but leave the rest of the original layers, then we could view those layers as supervised feature engineering or, conversely, as an embedding mechanism. </span><span class="kobospan" id="kobo.1208.5">This trait reflects the concept of transfer learning. </span><span class="kobospan" id="kobo.1208.6">Ideally, the model is expected to lend itself well to our underlying problem so that we will choose to keep the vast majority of the original layers, and only a small minority would be replaced and trained. </span><span class="kobospan" id="kobo.1208.7">In this way, a large DL model that took many weeks to be pretrained can be transferred and adapted to a new problem </span><span><span class="kobospan" id="kobo.1209.1">in minutes.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1210.1">In our code, we set the model up in a way that we dictate exactly which of its layers we are looking to fine-tune. </span><span class="kobospan" id="kobo.1210.2">It is a design choice for us for this to be based on performance and also computation resources. </span><span class="kobospan" id="kobo.1210.3">One choice is to fine-tune the last layer right before the final</span><a id="_idIndexMarker716" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1211.1"> output, also known as the classification head. </span><span class="kobospan" id="kobo.1211.2">The alternative is to fine-tune all the layers. </span><span class="kobospan" id="kobo.1211.3">In our code, we explicitly call the model’s configuration, which controls which layer is fine-tuned, so the code can be changed in any way that suits </span><span><span class="kobospan" id="kobo.1212.1">the design.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1213.1">We configure the trainer to log the performance of the training in real time. </span><span class="kobospan" id="kobo.1213.2">It prints those logs out for us in a table so we can observe and monitor them. </span><span class="kobospan" id="kobo.1213.3">When the training is complete, we plot the progress of the training and the evaluation. </span><span class="kobospan" id="kobo.1213.4">This helps us see the relation between the evolution of the training results and the evaluation results. </span><span class="kobospan" id="kobo.1213.5">Since the </span><a id="_idTextAnchor361" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1214.1">evaluation set that the trainer uses can be viewed as a held-out set in the context of the trainer, this plot allows us to investigate underfitting </span><span><span class="kobospan" id="kobo.1215.1">and overfitting.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1216.1">Generating the training results – used for design choices</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1217.1">We reviewed the results of the training</span><a id="_idIndexMarker717" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1218.1"> set, along with the logs that the trainer printed out. </span><span class="kobospan" id="kobo.1218.2">We compared them to the baseline accuracy and observed an increase in accuracy. </span><span class="kobospan" id="kobo.1218.3">We learned about the quality of our design by iterating over several different design choices and comparing them. </span><span class="kobospan" id="kobo.1218.4">That process of iterating over many sets of design parameters would be automated into code to allow for a systematic evaluation of the optimal setting. </span><span class="kobospan" id="kobo.1218.5">We did</span><a id="_idTextAnchor362" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1219.1">n’t do that in our notebook just to keep things simple in the example. </span><span class="kobospan" id="kobo.1219.2">Once we believed we had found the optimal setting, we could say that the process </span><span><span class="kobospan" id="kobo.1220.1">was finished.</span></span></p>
<h4 class="calibre135"><span class="kobospan" id="kobo.1221.1">Generating the testing results – used for presenting performance</span></h4>
<p class="calibre6"><span class="kobospan" id="kobo.1222.1">As with the code in </span><a href="B18949_05_split_000.xhtml#_idTextAnchor130" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.1223.1">Chapter 5</span></em></span></a><span class="kobospan" id="kobo.1224.1">, here, too, we finished</span><a id="_idIndexMarker718" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1225.1"> by reviewing the test results. </span><span class="kobospan" id="kobo.1225.2">It is worth noting the difference between the evaluation set and the test set. </span><span class="kobospan" id="kobo.1225.3">One could suggest that since the trainer doesn’t use the evaluation set for training, it could be used as a held-out test set, thus saving the need to exclude so many observations from training and supplying the model with more labeled data. </span><span class="kobospan" id="kobo.1225.4">However, while the trainer didn’t use the evaluation set, we did use it to make our design decisions. </span><span class="kobospan" id="kobo.1225.5">For instance, we observed the plot from the preceding section and judged which number of epochs is optimal to achieve optimal fit</span><a id="_idTextAnchor363" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1226.1">ting. </span><span class="kobospan" id="kobo.1226.2">In </span><a href="B18949_05_split_000.xhtml#_idTextAnchor130" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.1227.1">Chapter 5</span></em></span></a><span class="kobospan" id="kobo.1228.1">, an evaluation set was used too, but we didn’t need to explicitly define it; it was carried out as a part of the K-fold </span><span><span class="kobospan" id="kobo.1229.1">cross-validation mechanism.</span></span></p>
<h1 id="_idParaDest-141" class="calibre4"><a id="_idTextAnchor364" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1230.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.1231.1">In this enlightening chapter, we embarked on a comprehensive exploration of DL and its remarkable application to text classification tasks through language models. </span><span class="kobospan" id="kobo.1231.2">We began with an overview of DL, revealing its profound ability to learn complex patterns from vast amounts of data and its indisputable role in advancing state-of-the-art </span><span><span class="kobospan" id="kobo.1232.1">NLP systems.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1233.1">We then delved into the transformative world of transformer models, which have revolutionized NLP by providing an effective alternative to traditional RNNs and CNNs for processing sequence data. </span><span class="kobospan" id="kobo.1233.2">By unpacking the attention mechanism—a key feature in transformers—we highlighted its capacity to focus on different parts of the input sequence, hence facilitating a better understanding </span><span><span class="kobospan" id="kobo.1234.1">of context.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1235.1">Our journey continued with an in-depth exploration of the BERT model. </span><span class="kobospan" id="kobo.1235.2">We detailed its architecture, emphasizing its pioneering use of bidirectional training to generate contextually rich word embeddings, and we highlighted its pretraining process, which learns language semantics from a large </span><span><span class="kobospan" id="kobo.1236.1">text corpus.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1237.1">However, our exploration did not end there; we also introduced GPT, another transformative model that leverages the power of transformers in a slightly different way—focusing on generating human-like text. </span><span class="kobospan" id="kobo.1237.2">By comparing BERT and GPT, we shed light on their distinct strengths and </span><span><span class="kobospan" id="kobo.1238.1">use cases.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1239.1">The chapter culminated in a practical guide on how to design and implement a text classification model using these advanced models. </span><span class="kobospan" id="kobo.1239.2">We walked you through all the stages of this process, from data preprocessing and model configuration to training, evaluation, and finally, making predictions on </span><span><span class="kobospan" id="kobo.1240.1">unseen data.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1241.1">In essence, this chapter provided a well-rounded understanding of DL in NLP, transitioning from fundamental principles to hands-on applications. </span><span class="kobospan" id="kobo.1241.2">With this knowledge, you are now equipped to leverage the capabilities of transformer models, BERT, and GPT for your text classification tasks. </span><span class="kobospan" id="kobo.1241.3">Whether you are looking to delve further into the world of NLP or apply these skills in a practical setting, this chapter has equipped you with a firm foundation on which </span><span><span class="kobospan" id="kobo.1242.1">to build.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.1243.1">In this chapter, we introduced you to large language models. </span><span class="kobospan" id="kobo.1243.2">In the next chapter, we dive deeper into these models to learn more </span><span><span class="kobospan" id="kobo.1244.1">about them.</span></span></p>
</div>
</body></html>