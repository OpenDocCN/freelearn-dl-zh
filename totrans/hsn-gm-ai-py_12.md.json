["```py\npip install tensorboard --upgrade\n```", "```py\npip install future\n```", "```py\ntensorboard --logdir runs\n```", "```py\ntensorboard --logdir=/path_to_log_dir/ --port 6006\n```", "```py\nclass QRDQN(nn.Module):\n    def __init__(self, num_inputs, num_actions, num_quants):\n        super(QRDQN, self).__init__()\n\n        self.num_inputs = num_inputs\n        self.num_actions = num_actions\n        self.num_quants = num_quants\n\n        self.features = nn.Sequential(\n            nn.Linear(num_inputs, 32),\n            nn.ReLU(),\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, self.num_actions * self.num_quants)\n        )        \n        self.num_quants, use_cuda=USE_CUDA)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = self.features(x)    \n        x = x.view(batch_size, self.num_actions, self.num_quants)        \n        return x\n\n    def q_values(self, x):\n        x = self.forward(x)\n        return x.mean(2)\n\n    def act(self, state, epsilon):\n        if random.random() > epsilon:\n            state = autograd.Variable(torch.FloatTensor(np.array(state, dtype=np.float32)).unsqueeze(0), volatile=True)\n            qvalues = self.forward(state).mean(2)\n            action = qvalues.max(1)[1]\n            action = action.data.cpu().numpy()[0]\n        else:\n            action = random.randrange(self.num_actions)\n        return action\n```", "```py\ndef projection_distribution(dist, next_state, reward, done):\n    next_dist = target_model(next_state)\n    next_action = next_dist.mean(2).max(1)[1]\n    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_quant)\n    next_dist = next_dist.gather(1, next_action).squeeze(1).cpu().data\n\n    expected_quant = reward.unsqueeze(1) + 0.99 * next_dist * (1 - done.unsqueeze(1))\n    expected_quant = autograd.Variable(expected_quant)\n\n    quant_idx = torch.sort(dist, 1, descending=False)[1]\n\n    tau_hat = torch.linspace(0.0, 1.0 - 1./num_quant, num_quant) + 0.5 / num_quant\n    tau_hat = tau_hat.unsqueeze(0).repeat(batch_size, 1)\n    quant_idx = quant_idx.cpu().data\n    batch_idx = np.arange(batch_size)\n    tau = tau_hat[:, quant_idx][batch_idx, batch_idx]\n\n    return tau, expected_quant\n```", "```py\ncurrent_model = QRDQN(env.observation_space.shape[0], env.action_space.n, num_quant)\ntarget_model = QRDQN(env.observation_space.shape[0], env.action_space.n, num_quant)\n```", "```py\ndef compute_td_loss(batch_size):\n    state, action, reward, next_state, done = replay_buffer.sample(batch_size) \n\n    state = autograd.Variable(torch.FloatTensor(np.float32(state)))\n    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True)\n    action = autograd.Variable(torch.LongTensor(action))\n    reward = torch.FloatTensor(reward)\n    done = torch.FloatTensor(np.float32(done))\n\n    dist = current_model(state)\n    action = action.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, num_quant)\n    dist = dist.gather(1, action).squeeze(1)\n\n    tau, expected_quant = projection_distribution(dist, next_state, reward, done)\n    k = 1\n\n    huber_loss = 0.5 * tau.abs().clamp(min=0.0, max=k).pow(2)\n    huber_loss += k * (tau.abs() - tau.abs().clamp(min=0.0, max=k))\n    quantile_loss = (tau - (tau < 0).float()).abs() * huber_loss\n    loss = torch.tensor(quantile_loss.sum() / num_quant, requires_grad=True)\n\n    optimizer.zero_grad()\n    loss.backward()\n    nn.utils.clip_grad_norm(current_model.parameters(), 0.5)\n    optimizer.step()\n\n    return loss\n```", "```py\nnum_quant = 51\nVmin = -10\nVmax = 10\n```", "```py\nstate = env.reset()\nfor iteration in range(1, iterations + 1):\n    action = current_model.act(state, epsilon_by_frame(iteration))\n\n    next_state, reward, done, _ = env.step(action)\n    replay_buffer.push(state, action, reward, next_state, done)\n\n    state = next_state\n    episode_reward += reward\n\n    if done:\n        state = env.reset()\n        all_rewards.append(episode_reward)\n        episode_reward = 0\n\n    if len(replay_buffer) > batch_size:\n        loss = compute_td_loss(batch_size)\n        losses.append(loss.item())\n\n    if iteration % 200 == 0:\n        plot(iteration, all_rewards, losses, episode_reward)\n\n    if iteration % 1000 == 0:\n        update_target(current_model, target_model)\n```", "```py\ntensorboard --logdir=runs\n```", "```py\nfrom common.replay_buffer import ReplayBuffer\nfrom torch.utils.tensorboard import SummaryWriter\n\nenv_id = \"LunarLander-v2\"\nenv = gym.make(env_id)\nwriter = SummaryWriter()\n```", "```py\ndef plot(iteration, rewards, losses, ep_reward): \n    print(\"Outputing Iteration \" + str(iteration))\n    writer.add_scalar('Train/Rewards', rewards[-1], iteration)\n    writer.add_scalar('Train/Losses', losses[-1], iteration) \n    writer.add_scalar('Train/Exploration', epsilon_by_frame(iteration), iteration)\n    writer.add_scalar('Train/Episode', ep_reward, iteration)\n    writer.flush()\n```", "```py\nclass NoisyDQN(nn.Module):\n    def __init__(self, num_inputs, num_actions):\n        super(NoisyDQN, self).__init__()\n\n        self.linear = nn.Linear(env.observation_space.shape[0], 128)\n        self.noisy1 = NoisyLinear(128, 128)\n self.noisy2 = NoisyLinear(128, env.action_space.n)\n\n    def forward(self, x):\n        x = F.relu(self.linear(x))\n        x = F.relu(self.noisy1(x))\n        x = self.noisy2(x)\n        return x\n\n    def act(self, state):\n        state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n        q_value = self.forward(state)\n        action = q_value.max(1)[1].item()\n        return action\n\n    def reset_noise(self):\n        self.noisy1.reset_noise()\n        self.noisy2.reset_noise()\n```", "```py\ndef compute_td_loss(batch_size, beta):\n    state, action, reward, next_state, done, weights, indices = replay_buffer.sample(batch_size, beta) \n\n    state = autograd.Variable(torch.FloatTensor(np.float32(state)))\n    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)))\n    action = autograd.Variable(torch.LongTensor(action))\n    reward = autograd.Variable(torch.FloatTensor(reward))\n    done = autograd.Variable(torch.FloatTensor(np.float32(done)))\n    weights = autograd.Variable(torch.FloatTensor(weights))\n\n    q_values = current_model(state)\n    next_q_values = target_model(next_state)\n\n    q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n    next_q_value = next_q_values.max(1)[0]\n    expected_q_value = reward + gamma * next_q_value * (1 - done)\n\n    loss = (q_value - expected_q_value.detach()).pow(2) * weights\n    prios = loss + 1e-5\n    loss = loss.mean()\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\n    current_model.reset_noise()\n    target_model.reset_noise()\n\n    return loss\n```", "```py\nclass NoisyLinear(nn.Module):\n    def __init__(self, in_features, out_features, std_init=0.4):\n        super(NoisyLinear, self).__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.std_init = std_init\n\n        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n\n        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n\n        self.reset_parameters()\n        self.reset_noise()\n\n    def forward(self, x):\n        if self.training: \n            weight = self.weight_mu + self.weight_sigma.mul(autograd.Variable(self.weight_epsilon))\n            bias = self.bias_mu + self.bias_sigma.mul(autograd.Variable(self.bias_epsilon))\n        else:\n            weight = self.weight_mu\n            bias = self.bias_mu\n\n        return F.linear(x, weight, bias)\n\n    def reset_parameters(self):\n        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n\n        self.weight_mu.data.normal_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n\n        self.bias_mu.data.normal_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n\n    def reset_noise(self):\n        epsilon_in = self._scale_noise(self.in_features)\n        epsilon_out = self._scale_noise(self.out_features)\n\n        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n\n    def _scale_noise(self, size):\n        x = torch.randn(size)\n        x = x.sign().mul(x.abs().sqrt())\n        return x\n```", "```py\nbeta_start = 0.4\nbeta_iterations = 50000 \nbeta_by_iteration = lambda iteration: min(1.0, beta_start + iteration * (1.0 - beta_start) / beta_iterations)\n```", "```py\nassert beta > 0\n\nidxes = self._sample_proportional(batch_size)\n\nweights = []\np_min = self._it_min.min() / self._it_sum.sum()\nmax_weight = (p_min * len(self._storage)) ** (-beta)\n\nfor idx in idxes:\n    p_sample = self._it_sum[idx] / self._it_sum.sum()\n    weight = (p_sample * len(self._storage)) ** (-beta)\n    weights.append(weight / max_weight)\nweights = np.array(weights)\nencoded_sample = self._encode_sample(idxes)\nreturn tuple(list(encoded_sample) + [weights, idxes])\n```", "```py\nwriter.add_scalar('Train/Beta', beta_by_iteration(iteration), iteration)\n```", "```py\nclass RainbowDQN(nn.Module):\n    def __init__(self, num_inputs, num_actions, num_atoms, Vmin, Vmax):\n        super(RainbowDQN, self).__init__()\n\n        self.num_inputs = num_inputs\n        self.num_actions = num_actions\n        self.num_atoms = num_atoms\n        self.Vmin = Vmin\n        self.Vmax = Vmax\n\n        self.linear1 = nn.Linear(num_inputs, 32)\n        self.linear2 = nn.Linear(32, 64)\n\n        self.noisy_value1 = NoisyLinear(64, 64, use_cuda=False)\n        self.noisy_value2 = NoisyLinear(64, self.num_atoms, use_cuda=False)\n\n        self.noisy_advantage1 = NoisyLinear(64, 64, use_cuda=False)\n        self.noisy_advantage2 = NoisyLinear(64, self.num_atoms * self.num_actions, use_cuda=False)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        x = F.relu(self.linear1(x))\n        x = F.relu(self.linear2(x))\n\n        value = F.relu(self.noisy_value1(x))\n        value = self.noisy_value2(value)\n\n        advantage = F.relu(self.noisy_advantage1(x))\n        advantage = self.noisy_advantage2(advantage)\n\n        value = value.view(batch_size, 1, self.num_atoms)\n        advantage = advantage.view(batch_size, self.num_actions, self.num_atoms)        \n        x = value + advantage - advantage.mean(1, keepdim=True)\n        x = F.softmax(x.view(-1, self.num_atoms)).view(-1, self.num_actions, self.num_atoms)        \n        return x\n\n    def reset_noise(self):\n        self.noisy_value1.reset_noise()\n        self.noisy_value2.reset_noise()\n        self.noisy_advantage1.reset_noise()\n        self.noisy_advantage2.reset_noise()\n\n    def act(self, state):\n        state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n        dist = self.forward(state).data.cpu()\n        dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)\n        action = dist.sum(2).max(1)[1].numpy()[0]\n        return action\n```", "```py\ndef act(self, state):\n        state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True)\n        dist = self.forward(state).data.cpu()\n        dist = dist * torch.linspace(self.Vmin, self.Vmax, self.num_atoms)\n        action = dist.sum(2).max(1)[1].numpy()[0]\n        return action\n```", "```py\ndef projection_distribution(next_state, rewards, dones):\n    batch_size = next_state.size(0)\n\n    delta_z = float(Vmax - Vmin) / (num_atoms - 1)\n    support = torch.linspace(Vmin, Vmax, num_atoms)\n\n    next_dist = target_model(next_state).data.cpu() * support\n    next_action = next_dist.sum(2).max(1)[1]\n    next_action = next_action.unsqueeze(1).unsqueeze(1).expand(next_dist.size(0), 1, next_dist.size(2))\n    next_dist = next_dist.gather(1, next_action).squeeze(1)\n\n    rewards = rewards.unsqueeze(1).expand_as(next_dist)\n    dones = dones.unsqueeze(1).expand_as(next_dist)\n    support = support.unsqueeze(0).expand_as(next_dist)\n\n    Tz = rewards + (1 - dones) * 0.99 * support\n    Tz = Tz.clamp(min=Vmin, max=Vmax)\n    b = (Tz - Vmin) / delta_z\n    l = b.floor().long()\n    u = b.ceil().long()\n\n    offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long()\\\n                    .unsqueeze(1).expand(batch_size, num_atoms)\n\n    proj_dist = torch.zeros(next_dist.size()) \n    proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n    proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n\n    return proj_dist\n```", "```py\nepsilon_start = 1.0\nepsilon_final = 0.01\nepsilon_decay = 50000\n\nepsilon_by_frame = lambda iteration: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1\\. * iteration / epsilon_decay)\n```"]