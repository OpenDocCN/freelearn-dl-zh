- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating Better Prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a developer, you can request that an LLM completes a task by submitting a
    prompt to it. In the previous chapter, we saw some examples of prompts, such as
    “*Tell me a knock-knock joke*” and “*What is the flight duration between New York
    City and Rio de Janeiro?*” As LLMs became more powerful, the tasks that they could
    accomplish became more complex.
  prefs: []
  type: TYPE_NORMAL
- en: 'Researchers discovered that using different techniques to build prompts yielded
    vastly different results. The process of crafting prompts that improve the likelihood
    of getting the desired answer is called prompt engineering, and the value of creating
    better prompts gave birth to a new profession: **prompt engineer**. This is someone
    who doesn’t need to know how to code in any programming language but can create
    prompts using natural language that return the desired results.'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Semantic Kernel uses the concept of **prompt templating**, the creation
    of structured templates for prompts that contain placeholders for specific types
    of information and instructions that can be filled in or customized by the user
    or by the developer. By using prompt templates, developers can introduce multiple
    variables in prompts, separate the prompt engineering function from the coding
    function, and use advanced prompt techniques to increase accuracy in responses.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn about several techniques that will make your prompts
    more likely to return the results you want your users to see in the first attempt.
    You’ll learn how to employ prompts with multiple variables as well as how to create
    and use prompts that have multiple parameters to complete more complex tasks.
    Finally, you’ll uncover techniques that combine prompts in creative ways to improve
    accuracy in scenarios where LLMs are not very accurate – for example, when solving
    math problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Engineering prompts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts with multiple variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompts with multiple stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete this chapter, you will need to have a recent, supported version
    of your preferred Python or C# development environment:'
  prefs: []
  type: TYPE_NORMAL
- en: For Python, the minimum supported version is Python 3.10, and the recommended
    version is Python 3.11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For C#, the minimum supported version is .NET 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also need an **OpenAI API** key, obtained either directly through **OpenAI**
    or through **Microsoft**, through the **Azure OpenAI** service. Instructions on
    how to obtain these keys can be found in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014).
  prefs: []
  type: TYPE_NORMAL
- en: If you are using .NET, the code for this chapter can be found at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2).
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Python, the code for this chapter can be found at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can install the required packages by going to the GitHub repository and
    using the following: `pip install -``r requirements.txt`.'
  prefs: []
  type: TYPE_NORMAL
- en: A simple plugin template
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two simple ways of creating prompt templates.
  prefs: []
  type: TYPE_NORMAL
- en: The first is to generate the prompt from a string variable inside your code.
    This way is simple and convenient. We covered this method in the *Running a simple
    prompt* section of [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second is to use Semantic Kernel to help separate the development function
    from the prompt engineering function. As you saw in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014),
    you can create your requests to LLMs as functions in plugins. A plugin is a directory
    that contains multiple sub-directories, one per function. Each subdirectory will
    have exactly two files:'
  prefs: []
  type: TYPE_NORMAL
- en: A text file called `skprompt.txt` that contains the prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A configuration file called `config.json` that contains the parameters that
    will be used in the API call
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the prompt is maintained separately from the code, you, as an application
    developer, can focus on the code of your application and let a specialized prompt
    engineer work on the `skprompt.txt` files.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the second method – creating plugins in dedicated
    directories – because this method is more robust to changes. For example, if you
    are switching from Python to C# or using a new version of the .NET library and
    these changes require a lot of changes to be made to your code, at least you don’t
    need to change your prompts and function configurations.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this prompt plugin is like the one we used in the previous chapter.
    We are going to take the plugin we built in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014)
    and make several changes to `skprompt.txt` to observe the results and learn how
    different prompting techniques can substantially change outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: We are doing this again in detail at the beginning of this chapter even though
    we did something very similar in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014)
    because we’re going to use these steps several times as we explore prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: For the examples we’ll cover here, you can use both GPT-3.5 and GPT-4, but remember
    that GPT-4 is 30x more expensive. The results that are shown are from GPT-3.5
    unless indicated otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The skprompt.txt file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start with a very simple prompt, just asking directly what we want,
    without any additional context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: That’s it – that’s the whole file.
  prefs: []
  type: TYPE_NORMAL
- en: The config.json file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `config.json` file is very similar to the one we used in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014),
    but we changed three things:'
  prefs: []
  type: TYPE_NORMAL
- en: The description of the function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The name of the input variable under `input_variables`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The description of the input variable under `input_variables`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have defined the function, let’s call it.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the plugin from Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code we’re using here is very similar to what we used in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014).
    We’ve made three small changes: use only GPT-3.5, point to the appropriate plugin
    directory, `prompt_engineering`, and change the input variable’s name from `input`
    to `city`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s learn how to call the plugin in C#. Note that we don’t make any changes
    to `skprompt.txt` or `config.json`. We can use the same prompt and configuration,
    independent of the language.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the plugin from C#
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we did in Python, we just need to make three small changes: use only GPT-3.5,
    point to the appropriate plugin directory, `prompt_engineering`, and change the
    input variable’s name from `input` to `city`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instead of giving the top three attractions that I wanted to see, in one of
    the calls, GPT-3.5 created a 3-day list of attractions and gave me a list of six
    attractions. This is a typical problem that can happen when the prompt is not
    very specific. The list of attractions is very good and considers how hard it
    is to move around in New York City due to traffic: attractions on the same day
    are close to each other, or at least there is enough time to travel between them.
    Since a lot of GPT-3.5 training was done with data during the COVID period, the
    itinerary even includes a note to check for COVID closures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that your result may be different, and in some cases, you may get a response
    that looks like the original intent, with only three must-see attractions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This quick, simple prompt didn’t work consistently as expected, so let’s explore
    how to make things better by adding more information to the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the prompt to get better results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If the main way you interact with LLMs is using ChatGPT or Microsoft Copilot,
    you may have the impression that you can use very short prompts to get the results
    you want. As explained in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014), these
    applications modify your submission and add a lot of instructions to your prompt.
    As a developer, you’ll have to do the same. Here are a few tips to improve your
    prompt and obtain better results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Provide context**: Instead of simply asking for three must-see attractions,
    provide as much context as you can. Think about everything that you would ask
    someone who asked you the same question – for example, “*How many days are you
    staying?*,” “*What kind of things do you like and dislike?*,” or “*How are you
    going to be* *getting around?*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Be explicit about the format of the response**: If you want the answer to
    come out in a specific format, make sure you tell the LLM this. You may even provide
    some examples – for example, “*Answer with a single word – ‘Yes’* *or ‘No.’*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify the response’s length**: If you want a concise answer, specify the
    desired length – for example, “*In two sentences.*” If you want a longer answer,
    make it clear – for example, “*600 words*” or “*five paragraphs.*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revising the skprompt.txt file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using these hints, we’re going to rewrite the prompt to see if we can get improved
    responses consistently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The result
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here’s the response we get from the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we’ve created the prompt in such a way that you can use it with different
    cities. Here’s a run with London. Note that the response acknowledged my fear
    of heights, but simply told me to overcome it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: For many applications, you’ll need to make your prompt more configurable, and
    that will require adding more variables. We’ll do that next.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts with multiple variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can parameterize a lot of information in a prompt. For example, in our prompt
    that finds the best attractions, you can add multiple variables, such as the number
    of days that the person is staying, the things they like and dislike, and how
    many attractions they want to see.
  prefs: []
  type: TYPE_NORMAL
- en: In such a case, our prompt will become more complex, so we will need to create
    a new `skprompt.txt` file. Since our prompt will be a new function and have multiple
    parameters, we will also need to create a new `config.json` file.
  prefs: []
  type: TYPE_NORMAL
- en: These two files can be found in the `plugins/prompt_engineering/attractions_multiple_variables`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Skprompt.txt
  prefs: []
  type: TYPE_NORMAL
- en: 'To add more variables to a prompt, simply add them within double curly brackets
    with a dollar sign before their name. The following code shows how to add several
    variables (`city`, `n_days`, `likes`, `dislikes`, and `n_attractions`) to a single
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see the changes in the function configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Config.json
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `config.json` file for multiple variables is almost the same as the one
    we use with a single variable, but we need to add the details for all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now that we’ve configured the new function, let’s learn how to call it in Python
    and C#.
  prefs: []
  type: TYPE_NORMAL
- en: Requesting a complex itinerary with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compared to calling a prompt with a single parameter, the only change we need
    to make to call a prompt with multiple parameters is on the `KernelArguments`
    object. When passing `KernelArguments` as a parameter to `kernel.invoke`, we must
    add all the parameters we need to the object, as shown here. One thing to note
    is that the parameters are all strings since LLMs work best with text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the C# code.
  prefs: []
  type: TYPE_NORMAL
- en: Requesting a complex itinerary with C#
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This time, we will create a `KernelArguments` object called `function_arguments`
    outside of the function invocation and pre-fill the five variables with the content
    we want. Then, we will pass this object to the invocation call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s see the results.
  prefs: []
  type: TYPE_NORMAL
- en: The result of the complex itinerary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The result considers all the input variables. It suggests a park – the High
    Line – even though I told the LLM that I dislike parks. It does explain that it
    is a very unusual park, and if you know New York, it doesn’t feel like a park
    at all. I think that a person who doesn’t enjoy the traditional park experience
    would enjoy the High Line, so the LLM did a very good job once it got more context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: With that, we’ve learned how to improve our prompts to get better results. However,
    there are some cases in which LLMs fail to provide good answers, even after we
    use all the techniques mentioned previously. A very common case is solving math
    problems. We will explore this next.
  prefs: []
  type: TYPE_NORMAL
- en: Issues when answering math problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the results from LLMs are impressive, sometimes, models can get confused
    by seemingly simple questions. This tends to be more frequent when the question
    involves math. For example, I ran the following prompt in GPT-3.5 five times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The plugin I used to run this prompt is under `plugins/prompt_engineering/solve_math_problem`.
  prefs: []
  type: TYPE_NORMAL
- en: The correct answer is 67 because when I was 6, my sister was 3\. Now, 64 years
    later, I’m 70, so she would be 67.
  prefs: []
  type: TYPE_NORMAL
- en: Here are the results of the five runs on GPT-3.5\. The first result was incorrect,
    saying that “*my sister is 64 years younger than her* *current age*:”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The second and third attempts gave the correct answer. Here’s the second attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the third attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'But the fourth and fifth attempts gave wrong answers again. Here’s the fourth
    attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the fifth attempt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Therefore, in five attempts, only two (40%) returned correct answers, which
    is not an experience that you want your users to have.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to address this issue in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Multistage prompts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to improve the accuracy of LLMs when doing math is to use multistage
    prompts. In this technique, the answer from the first prompt is passed to the
    second as a parameter. We’re going to illustrate this with the **Chain-of-Thought**
    (**CoT**) technique.
  prefs: []
  type: TYPE_NORMAL
- en: CoT – “Let’s think step by step”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the paper *Large Language Models are Zero-Shot Reasoners* [1], the authors
    found that simply adding “Let’s think step by step” right after the question can
    help improve the accuracy of LLMs a lot. Their proposed process works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ask the intended question, but instead of asking the LLM to answer, simply append
    “*Let’s think step by step*” at the end.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The LLM will answer with a process to answer the question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the question from *step 1* with the process from s*tep 2* in a new prompt,
    and finish with “*Therefore, the* *answer is…*:”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The Zero-shot-CoT method](img/B21826_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – The Zero-shot-CoT method
  prefs: []
  type: TYPE_NORMAL
- en: They called their process *Zero-shot-Chain-of-Thought* or *Zero-shot-CoT*. The
    *Zero-shot* part means that you don’t need to give any example answers to the
    LLM; you can give the question directly. This is to differentiate the process
    from **few-shot**, which is when you provide the LLM several examples of expected
    answers in your prompt, making the prompt substantially larger. The *CoT* part
    describes the process of asking the LLM to provide a reasoning framework and adding
    the LLM’s reasoning to the question.
  prefs: []
  type: TYPE_NORMAL
- en: The authors tested several different phrases to obtain the CoT from the LLM,
    such as “*Let’s think about it logically*” and “*Let’s think like a detective
    step by step*,” and found that simply “*Let’s think step by step*” yielded the
    best results.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Zero-shot-CoT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need two prompts – one for both steps. For the first step, we will call
    `solve_math_problem_v2`. The prompt simply restates the problem and adds “*Let’s
    think step by step*” at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The prompt for the second step, which we will call `chain_of_thought`, repeats
    the first prompt, includes the answer for the first prompt, and then asks for
    the solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Given this prompt, the `config.json` file needs two `input_variables`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s learn how to call the prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CoT with Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To make the steps explicit, I broke the program into two parts. The first one
    asks for the CoT and shows it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part shows the answer. If all you care about is the answer, you
    don’t need to print the steps, but you still need to use the LLM to calculate
    the steps because they are a required parameter to the CoT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Implementing CoT with C#
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we did in Python, we can break the C# program into two parts. Part one will
    show the reasoning steps elicited by the CoT prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The second part parses and reports the answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Results for CoT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I ran the program five times, and it got the right answer every time. The answers
    aren’t deterministic, so it may be that you run it on your machine and get one
    or two wrong answers. In the paper, the authors claimed that this method achieves
    78.7% success in problems of this type, where the usual accuracy of LLMs is around
    17.7%.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at two sample responses. Here’s the first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Instead of running the program manually, we can automate it. Let's see how.
  prefs: []
  type: TYPE_NORMAL
- en: An ensemble of answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the CoT technique helps a lot, we still have a 78.7% average accuracy,
    and that may not be enough. To address this problem, one technique that is frequently
    used is to ask the model the same question several times and compile only the
    most frequently given answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we will make a minor modification to our CoT prompt and call
    it `chain_of_thought_v2`. We will simply ask the LLM to answer in Arabic numerals,
    to make it easier to compare the answers in a later step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We also need to change the program and ask it to run several times. For the
    next example, I chose *N* = 7\. We will collect the answers and choose the answer
    that appears more frequently. Note that each call using this method is *N* times
    more expensive and takes *N* times longer than a single call. Accuracy is not
    free.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically running an ensemble with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s run CoT seven times. For each time we run it, we’ll add the result to
    a list. Then, we’ll take advantage of the `set` data structure to quickly get
    the most common element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Let’s see how to implement this in C#.
  prefs: []
  type: TYPE_NORMAL
- en: Automatically running an ensemble with C#
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The C# code uses the same idea as the Python code: we run the model seven times
    and store the results. Then, we search the results for the most frequent answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Combining CoT and the ensemble method substantially increases the likelihood
    of getting a correct response. In the paper, the authors obtained 99.8% correct
    results, at the expense of making 10 LLM calls per question.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned several techniques you can utilize to improve your
    prompts to obtain better results. You learned to use longer prompts that ensure
    that the LLM has the necessary context to provide the desired response. You also
    learned how to add multiple parameters to a prompt. Then, you learned how to chain
    prompts and how to implement the CoT method to help the LLM provide more accurate
    results. Finally, you learned how to ensemble several responses to increase accuracy.
    This accuracy, however, comes at a cost.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have mastered prompts, in the next chapter, we will explore how
    to customize plugins and their native and semantic functions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language
    Models are Zero-Shot Reasoners.” arXiv, Jan. 29, 2023\. Accessed: Jun. 06, 2023\.
    [Online]. Available: [http://arxiv.org/abs/2205.11916](http://arxiv.org/abs/2205.11916)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Creating AI Applications with Semantic Kernel'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will go deep inside Semantic Kernel and learn how to use it
    to solve problems. We start by adding functions to a kernel, and then we use functions
    to solve a problem. The real power comes next when we ask the kernel to solve
    a problem on its own. Finally, we learn how to keep history for our kernel using
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B21826_03.xhtml#_idTextAnchor071)*, Extending Semantic Kernel*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21826_04.xhtml#_idTextAnchor086)*, Performing Complex Actions
    by Chaining Functions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21826_05.xhtml#_idTextAnchor106)*, Programming with Planners*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21826_06.xhtml#_idTextAnchor120)*, Adding Memories to Your AI
    Application*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
