- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Creating Better Prompts
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建更好的提示
- en: As a developer, you can request that an LLM completes a task by submitting a
    prompt to it. In the previous chapter, we saw some examples of prompts, such as
    “*Tell me a knock-knock joke*” and “*What is the flight duration between New York
    City and Rio de Janeiro?*” As LLMs became more powerful, the tasks that they could
    accomplish became more complex.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为开发者，你可以通过向大型语言模型提交提示来请求它完成一项任务。在上一章中，我们看到了一些提示的例子，例如“*讲一个敲门笑话*”和“*纽约市和里约热内卢之间的飞行时间是多少？*”。随着大型语言模型变得更加强大，它们能够完成的任务也变得更加复杂。
- en: 'Researchers discovered that using different techniques to build prompts yielded
    vastly different results. The process of crafting prompts that improve the likelihood
    of getting the desired answer is called prompt engineering, and the value of creating
    better prompts gave birth to a new profession: **prompt engineer**. This is someone
    who doesn’t need to know how to code in any programming language but can create
    prompts using natural language that return the desired results.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员发现，使用不同的技术构建提示会产生截然不同的结果。提高获得期望答案可能性的提示构建过程被称为提示工程，而创建更好的提示的价值催生了一个新的职业：**提示工程师**。这样的人不需要知道如何用任何编程语言编码，但可以使用自然语言创建提示，以返回期望的结果。
- en: Microsoft Semantic Kernel uses the concept of **prompt templating**, the creation
    of structured templates for prompts that contain placeholders for specific types
    of information and instructions that can be filled in or customized by the user
    or by the developer. By using prompt templates, developers can introduce multiple
    variables in prompts, separate the prompt engineering function from the coding
    function, and use advanced prompt techniques to increase accuracy in responses.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Semantic Kernel 使用 **提示模板** 的概念，为包含特定类型信息和指令的提示创建结构化模板，这些信息和指令可以由用户或开发者填充或自定义。通过使用提示模板，开发者可以在提示中引入多个变量，将提示工程功能与编码功能分离，并使用高级提示技术来提高响应的准确性。
- en: In this chapter, you’ll learn about several techniques that will make your prompts
    more likely to return the results you want your users to see in the first attempt.
    You’ll learn how to employ prompts with multiple variables as well as how to create
    and use prompts that have multiple parameters to complete more complex tasks.
    Finally, you’ll uncover techniques that combine prompts in creative ways to improve
    accuracy in scenarios where LLMs are not very accurate – for example, when solving
    math problems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，你将了解几种技术，这些技术将使你的提示更有可能在第一次尝试中返回你希望用户看到的预期结果。你将学习如何使用具有多个变量的提示，以及如何创建和使用具有多个参数的提示以完成更复杂的任务。最后，你将发现将提示以创造性的方式结合起来的技术，以在大型语言模型不太准确的情况下提高准确性——例如，在解决数学问题时。
- en: 'In this chapter, we’ll be covering the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Engineering prompts
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Prompts with multiple variables
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多变量提示
- en: Prompts with multiple stages
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多阶段提示
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To complete this chapter, you will need to have a recent, supported version
    of your preferred Python or C# development environment:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章，你需要拥有你首选的 Python 或 C# 开发环境的最新、受支持的版本：
- en: For Python, the minimum supported version is Python 3.10, and the recommended
    version is Python 3.11
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Python，最低支持的版本是 Python 3.10，推荐版本是 Python 3.11
- en: For C#, the minimum supported version is .NET 8
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 C#，最低支持的版本是 .NET 8
- en: You will also need an **OpenAI API** key, obtained either directly through **OpenAI**
    or through **Microsoft**, through the **Azure OpenAI** service. Instructions on
    how to obtain these keys can be found in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要一个 **OpenAI API** 密钥，可以通过 **OpenAI** 或通过 **Microsoft** 的 **Azure OpenAI**
    服务获得。如何获取这些密钥的说明可以在[*第1章*](B21826_01.xhtml#_idTextAnchor014)中找到。
- en: If you are using .NET, the code for this chapter can be found at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 .NET，本章的代码可以在[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch2)找到。
- en: If you are using Python, the code for this chapter can be found at [https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Python，本章的代码可以在[https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2](https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch2)找到。
- en: 'You can install the required packages by going to the GitHub repository and
    using the following: `pip install -``r requirements.txt`.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过访问GitHub仓库并使用以下命令安装所需的包：`pip install -r requirements.txt`。
- en: A simple plugin template
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个简单的插件模板
- en: There are two simple ways of creating prompt templates.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 创建提示模板有两种简单的方法。
- en: The first is to generate the prompt from a string variable inside your code.
    This way is simple and convenient. We covered this method in the *Running a simple
    prompt* section of [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是从代码中的字符串变量生成提示。这种方法简单方便。我们在[*第一章*](B21826_01.xhtml#_idTextAnchor014)的*运行简单提示*部分介绍了这种方法。
- en: 'The second is to use Semantic Kernel to help separate the development function
    from the prompt engineering function. As you saw in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014),
    you can create your requests to LLMs as functions in plugins. A plugin is a directory
    that contains multiple sub-directories, one per function. Each subdirectory will
    have exactly two files:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是使用Semantic Kernel来帮助将开发功能与提示工程功能分开。正如你在[*第一章*](B21826_01.xhtml#_idTextAnchor014)中看到的，你可以在插件中创建对LLMs的请求作为函数。插件是一个包含多个子目录的目录，每个子目录对应一个功能。每个子目录将恰好有两个文件：
- en: A text file called `skprompt.txt` that contains the prompt
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含提示的文本文件`skprompt.txt`
- en: A configuration file called `config.json` that contains the parameters that
    will be used in the API call
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为`config.json`的配置文件，其中包含将用于API调用的参数
- en: Since the prompt is maintained separately from the code, you, as an application
    developer, can focus on the code of your application and let a specialized prompt
    engineer work on the `skprompt.txt` files.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提示与代码分开维护，因此作为应用开发者，你可以专注于你的应用程序代码，让专门的提示工程师处理`skprompt.txt`文件。
- en: In this chapter, we will focus on the second method – creating plugins in dedicated
    directories – because this method is more robust to changes. For example, if you
    are switching from Python to C# or using a new version of the .NET library and
    these changes require a lot of changes to be made to your code, at least you don’t
    need to change your prompts and function configurations.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注第二种方法——在专用目录中创建插件，因为这种方法对变化更加稳健。例如，如果你从Python切换到C#或使用.NET库的新版本，并且这些更改需要对你代码进行大量修改，至少你不需要更改你的提示和功能配置。
- en: The code for this prompt plugin is like the one we used in the previous chapter.
    We are going to take the plugin we built in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014)
    and make several changes to `skprompt.txt` to observe the results and learn how
    different prompting techniques can substantially change outcomes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示插件的代码与我们在上一章中使用的是一样的。我们将对[*第一章*](B21826_01.xhtml#_idTextAnchor014)中构建的插件进行一些修改，对`skprompt.txt`文件进行更改，以观察结果并学习不同的提示技术如何显著改变结果。
- en: We are doing this again in detail at the beginning of this chapter even though
    we did something very similar in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014)
    because we’re going to use these steps several times as we explore prompt engineering.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章的开头再次详细地介绍这个过程，尽管我们在[*第一章*](B21826_01.xhtml#_idTextAnchor014)中已经介绍过类似的内容，因为在我们探索提示工程的过程中，我们将多次使用这些步骤。
- en: For the examples we’ll cover here, you can use both GPT-3.5 and GPT-4, but remember
    that GPT-4 is 30x more expensive. The results that are shown are from GPT-3.5
    unless indicated otherwise.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们将要介绍的内容，你可以使用GPT-3.5和GPT-4，但请记住，GPT-4的价格是GPT-3.5的30倍。除非另有说明，否则显示的结果来自GPT-3.5。
- en: The skprompt.txt file
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`skprompt.txt`文件'
- en: 'We will start with a very simple prompt, just asking directly what we want,
    without any additional context:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个非常简单的提示开始，直接询问我们想要的内容，没有任何额外的上下文：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That’s it – that’s the whole file.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了——这就是整个文件。
- en: The config.json file
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`config.json`文件'
- en: 'The `config.json` file is very similar to the one we used in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014),
    but we changed three things:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`config.json`文件与我们[*第一章*](B21826_01.xhtml#_idTextAnchor014)中使用的非常相似，但我们更改了三件事：'
- en: The description of the function
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数的描述
- en: The name of the input variable under `input_variables`
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `input_variables` 下的输入变量名称
- en: The description of the input variable under `input_variables`
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `input_variables` 下的输入变量描述
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have defined the function, let’s call it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了函数，让我们来调用它。
- en: Calling the plugin from Python
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Python 调用插件
- en: 'The code we’re using here is very similar to what we used in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014).
    We’ve made three small changes: use only GPT-3.5, point to the appropriate plugin
    directory, `prompt_engineering`, and change the input variable’s name from `input`
    to `city`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用的代码与我们[*第一章*](B21826_01.xhtml#_idTextAnchor014)中使用的非常相似。我们做了三个小的改动：只使用
    GPT-3.5，指向适当的插件目录 `prompt_engineering`，并将输入变量的名称从 `input` 改为 `city`：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let’s learn how to call the plugin in C#. Note that we don’t make any changes
    to `skprompt.txt` or `config.json`. We can use the same prompt and configuration,
    independent of the language.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习如何从 C# 调用插件。请注意，我们没有对 `skprompt.txt` 或 `config.json` 进行任何修改。我们可以使用相同的提示词和配置，不受语言限制。
- en: Calling the plugin from C#
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 C# 调用插件
- en: 'As we did in Python, we just need to make three small changes: use only GPT-3.5,
    point to the appropriate plugin directory, `prompt_engineering`, and change the
    input variable’s name from `input` to `city`:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 Python 中做的那样，我们只需要做三个小的改动：只使用 GPT-3.5，指向适当的插件目录 `prompt_engineering`，并将输入变量的名称从
    `input` 改为 `city`：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let’s see the results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果。
- en: Results
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'Instead of giving the top three attractions that I wanted to see, in one of
    the calls, GPT-3.5 created a 3-day list of attractions and gave me a list of six
    attractions. This is a typical problem that can happen when the prompt is not
    very specific. The list of attractions is very good and considers how hard it
    is to move around in New York City due to traffic: attractions on the same day
    are close to each other, or at least there is enough time to travel between them.
    Since a lot of GPT-3.5 training was done with data during the COVID period, the
    itinerary even includes a note to check for COVID closures.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是给出我想看的三个顶级景点，在一次调用中，GPT-3.5 创建了一个3天的景点列表，并给了我一个包含六个景点的列表。这是当提示词不够具体时可能发生的典型问题。景点列表非常好，并考虑到了由于交通拥堵，在纽约市移动的难度：同一天的景点彼此靠近，或者至少有足够的时间在它们之间旅行。由于大量的
    GPT-3.5 训练是在 COVID 期间进行的，行程甚至还包括了检查 COVID 关闭的备注。
- en: 'Note that your result may be different, and in some cases, you may get a response
    that looks like the original intent, with only three must-see attractions:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你的结果可能会有所不同，在某些情况下，你可能得到一个看起来像原始意图的响应，只有三个必看的景点：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This quick, simple prompt didn’t work consistently as expected, so let’s explore
    how to make things better by adding more information to the prompt.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个快速、简单的提示词并没有像预期的那样始终如一地工作，所以让我们通过在提示词中添加更多信息来探索如何让事情变得更好。
- en: Improving the prompt to get better results
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提高提示词以获得更好的结果
- en: 'If the main way you interact with LLMs is using ChatGPT or Microsoft Copilot,
    you may have the impression that you can use very short prompts to get the results
    you want. As explained in [*Chapter 1*](B21826_01.xhtml#_idTextAnchor014), these
    applications modify your submission and add a lot of instructions to your prompt.
    As a developer, you’ll have to do the same. Here are a few tips to improve your
    prompt and obtain better results:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你主要通过 ChatGPT 或 Microsoft Copilot 与 LLM 交互，你可能会有这样的印象：你可以使用非常短的提示词来得到你想要的结果。如[*第一章*](B21826_01.xhtml#_idTextAnchor014)中所述，这些应用程序修改了你的提交，并在提示词中添加了大量指令。作为一个开发者，你也必须这样做。以下是一些改进提示词并获得更好结果的建议：
- en: '**Provide context**: Instead of simply asking for three must-see attractions,
    provide as much context as you can. Think about everything that you would ask
    someone who asked you the same question – for example, “*How many days are you
    staying?*,” “*What kind of things do you like and dislike?*,” or “*How are you
    going to be* *getting around?*”'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供上下文**：而不仅仅是要求三个必看的景点，尽可能多地提供上下文。想想如果你被问同样的问题，你会问些什么——例如，“*你将停留多少天？*”，“*你喜欢和不喜欢什么？*”，或者“*你将如何四处走动？*”'
- en: '**Be explicit about the format of the response**: If you want the answer to
    come out in a specific format, make sure you tell the LLM this. You may even provide
    some examples – for example, “*Answer with a single word – ‘Yes’* *or ‘No.’*”'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**明确响应的格式**：如果你想以特定的格式得到答案，确保你告诉 LLM 这一点。你甚至可以提供一些示例——例如，“*用单个词回答——‘是’* 或 ‘否。’*”'
- en: '**Specify the response’s length**: If you want a concise answer, specify the
    desired length – for example, “*In two sentences.*” If you want a longer answer,
    make it clear – for example, “*600 words*” or “*five paragraphs.*”'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指定响应的长度**：如果你想得到一个简洁的回答，指定所需的长度——例如，“*两句话*”如果你想得到一个更长的回答，要明确指出——例如，“*600字*”或“*五段*”'
- en: Revising the skprompt.txt file
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修订 skprompt.txt 文件
- en: 'Using these hints, we’re going to rewrite the prompt to see if we can get improved
    responses consistently:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些提示，我们将重写提示以查看我们是否可以得到一致的改进响应：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: 'Here’s the response we get from the prompt:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这是提示返回的响应：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that we’ve created the prompt in such a way that you can use it with different
    cities. Here’s a run with London. Note that the response acknowledged my fear
    of heights, but simply told me to overcome it:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们创建的提示是这样的，你可以用它来应对不同的城市。这里是一个使用伦敦的示例。注意，响应承认了我对高度恐惧，但只是告诉我克服它：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: For many applications, you’ll need to make your prompt more configurable, and
    that will require adding more variables. We’ll do that next.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多应用，你需要使你的提示更加可配置，这将需要添加更多变量。我们将在下一节中这样做。
- en: Prompts with multiple variables
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有多个变量的提示
- en: You can parameterize a lot of information in a prompt. For example, in our prompt
    that finds the best attractions, you can add multiple variables, such as the number
    of days that the person is staying, the things they like and dislike, and how
    many attractions they want to see.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在提示中参数化很多信息。例如，在我们的最佳景点查找提示中，你可以添加多个变量，例如这个人停留的天数、他们喜欢和不喜欢的事物，以及他们想看多少个景点。
- en: In such a case, our prompt will become more complex, so we will need to create
    a new `skprompt.txt` file. Since our prompt will be a new function and have multiple
    parameters, we will also need to create a new `config.json` file.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的提示将变得更加复杂，因此我们需要创建一个新的 `skprompt.txt` 文件。由于我们的提示将是一个新的函数并且有多个参数，我们还需要创建一个新的
    `config.json` 文件。
- en: These two files can be found in the `plugins/prompt_engineering/attractions_multiple_variables`
    folder.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个文件可以在 `plugins/prompt_engineering/attractions_multiple_variables` 文件夹中找到。
- en: Skprompt.txt
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Skprompt.txt
- en: 'To add more variables to a prompt, simply add them within double curly brackets
    with a dollar sign before their name. The following code shows how to add several
    variables (`city`, `n_days`, `likes`, `dislikes`, and `n_attractions`) to a single
    prompt:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要向提示中添加更多变量，只需在双大括号内添加它们，并在变量名前加上美元符号。以下代码展示了如何将多个变量（`city`、`n_days`、`likes`、`dislikes`
    和 `n_attractions`）添加到单个提示中：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, let’s see the changes in the function configuration.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看函数配置的变化。
- en: Config.json
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Config.json
- en: 'Our `config.json` file for multiple variables is almost the same as the one
    we use with a single variable, but we need to add the details for all the variables:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于多个变量的 `config.json` 文件几乎与用于单个变量的文件相同，但我们需要为所有变量添加详细信息：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now that we’ve configured the new function, let’s learn how to call it in Python
    and C#.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了新的函数，让我们学习如何在 Python 和 C# 中调用它。
- en: Requesting a complex itinerary with Python
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 请求复杂行程
- en: 'Compared to calling a prompt with a single parameter, the only change we need
    to make to call a prompt with multiple parameters is on the `KernelArguments`
    object. When passing `KernelArguments` as a parameter to `kernel.invoke`, we must
    add all the parameters we need to the object, as shown here. One thing to note
    is that the parameters are all strings since LLMs work best with text:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与调用单个参数的提示相比，调用多个参数的提示所需做的唯一更改是在 `KernelArguments` 对象上。当将 `KernelArguments`
    作为参数传递给 `kernel.invoke` 时，我们必须将所有需要的参数添加到对象中，如这里所示。需要注意的是，参数都是字符串，因为 LLMs 在处理文本时表现最佳：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Let’s see the C# code.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 C# 代码。
- en: Requesting a complex itinerary with C#
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 C# 请求复杂行程
- en: 'This time, we will create a `KernelArguments` object called `function_arguments`
    outside of the function invocation and pre-fill the five variables with the content
    we want. Then, we will pass this object to the invocation call:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将在函数调用外部创建一个名为 `function_arguments` 的 `KernelArguments` 对象，并预先填充我们想要的内容的五个变量。然后，我们将把这个对象传递给调用调用：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, let’s see the results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看结果。
- en: The result of the complex itinerary
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂行程的结果
- en: 'The result considers all the input variables. It suggests a park – the High
    Line – even though I told the LLM that I dislike parks. It does explain that it
    is a very unusual park, and if you know New York, it doesn’t feel like a park
    at all. I think that a person who doesn’t enjoy the traditional park experience
    would enjoy the High Line, so the LLM did a very good job once it got more context:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 结果考虑了所有输入变量。它建议一个公园——高线公园——尽管我告诉了LLM我不喜欢公园。它确实解释说这是一个非常不寻常的公园，如果你了解纽约，它根本不像一个公园。我认为一个不喜欢传统公园体验的人会喜欢高线公园，所以LLM在获得更多上下文后做得非常好：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With that, we’ve learned how to improve our prompts to get better results. However,
    there are some cases in which LLMs fail to provide good answers, even after we
    use all the techniques mentioned previously. A very common case is solving math
    problems. We will explore this next.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经学会了如何改进我们的提示以获得更好的结果。然而，有些情况下，即使我们使用了之前提到的所有技术，LLMs也未能提供好的答案。一个非常常见的情况是解决数学问题。我们将接下来探讨这个问题。
- en: Issues when answering math problems
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回答数学问题时的问题
- en: 'Although the results from LLMs are impressive, sometimes, models can get confused
    by seemingly simple questions. This tends to be more frequent when the question
    involves math. For example, I ran the following prompt in GPT-3.5 five times:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LLMs的结果令人印象深刻，但有时模型会被看似简单的问题搞混。当问题涉及数学时，这种情况更为常见。例如，我在GPT-3.5上运行了以下提示五次：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The plugin I used to run this prompt is under `plugins/prompt_engineering/solve_math_problem`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我用来运行此提示的插件位于`plugins/prompt_engineering/solve_math_problem`。
- en: The correct answer is 67 because when I was 6, my sister was 3\. Now, 64 years
    later, I’m 70, so she would be 67.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案是67，因为当我6岁时，我妹妹是3岁。现在，64年后，我70岁，所以她会是67岁。
- en: Here are the results of the five runs on GPT-3.5\. The first result was incorrect,
    saying that “*my sister is 64 years younger than her* *current age*:”
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是GPT-3.5上五次运行的成果。第一次结果是错误的，说“*我的妹妹比她现在的年龄小64岁*：”
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The second and third attempts gave the correct answer. Here’s the second attempt:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次和第三次尝试给出了正确答案。这是第二次尝试：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here’s the third attempt:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第三次尝试：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'But the fourth and fifth attempts gave wrong answers again. Here’s the fourth
    attempt:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 但第四次和第五次尝试又给出了错误答案。这是第四次尝试：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here’s the fifth attempt:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第五次尝试：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Therefore, in five attempts, only two (40%) returned correct answers, which
    is not an experience that you want your users to have.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在五次尝试中，只有两次（40%）返回了正确答案，这不是你希望用户拥有的体验。
- en: We will learn how to address this issue in the next section.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中学习如何解决这个问题。
- en: Multistage prompts
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多阶段提示
- en: One way to improve the accuracy of LLMs when doing math is to use multistage
    prompts. In this technique, the answer from the first prompt is passed to the
    second as a parameter. We’re going to illustrate this with the **Chain-of-Thought**
    (**CoT**) technique.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数学时提高LLMs准确性的一个方法是多阶段提示。在这个技术中，第一次提示的答案作为参数传递给第二次。我们将使用**思维链**（**CoT**）技术来展示这一点。
- en: CoT – “Let’s think step by step”
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CoT – “让我们一步一步思考”
- en: 'In the paper *Large Language Models are Zero-Shot Reasoners* [1], the authors
    found that simply adding “Let’s think step by step” right after the question can
    help improve the accuracy of LLMs a lot. Their proposed process works as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文《大型语言模型是零样本推理者》[1]中，作者发现，在问题后简单添加“让我们一步一步思考”可以大大提高LLMs的准确性。他们提出的过程如下：
- en: Ask the intended question, but instead of asking the LLM to answer, simply append
    “*Let’s think step by step*” at the end.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提出你想要的问题，但不是要求LLM回答，而是在结尾简单地添加“*让我们一步一步思考*”。
- en: The LLM will answer with a process to answer the question.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LLM将以回答问题的过程来回答。
- en: Combine the question from *step 1* with the process from s*tep 2* in a new prompt,
    and finish with “*Therefore, the* *answer is…*:”
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将步骤1中的问题与步骤2中的过程结合到一个新的提示中，并以“*因此，答案是…*”结束。
- en: '![Figure 2.1 – The Zero-shot-CoT method](img/B21826_02_1.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – Zero-shot-CoT方法](img/B21826_02_1.jpg)'
- en: Figure 2.1 – The Zero-shot-CoT method
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – Zero-shot-CoT方法
- en: They called their process *Zero-shot-Chain-of-Thought* or *Zero-shot-CoT*. The
    *Zero-shot* part means that you don’t need to give any example answers to the
    LLM; you can give the question directly. This is to differentiate the process
    from **few-shot**, which is when you provide the LLM several examples of expected
    answers in your prompt, making the prompt substantially larger. The *CoT* part
    describes the process of asking the LLM to provide a reasoning framework and adding
    the LLM’s reasoning to the question.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 他们将他们的过程称为 *Zero-shot-Chain-of-Thought* 或 *Zero-shot-CoT*。*Zero-shot* 部分意味着你不需要向
    LLM 提供任何示例答案；你可以直接提出问题。这是为了区分与 **few-shot** 的过程，few-shot 是你在提示中提供 LLM 几个预期答案的示例，使提示变得更大。*CoT*
    部分描述了要求 LLM 提供推理框架的过程，并将 LLM 的推理添加到问题中。
- en: The authors tested several different phrases to obtain the CoT from the LLM,
    such as “*Let’s think about it logically*” and “*Let’s think like a detective
    step by step*,” and found that simply “*Let’s think step by step*” yielded the
    best results.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 作者测试了几个不同的短语来从 LLM 获取 CoT，例如“*让我们逻辑地思考它*”和“*让我们像侦探一样一步一步思考*”，并发现简单地“*让我们一步一步思考*”产生了最佳结果。
- en: Implementing Zero-shot-CoT
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 Zero-shot-CoT
- en: 'We need two prompts – one for both steps. For the first step, we will call
    `solve_math_problem_v2`. The prompt simply restates the problem and adds “*Let’s
    think step by step*” at the end:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两个提示 - 一个用于两个步骤。对于第一步，我们将调用 `solve_math_problem_v2`。提示简单地重申了问题，并在最后添加了“*让我们一步一步思考*”：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The prompt for the second step, which we will call `chain_of_thought`, repeats
    the first prompt, includes the answer for the first prompt, and then asks for
    the solution:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步的提示，我们将称之为 `chain_of_thought`，重复了第一个提示，包括第一个提示的答案，然后要求提供解决方案：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Given this prompt, the `config.json` file needs two `input_variables`:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这个提示，`config.json` 文件需要两个 `input_variables`：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s learn how to call the prompts.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何调用提示。
- en: Implementing CoT with Python
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python 实现 CoT
- en: 'To make the steps explicit, I broke the program into two parts. The first one
    asks for the CoT and shows it:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使步骤明确，我将程序分成两部分。第一部分请求 CoT 并显示它：
- en: '[PRE22]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The second part shows the answer. If all you care about is the answer, you
    don’t need to print the steps, but you still need to use the LLM to calculate
    the steps because they are a required parameter to the CoT:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分展示了答案。如果你只关心答案，你不需要打印步骤，但你仍然需要使用 LLM 来计算步骤，因为它们是 CoT 的必需参数：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Implementing CoT with C#
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 C# 实现 CoT
- en: 'As we did in Python, we can break the C# program into two parts. Part one will
    show the reasoning steps elicited by the CoT prompt:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 Python 中做的那样，我们可以将 C# 程序分成两部分。第一部分将展示由 CoT 提示引发的推理步骤：
- en: '[PRE24]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The second part parses and reports the answer:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分解析并报告答案：
- en: '[PRE25]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Let’s see the results.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果。
- en: Results for CoT
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CoT 的结果
- en: I ran the program five times, and it got the right answer every time. The answers
    aren’t deterministic, so it may be that you run it on your machine and get one
    or two wrong answers. In the paper, the authors claimed that this method achieves
    78.7% success in problems of this type, where the usual accuracy of LLMs is around
    17.7%.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我运行了程序五次，每次都得到了正确答案。答案不是确定的，所以你可能在你的机器上运行它时得到一两个错误答案。在论文中，作者声称这种方法在类似类型的问题上实现了
    78.7% 的成功率，而 LLM 的通常准确率约为 17.7%。
- en: 'Let’s look at two sample responses. Here’s the first:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两个示例响应。首先是：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here’s the second:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个：
- en: '[PRE27]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Instead of running the program manually, we can automate it. Let's see how.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以自动化程序而不是手动运行它。让我们看看如何做。
- en: An ensemble of answers
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案集合
- en: While the CoT technique helps a lot, we still have a 78.7% average accuracy,
    and that may not be enough. To address this problem, one technique that is frequently
    used is to ask the model the same question several times and compile only the
    most frequently given answer.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CoT 技术帮助很大，但我们仍然有 78.7% 的平均准确率，这可能还不够。为了解决这个问题，常用的一个技术是多次向模型提出相同的问题，并仅编译最频繁给出的答案。
- en: 'To achieve this, we will make a minor modification to our CoT prompt and call
    it `chain_of_thought_v2`. We will simply ask the LLM to answer in Arabic numerals,
    to make it easier to compare the answers in a later step:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将对我们的 CoT 提示进行微小修改，并将其称为 `chain_of_thought_v2`。我们将简单地要求 LLM 使用阿拉伯数字回答，以便在后续步骤中更容易比较答案：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We also need to change the program and ask it to run several times. For the
    next example, I chose *N* = 7\. We will collect the answers and choose the answer
    that appears more frequently. Note that each call using this method is *N* times
    more expensive and takes *N* times longer than a single call. Accuracy is not
    free.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要更改程序并要求它运行多次。对于下一个示例，我选择了*N* = 7。我们将收集答案并选择出现频率更高的答案。请注意，使用此方法每次调用都比单次调用贵*N*倍，耗时也长*N*倍。准确性不是免费的。
- en: Automatically running an ensemble with Python
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Python自动运行集成
- en: 'Let’s run CoT seven times. For each time we run it, we’ll add the result to
    a list. Then, we’ll take advantage of the `set` data structure to quickly get
    the most common element:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行CoT七次。每次运行它，我们将结果添加到一个列表中。然后，我们将利用`set`数据结构来快速获取最常见的元素：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Let’s see how to implement this in C#.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在C#中实现这一点。
- en: Automatically running an ensemble with C#
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用C#自动运行集成
- en: 'The C# code uses the same idea as the Python code: we run the model seven times
    and store the results. Then, we search the results for the most frequent answer:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: C#代码与Python代码使用相同的思想：我们运行模型七次并存储结果。然后，我们在结果中搜索最频繁的答案：
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Combining CoT and the ensemble method substantially increases the likelihood
    of getting a correct response. In the paper, the authors obtained 99.8% correct
    results, at the expense of making 10 LLM calls per question.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 将CoT和集成方法结合起来，大大增加了获得正确响应的可能性。在论文中，作者们以每题10次LLM调用的代价，获得了99.8%的正确结果。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned several techniques you can utilize to improve your
    prompts to obtain better results. You learned to use longer prompts that ensure
    that the LLM has the necessary context to provide the desired response. You also
    learned how to add multiple parameters to a prompt. Then, you learned how to chain
    prompts and how to implement the CoT method to help the LLM provide more accurate
    results. Finally, you learned how to ensemble several responses to increase accuracy.
    This accuracy, however, comes at a cost.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了你可以利用的几种技术来改进你的提示以获得更好的结果。你学习了使用更长的提示，确保LLM有必要的上下文来提供所需的响应。你还学习了如何向提示添加多个参数。然后，你学习了如何链式使用提示以及如何实现CoT方法以帮助LLM提供更准确的结果。最后，你学习了如何集成多个响应以提高准确性。然而，这种准确性是有代价的。
- en: Now that we have mastered prompts, in the next chapter, we will explore how
    to customize plugins and their native and semantic functions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了提示，在下一章中，我们将探讨如何自定义插件及其原生和语义函数。
- en: References
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language
    Models are Zero-Shot Reasoners.” arXiv, Jan. 29, 2023\. Accessed: Jun. 06, 2023\.
    [Online]. Available: [http://arxiv.org/abs/2205.11916](http://arxiv.org/abs/2205.11916)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large Language
    Models are Zero-Shot Reasoners.” arXiv, Jan. 29, 2023\. 访问时间：Jun. 06, 2023\. [在线].
    可用：[http://arxiv.org/abs/2205.11916](http://arxiv.org/abs/2205.11916)'
- en: 'Part 2: Creating AI Applications with Semantic Kernel'
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：使用语义内核创建AI应用
- en: In this part, we will go deep inside Semantic Kernel and learn how to use it
    to solve problems. We start by adding functions to a kernel, and then we use functions
    to solve a problem. The real power comes next when we ask the kernel to solve
    a problem on its own. Finally, we learn how to keep history for our kernel using
    memory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们将深入了解语义内核，学习如何使用它来解决问题。我们首先向内核添加函数，然后使用函数解决问题。真正的力量在于当我们要求内核自己解决问题时。最后，我们学习如何使用内存为我们的内核保存历史记录。
- en: 'This part includes the following chapters:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分包括以下章节：
- en: '[*Chapter 3*](B21826_03.xhtml#_idTextAnchor071)*, Extending Semantic Kernel*'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第3章*](B21826_03.xhtml#_idTextAnchor071)*，扩展语义内核*'
- en: '[*Chapter 4*](B21826_04.xhtml#_idTextAnchor086)*, Performing Complex Actions
    by Chaining Functions*'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第4章*](B21826_04.xhtml#_idTextAnchor086)*，通过链式函数执行复杂操作*'
- en: '[*Chapter 5*](B21826_05.xhtml#_idTextAnchor106)*, Programming with Planners*'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第5章*](B21826_05.xhtml#_idTextAnchor106)*，使用规划器编程*'
- en: '[*Chapter 6*](B21826_06.xhtml#_idTextAnchor120)*, Adding Memories to Your AI
    Application*'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B21826_06.xhtml#_idTextAnchor120)*，将记忆添加到您的AI应用中*'
