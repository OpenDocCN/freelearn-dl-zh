["```py\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\ndef stratified_pretraining_split(\n    data, text_column, label_column, test_size=0.1, random_state=42\n):\n    sss = StratifiedShuffleSplit(\n        n_splits=1, test_size=test_size, random_state=random_state)\n    for train_index, test_index in sss.split(\n        data[text_column], data[label_column]\n    ):\n        train_data = data.iloc[train_index]\n        test_data = data.iloc[test_index]\n    return train_data, test_data\n# Example usage\ndata = pd.read_csv('your_pretraining_data.csv')\ntrain_data, test_data = stratified_pretraining_split(\n    data, 'text', 'domain')\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Test set size: {len(test_data)}\")\n```", "```py\nimport pandas as pd\ndef time_based_finetuning_split(data, timestamp_column, split_date):\n    data[timestamp_column] = pd.to_datetime(data[timestamp_column])\n    train_data = data[data[timestamp_column] < split_date]\n    test_data = data[data[timestamp_column] >= split_date]\n    return train_data, test_data\n# Example usage\ndata = pd.read_csv('your_finetuning_data.csv')\nsplit_date = '2023-01-01'\ntrain_data, test_data = time_based_finetuning_split(\n    data, 'timestamp', split_date)\nprint(f\"Training set size: {len(train_data)}\")\nprint(f\"Test set size: {len(test_data)}\")\n```", "```py\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\nimport torch\nimport numpy as np\n# Example class distribution (e.g., from dataset labels)\nlabels = [0, 0, 0, 1, 1, 2]  # Class 2 is underrepresented\n# --- 1\\. Class Weighting ---\n# Compute weights inversely proportional to class frequencies\nclass_weights = compute_class_weight(\n    'balanced', classes=np.unique(labels), y=labels)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)\n# Pass weights to loss function\nloss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n# --- 2\\. Oversampling with Weighted Sampler ---\n# Create sample weights: inverse of class frequency for each label\nlabel_counts = np.bincount(labels)\nsample_weights = [1.0 / label_counts[label] for label in labels]\n# Create sampler for DataLoader\nsampler = WeightedRandomSampler(\n    weights=sample_weights, num_samples=len(labels), replacement=True\n)\n# Use the sampler in your DataLoader\n# Assuming `train_dataset` is a PyTorch Dataset object\ntrain_loader = DataLoader(train_dataset, sampler=sampler,\n    batch_size=4)\n```", "```py\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\nimport torch\ndef few_shot_evaluate(\n    model, tokenizer, task_description, examples, test_instance\n):\n    prompt = f\"{task_description}\\n\\nExamples:\\n\"\n    for example in examples:\n        prompt += (\n            f\"Input: {example['input']}\\n\"\n            f\"Output: {example['output']}\\n\\n\"\n        )\n    prompt += f\"Input: {test_instance}\\nOutput:\"\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    with torch.no_grad():\n        output = model.generate(input_ids, max_length=100,\n            num_return_sequences=1)\n    generated_text = tokenizer.decode(output[0],\n        skip_special_tokens=True)\n    return generated_text.split(\"Output:\")[-1].strip()\n# Example usage\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-large')\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\ntask_description = \"Classify the sentiment of the following movie reviews as positive or negative.\"\nexamples = [\n    {\"input\": \"This movie was fantastic!\", \"output\": \"Positive\"},\n    {\"input\": \"I hated every minute of it.\", \"output\": \"Negative\"}\n]\ntest_instance = \"The acting was superb, but the plot was confusing.\"\nresult = few_shot_evaluate(\n    model, tokenizer, task_description, examples, test_instance\n)\nprint(f\"Few-shot evaluation result: {result}\")\n```", "```py\ndef zero_shot_evaluate(\n    model, tokenizer, task_description, test_instance\n):\n    prompt = f\"{task_description}\\n\\nInput: {test_instance}\\nOutput:\"\n    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n    with torch.no_grad():\n        output = model.generate(\n            input_ids, max_length=100, num_return_sequences=1\n        )\n    generated_text = tokenizer.decode(output[0],\n        skip_special_tokens=True)\n    return generated_text.split(\"Output:\")[-1].strip()\n# Example usage\ntask_description = \"Classify the following text into one of these categories: Science, Politics, Sports, Entertainment.\"\ntest_instance = \"NASA's Mars rover has discovered traces of ancient microbial life.\"\nresult = zero_shot_evaluate(\n    model, tokenizer, task_description, test_instance\n)\nprint(f\"Zero-shot evaluation result: {result}\")\n```", "```py\ndef evaluate_domain_adaptation(\n    model, tokenizer, source_domain_data, target_domain_data\n):\n    def predict(text):\n        inputs = tokenizer(\n            text, return_tensors='pt', truncation=True, padding=True\n        )\n        outputs = model(inputs)\n        return torch.argmax(outputs.logits, dim=1).item()\n    # Evaluate on source domain\n    source_predictions = [\n        predict(text) for text in source_domain_data['text']\n    ]\n    source_accuracy = accuracy_score(\n        source_domain_data['label'], source_predictions\n)\n    # Evaluate on target domain\n    target_predictions = [\n        predict(text) for text in target_domain_data['text']\n    ]\n    target_accuracy = accuracy_score(\n        target_domain_data['label'], target_predictions\n)\n    return {\n        'source_accuracy': source_accuracy,\n        'target_accuracy': target_accuracy,\n        'adaptation_drop': source_accuracy - target_accuracy\n    }\n```", "```py\nsource_domain_data = load_source_domain_data()  # Replace with actual data loading\ntarget_domain_data = load_target_domain_data()  # Replace with actual data loading\nresults = evaluate_domain_adaptation(\n    model, tokenizer, source_domain_data, target_domain_data\n)\nprint(f\"Source domain accuracy: {results['source_accuracy']:.2f}\")\nprint(f\"Target domain accuracy: {results['target_accuracy']:.2f}\")\nprint(f\"Adaptation drop: {results['adaptation_drop']:.2f}\")\n```", "```py\ndef evaluate_task_generalization(\n    model_name, tasks=['mnli', 'qqp', 'qnli', 'sst2']\n):\n    results = {}\n    for task in tasks:\n        model = \\\n            AutoModelForSequenceClassification.from_pretrained(\n            model_name)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        dataset = load_dataset('glue', task)\n        def tokenize_function(examples):\n            return tokenizer(\n                examples['sentence'], truncation=True, padding=True)\n        tokenized_datasets = dataset.map(tokenize_function,\n            batched=True)\n        training_args = TrainingArguments(\n            output_dir=f\"./results_{task}\",\n            evaluation_strategy=\"epoch\",\n            num_train_epochs=1,\n        )\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            train_dataset=tokenized_datasets['train'],\n            eval_dataset=tokenized_datasets['validation'],\n        )\n        eval_results = trainer.evaluate()\n        results[task] = eval_results['eval_accuracy']\n    return results\n```", "```py\nmodel_name = \"bert-base-uncased\"  # Replace with your model\ngeneralization_results = evaluate_task_generalization(model_name)\nfor task, accuracy in generalization_results.items():\n    print(f\"{task} accuracy: {accuracy:.2f}\")\n```", "```py\n    def evaluate_continual_learning(\n        model_name, tasks=['sst2', 'qnli', 'qqp'], num_epochs=3\n    ):\n        model = \\\n            AutoModelForSequenceClassification.from_\n            pretrained(model_name)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        results = {}\n    ```", "```py\n    def preprocess_function(examples, task):\n        # Different tasks have different input formats\n        if task == 'qqp':\n            texts = (examples['question1'], examples['question2'])\n        elif task == 'qnli':\n            texts = (examples['question'], examples['sentence'])\n        else:  # sst2\n            texts = (examples['sentence'], None)\n        tokenized = tokenizer(*texts, padding=True, truncation=True)\n        tokenized['labels'] = examples['label']\n        return tokenized\n    ```", "```py\n    for task in tasks:\n        dataset = load_dataset('glue', task)\n        tokenized_dataset = dataset.map(\n            lambda x: preprocess_function(x, task),\n            batched=True,\n            remove_columns=dataset['train'].column_names\n        )\n        model.config.num_labels = 3 if task == 'mnli' else 2\n    ```", "```py\n    trainer = Trainer(\n        model=model,\n        args=TrainingArguments(\n            output_dir=f\"./results_{task}\",\n            num_train_epochs=num_epochs,\n            learning_rate=2e-5,\n            per_device_train_batch_size=16,\n            per_device_eval_batch_size=16,\n            evaluation_strategy=\"epoch\"\n        ),\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=tokenized_dataset['validation']\n    )\n    trainer.train()\n    ```", "```py\n    task_results = {}\n    for eval_task in tasks[:tasks.index(task)+1]:\n        eval_dataset = load_dataset('glue', eval_task)['validation']\n        eval_tokenized = eval_dataset.map(\n            lambda x: preprocess_function(x, eval_task),\n            batched=True,\n            remove_columns=eval_dataset.column_names\n        )\n        eval_results = trainer.evaluate(eval_dataset=eval_tokenized)\n        task_results[eval_task] = eval_results['eval_accuracy']\n    results[task] = task_results\n    ```", "```py\n    model_name = \"bert-base-uncased\"  # Replace with your model\n    cl_results = evaluate_continual_learning(model_name)\n    for task, task_results in cl_results.items():\n        print(f\"\\nAfter training on {task}:\")\n        for eval_task, accuracy in task_results.items():\n            print(f\"  {eval_task} accuracy: {accuracy:.2f}\")\n    ```", "```py\nfrom datasketch import MinHash, MinHashLSH\nimport numpy as np\ndef deduplicate_data(texts, threshold=0.8):\n    # Initialize LSH index for fast similarity search\n    lsh = MinHashLSH(threshold=threshold, num_perm=128)\n    unique_texts = []\n    for idx, text in enumerate(texts):\n        minhash = MinHash(num_perm=128)\n        for ngram in get_ngrams(text):\n            minhash.update(ngram.encode('utf8'))\n        if not lsh.query(minhash):  # Check if similar text exists\n            lsh.insert(str(idx), minhash)\n            unique_texts.append(text)\n    return unique_texts\n```", "```py\n    from sklearn.model_selection import StratifiedKFold\n    from collections import defaultdict\n    def create_efficient_splits(data, labels, n_splits=5):\n        # Group data by domain\n        domain_data = defaultdict(list)\n        for text, domain in zip(data, labels):\n            domain_data[domain].append(text)\n        # Create stratified splits\n        skf = StratifiedKFold(n_splits=n_splits, shuffle=True)\n        splits = []\n        for train_idx, val_idx in skf.split(data, labels):\n            splits.append((train_idx, val_idx))\n        return splits\n    ```", "```py\n    def evaluate_domain_performance(model, tokenizer, eval_data):\n        domain_scores = defaultdict(list)\n        for text, domain in eval_data:\n            inputs = tokenizer(text, return_tensors='pt')\n            with torch.no_grad():\n                outputs = model(inputs)\n                score = outputs.logits.mean().item()\n                domain_scores[domain].append(score)\n        # Calculate domain-specific metrics\n        return {domain: np.mean(scores)\n            for domain, scores in domain_scores.items()}\n    ```", "```py\n    def evaluate_with_prompt_ensemble(\n        model, tokenizer, text, base_prompt\n    ):\n        prompt_variants = [\n            f\"{base_prompt}: {text}\",\n            f\"Please {base_prompt.lower()}: {text}\",\n            f\"I want you to {base_prompt.lower()}: {text}\"\n        ]\n        responses = []\n        for prompt in prompt_variants:\n            inputs = tokenizer(prompt, return_tensors='pt')\n            with torch.no_grad():\n                output = model.generate(inputs, max_length=100)\n                responses.append(tokenizer.decode(output[0]))\n        # Aggregate responses (e.g., by voting or averaging)\n        return aggregate_responses(responses)\n    ```", "```py\ndef robust_evaluation_pipeline(model, data, domains):\n    # First deduplicate the data\n    clean_data = deduplicate_data(data)\n    # Create efficient splits\n    splits = create_efficient_splits(clean_data, domains)\n    # Evaluate across domains with prompt ensembles\n    results = defaultdict(dict)\n    for domain in domains:\n        domain_data = [d for d, dom in zip(clean_data, domains)\n            if dom == domain]\n        scores = evaluate_with_prompt_ensemble(model, tokenizer,\n            domain_data, \"analyze\")\n        results[domain] = scores\n    return results\n```"]