- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing LLMs, RAGs, and Neo4j Knowledge Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Artificial Intelligence** (**AI**) is evolving beyond niche and specialized
    fields to become more accessible and able to assist with day-to-day tasks. One
    of the best examples is the explosive advent of **Generative AI** (**GenAI**).
    In the last few years, GenAI has created a lot of excitement both for technology
    builders and regular users with its ease of use and ability to understand and
    answer questions the way humans can. The breakthroughs in **Large Language Models**
    (**LLMs**) have propelled GenAI to the forefront. This has opened up a lot of
    opportunities for businesses to change how they interact with their customers.
    Customers can ask a question in natural language and get an answer without needing
    a human to be available to understand the question or understand the data to extract
    intelligence from it. While GenAI has taken big strides in different fields with
    different modalities, such as text, audio, and video, our focus throughout this
    book remains on LLMs and their applications in business and industry use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take a look at GenAI through the lens of LLMs, its
    impact, pitfalls, and ethical concerns. To set the stage for this book, we will
    briefly introduce techniques that can augment LLMs to make them more effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Outlining the evolution of GenAI through the lens of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the importance of RAGs and knowledge graphs in LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Neo4j knowledge graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlining the evolution of GenAI through the lens of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In late 2022, OpenAI took the world by storm by releasing an AI engine called
    ChatGPT that could understand language like humans and interact with users in
    natural language. This was the best representation of GenAI in a long time. AI
    concepts started as rules-based systems and evolved into machine learning algorithms
    in the ‘90s. With the rise of deep learning and LLMs, the concept of GenAI became
    more popular. These AI systems could generate new content after being trained
    using existing content. OpenAI’s GPT-3 LLM model was one of the first LLMs that
    captured the interest of the masses. GenAI can be used to get answers in a manner
    that feels like human interaction and it can also be used to generate images by
    providing a text description, describe an image as text content, generate videos
    using text content, and many other things. It can enhance creativity, accelerate
    research and development, enable a simple understanding of complex concepts, and
    improve personalization.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of LLMs is at the heart of GenAI’s popularity. Let’s take a look
    at LLMs and how they have propelled GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An LLM is a machine learning model that is built for natural language processing
    and can understand language constructs and generate content in that language based
    on the training.
  prefs: []
  type: TYPE_NORMAL
- en: Before the popularity of GPT-3, a considerable amount of research had been conducted
    on LLMs fover several years. Some of the notable works that pioneered LLMs include
    Google’s **Bidirectional Encoder Representations from Transformers** (**BERT**
    (https://github.com/google-research/bert)) and **Generative Pre-trained Transformer**
    (**GPT**) from OpenAI. LLM training requires a lot of parameters and computing
    power.
  prefs: []
  type: TYPE_NORMAL
- en: At their core, LLMs are a type of **Recurrent Neural Network** (**RNN**) architecture.
    Traditional RNNs struggle with long-term dependencies in sequential data. To address
    this, LLMs often leverage architectures such as **Long Short-Term Memory** (**LSTM**)
    networks or transformers. These architectures allow the model to learn complex
    relationships between words, even words that are separated by large distances
    within the training text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple illustration of a basic LLM architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 — Flowchart explaining a basic LLM architecture](img/B31107_01_1-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 — Flowchart explaining a basic LLM architecture
  prefs: []
  type: TYPE_NORMAL
- en: Let’s dissect this architecture
  prefs: []
  type: TYPE_NORMAL
- en: '**Input layer**: This layer receives the initial text prompt or sequence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding layer**: Words in the input sequence are converted into numerical
    vectors, capturing their semantic meaning'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder**: This is a multi-layered RNN (e.g., LSTM) or transformer that processes
    the sequence of embedded words, capturing contextual information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: The decoder utilizes the encoded representation to generate the
    output sequence one word at a time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can read more about LLMs in this paper: https://arxiv.org/pdf/2307.06435.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Building an LLM requires a lot of effort and resources. Let’s look at the number
    of parameters used by OpenAI to train each GPT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPT-1**: This is the first model and used 117 million parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-2**: This model used 1.5 billion parameters to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-3**: This model was the first general-purpose model released. 175 billion
    parameters were used to train this model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT-4 series**: This is the latest model released by OpenAI. 170 trillion
    parameters were used to train this model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These training figures demonstrate that with each new version, the number of
    parameters increased by several orders of magnitude. This means more and more
    computing power is needed to train these models. Similar training numbers can
    be observed for other LLM models too.
  prefs: []
  type: TYPE_NORMAL
- en: While GenAI is a great technology, there are pitfalls as well as legal and ethical
    concerns about the application of this technology. We will take a look at them
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GenAI’s pitfalls and ethical concerns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While LLMs are great at summarizing, generating context, and other use cases,
    they still do not understand the language per se. They recognize patterns based
    on the training text to generate new text. They also don’t understand facts or
    understand emotions or ethics. They are simply predicting the next token and generating
    text. Because of these pitfalls, content generated by GenAI can have huge consequences.
  prefs: []
  type: TYPE_NORMAL
- en: To understand and address these aspects, we need to first identify any harmful
    or inaccurate content that is being generated and address it either by retraining
    the model or adding separate checks and balances to make sure this content is
    not used as output.
  prefs: []
  type: TYPE_NORMAL
- en: For example, there have been recent cases about using LLMs to generate legal
    briefs, where LLMs have created non-existent cases and generated a legal brief
    based on these cases. While technically it might have generated a solution that
    is requested, this is legally not correct. There have also been cases where LLMs
    are used to generate offensive images and videos and shared on the internet. Since
    it is difficult to identify content generated by AI, it is easy to be fooled by
    this content. This is neither socially, legally, or ethically acceptable. There
    are quite a few examples where LLMs simply make up facts.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial on the Microsoft site (https://learn.microsoft.com/en-us/training/modules/responsible-ai-studio/)
    provides a detailed explanation of these concerns and how we can identify them.
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval-Augmented Generation** (**RAG**) and **knowledge graphs** together
    can help address these issues, which we discuss next.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the importance of RAGs and knowledge graphs in LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To address the pitfalls of GenAI, we can either fine-tune the model or ground
    the responses using other sources.
  prefs: []
  type: TYPE_NORMAL
- en: '**Fine-tuning** involves training an existing model with additional information,
    which can result in high-quality responses. But this can be a complex and time-consuming
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: The **RAG approach** involves providing extra information when we are asking
    the LLM a question.
  prefs: []
  type: TYPE_NORMAL
- en: With this approach, you can integrate knowledge repositories into the generative
    process. In this scenario, LLM can leverage the extra information retrieved from
    other sources and tune the response to match the information provided, thus grounding
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'These repositories and sources can include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Publicly available structured datasets** (e.g., scientific databases such
    as PubMed or publicly accessible encyclopedic resources such as Wikipedia)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enterprise knowledge bases** (e.g., internal company documentation, product
    catalogs, or compliance-related content with strict privacy and security requirements)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain-specific sources** (e.g., legal case records, medical guidelines,
    or technical manuals tailored to specific industries)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By incorporating relevant information from these repositories and sources, RAG
    empowers LLMs to generate output that is not only factually accurate but also
    contextually aligned with the task at hand. Unlike the static knowledge encoded
    in the LLM’s training data, these additional data sources allow real-time retrieval
    of up-to-date and specialized information, addressing challenges such as data
    freshness, accuracy, and specificity. We will cover RAG in detail in [*Chapter
    2*](Preface.xhtml#_idTextAnchor011).
  prefs: []
  type: TYPE_NORMAL
- en: Another source of information to enable RAG is knowledge graphs. Let’s briefly
    talk about them and their role in the LLM landscape.
  prefs: []
  type: TYPE_NORMAL
- en: The role of knowledge graphs in LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowledge graphs play a huge role in generating creative and contextually rich
    content for LLMs. They provide a structured, interconnected foundation and make
    information retrieval more
  prefs: []
  type: TYPE_NORMAL
- en: relevant and insightful by grounding the AI results in a complex and multi-layered
    understanding of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Representing the data as a graph opens up more avenues to understand the data.
    At the same time, a knowledge graph cannot be a static entity that represents
    data in only one dimension that’s fixed. Its true power lies in its ability to
    be dynamic and multi-dimensional. It can capture temporal, spatial, or contextual
    information in real time through live data feeds.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from being an important tool for storing information, knowledge graphs
    are the backbone of intelligent, context-aware AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several reasons why knowledge graphs are essential for GenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Enhanced contextual understanding**: Knowledge graphs allow GenAI systems
    to retrieve relevant information based on relationships, not just isolated facts.
    For example, in healthcare, a knowledge graph could link symptoms, diseases, and
    treatments, enabling GenAI to suggest more accurate diagnostic insights based
    on interconnected medical knowledge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient data retrieval**: Unlike traditional databases, knowledge graphs
    allow multi-hop reasoning, from which GenAI can draw insights across several degrees
    of separation. This is invaluable in fields such as finance, where GenAI can use
    knowledge graphs to reveal hidden relationships between entities such as customers,
    transactions, and market trends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration of vector embeddings**: When combined with vector embeddings,
    knowledge graphs enable GenAI to understand and respond to more nuanced queries.
    Vector embeddings capture semantic similarities between data points, which knowledge
    graphs then contextualize, creating a powerful blend of accuracy and relevance
    in responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-world impact**: Major organizations are already harnessing the power
    of knowledge graphs to enhance GenAI applications. For instance, companies in
    e-commerce use knowledge graphs to provide product recommendations that are not
    just relevant but contextually rich, drawing from diverse data sources such as
    customer reviews, purchase history, and product features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By integrating knowledge graphs, GenAI models transcend traditional data limitations,
    helping to create smarter, more reliable applications across different fields.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now talk about **Neo4j knowledge graphs**.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Neo4j knowledge graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A knowledge graph is dynamic and continues to evolve based on how data and relationships
    within the data evolve with time.
  prefs: []
  type: TYPE_NORMAL
- en: Neo4j is a database that excels with its ability to store data in graphs. For
    example, in a store, most products are laid out in a certain grouping and stay
    in those groups. But there is an exception to this arrangement. When a store wants
    to promote some products, they are placed at the front of the store. This kind
    of flexible thought process should be adapted for our knowledge graph implementation.
    As the semantics of data evolves the knowledge graph should be able to capture
    this change.
  prefs: []
  type: TYPE_NORMAL
- en: Neo4j, with its multiple labels for nodes and its optional schema approach,
    makes it easy to keep our graph relevant by helping us to persist (retain) our
    understanding of data as an extra label on the node, or a specific relationship
    that provides more relevant context between the nodes. We will take a deeper look
    at how we can build a Neo4j knowledge graph from the ground up in the upcoming
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s see how a Neo4j knowledge graph works to enhance an LLM’s response.
  prefs: []
  type: TYPE_NORMAL
- en: Using Neo4j knowledge graphs with LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose there is an LLM-based chatbot integrated with a Neo4j knowledge graph.
    This GenAI chatbot is designed to answer medical queries. *Figure 1.2* illustrates
    how a Neo4j knowledge graph can enhance this chatbot’s medical reasoning by linking
    structured patient symptom records with unstructured insights from medical research
    papers and clinical trials.
  prefs: []
  type: TYPE_NORMAL
- en: 'The unstructured text undergoes embedding-based processing using models from
    providers such as **Ollama**, **OpenAI**, and **Hugging Face**, followed by **Named
    Entity Recognition** (**NER**), to extract key entities such as symptoms and treatments.
    This data is integrated into a Neo4j knowledge graph, where documents mention
    symptoms and treatments, patients show symptoms, and symptoms are linked to potential
    treatments. This enables **multi-hop reasoning**, allowing a chatbot to efficiently
    answer complex queries such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Which patients are showing symptoms similar to flu and also showed symptoms
    of COVID-19 in the past?*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 — Neo4j knowledge graph driven Gen-AI for healthcare](img/B31107_01_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 — Neo4j knowledge graph driven Gen-AI for healthcare
  prefs: []
  type: TYPE_NORMAL
- en: 'To retrieve the result of this query, a **multi-hop knowledge graph query path**
    (*Figure 1.2*) will be followed in this order:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve symptoms linked to flu from research documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify patients currently showing those symptoms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cross-reference past patient records for COVID-19 symptoms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return patients who match both conditions with supporting document sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With this approach, the LLM response can be grounded to generate factually correct,
    relevant, and up-to-date results to support medical decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: A similar approach can be used to augment LLMs that support other applications.
  prefs: []
  type: TYPE_NORMAL
- en: We have now looked at how knowledge graphs enhance GenAI’s ability to provide
    contextually rich, accurate insights. But how does this transformative power translate
    into concrete benefits in real life? We will continue this journey in the rest
    of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the evolution of GenAI in the context of LLMs.
    We also looked at how RAG and knowledge graphs are key enablers of this transformation
    and help provide structure and context, improving an LLM’s accuracy and reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead, the next chapter dives deep into RAG — a technique that significantly
    enhances GenAI’s accuracy by grounding responses in retrieved, verified information.
  prefs: []
  type: TYPE_NORMAL
