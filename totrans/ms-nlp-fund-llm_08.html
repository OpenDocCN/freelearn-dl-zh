<html><head></head><body>
<div id="_idContainer363" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-177"><a id="_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-178" class="calibre4"><a id="_idTextAnchor441" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Accessing the Power of Large Language Models: Advanced Setup and Integration with RAG</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.3.1">In this dynamic era of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">Artificial Intelligence</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">AI</span></strong><span class="kobospan" id="kobo.7.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.8.1">Machine Learning</span></strong><span class="kobospan" id="kobo.9.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.10.1">ML</span></strong><span class="kobospan" id="kobo.11.1">), understanding the vast assortment of available resources and learning how to utilize them effectively is vital. </span><strong class="bold"><span class="kobospan" id="kobo.12.1">Large Language Models</span></strong><span class="kobospan" id="kobo.13.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.14.1">LLMs</span></strong><span class="kobospan" id="kobo.15.1">) such as GPT-4 have revolutionized the field of </span><strong class="bold"><span class="kobospan" id="kobo.16.1">Natural</span></strong> <strong class="bold"><span class="kobospan" id="kobo.17.1">Language Processing</span></strong><span class="kobospan" id="kobo.18.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.19.1">NLP</span></strong><span class="kobospan" id="kobo.20.1">) by showcasing unprecedented performance in diverse tasks, from content generation to complex problem-solving. </span><span class="kobospan" id="kobo.20.2">Their immense potential extends not only to understanding and generating human-like text but also to bridging the gap between machines and humans, in terms of communication and task automation. </span><span class="kobospan" id="kobo.20.3">Embracing the practical applications of LLMs can empower businesses, researchers, and developers to create more intuitive, intelligent, and efficient systems that cater to a wide range of requirements. </span><span class="kobospan" id="kobo.20.4">This chapter offers a guide to setting up access to LLMs, walking you through using them and building pipelines </span><span><span class="kobospan" id="kobo.21.1">with them.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.22.1">Our journey begins by delving into closed source models that utilize </span><strong class="bold"><span class="kobospan" id="kobo.23.1">Application Programming Interfaces</span></strong><span class="kobospan" id="kobo.24.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.25.1">APIs</span></strong><span class="kobospan" id="kobo.26.1">), taking OpenAI’s API as a quintessential example. </span><span class="kobospan" id="kobo.26.2">We will walk you through a practical scenario, illustrating how you can interact with this API using an API key within your Python code, demonstrating the potential applications of such models in </span><span><span class="kobospan" id="kobo.27.1">real-world contexts.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.28.1">As we advance, we will shift our focus to the realm of open source tools, giving you a rundown of widely employed open source models that can be manipulated via Python. </span><span class="kobospan" id="kobo.28.2">We aim to provide a grasp of the power and versatility these models provide, emphasizing the community-driven benefits of open </span><span><span class="kobospan" id="kobo.29.1">source development.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.30.1">Subsequently, we will introduce you to retrieval-augmented generation and, specifically, LangChain, a robust tool specifically engineered for interaction with LLMs. </span><span class="kobospan" id="kobo.30.2">LangChain is essential for the practical application of LLMs because it provides a unified and abstracted interface to them, as well as a suite of tools and modules that simplify the development and deployment of LLM-powered applications. </span><span class="kobospan" id="kobo.30.3">We’ll guide you through the foundational concept of LangChain, highlighting its distinctive methodology to circumvent the inherent challenges posed </span><span><span class="kobospan" id="kobo.31.1">by LLMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.32.1">The cornerstone of this methodology is the transformation of data into embeddings. </span><span class="kobospan" id="kobo.32.2">We will shed light on the pivotal role that </span><strong class="bold"><span class="kobospan" id="kobo.33.1">Language Models</span></strong><span class="kobospan" id="kobo.34.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.35.1">LMs</span></strong><span class="kobospan" id="kobo.36.1">) and LLMs play in this transformation, demonstrating how they are engaged in creating these embeddings. </span><span class="kobospan" id="kobo.36.2">Following this, we will discuss the process of establishing a local vector database, giving you a brief overview of vector databases and their crucial role in managing and retrieving </span><span><span class="kobospan" id="kobo.37.1">these embeddings.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.38.1">Then, we will address the configuration of an LLM for prompting, which could potentially be the same LLM used for the embedding process. </span><span class="kobospan" id="kobo.38.2">We will take you through the stepwise setup procedure, detailing the advantages and potential applications of </span><span><span class="kobospan" id="kobo.39.1">this strategy.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.40.1">In the penultimate segment, we will touch upon the topic of deploying LLMs to the cloud. </span><span class="kobospan" id="kobo.40.2">The scalability and cost-effectiveness of cloud services have led to an increased adoption of hosting AI models. </span><span class="kobospan" id="kobo.40.3">We will provide an overview of the leading cloud service providers, including </span><strong class="bold"><span class="kobospan" id="kobo.41.1">Microsoft Azure</span></strong><span class="kobospan" id="kobo.42.1">, </span><strong class="bold"><span class="kobospan" id="kobo.43.1">Amazon Web Services</span></strong><span class="kobospan" id="kobo.44.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.45.1">AWS</span></strong><span class="kobospan" id="kobo.46.1">), and </span><strong class="bold"><span class="kobospan" id="kobo.47.1">Google Cloud Platform</span></strong><span class="kobospan" id="kobo.48.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.49.1">GCP</span></strong><span class="kobospan" id="kobo.50.1">), giving you insights into their service offerings and how they can be harnessed for </span><span><span class="kobospan" id="kobo.51.1">LLM deployment.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.52.1">As we embark on this exploration of LLMs, it’s crucial to acknowledge the continuously evolving data landscape that these models operate within. </span><span class="kobospan" id="kobo.52.2">The dynamic nature of data – its growth in volume, diversity, and complexity – necessitates a forward-looking approach to how we develop, deploy, and maintain LLMs. </span><span class="kobospan" id="kobo.52.3">In the subsequent chapters, particularly </span><a href="B18949_10.xhtml#_idTextAnchor525" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.53.1">Chapter 10</span></em></span></a><span class="kobospan" id="kobo.54.1">, we will delve deeper into the strategic implications of these evolving data landscapes, preparing you to navigate the challenges and opportunities they present. </span><span class="kobospan" id="kobo.54.2">This foundational understanding will not only enhance your immediate work with LLMs but also ensure your projects remain resilient and relevant in the face of rapid technological and </span><span><span class="kobospan" id="kobo.55.1">data-driven changes.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.56.1">Let’s go through the main topics covered in </span><span><span class="kobospan" id="kobo.57.1">the chapter:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.58.1">Setting up an LLM application – API-based closed </span><span><span class="kobospan" id="kobo.59.1">source models</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.60.1">Prompt engineering and </span><span><span class="kobospan" id="kobo.61.1">priming GPT</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.62.1">Setting up an LLM application – local open </span><span><span class="kobospan" id="kobo.63.1">source models</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.64.1">Employing LLMs from Hugging Face </span><span><span class="kobospan" id="kobo.65.1">via Python</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.66.1">Exploring advanced system design – RAG </span><span><span class="kobospan" id="kobo.67.1">and LangChain</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.68.1">Reviewing a simple LangChain setup in a </span><span><span class="kobospan" id="kobo.69.1">Jupyter notebook</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.70.1">LLMs in </span><span><span class="kobospan" id="kobo.71.1">the cloud</span></span></li>
</ul>
<h1 id="_idParaDest-179" class="calibre4"><a id="_idTextAnchor442" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.72.1">Technical requirements</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.73.1">For this chapter, the following will </span><span><span class="kobospan" id="kobo.74.1">be necessary:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.75.1">Programming knowledge</span></strong><span class="kobospan" id="kobo.76.1">: Familiarity with Python programming is a must, since the open source models, OpenAI’s API, and LangChain are all illustrated using </span><span><span class="kobospan" id="kobo.77.1">Python code.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.78.1">Access to OpenAI’s API</span></strong><span class="kobospan" id="kobo.79.1">: An API key from OpenAI will be required to explore closed source models. </span><span class="kobospan" id="kobo.79.2">This can be obtained by creating an account with OpenAI and agreeing to their terms </span><span><span class="kobospan" id="kobo.80.1">of service.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.81.1">Open source models</span></strong><span class="kobospan" id="kobo.82.1">: Access to the specific open source models mentioned in this chapter will be necessary. </span><span class="kobospan" id="kobo.82.2">These can be accessed and downloaded from their respective repositories or via package managers such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.83.1">pip</span></strong> <span><span class="kobospan" id="kobo.84.1">or </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.85.1">conda</span></strong></span><span><span class="kobospan" id="kobo.86.1">.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.87.1">A local development environment</span></strong><span class="kobospan" id="kobo.88.1">: A local development environment setup with Python installed is required. </span><span class="kobospan" id="kobo.88.2">An </span><strong class="bold"><span class="kobospan" id="kobo.89.1">Integrated Development Environment</span></strong><span class="kobospan" id="kobo.90.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.91.1">IDE</span></strong><span class="kobospan" id="kobo.92.1">) such as PyCharm, Jupyter Notebook, or a simple text editor can be used. </span><span class="kobospan" id="kobo.92.2">Note that we recommend a free Google Colab notebook, as it encapsulates all these requirements in a seamless </span><span><span class="kobospan" id="kobo.93.1">web interface.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.94.1">The ability to install libraries</span></strong><span class="kobospan" id="kobo.95.1">: You must have permission to install the required Python libraries, such as NumPy, SciPy, TensorFlow, and PyTorch. </span><span class="kobospan" id="kobo.95.2">Note that the code we provide includes the required installations, and you won’t have to install them beforehand. </span><span class="kobospan" id="kobo.95.3">We simply stress that you should have permission to do so, which we expect you would. </span><span class="kobospan" id="kobo.95.4">Specifically, using a free Google Colab notebook </span><span><span class="kobospan" id="kobo.96.1">would suffice.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.97.1">Hardware requirements</span></strong><span class="kobospan" id="kobo.98.1">: Depending on the complexity and size of the models you’re working with, a computer with sufficient processing power (potentially including a good GPU for ML tasks) and ample memory will be required. </span><span class="kobospan" id="kobo.98.2">This is only relevant when choosing to not use the free </span><span><span class="kobospan" id="kobo.99.1">Google Colab.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.100.1">Now that we’ve grasped the transformative potential of LLMs and the variety of tools available, let’s delve deeper and explore how to effectively set up LLM applications </span><span><span class="kobospan" id="kobo.101.1">using APIs.</span></span></p>
<h1 id="_idParaDest-180" class="calibre4"><a id="_idTextAnchor443" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.102.1">Setting up an LLM application – API-based closed source models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.103.1">When looking to employ models in general and LLMs in particular, there are various design choices and trade-offs. </span><span class="kobospan" id="kobo.103.2">One key </span><a id="_idIndexMarker815" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.104.1">choice is whether to host a model locally in your local environment or to employ it remotely, accessing it via a communication channel. </span><span class="kobospan" id="kobo.104.2">Local development environments would be wherever your code runs, whether that’s your personal computer, your on-premises server, your cloud environment, and so on. </span><span class="kobospan" id="kobo.104.3">The choice you make will impact many aspects, such as cost, information security, maintenance needs, network overload, and </span><span><span class="kobospan" id="kobo.105.1">inference speed.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.106.1">In this section, we will introduce a quick and simple approach to employing an LLM remotely via an API. </span><span class="kobospan" id="kobo.106.2">This approach is quick and simple as it rids us of the need to allocate unusual computation resources to host the LLM locally. </span><span class="kobospan" id="kobo.106.3">An LLM typically requires amounts of memory and computation resources that aren’t common in </span><span><span class="kobospan" id="kobo.107.1">personal environments.</span></span></p>
<h2 id="_idParaDest-181" class="calibre7"><a id="_idTextAnchor444" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.108.1">Choosing a remote LLM provider</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.109.1">Before diving into implementation, we</span><a id="_idIndexMarker816" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.110.1"> need to select a suitable LLM provider that aligns with our project requirements. </span><span class="kobospan" id="kobo.110.2">OpenAI, for example, offers several versions of the GPT-3.5 and GPT-4 models with comprehensive </span><span><span class="kobospan" id="kobo.111.1">API documentation.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.112.1">OpenAI’s remote GPT access in Python via an API</span><a id="_idTextAnchor445" class="calibre5 pcalibre1 pcalibre"/></h3>
<p class="calibre6"><span class="kobospan" id="kobo.113.1">To gain access to OpenAI’s LLM API, we </span><a id="_idIndexMarker817" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.114.1">need to create an account on their website. </span><span class="kobospan" id="kobo.114.2">This process involves registration, account verification, and obtaining </span><span><span class="kobospan" id="kobo.115.1">API credentials.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.116.1">OpenAI’s website provides guidance for these common actions, and you will be able to get set </span><span><span class="kobospan" id="kobo.117.1">up quickl</span><a id="_idTextAnchor446" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.118.1">y.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.119.1">Once registered, we should familiarize ourselves with OpenAI’s API documentation. </span><span class="kobospan" id="kobo.119.2">This documentation will guide us through the various endpoints, methods, and parameters available to interact with </span><span><span class="kobospan" id="kobo.120.1">the LLMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.121.1">The first hands-on experience we will take on will b</span><a id="_idTextAnchor447" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.122.1">e employing OpenAI’s LLMs via Python. </span><span class="kobospan" id="kobo.122.2">We have put together a</span><a id="_idIndexMarker818" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.123.1"> notebook that presents the simple steps of employing OpenAI’s GPT model via an API. </span><span class="kobospan" id="kobo.123.2">Refer to the </span><strong class="source-inline"><span class="kobospan" id="kobo.124.1">Ch8_Setting_Up_Close_Source_and_Open_Source_LLMs.ipynb </span></strong><span class="kobospan" id="kobo.125.1">notebook. </span><span class="kobospan" id="kobo.125.2">This notebook, called </span><em class="italic"><span class="kobospan" id="kobo.126.1">Setting Up Close Source and Open Source LLMs</span></em><span class="kobospan" id="kobo.127.1">, will be utilized in the current section about OpenAI’s API, and also in the subsequent section about setting up </span><span><span class="kobospan" id="kobo.128.1">local LLMs.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.129.1">Let’s walk through </span><span><span class="kobospan" id="kobo.130.1">the code:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.131.1">We start by installing the required Python libraries. </span><span class="kobospan" id="kobo.131.2">In particular, to communicate with the LLM API, we need to install the necessary </span><span><span class="kobospan" id="kobo.132.1">Python library:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.133.1">
!pip install --upgrade op</span><a id="_idTextAnchor448" class="calibre314 pcalibre1 pcalibre"/><span class="kobospan1" id="kobo.134.1">enai</span></pre></li> <li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.135.1">Define OpenAI’s API key</span></strong><span class="kobospan" id="kobo.136.1">: Before making requests to the LLM API, we must embed our personal API key in the library’s configuration. </span><span class="kobospan" id="kobo.136.2">The API key is made available for you on OpenAI’s website when you register. </span><span class="kobospan" id="kobo.136.3">This can be done by either explicitly pasting the key’s string to be hardcoded in our code or reading it from a file that holds that string. </span><span class="kobospan" id="kobo.136.4">Note that the former is the simplest way to showcase the API, as it doesn’t require an additional file to be set up, but it may not be the right choice when working in a shared </span><span><span class="kobospan" id="kobo.137.1">development environment:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.138.1">
openai.api_key = "&lt;your key&gt;"</span></pre></li> <li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.139.1">Settings – set the model’s configurations</span></strong><span class="kobospan" id="kobo.140.1">. </span><span class="kobospan" id="kobo.140.2">Here, we set the various parameters that control the </span><span><span class="kobospan" id="kobo.141.1">model’s behavior.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.142.1">As the foundation is set for connecting to LLMs through APIs, it’s valuable to turn our attention to an equally important aspect – prompt engineering and priming, the art of effectively communicating with </span><span><span class="kobospan" id="kobo.143.1">these mode</span><a id="_idTextAnchor449" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.144.1">ls.</span></span></p>
<h1 id="_idParaDest-182" class="calibre4"><a id="_idTextAnchor450" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.145.1">Prompt engineering and priming GPT</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.146.1">Let us pause and provide some context before returning to discuss the next part of </span><span><span class="kobospan" id="kobo.147.1">the code.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.148.1">Prompt engineering is a technique</span><a id="_idIndexMarker819" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.149.1"> used in NLP to design effective prompts or instructions when interacting with LLMs. </span><span class="kobospan" id="kobo.149.2">It involves carefully crafting the input given to a model to elicit the desired output. </span><span class="kobospan" id="kobo.149.3">By providing specific cues, context, or constraints in the prompts, prompt engineering aims to guide the model’s behavior and encourage the generation of more accurate, relevant, or targeted responses. </span><span class="kobospan" id="kobo.149.4">The process often involves iterative refinement, experimentation, and understanding the model’s strengths and limitations to optimize the prompt for improved performance in various tasks, such as question-answering summarization or conversation generation. </span><span class="kobospan" id="kobo.149.5">Effective prompt engineering plays a vital role in harnessing the capabilities of LMs and shaping their output to meet specific </span><span><span class="kobospan" id="kobo.150.1">user requirements.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.151.1">Let’s review one of the most impactful tools in prompt engineering, </span><strong class="bold"><span class="kobospan" id="kobo.152.1">priming</span></strong><span class="kobospan" id="kobo.153.1">. </span><span class="kobospan" id="kobo.153.2">Priming GPT via an API involves providing initial context to the model before generating a response. </span><span class="kobospan" id="kobo.153.3">The priming step helps set the direction and style of the generated content. </span><span class="kobospan" id="kobo.153.4">By giving the model relevant information or examples related to the desired output, we can guide its understanding and encourage more focused and coherent responses. </span><span class="kobospan" id="kobo.153.5">Priming can be done by including specific instructions, context, or even partial sentences that align with the desired outcome. </span><span class="kobospan" id="kobo.153.6">Effective priming enhances the model’s ability to generate responses that better match the user’s intent or </span><span><span class="kobospan" id="kobo.154.1">specific requirements.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.155.1">Priming is done by introducing GPT </span><a id="_idIndexMarker820" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.156.1">with several types </span><span><span class="kobospan" id="kobo.157.1">of messages:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><span class="kobospan" id="kobo.158.1">The main message is the </span><strong class="bold"><span class="kobospan" id="kobo.159.1">system prompt</span></strong><span class="kobospan" id="kobo.160.1">. </span><span class="kobospan" id="kobo.160.2">This </span><a id="_idIndexMarker821" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.161.1">message instructs the model about the </span><em class="italic"><span class="kobospan" id="kobo.162.1">role</span></em><span class="kobospan" id="kobo.163.1"> it may play, the way it should answer the questions, the constraints it may have, and </span><span><span class="kobospan" id="kobo.164.1">so on.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.165.1">The second type of message</span><a id="_idIndexMarker822" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.166.1"> is a </span><strong class="bold"><span class="kobospan" id="kobo.167.1">user prompt</span></strong><span class="kobospan" id="kobo.168.1">. </span><span class="kobospan" id="kobo.168.2">A user prompt is sent to the model in the priming phase, and it represents an example user prompt, much like the prompt you may enter in ChatGPT’s web interface. </span><span class="kobospan" id="kobo.168.3">However, when priming, this message could be presented to the model as an example of how it should address such a prompt. </span><span class="kobospan" id="kobo.168.4">The developer will introduce a user prompt of some sort and will then show the model how it is expected to answer </span><span><span class="kobospan" id="kobo.169.1">that prompt.</span></span><p class="calibre6"><span class="kobospan" id="kobo.170.1">For example, observe</span><a id="_idIndexMarker823" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.171.1"> this </span><span><span class="kobospan" id="kobo.172.1">priming code:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.173.1">
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
           {"role": "system",
               "content": "You are a helpful assistant. </span><span class="kobospan1" id="kobo.173.2">You provide short answers and you format them in Markdown syntax"},
              {"role": "user",
               "content": "How do I import the Python library pandas?"},
              {"role": "assistant",
               "content": "This is how you would import pandas:  \n```\nimport pandas as pd\n```"},
              {"role": "user",
               "content": "How do I import the python library numpy?"}
        ])
text = response.choices[0].message.content.strip()
print(text)
)</span></pre><p class="calibre6"><span class="kobospan" id="kobo.174.1">This is </span><span><span class="kobospan" id="kobo.175.1">the</span></span><span><a id="_idIndexMarker824" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.176.1"> output:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.177.1">To import numpy, you can use the following syntax: 
```python  
import numpy as np  
```</span></pre></li> </ul>
<p class="calibre6"><span class="kobospan" id="kobo.178.1">You can see that we prime the model to provide concise answers in a Markdown format. </span><span class="kobospan" id="kobo.178.2">The example that is used to teach the model is in the form of a question and an answer. </span><span class="kobospan" id="kobo.178.3">The question is via a user prompt, and the way we tell the model what the potential answer is is provided via an assistant prompt. </span><span class="kobospan" id="kobo.178.4">We then provide the model with another user prompt; this one is the actual prompt we’d like the model to address for us, and as shown in the output, it gets </span><span><span class="kobospan" id="kobo.179.1">it right.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.180.1">By looking at OpenAI’s documentation about prompt engineering, you’ll find that there are additional types of prompts to </span><a id="_idIndexMarker825" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.181.1">prime the GPT </span><span><span class="kobospan" id="kobo.182.1">models with.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.183.1">Going back to our notebook </span><a id="_idIndexMarker826" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.184.1">and code, in this section, we leverage </span><em class="italic"><span class="kobospan" id="kobo.185.1">GPT-3.5 Turbo</span></em><span class="kobospan" id="kobo.186.1">. </span><span class="kobospan" id="kobo.186.2">We prime it in the simplest manner, only giving it a system prompt to provide directions in order to showcase how additional functionality could stem from the system prompt. </span><span class="kobospan" id="kobo.186.3">We tell the model to finish a response by alerting us about typos in the prompt and </span><span><span class="kobospan" id="kobo.187.1">correcting them.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.188.1">We then provide our desired prompt in the user prompt section, and we insert a few typ</span><a id="_idTextAnchor451" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.189.1">os into it. </span><span class="kobospan" id="kobo.189.2">Run that code and give it </span><span><span class="kobospan" id="kobo.190.1">a shot.</span></span></p>
<h2 id="_idParaDest-183" class="calibre7"><a id="_idTextAnchor452" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.191.1">Experimenting with OpenAI’s GPT model</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.192.1">At this stage, we send our prompts to </span><span><span class="kobospan" id="kobo.193.1">the</span></span><span><a id="_idIndexMarker827" class="calibre5 pcalibre1 pcalibre"/></span><span><span class="kobospan" id="kobo.194.1"> model.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.195.1">The following simple example code is run once in the </span><em class="italic"><span class="kobospan" id="kobo.196.1">Setting Up Close Source and Open Source LLMs</span></em><span class="kobospan" id="kobo.197.1"> notebook. </span><span class="kobospan" id="kobo.197.2">You can wrap it in a function and call it repeatedly in your </span><span><span class="kobospan" id="kobo.198.1">own code.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.199.1">Some aspects worth noticing are </span><span><span class="kobospan" id="kobo.200.1">as follows:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.201.1">Parsing and processing the returned output from the model</span></strong><span class="kobospan" id="kobo.202.1">: We structure the output response in a coherent manner for the user </span><span><span class="kobospan" id="kobo.203.1">to read:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.204.1">
print(f"Prompt: {user_prompt_oai}\n\n{openai_model}'s Response: \n{response_oai}")</span></pre></li> <li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.205.1">Error handling</span></strong><span class="kobospan" id="kobo.206.1">: We designed the code to allow for several failed attempts before accepting a failure to use </span><span><span class="kobospan" id="kobo.207.1">the API:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.208.1">
except Exception as output:
    attempts += 1
    if attempts &gt;= max_attempts:
        […]</span></pre></li> <li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.209.1">Rate limits and cost mitigation</span></strong><span class="kobospan" id="kobo.210.1">: We don’t implement such restrictions here, but it would be ideal to </span><a id="_idIndexMarker828" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.211.1">have both of these in an experimental setting and perhaps </span><span><span class="kobospan" id="kobo.212.1">in</span><a id="_idTextAnchor453" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.213.1"> production.</span></span><p class="calibre6"><span class="kobospan" id="kobo.214.1">The result of the preceding code is demonstrated </span><span><span class="kobospan" id="kobo.215.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.216.1">
Prompt: If neuroscience could extract the last thoughts a person had before they dyed, how would the world be different?
</span><span class="kobospan1" id="kobo.216.2">gpt-3.5-turbo's Response:
If neuroscience could extract the last thoughts a person had before they died, it would have profound implications for various aspects of society.
</span><span class="kobospan1" id="kobo.216.3">This ability could potentially revolutionize fields such as psychology, criminology, and end-of-life care.
</span><span class="kobospan1" id="kobo.216.4">Understanding a person's final thoughts could provide valuable insights into their state of mind, emotional well-being, and potentially help unravel mysteries surrounding their death.
</span><span class="kobospan1" id="kobo.216.5">It could also offer comfort to loved ones by providing a glimpse into the innermost thoughts of the deceased.
</span><span class="kobospan1" id="kobo.216.6">However, such technology would raise significant ethical concerns regarding privacy, consent, and the potential misuse of this information.
</span><span class="kobospan1" id="kobo.216.7">Overall, the world would be both fascinated and apprehensive about the implications of this groundbreaking capability.
</span><span class="kobospan1" id="kobo.216.8">Typos in the prompt:
1. </span><span class="kobospan1" id="kobo.216.9">"dyed" should be "died"
2. </span><span class="kobospan1" id="kobo.216.10">"diferent" should be "different"
Corrections:
If neuroscience could extract the last thoughts a person had before they died, how would the world be different?</span></pre></li> </ul>
<p class="calibre6"><span class="kobospan" id="kobo.217.1">The model provided us with a legitimate, concise response. </span><span class="kobospan" id="kobo.217.2">It then notified us about the typos, which are perfectly in line with the system prompt we provided </span><span><span class="kobospan" id="kobo.218.1">it with.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.219.1">That was an example</span><a id="_idIndexMarker829" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.220.1"> showcasing the employment of a remote, off-premises, closed source LLM. </span><span class="kobospan" id="kobo.220.2">While leveraging the power of paid APIs such as OpenAI offers convenience and cutting-edge performance, there’s also immense potential in tapping into free open source LLMs. </span><span class="kobospan" id="kobo.220.3">Let’s explore these cost-effective </span><span><span class="kobospan" id="kobo.221.1">alte</span><a id="_idTextAnchor454" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.222.1">rnatives next.</span></span></p>
<h1 id="_idParaDest-184" class="calibre4"><a id="_idTextAnchor455" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.223.1">Setting up an LLM application – local open source models</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.224.1">Now, we shall touch on the </span><a id="_idIndexMarker830" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.225.1">complementary approach to a closed source implementation, that is, an open source, </span><span><span class="kobospan" id="kobo.226.1">local implementation.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.227.1">We will see how you can achieve a similar functional outcome to the one we reviewed in the previous section, without having to register for an account, pay, or share prompts that contain possibly sensitive information with a third-party vendor, such </span><span><span class="kobospan" id="kobo.228.1">as OpenAI.</span></span></p>
<h2 id="_idParaDest-185" class="calibre7"><a id="_idTextAnchor456" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.229.1">About the different aspects that distinguish between open source and closed source</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.230.1">When selecting between </span><a id="_idIndexMarker831" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.231.1">open source LLMs, such as LLaMA and GPT-J, and closed source, API-based models such as OpenAI’s GPT, several critical factors must </span><span><span class="kobospan" id="kobo.232.1">be considered.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.233.1">Firstly, cost is a major factor. </span><span class="kobospan" id="kobo.233.2">Open source LLMs often have no licensing fees, but they require significant computational resources for training and inference, which can be expensive. </span><span class="kobospan" id="kobo.233.3">Closed source models, while potentially carrying a subscription or pay-per-use fee, eliminate the need for substantial </span><span><span class="kobospan" id="kobo.234.1">hardware investments.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.235.1">Processing speed and maintenance are closely linked to computational resources. </span><span class="kobospan" id="kobo.235.2">Open source LLMs, if deployed on powerful enough systems, can offer high processing speeds but require ongoing maintenance and updates by the implementing team. </span><span class="kobospan" id="kobo.235.3">In contrast, closed source models managed by the provider ensure continual maintenance and model updates, often with better efficiency and reduced downtime, but processing speed can be dependent on the provider’s infrastructure and </span><span><span class="kobospan" id="kobo.236.1">network latency.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.237.1">Regarding model updates, open source models offer more control but require a proactive approach to incorporate the latest research and improvements. </span><span class="kobospan" id="kobo.237.2">Closed source models, however, are regularly updated by the provider, ensuring access to the latest advancements without</span><a id="_idIndexMarker832" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.238.1"> additional effort from </span><span><span class="kobospan" id="kobo.239.1">the user.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.240.1">Security and privacy are paramount in both scenarios. </span><span class="kobospan" id="kobo.240.2">Open source models can be more secure, as they can be run on private servers, ensuring data privacy. </span><span class="kobospan" id="kobo.240.3">However, they demand robust in-house security protocols. </span><span class="kobospan" id="kobo.240.4">Closed source models, managed by external providers, often come with built-in security measures but pose potential privacy risks, due to data handling by </span><span><span class="kobospan" id="kobo.241.1">third parties.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.242.1">Overall, the choice between open source and closed source LLMs hinges on the trade-off between cost, control, and convenience, with each option presenting its own set of advantages </span><span><span class="kobospan" id="kobo.243.1">and challenges.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.244.1">With that in mind, let’s revisit Hugging Face, the company that put together the largest and most approachable hub for free LMs. </span><span class="kobospan" id="kobo.244.2">In the following example, we will leverage Hugging Face’s easy and free </span><span><span class="kobospan" id="kobo.245.1">libr</span><a id="_idTextAnchor457" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.246.1">ary: transformers.</span></span></p>
<h2 id="_idParaDest-186" class="calibre7"><a id="_idTextAnchor458" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.247.1">Hugging Face’s hub of models</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.248.1">When looking to choose an LLM for our task, we recommend referring to Hugging Face’s Models online page. </span><span class="kobospan" id="kobo.248.2">They offer an enormous </span><a id="_idIndexMarker833" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.249.1">amount of Python-based, open source LLMs. </span><span class="kobospan" id="kobo.249.2">Every model has a page dedicated to it, where you can find information about it, including the syntax needed to employ that model via Python code in your </span><span><span class="kobospan" id="kobo.250.1">personal environment.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.251.1">It should be noted that in order to implement a model locally, you must have an internet connection from the machine that runs the Python code. </span><span class="kobospan" id="kobo.251.2">However, as this requirement may become a bottleneck in some cases – for instance, when the development environment is restricted by a company’s intranet or has limited internet access due to firewall restrictions – there are alternative approaches. </span><span class="kobospan" id="kobo.251.3">Our recommended approach is to clone the model repository from Hugging Face’s domain. </span><span class="kobospan" id="kobo.251.4">That is a less trivial and less-used approach. </span><span class="kobospan" id="kobo.251.5">Hugging Face provides the necessary cloning commands on each</span><a id="_idTextAnchor459" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.252.1"> model’s </span><span><span class="kobospan" id="kobo.253.1">web page.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.254.1">Choosing a model</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.255.1">When looking to choose a model, there may be several factors that come into play. </span><span class="kobospan" id="kobo.255.2">Depending on your intentions, you may care about configuration speed, processing speed, storage space, computation resources, legal usage restrictions, and so on. </span><span class="kobospan" id="kobo.255.3">Another factor worth noting is the popularity of a model. </span><span class="kobospan" id="kobo.255.4">It attests to how frequently that model is chosen by other </span><a id="_idIndexMarker834" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.256.1">developers in the community. </span><span class="kobospan" id="kobo.256.2">For instance, if you look for LMs that are labeled for zero-shot classification, you will find a very large collection of available models. </span><span class="kobospan" id="kobo.256.3">But, if you then narrow the search some more so to only be left with models that were trained on data from news articles, you would be left with a much smaller set of available models. </span><span class="kobospan" id="kobo.256.4">In which case, you may want to refer to the popularity of each model and start your exploration with the model that was used </span><span><span class="kobospan" id="kobo.257.1">the most.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.258.1">Other factors that may interest you could be publications about the model, the model’s developers, the company or university that released the model, the dataset that the model was trained on, the architecture the model was designed by, the evaluation metrics, and other potential factors that may be available on each model’s web page on Hugg</span><a id="_idTextAnchor460" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.259.1">ing </span><span><span class="kobospan" id="kobo.260.1">Face’s website.</span></span></p>
<h1 id="_idParaDest-187" class="calibre4"><a id="_idTextAnchor461" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.261.1">Employing LLMs from Hugging Face via Python</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.262.1">Now, we will review a code notebook that exemplifies implementing an open source LLM locally using </span><a id="_idIndexMarker835" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.263.1">Hugging Face’s free resources. </span><span class="kobospan" id="kobo.263.2">We will continue with the same notebook from the previous section, </span><em class="italic"><span class="kobospan" id="kobo.264.1">Setting Up Close Source a</span><a id="_idTextAnchor462" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.265.1">nd Open </span></em><span><em class="italic"><span class="kobospan" id="kobo.266.1">Source </span></em></span><span><span class="kobospan" id="kobo.267.1">LLMs:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.268.1">Install the required Python libraries</span></strong><span class="kobospan" id="kobo.269.1">: To freely work with Hugging Face’s open source models and other various resources, we need to install the necessary </span><span><span class="kobospan" id="kobo.270.1">Python library.</span></span><p class="calibre6"><span class="kobospan" id="kobo.271.1">Via </span><strong class="source-inline"><span class="kobospan" id="kobo.272.1">pip</span></strong><span class="kobospan" id="kobo.273.1"> on the Terminal, we will run </span><span><span class="kobospan" id="kobo.274.1">the following:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.275.1">
pip install –upgrade transformers</span></pre><p class="calibre6"><span class="kobospan" id="kobo.276.1">Alternatively, if running directly from a Jupyter notebook, add </span><strong class="source-inline"><span class="kobospan" id="kobo.277.1">!</span></strong><span class="kobospan" id="kobo.278.1"> to the begin</span><a id="_idTextAnchor463" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.279.1">ning of </span><span><span class="kobospan" id="kobo.280.1">the command.</span></span></p></li> <li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.281.1">Experiment with Microsoft’s DialoGPT-medium</span></strong><span class="kobospan" id="kobo.282.1">: This LLM is dedicated to conversational applications. </span><span class="kobospan" id="kobo.282.2">It was generated by Microsoft and achieved high scores when compared to </span><a id="_idIndexMarker836" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.283.1">other LLMs on common benchmarks. </span><span class="kobospan" id="kobo.283.2">For that reason, it is also quite popular on Hugging Face’s platform, in the sense that it is downloaded frequently by </span><span><span class="kobospan" id="kobo.284.1">ML developers.</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.285.1">In the </span><strong class="bold"><span class="kobospan" id="kobo.286.1">Settings</span></strong><span class="kobospan" id="kobo.287.1"> code section in the notebook, we will define the parameters for this code and import the model and </span><span><span class="kobospan" id="kobo.288.1">its tokenizer:</span></span><pre class="source-code">
<strong class="source-inline2"><span class="kobospan1" id="kobo.289.1">hf_model = "microsoft/DialoGPT-medium"</span></strong>
<strong class="source-inline2"><span class="kobospan1" id="kobo.290.1">max_length = 1000</span></strong>
<strong class="source-inline2"><span class="kobospan1" id="kobo.291.1">tokenizer = AutoTokenizer.from_pretrained(hf_model)</span></strong>
<strong class="source-inline2"><span class="kobospan1" id="kobo.292.1">model = AutoModelForCausalLM.from_pretrained(hf_model)</span></strong></pre><p class="calibre6"><span class="kobospan" id="kobo.293.1">Note that this code requires access to the internet. </span><span class="kobospan" id="kobo.293.2">Even though the model is deployed locally, an internet connection is required to import it. </span><span class="kobospan" id="kobo.293.3">Again, if you wish, you can clone the model’s repo from Hugging Face and then no longer be required to have ac</span><a id="_idTextAnchor464" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.294.1">cess to </span><span><span class="kobospan" id="kobo.295.1">the internet.</span></span></p></li> <li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.296.1">Define the prompt</span></strong><span class="kobospan" id="kobo.297.1">: As can be seen in the following code block, we picked a straightforward prompt here, much like a user prompt for the</span><a id="_idTextAnchor465" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.298.1">GPT-3.5-Turbo model.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.299.1">Experiment with the model</span></strong><span class="kobospan" id="kobo.300.1">: Here, we have the syntax that suits this code. </span><span class="kobospan" id="kobo.300.2">If you want to create a rolling conversation with this model, you wrap this code in a function and iterate over it, collecting prompts from t</span><a id="_idTextAnchor466" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.301.1">he user in </span><span><span class="kobospan" id="kobo.302.1">real time.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.303.1">The result</span></strong><span class="kobospan" id="kobo.304.1">: The resulting prompt is, </span><strong class="source-inline1"><span class="kobospan" id="kobo.305.1">If dinosaurs were alive today, would they possess a threat </span></strong><span><strong class="source-inline1"><span class="kobospan" id="kobo.306.1">to people?:</span></strong></span><pre class="source-code"><span class="kobospan1" id="kobo.307.1">
microsoft/DialoGPT-medium's Response:
I think they would be more afraid of the humans</span></pre></li> </ol>
<p class="calibre6"><span class="kobospan" id="kobo.308.1">This section established the tremendous value proposition that LLMs can bring. </span><span class="kobospan" id="kobo.308.2">We now have the necessary background </span><a id="_idIndexMarker837" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.309.1">to learn and explore a new frontier in efficient LLM application development – constructing pipelines using tools such as LangChain. </span><span class="kobospan" id="kobo.309.2">Let’s dive into t</span><a id="_idTextAnchor467" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.310.1">his </span><span><span class="kobospan" id="kobo.311.1">advanced approach.</span></span></p>
<h1 id="_idParaDest-188" class="calibre4"><a id="_idTextAnchor468" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.312.1">Exploring advanced system design – RAG and LangChain</span></h1>
<p class="calibre6"><strong class="bold"><span class="kobospan" id="kobo.313.1">Retrieval-Augmented Generation</span></strong><span class="kobospan" id="kobo.314.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.315.1">RAG</span></strong><span class="kobospan" id="kobo.316.1">) is a development framework designed for seamless interaction with LLMs. </span><span class="kobospan" id="kobo.316.2">LLMs, by virtue of their </span><a id="_idIndexMarker838" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.317.1">generalist nature, are capable of performing a vast array of tasks competently. </span><span class="kobospan" id="kobo.317.2">However, their generality often precludes them from delivering detailed, nuanced responses to queries that necessitate specialized knowledge or in-depth expertise in a domain. </span><span class="kobospan" id="kobo.317.3">For instance, if you aspire to use an LLM to address queries concerning a specific discipline, such as law or medicine, it might satisfactorily answer general queries but fail to respond accurately to those needing detailed insights or </span><span><span class="kobospan" id="kobo.318.1">up-to-date knowledge.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.319.1">RAG designs offer a comprehensive solution to the limitations typically encountered in LLM processing. </span><span class="kobospan" id="kobo.319.2">In a RAG framework, the text corpus undergoes initial preprocessing, where it’s segmented into summaries or distinct chunks and then embedded within a vector space. </span><span class="kobospan" id="kobo.319.3">When a query is made, the model identifies the most relevant segments of this data and utilizes them to form a response. </span><span class="kobospan" id="kobo.319.4">This process involves a combination of offline data preprocessing, online information retrieval, and the application of the LLM for response generation. </span><span class="kobospan" id="kobo.319.5">It’s a versatile approach that can be adapted to a variety of tasks, including code generation and semantic search. </span><span class="kobospan" id="kobo.319.6">RAG models function as an abstraction layer that orchestrates these processes. </span><span class="kobospan" id="kobo.319.7">The efficacy of this method is continually increasing, with its applications expanding as LLMs evolve and require more contextually rich data during prompt processing. </span><span class="kobospan" id="kobo.319.8">In </span><a href="B18949_10.xhtml#_idTextAnchor525" class="calibre5 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.320.1">Chapter 10</span></em></span></a><span class="kobospan" id="kobo.321.1">, we will present a deeper discussion of RAG models and their role in the future of </span><span><span class="kobospan" id="kobo.322.1">LLM solutions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.323.1">Now that we’ve introduced the premise and capabilities of RAG models, let’s focus on one particular example, called LangChain. </span><span class="kobospan" id="kobo.323.2">We will review the nuts and bolts of its design principles and how it interfaces with </span><span><span class="kobospan" id="kobo.324.1">data sources.</span></span></p>
<h2 id="_idParaDest-189" class="calibre7"><a id="_idTextAnchor469" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.325.1">LangChain’s design concepts</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.326.1">In this section, we will dissect the core methodologies and architectural decisions that make LangChain stand out. </span><span class="kobospan" id="kobo.326.2">This will </span><a id="_idIndexMarker839" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.327.1">give us insights into its structural framework, the efficiency of data handling, and its innovative approach to integrating LLMs with various </span><span><span class="kobospan" id="kobo.328.1">data sources.</span></span></p>
<h2 id="_idParaDest-190" class="calibre7"><a id="_idTextAnchor470" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.329.1">Data sources</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.330.1">One of the most significant</span><a id="_idIndexMarker840" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.331.1"> virtues of LangChain is the ability to connect an arbitrary LLM to a defined data source. </span><span class="kobospan" id="kobo.331.2">By arbitrary, we mean that it could be any </span><em class="italic"><span class="kobospan" id="kobo.332.1">off-the-shelf</span></em><span class="kobospan" id="kobo.333.1"> LLM that was designed and trained with no specific regard to the data we are looking to connect it to. </span><span class="kobospan" id="kobo.333.2">Employing LangChain allows us to customize it to our domain. </span><span class="kobospan" id="kobo.333.3">The data source is to be used for reference when structuring the answer to the user prompt. </span><span class="kobospan" id="kobo.333.4">That data may be proprietary data owned by a company or local personal information on your </span><span><span class="kobospan" id="kobo.334.1">personal machine.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.335.1">However, when it comes to leveraging a given database, LangChain does more than point the LLM to the data; it employs a particular processing scheme and makes it quick and efficient. </span><span class="kobospan" id="kobo.335.2">It creates a </span><span><span class="kobospan" id="kobo.336.1">vector database.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.337.1">Given raw text data, be it free text in a </span><strong class="source-inline"><span class="kobospan" id="kobo.338.1">.txt</span></strong><span class="kobospan" id="kobo.339.1"> file, formatted files, or other various data structures of text, a vector database is created by chunking the text into appropriate lengths and creating numerical text embeddings, using a designated model. </span><span class="kobospan" id="kobo.339.2">Note that if the designated embedding model is chosen to be an LLM, it doesn’t have to be the same LLM that is used for prompting. </span><span class="kobospan" id="kobo.339.3">For instance, the embedding model could be picked to be a free, sub-optimal, open source LLM, and the prompting model could be a paid LLM with optimal performance. </span><span class="kobospan" id="kobo.339.4">Those embeddings are then stored in a vector database. </span><span class="kobospan" id="kobo.339.5">You can clearly see that this approach is extremely storage-efficient, as we transform text, and perhaps encoded text, into a finite set of numerical values, which by its nature </span><span><span class="kobospan" id="kobo.340.1">is dense.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.341.1">When a user enters a prompt, a search mechanism identifies the relevant data chunks in the embedded data source. </span><span class="kobospan" id="kobo.341.2">The prompt gets embedded with the same designated embedding model. </span><span class="kobospan" id="kobo.341.3">Then, the search mechanism applies a similarity metric, such as cosine similarity, for example, and finds the most similar text chunks in the defined data source. </span><span class="kobospan" id="kobo.341.4">Then, the original text of these chunks is retrieved. </span><span class="kobospan" id="kobo.341.5">The original prompt is then sent again, this time to the prompting LLM. </span><span class="kobospan" id="kobo.341.6">The difference is that, this time, the prompt consists of more than just the original user’s prompt; it also consists of the retrieved text as a reference. </span><span class="kobospan" id="kobo.341.7">This enables the LLM to get a question and a rich text supplement for reference. </span><span class="kobospan" id="kobo.341.8">The LLM then can refer to the added information as </span><span><span class="kobospan" id="kobo.342.1">a reference.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.343.1">If it weren’t for this design, when the user wanted to find an answer to their question, they would need to read through the vast material and find the relevant section. </span><span class="kobospan" id="kobo.343.2">For instance, the material may be a company’s entire product methodology, consisting of many PDF documents. </span><span class="kobospan" id="kobo.343.3">This</span><a id="_idIndexMarker841" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.344.1"> process leverages an automated smart search mechanism that narrows the relevant material down to an amount of text that can fit into a prompt. </span><span class="kobospan" id="kobo.344.2">Then, the LLM frames the answer to the question and presents it to the user immediately. </span><span class="kobospan" id="kobo.344.3">If you wish, the pipeline can be designed to quote the original text that it used to frame the answer, thus allowing for transparency </span><span><span class="kobospan" id="kobo.345.1">and verification.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.346.1">This paradigm is portrayed in </span><span><em class="italic"><span class="kobospan" id="kobo.347.1">Figure 8</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.348.1">.1</span></em></span><span><span class="kobospan" id="kobo.349.1">:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.350.1">.</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer362">
<span class="kobospan" id="kobo.351.1"><img alt="Figure 8.1 – The paradigm of a typical LangChain pipeline" src="image/B18949_08_1.jpg" class="calibre3"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.352.1">Figure 8.1 – The paradigm of a typical LangChain pipeline</span></p>
<p class="calibre6"><span class="kobospan" id="kobo.353.1">In order to explain the prompt engineering behind the LangChain pipeline, let’s review a financial information use case. </span><span class="kobospan" id="kobo.353.2">Your data source is a cohort of </span><strong class="bold"><span class="kobospan" id="kobo.354.1">Securities</span></strong> <strong class="bold"><span class="kobospan" id="kobo.355.1">&amp;</span></strong> <strong class="bold"><span class="kobospan" id="kobo.356.1">Exchange</span></strong> <strong class="bold"><span class="kobospan" id="kobo.357.1">Commission</span></strong><span class="kobospan" id="kobo.358.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.359.1">SEC</span></strong><span class="kobospan" id="kobo.360.1">) filings of public companies from the US. </span><span class="kobospan" id="kobo.360.2">You are looking</span><a id="_idIndexMarker842" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.361.1"> to identify companies that gave dividends to their stock holders, and in </span><span><span class="kobospan" id="kobo.362.1">what year.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.363.1">Your prompt would be </span><span><span class="kobospan" id="kobo.364.1">as follows:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.365.1">
Which filings mention that the company gave dividends in the year 2023?</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.366.1">The pipeline then embeds this question and looks for text chunks with similar context (e.g., that discuss paid dividends). </span><span class="kobospan" id="kobo.366.2">It identifies many such chunks, such as </span><span><span class="kobospan" id="kobo.367.1">the following:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.368.1">
"Dividend Policy. </span><span class="kobospan1" id="kobo.368.2">Dividends are paid at the discretion of the Board of Directors. </span><span class="kobospan1" id="kobo.368.3">In fiscal 2023, we paid aggregate quarterly cash dividends of $8.79 per share […]"</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.369.1">The LangChain pipeline then forms a new prompt that includes the text of the identified chunks. </span><span class="kobospan" id="kobo.369.2">In this example, we assume the prompted LLM is OpenAI’s GPT. </span><span class="kobospan" id="kobo.369.3">LangChain embeds the information in the system prompt sent to OpenAI’s </span><span><span class="kobospan" id="kobo.370.1">GPT model:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.371.1">
"prompts": [
    "System: Use the following pieces of context to answer the user's question. </span><span class="kobospan1" id="kobo.371.2">\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n Dividend Policy. </span><span class="kobospan1" id="kobo.371.3">Dividends are paid at the […]"
]</span></pre> <p class="calibre6"><span class="kobospan" id="kobo.372.1">As we can see, the system prompt is used to instruct the model how to act and then to provide </span><span><span class="kobospan" id="kobo.373.1">the context.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.374.1">Now that we have an understanding of the foundational approach and benefits of LangChain, let’s go deeper into its intricate design concepts, starting with how it bridges LLMs to diverse data </span><span><span class="kobospan" id="kobo.375.1">sources efficiently.</span></span></p>
<h2 id="_idParaDest-191" class="calibre7"><a id="_idTextAnchor471" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.376.1">Data that is not pre-embedded</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.377.1">While the preceding description is of data</span><a id="_idIndexMarker843" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.378.1"> that is preprocessed to take the form of a vector database, another approach is to set up access to external data sources that are not yet processed into an embedding form. </span><span class="kobospan" id="kobo.378.2">For instance, you may wish to leverage a SQL database to supplement other data sources. </span><span class="kobospan" id="kobo.378.3">This approach</span><a id="_idIndexMarker844" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.379.1"> is referred to as </span><strong class="bold"><span class="kobospan" id="kobo.380.1">multiple </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.381.1">retrieval sources</span></strong></span><span><span class="kobospan" id="kobo.382.1">.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.383.1">We’ve now explored the</span><a id="_idIndexMarker845" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.384.1"> ways LangChain efficiently interfaces with various data sources; now, it is essential to grasp the core structural elements that enable its functionalities – chains </span><span><span class="kobospan" id="kobo.385.1">and agents.</span></span></p>
<h2 id="_idParaDest-192" class="calibre7"><a id="_idTextAnchor472" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.386.1">Chains</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.387.1">The atomic building blocks within</span><a id="_idIndexMarker846" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.388.1"> LangChain are called components. </span><span class="kobospan" id="kobo.388.2">Typical components could be a prompt template, access to various data sources, and access to LLMs. </span><span class="kobospan" id="kobo.388.3">When combining various components to form a system, we form a chain. </span><span class="kobospan" id="kobo.388.4">A chain can represent a complete </span><span><span class="kobospan" id="kobo.389.1">LLM-driven application.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.390.1">We will now present the concept of agents and walk through a code example that showcases how chains and agents come together, creating a capability that would have been quite complex not too </span><span><span class="kobospan" id="kobo.391.1">long ago.</span></span></p>
<h2 id="_idParaDest-193" class="calibre7"><a id="_idTextAnchor473" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.392.1">Agents</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.393.1">The next layer of complexity over chains is agents. </span><span class="kobospan" id="kobo.393.2">Agents leverage chains by employing them and complementing them</span><a id="_idIndexMarker847" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.394.1"> with additional calculations and decisions. </span><span class="kobospan" id="kobo.394.2">While a chain may yield a response to a simple request prompt, an agent would process the response and act upon it with further downstream processing based on a </span><span><span class="kobospan" id="kobo.395.1">prescribed logic.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.396.1">You can view agents as a reasoning mechanism that employs what we call a tool. </span><span class="kobospan" id="kobo.396.2">Tools complement LLMs by connecting them with other data </span><span><span class="kobospan" id="kobo.397.1">or functions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.398.1">Given the typical LLM shortcomings that prevent LLMs from being perfect multitaskers, agents employ tools in a prescribed and monitored manner, allowing them to retrieve necessary information, leverage it as context, and execute actions using designated existing solutions. </span><span class="kobospan" id="kobo.398.2">Agents then observe the results and employ the prescribed logic for further </span><span><span class="kobospan" id="kobo.399.1">downstream processes.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.400.1">As an example, assume we want to calculate the salary trajectory for an average entry-level programmer in our area. </span><span class="kobospan" id="kobo.400.2">This task is comprised of three key sub-tasks – finding out what that average starting salary is, identifying the factors for salary growth (e.g., a change in the cost of living, or a typical merit increase), and then projecting onward. </span><span class="kobospan" id="kobo.400.3">An ideal LLM would be able to do the entire process by itself, not requiring anything more than a coherent prompt. </span><span class="kobospan" id="kobo.400.4">However, given the typical shortcomings, such as hallucinations and limited training data, current LLMs would not be able to perform this entire process to a level where it could be productionized within a commercial product. </span><span class="kobospan" id="kobo.400.5">A best practice is to break it down and monitor the thought process </span><span><span class="kobospan" id="kobo.401.1">via agents.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.402.1">In its most simple design, this </span><a id="_idIndexMarker848" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.403.1">would require </span><span><span class="kobospan" id="kobo.404.1">the following:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.405.1">Defining an agent that can access the internet and that can calculate future values of time series, given </span><span><span class="kobospan" id="kobo.406.1">growth factors</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.407.1">Providing the agent with a </span><span><span class="kobospan" id="kobo.408.1">comprehensive prompt</span></span></li>
<li class="calibre15"><span class="kobospan" id="kobo.409.1">The agent breaks the prompt down into the </span><span><span class="kobospan" id="kobo.410.1">different sub-tasks:</span></span><ol class="calibre285"><li class="upper-roman"><span class="kobospan" id="kobo.411.1">Fetching the average salary from </span><span><span class="kobospan" id="kobo.412.1">the internet</span></span></li><li class="upper-roman"><span class="kobospan" id="kobo.413.1">Fetching the </span><span><span class="kobospan" id="kobo.414.1">growth factors</span></span></li><li class="upper-roman"><span class="kobospan" id="kobo.415.1">Employing the calculation tool by applying the growth factors to the starting salary and creating a future time series for </span><span><span class="kobospan" id="kobo.416.1">salary values</span></span></li></ol></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.417.1">To exemplify the agentic approach, let's review a simple task that involves fetching a particular detail from the web, and using it to perform </span><span><span class="kobospan" id="kobo.418.1">a calculation.</span></span></p>
<ol class="calibre16">
<li class="calibre15"><span class="kobospan" id="kobo.419.1">First, install </span><span><span class="kobospan" id="kobo.420.1">these packages:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.421.1">
!pip install openai
!pip install wikipedia
!pip install langchain
!pip install langchain-openai</span></pre></li> <li class="calibre15"><span class="kobospan" id="kobo.422.1">Then, run the </span><span><span class="kobospan" id="kobo.423.1">following code:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.424.1">
from langchain.agents import load_tools, initialize_agent
from langchain_openai import OpenAI
import os
os.environ["OPENAI_API_KEY"] = "&lt;your API key&gt;"
llm = OpenAI(model_name='gpt-3.5-turbo-instruct')
tools = load_tools(["wikipedia", "llm-math"], llm=llm)
agent = initialize_agent(tools, llm=llm, agent="zero-shot-react-description", verbose=True)
agent.run("Figure out how many pages are there in the book Animal Farm. </span><span class="kobospan1" id="kobo.424.2">Then calculate how many minutes would it take me to read it if it takes me two minutes to read one page.")</span></pre><p class="calibre6"><span class="kobospan" id="kobo.425.1">The output is then shown </span><span><span class="kobospan" id="kobo.426.1">as follows:</span></span></p><pre class="source-code"><span class="kobospan1" id="kobo.427.1">&gt; Finished chain.
</span><span class="kobospan1" id="kobo.427.2">'It would take me approximately 224 minutes or 3 hours and 44 minutes to read Animal Farm.'</span></pre></li> </ol>
<p class="calibre6"><span class="kobospan" id="kobo.428.1">Note that we didn’t apply any </span><a id="_idIndexMarker849" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.429.1">method to fix the LLM to reproduce this exact response. </span><span class="kobospan" id="kobo.429.2">Running this code again will yield a slightly </span><span><span class="kobospan" id="kobo.430.1">different answer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.431.1">In the next chapter, we will dive deeper into several examples with code. </span><span class="kobospan" id="kobo.431.2">In particular, we will program a multi-agent framework, where a team of agents is working on a </span><span><span class="kobospan" id="kobo.432.1">joint project.</span></span></p>
<h2 id="_idParaDest-194" class="calibre7"><a id="_idTextAnchor474" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.433.1">Long-term memory and referring to prior conversations</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.434.1">Another very important concept is</span><a id="_idIndexMarker850" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.435.1"> long-term memory. </span><span class="kobospan" id="kobo.435.2">We discussed how LangChain complements an LLM’s knowledge by appending additional data sources, some of which may</span><a id="_idIndexMarker851" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.436.1"> be proprietary, making it highly customized for a particular use case. </span><span class="kobospan" id="kobo.436.2">However, it still lacks a very important function, the ability to refer to prior conversations and learn from them. </span><span class="kobospan" id="kobo.436.3">For instance, you can design an assistant for a project manager. </span><span class="kobospan" id="kobo.436.4">As the user interacts with it, they would ideally update each day about the progress of the work, the interactions, the challenges, and so on. </span><span class="kobospan" id="kobo.436.5">It would be best if the assistant could digest all that newly accumulated knowledge and sustain it. </span><span class="kobospan" id="kobo.436.6">That would allow for a scenario such </span><span><span class="kobospan" id="kobo.437.1">as this:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.438.1">User</span></strong><span class="kobospan" id="kobo.439.1">: “Where do we stand with regard to Jim’s </span><span><span class="kobospan" id="kobo.440.1">team’s task?”</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.441.1">Assistant</span></strong><span class="kobospan" id="kobo.442.1">: “According to the original roadmap, Jim’s team is to address the client’s feedback to the design of the prototype. </span><span class="kobospan" id="kobo.442.2">Based on the update from last week, the client provided only partial feedback, which you felt would not yet be suffi</span><a id="_idTextAnchor475" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.443.1">cient for Jim’s team to</span><a id="_idIndexMarker852" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.444.1">start work.”</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.445.1">We will touch more on the</span><a id="_idIndexMarker853" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.446.1"> concept of memory in the </span><span><span class="kobospan" id="kobo.447.1">next chapter.</span></span></p>
<h2 id="_idParaDest-195" class="calibre7"><a id="_idTextAnchor476" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.448.1">Ensuring continuous relevance through incremental updates and automated monitoring</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.449.1">To maintain the accuracy and relevance </span><a id="_idIndexMarker854" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.450.1">of LLM outputs in dynamic information environments, it’s imperative to implement strategies for the ongoing update and maintenance of vector databases. </span><span class="kobospan" id="kobo.450.2">As the corpus of knowledge continues to expand and evolve, so too must the embeddings that serve as the foundation for LLM responses. </span><span class="kobospan" id="kobo.450.3">Incorporating techniques for incremental updates allows these databases to refresh their embeddings as new information becomes available, ensuring that the LLMs can provide the most accurate and </span><span><span class="kobospan" id="kobo.451.1">up-to-date responses.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.452.1">Incremental updates involve periodically re-embedding existing data sources with the latest information. </span><span class="kobospan" id="kobo.452.2">This process can be automated to scan for updates in the data source, re-embed the new or updated content, and then integrate these refreshed embeddings into the existing vector database, without the need for a complete overhaul. </span><span class="kobospan" id="kobo.452.3">By doing so, we ensure that the database reflects the most current knowledge available, enhancing the LLM’s ability to deliver relevant and </span><span><span class="kobospan" id="kobo.453.1">nuanced responses.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.454.1">Automated monitoring plays a pivotal role in this ecosystem by continually assessing the quality and relevance of the LLM’s outputs. </span><span class="kobospan" id="kobo.454.2">This involves setting up systems that track the performance of the LLM, identifying areas where responses may be falling short due to outdated information or missing contexts. </span><span class="kobospan" id="kobo.454.3">When such gaps are identified, the monitoring system can trigger an incremental update process, ensuring that the database remains a robust and accurate reflection of the current </span><span><span class="kobospan" id="kobo.455.1">knowledge landscape.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.456.1">By embracing these strategies, we ensure that LangChain and similar RAG frameworks can sustain their effectiveness over time. </span><span class="kobospan" id="kobo.456.2">This approach not only enhances the relevance of LLM applications but also ensures that they can adapt to the rapidly evolving landscape of information, maintaining their position at the forefront of </span><span><span class="kobospan" id="kobo.457.1">NLP technology.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.458.1">W</span><a id="_idTextAnchor477" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.459.1">e can now get hands-on </span><span><span class="kobospan" id="kobo.460.1">with LangChain.</span></span></p>
<h1 id="_idParaDest-196" class="calibre4"><a id="_idTextAnchor478" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.461.1">Reviewing a simple LangChain setup in a Jupyter notebook</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.462.1">We are now ready to set up a </span><a id="_idIndexMarker855" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.463.1">complete pipeline that can later be lent to various </span><span><span class="kobospan" id="kobo.464.1">NLP applications.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.465.1">Refer to the </span><strong class="source-inline"><span class="kobospan" id="kobo.466.1">Ch8_Setting_Up_LangChain_Configurations_and_Pipeline.ipynb</span></strong><span class="kobospan" id="kobo.467.1"> notebook. </span><span class="kobospan" id="kobo.467.2">This notebook implements the LangChain framework. </span><span class="kobospan" id="kobo.467.3">We will walk through it step by step, e</span><a id="_idTextAnchor479" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.468.1">xplaining the different </span><a id="_idIndexMarker856" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.469.1">building blocks. </span><span class="kobospan" id="kobo.469.2">We chose a simple use case here, as the main point of this code is to show how to set up a </span><span><span class="kobospan" id="kobo.470.1">LangChain pipeline.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.471.1">In this scenario, we are in the healthcare sector. </span><span class="kobospan" id="kobo.471.2">We have many care givers; each has many patients they may see. </span><span class="kobospan" id="kobo.471.3">The physician in chief made a request on behalf of all the physicians in the hospital to be able to use a smart search across their notes. </span><span class="kobospan" id="kobo.471.4">They heard about the new emerging capabilities with LLMs, and they would like to have a tool where they can search within the medical reports </span><span><span class="kobospan" id="kobo.472.1">they wrote.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.473.1">For instance, one physician said </span><span><span class="kobospan" id="kobo.474.1">the following:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.475.1"> “</span><em class="italic"><span class="kobospan" id="kobo.476.1">I often come across research that may be relevant to a patient I saw months ago, but I don’t recall who that was. </span><span class="kobospan" id="kobo.476.2">I would like to have a tool where I can ask, ‘Who was that patient that complained about ear pain and had a family history of migraines?’, and it would find me </span></em><span><em class="italic"><span class="kobospan" id="kobo.477.1">that patient.</span></em></span><span><span class="kobospan" id="kobo.478.1">”</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.479.1">Thus, the</span><a id="_idTextAnchor480" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.480.1"> business objective here is </span><span><span class="kobospan" id="kobo.481.1">as follows:</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.482.1">“</span><em class="italic"><span class="kobospan" id="kobo.483.1">The CTO tasked us with putting together a quick prototype in the form of a Jupyter notebook. </span><span class="kobospan" id="kobo.483.2">We will collect several clinical reports from the hospital’s database, and we will use LangChain to search through them in the manner that the physician in the </span></em><span><em class="italic"><span class="kobospan" id="kobo.484.1">example described.”</span></em></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.485.1">Let’s jump right</span><a id="_idTextAnchor481" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.486.1"> in by designing the solution </span><span><span class="kobospan" id="kobo.487.1">in Python.</span></span></p>
<h2 id="_idParaDest-197" class="calibre7"><a id="_idTextAnchor482" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.488.1">Setting up a LangChain pipeline with Python</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.489.1">Diving into the practicalities of LangChain, this </span><a id="_idIndexMarker857" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.490.1">section will guide you step by step in setting up a LangChain pipeline using Python, from installing the necessary libraries to exe</span><a id="_idTextAnchor483" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.491.1">cuting sophisticated </span><span><span class="kobospan" id="kobo.492.1">similarity searches.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.493.1">Installing the required Python libraries</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.494.1">As always, we have a list of libraries</span><a id="_idIndexMarker858" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.495.1"> that we will need to install. </span><span class="kobospan" id="kobo.495.2">Since we are writing the code in a Jupyter notebook,</span><a id="_idTextAnchor484" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.496.1"> we can install them from within </span><span><span class="kobospan" id="kobo.497.1">the code:</span></span></p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.498.1">Load the text files with mock physician notes</span></strong><span class="kobospan" id="kobo.499.1">: Here, we put together some mock physician notes. </span><span class="kobospan" id="kobo.499.2">We load them and process them per the LangChain paradigm. </span><span class="kobospan" id="kobo.499.3">We stress that these aren’t real medical notes and that the people described there </span><span><span class="kobospan" id="kobo.500.1">don’t exist.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.501.1">Process the data so that it can be prepared for embedding</span></strong><span class="kobospan" id="kobo.502.1">: Here, we split the text per the requirements of the embedding model. </span><span class="kobospan" id="kobo.502.2">As we mentioned in previous chapters, LMs, such as those used for embedding, have a finite window of input text that they can process in a single batch. </span><span class="kobospan" id="kobo.502.3">That size is hardcoded in their design architectu</span><a id="_idTextAnchor485" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.503.1">re and is fixed for each </span><span><span class="kobospan" id="kobo.504.1">particular model.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.505.1">Create the embeddings that would be stored in the vector database</span></strong><span class="kobospan" id="kobo.506.1">: The vector database is one of the key pillars of the LangChain paradigm. </span><span class="kobospan" id="kobo.506.2">Here, we take the text and create an embedding for each item. </span><span class="kobospan" id="kobo.506.3">Those embeddings are then stored in a dedicated vector database. </span><span class="kobospan" id="kobo.506.4">The LangChain library allows you to work with several different vector databases. </span><span class="kobospan" id="kobo.506.5">While we chose one particular database, you can refer to the </span><strong class="bold"><span class="kobospan" id="kobo.507.1">Vector Store</span></strong><span class="kobospan" id="kobo.508.1"> page to read more about the </span><span><span class="kobospan" id="kobo.509.1">different choices.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.510.1">Create the vector database</span></strong><span class="kobospan" id="kobo.511.1">: Here, we create the vector database. </span><span class="kobospan" id="kobo.511.2">This process may be slightly different for each database choice. </span><span class="kobospan" id="kobo.511.3">However, the creators of these databases make sure to take away all of the hard work and leave you with a simple turnkey function that creates the database for you, given the appropriate embeddings in vector form. </span><span class="kobospan" id="kobo.511.4">We leverage Meta’s </span><strong class="bold"><span class="kobospan" id="kobo.512.1">Facebook AI Similarity Search</span></strong><span class="kobospan" id="kobo.513.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.514.1">FAISS</span></strong><span class="kobospan" id="kobo.515.1">) database, as it</span><a id="_idIndexMarker859" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.516.1"> is simple, quick to deploy, </span><span><span class="kobospan" id="kobo.517.1">and free.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.518.1">Perform a similarity search based on our in-house documents</span></strong><span class="kobospan" id="kobo.519.1">: This is the key part of the pipeline. </span><span class="kobospan" id="kobo.519.2">We introduce several questions and use LangChain’s similarity search to identify the physician notes that would best answer </span><span><span class="kobospan" id="kobo.520.1">our question.</span></span></li>
</ol>
<p class="calibre6"><span class="kobospan" id="kobo.521.1">As we can see, the similarity search function is able to do a good job with most of the questions. </span><span class="kobospan" id="kobo.521.2">It embeds the question and looks for reports whose embeddings </span><span><span class="kobospan" id="kobo.522.1">are similar.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.523.1">However, a similarity search could only go so far when it comes to answering the question correctly. </span><span class="kobospan" id="kobo.523.2">It is easy</span><a id="_idIndexMarker860" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.524.1"> to think of a question that discusses a matter that is very similar to one of the notes, yet a minor difference confuses the similarity search mechanism. </span><span class="kobospan" id="kobo.524.2">For instance, the similarity search process actually makes a mistake in question two, mistaking different months and, thus, providing a </span><span><span class="kobospan" id="kobo.525.1">wrong answer.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.526.1">In order to overcome this matter, we would want to do more than just a similarity search. </span><span class="kobospan" id="kobo.526.2">We would want an LLM to review the results of the similarity search and apply its judgment. </span><span class="kobospan" id="kobo.526.3">We w</span><a id="_idTextAnchor486" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.527.1">ill see how that’s done in the </span><span><span class="kobospan" id="kobo.528.1">next chapter.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.529.1">With our foundation set for LangChain’s practical applications in Python, let’s now move on to understanding how the cloud plays a pivotal role, especially when harnessing the true potential of L</span><a id="_idTextAnchor487" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.530.1">LMs in contemporary </span><span><span class="kobospan" id="kobo.531.1">computational paradigms.</span></span></p>
<h1 id="_idParaDest-198" class="calibre4"><a id="_idTextAnchor488" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.532.1">LLMs in the cloud</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.533.1">In this era of big data and computation, cloud platforms have emerged as vital tools for managing large-scale computations, providing infrastructure, storage, and services that can be rapidly provisioned and released with minimal </span><span><span class="kobospan" id="kobo.534.1">management effort.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.535.1">This section will focus on computation environments in the cloud. </span><span class="kobospan" id="kobo.535.2">These have become the dominant choice for many leading companies and institutions. </span><span class="kobospan" id="kobo.535.3">As an organization, having a computation </span><a id="_idIndexMarker861" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.536.1">environment in the cloud versus on-premises makes a major difference. </span><span class="kobospan" id="kobo.536.2">It impacts the ability to share resources and manage allocations, maintenance, and cost. </span><span class="kobospan" id="kobo.536.3">There are many trade-offs for employing cloud services instead of owning physical machines. </span><span class="kobospan" id="kobo.536.4">You can learn about them by searching online or even asking a chat LLM </span><span><span class="kobospan" id="kobo.537.1">about them.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.538.1">One significant difference with cloud computing is the ecosystem that the providers have built around it. </span><span class="kobospan" id="kobo.538.2">When you pick a cloud provider as your computation hub, you tap into a whole suite of additional products and services, opening up a new world of capabilities that would not be as accessible to </span><span><span class="kobospan" id="kobo.539.1">you otherwise.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.540.1">In this section, we w</span><a id="_idTextAnchor489" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.541.1">ill focus on the LLM aspect of </span><span><span class="kobospan" id="kobo.542.1">those services.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.543.1">The three primary cloud platforms are AWS, Microsoft Azure, and GCP. </span><span class="kobospan" id="kobo.543.2">These platforms offer a myriad of services, catering to the varying needs of businesses and developers. </span><span class="kobospan" id="kobo.543.3">When it comes to NLP and LLMs, each platform provides dedicated resources and services to facilitate </span><a id="_idIndexMarker862" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.544.1">experimentation, deployment, </span><span><span class="kobospan" id="kobo.545.1">and production.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.546.1">Let’s explore each of these platfor</span><a id="_idTextAnchor490" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor491" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor492" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor493" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor494" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor495" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor496" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor497" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor498" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor499" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.547.1">ms to see how they cater to our </span><span><span class="kobospan" id="kobo.548.1">specific needs.</span></span></p>
<h2 id="_idParaDest-199" class="calibre7"><a id="_idTextAnchor500" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.549.1">AWS</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.550.1">AWS remains a dominant force in the </span><a id="_idIndexMarker863" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.551.1">cloud computing landscape, providing a comprehensive and evolving suite</span><a id="_idIndexMarker864" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.552.1"> of services that cater to the needs of ML and AI development. </span><span class="kobospan" id="kobo.552.2">AWS is renowned for its robust infrastructure, extensive service offerings, and deep integration with ML tools and frameworks, making it a preferred platform for developers and data scientists looking to innovate </span><span><span class="kobospan" id="kobo.553.1">with LLMs.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.554.1">Experimenting with LLMs on AWS</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.555.1">AWS provides a rich ecosystem of </span><a id="_idIndexMarker865" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.556.1">tools and services designed to facilitate the </span><a id="_idIndexMarker866" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.557.1">development and experimentation with LLMs, ensuring that researchers and developers have access to the most advanced </span><span><span class="kobospan" id="kobo.558.1">ML capabilities:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.559.1">Amazon SageMaker</span></strong><span class="kobospan" id="kobo.560.1">: The cornerstone of ML on AWS, SageMaker is a fully managed service that streamlines the entire ML</span><a id="_idIndexMarker867" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.561.1"> workflow. </span><span class="kobospan" id="kobo.561.2">It offers Jupyter notebook instances for experimentation, broad framework support, including TensorFlow and PyTorch, and a range of tools for model building, training, and debugging. </span><span class="kobospan" id="kobo.561.3">SageMaker’s capabilities have been continually enhanced to support the complexities of training and fine-tuning LLMs, providing scalable compute options and optimized </span><span><span class="kobospan" id="kobo.562.1">ML environments.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.563.1">AWS Deep Learning Containers and Deep Learning AMIs</span></strong><span class="kobospan" id="kobo.564.1">: For those looking to customize their ML environments, AWS offers Deep Learning Containers and </span><strong class="bold"><span class="kobospan" id="kobo.565.1">Amazon Machine Images</span></strong><span class="kobospan" id="kobo.566.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.567.1">AMIs</span></strong><span class="kobospan" id="kobo.568.1">) pre-installed with popular ML frameworks. </span><span class="kobospan" id="kobo.568.2">These resources </span><a id="_idIndexMarker868" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.569.1">simplify the setup process for LLM experiments, allowing developers to focus on innovation rather than </span><span><span class="kobospan" id="kobo.570.1">infrastructure configuration.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.571.1">Pre-trained models and SageMaker JumpStart</span></strong><span class="kobospan" id="kobo.572.1">: AWS has expanded its library of pre-trained models accessible through SageMaker JumpStart, facilitating quick </span><a id="_idIndexMarker869" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.573.1">experimentation with LLMs for a variety of </span><a id="_idIndexMarker870" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.574.1">NLP tasks. </span><span class="kobospan" id="kobo.574.2">JumpStart also offers solution templates and executable example notebooks, making it easier for developers to start and scale their </span><span><span class="kobospan" id="kobo.575.1">ML projects.</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.576.1">Deploying and productionizing LLMs on AWS</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.577.1">AWS provides a suite of services designed to</span><a id="_idIndexMarker871" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.578.1"> efficiently deploy and manage LLMs at scale, ensuring that models are easily </span><a id="_idIndexMarker872" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.579.1">accessible and performant under </span><span><span class="kobospan" id="kobo.580.1">varying loads:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.581.1">SageMaker endpoints</span></strong><span class="kobospan" id="kobo.582.1">: To deploy LLMs, SageMaker endpoints offer fully managed hosting services with</span><a id="_idIndexMarker873" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.583.1"> auto-scaling capabilities. </span><span class="kobospan" id="kobo.583.2">This service allows developers to deploy trained models into production quickly, with the infrastructure automatically adjusting to the demands of </span><span><span class="kobospan" id="kobo.584.1">the application.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.585.1">Elastic Inference and Amazon EC2 Inf1 instances</span></strong><span class="kobospan" id="kobo.586.1">: To optimize inference costs, AWS offers Elastic Inference, which adds GPU-powered inference acceleration to SageMaker instances. </span><span class="kobospan" id="kobo.586.2">For even greater performance and cost efficiency, Amazon EC2 Inf1 instances, powered by AWS Inferentia chips, provide high-throughput and low-latency inference for </span><span><span class="kobospan" id="kobo.587.1">DL models.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.588.1">AWS Lambda and Amazon Bedrock</span></strong><span class="kobospan" id="kobo.589.1">: For serverless deployment, AWS Lambda supports running inference without provisioning or managing servers, ideal for applications with variable demand. </span><span class="kobospan" id="kobo.589.2">Amazon Bedrock, represents a significant leap forward, offering serverless access to foundational models through APIs, model customization, and seamless integration within an organizational network, ensuring data privacy </span><span><span class="kobospan" id="kobo.590.1">and security.</span></span></li>
</ul>
<p class="calibre6"><span class="kobospan" id="kobo.591.1">Let’s move on to the next topic, </span><span><span class="kobospan" id="kobo.592.1">Microsoft Aure.</span></span></p>
<h2 id="_idParaDest-200" class="calibre7"><a id="_idTextAnchor501" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.593.1">Microsoft Azure</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.594.1">Microsoft Azure stands at the </span><a id="_idIndexMarker874" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.595.1">forefront of cloud computing services, offering a robust platform for the development, deployment, and management of ML and LLMs. </span><span class="kobospan" id="kobo.595.2">Leveraging its strategic partnership with OpenAI, Azure provides exclusive cloud access</span><a id="_idIndexMarker875" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.596.1"> to GPT models, positioning itself as a critical resource for developers and data scientists aiming to harness the power of advanced NLP technologies. </span><span class="kobospan" id="kobo.596.2">Recent enhancements have expanded Azure’s capabilities, making it an even more attractive choice for those looking to push the boundaries of AI and </span><span><span class="kobospan" id="kobo.597.1">ML applications.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.598.1">Experimenting with LLMs on Azure</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.599.1">Azure has significantly enriched its</span><a id="_idIndexMarker876" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.600.1"> offerings to support research and experimentation with LLMs, providing a variety of tools and platforms that cater to the</span><a id="_idIndexMarker877" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.601.1"> diverse needs of the </span><a id="_idIndexMarker878" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.602.1">AI </span><span><span class="kobospan" id="kobo.603.1">development community:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.604.1">Azure OpenAI Service</span></strong><span class="kobospan" id="kobo.605.1">: This directly</span><a id="_idIndexMarker879" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.606.1"> integrates OpenAI’s cutting-edge models, including the latest GPT versions, DALL·E, and Codex, into the Azure ecosystem. </span><span class="kobospan" id="kobo.606.2">This service enables developers to easily incorporate sophisticated AI functionalities into their applications, with the added benefits of Azure’s scalability and </span><span><span class="kobospan" id="kobo.607.1">management tools.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.608.1">Azure Machine Learning (Azure ML)</span></strong><span class="kobospan" id="kobo.609.1">: This offers</span><a id="_idIndexMarker880" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.610.1"> an advanced environment for the custom training and fine-tuning of LLMs on specific datasets, allowing for enhanced model performance on niche tasks. </span><span class="kobospan" id="kobo.610.2">Azure ML Studio’s pre-built and customizable Jupyter notebook templates support a wide range of programming languages and frameworks, facilitating a seamless </span><span><span class="kobospan" id="kobo.611.1">experimentation process.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.612.1">Azure Cognitive Services</span></strong><span class="kobospan" id="kobo.613.1">: This provides</span><a id="_idIndexMarker881" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.614.1"> access to a suite of pre-built AI services, including text analytics, speech services, and decision-making capabilities powered by LLMs. </span><span class="kobospan" id="kobo.614.2">These services enable developers to add complex AI functions to</span><a id="_idIndexMarker882" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.615.1"> applications</span><a id="_idIndexMarker883" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.616.1"> quickly, without deep </span><span><span class="kobospan" id="kobo.617.1">ML expertise.</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.618.1">Deploying and productionizing LLMs on Azure</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.619.1">Azure’s infrastructure and services offer</span><a id="_idIndexMarker884" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.620.1"> comprehensive solutions for the deployment and productionization of LLM applications, ensuring scalability, performance, </span><span><span class="kobospan" id="kobo.621.1">and security:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.622.1">Deployment options</span></strong><span class="kobospan" id="kobo.623.1">: Azure supports various deployment </span><a id="_idIndexMarker885" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.624.1">scenarios through </span><strong class="bold"><span class="kobospan" id="kobo.625.1">Azure Container Instances</span></strong><span class="kobospan" id="kobo.626.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.627.1">ACI</span></strong><span class="kobospan" id="kobo.628.1">) for lightweight deployment </span><a id="_idIndexMarker886" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.629.1">needs and </span><strong class="bold"><span class="kobospan" id="kobo.630.1">Azure Kubernetes Service</span></strong><span class="kobospan" id="kobo.631.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.632.1">AKS</span></strong><span class="kobospan" id="kobo.633.1">) for larger, more complex applications requiring high scalability. </span><span class="kobospan" id="kobo.633.2">These services allow</span><a id="_idIndexMarker887" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.634.1"> for the efficient scaling of LLM applications to meet </span><span><span class="kobospan" id="kobo.635.1">user demand.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.636.1">Model management</span></strong><span class="kobospan" id="kobo.637.1">: Through Azure ML, developers can manage the life cycle of their models, including version control, auditing, and governance. </span><span class="kobospan" id="kobo.637.2">This ensures that deployed models are not only performant but also comply with industry standards and </span><span><span class="kobospan" id="kobo.638.1">regulatory requirements.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.639.1">Security and compliance</span></strong><span class="kobospan" id="kobo.640.1">: Azure emphasizes security and compliance across all its services, providing features such as data encryption, access control, and comprehensive compliance certifications. </span><span class="kobospan" id="kobo.640.2">This commitment ensures that applications built and deployed on Azure meet the highest standards for data protection </span><span><span class="kobospan" id="kobo.641.1">and privacy.</span></span></li>
</ul>
<h2 id="_idParaDest-201" class="calibre7"><a id="_idTextAnchor502" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.642.1">GCP</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.643.1">GCP continues to be a powerhouse in </span><a id="_idIndexMarker888" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.644.1">cloud computing, providing an extensive suite of services that cater to the evolving needs of AI and ML development. </span><span class="kobospan" id="kobo.644.2">Known for its cutting-edge innovations in AI and ML, GCP offers a rich ecosystem of tools and services that facilitate the development, deployment, and scaling of LLMs, making it an ideal platform for developers and researchers aiming to leverage the latest in </span><span><span class="kobospan" id="kobo.645.1">AI technology.</span></span></p>
<h3 class="calibre8"><span class="kobospan" id="kobo.646.1">Experimenting with LLMs on GCP</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.647.1">GCP has further enhanced its capabilities for </span><a id="_idIndexMarker889" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.648.1">experimenting with and developing LLMs, offering</span><a id="_idIndexMarker890" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.649.1"> a comprehensive set of tools that support the entire ML workflow, from data ingestion and model training to hyperparameter tuning </span><span><span class="kobospan" id="kobo.650.1">and evaluation:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.651.1">Vertex AI</span></strong><span class="kobospan" id="kobo.652.1">: At the heart of </span><a id="_idIndexMarker891" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.653.1">GCP’s ML offerings, Vertex AI provides an integrated suite of tools and services that streamline the ML workflow. </span><span class="kobospan" id="kobo.653.2">It offers </span><a id="_idIndexMarker892" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.654.1">advanced features for training and fine-tuning LLMs, including AutoML capabilities for automating the selection of optimal model architectures and hyperparameters. </span><span class="kobospan" id="kobo.654.2">Vertex AI’s integration with GCP’s robust data and analytics services makes it easier to manage large datasets that are essential for </span><span><span class="kobospan" id="kobo.655.1">training LLMs.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.656.1">An IDE</span></strong><span class="kobospan" id="kobo.657.1">: The built-in notebooks service within Vertex AI offers a fully managed JupyterLab environment, enabling developers to write, run, and debug ML code seamlessly. </span><span class="kobospan" id="kobo.657.2">This environment is optimized for ML development, supporting popular frameworks such as TensorFlow and PyTorch, which are crucial for building and experimenting </span><span><span class="kobospan" id="kobo.658.1">with LLMs.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.659.1">AI and ML libraries</span></strong><span class="kobospan" id="kobo.660.1">: GCP continues to expand its library of pre-trained models and ML APIs, including those specifically designed for NLP and understanding. </span><span class="kobospan" id="kobo.660.2">These tools allow developers to integrate advanced NLP capabilities into their </span><span><span class="kobospan" id="kobo.661.1">applications rapidly.</span></span></li>
</ul>
<h3 class="calibre8"><span class="kobospan" id="kobo.662.1">Deploying and productionizing LLMs on GCP</span></h3>
<p class="calibre6"><span class="kobospan" id="kobo.663.1">GCP provides robust and scalable solutions for deploying and productionizing LLMs, ensuring that applications built on its platform can</span><a id="_idIndexMarker893" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.664.1"> meet the demands of </span><span><span class="kobospan" id="kobo.665.1">real-world usage:</span></span></p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.666.1">Vertex AI prediction</span></strong><span class="kobospan" id="kobo.667.1">: Once an LLM</span><a id="_idIndexMarker894" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.668.1"> is trained, Vertex AI’s prediction service allows for the easy deployment of models as fully managed, auto-scaling endpoints. </span><span class="kobospan" id="kobo.668.2">This service simplifies the process of making your LLMs accessible to applications, with the infrastructure automatically adjusting to the </span><span><span class="kobospan" id="kobo.669.1">workload demands.</span></span></li>
<li class="calibre15"><strong class="bold"><span class="kobospan" id="kobo.670.1">Google Kubernetes Engine (GKE)</span></strong><span class="kobospan" id="kobo.671.1">: For more complex deployment scenarios requiring high availability and scalability, GKE offers a managed environment to deploy containerized LLM applications. </span><span class="kobospan" id="kobo.671.2">GKE’s global infrastructure ensures that your models are highly available and can scale to meet the needs of</span><a id="_idIndexMarker895" class="calibre5 pcalibre1 pcalibre"/> <span><span class="kobospan" id="kobo.672.1">enterprise-level applications.</span></span></li>
</ul>
<h2 id="_idParaDest-202" class="calibre7"><a id="_idTextAnchor503" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.673.1">Concluding cloud services</span></h2>
<p class="calibre6"><span class="kobospan" id="kobo.674.1">The landscape of cloud computing </span><a id="_idIndexMarker896" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.675.1">continues to evolve rapidly, with AWS, Azure, and GCP each offering unique advantages for the development and deployment of LLMs. </span><span class="kobospan" id="kobo.675.2">AWS stands out for its broad infrastructure and deep integration with ML tools, making it ideal for a wide range of ML and AI projects. </span><span class="kobospan" id="kobo.675.3">Azure, with its exclusive access to OpenAI’s models and deep integration within the Microsoft ecosystem, offers unparalleled opportunities for enterprises looking to leverage the cutting edge of AI technology. </span><span class="kobospan" id="kobo.675.4">GCP, recognized for its innovation in AI and ML, provides tools and services that mirror Google’s internal AI advancements, appealing to those seeking the latest in AI research and development. </span><span class="kobospan" id="kobo.675.5">As the capabilities of these platforms continue to expand, the choice between them will increasingly depend on specific project needs, organizational alignment, and strategic partnerships, underscoring the importance of a thoughtfu</span><a id="_idTextAnchor504" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.676.1">l evaluation based on the current and future landscape of cloud-based AI </span><span><span class="kobospan" id="kobo.677.1">and ML.</span></span></p>
<h1 id="_idParaDest-203" class="calibre4"><a id="_idTextAnchor505" class="calibre5 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.678.1">Summary</span></h1>
<p class="calibre6"><span class="kobospan" id="kobo.679.1">As the world of NLP and LLMs continues to grow rapidly, so do the various practices of system design. </span><span class="kobospan" id="kobo.679.2">In this chapter, we reviewed the design process of LLM applications and pipelines. </span><span class="kobospan" id="kobo.679.3">We discussed the components of these approaches, touching on both API-based closed source and local open source solutions. </span><span class="kobospan" id="kobo.679.4">We then gave you hands-on experience </span><span><span class="kobospan" id="kobo.680.1">with code.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.681.1">We later delved deeper into the system design process and introduced LangChain. </span><span class="kobospan" id="kobo.681.2">We reviewed what LangChain comprises and experimented with an example pipeline </span><span><span class="kobospan" id="kobo.682.1">in code.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.683.1">To complement the system design process, we surveyed leading cloud services that allow you to experiment, develop, and deploy </span><span><span class="kobospan" id="kobo.684.1">LLM-based solutions.</span></span></p>
<p class="calibre6"><span class="kobospan" id="kobo.685.1">In the next chapter, we’ll focus on particular practical use cases, accompanied </span><span><span class="kobospan" id="kobo.686.1">with code.</span></span></p>
</div>
</body></html>