<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-177"><a id="_idTextAnchor440" class="calibre5 pcalibre1 pcalibre"/>8</h1>
<h1 id="_idParaDest-178" class="calibre4"><a id="_idTextAnchor441" class="calibre5 pcalibre1 pcalibre"/>Accessing the Power of Large Language Models: Advanced Setup and Integration with RAG</h1>
<p class="calibre6">In this dynamic era of <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>) and <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>), understanding the vast assortment of available resources and learning how to utilize them effectively is vital. <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) such as GPT-4 have revolutionized the field of <strong class="bold">Natural</strong> <strong class="bold">Language Processing</strong> (<strong class="bold">NLP</strong>) by showcasing unprecedented performance in diverse tasks, from content generation to complex problem-solving. Their immense potential extends not only to understanding and generating human-like text but also to bridging the gap between machines and humans, in terms of communication and task automation. Embracing the practical applications of LLMs can empower businesses, researchers, and developers to create more intuitive, intelligent, and efficient systems that cater to a wide range of requirements. This chapter offers a guide to setting up access to LLMs, walking you through using them and building pipelines with them.</p>
<p class="calibre6">Our journey begins by delving into closed source models that utilize <strong class="bold">Application Programming Interfaces</strong> (<strong class="bold">APIs</strong>), taking OpenAI’s API as a quintessential example. We will walk you through a practical scenario, illustrating how you can interact with this API using an API key within your Python code, demonstrating the potential applications of such models in real-world contexts.</p>
<p class="calibre6">As we advance, we will shift our focus to the realm of open source tools, giving you a rundown of widely employed open source models that can be manipulated via Python. We aim to provide a grasp of the power and versatility these models provide, emphasizing the community-driven benefits of open source development.</p>
<p class="calibre6">Subsequently, we will introduce you to retrieval-augmented generation and, specifically, LangChain, a robust tool specifically engineered for interaction with LLMs. LangChain is essential for the practical application of LLMs because it provides a unified and abstracted interface to them, as well as a suite of tools and modules that simplify the development and deployment of LLM-powered applications. We’ll guide you through the foundational concept of LangChain, highlighting its distinctive methodology to circumvent the inherent challenges posed by LLMs.</p>
<p class="calibre6">The cornerstone of this methodology is the transformation of data into embeddings. We will shed light on the pivotal role that <strong class="bold">Language Models</strong> (<strong class="bold">LMs</strong>) and LLMs play in this transformation, demonstrating how they are engaged in creating these embeddings. Following this, we will discuss the process of establishing a local vector database, giving you a brief overview of vector databases and their crucial role in managing and retrieving these embeddings.</p>
<p class="calibre6">Then, we will address the configuration of an LLM for prompting, which could potentially be the same LLM used for the embedding process. We will take you through the stepwise setup procedure, detailing the advantages and potential applications of this strategy.</p>
<p class="calibre6">In the penultimate segment, we will touch upon the topic of deploying LLMs to the cloud. The scalability and cost-effectiveness of cloud services have led to an increased adoption of hosting AI models. We will provide an overview of the leading cloud service providers, including <strong class="bold">Microsoft Azure</strong>, <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), and <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), giving you insights into their service offerings and how they can be harnessed for LLM deployment.</p>
<p class="calibre6">As we embark on this exploration of LLMs, it’s crucial to acknowledge the continuously evolving data landscape that these models operate within. The dynamic nature of data – its growth in volume, diversity, and complexity – necessitates a forward-looking approach to how we develop, deploy, and maintain LLMs. In the subsequent chapters, particularly <a href="B18949_10.xhtml#_idTextAnchor525" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 10</em></a>, we will delve deeper into the strategic implications of these evolving data landscapes, preparing you to navigate the challenges and opportunities they present. This foundational understanding will not only enhance your immediate work with LLMs but also ensure your projects remain resilient and relevant in the face of rapid technological and data-driven changes.</p>
<p class="calibre6">Let’s go through the main topics covered in the chapter:</p>
<ul class="calibre14">
<li class="calibre15">Setting up an LLM application – API-based closed source models</li>
<li class="calibre15">Prompt engineering and priming GPT</li>
<li class="calibre15">Setting up an LLM application – local open source models</li>
<li class="calibre15">Employing LLMs from Hugging Face via Python</li>
<li class="calibre15">Exploring advanced system design – RAG and LangChain</li>
<li class="calibre15">Reviewing a simple LangChain setup in a Jupyter notebook</li>
<li class="calibre15">LLMs in the cloud</li>
</ul>
<h1 id="_idParaDest-179" class="calibre4"><a id="_idTextAnchor442" class="calibre5 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre6">For this chapter, the following will be necessary:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Programming knowledge</strong>: Familiarity with Python programming is a must, since the open source models, OpenAI’s API, and LangChain are all illustrated using Python code.</li>
<li class="calibre15"><strong class="bold">Access to OpenAI’s API</strong>: An API key from OpenAI will be required to explore closed source models. This can be obtained by creating an account with OpenAI and agreeing to their terms of service.</li>
<li class="calibre15"><strong class="bold">Open source models</strong>: Access to the specific open source models mentioned in this chapter will be necessary. These can be accessed and downloaded from their respective repositories or via package managers such as <strong class="source-inline1">pip</strong> or <strong class="source-inline1">conda</strong>.</li>
<li class="calibre15"><strong class="bold">A local development environment</strong>: A local development environment setup with Python installed is required. An <strong class="bold">Integrated Development Environment</strong> (<strong class="bold">IDE</strong>) such as PyCharm, Jupyter Notebook, or a simple text editor can be used. Note that we recommend a free Google Colab notebook, as it encapsulates all these requirements in a seamless web interface.</li>
<li class="calibre15"><strong class="bold">The ability to install libraries</strong>: You must have permission to install the required Python libraries, such as NumPy, SciPy, TensorFlow, and PyTorch. Note that the code we provide includes the required installations, and you won’t have to install them beforehand. We simply stress that you should have permission to do so, which we expect you would. Specifically, using a free Google Colab notebook would suffice.</li>
<li class="calibre15"><strong class="bold">Hardware requirements</strong>: Depending on the complexity and size of the models you’re working with, a computer with sufficient processing power (potentially including a good GPU for ML tasks) and ample memory will be required. This is only relevant when choosing to not use the free Google Colab.</li>
</ul>
<p class="calibre6">Now that we’ve grasped the transformative potential of LLMs and the variety of tools available, let’s delve deeper and explore how to effectively set up LLM applications using APIs.</p>
<h1 id="_idParaDest-180" class="calibre4"><a id="_idTextAnchor443" class="calibre5 pcalibre1 pcalibre"/>Setting up an LLM application – API-based closed source models</h1>
<p class="calibre6">When looking to employ models in general and LLMs in particular, there are various design choices and trade-offs. One key <a id="_idIndexMarker815" class="calibre5 pcalibre1 pcalibre"/>choice is whether to host a model locally in your local environment or to employ it remotely, accessing it via a communication channel. Local development environments would be wherever your code runs, whether that’s your personal computer, your on-premises server, your cloud environment, and so on. The choice you make will impact many aspects, such as cost, information security, maintenance needs, network overload, and inference speed.</p>
<p class="calibre6">In this section, we will introduce a quick and simple approach to employing an LLM remotely via an API. This approach is quick and simple as it rids us of the need to allocate unusual computation resources to host the LLM locally. An LLM typically requires amounts of memory and computation resources that aren’t common in personal environments.</p>
<h2 id="_idParaDest-181" class="calibre7"><a id="_idTextAnchor444" class="calibre5 pcalibre1 pcalibre"/>Choosing a remote LLM provider</h2>
<p class="calibre6">Before diving into implementation, we<a id="_idIndexMarker816" class="calibre5 pcalibre1 pcalibre"/> need to select a suitable LLM provider that aligns with our project requirements. OpenAI, for example, offers several versions of the GPT-3.5 and GPT-4 models with comprehensive API documentation.</p>
<h3 class="calibre8">OpenAI’s remote GPT access in Python via an API<a id="_idTextAnchor445" class="calibre5 pcalibre1 pcalibre"/></h3>
<p class="calibre6">To gain access to OpenAI’s LLM API, we <a id="_idIndexMarker817" class="calibre5 pcalibre1 pcalibre"/>need to create an account on their website. This process involves registration, account verification, and obtaining API credentials.</p>
<p class="calibre6">OpenAI’s website provides guidance for these common actions, and you will be able to get set up quickl<a id="_idTextAnchor446" class="calibre5 pcalibre1 pcalibre"/>y.</p>
<p class="calibre6">Once registered, we should familiarize ourselves with OpenAI’s API documentation. This documentation will guide us through the various endpoints, methods, and parameters available to interact with the LLMs.</p>
<p class="calibre6">The first hands-on experience we will take on will b<a id="_idTextAnchor447" class="calibre5 pcalibre1 pcalibre"/>e employing OpenAI’s LLMs via Python. We have put together a<a id="_idIndexMarker818" class="calibre5 pcalibre1 pcalibre"/> notebook that presents the simple steps of employing OpenAI’s GPT model via an API. Refer to the <code>Ch8_Setting_Up_Close_Source_and_Open_Source_LLMs.ipynb </code>notebook. This notebook, called <em class="italic">Setting Up Close Source and Open Source LLMs</em>, will be utilized in the current section about OpenAI’s API, and also in the subsequent section about setting up local LLMs.</p>
<p class="calibre6">Let’s walk through the code:</p>
<ol class="calibre16">
<li class="calibre15">We start by installing the required Python libraries. In particular, to communicate with the LLM API, we need to install the necessary Python library:<pre class="source-code">
!pip install --upgrade op<a id="_idTextAnchor448" class="calibre314 pcalibre1 pcalibre"/>enai</pre></li> <li class="calibre15"><strong class="bold">Define OpenAI’s API key</strong>: Before making requests to the LLM API, we must embed our personal API key in the library’s configuration. The API key is made available for you on OpenAI’s website when you register. This can be done by either explicitly pasting the key’s string to be hardcoded in our code or reading it from a file that holds that string. Note that the former is the simplest way to showcase the API, as it doesn’t require an additional file to be set up, but it may not be the right choice when working in a shared development environment:<pre class="source-code">
openai.api_key = "&lt;your key&gt;"</pre></li> <li class="calibre15"><strong class="bold">Settings – set the model’s configurations</strong>. Here, we set the various parameters that control the model’s behavior.</li>
</ol>
<p class="calibre6">As the foundation is set for connecting to LLMs through APIs, it’s valuable to turn our attention to an equally important aspect – prompt engineering and priming, the art of effectively communicating with these mode<a id="_idTextAnchor449" class="calibre5 pcalibre1 pcalibre"/>ls.</p>
<h1 id="_idParaDest-182" class="calibre4"><a id="_idTextAnchor450" class="calibre5 pcalibre1 pcalibre"/>Prompt engineering and priming GPT</h1>
<p class="calibre6">Let us pause and provide some context before returning to discuss the next part of the code.</p>
<p class="calibre6">Prompt engineering is a technique<a id="_idIndexMarker819" class="calibre5 pcalibre1 pcalibre"/> used in NLP to design effective prompts or instructions when interacting with LLMs. It involves carefully crafting the input given to a model to elicit the desired output. By providing specific cues, context, or constraints in the prompts, prompt engineering aims to guide the model’s behavior and encourage the generation of more accurate, relevant, or targeted responses. The process often involves iterative refinement, experimentation, and understanding the model’s strengths and limitations to optimize the prompt for improved performance in various tasks, such as question-answering summarization or conversation generation. Effective prompt engineering plays a vital role in harnessing the capabilities of LMs and shaping their output to meet specific user requirements.</p>
<p class="calibre6">Let’s review one of the most impactful tools in prompt engineering, <strong class="bold">priming</strong>. Priming GPT via an API involves providing initial context to the model before generating a response. The priming step helps set the direction and style of the generated content. By giving the model relevant information or examples related to the desired output, we can guide its understanding and encourage more focused and coherent responses. Priming can be done by including specific instructions, context, or even partial sentences that align with the desired outcome. Effective priming enhances the model’s ability to generate responses that better match the user’s intent or specific requirements.</p>
<p class="calibre6">Priming is done by introducing GPT <a id="_idIndexMarker820" class="calibre5 pcalibre1 pcalibre"/>with several types of messages:</p>
<ul class="calibre14">
<li class="calibre15">The main message is the <strong class="bold">system prompt</strong>. This <a id="_idIndexMarker821" class="calibre5 pcalibre1 pcalibre"/>message instructs the model about the <em class="italic">role</em> it may play, the way it should answer the questions, the constraints it may have, and so on.</li>
<li class="calibre15">The second type of message<a id="_idIndexMarker822" class="calibre5 pcalibre1 pcalibre"/> is a <strong class="bold">user prompt</strong>. A user prompt is sent to the model in the priming phase, and it represents an example user prompt, much like the prompt you may enter in ChatGPT’s web interface. However, when priming, this message could be presented to the model as an example of how it should address such a prompt. The developer will introduce a user prompt of some sort and will then show the model how it is expected to answer that prompt.<p class="calibre6">For example, observe<a id="_idIndexMarker823" class="calibre5 pcalibre1 pcalibre"/> this priming code:</p><pre class="source-code">
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
           {"role": "system",
               "content": "You are a helpful assistant. You provide short answers and you format them in Markdown syntax"},
              {"role": "user",
               "content": "How do I import the Python library pandas?"},
              {"role": "assistant",
               "content": "This is how you would import pandas:  \n```\nimport pandas as pd\n```"},
              {"role": "user",
               "content": "How do I import the python library numpy?"}
        ])
text = response.choices[0].message.content.strip()
print(text)
)</pre><p class="calibre6">This is the<a id="_idIndexMarker824" class="calibre5 pcalibre1 pcalibre"/> output:</p><pre class="source-code">To import numpy, you can use the following syntax: 
```python  
import numpy as np  
```</pre></li> </ul>
<p class="calibre6">You can see that we prime the model to provide concise answers in a Markdown format. The example that is used to teach the model is in the form of a question and an answer. The question is via a user prompt, and the way we tell the model what the potential answer is is provided via an assistant prompt. We then provide the model with another user prompt; this one is the actual prompt we’d like the model to address for us, and as shown in the output, it gets it right.</p>
<p class="calibre6">By looking at OpenAI’s documentation about prompt engineering, you’ll find that there are additional types of prompts to <a id="_idIndexMarker825" class="calibre5 pcalibre1 pcalibre"/>prime the GPT models with.</p>
<p class="calibre6">Going back to our notebook <a id="_idIndexMarker826" class="calibre5 pcalibre1 pcalibre"/>and code, in this section, we leverage <em class="italic">GPT-3.5 Turbo</em>. We prime it in the simplest manner, only giving it a system prompt to provide directions in order to showcase how additional functionality could stem from the system prompt. We tell the model to finish a response by alerting us about typos in the prompt and correcting them.</p>
<p class="calibre6">We then provide our desired prompt in the user prompt section, and we insert a few typ<a id="_idTextAnchor451" class="calibre5 pcalibre1 pcalibre"/>os into it. Run that code and give it a shot.</p>
<h2 id="_idParaDest-183" class="calibre7"><a id="_idTextAnchor452" class="calibre5 pcalibre1 pcalibre"/>Experimenting with OpenAI’s GPT model</h2>
<p class="calibre6">At this stage, we send our prompts to the<a id="_idIndexMarker827" class="calibre5 pcalibre1 pcalibre"/> model.</p>
<p class="calibre6">The following simple example code is run once in the <em class="italic">Setting Up Close Source and Open Source LLMs</em> notebook. You can wrap it in a function and call it repeatedly in your own code.</p>
<p class="calibre6">Some aspects worth noticing are as follows:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Parsing and processing the returned output from the model</strong>: We structure the output response in a coherent manner for the user to read:<pre class="source-code">
print(f"Prompt: {user_prompt_oai}\n\n{openai_model}'s Response: \n{response_oai}")</pre></li> <li class="calibre15"><strong class="bold">Error handling</strong>: We designed the code to allow for several failed attempts before accepting a failure to use the API:<pre class="source-code">
except Exception as output:
    attempts += 1
    if attempts &gt;= max_attempts:
        […]</pre></li> <li class="calibre15"><strong class="bold">Rate limits and cost mitigation</strong>: We don’t implement such restrictions here, but it would be ideal to <a id="_idIndexMarker828" class="calibre5 pcalibre1 pcalibre"/>have both of these in an experimental setting and perhaps in<a id="_idTextAnchor453" class="calibre5 pcalibre1 pcalibre"/> production.<p class="calibre6">The result of the preceding code is demonstrated as follows:</p><pre class="source-code">
Prompt: If neuroscience could extract the last thoughts a person had before they dyed, how would the world be different?
gpt-3.5-turbo's Response:
If neuroscience could extract the last thoughts a person had before they died, it would have profound implications for various aspects of society.
This ability could potentially revolutionize fields such as psychology, criminology, and end-of-life care.
Understanding a person's final thoughts could provide valuable insights into their state of mind, emotional well-being, and potentially help unravel mysteries surrounding their death.
It could also offer comfort to loved ones by providing a glimpse into the innermost thoughts of the deceased.
However, such technology would raise significant ethical concerns regarding privacy, consent, and the potential misuse of this information.
Overall, the world would be both fascinated and apprehensive about the implications of this groundbreaking capability.
Typos in the prompt:
1. "dyed" should be "died"
2. "diferent" should be "different"
Corrections:
If neuroscience could extract the last thoughts a person had before they died, how would the world be different?</pre></li> </ul>
<p class="calibre6">The model provided us with a legitimate, concise response. It then notified us about the typos, which are perfectly in line with the system prompt we provided it with.</p>
<p class="calibre6">That was an example<a id="_idIndexMarker829" class="calibre5 pcalibre1 pcalibre"/> showcasing the employment of a remote, off-premises, closed source LLM. While leveraging the power of paid APIs such as OpenAI offers convenience and cutting-edge performance, there’s also immense potential in tapping into free open source LLMs. Let’s explore these cost-effective alte<a id="_idTextAnchor454" class="calibre5 pcalibre1 pcalibre"/>rnatives next.</p>
<h1 id="_idParaDest-184" class="calibre4"><a id="_idTextAnchor455" class="calibre5 pcalibre1 pcalibre"/>Setting up an LLM application – local open source models</h1>
<p class="calibre6">Now, we shall touch on the <a id="_idIndexMarker830" class="calibre5 pcalibre1 pcalibre"/>complementary approach to a closed source implementation, that is, an open source, local implementation.</p>
<p class="calibre6">We will see how you can achieve a similar functional outcome to the one we reviewed in the previous section, without having to register for an account, pay, or share prompts that contain possibly sensitive information with a third-party vendor, such as OpenAI.</p>
<h2 id="_idParaDest-185" class="calibre7"><a id="_idTextAnchor456" class="calibre5 pcalibre1 pcalibre"/>About the different aspects that distinguish between open source and closed source</h2>
<p class="calibre6">When selecting between <a id="_idIndexMarker831" class="calibre5 pcalibre1 pcalibre"/>open source LLMs, such as LLaMA and GPT-J, and closed source, API-based models such as OpenAI’s GPT, several critical factors must be considered.</p>
<p class="calibre6">Firstly, cost is a major factor. Open source LLMs often have no licensing fees, but they require significant computational resources for training and inference, which can be expensive. Closed source models, while potentially carrying a subscription or pay-per-use fee, eliminate the need for substantial hardware investments.</p>
<p class="calibre6">Processing speed and maintenance are closely linked to computational resources. Open source LLMs, if deployed on powerful enough systems, can offer high processing speeds but require ongoing maintenance and updates by the implementing team. In contrast, closed source models managed by the provider ensure continual maintenance and model updates, often with better efficiency and reduced downtime, but processing speed can be dependent on the provider’s infrastructure and network latency.</p>
<p class="calibre6">Regarding model updates, open source models offer more control but require a proactive approach to incorporate the latest research and improvements. Closed source models, however, are regularly updated by the provider, ensuring access to the latest advancements without<a id="_idIndexMarker832" class="calibre5 pcalibre1 pcalibre"/> additional effort from the user.</p>
<p class="calibre6">Security and privacy are paramount in both scenarios. Open source models can be more secure, as they can be run on private servers, ensuring data privacy. However, they demand robust in-house security protocols. Closed source models, managed by external providers, often come with built-in security measures but pose potential privacy risks, due to data handling by third parties.</p>
<p class="calibre6">Overall, the choice between open source and closed source LLMs hinges on the trade-off between cost, control, and convenience, with each option presenting its own set of advantages and challenges.</p>
<p class="calibre6">With that in mind, let’s revisit Hugging Face, the company that put together the largest and most approachable hub for free LMs. In the following example, we will leverage Hugging Face’s easy and free libr<a id="_idTextAnchor457" class="calibre5 pcalibre1 pcalibre"/>ary: transformers.</p>
<h2 id="_idParaDest-186" class="calibre7"><a id="_idTextAnchor458" class="calibre5 pcalibre1 pcalibre"/>Hugging Face’s hub of models</h2>
<p class="calibre6">When looking to choose an LLM for our task, we recommend referring to Hugging Face’s Models online page. They offer an enormous <a id="_idIndexMarker833" class="calibre5 pcalibre1 pcalibre"/>amount of Python-based, open source LLMs. Every model has a page dedicated to it, where you can find information about it, including the syntax needed to employ that model via Python code in your personal environment.</p>
<p class="calibre6">It should be noted that in order to implement a model locally, you must have an internet connection from the machine that runs the Python code. However, as this requirement may become a bottleneck in some cases – for instance, when the development environment is restricted by a company’s intranet or has limited internet access due to firewall restrictions – there are alternative approaches. Our recommended approach is to clone the model repository from Hugging Face’s domain. That is a less trivial and less-used approach. Hugging Face provides the necessary cloning commands on each<a id="_idTextAnchor459" class="calibre5 pcalibre1 pcalibre"/> model’s web page.</p>
<h3 class="calibre8">Choosing a model</h3>
<p class="calibre6">When looking to choose a model, there may be several factors that come into play. Depending on your intentions, you may care about configuration speed, processing speed, storage space, computation resources, legal usage restrictions, and so on. Another factor worth noting is the popularity of a model. It attests to how frequently that model is chosen by other <a id="_idIndexMarker834" class="calibre5 pcalibre1 pcalibre"/>developers in the community. For instance, if you look for LMs that are labeled for zero-shot classification, you will find a very large collection of available models. But, if you then narrow the search some more so to only be left with models that were trained on data from news articles, you would be left with a much smaller set of available models. In which case, you may want to refer to the popularity of each model and start your exploration with the model that was used the most.</p>
<p class="calibre6">Other factors that may interest you could be publications about the model, the model’s developers, the company or university that released the model, the dataset that the model was trained on, the architecture the model was designed by, the evaluation metrics, and other potential factors that may be available on each model’s web page on Hugg<a id="_idTextAnchor460" class="calibre5 pcalibre1 pcalibre"/>ing Face’s website.</p>
<h1 id="_idParaDest-187" class="calibre4"><a id="_idTextAnchor461" class="calibre5 pcalibre1 pcalibre"/>Employing LLMs from Hugging Face via Python</h1>
<p class="calibre6">Now, we will review a code notebook that exemplifies implementing an open source LLM locally using <a id="_idIndexMarker835" class="calibre5 pcalibre1 pcalibre"/>Hugging Face’s free resources. We will continue with the same notebook from the previous section, <em class="italic">Setting Up Close Source a<a id="_idTextAnchor462" class="calibre5 pcalibre1 pcalibre"/>nd Open </em><em class="italic">Source </em>LLMs:</p>
<ol class="calibre16">
<li class="calibre15"><code>pip</code> on the Terminal, we will run the following:</p><pre class="source-code">
pip install –upgrade transformers</pre><p class="calibre6">Alternatively, if running directly from a Jupyter notebook, add <code>!</code> to the begin<a id="_idTextAnchor463" class="calibre5 pcalibre1 pcalibre"/>ning of the command.</p></li> <li class="calibre15"><strong class="bold">Experiment with Microsoft’s DialoGPT-medium</strong>: This LLM is dedicated to conversational applications. It was generated by Microsoft and achieved high scores when compared to <a id="_idIndexMarker836" class="calibre5 pcalibre1 pcalibre"/>other LLMs on common benchmarks. For that reason, it is also quite popular on Hugging Face’s platform, in the sense that it is downloaded frequently by ML developers.</li>
<li class="calibre15">In the <strong class="bold">Settings</strong> code section in the notebook, we will define the parameters for this code and import the model and its tokenizer:<pre class="source-code">
<strong class="source-inline2">hf_model = "microsoft/DialoGPT-medium"</strong>
<strong class="source-inline2">max_length = 1000</strong>
<strong class="source-inline2">tokenizer = AutoTokenizer.from_pretrained(hf_model)</strong>
<strong class="source-inline2">model = AutoModelForCausalLM.from_pretrained(hf_model)</strong></pre><p class="calibre6">Note that this code requires access to the internet. Even though the model is deployed locally, an internet connection is required to import it. Again, if you wish, you can clone the model’s repo from Hugging Face and then no longer be required to have ac<a id="_idTextAnchor464" class="calibre5 pcalibre1 pcalibre"/>cess to the internet.</p></li> <li class="calibre15"><strong class="bold">Define the prompt</strong>: As can be seen in the following code block, we picked a straightforward prompt here, much like a user prompt for the<a id="_idTextAnchor465" class="calibre5 pcalibre1 pcalibre"/> GPT-3.5-Turbo model.</li>
<li class="calibre15"><strong class="bold">Experiment with the model</strong>: Here, we have the syntax that suits this code. If you want to create a rolling conversation with this model, you wrap this code in a function and iterate over it, collecting prompts from t<a id="_idTextAnchor466" class="calibre5 pcalibre1 pcalibre"/>he user in real time.</li>
<li class="calibre15"><strong class="bold">The result</strong>: The resulting prompt is, <strong class="source-inline1">If dinosaurs were alive today, would they possess a threat </strong><strong class="source-inline1">to people?:</strong><pre class="source-code">
microsoft/DialoGPT-medium's Response:
I think they would be more afraid of the humans</pre></li> </ol>
<p class="calibre6">This section established the tremendous value proposition that LLMs can bring. We now have the necessary background <a id="_idIndexMarker837" class="calibre5 pcalibre1 pcalibre"/>to learn and explore a new frontier in efficient LLM application development – constructing pipelines using tools such as LangChain. Let’s dive into t<a id="_idTextAnchor467" class="calibre5 pcalibre1 pcalibre"/>his advanced approach.</p>
<h1 id="_idParaDest-188" class="calibre4"><a id="_idTextAnchor468" class="calibre5 pcalibre1 pcalibre"/>Exploring advanced system design – RAG and LangChain</h1>
<p class="calibre6"><strong class="bold">Retrieval-Augmented Generation</strong> (<strong class="bold">RAG</strong>) is a development framework designed for seamless interaction with LLMs. LLMs, by virtue of their <a id="_idIndexMarker838" class="calibre5 pcalibre1 pcalibre"/>generalist nature, are capable of performing a vast array of tasks competently. However, their generality often precludes them from delivering detailed, nuanced responses to queries that necessitate specialized knowledge or in-depth expertise in a domain. For instance, if you aspire to use an LLM to address queries concerning a specific discipline, such as law or medicine, it might satisfactorily answer general queries but fail to respond accurately to those needing detailed insights or up-to-date knowledge.</p>
<p class="calibre6">RAG designs offer a comprehensive solution to the limitations typically encountered in LLM processing. In a RAG framework, the text corpus undergoes initial preprocessing, where it’s segmented into summaries or distinct chunks and then embedded within a vector space. When a query is made, the model identifies the most relevant segments of this data and utilizes them to form a response. This process involves a combination of offline data preprocessing, online information retrieval, and the application of the LLM for response generation. It’s a versatile approach that can be adapted to a variety of tasks, including code generation and semantic search. RAG models function as an abstraction layer that orchestrates these processes. The efficacy of this method is continually increasing, with its applications expanding as LLMs evolve and require more contextually rich data during prompt processing. In <a href="B18949_10.xhtml#_idTextAnchor525" class="calibre5 pcalibre1 pcalibre"><em class="italic">Chapter 10</em></a>, we will present a deeper discussion of RAG models and their role in the future of LLM solutions.</p>
<p class="calibre6">Now that we’ve introduced the premise and capabilities of RAG models, let’s focus on one particular example, called LangChain. We will review the nuts and bolts of its design principles and how it interfaces with data sources.</p>
<h2 id="_idParaDest-189" class="calibre7"><a id="_idTextAnchor469" class="calibre5 pcalibre1 pcalibre"/>LangChain’s design concepts</h2>
<p class="calibre6">In this section, we will dissect the core methodologies and architectural decisions that make LangChain stand out. This will <a id="_idIndexMarker839" class="calibre5 pcalibre1 pcalibre"/>give us insights into its structural framework, the efficiency of data handling, and its innovative approach to integrating LLMs with various data sources.</p>
<h2 id="_idParaDest-190" class="calibre7"><a id="_idTextAnchor470" class="calibre5 pcalibre1 pcalibre"/>Data sources</h2>
<p class="calibre6">One of the most significant<a id="_idIndexMarker840" class="calibre5 pcalibre1 pcalibre"/> virtues of LangChain is the ability to connect an arbitrary LLM to a defined data source. By arbitrary, we mean that it could be any <em class="italic">off-the-shelf</em> LLM that was designed and trained with no specific regard to the data we are looking to connect it to. Employing LangChain allows us to customize it to our domain. The data source is to be used for reference when structuring the answer to the user prompt. That data may be proprietary data owned by a company or local personal information on your personal machine.</p>
<p class="calibre6">However, when it comes to leveraging a given database, LangChain does more than point the LLM to the data; it employs a particular processing scheme and makes it quick and efficient. It creates a vector database.</p>
<p class="calibre6">Given raw text data, be it free text in a <code>.txt</code> file, formatted files, or other various data structures of text, a vector database is created by chunking the text into appropriate lengths and creating numerical text embeddings, using a designated model. Note that if the designated embedding model is chosen to be an LLM, it doesn’t have to be the same LLM that is used for prompting. For instance, the embedding model could be picked to be a free, sub-optimal, open source LLM, and the prompting model could be a paid LLM with optimal performance. Those embeddings are then stored in a vector database. You can clearly see that this approach is extremely storage-efficient, as we transform text, and perhaps encoded text, into a finite set of numerical values, which by its nature is dense.</p>
<p class="calibre6">When a user enters a prompt, a search mechanism identifies the relevant data chunks in the embedded data source. The prompt gets embedded with the same designated embedding model. Then, the search mechanism applies a similarity metric, such as cosine similarity, for example, and finds the most similar text chunks in the defined data source. Then, the original text of these chunks is retrieved. The original prompt is then sent again, this time to the prompting LLM. The difference is that, this time, the prompt consists of more than just the original user’s prompt; it also consists of the retrieved text as a reference. This enables the LLM to get a question and a rich text supplement for reference. The LLM then can refer to the added information as a reference.</p>
<p class="calibre6">If it weren’t for this design, when the user wanted to find an answer to their question, they would need to read through the vast material and find the relevant section. For instance, the material may be a company’s entire product methodology, consisting of many PDF documents. This<a id="_idIndexMarker841" class="calibre5 pcalibre1 pcalibre"/> process leverages an automated smart search mechanism that narrows the relevant material down to an amount of text that can fit into a prompt. Then, the LLM frames the answer to the question and presents it to the user immediately. If you wish, the pipeline can be designed to quote the original text that it used to frame the answer, thus allowing for transparency and verification.</p>
<p class="calibre6">This paradigm is portrayed in <em class="italic">Figure 8</em><em class="italic">.1</em>:</p>
<p class="calibre6">.</p>
<div><div><img alt="Figure 8.1 – The paradigm of a typical LangChain pipeline" src="img/B18949_08_1.jpg" class="calibre3"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 8.1 – The paradigm of a typical LangChain pipeline</p>
<p class="calibre6">In order to explain the prompt engineering behind the LangChain pipeline, let’s review a financial information use case. Your data source is a cohort of <strong class="bold">Securities</strong> <strong class="bold">&amp;</strong> <strong class="bold">Exchange</strong> <strong class="bold">Commission</strong> (<strong class="bold">SEC</strong>) filings of public companies from the US. You are looking<a id="_idIndexMarker842" class="calibre5 pcalibre1 pcalibre"/> to identify companies that gave dividends to their stock holders, and in what year.</p>
<p class="calibre6">Your prompt would be as follows:</p>
<pre class="source-code">
Which filings mention that the company gave dividends in the year 2023?</pre> <p class="calibre6">The pipeline then embeds this question and looks for text chunks with similar context (e.g., that discuss paid dividends). It identifies many such chunks, such as the following:</p>
<pre class="source-code">
"Dividend Policy. Dividends are paid at the discretion of the Board of Directors. In fiscal 2023, we paid aggregate quarterly cash dividends of $8.79 per share […]"</pre> <p class="calibre6">The LangChain pipeline then forms a new prompt that includes the text of the identified chunks. In this example, we assume the prompted LLM is OpenAI’s GPT. LangChain embeds the information in the system prompt sent to OpenAI’s GPT model:</p>
<pre class="source-code">
"prompts": [
    "System: Use the following pieces of context to answer the user's question. \nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\n----------------\n Dividend Policy. Dividends are paid at the […]"
]</pre> <p class="calibre6">As we can see, the system prompt is used to instruct the model how to act and then to provide the context.</p>
<p class="calibre6">Now that we have an understanding of the foundational approach and benefits of LangChain, let’s go deeper into its intricate design concepts, starting with how it bridges LLMs to diverse data sources efficiently.</p>
<h2 id="_idParaDest-191" class="calibre7"><a id="_idTextAnchor471" class="calibre5 pcalibre1 pcalibre"/>Data that is not pre-embedded</h2>
<p class="calibre6">While the preceding description is of data<a id="_idIndexMarker843" class="calibre5 pcalibre1 pcalibre"/> that is preprocessed to take the form of a vector database, another approach is to set up access to external data sources that are not yet processed into an embedding form. For instance, you may wish to leverage a SQL database to supplement other data sources. This approach<a id="_idIndexMarker844" class="calibre5 pcalibre1 pcalibre"/> is referred to as <strong class="bold">multiple </strong><strong class="bold">retrieval sources</strong>.</p>
<p class="calibre6">We’ve now explored the<a id="_idIndexMarker845" class="calibre5 pcalibre1 pcalibre"/> ways LangChain efficiently interfaces with various data sources; now, it is essential to grasp the core structural elements that enable its functionalities – chains and agents.</p>
<h2 id="_idParaDest-192" class="calibre7"><a id="_idTextAnchor472" class="calibre5 pcalibre1 pcalibre"/>Chains</h2>
<p class="calibre6">The atomic building blocks within<a id="_idIndexMarker846" class="calibre5 pcalibre1 pcalibre"/> LangChain are called components. Typical components could be a prompt template, access to various data sources, and access to LLMs. When combining various components to form a system, we form a chain. A chain can represent a complete LLM-driven application.</p>
<p class="calibre6">We will now present the concept of agents and walk through a code example that showcases how chains and agents come together, creating a capability that would have been quite complex not too long ago.</p>
<h2 id="_idParaDest-193" class="calibre7"><a id="_idTextAnchor473" class="calibre5 pcalibre1 pcalibre"/>Agents</h2>
<p class="calibre6">The next layer of complexity over chains is agents. Agents leverage chains by employing them and complementing them<a id="_idIndexMarker847" class="calibre5 pcalibre1 pcalibre"/> with additional calculations and decisions. While a chain may yield a response to a simple request prompt, an agent would process the response and act upon it with further downstream processing based on a prescribed logic.</p>
<p class="calibre6">You can view agents as a reasoning mechanism that employs what we call a tool. Tools complement LLMs by connecting them with other data or functions.</p>
<p class="calibre6">Given the typical LLM shortcomings that prevent LLMs from being perfect multitaskers, agents employ tools in a prescribed and monitored manner, allowing them to retrieve necessary information, leverage it as context, and execute actions using designated existing solutions. Agents then observe the results and employ the prescribed logic for further downstream processes.</p>
<p class="calibre6">As an example, assume we want to calculate the salary trajectory for an average entry-level programmer in our area. This task is comprised of three key sub-tasks – finding out what that average starting salary is, identifying the factors for salary growth (e.g., a change in the cost of living, or a typical merit increase), and then projecting onward. An ideal LLM would be able to do the entire process by itself, not requiring anything more than a coherent prompt. However, given the typical shortcomings, such as hallucinations and limited training data, current LLMs would not be able to perform this entire process to a level where it could be productionized within a commercial product. A best practice is to break it down and monitor the thought process via agents.</p>
<p class="calibre6">In its most simple design, this <a id="_idIndexMarker848" class="calibre5 pcalibre1 pcalibre"/>would require the following:</p>
<ol class="calibre16">
<li class="calibre15">Defining an agent that can access the internet and that can calculate future values of time series, given growth factors</li>
<li class="calibre15">Providing the agent with a comprehensive prompt</li>
<li class="calibre15">The agent breaks the prompt down into the different sub-tasks:<ol class="calibre285"><li class="upper-roman">Fetching the average salary from the internet</li><li class="upper-roman">Fetching the growth factors</li><li class="upper-roman">Employing the calculation tool by applying the growth factors to the starting salary and creating a future time series for salary values</li></ol></li>
</ol>
<p class="calibre6">To exemplify the agentic approach, let's review a simple task that involves fetching a particular detail from the web, and using it to perform a calculation.</p>
<ol class="calibre16">
<li class="calibre15">First, install these packages:<pre class="source-code">
!pip install openai
!pip install wikipedia
!pip install langchain
!pip install langchain-openai</pre></li> <li class="calibre15">Then, run the following code:<pre class="source-code">
from langchain.agents import load_tools, initialize_agent
from langchain_openai import OpenAI
import os
os.environ["OPENAI_API_KEY"] = "&lt;your API key&gt;"
llm = OpenAI(model_name='gpt-3.5-turbo-instruct')
tools = load_tools(["wikipedia", "llm-math"], llm=llm)
agent = initialize_agent(tools, llm=llm, agent="zero-shot-react-description", verbose=True)
agent.run("Figure out how many pages are there in the book Animal Farm. Then calculate how many minutes would it take me to read it if it takes me two minutes to read one page.")</pre><p class="calibre6">The output is then shown as follows:</p><pre class="source-code">&gt; Finished chain.
'It would take me approximately 224 minutes or 3 hours and 44 minutes to read Animal Farm.'</pre></li> </ol>
<p class="calibre6">Note that we didn’t apply any <a id="_idIndexMarker849" class="calibre5 pcalibre1 pcalibre"/>method to fix the LLM to reproduce this exact response. Running this code again will yield a slightly different answer.</p>
<p class="calibre6">In the next chapter, we will dive deeper into several examples with code. In particular, we will program a multi-agent framework, where a team of agents is working on a joint project.</p>
<h2 id="_idParaDest-194" class="calibre7"><a id="_idTextAnchor474" class="calibre5 pcalibre1 pcalibre"/>Long-term memory and referring to prior conversations</h2>
<p class="calibre6">Another very important concept is<a id="_idIndexMarker850" class="calibre5 pcalibre1 pcalibre"/> long-term memory. We discussed how LangChain complements an LLM’s knowledge by appending additional data sources, some of which may<a id="_idIndexMarker851" class="calibre5 pcalibre1 pcalibre"/> be proprietary, making it highly customized for a particular use case. However, it still lacks a very important function, the ability to refer to prior conversations and learn from them. For instance, you can design an assistant for a project manager. As the user interacts with it, they would ideally update each day about the progress of the work, the interactions, the challenges, and so on. It would be best if the assistant could digest all that newly accumulated knowledge and sustain it. That would allow for a scenario such as this:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">User</strong>: “Where do we stand with regard to Jim’s team’s task?”</li>
<li class="calibre15"><strong class="bold">Assistant</strong>: “According to the original roadmap, Jim’s team is to address the client’s feedback to the design of the prototype. Based on the update from last week, the client provided only partial feedback, which you felt would not yet be suffi<a id="_idTextAnchor475" class="calibre5 pcalibre1 pcalibre"/>cient for Jim’s team to<a id="_idIndexMarker852" class="calibre5 pcalibre1 pcalibre"/> start work.”</li>
</ul>
<p class="calibre6">We will touch more on the<a id="_idIndexMarker853" class="calibre5 pcalibre1 pcalibre"/> concept of memory in the next chapter.</p>
<h2 id="_idParaDest-195" class="calibre7"><a id="_idTextAnchor476" class="calibre5 pcalibre1 pcalibre"/>Ensuring continuous relevance through incremental updates and automated monitoring</h2>
<p class="calibre6">To maintain the accuracy and relevance <a id="_idIndexMarker854" class="calibre5 pcalibre1 pcalibre"/>of LLM outputs in dynamic information environments, it’s imperative to implement strategies for the ongoing update and maintenance of vector databases. As the corpus of knowledge continues to expand and evolve, so too must the embeddings that serve as the foundation for LLM responses. Incorporating techniques for incremental updates allows these databases to refresh their embeddings as new information becomes available, ensuring that the LLMs can provide the most accurate and up-to-date responses.</p>
<p class="calibre6">Incremental updates involve periodically re-embedding existing data sources with the latest information. This process can be automated to scan for updates in the data source, re-embed the new or updated content, and then integrate these refreshed embeddings into the existing vector database, without the need for a complete overhaul. By doing so, we ensure that the database reflects the most current knowledge available, enhancing the LLM’s ability to deliver relevant and nuanced responses.</p>
<p class="calibre6">Automated monitoring plays a pivotal role in this ecosystem by continually assessing the quality and relevance of the LLM’s outputs. This involves setting up systems that track the performance of the LLM, identifying areas where responses may be falling short due to outdated information or missing contexts. When such gaps are identified, the monitoring system can trigger an incremental update process, ensuring that the database remains a robust and accurate reflection of the current knowledge landscape.</p>
<p class="calibre6">By embracing these strategies, we ensure that LangChain and similar RAG frameworks can sustain their effectiveness over time. This approach not only enhances the relevance of LLM applications but also ensures that they can adapt to the rapidly evolving landscape of information, maintaining their position at the forefront of NLP technology.</p>
<p class="calibre6">W<a id="_idTextAnchor477" class="calibre5 pcalibre1 pcalibre"/>e can now get hands-on with LangChain.</p>
<h1 id="_idParaDest-196" class="calibre4"><a id="_idTextAnchor478" class="calibre5 pcalibre1 pcalibre"/>Reviewing a simple LangChain setup in a Jupyter notebook</h1>
<p class="calibre6">We are now ready to set up a <a id="_idIndexMarker855" class="calibre5 pcalibre1 pcalibre"/>complete pipeline that can later be lent to various NLP applications.</p>
<p class="calibre6">Refer to the <code>Ch8_Setting_Up_LangChain_Configurations_and_Pipeline.ipynb</code> notebook. This notebook implements the LangChain framework. We will walk through it step by step, e<a id="_idTextAnchor479" class="calibre5 pcalibre1 pcalibre"/>xplaining the different <a id="_idIndexMarker856" class="calibre5 pcalibre1 pcalibre"/>building blocks. We chose a simple use case here, as the main point of this code is to show how to set up a LangChain pipeline.</p>
<p class="calibre6">In this scenario, we are in the healthcare sector. We have many care givers; each has many patients they may see. The physician in chief made a request on behalf of all the physicians in the hospital to be able to use a smart search across their notes. They heard about the new emerging capabilities with LLMs, and they would like to have a tool where they can search within the medical reports they wrote.</p>
<p class="calibre6">For instance, one physician said the following:</p>
<p class="calibre6"> “<em class="italic">I often come across research that may be relevant to a patient I saw months ago, but I don’t recall who that was. I would like to have a tool where I can ask, ‘Who was that patient that complained about ear pain and had a family history of migraines?’, and it would find me </em><em class="italic">that patient.</em>”</p>
<p class="calibre6">Thus, the<a id="_idTextAnchor480" class="calibre5 pcalibre1 pcalibre"/> business objective here is as follows:</p>
<p class="calibre6">“<em class="italic">The CTO tasked us with putting together a quick prototype in the form of a Jupyter notebook. We will collect several clinical reports from the hospital’s database, and we will use LangChain to search through them in the manner that the physician in the </em><em class="italic">example described.”</em></p>
<p class="calibre6">Let’s jump right<a id="_idTextAnchor481" class="calibre5 pcalibre1 pcalibre"/> in by designing the solution in Python.</p>
<h2 id="_idParaDest-197" class="calibre7"><a id="_idTextAnchor482" class="calibre5 pcalibre1 pcalibre"/>Setting up a LangChain pipeline with Python</h2>
<p class="calibre6">Diving into the practicalities of LangChain, this <a id="_idIndexMarker857" class="calibre5 pcalibre1 pcalibre"/>section will guide you step by step in setting up a LangChain pipeline using Python, from installing the necessary libraries to exe<a id="_idTextAnchor483" class="calibre5 pcalibre1 pcalibre"/>cuting sophisticated similarity searches.</p>
<h3 class="calibre8">Installing the required Python libraries</h3>
<p class="calibre6">As always, we have a list of libraries<a id="_idIndexMarker858" class="calibre5 pcalibre1 pcalibre"/> that we will need to install. Since we are writing the code in a Jupyter notebook,<a id="_idTextAnchor484" class="calibre5 pcalibre1 pcalibre"/> we can install them from within the code:</p>
<ol class="calibre16">
<li class="calibre15"><strong class="bold">Load the text files with mock physician notes</strong>: Here, we put together some mock physician notes. We load them and process them per the LangChain paradigm. We stress that these aren’t real medical notes and that the people described there don’t exist.</li>
<li class="calibre15"><strong class="bold">Process the data so that it can be prepared for embedding</strong>: Here, we split the text per the requirements of the embedding model. As we mentioned in previous chapters, LMs, such as those used for embedding, have a finite window of input text that they can process in a single batch. That size is hardcoded in their design architectu<a id="_idTextAnchor485" class="calibre5 pcalibre1 pcalibre"/>re and is fixed for each particular model.</li>
<li class="calibre15"><strong class="bold">Create the embeddings that would be stored in the vector database</strong>: The vector database is one of the key pillars of the LangChain paradigm. Here, we take the text and create an embedding for each item. Those embeddings are then stored in a dedicated vector database. The LangChain library allows you to work with several different vector databases. While we chose one particular database, you can refer to the <strong class="bold">Vector Store</strong> page to read more about the different choices.</li>
<li class="calibre15"><strong class="bold">Create the vector database</strong>: Here, we create the vector database. This process may be slightly different for each database choice. However, the creators of these databases make sure to take away all of the hard work and leave you with a simple turnkey function that creates the database for you, given the appropriate embeddings in vector form. We leverage Meta’s <strong class="bold">Facebook AI Similarity Search</strong> (<strong class="bold">FAISS</strong>) database, as it<a id="_idIndexMarker859" class="calibre5 pcalibre1 pcalibre"/> is simple, quick to deploy, and free.</li>
<li class="calibre15"><strong class="bold">Perform a similarity search based on our in-house documents</strong>: This is the key part of the pipeline. We introduce several questions and use LangChain’s similarity search to identify the physician notes that would best answer our question.</li>
</ol>
<p class="calibre6">As we can see, the similarity search function is able to do a good job with most of the questions. It embeds the question and looks for reports whose embeddings are similar.</p>
<p class="calibre6">However, a similarity search could only go so far when it comes to answering the question correctly. It is easy<a id="_idIndexMarker860" class="calibre5 pcalibre1 pcalibre"/> to think of a question that discusses a matter that is very similar to one of the notes, yet a minor difference confuses the similarity search mechanism. For instance, the similarity search process actually makes a mistake in question two, mistaking different months and, thus, providing a wrong answer.</p>
<p class="calibre6">In order to overcome this matter, we would want to do more than just a similarity search. We would want an LLM to review the results of the similarity search and apply its judgment. We w<a id="_idTextAnchor486" class="calibre5 pcalibre1 pcalibre"/>ill see how that’s done in the next chapter.</p>
<p class="calibre6">With our foundation set for LangChain’s practical applications in Python, let’s now move on to understanding how the cloud plays a pivotal role, especially when harnessing the true potential of L<a id="_idTextAnchor487" class="calibre5 pcalibre1 pcalibre"/>LMs in contemporary computational paradigms.</p>
<h1 id="_idParaDest-198" class="calibre4"><a id="_idTextAnchor488" class="calibre5 pcalibre1 pcalibre"/>LLMs in the cloud</h1>
<p class="calibre6">In this era of big data and computation, cloud platforms have emerged as vital tools for managing large-scale computations, providing infrastructure, storage, and services that can be rapidly provisioned and released with minimal management effort.</p>
<p class="calibre6">This section will focus on computation environments in the cloud. These have become the dominant choice for many leading companies and institutions. As an organization, having a computation <a id="_idIndexMarker861" class="calibre5 pcalibre1 pcalibre"/>environment in the cloud versus on-premises makes a major difference. It impacts the ability to share resources and manage allocations, maintenance, and cost. There are many trade-offs for employing cloud services instead of owning physical machines. You can learn about them by searching online or even asking a chat LLM about them.</p>
<p class="calibre6">One significant difference with cloud computing is the ecosystem that the providers have built around it. When you pick a cloud provider as your computation hub, you tap into a whole suite of additional products and services, opening up a new world of capabilities that would not be as accessible to you otherwise.</p>
<p class="calibre6">In this section, we w<a id="_idTextAnchor489" class="calibre5 pcalibre1 pcalibre"/>ill focus on the LLM aspect of those services.</p>
<p class="calibre6">The three primary cloud platforms are AWS, Microsoft Azure, and GCP. These platforms offer a myriad of services, catering to the varying needs of businesses and developers. When it comes to NLP and LLMs, each platform provides dedicated resources and services to facilitate <a id="_idIndexMarker862" class="calibre5 pcalibre1 pcalibre"/>experimentation, deployment, and production.</p>
<p class="calibre6">Let’s explore each of these platfor<a id="_idTextAnchor490" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor491" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor492" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor493" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor494" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor495" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor496" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor497" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor498" class="calibre5 pcalibre1 pcalibre"/><a id="_idTextAnchor499" class="calibre5 pcalibre1 pcalibre"/>ms to see how they cater to our specific needs.</p>
<h2 id="_idParaDest-199" class="calibre7"><a id="_idTextAnchor500" class="calibre5 pcalibre1 pcalibre"/>AWS</h2>
<p class="calibre6">AWS remains a dominant force in the <a id="_idIndexMarker863" class="calibre5 pcalibre1 pcalibre"/>cloud computing landscape, providing a comprehensive and evolving suite<a id="_idIndexMarker864" class="calibre5 pcalibre1 pcalibre"/> of services that cater to the needs of ML and AI development. AWS is renowned for its robust infrastructure, extensive service offerings, and deep integration with ML tools and frameworks, making it a preferred platform for developers and data scientists looking to innovate with LLMs.</p>
<h3 class="calibre8">Experimenting with LLMs on AWS</h3>
<p class="calibre6">AWS provides a rich ecosystem of <a id="_idIndexMarker865" class="calibre5 pcalibre1 pcalibre"/>tools and services designed to facilitate the <a id="_idIndexMarker866" class="calibre5 pcalibre1 pcalibre"/>development and experimentation with LLMs, ensuring that researchers and developers have access to the most advanced ML capabilities:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Amazon SageMaker</strong>: The cornerstone of ML on AWS, SageMaker is a fully managed service that streamlines the entire ML<a id="_idIndexMarker867" class="calibre5 pcalibre1 pcalibre"/> workflow. It offers Jupyter notebook instances for experimentation, broad framework support, including TensorFlow and PyTorch, and a range of tools for model building, training, and debugging. SageMaker’s capabilities have been continually enhanced to support the complexities of training and fine-tuning LLMs, providing scalable compute options and optimized ML environments.</li>
<li class="calibre15"><strong class="bold">AWS Deep Learning Containers and Deep Learning AMIs</strong>: For those looking to customize their ML environments, AWS offers Deep Learning Containers and <strong class="bold">Amazon Machine Images</strong> (<strong class="bold">AMIs</strong>) pre-installed with popular ML frameworks. These resources <a id="_idIndexMarker868" class="calibre5 pcalibre1 pcalibre"/>simplify the setup process for LLM experiments, allowing developers to focus on innovation rather than infrastructure configuration.</li>
<li class="calibre15"><strong class="bold">Pre-trained models and SageMaker JumpStart</strong>: AWS has expanded its library of pre-trained models accessible through SageMaker JumpStart, facilitating quick <a id="_idIndexMarker869" class="calibre5 pcalibre1 pcalibre"/>experimentation with LLMs for a variety of <a id="_idIndexMarker870" class="calibre5 pcalibre1 pcalibre"/>NLP tasks. JumpStart also offers solution templates and executable example notebooks, making it easier for developers to start and scale their ML projects.</li>
</ul>
<h3 class="calibre8">Deploying and productionizing LLMs on AWS</h3>
<p class="calibre6">AWS provides a suite of services designed to<a id="_idIndexMarker871" class="calibre5 pcalibre1 pcalibre"/> efficiently deploy and manage LLMs at scale, ensuring that models are easily <a id="_idIndexMarker872" class="calibre5 pcalibre1 pcalibre"/>accessible and performant under varying loads:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">SageMaker endpoints</strong>: To deploy LLMs, SageMaker endpoints offer fully managed hosting services with<a id="_idIndexMarker873" class="calibre5 pcalibre1 pcalibre"/> auto-scaling capabilities. This service allows developers to deploy trained models into production quickly, with the infrastructure automatically adjusting to the demands of the application.</li>
<li class="calibre15"><strong class="bold">Elastic Inference and Amazon EC2 Inf1 instances</strong>: To optimize inference costs, AWS offers Elastic Inference, which adds GPU-powered inference acceleration to SageMaker instances. For even greater performance and cost efficiency, Amazon EC2 Inf1 instances, powered by AWS Inferentia chips, provide high-throughput and low-latency inference for DL models.</li>
<li class="calibre15"><strong class="bold">AWS Lambda and Amazon Bedrock</strong>: For serverless deployment, AWS Lambda supports running inference without provisioning or managing servers, ideal for applications with variable demand. Amazon Bedrock, represents a significant leap forward, offering serverless access to foundational models through APIs, model customization, and seamless integration within an organizational network, ensuring data privacy and security.</li>
</ul>
<p class="calibre6">Let’s move on to the next topic, Microsoft Aure.</p>
<h2 id="_idParaDest-200" class="calibre7"><a id="_idTextAnchor501" class="calibre5 pcalibre1 pcalibre"/>Microsoft Azure</h2>
<p class="calibre6">Microsoft Azure stands at the <a id="_idIndexMarker874" class="calibre5 pcalibre1 pcalibre"/>forefront of cloud computing services, offering a robust platform for the development, deployment, and management of ML and LLMs. Leveraging its strategic partnership with OpenAI, Azure provides exclusive cloud access<a id="_idIndexMarker875" class="calibre5 pcalibre1 pcalibre"/> to GPT models, positioning itself as a critical resource for developers and data scientists aiming to harness the power of advanced NLP technologies. Recent enhancements have expanded Azure’s capabilities, making it an even more attractive choice for those looking to push the boundaries of AI and ML applications.</p>
<h3 class="calibre8">Experimenting with LLMs on Azure</h3>
<p class="calibre6">Azure has significantly enriched its<a id="_idIndexMarker876" class="calibre5 pcalibre1 pcalibre"/> offerings to support research and experimentation with LLMs, providing a variety of tools and platforms that cater to the<a id="_idIndexMarker877" class="calibre5 pcalibre1 pcalibre"/> diverse needs of the <a id="_idIndexMarker878" class="calibre5 pcalibre1 pcalibre"/>AI development community:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Azure OpenAI Service</strong>: This directly<a id="_idIndexMarker879" class="calibre5 pcalibre1 pcalibre"/> integrates OpenAI’s cutting-edge models, including the latest GPT versions, DALL·E, and Codex, into the Azure ecosystem. This service enables developers to easily incorporate sophisticated AI functionalities into their applications, with the added benefits of Azure’s scalability and management tools.</li>
<li class="calibre15"><strong class="bold">Azure Machine Learning (Azure ML)</strong>: This offers<a id="_idIndexMarker880" class="calibre5 pcalibre1 pcalibre"/> an advanced environment for the custom training and fine-tuning of LLMs on specific datasets, allowing for enhanced model performance on niche tasks. Azure ML Studio’s pre-built and customizable Jupyter notebook templates support a wide range of programming languages and frameworks, facilitating a seamless experimentation process.</li>
<li class="calibre15"><strong class="bold">Azure Cognitive Services</strong>: This provides<a id="_idIndexMarker881" class="calibre5 pcalibre1 pcalibre"/> access to a suite of pre-built AI services, including text analytics, speech services, and decision-making capabilities powered by LLMs. These services enable developers to add complex AI functions to<a id="_idIndexMarker882" class="calibre5 pcalibre1 pcalibre"/> applications<a id="_idIndexMarker883" class="calibre5 pcalibre1 pcalibre"/> quickly, without deep ML expertise.</li>
</ul>
<h3 class="calibre8">Deploying and productionizing LLMs on Azure</h3>
<p class="calibre6">Azure’s infrastructure and services offer<a id="_idIndexMarker884" class="calibre5 pcalibre1 pcalibre"/> comprehensive solutions for the deployment and productionization of LLM applications, ensuring scalability, performance, and security:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Deployment options</strong>: Azure supports various deployment <a id="_idIndexMarker885" class="calibre5 pcalibre1 pcalibre"/>scenarios through <strong class="bold">Azure Container Instances</strong> (<strong class="bold">ACI</strong>) for lightweight deployment <a id="_idIndexMarker886" class="calibre5 pcalibre1 pcalibre"/>needs and <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>) for larger, more complex applications requiring high scalability. These services allow<a id="_idIndexMarker887" class="calibre5 pcalibre1 pcalibre"/> for the efficient scaling of LLM applications to meet user demand.</li>
<li class="calibre15"><strong class="bold">Model management</strong>: Through Azure ML, developers can manage the life cycle of their models, including version control, auditing, and governance. This ensures that deployed models are not only performant but also comply with industry standards and regulatory requirements.</li>
<li class="calibre15"><strong class="bold">Security and compliance</strong>: Azure emphasizes security and compliance across all its services, providing features such as data encryption, access control, and comprehensive compliance certifications. This commitment ensures that applications built and deployed on Azure meet the highest standards for data protection and privacy.</li>
</ul>
<h2 id="_idParaDest-201" class="calibre7"><a id="_idTextAnchor502" class="calibre5 pcalibre1 pcalibre"/>GCP</h2>
<p class="calibre6">GCP continues to be a powerhouse in <a id="_idIndexMarker888" class="calibre5 pcalibre1 pcalibre"/>cloud computing, providing an extensive suite of services that cater to the evolving needs of AI and ML development. Known for its cutting-edge innovations in AI and ML, GCP offers a rich ecosystem of tools and services that facilitate the development, deployment, and scaling of LLMs, making it an ideal platform for developers and researchers aiming to leverage the latest in AI technology.</p>
<h3 class="calibre8">Experimenting with LLMs on GCP</h3>
<p class="calibre6">GCP has further enhanced its capabilities for <a id="_idIndexMarker889" class="calibre5 pcalibre1 pcalibre"/>experimenting with and developing LLMs, offering<a id="_idIndexMarker890" class="calibre5 pcalibre1 pcalibre"/> a comprehensive set of tools that support the entire ML workflow, from data ingestion and model training to hyperparameter tuning and evaluation:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Vertex AI</strong>: At the heart of <a id="_idIndexMarker891" class="calibre5 pcalibre1 pcalibre"/>GCP’s ML offerings, Vertex AI provides an integrated suite of tools and services that streamline the ML workflow. It offers <a id="_idIndexMarker892" class="calibre5 pcalibre1 pcalibre"/>advanced features for training and fine-tuning LLMs, including AutoML capabilities for automating the selection of optimal model architectures and hyperparameters. Vertex AI’s integration with GCP’s robust data and analytics services makes it easier to manage large datasets that are essential for training LLMs.</li>
<li class="calibre15"><strong class="bold">An IDE</strong>: The built-in notebooks service within Vertex AI offers a fully managed JupyterLab environment, enabling developers to write, run, and debug ML code seamlessly. This environment is optimized for ML development, supporting popular frameworks such as TensorFlow and PyTorch, which are crucial for building and experimenting with LLMs.</li>
<li class="calibre15"><strong class="bold">AI and ML libraries</strong>: GCP continues to expand its library of pre-trained models and ML APIs, including those specifically designed for NLP and understanding. These tools allow developers to integrate advanced NLP capabilities into their applications rapidly.</li>
</ul>
<h3 class="calibre8">Deploying and productionizing LLMs on GCP</h3>
<p class="calibre6">GCP provides robust and scalable solutions for deploying and productionizing LLMs, ensuring that applications built on its platform can<a id="_idIndexMarker893" class="calibre5 pcalibre1 pcalibre"/> meet the demands of real-world usage:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="bold">Vertex AI prediction</strong>: Once an LLM<a id="_idIndexMarker894" class="calibre5 pcalibre1 pcalibre"/> is trained, Vertex AI’s prediction service allows for the easy deployment of models as fully managed, auto-scaling endpoints. This service simplifies the process of making your LLMs accessible to applications, with the infrastructure automatically adjusting to the workload demands.</li>
<li class="calibre15"><strong class="bold">Google Kubernetes Engine (GKE)</strong>: For more complex deployment scenarios requiring high availability and scalability, GKE offers a managed environment to deploy containerized LLM applications. GKE’s global infrastructure ensures that your models are highly available and can scale to meet the needs of<a id="_idIndexMarker895" class="calibre5 pcalibre1 pcalibre"/> enterprise-level applications.</li>
</ul>
<h2 id="_idParaDest-202" class="calibre7"><a id="_idTextAnchor503" class="calibre5 pcalibre1 pcalibre"/>Concluding cloud services</h2>
<p class="calibre6">The landscape of cloud computing <a id="_idIndexMarker896" class="calibre5 pcalibre1 pcalibre"/>continues to evolve rapidly, with AWS, Azure, and GCP each offering unique advantages for the development and deployment of LLMs. AWS stands out for its broad infrastructure and deep integration with ML tools, making it ideal for a wide range of ML and AI projects. Azure, with its exclusive access to OpenAI’s models and deep integration within the Microsoft ecosystem, offers unparalleled opportunities for enterprises looking to leverage the cutting edge of AI technology. GCP, recognized for its innovation in AI and ML, provides tools and services that mirror Google’s internal AI advancements, appealing to those seeking the latest in AI research and development. As the capabilities of these platforms continue to expand, the choice between them will increasingly depend on specific project needs, organizational alignment, and strategic partnerships, underscoring the importance of a thoughtfu<a id="_idTextAnchor504" class="calibre5 pcalibre1 pcalibre"/>l evaluation based on the current and future landscape of cloud-based AI and ML.</p>
<h1 id="_idParaDest-203" class="calibre4"><a id="_idTextAnchor505" class="calibre5 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre6">As the world of NLP and LLMs continues to grow rapidly, so do the various practices of system design. In this chapter, we reviewed the design process of LLM applications and pipelines. We discussed the components of these approaches, touching on both API-based closed source and local open source solutions. We then gave you hands-on experience with code.</p>
<p class="calibre6">We later delved deeper into the system design process and introduced LangChain. We reviewed what LangChain comprises and experimented with an example pipeline in code.</p>
<p class="calibre6">To complement the system design process, we surveyed leading cloud services that allow you to experiment, develop, and deploy LLM-based solutions.</p>
<p class="calibre6">In the next chapter, we’ll focus on particular practical use cases, accompanied with code.</p>
</div>
</body></html>