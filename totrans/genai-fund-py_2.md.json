["```py\n!pip install pytorch-fid torch diffusers clip transformers accelerate\n```", "```py\nfrom typing import List\nimport torch\nimport matplotlib.pyplot as plt\nfrom diffusers import StableDiffusionPipeline, DDPMScheduler\n```", "```py\ndef load_model(model_id: str) -> StableDiffusionPipeline:\n    \"\"\"Load model with provided model_id.\"\"\"\n    return StableDiffusionPipeline.from_pretrained(\n        model_id, \n        torch_dtype=torch.float16, \n        revision=\"fp16\", \n        use_auth_token=False\n    ).to(\"cuda\")\ndef generate_images(\n    pipe: StableDiffusionPipeline, \n    prompts: List[str]\n) -> torch.Tensor:\n    \"\"\"Generate images based on provided prompts.\"\"\"\n    with torch.autocast(\"cuda\"):\n        images = pipe(prompts).images\n    return images\ndef render_images(images: torch.Tensor):\n    \"\"\"Plot the generated images.\"\"\"\n    plt.figure(figsize=(10, 5))\n    for i, img in enumerate(images):\n        plt.subplot(1, 2, i + 1)\n        plt.imshow(img)\n        plt.axis(\"off\")\n    plt.show()\n```", "```py\n# Execution\nmodel_id = \"CompVis/stable-diffusion-v1-4\"\nprompts = [\n    \"A hyper-realistic photo of a friendly lion\",\n    \"A stylized oil painting of a NYC Brownstone\"\n]\npipe = load_model(model_id)\nimages = generate_images(pipe, prompts)\nrender_images(images)\n```", "```py\nfrom typing import List, Tuple\nfrom PIL import Image\nimport requests\nfrom transformers import CLIPProcessor, CLIPModel\nimport torch\n```", "```py\n# Constants\nCLIP_REPO = \"openai/clip-vit-base-patch32\"\ndef load_model_and_processor(\n    model_name: str\n) -> Tuple[CLIPModel, CLIPProcessor]:\n    \"\"\"\n    Loads the CLIP model and processor.\n    \"\"\"\n    model = CLIPModel.from_pretrained(model_name)\n    processor = CLIPProcessor.from_pretrained(model_name)\n    return model, processor\n```", "```py\ndef process_inputs(\n    processor: CLIPProcessor, prompts: List[str],\n    images: List[Image.Image]) -> dict:\n\"\"\"\nProcesses the inputs using the CLIP processor.\n\"\"\"\n    return processor(text=prompts, images=images,\n        return_tensors=\"pt\", padding=True)\n```", "```py\ndef get_probabilities(\n    model: CLIPModel, inputs: dict) -> torch.Tensor:\n\"\"\"\nComputes the probabilities using the CLIP model.\n\"\"\"\n    outputs = model(**inputs)\n    logits = outputs.logits_per_image\n    # Define temperature - higher temperature will make the distribution more uniform.\n    T = 10\n    # Apply temperature to the logits\n    temp_adjusted_logits = logits / T\n    probs = torch.nn.functional.softmax(\n        temp_adjusted_logits, dim=1)\n    return probs\n```", "```py\ndef display_images_with_scores(\n    images: List[Image.Image], scores: torch.Tensor) -> None:\n\"\"\"\nDisplays the images alongside their scores.\n\"\"\"\n    # Set print options for readability\n    torch.set_printoptions(precision=2, sci_mode=False)\n    for i, image in enumerate(images):\n        print(f\"Image {i + 1}:\")\n        display(image)\n        print(f\"Scores: {scores[i, :]}\")\n        print()\n```", "```py\n# Load CLIP model\nmodel, processor = load_model_and_processor(CLIP_REPO)\n# Process image and text inputs together\ninputs = process_inputs(processor, prompts, images)\n# Extract the probabilities\nprobs = get_probabilities(model, inputs)\n# Display each image with corresponding scores\ndisplay_images_with_scores(images, probs)\n```"]