- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: 'NLP Evolution and Transformers: Exploring NLPs and LLMs'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP演变和Transformer：探索NLPs和LLMs
- en: In the previous introductory chapter, you gained a fundamental understanding
    of generative AI, including a primer on the growing complexity of generative AI
    applications, along with a brief introduction to cloud computing for scalability
    and cost-effectiveness and the key components of data storage, security, and collaboration.
    You also learned one of the more exciting aspects of generative AI, which can
    also be a hurdle, which is how to stay up to date with cutting-edge AI technologies
    such as GenAI.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的介绍性章节中，您对生成式AI有了基本了解，包括对生成式AI应用日益增长的复杂性的入门介绍，以及关于云计算的可扩展性和成本效益的简要介绍，以及数据存储、安全和协作的关键组件。您还了解到了生成式AI的一个更令人兴奋的方面，它也可能是一个障碍，那就是如何跟上像GenAI这样的尖端AI技术的最新发展。
- en: In this chapter, we will explore the capabilities of ChatGPT, specifically with
    regard to its conversation input and response abilities. We will delve deeper
    into how LLMs are able to understand and respond to user queries and learn and
    adapt to new information. The information provided will be useful for individuals
    who are looking to understand more about how AI assistants, such as ChatGPT, work
    and how they can be utilized to help people find information more efficiently
    and effectively; subsequently, we will be expanding on this topic in relation
    to the NLP and prompt engineering topics discussed in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).
    By the end of this chapter, we hope you will get a deeper understanding of the
    progression of NLP and generative AI techniques by exploring the capabilities
    of various text-based tasks for prompts and responses, along with conversational
    flows and integration.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨ChatGPT的能力，特别是关于其对话输入和响应能力。我们将深入探讨LLMs如何理解和响应用户查询，以及如何学习和适应新信息。提供的信息将有助于那些希望了解更多关于AI助手（如ChatGPT）如何工作以及如何利用它们帮助人们更高效、更有效地找到信息的人；随后，我们将在此基础上扩展到第[*第5章*](B21443_05.xhtml#_idTextAnchor098)中讨论的NLP和提示工程主题。在本章结束时，我们希望您通过探索各种基于文本的任务的能力，包括提示和响应、对话流程和集成，对NLP和生成式AI技术的进展有更深入的理解。
- en: 'We will cover the following main topics in the chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主要内容：
- en: NLP evolution and the rise of transformers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP演变和Transformer的兴起
- en: Conversation prompts and completions – under the covers
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对话提示和完成 – 内部机制
- en: LLMs landscape, progression, and expansion
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs的景观、进展和扩展
- en: '![Figure 2.1 – How profound transformers have become](img/B21443_02_1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – Transformer变得多么深刻](img/B21443_02_1.jpg)'
- en: Figure 2.1 – How profound transformers have become
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – Transformer变得多么深刻
- en: NLP evolution and the rise of transformers
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NLP演变和Transformer的兴起
- en: 'NLP, or natural language processing, is the field of artificial intelligence
    that gives computers the ability to understand and manipulate human language using
    common spoken (or otherwise) language instead of what was traditionally given
    as input to computers in the past: computer programming language. Over the past
    several decades, these computer programming languages became more “natural” with
    fluency:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: NLP，或自然语言处理，是人工智能领域的一个分支，它使计算机能够使用常见的口语（或其他）语言来理解和操作人类语言，而不是像过去传统上给计算机作为输入的那样：计算机编程语言。在过去的几十年里，这些计算机编程语言变得更加“自然”，更加流畅：
- en: '![Figure 2.2 – Brief timeline of NLP evolution](img/B21443_02_2.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图2.2 – NLP演变的简要时间线](img/B21443_02_2.jpg)'
- en: Figure 2.2 – Brief timeline of NLP evolution
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – NLP演变的简要时间线
- en: Over time, there has been significant advancement in the field of NLP, with
    computers increasingly improving in their ability for text generation due to the
    emergence of neural networks. Text generation itself isn’t a novel idea, but earlier
    language models before 2017 predominantly utilized ML architectures known as **recurrent
    neural networks** (**RNNs**) and **convolutional neural** **networks** (**CNNs**).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，在自然语言处理（NLP）领域取得了显著的进步，由于神经网络的出现，计算机在文本生成方面的能力不断提高。文本生成本身并不是一个新颖的想法，但2017年之前的早期语言模型主要利用了被称为**循环神经网络**（**RNNs**）和**卷积神经网络**（**CNNs**）的机器学习架构。
- en: RNNs are a type of neural network architecture that excels at processing sequence
    data. They process input in a sequential manner, carrying information from one
    step in the sequence to the next. This makes them quite useful for tasks such
    as text generation, translation, and sentiment analysis.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: RNN是一种擅长处理序列数据的神经网络架构。它们以顺序方式处理输入，将信息从一个序列步骤传递到下一个步骤。这使得它们在文本生成、翻译和情感分析等任务中非常有用。
- en: A CNN is a type of deep learning architecture designed to process and analyze
    visual data, such as images and videos, by using specialized layers called convolutional
    layers. These layers apply filters to extract relevant features from the input
    data, capturing patterns and hierarchies of information. CNNs are primarily used
    for tasks such as image classification, object detection, and image segmentation
    in computer vision. In **natural language processing** (**NLP**), CNNs can also
    be applied to tasks such as text classification and sentiment analysis, where
    the input text is transformed into a matrix-like structure to capture local patterns
    and relationships among words or characters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CNN是一种深度学习架构，它通过使用称为卷积层的专用层来处理和分析视觉数据，例如图像和视频。这些层应用过滤器从输入数据中提取相关特征，捕捉信息的模式和层次结构。CNN主要用于计算机视觉中的图像分类、目标检测和图像分割等任务。在**自然语言处理**（**NLP**）中，CNN也可以应用于文本分类和情感分析等任务，其中输入文本被转换为类似矩阵的结构，以捕捉词语或字符之间的局部模式和关系。
- en: The main drawbacks of RNNs and CNNs
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RNN和CNN的主要缺点
- en: Despite the sophistication of RNNs, their potential could not be fully harnessed
    due to certain constraints. RNNs often struggle with the “Vanishing gradient problem”
    during training, which hampers their ability to learn from long sequences and
    retain long-term dependencies.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RNN很复杂，但由于某些限制，它们的潜力无法完全发挥。RNN在训练过程中经常遇到“梯度消失问题”，这阻碍了它们从长序列中学习和保留长期依赖关系的能力。
- en: Additionally, the inherently “sequential processing” of RNNs does not allow
    for efficient parallelization, significantly slowing down training in an age where
    GPU-based parallel processing is standard for deep learning models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RNN固有的“顺序处理”不允许进行有效的并行化，这在深度学习模型中基于GPU的并行处理成为标准的时代，显著减慢了训练速度。
- en: RNNs were, thus, limited in computing and memory. In order to predict the next
    word in a sentence, the models need to know more than the previous few words;
    they also need to understand the context of the word in a sentence, paragraph,
    or whole document.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RNN在计算和内存方面有限。为了预测句子中的下一个词，模型需要知道不仅仅是前面的几个词；它们还需要理解句子、段落或整个文档中该词的上下文。
- en: Let’s explain this concept with an example by using the sentence
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用以下句子来解释这个概念
- en: “*The water in the ocean has a lot of salt, it’s a bit choppy and it* *tastes*
    *sweet**.*”
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: “*海洋中的水含有大量的盐，有点波涛汹涌，而且* *尝起来* *很甜**。””
- en: In the preceding sentence, the RNN might generate a continuation, indicating
    the water is sweet instead of salty. The reason for this is because of only taking
    the last few words into consideration and not the context of the whole sentence.
    The RNN will have forgotten the context from earlier in the text that might indicate
    the taste of the water from the ocean.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的句子中，RNN可能会生成一个延续，表明水是甜的而不是咸的。原因是它只考虑了最后几个词，而没有考虑整个句子的上下文。RNN会忘记文本中较早的部分，这些部分可能表明来自海洋的水的味道。
- en: Similarly, CNNs have revolutionized image analysis by automatically learning
    hierarchical features through layers of convolutions. Despite their success, CNNs
    are limited in that they have a fixed receptive field size and operate in a local
    context. This limitation makes it challenging for them to capture global dependencies
    and the relationships present in sequences of varying lengths. For instance, in
    image classification, while CNNs excel at recognizing local patterns, they struggle
    to grasp the overall context of an image, hindering their ability to understand
    complex relationships between objects or regions. Consider an image of a cat chasing
    a mouse with a dog watching in the background. A CNN might effectively identify
    the cat, mouse, and dog based on their local features. However, understanding
    the intricate relationships, e.g., the cat is chasing the mouse, and the dog is
    a passive observer, may be challenging for a CNN.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，卷积神经网络通过自动通过卷积层学习层次特征，已经彻底改变了图像分析。尽管它们取得了成功，但CNNs在固定感受野大小和局部上下文中操作的限制，使得它们难以捕捉全局依赖关系和不同长度序列中存在的关联。例如，在图像分类中，虽然CNNs在识别局部模式方面表现出色，但它们难以把握图像的整体上下文，这阻碍了它们理解物体或区域之间复杂关系的能力。考虑一张猫追逐老鼠，狗在背景中观看的图片。CNN可能会根据它们的局部特征有效地识别猫、老鼠和狗。然而，理解复杂的关联，例如猫在追逐老鼠，狗是一个被动的观察者，可能对CNN来说是一个挑战。
- en: So, how did we finally overcome the challenges of CNNs? It was done by using
    a concept known as transformer model architecture and its “self-attention mechanism,”
    which is described in the next section. This would not only identify the individual
    animals but also capture the contextual interactions, such as the chase sequence
    and the dog’s passive stance.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们是如何最终克服卷积神经网络（CNNs）的挑战的呢？这是通过使用一个被称为变换器模型架构及其“自注意力机制”的概念来实现的，这些内容将在下一节中描述。这不仅能够识别单个动物，还能捕捉到上下文交互，例如追逐序列和狗的被动姿态。
- en: However, before we really peel back the layers on how transformers work, the
    following is a reference timeline about the strengths of NLP coupled with LLMs.
    Once you realize the benefits and the “why,” we can then dive into the “how.”
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们真正揭开变换器工作原理的层之前，以下是一个关于NLP结合LLMs优势的参考时间线。一旦你意识到好处和“为什么”，我们就可以深入探讨“如何”。
- en: NLP and the strengths of generative AI in LLMs
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP和LLMs中生成式AI的优势
- en: 'This section provides a broad overview of NLP with LLMs before the next section,
    where we explain more about transformers: the powerful engine behind LLMs.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节在下一节解释更多关于变换器的内容之前，提供了一个关于NLP与LLMs中生成式AI优势的概述：变换器是LLMs背后的强大引擎。
- en: '**Large language models** (**LLMs**) are incredibly potent language models
    that are transforming our comprehension and creation of human language. But what’s
    their connection to NLP? It’s rather fundamental. NLP lays out the structure and
    goals for interpreting and generating human language, whereas LLMs serve as sophisticated
    tools that facilitate the realization of these goals on a grand scale, handling
    intricate tasks with remarkable precision.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**大型语言模型**（**LLMs**）是极其强大的语言模型，正在改变我们对人类语言的理解和创造。但它们与自然语言处理（NLP）有什么联系呢？这是非常基础的。NLP为解释和生成人类语言的结构和目标制定了框架，而LLMs则作为复杂的工具，在广泛规模上实现这些目标，以惊人的精确度处理复杂任务。'
- en: As mentioned earlier, NLP is a branch of machine learning that enables computers
    to understand, process, and generate human language. It combines computer science
    and linguistics. For example, there is a massive amount of audio and text data
    generated by organizations from various communication channels. These data can
    be processed by NLP models to automatically process data, determine sentiments,
    summarize, and find answers, key topics, or even respond effectively.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，自然语言处理（NLP）是机器学习的一个分支，它使计算机能够理解、处理和生成人类语言。它结合了计算机科学和语言学。例如，来自各种通信渠道的组织产生了大量的音频和文本数据。这些数据可以通过NLP模型进行处理，以自动处理数据、确定情感、总结，甚至找到关键主题或有效回应。
- en: As a quick, simple example, the audio data generated by call centers can be
    converted to text and processed by NLP models to determine both the issue the
    customer is facing and also the sentiment of the customer (whether they are happy,
    upset, nonchalant, and so on).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为快速简单的例子，呼叫中心产生的音频数据可以被转换成文本，并通过NLP模型进行处理，以确定客户面临的问题以及客户的情感（无论是高兴、沮丧、冷漠等等）。
- en: Important note
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: NLP is the technology behind search engines, such as Bing and Google, voice
    assistants, such as Alexa and Siri, and powerful conversational agents, such as
    ChatGPT.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）是搜索引擎（如Bing和Google）、语音助手（如Alexa和Siri）以及强大的对话代理（如ChatGPT）背后的技术。
- en: From this, it would appear that NLP technology should address all of our needs.
    So why should we have to deal with LLMs and GenAI at all?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，似乎自然语言处理技术应该满足我们的所有需求。那么，为什么我们还要处理LLMs和生成式AI呢？
- en: By taking a step back briefly and looking at the preceding evolution timeline,
    the inception of **advanced NLP** can be traced back to 2013 with the advent of
    word2vec, a model introduced by Google that transformed words into dense vectors
    based on contextual relationships. A vector is defined as an object that has both
    a magnitude and a direction and is represented in a numerical array format.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简要回顾先前的演变时间线，我们可以将**高级NLP**的诞生追溯到2013年，当时Google推出了word2vec模型，该模型基于上下文关系将单词转换成密集向量。向量被定义为具有大小和方向的物体，并以数值数组格式表示。
- en: This was revolutionary, as it captured semantic nuances that older models couldn’t
    grasp. However, they couldn’t focus on different parts of the text to form a larger
    understanding. For example, various words in a sentence, or multiple sentences,
    could not be related to one another for a full understanding of a sentence or
    paragraph. This limitation was tackled by attention mechanisms, which were introduced
    in the 2017 paper *Attention Is All You Need*. These mechanisms led to the transformer
    architecture, the backbone of the foundational LLM models we see today, which
    allowed models to form an understanding of text beyond just words and sentences.
    There will be more on this a bit later, but first, let’s cover why we want to
    use LLMs and look at some areas where LLMs can enhance NLP.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项革命性的进步，因为它捕捉到了旧模型无法理解的语义细微差别。然而，它们无法专注于文本的不同部分以形成更全面的理解。例如，句子中的各种单词或多个句子不能相互关联，以全面理解句子或段落。这种局限性通过2017年论文《Attention
    Is All You Need》中引入的注意力机制得到了解决。这些机制导致了Transformer架构的出现，这是今天我们看到的基础LLM模型的骨干，它使模型能够形成对文本的理解，而不仅仅是单词和句子。关于这一点，我们稍后会详细讨论，但首先，让我们来看看为什么我们要使用LLMs，以及LLMs可以增强NLP的哪些领域。
- en: 'NLP plus LLMs equals expanded possibilities:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: NLP加上LLMs等于扩展的可能性：
- en: '**Understanding language**: LLMs are adept at comprehending and processing
    a vast array of language inputs, making them useful for a variety of linguistic
    tasks. LLMs can be used to build advanced chatbots and virtual assistants. They
    can understand and respond to customer inquiries, provide information, and execute
    tasks, improving the efficiency and quality of customer service.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解语言**：LLMs擅长理解和处理各种语言输入，使它们在多种语言任务中非常有用。LLMs可用于构建高级聊天机器人和虚拟助手。它们可以理解并回应客户咨询，提供信息并执行任务，提高客户服务的效率和品质。'
- en: '**Text generation**: LLMs can generate coherent and contextually appropriate
    text, enabling applications such as chatbots, content creation, copywriting, and
    more.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本生成**：LLMs可以生成连贯且上下文适当的文本，使聊天机器人、内容创作、文案撰写等应用成为可能。'
- en: LLMs can enhance efficiency in internal and external communications by recommending
    and suggesting words or reviewing your content.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以通过推荐和建议单词或审查你的内容来提高内部和外部沟通的效率。
- en: '**Language translation**: LLMs can directly translate text between different
    languages, aiding cross-cultural communication and language learning.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言翻译**：LLMs可以直接在不同语言之间翻译文本，促进跨文化交流和语言学习。'
- en: As LLMs can provide translation between multiple languages, this can help businesses
    operate more efficiently in a globalized world by breaking down language barriers.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于LLMs可以提供多种语言之间的翻译，这可以帮助企业在全球化世界中更有效地运营，打破语言障碍。
- en: '**Sentiment analysis**: LLMs can analyze text to determine its sentiment (positive,
    negative, or neutral), providing valuable insights for applications such as customer
    feedback analysis. LLMs can analyze customer feedback, reviews, or social media
    posts to assess public sentiment toward a brand, product, or service. This can
    help with business strategies and decision-making processes.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：LLMs可以分析文本以确定其情感（正面、负面或中性），为如客户反馈分析等应用提供有价值的见解。LLMs可以分析客户反馈、评论或社交媒体帖子，以评估公众对一个品牌、产品或服务的看法。这有助于商业策略和决策过程。'
- en: '**Question answering**: LLMs can understand and provide accurate answers to
    a wide range of questions, making it possible to build an organization-specific
    enterprise search engine.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**：LLMs可以理解和提供对广泛问题的准确答案，这使得构建特定组织的搜索引擎成为可能。'
- en: '**Text summarization**: LLMs can condense long pieces of text into shorter
    summaries, aiding in information processing and comprehension. LLMs can summarize
    long documents, articles, or reports, making it easier to digest large amounts
    of information quickly while identifying key areas or the next steps.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要**：LLMs可以将长篇文本压缩成更短的摘要，有助于信息处理和理解。LLMs可以总结长文档、文章或报告，使得快速消化大量信息并识别关键区域或下一步行动变得更加容易。'
- en: '**Adaptability**: LLMs can generate text in various styles, tones, or formats,
    adapting to specific user needs or application requirements. For example, you
    can ask ChatGPT to define and describe Photosynthesis in plants for your 6-year-old
    child in the style of a pirate. In relation to this, by using data about user
    behavior and preferences, LLMs can generate personalized content or product recommendations,
    thus improving user experience and potentially increasing sales for retail businesses.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适应性**：LLMs可以以各种风格、语气或格式生成文本，适应特定的用户需求或应用要求。例如，你可以要求ChatGPT以海盗风格为你的6岁孩子定义并描述植物的光合作用。与此相关的是，通过使用关于用户行为和偏好的数据，LLMs可以生成个性化的内容或产品推荐，从而改善用户体验并可能增加零售企业的销售额。'
- en: '**Context maintenance**: Although they only have short-term memory, LLMs can
    maintain conversational context over extended interactions with the right prompt
    engineering techniques, improving the coherence and relevance of their responses.
    We will cover prompt engineering techniques in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098)
    of this book.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**上下文维护**：尽管它们只有短期记忆，但通过适当的提示工程技巧，LLMs可以在扩展交互中维持对话上下文，提高其回答的连贯性和相关性。我们将在本书的[*第5章*](B21443_05.xhtml#_idTextAnchor098)中介绍提示工程技巧。'
- en: '**Creativity**: LLMs can generate novel text, opening more possibilities for
    creative applications such as story generation or poetry creation. From writing
    articles, reports, and marketing copy to generating creative content, LLMs can
    automate and enhance various content creation tasks.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创造力**：大型语言模型（LLMs）可以生成新颖的文本，为故事生成或诗歌创作等创意应用提供了更多可能性。从撰写文章、报告和营销文案到生成创意内容，LLMs可以自动化并增强各种内容创作任务。'
- en: 'Here, we have listed a few of the areas where large language models have enhanced
    the functionality of natural language processing. Now that you can appreciate
    the fact LLMs can provide enhancements to any NLP services, and also to our everyday
    lives, let’s take the next step: a deeper dive into transformers and the attention
    mechanism, which gives LLMs their power to run generative AI.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们列出了一些大型语言模型增强自然语言处理功能性的领域。既然你已经认识到LLMs可以为任何NLP服务以及我们的日常生活提供增强，那么让我们继续下一步：深入探讨Transformer和注意力机制，这是LLMs运行生成式AI能力的来源。
- en: How do transformers work?
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变压器是如何工作的？
- en: The introduction of transformer architecture addresses the preceding shortcomings
    of RNNs and CNNs. Transformers use an attention mechanism, which allows the model
    to focus on different parts of the input when generating each word in the output.
    Simply put, the attention mechanism measures how words interrelate in a sentence,
    paragraph, or section. For LLMs, the underlying transformer is a set of deep learning
    neural networks that consist of an encoder component and a decoder component that
    exist within the concept of self-attention capability. During self-attention,
    an LLM will assign weights to different words based on their relevance to the
    current word being processed, and this is what gives the model its power. This
    attention mechanism dynamically enables LLMs to focus on critical contextual information
    while also disregarding nonrelevant items/words at the same time. In other words,
    the encoder and decoder components extract meanings from a sequence of text and
    understand the relationships between the words and phrases in it.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构的引入解决了 RNN 和 CNN 的先前不足。Transformers 使用注意力机制，这使得模型在生成输出中的每个单词时能够关注输入的不同部分。简单来说，注意力机制衡量句子、段落或部分中单词之间的关系。对于
    LLMs，底层的 Transformer 是一组包含编码器组件和解码器组件的深度学习神经网络，这些组件存在于自注意力能力的概念中。在自注意力过程中，LLM
    会根据当前正在处理的单词的相关性为不同的单词分配权重，这就是模型获得其力量的原因。这种注意力机制动态地使 LLMs 能够关注关键上下文信息，同时忽略不相关的项目/单词。换句话说，编码器和解码器组件从一系列文本中提取意义，并理解其中单词和短语之间的关系。
- en: This allows transformers to maintain a better sense of long-term **context**
    compared to RNNs and CNNs. Positional encodings allow for the handling of sequence
    order and transformers allow for the **parallel processing** of sequences, making
    LLMs much faster to train compared to RNNs. The foundational models underpinning
    ChatGPT, known as GPT models, employ this transformer architecture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 Transformer 相比于 RNN 和 CNN，能够更好地保持长期 **上下文** 的感觉。位置编码允许处理序列顺序，而 Transformer
    允许序列的 **并行处理**，这使得 LLMs 的训练速度比 RNNs 快得多。支撑 ChatGPT 的基础模型，即 GPT 模型，采用了这种 Transformer
    架构。
- en: 'When first introduced, the transformer architecture was originally designed
    for translation and is described in the now famous publication by Google: *Attention
    is All You Need* (see [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    for a deeper look). From this publication, we show the original transformer architecture
    in the following image, and we have added the encoder on the left and the decoder
    on the right for your high-level understanding:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当首次引入时，Transformer 架构最初是为翻译设计的，并在谷歌现在著名的出版物《*Attention is All You Need*》中有描述（参见[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)以深入了解）。从这篇出版物中，我们展示了以下图像中的原始
    Transformer 架构，并在左侧添加了编码器，在右侧添加了解码器，以便您从高层次理解：
- en: '![Figure 2.3 – Transformer model architecture](img/B21443_02_3.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – Transformer 模型架构](img/B21443_02_3.jpg)'
- en: Figure 2.3 – Transformer model architecture
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – Transformer 模型架构
- en: While the preceding image can be daunting to some, especially to beginners in
    the field of generative AI, you do not necessarily need to have a firm understanding
    of each subcomponent of the transformer model architecture in the same way that
    most people do not need to know the internal workings of an automobile engine
    in order to drive a car. We will only cover the main input and outputs of the
    transformer architecture, and there is a simplified view later in this chapter
    to describe some of the inner workings and flow. We will continue to emphasize
    and repeat various aspects of the transformer model, as this can be a difficult
    concept to grasp, especially for those new to generative AI and LLMs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的图像可能对一些人来说令人畏惧，尤其是对生成 AI 领域的新手来说，您并不一定需要像大多数人不需要了解汽车引擎的内部工作原理一样，对 Transformer
    模型架构的每个子组件有深入的了解。我们只会涵盖 Transformer 架构的主要输入和输出，在本章的后面有一个简化的视图来描述一些内部工作原理和流程。我们将继续强调和重复
    Transformer 模型的各个方面，因为这可能是一个难以理解的概念，尤其是对于初学者和 LLMs。
- en: From the original purpose of language translations in 2017, the transformer
    model architecture became the underpinning framework for future generative AI
    models, leading to the emergence of ChatGPT; the letter **T** in GPT stands for
    **transformer** (GPT).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2017 年语言翻译的原始目的出发，Transformer 模型架构成为了未来生成式 AI 模型的底层框架，导致了 ChatGPT 的出现；GPT
    中的字母 **T** 代表 **Transformer**（GPT）。
- en: Benefits of transformers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转换器的优势
- en: As mentioned earlier, transformers are a type of neural network architecture
    that replaces traditional RNNs and CNNs with an entirely attention-based mechanism.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，转换器是一种神经网络架构，它用完全基于注意力的机制取代了传统的RNN和CNN。
- en: But how does the attention mechanism work?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但注意力机制是如何工作的呢？
- en: Attention does this by calculating “soft” weights for each word in the context
    window and doing this in parallel in the transformer model vs. sequentially in
    the RNN/CNN models. These “soft” weights can, and often do, change during the
    runtime of the model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制通过计算上下文窗口中每个词的“软”权重来实现这一点，在转换器模型中是并行进行的，而在RNN/CNN模型中是顺序进行的。这些“软”权重在模型运行时可以，并且经常发生变化。
- en: 'The benefits of transformers are the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器的优势如下：
- en: They scale efficiently to use multi-core GPUs and parallel processing training
    data; hence, they can make use of much, much larger datasets.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以高效地扩展以使用多核GPU和并行处理训练数据；因此，它们可以利用大得多的数据集。
- en: They pay attention to the meaning of input.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它关注输入的意义。
- en: They learn the relevance of every word and their context in a sentence/paragraph,
    not just the neighboring words, as with RNNs and CNNs.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们学习每个词在句子/段落中的相关性和上下文，而不仅仅是像RNN和CNN那样学习邻近的词。
- en: 'Let’s take a look at a visual representation of how the words of the sentence,
    “*The musician taught the student with the piano*,” relate to one another from
    the perspective of a transformer:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从转换器的角度，句子“*The musician taught the student with the piano*”中的词是如何相互关联的视觉表示：
- en: '![Figure 2.4 – Sentence context relationships](img/B21443_02_4.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4 – 句子上下文关系](img/B21443_02_4.jpg)'
- en: Figure 2.4 – Sentence context relationships
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 句子上下文关系
- en: As stated in the preceding example, transformers are able to link every word,
    determine the relationships between every word in the input (even if they are
    immediately preceding or succeeding word(s)), and understand the context of the
    word in a sentence. In the preceding image, the colored lines represent stronger
    relationships.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所述，转换器能够连接每个词，确定输入中每个词之间的关系（即使它们是紧邻的词），并理解句子中词的上下文。在前面的图像中，彩色线条代表更强的关系。
- en: Thus, transformers use modern mathematical techniques, such as attention or
    self-attention, to determine the inter-relationships and dependencies among data
    elements, even when they are far apart. This gives the model the ability to learn
    who taught the student and with what instrument, etc.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，转换器使用现代数学技术，如注意力和自注意力，来确定数据元素之间的相互关系和依赖性，即使它们相隔很远。这使得模型能够学习谁教了学生以及用什么乐器等。
- en: There are multiple layers in the transformer deep learning architecture, such
    as the embedding layer, self-attention, and multi-headed attention, as well as
    the multiple encoder models themselves. While a detailed understanding of the
    transformer architecture isn’t essential for successful prompt engineering or
    understanding generative AI, having a foundational grasp of the transformer model,
    a critical aspect of LLM’s and ChatGPT’s underlying architecture, is important
    for any cloud solution design.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器深度学习架构中有多个层，如嵌入层、自注意力和多头注意力，以及多个编码器模型本身。虽然对转换器架构的深入了解对于成功的提示工程或理解生成式AI不是必需的，但了解转换器模型，这是LLMs和ChatGPT底层架构的关键方面，对于任何云解决方案设计都是重要的。
- en: As we have talked about benefits, let’s also mention a negative aspect of transformers;
    they can sometimes produce a by-product that also affects LLMs, which we briefly
    mentioned in the first chapter but will again mention here as we are discussing
    transformers and that is the concept of “hallucinations.” A hallucination is basically
    incorrect information returned by an LLM model. This hallucination is response
    output, which is inconsistent with the prompt and is often due to a few reasons,
    such as the actual training data used to train the LLM model itself being incomplete
    or spurious. We wanted to mention it here, but we will discuss hallucinations
    in later chapters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了优势，让我们也提一下转换器的一个负面方面；它们有时会产生副产品，这也会影响LLMs，我们在第一章中简要提到了这一点，但在这里再次提及，因为我们正在讨论转换器，那就是“幻觉”的概念。幻觉基本上是LLM模型返回的错误信息。这种幻觉是响应输出，与提示不一致，通常是由于几个原因，例如用于训练LLM模型的实际训练数据本身不完整或不准确。我们想在这里提一下，但将在后面的章节中讨论幻觉。
- en: For now, let’s dive into the inner workings of transformer architecture and
    explore the transformer concept a bit more with some examples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨transformer架构的内部工作原理，并通过一些示例更深入地探索transformer概念。
- en: Conversation prompts and completions – under the covers
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对话提示和完成 - 内部机制
- en: Prompts, or the input entered by you or an application/service, play a crucial
    role in NLP + LLMs by facilitating the interaction between humans and language
    models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 提示，或者你或应用程序/服务输入的内容，在NLP + LLMs中起着至关重要的作用，因为它促进了人类与语言模型之间的交互。
- en: If you have had any experience with GenAI, you may have already entered a prompt
    into an online service such as chat.bing.com. A prompt is to an LLM what a search
    term is to a web search engine, but each can take a prompt input and run some
    action(s) against such input. Just like you would intelligently enter search terms
    into a search engine to find the content you are looking for, the same can be
    said about entering prompts intelligently. This concept is known as prompt engineering,
    and we devote an entire chapter to prompt engineering later in this book, which
    will describe the “how” of writing an effective prompt to get the results you
    need.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经有过与GenAI相关的经验，你可能已经在一个在线服务中输入过提示，例如chat.bing.com。提示对于LLM来说，就像搜索词对于搜索引擎一样，但每个都可以接受提示输入并针对该输入执行一些操作。就像你会在搜索引擎中智能地输入搜索词来找到你想要的内容一样，同样，智能地输入提示也是一样的。这个概念被称为提示工程，我们将在本书后面的章节中专门讨论提示工程，其中将描述如何编写一个有效的提示以获得所需的结果。
- en: 'Some of you who are newer to the generative AI space might wonder why we need
    to understand how to write a prompt at all. Let’s provide a simple analogy: if
    you think of a **database administrator** (**DBA**) who needs to pull (query)
    specific data from a vast database with many tables (say, a typical customer sales
    database) in order to understand the trends and forecasting of sales to ensure
    there is enough product, you have to analyze the historical data. However, if
    the DBA cannot put together a proper query to build a report of past sales history,
    any forecasting and future trends will be completely incorrect.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些刚接触生成式AI领域的人来说，可能会 wonder为什么我们需要了解如何编写提示。让我们提供一个简单的类比：如果你想到一个**数据库管理员**（DBA），他需要从包含许多表（例如，典型的客户销售数据库）的大量数据库中提取（查询）特定数据，以便了解销售趋势和预测，确保有足够的产品，你必须分析历史数据。然而，如果DBA无法构建一个合适的查询来构建过去销售历史的报告，任何预测和未来趋势都将完全错误。
- en: Similarly, a poorly constructed prompt is like using a dull knife, you’re unlikely
    to get great results. Thus, prompt engineering is crucial to generate useful responses.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，一个构建不良的提示就像使用一把钝刀，你不太可能得到好的结果。因此，提示工程对于生成有用的响应至关重要。
- en: For now, let’s take a look at the inputs of the transformer in a bit more detail.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看看transformer的输入。
- en: Prompt and completion flow simplified
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示和完成流程简化
- en: There are already countless transformer models, such as **GPT**, **Llama 2**,
    **Dolly**, **BERT**, **BART**, **T5**, and so on. These are essentially LLMs and,
    as you already know from [*Chapter 1*](B21443_01.xhtml#_idTextAnchor015), they
    are trained on vast quantities of unstructured text in a self-supervised manner.
    In this self-supervised learning, the training objective is automatically derived
    from the model’s inputs, eliminating the need for human-annotated labels or input
    (more on this later in this section). This allowed the transformer models or LLMs
    to be massive in terms of their parameters. GPT-4 has more than 1.75 trillion
    parameters alone. Sam Altman stated that the cost of training GPT-4 alone was
    more than $100 million ([https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/))!
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有无数个transformer模型，例如**GPT**、**Llama 2**、**Dolly**、**BERT**、**BART**、**T5**等等。这些本质上都是LLM，正如你从[*第一章*](B21443_01.xhtml#_idTextAnchor015)中已经知道的，它们是以自监督的方式在大量无结构文本上训练的。在这种自监督学习中，训练目标自动从模型的输入中导出，消除了对人工标注标签或输入的需求（关于这一点，本节后面将详细介绍）。这使得transformer模型或LLM在参数方面变得巨大。GPT-4
    alone就有超过1750亿个参数。Sam Altman表示，仅训练GPT-4的成本就超过1亿美元([https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/))！
- en: Such models gain a statistical comprehension of the language they are trained
    on. However, they are not particularly useful for specific practical tasks. To
    overcome this, the pre-trained model undergoes a process known as transfer learning.
    In this phase, the model is fine-tuned in a supervised manner, meaning it uses
    human-annotated labels for a specific task. We will cover fine-tuning in further
    detail in the next chapter, but for now, let’s look at the overall flow of a simple
    task. One such task could be predicting the next word in a sentence after reading
    the previous *n* words. This is referred to as causal language modeling since
    the output is dependent on past and present inputs but not future ones.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的模型对其训练语言获得了统计上的理解。然而，它们对于特定的实际任务并不是特别有用。为了克服这一点，预训练模型经过了一个称为迁移学习的过程。在这一阶段，模型以监督的方式进行微调，这意味着它使用针对特定任务的标注数据。我们将在下一章更详细地介绍微调，但就目前而言，让我们看看简单任务的整体流程。这样的任务之一可能是阅读前
    *n* 个单词后预测句子中的下一个单词。这被称为因果语言模型，因为输出依赖于过去和现在的输入，但不依赖于未来的输入。
- en: 'Let’s take a look at this simplified input/output flow, as mapped to the transformer
    model architecture, by using a financial news article as input and summarizing
    the document using a summarization LLM model:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用金融新闻文章作为输入，并使用摘要 LLM 模型总结文档，来看看这个简化的输入/输出流程，映射到变压器模型架构：
- en: '![Figure 2.5 – Simplified visual of how prompt/completions work in a typical
    LLM](img/B21443_02_5.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 典型 LLM 中提示/补全工作简化的视觉表示](img/B21443_02_5.jpg)'
- en: Figure 2.5 – Simplified visual of how prompt/completions work in a typical LLM
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 典型 LLM 中提示/补全工作简化的视觉表示
- en: 'In the preceding simplified transformer architecture, the interaction is the
    input/output described in the white boxes. The larger gray box is the entirety
    of the processing taking place without user interaction. Some of the phases in
    the prompt and completion sequence in the preceding image include the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的简化变压器架构中，交互是描述在白色框中的输入/输出。较大的灰色框是用户交互之外进行的全部处理。前面图像中提示和补全序列中的某些阶段包括以下内容：
- en: '**Input prompt**: The user interacts with the system by providing input. This
    input can exist in various forms, such as text, voice, or other modalities. In
    our example, a financial news article was the input.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入提示**：用户通过提供输入与系统交互。这种输入可以存在于各种形式，如文本、语音或其他模式。在我们的例子中，金融新闻文章是输入。'
- en: '**Additional prompt engineering**: In the case of summarizing a news article,
    typically, we do not need additional prompt engineering. Although we have an entire
    chapter devoted to covering prompt engineering later, it is enough to know that
    different prompts will generate different outcomes/completions and prompting is
    a skill in itself.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**额外的提示工程**：在总结新闻文章的情况下，通常我们不需要额外的提示工程。尽管我们将在后面的章节中专门介绍提示工程，但了解不同的提示会产生不同的结果/补全，提示本身也是一种技能就足够了。'
- en: '**Input text**: This is the area where the finalized input is taken in human
    readable form and passed on to computer processing (the tokenizer). For example,
    this could be a combination of the original user input and any additional inputs
    such as datasets. For our example, we used a single financial news article to
    summarize; however, this could have very well included many additional data points,
    such as the historical datasets of a financial platform, such as the US stock
    markets.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入文本**：这是将最终输入以人类可读的形式接收并传递给计算机处理（分词器）的区域。例如，这可能包括原始用户输入和任何额外的输入，如数据集。在我们的例子中，我们使用了一篇金融新闻文章来总结；然而，这完全可以包括许多额外的数据点，例如金融平台的歷史数据集，如美国股市。'
- en: '**Tokenizer**: In this layer, the news article would be converted into tokens
    and encoded into a vectorized service (more on this in [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070),
    RAGs to Riches).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词器**：在这一层，新闻文章会被转换成标记并编码成向量化的服务（更多内容请参考[*第 4 章*](B21443_04.xhtml#_idTextAnchor070)，RAGs
    to Riches）。'
- en: '**Encoded input**: The encoder takes each tokenized section as input and processes
    and prepares the encoding for the LLM summarization model.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码输入**：编码器将每个标记化部分作为输入，并处理和准备 LLM 摘要模型的编码。'
- en: '**Summarization model (an LLM)**: This is the hardest working layer, where
    the deep learning neural network of the LLM model resides. The LLM will add relationship
    weights to each word to generate relevant context and, in our example of a financial
    news article, will summarize the article into shortened, relevant, contextual
    concepts.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**总结模型（一个LLM）**：这是最辛苦的一层，其中包含了LLM模型的深度学习神经网络。LLM将为每个词添加关系权重，以生成相关上下文，在我们的金融新闻文章示例中，它将文章总结为简短、相关、上下文的概念。'
- en: '**Encoded output and tokenizer (decoded)**: The decoder takes the processed
    information from the encoder and its internal state to formulate a response. This
    response can manifest as text, audio, or even actions for downstream use. In our
    example, the output is an encoded text summary of a financial news article that
    is still in a numerical format.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码输出和解码器（编码器）**：解码器从编码器及其内部状态接收处理过的信息，以形成响应。这种响应可以表现为文本、音频，甚至用于下游使用的动作。在我们的例子中，输出是一个编码的文本摘要，它仍然以数值格式存在。'
- en: '**Output/completion**: This is the information returned to you, also known
    as the output. In our example of a long financial news article, you now have a
    summarized, shortened article.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出/完成**：这是返回给您的信息，也称为输出。在我们的长金融新闻文章示例中，您现在拥有一个总结的、简短的文章。'
- en: As you can see in our preceding simple example, taking a longer article (or
    any other text input) as input leads to a summarized article, with all the salient
    point(s) highlighted in a shortened and easily digestible format. This has many
    relevant business and personal scenarios, and I am sure you can think of how you
    can apply this to your everyday tasks. This is all done due to the transformer
    architecture!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在我们前面的简单示例中所见，将较长的文章（或任何其他文本输入）作为输入，会生成一个总结文章，其中所有显著点都以简短且易于消化的格式突出显示。这适用于许多相关商业和个人场景，我相信您能想到如何将其应用到您的日常任务中。这一切都归功于变换器架构！
- en: Beyond the preceding illustration, as mentioned at the start of this section,
    prompts can also include outputs from other services or LLM queries, instead of
    direct user input. In other words, rather than a human interacting with and posing
    a question or prompt to an LLM model, the input into that LLM model is really
    just output from another completion. This allows for chaining the output from
    one model to the input for another model, allowing for the creation of complex
    and dynamic interactions, tasks, or applications.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面的说明，如本节开头所述，提示也可以包括来自其他服务或LLM查询的输出，而不是直接的用户输入。换句话说，而不是人类与LLM模型互动并提问或给出提示，输入到LLM模型中的实际上是另一个完成的输出。这允许将一个模型的输出链接到另一个模型的输入，从而允许创建复杂和动态的交互、任务或应用。
- en: LLMs landscape, progression, and expansion
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的领域、进展和扩展
- en: We can write many chapters on how modern LLMs have leveraged transformer model
    architecture, along with its explosive expansion and the numerous models being
    created on almost on a daily basis. However, in this last section, let’s distill
    the usage of LLMs and their progression thus far and also add an exciting new
    layer of additional expansion to the functionality of LLMs using AutoGen.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以写很多章节来讲述现代LLM如何利用变换器模型架构，以及其爆炸性的扩展和几乎每天都有新模型被创建的情况。然而，在本节的最后部分，让我们提炼LLM的使用及其迄今为止的进展，并使用AutoGen添加一个令人兴奋的新层，以扩展LLM的功能。
- en: Exploring the landscape of transformer architectures
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索变换器架构的领域
- en: 'With their ability to handle a myriad of tasks, transformer models have revolutionized
    the field of natural language processing. By tweaking their architecture, we can
    create different types of transformer models, each with its unique applications.
    Let’s delve into three prevalent types:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们能够处理众多任务，变换器模型已经彻底改变了自然语言处理领域。通过调整其架构，我们可以创建不同类型的变换器模型，每种模型都有其独特的应用。让我们深入了解三种流行的类型：
- en: '**Models with encoders only**: These models, equipped solely with an encoder,
    are typically employed for tasks that involve understanding the context of the
    input, such as text classification, sentiment analysis, and question answering.
    A prime example is Google’s bi-directional encoder representations from transformers
    (BERT). BERT stands out for its ability to understand context in both directions
    (left to right and right to left), thanks to its pre-training on extensive text
    corpora. This bi-directional context understanding makes BERT a popular choice
    for tasks such as sentiment analysis and named entity recognition.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅具有编码器的模型**：这些模型仅配备编码器，通常用于涉及理解输入上下文的任务，例如文本分类、情感分析和问答。一个典型的例子是谷歌的双向编码器表示（BERT）。BERT因其能够理解双向上下文（从左到右和从右到左）的能力而脱颖而出，这得益于其在大量文本语料库上的预训练。这种双向上下文理解使BERT成为情感分析和命名实体识别等任务的流行选择。'
- en: '**Models with decoders only**: These models exclusively utilize a decoder and
    are primarily used for tasks that involve generating text, such as text generation,
    machine translation, and summarization. GPT (generative pre-trained transformer)
    is a notable instance of such models. GPT is celebrated for its creative text
    generation capabilities, achieved through a uni-directional decoder for autoregressive
    language modeling. This makes GPT particularly adept at tasks such as story generation
    and dialogue completion.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅具有解码器的模型**：这些模型仅使用解码器，主要用于涉及生成文本的任务，如文本生成、机器翻译和摘要。GPT（生成式预训练变压器）是这类模型的典型例子。GPT因其通过单向解码器实现的自回归语言建模的创造性文本生成能力而备受赞誉。这使得GPT特别擅长故事生成和对话补全等任务。'
- en: '**Models with both encoders and decoders**: These models amalgamate an encoder
    and a decoder, making them suitable for tasks that necessitate understanding the
    input and generating output. This includes tasks such as machine translation and
    dialogue generation. T5 (text-to-text transfer transformer) exemplifies this category.
    T5 presents a unified framework where every NLP task is treated as a text-to-text
    problem, employing both encoders and decoders. This endows T5 with remarkable
    versatility, enabling it to handle a wide array of tasks, from summarization to
    translation.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同时具有编码器和解码器的模型**：这些模型结合了编码器和解码器，适合需要理解输入并生成输出的任务。这包括机器翻译和对话生成等任务。T5（文本到文本迁移变压器）是这一类别的代表。T5提供了一个统一的框架，其中每个NLP任务都被视为一个文本到文本问题，同时使用编码器和解码器。这使得T5具有非凡的通用性，能够处理从摘要到翻译的广泛任务。'
- en: By understanding these different types of transformer models, we can better
    appreciate the flexibility and power of the transformer architecture in tackling
    diverse NLP tasks, and this can help us select which model is best suited for
    a cloud solution use case.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过理解这些不同类型的变压器模型，我们可以更好地欣赏变压器架构在处理各种NLP任务时的灵活性和强大功能，这有助于我们选择最适合云解决方案用例的模型。
- en: As you learn more about LLMs and where they are heading in the future in the
    subsequent chapters, please keep in mind these models are evolving quickly, and
    their support services and frameworks are evolving just as quickly. An exciting
    area where LLM use is both evolving and expanding is around the concept of AutoGen.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续章节中，当你了解更多关于LLMs以及它们未来发展方向时，请记住这些模型正在迅速演变，它们的支持服务和框架也在迅速演变。一个既在演变又在扩展的令人兴奋的领域是围绕AutoGen概念的应用。
- en: AutoGen
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AutoGen
- en: 'At the time of writing, significant work is being done by Microsoft Research
    on the next major breakthrough: autonomous agents, or AutoGen. AutoGen hopes to
    take LLMs and the evolution of the transformer model architecture to the next
    level. The Microsoft AutoGen framework is an open source platform for building
    multi-agent systems using large language models; we feel that this will have a
    significant impact on the generative AI space.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，微软研究院正在对下一个重大突破进行重要研究：自主代理，或称为AutoGen。AutoGen的目标是将大型语言模型（LLMs）和变压器模型架构的演变提升到新的水平。微软AutoGen框架是一个开源平台，用于构建使用大型语言模型的多元代理系统；我们相信这将显著影响生成式人工智能领域。
- en: Thus, later in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117), we will describe
    the concept and potential of autonomous agents driven by large language models
    and how they can augment human capabilities and solve complex problems. We will
    also show how LLM models that use AutoGen can perform tasks such as reasoning,
    planning, perception, self-improvement, self-evaluation, memory, personalization,
    and communication via the use of various prompt engineering techniques.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在[*第六章*](B21443_06.xhtml#_idTextAnchor117)的后续部分，我们将描述由大型语言模型驱动的自主代理的概念和潜力，以及它们如何增强人类能力并解决复杂问题。我们还将展示使用AutoGen的LLM模型如何通过各种提示工程技术执行推理、规划、感知、自我改进、自我评估、记忆、个性化以及通信等任务。
- en: As you might be able to conclude, the possibilities are endless once we understand
    how multiple large language models + AutoGen can work together in different ways,
    such as in hierarchies, networks, or swarms, to increase computing and reasoning
    power and solve more complex problems, including problems that may not even exist
    today!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经得出的结论，一旦我们了解多个大型语言模型+AutoGen如何以不同的方式协同工作，例如在层次结构、网络或群体中，以增加计算和推理能力并解决更复杂的问题，包括可能今天甚至不存在的问题，那么可能性将是无限的！
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the topic of generative AI and its applications,
    such as ChatGPT, and gave an overview of the main concepts and components involved,
    such as cloud computing, NLP, and the transformer model. Since its introduction
    in 2017, the original transformer model has expanded, leading to explosive growth
    in models and techniques that extend beyond only NLP-type tasks.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了生成式AI及其应用，如ChatGPT，并概述了涉及的主要概念和组件，如云计算、NLP和Transformer模型。自2017年引入以来，原始的Transformer模型已经扩展，导致模型和技术的爆炸性增长，这些模型和技术的应用范围已远远超出仅限于NLP类型的任务。
- en: We also briefly traced the development of NLP from RNNs and CNNs to the transformer
    model and explained how transformers overcome the limitations of the former models
    by using attention mechanisms and parallel processing. We covered how prompts,
    or user inputs, are processed by the transformer models to generate responses
    or completions using various variables and scenarios.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还简要回顾了NLP从RNNs和CNNs到Transformer模型的发展历程，并解释了Transformer如何通过使用注意力机制和并行处理来克服前者的局限性。我们涵盖了提示或用户输入如何被Transformer模型处理以生成响应或完成，使用各种变量和场景。
- en: Finally, we provided a brief overview of the LLM landscape and how various transformer
    architectures can be used for a variety of tasks and different use cases, along
    with their progression, touching on their expansion into many different areas
    outside the LLM models themselves, such as with AutoGen, which we will cover in
    depth in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要概述了LLM领域的概览以及各种Transformer架构如何用于各种任务和不同的用例，包括它们的进展，以及它们如何扩展到LLM模型本身之外的不同领域，例如AutoGen，我们将在[*第六章*](B21443_06.xhtml#_idTextAnchor117)中深入探讨。
- en: In the next chapter, we will discuss building domain-specific LLMs by using
    the concept of fine-tuning; then, we will discuss the next logical step in LLM
    model management and another important tool to have in your generative AI toolbox!
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论通过使用微调的概念来构建特定领域的LLMs；然后，我们将讨论LLM模型管理中的下一个逻辑步骤，以及您在生成式AI工具箱中需要的重要工具！
- en: References
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Transformer publication: *Attention is All You* *Need*; [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer论文：*Attention is All You Need*；[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: '*Training GPT-4 cost over $100* *million*; [https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练GPT-4的成本超过1亿美元*；[https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)'
- en: '*Transformer Architecture: The Engine behind* *ChatGPT*; [https://tinyurl.com/6k99bw98](https://tinyurl.com/6k99bw98)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Transformer架构：ChatGPT背后的引擎*；[https://tinyurl.com/6k99bw98](https://tinyurl.com/6k99bw98)'
- en: 'Part 2: Techniques for Tailoring LLMs'
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分：调整LLMs的技术
- en: This section highlights key techniques that have emerged in recent years to
    customize **Large Language Models** (**LLMs**) for specific business needs, such
    as fine-tuning. It also addresses current challenges, including mitigating hallucinations
    and extending training cut-off dates, to incorporate up-to-date information through
    methods such as **Retrieval Augmented Generation** (**RAG**). Additionally, we
    will explore prompt engineering techniques to enhance effective communication
    with AI.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本节突出了近年来出现的关键技术，这些技术用于针对特定商业需求定制**大型语言模型**（**LLMs**），例如微调。它还讨论了当前挑战，包括减轻幻觉和延长训练截止日期，通过如**检索增强生成**（**RAG**）等方法来整合最新信息。此外，我们将探讨提示工程技术，以增强与人工智能的有效沟通。
- en: 'This part contains the following chapters:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 3*](B21443_03.xhtml#_idTextAnchor052), *Fine Tuning: Building Domain-Specific
    LLM Applications*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第三章*](B21443_03.xhtml#_idTextAnchor052), *微调：构建特定领域LLM应用*'
- en: '[*Chapter 4*](B21443_04.xhtml#_idTextAnchor070), *RAGs to Riches: Elevating
    AI with External Data*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第四章*](B21443_04.xhtml#_idTextAnchor070), *RAGs to Riches：利用外部数据提升人工智能*'
- en: '[*Chapter 5*](B21443_05.xhtml#_idTextAnchor098), *Effective Prompt Engineering
    Strategies: Unlocking Wisdom Through AI*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第五章*](B21443_05.xhtml#_idTextAnchor098), *有效提示工程策略：通过人工智能开启智慧*'
