- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP Evolution and Transformers: Exploring NLPs and LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous introductory chapter, you gained a fundamental understanding
    of generative AI, including a primer on the growing complexity of generative AI
    applications, along with a brief introduction to cloud computing for scalability
    and cost-effectiveness and the key components of data storage, security, and collaboration.
    You also learned one of the more exciting aspects of generative AI, which can
    also be a hurdle, which is how to stay up to date with cutting-edge AI technologies
    such as GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the capabilities of ChatGPT, specifically with
    regard to its conversation input and response abilities. We will delve deeper
    into how LLMs are able to understand and respond to user queries and learn and
    adapt to new information. The information provided will be useful for individuals
    who are looking to understand more about how AI assistants, such as ChatGPT, work
    and how they can be utilized to help people find information more efficiently
    and effectively; subsequently, we will be expanding on this topic in relation
    to the NLP and prompt engineering topics discussed in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098).
    By the end of this chapter, we hope you will get a deeper understanding of the
    progression of NLP and generative AI techniques by exploring the capabilities
    of various text-based tasks for prompts and responses, along with conversational
    flows and integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following main topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: NLP evolution and the rise of transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversation prompts and completions – under the covers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs landscape, progression, and expansion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.1 – How profound transformers have become](img/B21443_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – How profound transformers have become
  prefs: []
  type: TYPE_NORMAL
- en: NLP evolution and the rise of transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'NLP, or natural language processing, is the field of artificial intelligence
    that gives computers the ability to understand and manipulate human language using
    common spoken (or otherwise) language instead of what was traditionally given
    as input to computers in the past: computer programming language. Over the past
    several decades, these computer programming languages became more “natural” with
    fluency:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Brief timeline of NLP evolution](img/B21443_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Brief timeline of NLP evolution
  prefs: []
  type: TYPE_NORMAL
- en: Over time, there has been significant advancement in the field of NLP, with
    computers increasingly improving in their ability for text generation due to the
    emergence of neural networks. Text generation itself isn’t a novel idea, but earlier
    language models before 2017 predominantly utilized ML architectures known as **recurrent
    neural networks** (**RNNs**) and **convolutional neural** **networks** (**CNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are a type of neural network architecture that excels at processing sequence
    data. They process input in a sequential manner, carrying information from one
    step in the sequence to the next. This makes them quite useful for tasks such
    as text generation, translation, and sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN is a type of deep learning architecture designed to process and analyze
    visual data, such as images and videos, by using specialized layers called convolutional
    layers. These layers apply filters to extract relevant features from the input
    data, capturing patterns and hierarchies of information. CNNs are primarily used
    for tasks such as image classification, object detection, and image segmentation
    in computer vision. In **natural language processing** (**NLP**), CNNs can also
    be applied to tasks such as text classification and sentiment analysis, where
    the input text is transformed into a matrix-like structure to capture local patterns
    and relationships among words or characters.
  prefs: []
  type: TYPE_NORMAL
- en: The main drawbacks of RNNs and CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the sophistication of RNNs, their potential could not be fully harnessed
    due to certain constraints. RNNs often struggle with the “Vanishing gradient problem”
    during training, which hampers their ability to learn from long sequences and
    retain long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the inherently “sequential processing” of RNNs does not allow
    for efficient parallelization, significantly slowing down training in an age where
    GPU-based parallel processing is standard for deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs were, thus, limited in computing and memory. In order to predict the next
    word in a sentence, the models need to know more than the previous few words;
    they also need to understand the context of the word in a sentence, paragraph,
    or whole document.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explain this concept with an example by using the sentence
  prefs: []
  type: TYPE_NORMAL
- en: “*The water in the ocean has a lot of salt, it’s a bit choppy and it* *tastes*
    *sweet**.*”
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding sentence, the RNN might generate a continuation, indicating
    the water is sweet instead of salty. The reason for this is because of only taking
    the last few words into consideration and not the context of the whole sentence.
    The RNN will have forgotten the context from earlier in the text that might indicate
    the taste of the water from the ocean.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, CNNs have revolutionized image analysis by automatically learning
    hierarchical features through layers of convolutions. Despite their success, CNNs
    are limited in that they have a fixed receptive field size and operate in a local
    context. This limitation makes it challenging for them to capture global dependencies
    and the relationships present in sequences of varying lengths. For instance, in
    image classification, while CNNs excel at recognizing local patterns, they struggle
    to grasp the overall context of an image, hindering their ability to understand
    complex relationships between objects or regions. Consider an image of a cat chasing
    a mouse with a dog watching in the background. A CNN might effectively identify
    the cat, mouse, and dog based on their local features. However, understanding
    the intricate relationships, e.g., the cat is chasing the mouse, and the dog is
    a passive observer, may be challenging for a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: So, how did we finally overcome the challenges of CNNs? It was done by using
    a concept known as transformer model architecture and its “self-attention mechanism,”
    which is described in the next section. This would not only identify the individual
    animals but also capture the contextual interactions, such as the chase sequence
    and the dog’s passive stance.
  prefs: []
  type: TYPE_NORMAL
- en: However, before we really peel back the layers on how transformers work, the
    following is a reference timeline about the strengths of NLP coupled with LLMs.
    Once you realize the benefits and the “why,” we can then dive into the “how.”
  prefs: []
  type: TYPE_NORMAL
- en: NLP and the strengths of generative AI in LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides a broad overview of NLP with LLMs before the next section,
    where we explain more about transformers: the powerful engine behind LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Large language models** (**LLMs**) are incredibly potent language models
    that are transforming our comprehension and creation of human language. But what’s
    their connection to NLP? It’s rather fundamental. NLP lays out the structure and
    goals for interpreting and generating human language, whereas LLMs serve as sophisticated
    tools that facilitate the realization of these goals on a grand scale, handling
    intricate tasks with remarkable precision.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, NLP is a branch of machine learning that enables computers
    to understand, process, and generate human language. It combines computer science
    and linguistics. For example, there is a massive amount of audio and text data
    generated by organizations from various communication channels. These data can
    be processed by NLP models to automatically process data, determine sentiments,
    summarize, and find answers, key topics, or even respond effectively.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick, simple example, the audio data generated by call centers can be
    converted to text and processed by NLP models to determine both the issue the
    customer is facing and also the sentiment of the customer (whether they are happy,
    upset, nonchalant, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: NLP is the technology behind search engines, such as Bing and Google, voice
    assistants, such as Alexa and Siri, and powerful conversational agents, such as
    ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: From this, it would appear that NLP technology should address all of our needs.
    So why should we have to deal with LLMs and GenAI at all?
  prefs: []
  type: TYPE_NORMAL
- en: By taking a step back briefly and looking at the preceding evolution timeline,
    the inception of **advanced NLP** can be traced back to 2013 with the advent of
    word2vec, a model introduced by Google that transformed words into dense vectors
    based on contextual relationships. A vector is defined as an object that has both
    a magnitude and a direction and is represented in a numerical array format.
  prefs: []
  type: TYPE_NORMAL
- en: This was revolutionary, as it captured semantic nuances that older models couldn’t
    grasp. However, they couldn’t focus on different parts of the text to form a larger
    understanding. For example, various words in a sentence, or multiple sentences,
    could not be related to one another for a full understanding of a sentence or
    paragraph. This limitation was tackled by attention mechanisms, which were introduced
    in the 2017 paper *Attention Is All You Need*. These mechanisms led to the transformer
    architecture, the backbone of the foundational LLM models we see today, which
    allowed models to form an understanding of text beyond just words and sentences.
    There will be more on this a bit later, but first, let’s cover why we want to
    use LLMs and look at some areas where LLMs can enhance NLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'NLP plus LLMs equals expanded possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding language**: LLMs are adept at comprehending and processing
    a vast array of language inputs, making them useful for a variety of linguistic
    tasks. LLMs can be used to build advanced chatbots and virtual assistants. They
    can understand and respond to customer inquiries, provide information, and execute
    tasks, improving the efficiency and quality of customer service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generation**: LLMs can generate coherent and contextually appropriate
    text, enabling applications such as chatbots, content creation, copywriting, and
    more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can enhance efficiency in internal and external communications by recommending
    and suggesting words or reviewing your content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Language translation**: LLMs can directly translate text between different
    languages, aiding cross-cultural communication and language learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As LLMs can provide translation between multiple languages, this can help businesses
    operate more efficiently in a globalized world by breaking down language barriers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: LLMs can analyze text to determine its sentiment (positive,
    negative, or neutral), providing valuable insights for applications such as customer
    feedback analysis. LLMs can analyze customer feedback, reviews, or social media
    posts to assess public sentiment toward a brand, product, or service. This can
    help with business strategies and decision-making processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**: LLMs can understand and provide accurate answers to
    a wide range of questions, making it possible to build an organization-specific
    enterprise search engine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization**: LLMs can condense long pieces of text into shorter
    summaries, aiding in information processing and comprehension. LLMs can summarize
    long documents, articles, or reports, making it easier to digest large amounts
    of information quickly while identifying key areas or the next steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptability**: LLMs can generate text in various styles, tones, or formats,
    adapting to specific user needs or application requirements. For example, you
    can ask ChatGPT to define and describe Photosynthesis in plants for your 6-year-old
    child in the style of a pirate. In relation to this, by using data about user
    behavior and preferences, LLMs can generate personalized content or product recommendations,
    thus improving user experience and potentially increasing sales for retail businesses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Context maintenance**: Although they only have short-term memory, LLMs can
    maintain conversational context over extended interactions with the right prompt
    engineering techniques, improving the coherence and relevance of their responses.
    We will cover prompt engineering techniques in [*Chapter 5*](B21443_05.xhtml#_idTextAnchor098)
    of this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creativity**: LLMs can generate novel text, opening more possibilities for
    creative applications such as story generation or poetry creation. From writing
    articles, reports, and marketing copy to generating creative content, LLMs can
    automate and enhance various content creation tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we have listed a few of the areas where large language models have enhanced
    the functionality of natural language processing. Now that you can appreciate
    the fact LLMs can provide enhancements to any NLP services, and also to our everyday
    lives, let’s take the next step: a deeper dive into transformers and the attention
    mechanism, which gives LLMs their power to run generative AI.'
  prefs: []
  type: TYPE_NORMAL
- en: How do transformers work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The introduction of transformer architecture addresses the preceding shortcomings
    of RNNs and CNNs. Transformers use an attention mechanism, which allows the model
    to focus on different parts of the input when generating each word in the output.
    Simply put, the attention mechanism measures how words interrelate in a sentence,
    paragraph, or section. For LLMs, the underlying transformer is a set of deep learning
    neural networks that consist of an encoder component and a decoder component that
    exist within the concept of self-attention capability. During self-attention,
    an LLM will assign weights to different words based on their relevance to the
    current word being processed, and this is what gives the model its power. This
    attention mechanism dynamically enables LLMs to focus on critical contextual information
    while also disregarding nonrelevant items/words at the same time. In other words,
    the encoder and decoder components extract meanings from a sequence of text and
    understand the relationships between the words and phrases in it.
  prefs: []
  type: TYPE_NORMAL
- en: This allows transformers to maintain a better sense of long-term **context**
    compared to RNNs and CNNs. Positional encodings allow for the handling of sequence
    order and transformers allow for the **parallel processing** of sequences, making
    LLMs much faster to train compared to RNNs. The foundational models underpinning
    ChatGPT, known as GPT models, employ this transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'When first introduced, the transformer architecture was originally designed
    for translation and is described in the now famous publication by Google: *Attention
    is All You Need* (see [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
    for a deeper look). From this publication, we show the original transformer architecture
    in the following image, and we have added the encoder on the left and the decoder
    on the right for your high-level understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Transformer model architecture](img/B21443_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Transformer model architecture
  prefs: []
  type: TYPE_NORMAL
- en: While the preceding image can be daunting to some, especially to beginners in
    the field of generative AI, you do not necessarily need to have a firm understanding
    of each subcomponent of the transformer model architecture in the same way that
    most people do not need to know the internal workings of an automobile engine
    in order to drive a car. We will only cover the main input and outputs of the
    transformer architecture, and there is a simplified view later in this chapter
    to describe some of the inner workings and flow. We will continue to emphasize
    and repeat various aspects of the transformer model, as this can be a difficult
    concept to grasp, especially for those new to generative AI and LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: From the original purpose of language translations in 2017, the transformer
    model architecture became the underpinning framework for future generative AI
    models, leading to the emergence of ChatGPT; the letter **T** in GPT stands for
    **transformer** (GPT).
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, transformers are a type of neural network architecture
    that replaces traditional RNNs and CNNs with an entirely attention-based mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: But how does the attention mechanism work?
  prefs: []
  type: TYPE_NORMAL
- en: Attention does this by calculating “soft” weights for each word in the context
    window and doing this in parallel in the transformer model vs. sequentially in
    the RNN/CNN models. These “soft” weights can, and often do, change during the
    runtime of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The benefits of transformers are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: They scale efficiently to use multi-core GPUs and parallel processing training
    data; hence, they can make use of much, much larger datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They pay attention to the meaning of input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They learn the relevance of every word and their context in a sentence/paragraph,
    not just the neighboring words, as with RNNs and CNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a look at a visual representation of how the words of the sentence,
    “*The musician taught the student with the piano*,” relate to one another from
    the perspective of a transformer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Sentence context relationships](img/B21443_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Sentence context relationships
  prefs: []
  type: TYPE_NORMAL
- en: As stated in the preceding example, transformers are able to link every word,
    determine the relationships between every word in the input (even if they are
    immediately preceding or succeeding word(s)), and understand the context of the
    word in a sentence. In the preceding image, the colored lines represent stronger
    relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, transformers use modern mathematical techniques, such as attention or
    self-attention, to determine the inter-relationships and dependencies among data
    elements, even when they are far apart. This gives the model the ability to learn
    who taught the student and with what instrument, etc.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple layers in the transformer deep learning architecture, such
    as the embedding layer, self-attention, and multi-headed attention, as well as
    the multiple encoder models themselves. While a detailed understanding of the
    transformer architecture isn’t essential for successful prompt engineering or
    understanding generative AI, having a foundational grasp of the transformer model,
    a critical aspect of LLM’s and ChatGPT’s underlying architecture, is important
    for any cloud solution design.
  prefs: []
  type: TYPE_NORMAL
- en: As we have talked about benefits, let’s also mention a negative aspect of transformers;
    they can sometimes produce a by-product that also affects LLMs, which we briefly
    mentioned in the first chapter but will again mention here as we are discussing
    transformers and that is the concept of “hallucinations.” A hallucination is basically
    incorrect information returned by an LLM model. This hallucination is response
    output, which is inconsistent with the prompt and is often due to a few reasons,
    such as the actual training data used to train the LLM model itself being incomplete
    or spurious. We wanted to mention it here, but we will discuss hallucinations
    in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s dive into the inner workings of transformer architecture and
    explore the transformer concept a bit more with some examples.
  prefs: []
  type: TYPE_NORMAL
- en: Conversation prompts and completions – under the covers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompts, or the input entered by you or an application/service, play a crucial
    role in NLP + LLMs by facilitating the interaction between humans and language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: If you have had any experience with GenAI, you may have already entered a prompt
    into an online service such as chat.bing.com. A prompt is to an LLM what a search
    term is to a web search engine, but each can take a prompt input and run some
    action(s) against such input. Just like you would intelligently enter search terms
    into a search engine to find the content you are looking for, the same can be
    said about entering prompts intelligently. This concept is known as prompt engineering,
    and we devote an entire chapter to prompt engineering later in this book, which
    will describe the “how” of writing an effective prompt to get the results you
    need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of you who are newer to the generative AI space might wonder why we need
    to understand how to write a prompt at all. Let’s provide a simple analogy: if
    you think of a **database administrator** (**DBA**) who needs to pull (query)
    specific data from a vast database with many tables (say, a typical customer sales
    database) in order to understand the trends and forecasting of sales to ensure
    there is enough product, you have to analyze the historical data. However, if
    the DBA cannot put together a proper query to build a report of past sales history,
    any forecasting and future trends will be completely incorrect.'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a poorly constructed prompt is like using a dull knife, you’re unlikely
    to get great results. Thus, prompt engineering is crucial to generate useful responses.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let’s take a look at the inputs of the transformer in a bit more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt and completion flow simplified
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are already countless transformer models, such as **GPT**, **Llama 2**,
    **Dolly**, **BERT**, **BART**, **T5**, and so on. These are essentially LLMs and,
    as you already know from [*Chapter 1*](B21443_01.xhtml#_idTextAnchor015), they
    are trained on vast quantities of unstructured text in a self-supervised manner.
    In this self-supervised learning, the training objective is automatically derived
    from the model’s inputs, eliminating the need for human-annotated labels or input
    (more on this later in this section). This allowed the transformer models or LLMs
    to be massive in terms of their parameters. GPT-4 has more than 1.75 trillion
    parameters alone. Sam Altman stated that the cost of training GPT-4 alone was
    more than $100 million ([https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/))!
  prefs: []
  type: TYPE_NORMAL
- en: Such models gain a statistical comprehension of the language they are trained
    on. However, they are not particularly useful for specific practical tasks. To
    overcome this, the pre-trained model undergoes a process known as transfer learning.
    In this phase, the model is fine-tuned in a supervised manner, meaning it uses
    human-annotated labels for a specific task. We will cover fine-tuning in further
    detail in the next chapter, but for now, let’s look at the overall flow of a simple
    task. One such task could be predicting the next word in a sentence after reading
    the previous *n* words. This is referred to as causal language modeling since
    the output is dependent on past and present inputs but not future ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at this simplified input/output flow, as mapped to the transformer
    model architecture, by using a financial news article as input and summarizing
    the document using a summarization LLM model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Simplified visual of how prompt/completions work in a typical
    LLM](img/B21443_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – Simplified visual of how prompt/completions work in a typical LLM
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding simplified transformer architecture, the interaction is the
    input/output described in the white boxes. The larger gray box is the entirety
    of the processing taking place without user interaction. Some of the phases in
    the prompt and completion sequence in the preceding image include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input prompt**: The user interacts with the system by providing input. This
    input can exist in various forms, such as text, voice, or other modalities. In
    our example, a financial news article was the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Additional prompt engineering**: In the case of summarizing a news article,
    typically, we do not need additional prompt engineering. Although we have an entire
    chapter devoted to covering prompt engineering later, it is enough to know that
    different prompts will generate different outcomes/completions and prompting is
    a skill in itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input text**: This is the area where the finalized input is taken in human
    readable form and passed on to computer processing (the tokenizer). For example,
    this could be a combination of the original user input and any additional inputs
    such as datasets. For our example, we used a single financial news article to
    summarize; however, this could have very well included many additional data points,
    such as the historical datasets of a financial platform, such as the US stock
    markets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenizer**: In this layer, the news article would be converted into tokens
    and encoded into a vectorized service (more on this in [*Chapter 4*](B21443_04.xhtml#_idTextAnchor070),
    RAGs to Riches).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoded input**: The encoder takes each tokenized section as input and processes
    and prepares the encoding for the LLM summarization model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Summarization model (an LLM)**: This is the hardest working layer, where
    the deep learning neural network of the LLM model resides. The LLM will add relationship
    weights to each word to generate relevant context and, in our example of a financial
    news article, will summarize the article into shortened, relevant, contextual
    concepts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoded output and tokenizer (decoded)**: The decoder takes the processed
    information from the encoder and its internal state to formulate a response. This
    response can manifest as text, audio, or even actions for downstream use. In our
    example, the output is an encoded text summary of a financial news article that
    is still in a numerical format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output/completion**: This is the information returned to you, also known
    as the output. In our example of a long financial news article, you now have a
    summarized, shortened article.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see in our preceding simple example, taking a longer article (or
    any other text input) as input leads to a summarized article, with all the salient
    point(s) highlighted in a shortened and easily digestible format. This has many
    relevant business and personal scenarios, and I am sure you can think of how you
    can apply this to your everyday tasks. This is all done due to the transformer
    architecture!
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the preceding illustration, as mentioned at the start of this section,
    prompts can also include outputs from other services or LLM queries, instead of
    direct user input. In other words, rather than a human interacting with and posing
    a question or prompt to an LLM model, the input into that LLM model is really
    just output from another completion. This allows for chaining the output from
    one model to the input for another model, allowing for the creation of complex
    and dynamic interactions, tasks, or applications.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs landscape, progression, and expansion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can write many chapters on how modern LLMs have leveraged transformer model
    architecture, along with its explosive expansion and the numerous models being
    created on almost on a daily basis. However, in this last section, let’s distill
    the usage of LLMs and their progression thus far and also add an exciting new
    layer of additional expansion to the functionality of LLMs using AutoGen.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the landscape of transformer architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With their ability to handle a myriad of tasks, transformer models have revolutionized
    the field of natural language processing. By tweaking their architecture, we can
    create different types of transformer models, each with its unique applications.
    Let’s delve into three prevalent types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Models with encoders only**: These models, equipped solely with an encoder,
    are typically employed for tasks that involve understanding the context of the
    input, such as text classification, sentiment analysis, and question answering.
    A prime example is Google’s bi-directional encoder representations from transformers
    (BERT). BERT stands out for its ability to understand context in both directions
    (left to right and right to left), thanks to its pre-training on extensive text
    corpora. This bi-directional context understanding makes BERT a popular choice
    for tasks such as sentiment analysis and named entity recognition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models with decoders only**: These models exclusively utilize a decoder and
    are primarily used for tasks that involve generating text, such as text generation,
    machine translation, and summarization. GPT (generative pre-trained transformer)
    is a notable instance of such models. GPT is celebrated for its creative text
    generation capabilities, achieved through a uni-directional decoder for autoregressive
    language modeling. This makes GPT particularly adept at tasks such as story generation
    and dialogue completion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models with both encoders and decoders**: These models amalgamate an encoder
    and a decoder, making them suitable for tasks that necessitate understanding the
    input and generating output. This includes tasks such as machine translation and
    dialogue generation. T5 (text-to-text transfer transformer) exemplifies this category.
    T5 presents a unified framework where every NLP task is treated as a text-to-text
    problem, employing both encoders and decoders. This endows T5 with remarkable
    versatility, enabling it to handle a wide array of tasks, from summarization to
    translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By understanding these different types of transformer models, we can better
    appreciate the flexibility and power of the transformer architecture in tackling
    diverse NLP tasks, and this can help us select which model is best suited for
    a cloud solution use case.
  prefs: []
  type: TYPE_NORMAL
- en: As you learn more about LLMs and where they are heading in the future in the
    subsequent chapters, please keep in mind these models are evolving quickly, and
    their support services and frameworks are evolving just as quickly. An exciting
    area where LLM use is both evolving and expanding is around the concept of AutoGen.
  prefs: []
  type: TYPE_NORMAL
- en: AutoGen
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the time of writing, significant work is being done by Microsoft Research
    on the next major breakthrough: autonomous agents, or AutoGen. AutoGen hopes to
    take LLMs and the evolution of the transformer model architecture to the next
    level. The Microsoft AutoGen framework is an open source platform for building
    multi-agent systems using large language models; we feel that this will have a
    significant impact on the generative AI space.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, later in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117), we will describe
    the concept and potential of autonomous agents driven by large language models
    and how they can augment human capabilities and solve complex problems. We will
    also show how LLM models that use AutoGen can perform tasks such as reasoning,
    planning, perception, self-improvement, self-evaluation, memory, personalization,
    and communication via the use of various prompt engineering techniques.
  prefs: []
  type: TYPE_NORMAL
- en: As you might be able to conclude, the possibilities are endless once we understand
    how multiple large language models + AutoGen can work together in different ways,
    such as in hierarchies, networks, or swarms, to increase computing and reasoning
    power and solve more complex problems, including problems that may not even exist
    today!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the topic of generative AI and its applications,
    such as ChatGPT, and gave an overview of the main concepts and components involved,
    such as cloud computing, NLP, and the transformer model. Since its introduction
    in 2017, the original transformer model has expanded, leading to explosive growth
    in models and techniques that extend beyond only NLP-type tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We also briefly traced the development of NLP from RNNs and CNNs to the transformer
    model and explained how transformers overcome the limitations of the former models
    by using attention mechanisms and parallel processing. We covered how prompts,
    or user inputs, are processed by the transformer models to generate responses
    or completions using various variables and scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we provided a brief overview of the LLM landscape and how various transformer
    architectures can be used for a variety of tasks and different use cases, along
    with their progression, touching on their expansion into many different areas
    outside the LLM models themselves, such as with AutoGen, which we will cover in
    depth in [*Chapter 6*](B21443_06.xhtml#_idTextAnchor117).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss building domain-specific LLMs by using
    the concept of fine-tuning; then, we will discuss the next logical step in LLM
    model management and another important tool to have in your generative AI toolbox!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transformer publication: *Attention is All You* *Need*; [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training GPT-4 cost over $100* *million*; [https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/](https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Transformer Architecture: The Engine behind* *ChatGPT*; [https://tinyurl.com/6k99bw98](https://tinyurl.com/6k99bw98)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 2: Techniques for Tailoring LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section highlights key techniques that have emerged in recent years to
    customize **Large Language Models** (**LLMs**) for specific business needs, such
    as fine-tuning. It also addresses current challenges, including mitigating hallucinations
    and extending training cut-off dates, to incorporate up-to-date information through
    methods such as **Retrieval Augmented Generation** (**RAG**). Additionally, we
    will explore prompt engineering techniques to enhance effective communication
    with AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B21443_03.xhtml#_idTextAnchor052), *Fine Tuning: Building Domain-Specific
    LLM Applications*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B21443_04.xhtml#_idTextAnchor070), *RAGs to Riches: Elevating
    AI with External Data*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21443_05.xhtml#_idTextAnchor098), *Effective Prompt Engineering
    Strategies: Unlocking Wisdom Through AI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
