- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Applying Pretrained Generative Models: From Prototype to Production'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapters, we explored the fundamentals of generative AI, explored
    various generative models, such as **generative adversarial networks** (**GANs**),
    diffusers, and transformers, and learned about the transformative impact of **natural
    language processing** (**NLP**). As we transition into the practical aspects of
    applying generative AI, we should ground our exploration in a practical example.
    This approach will provide a concrete context, making the technical aspects more
    relatable and the learning experience more engaging.
  prefs: []
  type: TYPE_NORMAL
- en: We will introduce “StyleSprint,” a clothing shop looking to enhance its online
    presence. One way to achieve this is by crafting unique and engaging product descriptions
    for its various products. However, manually creating captivating descriptions
    for a large inventory is challenging. This situation is prime opportunity for
    the application of generative AI. By leveraging a pretrained generative model,
    StyleSprint can automate the crafting of compelling product descriptions, saving
    considerable time and enriching the online shopping experience for its customers.
  prefs: []
  type: TYPE_NORMAL
- en: As we step into the practical application of a pretrained generative **large
    language models** (**LLM**), the first order of business is to set up a Python
    environment conducive to prototyping with generative models. This setup is vital
    for transitioning the project from a prototype to a production-ready state, setting
    the stage for StyleSprint to realize its goal of automated content generation.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapters 2* and *3*, we used Google Colab for prototyping due to its ease
    of use and accessible GPU resources. It served as a great platform to test ideas
    quickly. However, as we shift our focus toward deploying our generative model
    in a real-world setting, it is essential to understand the transition from a prototyping
    environment such as Google Colab to a more robust, production-ready setup. This
    transition will ensure our solution is scalable, reliable, and well-optimized
    for handling real-world traffic. In this chapter, we will walk through the steps
    in setting up a production-ready Python environment, underscoring the crucial
    considerations for a smooth transition from prototype to production.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, we will understand the process of taking a generative
    application from a prototyping environment to a production-ready setup. We will
    define a reliable and repeatable strategy for evaluating, monitoring, and deploying
    models to production.
  prefs: []
  type: TYPE_NORMAL
- en: Prototyping environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jupyter notebooks provide an interactive computing environment to combine code
    execution, text, mathematics, plots, and rich media into a single document. They
    are ideal for prototyping and interactive development, making them a popular choice
    among data scientists, researchers, and engineers. Here is what they offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kernel**: At the heart of a Jupyter notebook is a kernel, a computational
    engine that executes the code contained in the notebook. For Python, this is typically
    an IPython kernel. This kernel remains active and maintains the state of your
    notebook’s computations while the notebook is open.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive execution**: Code cells allow you to write and execute code interactively,
    inspecting the results and tweaking the code as necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pip` or `conda` commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualization**: You can embed plots, graphs, and other visualizations to
    explore data and results interactively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Documentation**: Combining Markdown cells with code cells allows for well-documented,
    self-contained notebooks that explain the code and its output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A drawback to Jupyter notebooks is that they typically rely on the computational
    resources of your personal computer. Most personal laptops and desktops are not
    optimized or equipped to handle computationally intensive processes. Having adequate
    computational resources is crucial for managing the computational complexity of
    experimenting with an LLM. Fortunately, we can extend the capabilities of a Jupyter
    notebook with cloud-based platforms that offer computational accelerators such
    as **graphics processing units** (**GPUs**) and **tensor processing units** (**TPUs**).
    For example, Google Colab instantly enhances Jupyter notebooks, making them conducive
    to computationally intensive experimentation. Here are some of the key features
    of a cloud-based notebook environment such as Google Colab:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GPU/TPU access**: Provides free or affordable access to GPU and TPU resources
    for accelerated computation, which is crucial when working with demanding machine
    learning models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaboration**: Permits easy sharing and real-time collaboration, similar
    to Google Docs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration**: Allows for easy storage and access to notebooks and data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s consider our StyleSprint scenario. We will want to explore a few different
    models to generate product descriptions before deciding on one that best fits
    StyleSprint’s goals. We can set up a minimal working prototype in Google Colab
    to compare models. Again, cloud-based platforms provide an optimal and accessible
    environment for initial testing, experimentation, and even some lightweight training
    of models. Here is how we might initially set up a generative model to start experimenting
    with automated product description generation for StyleSprint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In this simple setup, we’re installing the `transformers` library, which offers
    a convenient interface to various pretrained models. We then initialize a text
    generation pipeline with an open source version of GPT-Neo, capable of generating
    coherent and contextually relevant text. This setup serves as a starting point
    for StyleSprint to experiment with generating creative product descriptions on
    a small scale.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we will expand our experiment to evaluate and compare
    multiple pretrained generative models to determine which best meets our needs.
    However, before advancing further in our experimentation and prototyping, it is
    crucial to strategically pause and project forward. This deliberate forethought
    allows us to consider the necessary steps for effectively transitioning our experiment
    into a production environment. By doing so, we ensure a comprehensive view of
    the project from end to end, to align with long-term operational goals.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1: Moving from prototyping to production—the stages](img/B21773_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Moving from prototyping to production—the stages'
  prefs: []
  type: TYPE_NORMAL
- en: Transitioning to production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we plan for a production setup, we should first understand the intrinsic
    benefits and features of the prototyping environment we will want to carry forward
    to a production setting. Many of the features of prototyping environments such
    as Google Colab are deeply integrated and can easily go unnoticed, so it is important
    to dissect and catalog the features we will need in production. For example, the
    following features are inherent in Google Colab and will be critical in production:'
  prefs: []
  type: TYPE_NORMAL
- en: '`!pip install library_name`. In production, we will have to preinstall libraries
    or make sure we can install them as needed. We must also ensure that project-specific
    libraries do not interfere with other projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency isolation**: Google Colab automatically facilitates isolated dependencies,
    ensuring package installations and updates do not interfere with other projects.
    In production, we may also want to deploy various projects using the same infrastructure.
    Dependency isolation will be critical to prevent one project’s dependency updates
    from impacting other projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive code execution**: The interactive execution of code cells helps
    in testing individual code snippets, visualizing results, and debugging in real
    time. This convenience is not necessary in production but could be helpful for
    quick debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource accessibility**: With Colab, access to GPUs and TPUs is simplified,
    which is crucial for running computation-intensive tasks. For production, we will
    want to examine our dynamic computational needs and provision the appropriate
    infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data integration**: Colab offers simple connectivity to data sources for
    analysis and modeling. In production, we can either bootstrap our environment
    with data (i.e., deploy data directly into the environment) or ensure connectivity
    to remote data sources as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Versioning and collaboration**: Tracking versions of your project code with
    Google Colab can easily be accomplished using notebooks. Additionally, Colab is
    preconfigured to interact with Git. Git is a distributed version control system
    that is widely used for tracking changes in source code during software development.
    In production, we will also want to integrate Git to manage our code and synchronize
    it with a remote code repository such as GitHub or Bitbucket. Remote versioning
    ensures that our production environment always reflects the latest changes and
    enables ongoing collaboration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error handling and debugging**: In Colab, we have direct access to the Python
    runtime and can typically see error messages and tracebacks in real time to help
    identify and resolve issues. We will want the same level of visibility in production
    via adequate logging of system errors. In total, we want to carry over the convenience
    and simplicity of our Google Colab prototyping environment but provide the robustness
    and scalability required for production. To do so, we will map each of the key
    characteristics we laid out to a corresponding production solution. These key
    features should ensure a smooth transition for deploying StyleSprint’s generative
    model for automated product description generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping features to production setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure we can seamlessly transition our prototyping environment to production,
    we can leverage Docker, a leading containerization tool. **Containerization**
    tools package applications with their dependencies for consistent performance
    across different systems. A containerized approach will help us replicate Google
    Colab’s isolated, uniform environments, ensuring reliability and reducing potential
    compatibility issues in production. The table that follows describes how we can
    map each of the benefits of our prototyping environment to a production analog:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Environment** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | **Prototyping** | **Production** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Package management | Inherent through preinstalled package managers | Docker
    streamlines application deployment and consistency across environments including
    package managers. |'
  prefs: []
  type: TYPE_TB
- en: '| Dependency isolation | Inherent through notebooks | Docker can also ensure
    projects are cleanly isolated. |'
  prefs: []
  type: TYPE_TB
- en: '| Interactive code execution | Inherent through notebooks | Docker helps to
    maintain versions of Python that provide interactive code execution by default.
    However, we may want to connect an **integrated development environment** (**IDE**)
    to our production environment to interact with code remotely as needed. |'
  prefs: []
  type: TYPE_TB
- en: '| Resource accessibility | Inherent for cloud-based notebooks | GPU-enabled
    Docker containers enhance production by enabling structured GPU utilization, allowing
    scalable, efficient model performance. |'
  prefs: []
  type: TYPE_TB
- en: '| Data integration | Not inherent, and requires code-based integration | Integrating
    Docker with a remote data source, such as AWS S3 or Google Cloud Storage, provides
    secure and scalable solutions for importing and exporting data. |'
  prefs: []
  type: TYPE_TB
- en: '| Versioning and collaboration | Inherent through notebooks and preconfigured
    for Git | Integrating Docker with platforms such as GitHub or GitLab enables code
    collaboration and documentation. |'
  prefs: []
  type: TYPE_TB
- en: '| Error handling and debugging | Inherent through direct interactive access
    to runtime | We can embed Python libraries such as `logging` or `Loguru` in Docker
    deployments for enhanced error tracking in production. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: Transitioning features from Colab to production via Docker'
  prefs: []
  type: TYPE_NORMAL
- en: Having mapped out the features of our prototyping environment to corresponding
    tools and practices for a production setup, we are now better prepared to implement
    a generative model for StyleSprint in a production-ready environment. The transition
    entails setting up a stable, scalable, and reproducible Python environment, a
    crucial step for deploying our generative model to automate the generation of
    product descriptions in a real-world setting. As discussed, we can leverage Docker
    in tandem with GitHub and its **continuous integration/continuous deployment**
    (**CI/CD**) capabilities, providing a robust framework for this production deployment.
    A CI pipeline automates the integration of code changes from multiple contributors
    into a shared repository. We pair CI with CD to automate the deployment of our
    code to a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a production-ready environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed how to bridge the gap between prototyping and production
    environments. Cloud-based environments such as Google Colab provide a wealth of
    features that are not inherently available in production. Now that we have a better
    understanding of those characteristics, the next step is to implement a robust
    production setup to ensure that our application can handle real-world traffic,
    scale as needed, and remain stable over time.
  prefs: []
  type: TYPE_NORMAL
- en: The tools and practices in a production environment differ significantly from
    those in a prototyping environment. In production, scalability, reliability, resource
    management, and security become paramount, whereas, in a prototyping environment,
    the models are only relied upon by a few users for experimentation. In production,
    we could expect large-scale consumption from divisions throughout the organization.
    For example, in the StyleSprint scenario, there may be multiple departments or
    sub-brands hoping to automate their product descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: In the early stages of our StyleSprint project, we can use free and open source
    tools such as Docker and GitHub for tasks such as containerization, version control,
    and CI. These tools are offered and managed by a community of users, giving us
    a cost-effective solution. As StyleSprint expands, we might consider upgrading
    to paid or enterprise editions that offer advanced features and professional support.
    For the moment, our focus is on leveraging the capabilities of the open source
    versions. Next, we will walk through the practical implementation of these tools
    step by step. By the end, we will be ready to deploy a production-ready **model-as-a-service**
    (**MaaS**) for automatic product descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Local development setup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We begin by making sure we can connect to a production environment remotely.
    We can leverage an IDE, which is software that enables us to easily organize code
    and remotely connect to the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Studio Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Begin by installing **Visual Studio Code** (**VS Code**), a free code editor
    by Microsoft. It is preferred for its integrated Git control, terminal, and marketplace
    for extensions that enhance its functionality. It provides a conducive environment
    for writing, testing, and debugging code.
  prefs: []
  type: TYPE_NORMAL
- en: Project initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we set up a structured project directory to keep the code modular and
    organized. We will also initialize our working directory with Git, which enables
    us to synchronize code with a remote repository. As mentioned, we leverage Git
    to keep track of code changes and collaborate with others more seamlessly. Using
    the terminal window in Visual Studio, we can initialize the project using three
    simple commands. We use `mkdir` to create or “make” a directory. We use the `cd`
    command to change directories. Finally, we use `git init` to initialize our project
    with Git. Keep in mind that this assumes Git is installed. Instructions to install
    Git are made available on its website ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Docker setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll now move on to setting up a Docker container. A Docker container is an
    isolated environment that encapsulates an application and its dependencies, ensuring
    consistent operation across different systems. For clarity, we can briefly describe
    the key aspects of Docker as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Containers**: These are portable units comprising the application and its
    dependencies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Host operating system’s kernel**: When a Docker container is run on a host
    machine, it utilizes the kernel of the host’s operating system and resources to
    operate, but it does so in a way that is isolated from both the host system and
    other containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dockerfiles**: These are scripts used to create container images. They serve
    as a blueprint containing everything needed to run the application. This isolation
    and packaging method prevents application conflicts and promotes efficient resource
    use, streamlining development and deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A containerized approach will help ensure consistency and portability. For example,
    assume StyleSprint finds a cloud-based hosting provider that is more cost-effective;
    moving to the new provider is as simple as migrating a few configuration files.
  prefs: []
  type: TYPE_NORMAL
- en: We can install Docker from the official website. Docker provides easy-to-follow
    installation guides including support for various programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Once Docker is installed, we can create a Dockerfile in the project directory
    to specify the environment setup. For GPU support, we will want to start from
    an NVIDIA CUDA base image. Docker, like many other virtualized systems, operates
    using a concept called **images**. Images are a snapshot of a preconfigured environment
    that can be used as a starting point for a new project. In our case, we will want
    to start with a snapshot that integrates GPU support using the CUDA library, which
    is a parallel processing library provided by NVIDIA. This library will enable
    the virtualized environment (or container) to leverage any GPUs installed on the
    host machine. Leveraging GPUs will accelerate model inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can go ahead and create a Dockerfile with the specifications for our
    application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This Dockerfile serves as a blueprint that Docker follows to build our container.
    We initiate the process from an official NVIDIA CUDA base image to ensure GPU
    support. The working directory in the container is set to `/app`, where we then
    copy the contents of our project. Following that, we install the necessary packages
    listed in the `requirements.txt` file. `Port 80` is exposed for external access
    to our application. Lastly, we specify the command to launch our application,
    which is running `app.py` using the Python interpreter. This setup encapsulates
    all the necessary components, including GPU support, to ensure our generative
    model operates efficiently in a production-like environment.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also need a method for keeping track of our Python-specific dependencies.
    The container will include Python but will not have any indication as to what
    requirements our Python application has. We can specify those dependencies explicitly
    by defining a `requirements.txt` file in our project directory to list all the
    necessary Python packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Application code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we can create an `app.py` file for our application code. This is where we
    will write the code for our generative model, leveraging libraries such as PyTorch
    and Transformers. To expose our model as a service, we will use *FastAPI*, a modern,
    high-performance framework for building web APIs. A web API is a protocol that
    enables different software applications to communicate and exchange data over
    the internet, allowing them to use each other’s functions and services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet creates a minimal API that will serve the model responses
    whenever another application or software requests the `/generate/` endpoint. This
    will enable StyleSprint to host its model as a web service. This means that other
    applications (e.g., mobile apps, batch processes) can access the model using a
    simple URL. We can also add exception handling to provide an informative error
    message should the model produce any kind of error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a Docker setup, the next step is to deploy the application
    to the host server. We can streamline this process with a CI/CD pipeline. The
    goal is to fully automate all deployment steps, including a suite of tests to
    ensure that any code changes do not introduce any errors. We then leverage GitHub
    Actions to create a workflow that is directly integrated with a code repository.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a code repository
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can leverage the automation capabilities of GitHub, we will need
    a repository. Creating a GitHub repository is straightforward, following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sign up/log in to GitHub**: If you don’t have a GitHub account, sign up at
    [github.com](http://github.com). If you already have an account, just log in.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Go to the repository creation page**: Click the **+** icon in the top-right
    corner of the GitHub home page and select **New repository**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fill in the** **repository details**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Repository Name**: Choose a name for your repository'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description** (optional): Add a brief description of your repository'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visibility**: Select either **Public** (anyone can see this repository) or
    **Private** (only you and the collaborators you invite can see it)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.gitignore` file or choose a license. A `gitignore` file allows us to add
    paths or file types that should not be uploaded to the repository. For example,
    Python creates temporary files that are not critical to the application. Adding
    `` `__pycache__/` `` to the `gitignore` file will automatically ignore all contents
    of that directory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create repository**: Click the **Create** **repository** button.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With our repository setup complete, we can move on to defining our CI/CD pipeline
    to automate our deployments.
  prefs: []
  type: TYPE_NORMAL
- en: CI/CD setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create a pipeline, we will need a configuration file that outlines the stages
    of deployment and instructs the automation server to build and deploy our Docker
    container. Let’s look at the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In our GitHub repository, we can create a new file in the `.github/workflows`
    directory named `ci-cd.yml`. GitHub will automatically find any files in this
    directory to trigger deployments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open `ci-cd.yml` and define the following workflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this setup, our workflow consists of two primary jobs: build-and-test and
    deploy. The build-and-test job is responsible for checking out the code from the
    repository, building the Docker image, and executing any tests. On the other hand,
    the deploy job, which relies on completing build-and-test, handles *DockerHub*
    login and pushes the Docker image there. DockerHub, similar to GitHub, is a repository
    specifically for Docker images.'
  prefs: []
  type: TYPE_NORMAL
- en: For authenticating with DockerHub, it is advised to securely store your DockerHub
    credentials in your GitHub repository. This can be done by navigating to your
    repository on GitHub, clicking on `DOCKER_USERNAME` and `DOCKER_PASSWORD` as new
    repository secrets.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we did not have to perform any additional steps to execute the pipeline.
    The workflow is designed to trigger automatically upon a push (or upload) to the
    main branch. Recall that the entire process relies on the Git pattern where new
    changes are registered through a `commit` or check-in of code and a `push` or
    upload of code changes. Whenever changes are pushed, we can directly observe the
    entire pipeline in action within the **Actions** tab of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: We have now walked through all of the steps necessary to deploy our model to
    production. With all of this critical setup behind us, we can now return to choosing
    the best model for our project. The goal is to find a model that can effectively
    generate captivating product descriptions for StyleSprint. However, the variety
    of generative models available requires a thoughtful choice based on our project’s
    needs and constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we want to choose the right evaluation metrics and discuss other considerations
    that will guide us in making an informed decision for our project. This exploration
    will equip us with the knowledge needed to select a model that not only performs
    well but also aligns with our project objectives and the technical infrastructure
    we have established.
  prefs: []
  type: TYPE_NORMAL
- en: Model selection – choosing the right pretrained generative model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having established a minimal production environment in the previous section,
    we now focus on a pivotal aspect of our project – selecting the right generative
    model for generating engaging product descriptions. The choice of model is crucial
    as it significantly influences the effectiveness and efficiency of our solution.
    The objective is to automate the generation of compelling and accurate product
    descriptions for StyleSprint’s diverse range of retail products. By doing so,
    we aim to enrich the online shopping experience for customers while alleviating
    the manual workload of crafting unique product descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Our objective is to select a generative model that can adeptly handle nuanced
    and sophisticated text generation to significantly expedite the process of creating
    unique, engaging product descriptions, saving time and resources for StyleSprint.
  prefs: []
  type: TYPE_NORMAL
- en: In selecting our model, it is important to thoroughly evaluate various factors
    influencing its performance and suitability for the project.
  prefs: []
  type: TYPE_NORMAL
- en: Meeting project objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we can select and apply evaluation methods to our model selection process,
    we should first make sure we understand the project objectives. This involves
    defining the business problem, identifying any technical constraints, identifying
    any risk associated with the model, including interpretation of model outcomes,
    and ascertaining considerations for any potential disparate treatment or bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem definition**: In our scenario, the goal is to create accurate and
    engaging descriptions for a wide range of retail clothing. As StyleSprint’s product
    range may expand, the system should scale seamlessly to accommodate a larger inventory
    without significantly increasing operational costs. Performance expectations include
    compelling descriptions to attract potential customers, accuracy to avoid misrepresentation,
    and prompt generation to maintain an up-to-date online catalog. Additionally,
    StyleSprint may apply personalized content descriptions based on a user’s shopping
    history. This implies that the model may have to provide product descriptions
    in near-real-time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical constraints**: To maximize efficiency, there should not be any
    noticeable delay (latency) in responses from the model API. The system should
    be capable of real-time updates to the online catalog (as needed), and the hardware
    should support quick text generation without compromising quality while remaining
    cost-effective, especially as the product range expands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency and openness**: Generally, pretrained models from developers
    who disclose architectures and training data sources are preferred, as this level
    of transparency allows StyleSprint to have a clear understanding of any risks
    or legal implications associated with model use. Additionally, any usage restrictions
    imposed by using models provided as APIs, such as request or token limitations,
    should be understood as they could hinder scalability for a growing catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness**: Identifying and mitigating biases in model outputs to
    ensure fair and neutral representations is crucial, especially given StyleSprint’s
    diverse target audience. Ensuring that the generated descriptions are culturally
    sensitive is of paramount importance. Fair representation ensures that the descriptions
    accurately and fairly represent the products to all potential customers, irrespective
    of their individual characteristics or social backgrounds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Suitability of pretraining**: The underlying pretraining of generative models
    plays a significant role in their ability to generate meaningful and relevant
    text. Investigating the domains and data on which the models were pretrained or
    fine-tuned is important. A model pretrained on a broad dataset may be versatile
    but could lack domain-specific nuances. For StyleSprint, a model that is fine-tuned
    on fashion-related data or that has the ability to be fine-tuned on such data
    would be ideal to ensure the generated descriptions are relevant and appealing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantitative metrics**: Evaluating the quality of generated product descriptions
    for StyleSprint necessitates a combination of lexical and semantic metrics. Lexical
    overlap metrics measure the lexical similarity between generated and reference
    texts. Specifically, **Bilingual Evaluation Understudy** (**BLEU**) emphasizes
    n-gram precision, **Recall-Oriented Understudy for Gisting Evaluation** (**ROUGE**)
    focuses on n-gram recall, and **Metric for Evaluation of Translation with Explicit
    Ordering** (**METEOR**) aims for a more balanced evaluation by considering synonyms
    and stemming. For contextual and semantic evaluation, we use similarity metrics
    to assess the semantic coherence and relevance of the generated descriptions,
    often utilizing embeddings to represent text in a way that captures its meaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can further refine our assessment of the alignment between generated descriptions
    and product images using models such as **Contrastive Language-Image Pretraining**
    (**CLIP**). Recall that we used CLIP in [*Chapter 2*](B21773_02.xhtml#_idTextAnchor045)
    to score the compatibility between captions and a synthesized image. In this case,
    we can apply CLIP to measure whether our generated descriptions accurately reflect
    the visual aspects of the products. Collectively, these evaluation techniques
    provide objective methods for assessing the performance of the generative model
    in creating effective product descriptions for StyleSprint:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Qualitative metrics**: We introduce qualitative evaluation to measure nuances
    such as the engaging and creative nature of descriptions. We also want to ensure
    we consider equity and inclusivity in the generated content, which is critical
    to avoid biases or language that could alienate or offend certain groups. Methods
    for engagement assessment could include customer surveys or A/B testing, a systematic
    method for testing two competing solutions. Additionally, having a diverse group
    reviewing the content for equity and inclusivity could provide valuable insights.
    These steps help StyleSprint create captivating, respectful, and inclusive product
    descriptions, fostering a welcoming environment for all customers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: The computational resources required to run a model and the
    model’s ability to scale with increasing data are vital considerations. Models
    that demand extensive computational power may not be practical for real-time generation
    of product descriptions, especially as the product range expands. A balance between
    computational efficiency and output quality is essential to ensure cost-effectiveness
    and scalability for StyleSprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization and fine-tuning capabilities**: The ability to fine-tune or
    customize the model on domain-specific data is crucial for better aligning with
    brand-specific requirements. Exploring the availability and ease of fine-tuning
    can significantly impact the relevance and quality of generated descriptions,
    ensuring that they resonate well with the brand identity and product range of
    StyleSprint. In practice, some models are too large to fine-tune without considerable
    resources, even when efficient methods are applied. We will explore fine-tuning
    considerations in detail in the next chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have carefully considered how we might align the model to the project’s
    goals, we are almost ready to evaluate our initial model selection against a few
    others to ensure we make the right choice. However, before benchmarking, we should
    dedicate time to understanding one vital aspect of the model selection process:
    model size and computational complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Model size and computational complexity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The size of a generative model is often described by the number of parameters
    it has. Parameters in a model are the internal variables that are fine-tuned during
    the training process based on the training data. In the context of neural networks
    used in generative models, parameters typically refer to the weights and biases
    adjusted through training to minimize the discrepancy between predicted outputs
    and actual targets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, a model with more parameters can capture more complex patterns in
    the data, often leading to better performance on the task at hand. While larger
    models often perform better in terms of the quality of the generated text, there’s
    a point of diminishing returns beyond which increasing model size yields marginal
    improvements. Moreover, the increased size comes with its own set of challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational complexity**: Larger models require more computational power
    and memory, during both training and inference (the phase where the model is used
    to make predictions or generate new data based on the learned parameters). This
    can significantly increase the costs and the time required to train and use the
    model, making it less suitable for real-time applications or resource-constrained
    environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The number of parameters significantly impacts the computational complexity
    of a model. Each parameter in a model is a variable that must be stored in memory
    during computation, during both training and inference. Here are some specific
    considerations for computational requirements:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Memory and storage**: The total size of the model in memory is the product
    of the number of parameters and the size of each parameter (typically a 32-bit
    or 64-bit float). For instance, a model with 100 million parameters, each represented
    by a 32-bit float, would require approximately 400 MB of memory (100 million *
    32 bits = 400 million bits = 400 MB). Now consider a larger model, say with 10
    billion parameters; the memory requirement jumps to 40 GB (10 billion * 32 bits
    = 40 billion bits = 40 GB). This requirement is just for the parameters and does
    not account for other data and overheads the model needs for its operations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loading into memory**: When a model is used for inference, its parameters
    must be loaded into the RAM of the machine it’s running on. For a large model
    with 10 billion parameters, you would need a machine with enough RAM to accommodate
    the entire model, along with additional memory for the operational overhead, the
    input data, and the generated output. Suppose the model is too large to fit in
    memory. In that case, it may need to be **sharded** or distributed across multiple
    machines or loaded in parts, which can significantly complicate the deployment
    and operation of the model and also increase the latency of generating outputs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized hardware requirements**: Larger models require specialized hardware,
    such as powerful GPUs or TPUs, which could increase the project costs. As discussed,
    models with a large number of parameters require powerful computational resources
    for both training and inference. Hardware accelerators such as GPUs and TPUs are
    often employed to meet these demands. These hardware accelerators are designed
    to handle the parallel computation capabilities needed for the matrix multiplications
    and other operations inherent in neural network computations, speeding up the
    processing significantly compared to traditional **central processing** **units**
    (**CPUs**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud-based infrastructure can alleviate the complexity of setup but often has
    usage-based pricing. Understanding infrastructure costs on a granular level is
    vital to ensuring that StyleSprint stays within its budget.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Latency**: We’ve briefly discussed latency, but it is important to reiterate
    that larger models typically have higher latency, which could be a problem for
    applications that require real-time responses. In our case, we can process the
    descriptions as batches asynchronously. However, StyleSprint may have projects
    that require fast turnarounds, requiring batches to be completed in hours and
    not days.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of StyleSprint, the trade-off between model performance and size
    must be carefully evaluated to ensure the final model meets the project’s performance
    requirements while staying within budget and hardware constraints. StyleSprint
    was hoping to have near-real-time responses to provide personalized descriptions,
    which typically translates to a smaller model with less computational complexity.
    However, it was also important that the model remains highly accurate and aligns
    with branding standards for tone and voice, which may require a larger model trained
    or fine-tuned on a larger dataset. In practice, we can evaluate the performance
    of models relative to size and complexity through benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Benchmarking is a systematic process used to evaluate the performance of different
    generative models against predefined criteria. This process involves comparing
    the models on various metrics to understand their strengths, weaknesses, and suitability
    for the project. It is an empirical method (based on observation) to obtain data
    on how the models perform under similar conditions, providing insights that can
    inform the decision-making process for model selection.
  prefs: []
  type: TYPE_NORMAL
- en: In the StyleSprint scenario, benchmarking can be an invaluable exercise to navigate
    the trade-offs between model size, computational complexity, and the accuracy
    and creativity of generated descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: For our benchmarking exercise, we can return to our Google Colab prototyping
    environment to quickly load various generative models and run them through tests
    designed to evaluate their performance based on the considerations outlined in
    the previous sections, such as computational efficiency and text generation quality.
    Once we have completed our evaluation and comparison, we can make a few simple
    changes to our production application code and it will automatically redeploy.
    Benchmarking will be instrumental in measuring the quality of the descriptions
    relative to the model size and complexity. Recall that we will measure quality
    and overall model performance along several dimensions, including lexical and
    semantic similarity to a “gold standard” of human-written descriptions, and a
    qualitative assessment performed by a diverse group of reviewers.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to revisit and adapt our original prototyping code to include
    a few challenger models and apply evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Updating the prototyping environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our evaluation steps, there are a few key changes to our original experimentation
    setup in Google Colab. First, we will want to make sure we leverage performance
    acceleration. Google Colab offers acceleration via GPU or TPU environments. For
    this experiment, we will leverage GPU. We will also want to transition from the
    Transformers library to a slightly more versatile library such as Langchain, which
    allows us to test both open source models such as GPT-Neo and commercial models
    such as GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: GPU configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure you have a GPU enabled for better performance. Returning to Google Colab,
    we can follow these steps to enable GPU acceleration:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Runtime** in the top menu (see *Figure 4**.2*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.2: Runtime drop-down menu](img/B21773_04_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Runtime drop-down menu'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Change runtime type** from the drop-down menu, as shown in the preceding
    screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the pop-up window, select **GPU** from the **Hardware accelerator** drop-down
    menu (see *Figure 4**.3*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.3: Select GPU and click on Save](img/B21773_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Select GPU and click on Save'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **Save**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now your notebook is set up to use a GPU to significantly speed up the computations
    needed for the benchmarking process. You can verify the GPU availability using
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This code snippet will return `True` if a GPU is available and `False` otherwise.
    This setup ensures that you have the necessary computational resources to benchmark
    various generative models. The utilization of a GPU will be crucial when it comes
    to handling large models and extensive computations.
  prefs: []
  type: TYPE_NORMAL
- en: Loading pretrained models with LangChain
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In our first simple experiment, we relied on the Transformers library to load
    an open source version of GPT. However, for our benchmarking exercise, we want
    to evaluate the retail version of GPT-3 alongside open source models. We can leverage
    LangChain, a versatile library that provides a streamlined interface, to access
    both open source models from providers such as Hugging Face and closed source
    models such as OpenAI’s GPT-3.5\. LangChain offers a unified API that simplifies
    benchmarking and comparison through standardization. Here are the steps to do
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Install necessary libraries**: We begin by installing the required libraries
    in our Colab environment. LangChain simplifies the interaction with models hosted
    on OpenAI and Hugging Face.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Set up credentials**: We obtain the credentials from OpenAI for accessing
    GPT-3, GPT-4, or whichever closed source model we select. We also provide credentials
    for the Hugging Face Hub, which hosts over 350,000 open source models. We must
    store these credentials securely to prevent any unauthorized access, especially
    in the case where model usage has an associated cost.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Load models**: With LangChain, we can quickly load models and generate responses.
    The following example demonstrates how to load GPT-3 and GPT-Neo from Hugging
    Face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we have loaded two models that are significantly different in size.
    As the model signature suggests, GPT-Neo was trained on 2.7 billion parameters.
    Meanwhile, according to information available from OpenAI, Davinci was trained
    on 175 billion parameters. As discussed, a model that is significantly larger
    is expected to have captured much more complex patterns and will likely outperform
    a smaller model. However, these very large models are typically hosted by major
    providers and have higher usage costs. We will revisit cost considerations later.
    For now, we can continue to the next step, which is to prepare our testing data.
    Our test data should provide a baseline for model performance that will inform
    the cost versus performance trade-off.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up testing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this context, testing data should comprise product attributes from the StyleSprint
    website (e.g., available colors, sizes, materials, etc.) and existing product
    descriptions written by the StyleSprint team. The human-written descriptions serve
    as the “ground truth,” or the standard against which to compare the models’ generated
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: We can gather product data from existing datasets by scraping data from e-commerce
    websites or using a pre-collected dataset from StyleSprint’s database. We should
    also ensure a varied collection of products to test a model’s capability across
    different categories and styles. The process of dividing data into distinct groups
    or segments based on shared characteristics is typically referred to as segmentation.
    Understanding a model’s behavior across segments should give us an indication
    of how well it can perform across the entire family of products. For the purposes
    of this example, product data is made available in the GitHub companion to this
    book ([https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can extract relevant information for further processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We must also format the product data in a way that makes it ready to be input
    into the models for description generation. This could be just the product title
    or a combination of product attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will ask the model to generate a batch of product descriptions using
    each model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Now, with the testing data set up, we have a structured dataset of product information,
    reference descriptions, and images ready for use in the evaluation steps.
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative metrics evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have leveraged Langchain to load multiple models and prepared testing
    data, we are ready to begin applying evaluation metrics. These metrics capture
    accuracy and alignment with product images and will help us assess how well the
    models generate product descriptions compared to humans. As discussed, we focused
    on two categories of metrics, lexical and semantic similarity, which provide a
    measure of how many of the same words were used and how much semantic information
    is common to both the human and AI-generated product descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code block, we apply `BLEU`, `ROUGE`, and `METEOR` to evaluate
    the lexical similarity between the generated text and the reference text. Each
    of these has a reference-based assumption. This means that each metric assumes
    we are comparing against a human reference. We have already set aside our reference
    descriptions (or gold standard) for a diverse set of products to compare side-by-side
    with the generated descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can evaluate the semantic coherence and relevance of the generated descriptions
    using sentence embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Alignment with CLIP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We again leverage the CLIP model to evaluate the alignment between generated
    product descriptions and corresponding images, similar to our approach in [*Chapter
    2*](B21773_02.xhtml#_idTextAnchor045). The CLIP model, adept at correlating visual
    and textual content, scores the congruence between each product image and its
    associated generated and reference descriptions. The reference description serves
    as a human baseline for accuracy. These scores provide a quantitative measure
    of our generative model’s effectiveness at producing descriptions that correspond
    well to the product image. The following is a snippet from a component that processes
    the generated descriptions combined with corresponding images to generate a CLIP
    score. The full component code (including image pre-processing) is available in
    the `chapter 4` folder of this book’s GitHub repository at [https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: In evaluating product descriptions using the CLIP model, the alignment scores
    generated for each image-description pair are computed relative to other descriptions
    in the batch. Essentially, CLIP assesses how well a specific description (either
    generated or reference) aligns with a given image compared to other descriptions
    within the same batch. For example, a score of 33.79 indicates that the description
    aligns with the image 33.79% better than the other descriptions in the batch align
    with that image. In comparing against the reference, we expect that the scores
    based on the generated descriptions should align closely with the scores based
    on the reference descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have calculated lexical and semantic similarity to the reference
    scores, and alignment between images and generated descriptions relative to reference
    descriptions, we can evaluate our models holistically and interpret the outcome
    of our quantitative evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting outcomes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin with lexical similarity, which gives us an indication of similarity
    in phrasing and keywords between the reference and generated descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **BLEU** | **ROUGE** | **METEOR** |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 0.147 | 0.094 | 0.261 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-Neo | 0.132 | 0.05 | 0.059 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.2: Lexical similarity'
  prefs: []
  type: TYPE_NORMAL
- en: 'In evaluating text generated by GPT-3.5 and GPT-Neo models, we use several
    lexical similarity metrics: BLEU, ROUGE, and METEOR. BLEU scores, which assess
    the precision of matching phrases, show GPT-3.5 (0.147) slightly outperforming
    GPT-Neo (0.132). ROUGE scores, focusing on the recall of content, indicate that
    GPT-3.5 (0.094) better captures reference content than GPT-Neo (0.05). METEOR
    scores, combining both precision and recall with synonym matching, reveal a significant
    lead for GPT-3.5 (0.261) over GPT-Neo (0.059). Overall, these metrics suggest
    that GPT-3.5’s generated text aligns more closely with reference standards, both
    in word choice and content coverage, compared to that of GPT-Neo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we evaluate semantic similarity, which measures how closely the meanings
    of the generated text align with the reference text. This assessment goes beyond
    mere word-to-word matching and considers the context and overall intent of the
    sentences. Semantic similarity evaluates the extent to which the generated text
    captures the nuances, concepts, and themes present in the reference text, providing
    insight into the model’s ability to understand and replicate deeper semantic meanings:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Mean** **cosine similarity** |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 0.8192 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-Neo | 0.2289 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.3: Semantic similarity'
  prefs: []
  type: TYPE_NORMAL
- en: The mean cosine similarity scores reveal a stark contrast between the two models’
    performance in semantic similarity. GPT-3.5 shows a high degree of semantic alignment
    with the reference text. GPT-Neo’s significantly lower score suggests a relatively
    poor performance, indicating that the generated descriptions were fundamentally
    dissimilar to descriptions written by humans.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we review the CLIP scores, which tell us how well the generated descriptions
    align visually with the corresponding images. These scores, derived from a model
    trained to understand and correlate visual and textual data, provide a measure
    of the relevance and accuracy of the text in representing the visual content.
    High CLIP scores indicate a strong correlation between the text and the image,
    suggesting that the generated descriptions are not only textually coherent but
    also contextually appropriate and visually descriptive:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Mean CLIP** | **Reference delta** |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 26.195 | 2.815 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-Neo | 22.647 | 6.363 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.4: Comparative CLIP score analysis for GPT-3.5 and GPT-Neo models'
  prefs: []
  type: TYPE_NORMAL
- en: We calculated the CLIP scores from the reference descriptions, which represent
    the average alignment score between a set of benchmark descriptions and the corresponding
    images. We then calculated CLIP scores for each model and analyzed the delta.
    In concert with our other metrics, GPT-3.5 has a clear advantage over GPT-Neo,
    aligning more closely with the reference.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, GPT-3.5 appears to significantly outperform GPT-Neo across all quantitative
    measures. However, it is worth noting that GPT-3.5 incurs a higher cost and generally
    has a higher latency than GPT-Neo. In this case, the StyleSprint team would conduct
    a qualitative analysis to accurately determine whether the GPT-Neo descriptions
    do not align with brand guidelines and expectations, therefore making the cost
    of using the better model worthwhile. As discussed, the trade-off here is not
    clear-cut. StyleSprint must carefully consider that although using a commodity
    such as GPT-3.5 does not incur computational costs directly, on-demand costs could
    increase significantly as model usage rises.
  prefs: []
  type: TYPE_NORMAL
- en: The contrasting strengths of the two models pose a decision-making challenge.
    While one clearly excels in performance metrics and alignment with CLIP, implying
    higher accuracy and semantic correctness, the other is significantly more resource-efficient
    and scalable, which is crucial for cost-effectiveness. At this stage, it becomes
    critical to assess model outcomes qualitatively and to engage stakeholders to
    help understand organizational priorities.
  prefs: []
  type: TYPE_NORMAL
- en: With these considerations in mind, we’ll revisit qualitative considerations
    such as transparency, bias, and fairness and how they play into the broader picture
    of deploying a responsible and effective AI system.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing implicit or covert societal biases in AI systems is crucial to ensure
    responsible AI deployment. Although it may not seem obvious how a simple product
    description could introduce bias, the language used can inadvertently reinforce
    stereotypes or exclude certain groups. For instance, descriptions that consistently
    associate certain body types or skin tones with certain products or that unnecessarily
    default to gendered language can unintentionally perpetuate societal biases. However,
    with a structured mitigation approach, including algorithmic audits, increased
    model transparency, and stakeholder engagement, StyleSprint can make sure its
    brand promotes equity and inclusion.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing and mitigating biases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We present several considerations, as suggested by Costanza-Chock et al. in
    *Who Audits the Auditors? Recommendations from a field scan of the algorithmic*
    *auditing ecosystem*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Professional** **environment examination**: Creating a supportive professional
    environment is crucial for addressing algorithmic fairness. Implementing whistleblower
    protections facilitates the safe reporting of biases and unfair practices while
    establishing processes for individuals to report harms to ensure these concerns
    are addressed proactively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom versus standardized audit frameworks**: While custom audit frameworks
    are expected, considering standardized methods may enhance rigor and transparency
    in bias mitigation efforts. Engaging with external auditing entities could offer
    unbiased evaluations of StyleSprint’s AI systems, aligning with the observations
    by Costanza-Chock et al. (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Focusing on equity, not just equality**: Equity notions acknowledge differing
    needs, essential for a comprehensive approach to fairness. Performing intersectional
    and small population analyses could help you to understand and address biases
    beyond legally protected classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disclosure and transparency**: Disclosing audit methods and outcomes can
    foster a culture of transparency and continuous improvement. Officially released
    audits could help you establish best practices and gain stakeholder trust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mixed methods analyses**: As presented, a mix of technical and qualitative
    analyses could provide a holistic view of the system’s fairness. Engaging non-technical
    stakeholders could emphasize qualitative analyses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Community and stakeholder engagement**: Again, involving diverse groups and
    domain experts in audits could ensure diverse perspectives are considered in bias
    mitigation efforts. Establishing feedback loops with stakeholders could facilitate
    continuous improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous learning and improvement**: Staying updated on emerging standards
    and best practices regarding AI fairness is crucial for continuous improvement.
    Fostering a culture of learning could help in adapting to evolving fairness challenges
    and regulatory landscapes, thus ensuring StyleSprint’s AI systems remain fair
    and responsible over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transparency and explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generally, explainability in machine learning refers to the ability to understand
    the internal mechanics of a model, elucidating how it makes decisions or predictions
    based on given inputs. However, achieving explainability in generative models
    can be much more complex. As discussed, unlike discriminative machine learning
    models, generative models do not have the objective of learning a decision boundary,
    nor do they reflect a clear notion of features or a direct mapping between input
    features and predictions. This absence of feature-based decision-making makes
    traditional explainability techniques ineffective for generative foundational
    models such as GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can adopt some pragmatic transparency practices, such as clear
    documentation made accessible to all relevant stakeholders, to foster a shared
    understanding and expectations regarding the model’s capabilities and usage.
  prefs: []
  type: TYPE_NORMAL
- en: The topic of explainability is a critical space to watch, especially as generative
    models become more complex and their outcomes become increasingly more difficult
    to rationalize, which may present unknown risk implications.
  prefs: []
  type: TYPE_NORMAL
- en: Promising research from Anthropic, OpenAI, and others suggests that sparse autoencoders—neural
    networks that activate only a few neurons at a time—could facilitate the identification
    of abstract and understandable patterns. This method could help explain the network's
    behavior by highlighting features that align with human concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Final deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assuming we have carefully gathered quantitative and qualitative feedback regarding
    the best model for the job, we can select our model and update our production
    environment to deploy and serve it. We will continue to use FastAPI for creating
    a web server to serve our model, and Docker to containerize our application. However,
    now that we have been introduced to the simplicity of LangChain, we will continue
    to leverage its simplified interface. Our existing CI/CD pipeline will ensure
    streamlined automatic deployment and continuous application monitoring. This means
    that deploying our model is as simple as checking-in our latest code. We begin
    with updating our dependencies list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requirements.txt` file in your project to include the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Update the Dockerfile**: Modify your Dockerfile to ensure it installs the
    updated requirements and properly sets up the environment for running LangChain
    with FastAPI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Update the FastAPI application**: Modify your FastAPI application to utilize
    Langchain for interacting with GPT-3.5\. Ensure your OpenAI API key is securely
    stored and accessible to your application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Testing and monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the model is deployed, perform necessary tests to ensure the setup works
    as expected. Continue to monitor the system’s performance, errors, and other critical
    metrics to ensure reliable operation.
  prefs: []
  type: TYPE_NORMAL
- en: By this point, we have updated our production environment to deploy and serve
    GPT-3.5, facilitating the generation of text based on the prompts received via
    the FastAPI application. This setup ensures a scalable, maintainable, and secure
    deployment of our new generative model. However, we should also explore some best
    practices regarding application reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Maintenance and reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Maintaining reliability in our StyleSprint deployment is critical. As we employ
    Langchain with FastAPI, Docker, and CI/CD, it’s essential to set up monitoring,
    alerting, automatic remediation, and failover mechanisms. This section outlines
    a possible approach to ensure continuous operation and robustness in our production
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring tools**: Integrate monitoring tools within the CI/CD pipeline
    to continuously track system performance and model metrics. This step is fundamental
    for identifying and rectifying issues proactively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alerting mechanisms**: Establish alerting mechanisms to notify the maintenance
    team whenever anomalies or issues are detected. Tuning the alerting thresholds
    accurately is crucial to catch issues early and minimize false alarms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automatic remediation**: Utilize Kubernetes’ self-healing features and custom
    scripts triggered by certain alerts for automatic remediation. This setup aims
    to resolve common issues autonomously, reducing the need for human intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failover mechanisms**: Implement a failover mechanism by setting up secondary
    servers and databases. In case of primary server failure, these secondary setups
    take over to ensure continuous service availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular updates via CI/CD**: Employ the CI/CD pipeline for managing, testing,
    and deploying updates to LangChain, FastAPI, or other components of the stack.
    This process keeps the deployment updated and secure, reducing the maintenance
    burden significantly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By meticulously addressing each of these areas, you’ll be laying down a solid
    foundation for a reliable and maintainable StyleSprint deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter outlined the process of transitioning the StyleSprint generative
    AI prototype to a production-ready deployment for creating engaging product descriptions
    on an e-commerce platform. It started with setting up a robust Python environment
    using Docker, GitHub, and CI/CD pipelines for efficient dependency management,
    testing, and deployment. The focus then shifted to selecting a suitable pretrained
    model, emphasizing alignment with project goals, computational considerations,
    and responsible AI practices. This selection relied on both quantitative benchmarking
    and qualitative evaluation. We then outlined the deployment of the selected model
    using FastAPI and LangChain, ensuring a scalable and reliable production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Following the strategies outlined in this chapter will equip teams with the
    necessary insights and steps to successfully transition their generative AI prototype
    into a maintainable and value-adding production system. In the next chapter, we
    will explore fine-tuning and its importance in LLMs. We will also weigh in on
    the decision-making process, addressing when it is more beneficial to fine-tune
    versus zero or few-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Practical Applications of Generative AI'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part focuses on the practical applications of generative AI, including
    fine-tuning models for specific tasks, understanding domain adaptation, mastering
    prompt engineering, and addressing ethical considerations. It aims to provide
    hands-on insights and methodologies for effectively implementing and leveraging
    generative AI in various contexts with a focus on responsible adoption.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B21773_05.xhtml#_idTextAnchor180), *Fine-Tuning Generative Models
    for Specific Tasks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B21773_06.xhtml#_idTextAnchor211), *Understanding Domain Adaptation
    for Large Language Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B21773_07.xhtml#_idTextAnchor225), *Mastering the Fundamentals
    of Prompt Engineering*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B21773_08.xhtml#_idTextAnchor251), *Addressing Ethical Considerations
    and Charting a Path toward Trustworthy Generative AI*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
