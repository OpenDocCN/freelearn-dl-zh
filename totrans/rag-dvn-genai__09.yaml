- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Empowering AI Models: Fine-Tuning RAG Data and Human Feedback'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An organization that continually increases the volume of its RAG data will reach
    the threshold of non-parametric data (not pretrained on an LLM). At that point,
    the mass of RAG data accumulated might become extremely challenging to manage,
    posing issues related to storage costs, retrieval resources, and the capacity
    of the generative AI models themselves. Moreover, a pretrained generative AI model
    is trained up to a cutoff date. The model ignores new knowledge starting the very
    next day. This means that it will be impossible for a user to interact with a
    chat model on the content of a newspaper edition published after the cutoff date.
    That is when retrieval has a key role to play in providing RAG-driven content.
  prefs: []
  type: TYPE_NORMAL
- en: Companies like Google, Microsoft, Amazon, and other web giants may require exponential
    data and resources. Certain domains, such as the legal rulings in the United States,
    may indeed require vast amounts of data. However, this doesn’t apply to a wide
    range of domains. Many corporations do not need to maintain such large datasets,
    and in some cases, large portions of static data—like those in hard sciences—can
    remain stable for a long time. Such static data can be fine-tuned to reduce the
    volume of RAG data required.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, therefore, we will first examine the architecture of RAG data
    reduction through fine-tuning. We will focus on a dataset that contains ready-to-use
    documents but also stresses the human-feedback factor. We will demonstrate how
    to transform non-parametric data into parametric, fine-tuned data in an OpenAI
    model. Then, we will download and prepare the dataset from the previous chapter,
    converting the data into well-formatted prompt and completion pairs for fine-tuning
    in JSONL. We will fine-tune a cost-effective OpenAI model, `GPT-4o-mini`, which
    will prove sufficient for the completion task we will implement. Once the model
    is fine-tuned, we will test it on our dataset to verify that it has successfully
    taken our data into account. Finally, we will explore OpenAI’s metrics interface,
    which enables us to monitor our technical metrics, such as accuracy and usage
    metrics, to assess the cost-effectiveness of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The limits of managing RAG data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The challenge of determining what data to fine-tune
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a JSON dataset for fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running OpenAI’s processing tool to produce a JSONL dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning an OpenAI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing the fine-tuning processing time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running the fine-tuned model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by defining the architecture of the fine-tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of fine-tuning static RAG data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we question the usage of non-parametric RAG data when it exceeds
    a manageable threshold, as described in the *RAG versus fine-tuning* section in
    *Chapter 1*, *Why Retrieval Augmented Generation?*, which stated the principle
    of a threshold. *Figure 9.1* adapts the principle to this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a diagram of a heater  Description automatically generated](img/B31169_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Fine-tuning threshold reached for RAG data'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice that the processing (**D2**) and storage (**D3**) thresholds have been
    reached for static data versus the dynamic data in the RAG data environment. The
    threshold depends on each project and parameters such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The volume of RAG data to process**: Embedding data requires human and machine
    resources. Even if we don’t embed the data, piling up static data (data that is
    stable over a long period of time) makes no sense.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The volume of RAG data to store and retrieve**: At some point, if we keep
    stacking data up, much of it may overlap.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The retrievals require resources**: Even if the system is open source, there
    is still an increasing number of resources to manage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other factors, too, may come into play for each project. Whatever the reason,
    fine-tuning can be a good solution when we reach the RAG data threshold.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG ecosystem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will return to the RAG ecosystem described in *Chapter
    1*. We will focus on the specific components we need for this chapter. The following
    figure presents the fine-tuning components in color and the ones we will not need
    in gray:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a diagram  Description automatically generated](img/B31169_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Fine-tuning components of the RAG ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: 'The key features of the fine-tuning ecosystems we will build can be summarized
    in the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collecting (D1) and preparing (D2) the dataset**: We will download and process
    the human-crafted crowdsourced SciQ hard science dataset we implemented in the
    previous chapter: [https://huggingface.co/datasets/sciq](https://huggingface.co/datasets/sciq).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human feedback (E2)**: We can assume that human feedback played an important
    role in the SciQ hard science dataset. The dataset was controlled by humans and
    updated so we can think of it as a simulation of how reliable human feedback can
    be fine-tuned to alleviate the volume of RAG datasets. We can go further and say
    it is possible that, in real-life projects, the explanations present in the SciQ
    dataset can sometimes come from human evaluations of models, as we explored in
    *Chapter 5*, *Boosting RAG Performance with Expert Human Feedback*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tuning (T2)**: We will fine-tune a cost-effective OpenAI model, `GPT-4o-mini`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering (G3) and generation and output (G4)**: We will engineer
    the prompts as recommended by OpenAI and display the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metrics (E1)**: We will look at the main features of OpenAI’s Metrics interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now go to our keyboards to collect and process the SciQ dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing an environment has become complex with the rapid evolution of AI
    and cross-platform dependency conflicts, as we saw in *Chapter 2*, *RAG Embedding
    Vector Stores with Deep Lake and OpenAI*, in the *Setting up the environment*
    section. We will thus freeze the package versions when possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this program, open the `Fine_tuning_OpenAI_GPT_4o_mini.ipynb` notebook
    in the `Chapter09` directory on GitHub. The program first retrieves the OpenAI
    API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We then install `openai` and set the API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we install `jsonlines` to generate JSONL data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now install `datasets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Read the *Installing the environment* section of *Chapter 8*, *Dynamic RAG with
    Chroma and Hugging Face Llama*, for explanations of the dependency conflicts involved
    when installing `datasets`.
  prefs: []
  type: TYPE_NORMAL
- en: Some issues with the installation may occur but the dataset will be downloaded
    anyway. We must expect and accept such issues as the leading platforms continually
    update their packages and create conflicts with pre-installed environments such
    as Google Colab. You can create a special environment for this program. Bear in
    mind that your other programs might encounter issues due to other package constraints.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to prepare the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Preparing the dataset for fine-tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fine-tuning an OpenAI model requires careful preparation; otherwise, the fine-tuning
    job will fail. In this section, we will carry out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the dataset from Hugging Face and prepare it by processing its columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stream the dataset to a JSON file in JSONL format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The program begins by downloading the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Downloading and visualizing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will download the SciQ dataset we embedded in *Chapter 8*. As we saw, embedding
    thousands of documents takes time and resources. In this section, we will download
    the dataset, but this time, *we will not embed it*. We will let the OpenAI model
    handle that for us while fine-tuning the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program downloads the same Hugging Face dataset as in *Chapter 8* and filters
    the training portion of the dataset to include only non-empty records with the
    correct answer and support text to explain the answer to the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code then prints the number of filtered questions with support
    text. The output shows that we have a subset of 10,481 records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will load the dataset to a DataFrame and drop the distractor columns
    (those with wrong answers to the questions):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output displays the three columns we need:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Output displaying three columns'
  prefs: []
  type: TYPE_NORMAL
- en: We need the question that will become the prompt. The `correct_answer` and `support`
    columns will be used for the completion. Now that we have examined the dataset,
    we can stream the dataset directly to a JSON file.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Preparing the dataset for fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train the completion model we will use, we need to write a JSON file in the
    very precise JSONL format as required.
  prefs: []
  type: TYPE_NORMAL
- en: We download and process the dataset in the same way as we did to visualize it
    in the *1.1\. Downloading and visualizing the dataset* section, which is recommended
    to check the dataset before fine-tuning it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now write the messages for GPT-4o-mini in JSONL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We first define the detailed answer (`detailed_answer`) with the correct answer
    (`'correct_answer'`) and a supporting (`support`) explanation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we define the messages (`messages`) for the GPT-4o-mini model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{"role": "system", "content": ...}`: This sets the initial instruction for
    the language model, telling it to provide detailed answers to science questions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{"role": "user", "content": row[''question'']}`: This represents the user
    asking a question, taken from the `question` column of the DataFrame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`{"role": "assistant", "content": detailed_answer}`: This represents the assistant’s
    response, providing the detailed answer constructed earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now write our JSONL dataset to a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We have given the OpenAI model a structure it expects and has been trained
    to understand. We can load the JSON file we just created in a pandas DataFrame
    to verify its content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following excerpt of the file shows that we have successfully prepared
    the JSON file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31169_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: File excerpt'
  prefs: []
  type: TYPE_NORMAL
- en: That’s it! We are now ready to run a fine-tuning job.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Fine-tuning the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train the model, we retrieve our training file and create a fine-tuning
    job. We begin by creating an OpenAI client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we use the file we generated to create another training file that is uploaded
    to OpenAI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We print the file information for the dataset we are going to use for fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create and display the fine-tuning job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output first provides the name of the file, its purpose, its status, and
    the OpenAI name of the file ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The code displays the details of the fine-tuning job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides the details we need to monitor the job. Here is a brief
    description of some of the key-value pairs in the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Job ID`: `ftjob-O1OEE7eEyFNJsO2Eu5otzWA8`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Status`: `validating_files`. This means OpenAI is currently checking the training
    file to make sure it’s suitable for fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Model`: `gpt-4o-mini-2024-07-18`. We’re using a smaller, more cost-effective
    version of GPT-4 for fine-tuning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Training File`: `file-EUPGmm1yAd3axrQ0pyoeAKuE`. This is the file we’ve provided
    that contains the examples to teach the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some key hyperparameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_epochs`: `''auto''`: OpenAI will automatically determine the best number
    of training cycles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size`: `''auto''`: OpenAI will automatically choose the optimal batch
    size for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate_multiplier`: `''auto''`: OpenAI will automatically adjust the
    learning rate during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Created` `at`: `2024-06-30 08:20:50`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This information will prove useful if you wish to perform an in-depth study
    of fine-tuning OpenAI models. We can also use it to monitor and manage our fine-tuning
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Monitoring the fine-tunes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will extract the minimum information we need to monitor
    the jobs for all our fine-tunes. We will first query OpenAI to obtain the three
    latest fine-tuning jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We then initialize the lists of information we want to visualize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Following that, we iterate through `response` to retrieve the information we
    need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create a DataFrame with the information we extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we convert the timestamps to readable format and display the list
    of fine-tunes and their status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides a monitoring dashboard of the list of our jobs, as shown
    in *Figure 9.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Job list in the pandas DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that for job `0`, the status of the task is `running`. The status
    informs you of the different steps of the process such as validating the files,
    running, failed, or succeeded. In this case, the fine-tuning process is running.
    If you refresh this cell regularly, you will see the status.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now retrieve the most recent model trained for the `Fine-Tuned Model`
    column. If the training fails, this column will be empty. If not, we can retrieve
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will display the name of the latest fine-tuned model if there is
    one or inform us that no fine-tuned model is found. In this case, GPT-4o-mini
    was successfully trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: If a fine-tuned model is found, `generation=True`, it will trigger the OpenAI
    completion calls in the following cells. If no model is found, `generation=False`,
    it will not run the OpenAI API in the rest of the notebook to avoid using models
    that you are not training. You can set generation to `True` in a new cell and
    then select any fine-tuned model you wish.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the training job can take a while. You can refresh the pandas DataFrame
    from time to time. You can write code that checks the status of another job and
    waits for a name to appear for your training job or an error message. You can
    also wait for OpenAI to send you an email informing you that the training job
    is finished. If the training job fails, we must verify our training data for any
    inconsistencies, missing values, or incorrect labels. Additionally, ensure that
    the JSON file format adheres to OpenAI’s specified schema, including correct field
    names, data types, and structure.
  prefs: []
  type: TYPE_NORMAL
- en: Once the training job is finished, we can run completion tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Using the fine-tuned OpenAI model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to use our fine-tuned OpenAI `GPT-4o-mini` model. We will
    begin by defining a prompt based on a question taken from our initial dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal is to verify whether the dataset has been properly trained and will
    produce results similar to the completions we defined. We can now run the fine-tuned
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the request must fit our scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model=first_non_empty_model` is our pretrained model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prompt=prompt` is our predefined prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temperature=0.0` is set to a low value because we do not want any “creativity”
    for this hard science completion task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we run the request, we can format and display the response. The following
    code contains two cells to display and extract the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can print the raw response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output contains the response and information on the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We then extract the text of the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is a string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can format the response string into a nice paragraph with the Python
    wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that our data has been taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the initial completion for our prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of text  Description automatically generated](img/B31169_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Initial completion'
  prefs: []
  type: TYPE_NORMAL
- en: The response is thus satisfactory. This might not always be the case and might
    require more work on the datasets (better data, large volumes of data, etc.) incrementally
    until you have reached a satisfactory goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can save the name of your model in a text file or anywhere you wish. You
    can now run your model in another program using the name of your trained model,
    or you can reload this notebook at any time:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the `Installing the environment` section of this notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a prompt of your choice related to the dataset we trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of your model in the OpenAI completion request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the request and analyze the response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can consult OpenAI’s fine-tuning documentation for further information
    if necessary: [https://platform.openai.com/docs/guides/fine-tuning/fine-tuning](https://platform.openai.com/docs/guides/fine-tuning/fine-tuning).'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI provides a user interface to analyze the metrics of the training process
    and model. You can access the metrics related to your fine-tuned models at [https://platform.openai.com/finetune/](https://platform.openai.com/finetune/).
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface displays the list of your fine-tuned jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a phone  Description automatically generated](img/B31169_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: List of fine-tuned jobs'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose to view all the fine-tuning jobs, the ones that were successful,
    or the ones that failed. If we choose a job that was successful, for example,
    we can view the job details as shown in the following excerpt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Example view'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through the information provided in this figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Status**: Indicates the status of the fine-tuning process. In this case,
    we can see that the process was completed successfully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job ID**: A unique identifier for the fine-tuning job. This can be used to
    reference the job in queries or for support purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Base model**: Specifies the pretrained model used as the starting point for
    fine-tuning. In this case, `gpt-4o-mini` is a version of OpenAI’s models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output model**: This is the identifier for the model resulting from the fine-tuning.
    It incorporates changes and optimizations based on the specific training data
    provided.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Created at**: The date and time when the fine-tuning job was initiated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trained tokens**: The total number of tokens (pieces of text, such as words
    or punctuation) that were processed during training. This metric helps gauge the
    extent of training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epochs**: The number of complete passes the training data went through during
    fine-tuning. More epochs can lead to better learning but too many may lead to
    overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: The number of training examples utilized in one iteration of
    model training. Smaller batch sizes can offer more updates and refined learning
    but may take longer to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LR multiplier**: This refers to the learning rate multiplier, affecting how
    much the learning rate for the base model is adjusted during the fine-tuning process.
    A smaller multiplier can lead to smaller, more conservative updates to model weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seed**: A seed for the random number generator used in the training process.
    Providing a seed ensures that the training process is reproducible, meaning you
    can get the same results with the same input conditions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This information will help tailor the fine-tuning jobs to meet the specific
    needs of a project and explore alternative approaches to optimization and customization.
    In addition, the interface contains more information that we can explore to get
    an in-depth vision of the fine-tuning process. If we scroll down on the **Information**
    tab of our model, we can see metrics as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A green line graph with numbers  Description automatically generated](img/B31169_09_09-01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Metrics for a fine-tuned model'
  prefs: []
  type: TYPE_NORMAL
- en: Training loss and the other available information can guide our training strategies
    (data, files, and parameters).
  prefs: []
  type: TYPE_NORMAL
- en: '**Training loss** is a reliable metric used to evaluate the performance of
    a machine learning model during training. In this case, `Training loss (1.1570)`
    represents the model’s average error on the training dataset. Lower training loss
    values indicate that the model is better fitting the training data. A training
    loss of `1.1570` suggests that the model has learned to predict or classify its
    training data well during the fine-tuning process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also examine these values with the `Time` and `Step` information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a phone  Description automatically generated](img/B31169_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Training loss during the training job'
  prefs: []
  type: TYPE_NORMAL
- en: We must also measure the usage to monitor the cost per period and model. OpenAI
    provides a detailed interface at [https://platform.openai.com/usage](https://platform.openai.com/usage).
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning can indeed be an effective way to optimize RAG data if we make sure
    to train a model with high-quality data and the right parameters. Now, it’s time
    for us to summarize our journey and move to our next RAG-driven generative AI
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter’s goal was to show that as we accumulate RAG data, some data is
    dynamic and requires constant updates, and as such, cannot be fine-tuned easily.
    However, some data is static, meaning that it will remain stable for long periods
    of time. This data can become parametric (stored in the weights of a trained LLM).
  prefs: []
  type: TYPE_NORMAL
- en: We first downloaded and processed the SciQ dataset, which contains hard science
    questions. This stable data perfectly suits fine-tuning. It contains a question,
    answer, and support (explanation) structure, which makes the data effective for
    fine-tuning. Also, we can assume human feedback was required. We can even go as
    far as imagining this feedback could be provided by analyzing generative AI model
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: We converted the data we prepared into prompts and completions in a JSONL file
    following the recommendations of OpenAI’s preparation tool. The structure of JSONL
    was meant to be compatible with a completion model (prompt and completion) such
    as `GPT-4o-mini`. The program then fine-tuned the cost-effective `GPT-4o-mini`
    OpenAI model, following which we ran the model and found that the output was satisfactory.
    Finally, we explored the metrics of the fine-tuned model in the OpenAI metrics
    user interface.
  prefs: []
  type: TYPE_NORMAL
- en: We can conclude that fine-tuning can optimize RAG data in certain cases when
    necessary. However, we will take this process further in the next chapter, *Chapter
    10*, *RAG for Video Stock Production with Pinecone and OpenAI*, when we run the
    full-blown RAG-driven generative AI ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with yes or no:'
  prefs: []
  type: TYPE_NORMAL
- en: Do all organizations need to manage large volumes of RAG data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the GPT-4o-mini model described as insufficient for fine-tuning tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can pretrained models update their knowledge base after the cutoff date without
    retrieval systems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it the case that static data never changes and thus never requires updates?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is downloading data from Hugging Face the only source for preparing datasets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is all RAG data eventually embedded into the trained model’s parameters according
    to the document?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the chapter recommend using only new data for fine-tuning AI models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the OpenAI Metrics interface used to adjust the learning rate during model
    training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can the fine-tuning process be effectively monitored using the OpenAI dashboard?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is human feedback deemed unnecessary in the preparation of hard science datasets
    such as SciQ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenAI fine-tuning documentation: [https://platform.openai.com/docs/guides/fine-tuning/](https://platform.openai.com/docs/guides/fine-tuning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI pricing: [https://openai.com/api/pricing/](https://openai.com/api/pricing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Test of Fine-Tuning GPT by Astrophysical Data* by Yu Wang et al. is an interesting
    article on fine-tuning hard science data, which requires careful data preparation:
    [https://arxiv.org/pdf/2404.10019](https://arxiv.org/pdf/2404.10019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
