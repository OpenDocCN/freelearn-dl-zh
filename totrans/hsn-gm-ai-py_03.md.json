["```py\ndef Fibonacci(n): \n    if n<0: \n        print(\"Outside bounds\")\n    elif n==1:\n        return 0 # n==1, returns 0  \n    elif n==2: \n        return 1 # n==2, returns 1\n    else: \n        return Fibonacci(n-1)+Fibonacci(n-2) \nprint(Fibonacci(9))\n```", "```py\nreturn Fibonacci(n-1)+Fibonacci(n-2)\n```", "```py\ndef Fibonacci(n):     \n    if n<0: \n        print(\"Outside bounds\")\n    elif n==1: \n        return 0 # n==1, returns 0 \n    elif n==2: \n        return 1 # n==2, returns 1\n    else:   \n        print(\"Solving for {}\".format(n))\n        return Fibonacci(n-1)+Fibonacci(n-2)\nprint(Fibonacci(9))\n```", "```py\nfibSequence = [0,1]\n\ndef Fibonacci(n):     \n    if n<0: \n        print(\"Outside bounds\")\n   elif n<= len(fibSequence): \n return fibSequence[n-1] \n    else:   \n        print(\"Solving for {}\".format(n))\n        fibN = Fibonacci(n-1) + Fibonacci(n-2)\n fibSequence.append(fibN)\n return fibN \n\nprint(Fibonacci(9))\n```", "```py\nfibSequence = [0,1]\n```", "```py\nelif n<= len(fibSequence): \n        return fibSequence[n-1]\n```", "```py\nfibN = Fibonacci(n-1) + Fibonacci(n-2) fibSequence.append(fibN) return fibN \n```", "```py\nconda create -n chapter2 python=3.6\n```", "```py\nactivate chapter2\n```", "```py\npip install gym\n```", "```py\nfrom os import system, name\nimport time\nimport gym\nimport numpy as np\n\nenv = gym.make('FrozenLake-v0')\nenv.reset()\n\ndef clear():\n    if name == 'nt': \n        _ = system('cls')    \n    else: \n        _ = system('clear')\n\nfor _ in range(1000):\n    clear()\n    env.render()\n    time.sleep(.5)\n    env.step(env.action_space.sample()) # take a random action\nenv.close()\n```", "```py\nenv = gym.make('FrozenLake-v0')\nenv.reset()\n```", "```py\nenv.step(env.action_space.sample())\n```", "```py\nfrom os import system, name\nimport time\nimport gym\nimport numpy as np\nenv = gym.make('FrozenLake-v0')\nenv.reset()\n\ndef clear():\n    if name == 'nt': \n        _ = system('cls')    \n    else: \n        _ = system('clear')\n\ndef act(V, env, gamma, policy, state, v):\n    for action, action_prob in enumerate(policy[state]):                \n        for state_prob, next_state, reward, end in env.P[state][action]:                                        \n            v += action_prob * state_prob * (reward + gamma * V[next_state])                    \n            V[state] = v\n\ndef eval_policy(policy, env, gamma=1.0, theta=1e-9, terms=1e9):     \n    V = np.zeros(env.nS)  \n    delta = 0\n    for i in range(int(terms)): \n        for state in range(env.nS):            \n            act(V, env, gamma, policy, state, v=0.0)         \n        clear()\n        print(V)\n        time.sleep(1) \n        v = np.sum(V)\n        if v - delta < theta:\n            return V\n        else:\n            delta = v\n    return V\n\npolicy = np.ones([env.env.nS, env.env.nA]) / env.env.nA\nV = eval_policy(policy, env.env)\n\nprint(policy, V)\n```", "```py\nfor i in range(int(terms)):\n  for state in range(env.nS):\n    act(V, env, gamma, policy, state, v=0.0)\n  clear()\n  print(V)\n  time.sleep(1)\n  v = np.sum(V)\n  if v - delta < theta:\n    return V\n  else:\n    delta = v\nreturn V\n```", "```py\nfor action, action_prob in enumerate(policy[state]):   \n  for state_prob, next_state, reward, end\n in env.P[state][action]:   \n    v += action_prob * state_prob * (reward + gamma * V[next_state]) #update \n    V[state] = v\n```", "```py\ndef evaluate(V, action_values, env, gamma, state):\n    for action in range(env.nA):\n        for prob, next_state, reward, terminated in env.P[state][action]:\n            action_values[action] += prob * (reward + gamma * V[next_state])\n    return action_values\n\ndef lookahead(env, state, V, gamma):\n    action_values = np.zeros(env.nA)\n    return evaluate(V, action_values, env, gamma, state)\n\ndef improve_policy(env, gamma=1.0, terms=1e9):    \n    policy = np.ones([env.nS, env.nA]) / env.nA\n    evals = 1\n    for i in range(int(terms)):\n        stable = True       \n        V = eval_policy(policy, env, gamma=gamma)\n        for state in range(env.nS):\n            current_action = np.argmax(policy[state])\n            action_value = lookahead(env, state, V, gamma)\n            best_action = np.argmax(action_value)\n            if current_action != best_action:\n                stable = False               \n                policy[state] = np.eye(env.nA)[best_action]\n            evals += 1                \n            if stable:\n                return policy, V\n\n#replaced bottom code from previous sample with\npolicy, V = improve_policy(env.env) \nprint(policy, V)\n```", "```py\naction_value = lookahead(env, state, V, gamma)best_action = np.argmax(action_value)\nif current_action != best_action:                \n  stable = False   \n  policy[state] = np.eye(env.nA)[best_action]\nevals += 1 \n\nif stable:\n  return policy, V\n```", "```py\ndef value_iteration(env, gamma=1.0, theta=1e-9, terms=1e9):\n    V = np.zeros(env.nS)\n    for i in range(int(terms)):\n        delta = 0\n        for state in range(env.nS):\n            action_value = lookahead(env, state, V, gamma)\n            best_action_value = np.max(action_value)\n            delta = max(delta, np.abs(V[state] - best_action_value))\n            V[state] = best_action_value             \n        if delta < theta: break\n    policy = np.zeros([env.nS, env.nA])\n    for state in range(env.nS):\n        action_value = lookahead(env, state, V, gamma)\n        best_action = np.argmax(action_value)\n        policy[state, best_action] = 1.0\n    return policy, V\n\n#policy, V = improve_policy(env.env) \n#print(policy, V)\n\npolicy, V = value_iteration(env.env)\nprint(policy, V)\n```", "```py\nfor state in range(env.nS):\n action_value = lookahead(env, state, V, gamma)\n best_action_value = np.max(action_value)\n delta = max(delta, np.abs(V[state] - best_action_value))\n V[state] = best_action_value \n\n```", "```py\nif delta < theta: break\n```", "```py\npolicy = np.zeros([env.nS, env.nA])\n  for state in range(env.nS):\n    action_value = lookahead(env, state, V, gamma)\n    best_action = np.argmax(action_value)\n    policy[state, best_action] = 1.0\nreturn policy, V\n```", "```py\ndef play(env, episodes, policy):\n    wins = 0\n    total_reward = 0\n    for episode in range(episodes):\n        term = False\n        state = env.reset()\n        while not term:\n            action = np.argmax(policy[state])\n            next_state, reward, term, info = env.step(action)\n            total_reward += reward\n            state = next_state\n            if term and reward == 1.0:\n                wins += 1\n    average_reward = total_reward / episodes\n    return wins, total_reward, average_reward\n\npolicy, V = improve_policy(env.env)\nprint(policy, V)\n\nwins, total, avg = play(env.env, 1000, policy)\nprint(wins)\n\npolicy, V = value_iteration(env.env)\nprint(policy, V)\n\nwins, total, avg = play(env.env, 1000, policy)\nprint(wins)\n```", "```py\nwins, total, avg = play(env.env, 1000, policy)print(wins)\n```", "```py\nfor episode in range(episodes):\n  term = False\n  state = env.reset()\n  while not term:\n    action = np.argmax(policy[state])\n    next_state, reward, term, info = env.step(action)\n    total_reward += reward\n    state = next_state\n    if term and reward == 1.0:\n      wins += 1\naverage_reward = total_reward / episodes    return wins, total_reward, average_reward\n```", "```py\nnext_state, reward, term, info = env.step(action)\n```", "```py\ntotal_reward += reward\nstate = next_state\nif term and reward == 1.0:\n  wins += 1\n```"]