- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Stable Diffusion XL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the not-very-successful Stable Diffusion 2.0 and Stable Diffusion 2.1,
    July 2023 saw the launch of Stability AI’s latest release, **Stable Diffusion
    XL** (**SDXL**) [1]. I eagerly applied the model weights data as soon as registration
    was open. Both my tests and those conducted by the community indicate that SDXL
    has made significant strides forward. It now allows us to generate higher-quality
    images at increased resolutions, vastly outperforming the Stable Diffusion V1.5
    base model. Another notable enhancement is the ability to use more intuitive “natural
    language” prompts to generate images, eliminating the need to cobble together
    a multitude of “words” to form a meaningful prompt. Furthermore, we can now generate
    desired images with more concise prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'SDXL has improved in almost every aspect compared to the previous versions,
    and it is worth the time and effort to start using it for better and stable image
    generation. In this chapter, we will discuss in detail what’s new in SDXL and
    explain why the aforementioned changes led to its improvements. For example, we
    will explore what is new in the **Variational Autoencoder** (**VAE**), UNet, and
    TextEncoder design compared to Stable Diffusion V1.5\. In a nutshell, this chapter
    will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What’s new in SDXL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SDXL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we will use Python code to demonstrate the latest SDXL base and community
    models in action. We will cover basic usage and also advanced usage, such as loading
    multiple LoRA models and using unlimited weighted prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: What’s new in SDXL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SDXL is still a latent diffusion model, maintaining the same overall architecture
    used in Stable Diffusion v1.5\. According to the original paper behind SDXL [2],
    SDXL expands every component, making them wider and bigger. The SDXL backbone
    UNet is three times larger, there are two text encoders in the SDXL base model,
    and a separate diffusion-based refinement model is included. The overall architecture
    is shown in *Figure 16**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1: SDXL architecture](img/B21263_16_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: SDXL architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the refiner is optional; we can decide whether to use the refiner
    model or not. Next, let’s drill down to each component one by one.
  prefs: []
  type: TYPE_NORMAL
- en: The VAE of the SDXL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **VAE** is a pair of encoder and decoder neural networks. A VAE encoder encodes
    an image into a latent space, and its paired decoder can decode a latent image
    to a pixel image. Many articles on the web tell us that a VAE is a technique used
    to improve the quality of images; however, this is not the whole picture. The
    core responsibility of VAE in Stable Diffusion is converting pixel images to and
    from the latent space. Of course, a good VAE can improve the image quality by
    adding high-frequency details.
  prefs: []
  type: TYPE_NORMAL
- en: The VAE used in SDXL is a retrained one, using the same autoencoder architecture
    but with an increased batch size (256 versus 9) and, additionally, tracking the
    weights with an exponential moving average [2]. The new VAE outperforms the original
    model in all evaluated metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of these implementation differences, instead of reusing the VAE code
    introduced in [*Chapter 5*](B21263_05.xhtml#_idTextAnchor097), we will need to
    write new code if we decide to use VAE independently. Here, we will provide an
    example of some common usage of the SDXL VAE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize a VAE model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode an image using the VAE model. Before executing the following code, replace
    the `cat.png` file with a validated accessible image path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Decode an image from latent space:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, you first encode an image to latent space. An image in
    the latent space is invisible to our eyes, but it captures the features of an
    image in the latent space (in other words, in a high-dimensional vector space).
    Then, the decode part of the code decodes the image in the latent space to pixel
    space. From the preceding code, we know what the core functionality of a VAE is.
  prefs: []
  type: TYPE_NORMAL
- en: You might be curious as to why knowledge about the VAE is necessary. It has
    numerous applications. For instance, it allows you to save the generated latent
    image in a database and decode it only when needed. This method can reduce image
    storage by up to 90% without much loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: The UNet of SDXL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UNet model is the backbone neural network of SDXL. The UNet in SDXL is almost
    three times larger than the previous Stable Diffusion models. SDXL’s UNet is a
    2.6 GB billion parameter neural network, while the Stable Diffusion V1.5’s UNet
    has 860 million parameters. Although the current open source LLM model is much
    larger in terms of neural network size, SDXL’s UNet is, so far, the largest among
    those open source Diffusion models at the time of writing (October 2023), which
    directly leads to higher VRAM demands. 8 GB of VRAM can meet most of the use cases
    when using SD V1.5\. For SDXL, 15 GB of VRAM is commonly required; otherwise,
    we will need to reduce the image resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the model size expansion, SDXL rearranges its Transformer block’s position,
    which is crucial for better and more precise natural language-to-image guidance.
  prefs: []
  type: TYPE_NORMAL
- en: Two text encoders in SDXL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most significant changes in SDXL is the text encoder. SDXL uses two
    text encoders together, CLIP ViT-L [5] and OpenCLIP ViT-bigG (also named OpenCLIP
    G/14). Furthermore, SDXL uses pooled embeddings from OpenCLIP ViT-bigG.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP ViT-L is one of the most widely used models from OpenAI, which is also
    the text encoder or embedding model used in Stable Diffusion V1.5\. What is the
    OpenCLIP ViT-bigG model? OpenCLIP is an open source implementation of **CLIP**
    (**Contrastive Language-Image Pre-Training**). OpenCLIP G/14 is the largest and
    best OpenClip model trained on the LAION-2B dataset [9], a 100 TB dataset containing
    2 billion images. While the OpenAI CLIP model generates a 768-dimensional embedding
    vector, OpenClip G/14 outputs a 1,280-dimensional embedding. By concatenating
    the two embeddings (of the same length), a 2,048-dimensional embedding is output.
    This is much larger than the previous 768-dimensional embedding from Stable Diffusion
    v1.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the text encoding process, let’s take the sentence `a running
    dog` as input; the ordinary text tokenizer will first convert the sentence into
    tokens, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will return the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding result, `49406` is the beginning token and `49407` is the end
    token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the following code uses the CLIP text encoder to convert the tokens into
    embedding vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The result embedding tensor includes five 768 dimension vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code used OpenAI’s CLIP to convert the prompt text to 768-dimension
    embeddings. The following code uses the OpenClip G/14 model to encode the tokens
    into five 1,280-dimension embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The result embedding tensor includes five 1,280-dimension vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, the next question is, what are **pooled embeddings**? Embedding pooling
    is the process of converting a sequence of tokens into one embedding vector. In
    other words, pooling embedding is a lossy compression of information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the embedding process we used before, which encodes each token into
    an embedding vector, a pooled embedding is one vector that represents the whole
    input text. We can generate the pooled embedding from OpenClip using the following
    Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will return a `torch.Size([1, 1280])` pooled embedding vector
    from the text encoder. The maximum token size for a pooled embedding is `77`.
    In SDXL, the pooled embedding is provided to the UNet together with the token-level
    embedding from both CLIP and OpenCLIP, guiding the image generation.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry – you won’t need to manually provide these embeddings before using
    SDXL. `StableDiffusionXLPipeline` from the `Diffusers` package does everything
    for us. All we need to do is provide the prompt and negative prompt text. We will
    provide the sample code in the *Using* *SDXL* section.
  prefs: []
  type: TYPE_NORMAL
- en: The two-stage design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another design addition in SDXL is its refiner model. According to the SDXL
    paper [2], the refiner model is used to enhance an image by adding more details
    and making it better, especially during the last 10 steps.
  prefs: []
  type: TYPE_NORMAL
- en: The refiner model is just another image-to-image model that can help fix broken
    images and add more elements to the images generated by the base model.
  prefs: []
  type: TYPE_NORMAL
- en: Based on my observations, for community-shared checkpoint models, the refiner
    model may not be necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to use SDXL for common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Using SDXL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We briefly covered loading the SDXL model in [*Chapter 6*](B21263_06.xhtml#_idTextAnchor117)
    and SDXL ControlNet usage in [*Chapter 13*](B21263_13.xhtml#_idTextAnchor257).
    You can find the sample codes there. In this section, we will cover more common
    SDXL usages, including loading community-shared SDXL models and how to use the
    image-to-image pipeline to enhance the model, using SDXL with community-shared
    LoRA models, and the unlimited length prompt pipeline from Diffuser (provided
    by the author of this book).
  prefs: []
  type: TYPE_NORMAL
- en: Use SDXL community models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just months after the release of SDXL, the open source community has released
    countless fine-tuned SDXL models based on the base model from Stability AI. We
    can find these models on Hugging Face and CIVITAI ([https://civitai.com/](https://civitai.com/)),
    and the number keeps growing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, let’s load one model from HuggingFace, using the SDXL model ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the preceding code, `base_pipe.watermark = None` will remove the
    invisible watermark from the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, move the model to CUDA, generate an image, and then offload the model
    from CUDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With just one line prompt and not needing to provide any negative prompt, SDXL
    generates an amazing image, as shown in *Figure 16**.2:*
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2: A cat pilot generated by SDXL](img/B21263_16_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: A cat pilot generated by SDXL'
  prefs: []
  type: TYPE_NORMAL
- en: You may want to use the refiner model to enhance the image, but the refiner
    model doesn’t make a significant difference. Instead, we will use the image-to-image
    pipeline with the same model data to upscale the image.
  prefs: []
  type: TYPE_NORMAL
- en: Using SDXL image-to-image to enhance an image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s first upscale the image to twice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, start an image-to-image pipeline by reusing the model data from the previous
    text-to-image pipeline, saving the RAM and VRAM usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it is time to call the pipeline to further enhance the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we set the strength to `0.3` to preserve most of the original input
    image information. We will get a new, better image, as shown in *Figure 16**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3: The refined cat pilot image from an image-to-image pipeline](img/B21263_16_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: The refined cat pilot image from an image-to-image pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: While you might not notice many differences in this book at first glance, upon
    closer inspection of the image on a computer monitor, you will discover numerous
    additional details.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore how to utilize LoRA with Diffusers. If you’re unfamiliar
    with LoRA, I recommend turning back to [*Chapter 8*](B21263_08.xhtml#_idTextAnchor153),
    which delves into the usage of Stable Diffusion LoRA in greater detail, and [*Chapter
    21*](B21263_21.xhtml#_idTextAnchor405), which provides comprehensive coverage
    of LoRA training.
  prefs: []
  type: TYPE_NORMAL
- en: Using SDXL LoRA models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Not long ago, it was impossible to load LoRA using Diffusers, not to mention
    loading multiple LoRA models into one pipeline. With the massive work that has
    been done by the Diffusers team and community contributors, we can now load multiple
    LoRA models into the SDXL pipeline with the LoRA scale number specified.
  prefs: []
  type: TYPE_NORMAL
- en: 'And its usage is also extremely simple. It takes just two lines of code to
    add one LoRA to the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'To add two LoRA models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed in [*Chapter 8*](B21263_08.xhtml#_idTextAnchor153), there are
    two ways to use LoRA – one is merging with the backbone model weights, and the
    other is dynamic monkey patching. Here, for SDXL, the method is model merging,
    which means unloading a LoRA from the pipeline. To unload a LoRA model, we will
    need to load the LoRA again but with a negative `lora_scale`. For example, if
    we want to unload `lora2.safetensors` from the pipeline, we can use the following
    code to achieve it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides using `fuse_lora` to load a LoRA model, we can also use PEFT-integrated
    LoRA loading. The code is very similar to the one we just used, but we add one
    more parameter called `adapter_name`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We can adjust the LoRA scale dynamically with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can also disable LoRA as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can disable one of the two loaded LoRA models, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we disabled `lora1` while continuing to use `lora2`.
  prefs: []
  type: TYPE_NORMAL
- en: With proper LoRA management code, you can use SDXL with an unlimited number
    of LoRA models. Speaking of “unlimited,” next, we will cover the “unlimited” length
    prompt for SDXL.
  prefs: []
  type: TYPE_NORMAL
- en: Using SDXL with an unlimited prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, SDXL, like previous versions, supports only a maximum of 77 tokens
    for one-time image generation. In [*Chapter 10*](B21263_10.xhtml#_idTextAnchor197),
    we delved deep into implementing a text embedding encoder that supports weighted
    prompts without length limitation. For SDXL, the idea is similar but more complex
    and a bit harder to implement; after all, there are now two text encoders.
  prefs: []
  type: TYPE_NORMAL
- en: I built a long-weighted SDXL pipeline, `lpw_stable_diffusion_xl`, which is merged
    with the official `Diffusers` package. In this section, I will introduce the usage
    of this pipeline to enable a long-weighted and unlimited pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you have updated your `Diffusers` package to the latest version with
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, use the following code to use the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code uses `DiffusionPipeline` to load a custom pipeline, `lpw_stable_diffusion_xl`,
    contributed by an open source community member (i.e., me).
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the code, the prompt is multiplied by 2, making it definitely longer
    than 77 tokens. At the end of the prompt, `a (cute cat:1.5) aside` is appended.
    If the pipeline supports prompts longer than 77 tokens, there should be a cat
    in the generated result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The image generated from the preceding code is shown in *Figure 16**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4: A man with a cat, generated using an unlimited prompt length
    pipeline – lpw_stable_diffusion_xl](img/B21263_16_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: A man with a cat, generated using an unlimited prompt length pipeline
    – lpw_stable_diffusion_xl'
  prefs: []
  type: TYPE_NORMAL
- en: From the image, we can see that all elements in the prompt are reflected, and
    there is now a cute cat sitting alongside the man.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers the newest and best Stable Diffusion model – SDXL. We first
    introduced the basics of SDXL and why it is powerful and efficient, and then we
    drilled down into each component of the newly released model, covering VAE, UNet,
    text encoders, and the new two-stage design.
  prefs: []
  type: TYPE_NORMAL
- en: We provided a sample code for each of the components to help you understand
    SDXL inside out. These code samples can also be used to leverage the power of
    the individual components. For example, we can use VAE to compress images and
    a text encoder to generate text embeddings for images.
  prefs: []
  type: TYPE_NORMAL
- en: In the second half of this chapter, we covered some common use cases of SDXL,
    such as loading community-shared checkpoint models, using the image-to-image pipeline
    to enhance and upscale images, and introducing a simple and effective solution
    to load multiple LoRA models into one pipeline. Finally, we provided an end-to-end
    solution to use unlimited length-weighted prompts for SDXL.
  prefs: []
  type: TYPE_NORMAL
- en: With the help of SDXL, we can generate amazing images with short prompts and
    achieve much better results.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to discuss how to write Stable Diffusion prompts
    and leverage LLM to help produce and enhance prompts automatically.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'SDXL: [https://stability.ai/stable-diffusion](https://stability.ai/stable-diffusion)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis:
    [https://arxiv.org/abs/2307.01952](https://arxiv.org/abs/2307.01952)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stable Diffusion XL Diffusers: [https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl](https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CLIP from OpenAI: [https://openai.com/research/clip](https://openai.com/research/clip)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CLIP VIT Large model: [https://huggingface.co/openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'REACHING 80% ZERO-SHOT ACCURACY WITH OPENCLIP: VIT-G/14 TRAINED ON LAION-2B:
    [https://laion.ai/blog/giant-openclip/](https://laion.ai/blog/giant-openclip/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'CLIP-ViT-bigG-14-laion2B-39B-b160k: [https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'OpenCLIP GitHub repository: [https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LAION-5B: A NEW ERA OF OPEN LARGE-SCALE MULTI-MODAL DATASETS: [https://laion.ai/blog/laion-5b/](https://laion.ai/blog/laion-5b/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
