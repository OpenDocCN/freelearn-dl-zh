<html><head></head><body>
<div class="calibre1" id="_idContainer044">
<h1 class="chapternumber"><span class="kobospan" id="kobo.1.1">1</span></h1>
<h1 class="chaptertitle" id="_idParaDest-14"><span class="kobospan" id="kobo.2.1">Introduction to Large Language Models</span></h1>
<p class="normal"><span class="kobospan" id="kobo.3.1">Dear reader, welcome to </span><em class="italic"><span class="kobospan" id="kobo.4.1">Building Large Language Model Applications</span></em><span class="kobospan" id="kobo.5.1">! </span><span class="kobospan" id="kobo.5.2">In this book, we will explore the fascinating</span><a id="_idIndexMarker000" class="calibre3"/><span class="kobospan" id="kobo.6.1"> world of a new era of application developments, where </span><strong class="screentext"><span class="kobospan" id="kobo.7.1">large language models</span></strong><span class="kobospan" id="kobo.8.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.9.1">LLMs</span></strong><span class="kobospan" id="kobo.10.1">) are the main protagonists.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.11.1">During the last year, we all learned the power of generative </span><strong class="screentext"><span class="kobospan" id="kobo.12.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.13.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.14.1">AI</span></strong><span class="kobospan" id="kobo.15.1">) tools such as ChatGPT, Bing Chat, Bard, and Dall-E. </span><span class="kobospan" id="kobo.15.2">What impressed us the most was their stunning capabilities of generating human-like content based on user requests made in natural language. </span><span class="kobospan" id="kobo.15.3">It is, in fact, their conversational capabilities that made them so easily consumable and, therefore, popular as soon as they entered the market. </span><span class="kobospan" id="kobo.15.4">Thanks to this phase, we learned to acknowledge the power of generative AI and its core models: LLMs. </span><span class="kobospan" id="kobo.15.5">However, LLMs are more than language generators. </span><span class="kobospan" id="kobo.15.6">They can be also seen as reasoning engines that can become the brains of our intelligent applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.16.1">In this book, we will see the theory and practice of how to build LLM-powered applications, addressing a variety of scenarios and showing new components and frameworks that are entering the domain of software development in this new era of AI. </span><span class="kobospan" id="kobo.16.2">The book will start with </span><em class="italic"><span class="kobospan" id="kobo.17.1">Part 1</span></em><span class="kobospan" id="kobo.18.1">, where we will introduce the theory behind LLMs, the most promising LLMs in the market right now, and the emerging frameworks for LLMs-powered applications. </span><span class="kobospan" id="kobo.18.2">Afterward, we will move to a hands-on part where we will implement many applications using various LLMs, addressing different scenarios and real-world problems. </span><span class="kobospan" id="kobo.18.3">Finally, we will conclude the book with a third part, covering the emerging trends in the field of LLMs, alongside the risk of AI tools and how to mitigate them with responsible AI practices.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.19.1">So, let’s dive in and start with some definitions of the context we are moving in. </span><span class="kobospan" id="kobo.19.2">This chapter provides an introduction and deep dive into LLMs, a powerful set of deep learning neural networks that feature the domain of generative AI.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.20.1">In this chapter, we will cover the following topics:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.21.1">Understanding LLMs, their differentiators from classical machine learning models, and their relevant jargon</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.22.1">Overview of the most popular LLM architectures</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.23.1">How LLMs are trained and consumed</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.24.1">Base LLMs versus fine-tuned LLMs</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.25.1">By the end of this chapter, you will have the fundamental knowledge of what LLMs are, how they work, and how you can make them more tailored to your applications. </span><span class="kobospan" id="kobo.25.2">This will also pave the way for the concrete usage of LLMs in the hands-on part of this book, where we will see in practice how to embed LLMs within your applications.</span></p>
<h1 class="heading" id="_idParaDest-15"><span class="kobospan" id="kobo.26.1">What are large foundation models and LLMs?</span></h1>
<p class="normal"><span class="kobospan" id="kobo.27.1">LLMs are deep-learning-based</span><a id="_idIndexMarker001" class="calibre3"/><span class="kobospan" id="kobo.28.1"> models that use many parameters to learn </span><a id="_idIndexMarker002" class="calibre3"/><span class="kobospan" id="kobo.29.1">from vast amounts of unlabeled texts. </span><span class="kobospan" id="kobo.29.2">They can perform various natural language processing tasks such as recognizing, summarizing, translating, predicting, and generating text.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.30.1">Definition</span></strong></p>
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.31.1">Deep learning</span></strong><span class="kobospan" id="kobo.32.1"> is a branch of machine learning</span><a id="_idIndexMarker003" class="calibre3"/><span class="kobospan" id="kobo.33.1"> that is characterized by neural networks with multiple layers, hence the term “deep.” </span><span class="kobospan" id="kobo.33.2">These deep neural networks can automatically learn hierarchical data representations, with each layer extracting increasingly abstract features from the input data. </span><span class="kobospan" id="kobo.33.3">The depth of these networks refers to the number of layers they possess, enabling them to effectively model intricate relationships and patterns in complex datasets.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.34.1">LLMs belong to a wider set of models that feature the AI subfield of generative AI: </span><strong class="screentext"><span class="kobospan" id="kobo.35.1">large foundation models</span></strong><span class="kobospan" id="kobo.36.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.37.1">LFMs</span></strong><span class="kobospan" id="kobo.38.1">). </span><span class="kobospan" id="kobo.38.2">Hence, in the following sections, we will explore the rise and development of LFMs and LLMs, as well as their technical architecture, which is a crucial task to understand their functioning and properly adopt those technologies within your applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.39.1">We will start by understanding why LFMs and LLMs differ from traditional AI models and how they represent a paradigm</span><a id="_idIndexMarker004" class="calibre3"/><span class="kobospan" id="kobo.40.1"> shift in this field. </span><span class="kobospan" id="kobo.40.2">We will then explore the technical functioning of LLMs, how they</span><a id="_idIndexMarker005" class="calibre3"/><span class="kobospan" id="kobo.41.1"> work, and the mechanisms behind their outcomes.</span></p>
<h2 class="heading1" id="_idParaDest-16"><span class="kobospan" id="kobo.42.1">AI paradigm shift – an introduction to foundation models</span></h2>
<p class="normal"><span class="kobospan" id="kobo.43.1">A foundation model refers to a type</span><a id="_idIndexMarker006" class="calibre3"/><span class="kobospan" id="kobo.44.1"> of pre-trained generative AI model that offers immense versatility by being adaptable for various specific tasks. </span><span class="kobospan" id="kobo.44.2">These models undergo extensive training on vast and diverse datasets, enabling them to grasp general patterns and relationships within the data – not just limited to textual but also covering other data formats such as images, audio, and video. </span><span class="kobospan" id="kobo.44.3">This initial pre-training phase equips the models with a strong foundational understanding across different domains, laying the groundwork for further fine-tuning. </span><span class="kobospan" id="kobo.44.4">This cross-domain capability</span><a id="_idIndexMarker007" class="calibre3"/><span class="kobospan" id="kobo.45.1"> differentiates generative AI models from standard </span><strong class="screentext"><span class="kobospan" id="kobo.46.1">natural language understanding</span></strong><span class="kobospan" id="kobo.47.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.48.1">NLU</span></strong><span class="kobospan" id="kobo.49.1">) algorithms.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.50.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.51.1">Generative AI and NLU algorithms are both related to </span><strong class="screentext"><span class="kobospan" id="kobo.52.1">natural language processing</span></strong><span class="kobospan" id="kobo.53.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.54.1">NLP</span></strong><span class="kobospan" id="kobo.55.1">), which is a branch of AI</span><a id="_idIndexMarker008" class="calibre3"/><span class="kobospan" id="kobo.56.1"> that deals with human language. </span><span class="kobospan" id="kobo.56.2">However, they have different goals and applications.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.57.1">The difference between</span><a id="_idIndexMarker009" class="calibre3"/><span class="kobospan" id="kobo.58.1"> generative AI and NLU</span><a id="_idIndexMarker010" class="calibre3"/><span class="kobospan" id="kobo.59.1"> algorithms is that generative AI aims to create new natural language content, while NLU algorithms aim to understand existing natural language content. </span><span class="kobospan" id="kobo.59.2">Generative AI can be used for tasks such as text summarization, text generation, image captioning, or style transfer. </span><span class="kobospan" id="kobo.59.3">NLU algorithms can be used for tasks such as chatbots, question answering, sentiment analysis, or machine translation.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.60.1">Foundation models are designed with transfer learning in mind, meaning they can effectively apply the knowledge acquired during pre-training to new, related tasks. </span><span class="kobospan" id="kobo.60.2">This transfer of knowledge enhances their adaptability, making them efficient at quickly mastering new tasks with relatively little additional training.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.61.1">One notable characteristic of foundation models</span><a id="_idIndexMarker011" class="calibre3"/><span class="kobospan" id="kobo.62.1"> is their large architecture, containing millions or even billions of parameters. </span><span class="kobospan" id="kobo.62.2">This extensive scale enables them to capture complex patterns and relationships within the data, contributing to their impressive performance across various tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.63.1">Due to their comprehensive pre-training and transfer learning capabilities, foundation models exhibit strong generalization skills. </span><span class="kobospan" id="kobo.63.2">This means they can perform well across a range of tasks and efficiently adapt to new, unseen data, eliminating the need for training separate models for individual tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.64.1">This paradigm shift in artificial neural network design offers considerable advantages, as foundation models, with their diverse training datasets, can adapt to different tasks based on users’ intent without compromising performance or efficiency. </span><span class="kobospan" id="kobo.64.2">In the past, creating and training distinct neural networks for each task, such as named entity recognition or sentiment analysis, would have been necessary, but now, foundation models provide a unified and powerful solution for multiple applications.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.65.1"><img alt="" role="presentation" src="../Images/B21714_01_01.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.66.1">Figure 1.1: From task-specific models to general models</span></p>
<p class="normal1"><span class="kobospan" id="kobo.67.1">Now, we said that LFMs are trained on a huge amount of heterogeneous data in different formats. </span><span class="kobospan" id="kobo.67.2">Whenever that data is unstructured, natural language data, we refer to the output LFM as an LLM, due to its focus on text understanding and generation.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.68.1"><img alt="A close-up of a white card  Description automatically generated" src="../Images/B21714_01_02.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.69.1">Figure 1.2: Features of LLMs</span></p>
<p class="normal1"><span class="kobospan" id="kobo.70.1">We can then say that an LLM is a type of foundation model specifically designed for NLP tasks. </span><span class="kobospan" id="kobo.70.2">These models, such as ChatGPT, BERT, Llama, and many others, are trained on vast amounts of text data and can generate human-like text, answer questions, perform translations, and more.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.71.1">Nevertheless, LLMs aren’t limited to performing text-related tasks. </span><span class="kobospan" id="kobo.71.2">As we will see throughout the book, those unique models can be seen as reasoning engines, extremely good in common sense reasoning. </span><span class="kobospan" id="kobo.71.3">This means that they can assist us in complex tasks, analytical problem-solving, enhanced connections, and insights among pieces of information.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.72.1">In fact, as LLMs mimic the way</span><a id="_idIndexMarker012" class="calibre3"/><span class="kobospan" id="kobo.73.1"> our brains are made (as we will see in the next section), their architectures are featured by connected neurons. </span><span class="kobospan" id="kobo.73.2">Now, human brains have about 100 trillion connections, way more than those within an LLM. </span><span class="kobospan" id="kobo.73.3">Nevertheless, LLMs have proven to be much better at packing a lot of knowledge into those fewer connections than we are.</span></p>
<h2 class="heading1" id="_idParaDest-17"><span class="kobospan" id="kobo.74.1">Under the hood of an LLM</span></h2>
<p class="normal"><span class="kobospan" id="kobo.75.1">LLMs are a particular type of </span><strong class="screentext"><span class="kobospan" id="kobo.76.1">artificial neural networks</span></strong><span class="kobospan" id="kobo.77.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.78.1">ANNs</span></strong><span class="kobospan" id="kobo.79.1">): computational models inspired by the structure</span><a id="_idIndexMarker013" class="calibre3"/><span class="kobospan" id="kobo.80.1"> and functioning of the human brain. </span><span class="kobospan" id="kobo.80.2">They have proven to be highly effective in solving complex problems, particularly in areas like pattern recognition, classification, regression, and decision-making tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.81.1">The basic building block</span><a id="_idIndexMarker014" class="calibre3"/><span class="kobospan" id="kobo.82.1"> of an ANN is the </span><a id="_idIndexMarker015" class="calibre3"/><span class="kobospan" id="kobo.83.1">artificial neuron, also known as a node or unit. </span><span class="kobospan" id="kobo.83.2">These neurons are organized into layers, and the connections between neurons are weighted to represent the strength of the relationship between them. </span><span class="kobospan" id="kobo.83.3">Those weights represent the </span><strong class="screentext"><span class="kobospan" id="kobo.84.1">parameters</span></strong><span class="kobospan" id="kobo.85.1"> of the model that will be optimized</span><a id="_idIndexMarker016" class="calibre3"/><span class="kobospan" id="kobo.86.1"> during the training process.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.87.1">ANNs are, by definition, mathematical models that work with numerical data. </span><span class="kobospan" id="kobo.87.2">Hence, when it comes to unstructured, textual data as in the context of LLMs, there are two fundamental activities that are required to prepare data as model input:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.88.1">Tokenization</span></strong><span class="kobospan" id="kobo.89.1">: This is the process of breaking</span><a id="_idIndexMarker017" class="calibre3"/><span class="kobospan" id="kobo.90.1"> down a piece of text (a sentence, paragraph, or document) into</span><a id="_idIndexMarker018" class="calibre3"/><span class="kobospan" id="kobo.91.1"> smaller units called tokens. </span><span class="kobospan" id="kobo.91.2">These tokens can be words, subwords, or even characters, depending on the chosen tokenization scheme or algorithm. </span><span class="kobospan" id="kobo.91.3">The goal of tokenization is to create a structured representation of the text that can be easily processed by machine learning models.</span></li>
</ul>
<figure class="mediaobject"><span class="kobospan" id="kobo.92.1"><img alt="A black rectangle with a black arrow pointing to a black rectangle  Description automatically generated" src="../Images/B21714_01_03.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.93.1">Figure 1.3: Example of tokenization</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.94.1">Embedding</span></strong><span class="kobospan" id="kobo.95.1">: Once the text has been tokenized, each token</span><a id="_idIndexMarker019" class="calibre3"/><span class="kobospan" id="kobo.96.1"> is converted into a dense numerical vector called an embedding. </span><span class="kobospan" id="kobo.96.2">Embeddings are a way to represent words, subwords, or characters in a continuous vector space. </span><span class="kobospan" id="kobo.96.3">These embeddings are learned during the training of the language model and capture semantic relationships between tokens. </span><span class="kobospan" id="kobo.96.4">The numerical representation allows the model to perform mathematical operations on the tokens and understand the context in which they appear.</span></li>
</ul>
<figure class="mediaobject"><span class="kobospan" id="kobo.97.1"><img alt="A black and white diagram  Description automatically generated" src="../Images/B21714_01_04.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.98.1">Figure 1.4: Example of embedding</span></p>
<p class="normal1"><span class="kobospan" id="kobo.99.1">In summary, tokenization breaks</span><a id="_idIndexMarker020" class="calibre3"/><span class="kobospan" id="kobo.100.1"> down text into smaller units called tokens, and embeddings convert these tokens</span><a id="_idIndexMarker021" class="calibre3"/><span class="kobospan" id="kobo.101.1"> into dense numerical vectors. </span><span class="kobospan" id="kobo.101.2">This relationship allows LLMs to process and understand textual data in a meaningful and context-aware manner, enabling them to perform a wide range of NLP tasks with impressive accuracy.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.102.1">For example, let’s consider a two-dimensional embedding space where we want to vectorize the words Man, King, Woman, and Queen. </span><span class="kobospan" id="kobo.102.2">The idea is that the mathematical distance between each pair of those words should be representative of their semantic similarity. </span><span class="kobospan" id="kobo.102.3">This is illustrated by the following graph:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.103.1"><img alt="" role="presentation" src="../Images/B21714_01_05.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.104.1">Figure 1.5: Example of words embedding in a 2D space</span></p>
<p class="normal1"><span class="kobospan" id="kobo.105.1">As a result, if we properly embed the words, the relationship </span><strong class="screentext"><span class="kobospan" id="kobo.106.1">King </span></strong><span class="kobospan" id="kobo.107.1">–</span><strong class="screentext"><span class="kobospan" id="kobo.108.1"> Man </span></strong><span class="kobospan" id="kobo.109.1">+</span><strong class="screentext"><span class="kobospan" id="kobo.110.1"> Woman </span></strong><span class="kobospan" id="kobo.111.1">≈</span><strong class="screentext"><span class="kobospan" id="kobo.112.1"> Queen</span></strong><span class="kobospan" id="kobo.113.1"> should hold.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.114.1">Once we have the vectorized input, we can pass it into the multi-layered neural network. </span><span class="kobospan" id="kobo.114.2">There are three main types of layers:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.115.1">Input layer</span></strong><span class="kobospan" id="kobo.116.1">: The first layer of the neural network</span><a id="_idIndexMarker022" class="calibre3"/><span class="kobospan" id="kobo.117.1"> receives the input data. </span><span class="kobospan" id="kobo.117.2">Each neuron in this layer corresponds to a feature or attribute of the input data.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.118.1">Hidden layers</span></strong><span class="kobospan" id="kobo.119.1">: Between the input and output</span><a id="_idIndexMarker023" class="calibre3"/><span class="kobospan" id="kobo.120.1"> layers, there can be one or more hidden layers. </span><span class="kobospan" id="kobo.120.2">These layers process the input data through a series of mathematical transformations and extract relevant patterns and representations from the data.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.121.1">Output layer</span></strong><span class="kobospan" id="kobo.122.1">: The final layer of the neural network</span><a id="_idIndexMarker024" class="calibre3"/><span class="kobospan" id="kobo.123.1"> produces the desired output, which could be predictions, classifications, or other relevant results depending on the task the neural network is designed for.</span></li>
</ul>
<figure class="mediaobject"><span class="kobospan" id="kobo.124.1"><img alt="" role="presentation" src="../Images/B21714_01_06.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.125.1">Figure 1.6: High-level architecture of a generic ANN</span></p>
<p class="normal1"><span class="kobospan" id="kobo.126.1">The process of training</span><a id="_idIndexMarker025" class="calibre3"/><span class="kobospan" id="kobo.127.1"> an ANN involves the process of </span><strong class="screentext"><span class="kobospan" id="kobo.128.1">backpropagation</span></strong><span class="kobospan" id="kobo.129.1"> by iteratively adjusting the weights of the connections between neurons based on the training data and the desired outputs.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.130.1">Definition</span></strong></p>
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.131.1">Backpropagation</span></strong><span class="kobospan" id="kobo.132.1"> is an algorithm used in deep</span><a id="_idIndexMarker026" class="calibre3"/><span class="kobospan" id="kobo.133.1"> learning to train neural networks. </span><span class="kobospan" id="kobo.133.2">It involves two phases: the forward pass, where data is passed through the network to compute the output, and the backward pass, where errors are propagated backward to update the network’s parameters and improve its performance. </span><span class="kobospan" id="kobo.133.3">This iterative process helps the network learn from data and make accurate predictions.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.134.1">During backpropagation, the network learns by comparing its predictions with the ground truth and minimizing the error or loss between them. </span><span class="kobospan" id="kobo.134.2">The objective of training is to find the optimal set of weights that enables the neural network to make accurate predictions on new, unseen data.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.135.1">ANNs can vary in architecture, including the number of layers, the number of neurons in each layer, and the connections between them.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.136.1">When it comes to generative AI and LLMs, their remarkable capability of generating text based on our prompts is based on the statistical concept of Bayes’ theorem.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.137.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.138.1">Bayes’ theorem, named after</span><a id="_idIndexMarker027" class="calibre3"/><span class="kobospan" id="kobo.139.1"> the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. </span><span class="kobospan" id="kobo.139.2">It describes how to update the probability of a hypothesis based on new evidence. </span><span class="kobospan" id="kobo.139.3">Bayes’ theorem is particularly useful when we want to make inferences about unknown parameters or events in the presence of uncertainty. </span><span class="kobospan" id="kobo.139.4">According to Bayes’ theorem, given two events, A and B, we can define the conditional probability of A given B as:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.140.1"><img alt="" role="presentation" src="../Images/B21714_01_001.png" class="calibre4"/></span></figure>
<p class="normal1"><span class="kobospan" id="kobo.141.1">Where:</span></p>
<ul class="calibre14">
<li class="bulletlist"><em class="italic"><span class="kobospan" id="kobo.142.1">P</span></em><span class="kobospan" id="kobo.143.1">(</span><em class="italic"><span class="kobospan" id="kobo.144.1">B</span></em><span class="kobospan" id="kobo.145.1">|</span><em class="italic"><span class="kobospan" id="kobo.146.1">A</span></em><span class="kobospan" id="kobo.147.1">) = probability of </span><em class="italic"><span class="kobospan" id="kobo.148.1">B</span></em><span class="kobospan" id="kobo.149.1"> occurring</span><a id="_idIndexMarker028" class="calibre3"/><span class="kobospan" id="kobo.150.1"> given </span><em class="italic"><span class="kobospan" id="kobo.151.1">A</span></em><span class="kobospan" id="kobo.152.1">, also known as the likelihood of </span><em class="italic"><span class="kobospan" id="kobo.153.1">A</span></em><span class="kobospan" id="kobo.154.1"> given a fixed </span><em class="italic"><span class="kobospan" id="kobo.155.1">B</span></em><span class="kobospan" id="kobo.156.1">.</span></li>
<li class="bulletlist1"><em class="italic"><span class="kobospan" id="kobo.157.1">P</span></em><span class="kobospan" id="kobo.158.1">(</span><em class="italic"><span class="kobospan" id="kobo.159.1">A</span></em><span class="kobospan" id="kobo.160.1">|</span><em class="italic"><span class="kobospan" id="kobo.161.1">B</span></em><span class="kobospan" id="kobo.162.1">) = probability of </span><em class="italic"><span class="kobospan" id="kobo.163.1">A</span></em><span class="kobospan" id="kobo.164.1"> occurring, given </span><em class="italic"><span class="kobospan" id="kobo.165.1">B</span></em><span class="kobospan" id="kobo.166.1">; also</span><a id="_idIndexMarker029" class="calibre3"/><span class="kobospan" id="kobo.167.1"> known as the posterior probability of </span><em class="italic"><span class="kobospan" id="kobo.168.1">A</span></em><span class="kobospan" id="kobo.169.1">, given </span><em class="italic"><span class="kobospan" id="kobo.170.1">B</span></em><span class="kobospan" id="kobo.171.1">.</span></li>
<li class="bulletlist1"><em class="italic"><span class="kobospan" id="kobo.172.1">P</span></em><span class="kobospan" id="kobo.173.1">(</span><em class="italic"><span class="kobospan" id="kobo.174.1">A</span></em><span class="kobospan" id="kobo.175.1">) and </span><em class="italic"><span class="kobospan" id="kobo.176.1">P</span></em><span class="kobospan" id="kobo.177.1">(</span><em class="italic"><span class="kobospan" id="kobo.178.1">B</span></em><span class="kobospan" id="kobo.179.1">) = probability of observing </span><em class="italic"><span class="kobospan" id="kobo.180.1">A</span></em><span class="kobospan" id="kobo.181.1"> or </span><em class="italic"><span class="kobospan" id="kobo.182.1">B</span></em><span class="kobospan" id="kobo.183.1"> without any conditions.</span></li>
</ul>
</div>
<p class="normal1"><span class="kobospan" id="kobo.184.1">Bayes’ theorem relates the conditional</span><a id="_idIndexMarker030" class="calibre3"/><span class="kobospan" id="kobo.185.1"> probability of an event based on new evidence with the a priori probability of the event. </span><span class="kobospan" id="kobo.185.2">Translated into the context of LLMs, we are saying that such a model functions by predicting the next most likely word, given the previous words prompted by the user.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.186.1">But how can LLMs know which is the next most likely word? </span><span class="kobospan" id="kobo.186.2">Well, thanks to the enormous amount of data on which LLMs have been trained (we will dive deeper into the process of training an LLM in the next sections). </span><span class="kobospan" id="kobo.186.3">Based on the training text corpus, the model will be able to identify, given a user’s prompt, the next most likely word or, more generally, text completion.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.187.1">For example, let’s consider</span><a id="_idIndexMarker031" class="calibre3"/><span class="kobospan" id="kobo.188.1"> the following prompt: “</span><em class="italic"><span class="kobospan" id="kobo.189.1">The cat is on the….</span></em><span class="kobospan" id="kobo.190.1">” and we want our LLM to complete this sentence. </span><span class="kobospan" id="kobo.190.2">However, the LLM may generate multiple candidate words, so we need a method to evaluate which of the candidates is the most likely one. </span><span class="kobospan" id="kobo.190.3">To do so, we can use Bayes’ theorem to select the most likely word given the context. </span><span class="kobospan" id="kobo.190.4">Let’s see the required steps:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.191.1">Prior probability</span></strong> <strong class="screentext"><span class="kobospan" id="kobo.192.1">P</span></strong><span class="kobospan" id="kobo.193.1">(</span><strong class="screentext"><span class="kobospan" id="kobo.194.1">A</span></strong><span class="kobospan" id="kobo.195.1">): The prior probability represents </span><a id="_idIndexMarker032" class="calibre3"/><span class="kobospan" id="kobo.196.1">the probability </span><a id="_idIndexMarker033" class="calibre3"/><span class="kobospan" id="kobo.197.1">of each candidate word being the next word in the context, based on the language model’s knowledge learned during training. </span><span class="kobospan" id="kobo.197.2">Let’s assume the LLM has three candidate words: “table,” “chair,” and “roof.”</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.198.1">P(“table”), P(“chain”), and P(“roof”) are the prior probabilities for each candidate word, based on the language model’s knowledge of the frequency of these words in the training data.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.199.1">Likelihood</span></strong><span class="kobospan" id="kobo.200.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.201.1">P</span></strong><span class="kobospan" id="kobo.202.1">(</span><strong class="screentext"><span class="kobospan" id="kobo.203.1">B</span></strong><span class="kobospan" id="kobo.204.1">|</span><strong class="screentext"><span class="kobospan" id="kobo.205.1">A</span></strong><span class="kobospan" id="kobo.206.1">)): The likelihood represents how well each</span><a id="_idIndexMarker034" class="calibre3"/><span class="kobospan" id="kobo.207.1"> candidate word</span><a id="_idIndexMarker035" class="calibre3"/><span class="kobospan" id="kobo.208.1"> fits the context “The cat is on the....” </span><span class="kobospan" id="kobo.208.2">This is the probability of observing the context given each candidate word. </span><span class="kobospan" id="kobo.208.3">The LLM calculates this based on the training data and how often each word appears in similar contexts.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.209.1">For example, if the LLM has seen many instances of “The cat is on the table,” it would assign a high likelihood to “table” as the next word in the given context. </span><span class="kobospan" id="kobo.209.2">Similarly, if it has seen many instances of “The cat is on the chair,” it would assign a high likelihood to “chair” as the next word.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.210.1">P(“The cat is on the table”), P(“The cat is on the chair”), and P(“The cat is on the roof”) are the likelihoods</span><a id="_idIndexMarker036" class="calibre3"/><span class="kobospan" id="kobo.211.1"> for each candidate word </span><a id="_idIndexMarker037" class="calibre3"/><span class="kobospan" id="kobo.212.1">given the context.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.213.1">Posterior probability</span></strong><span class="kobospan" id="kobo.214.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.215.1">P</span></strong><span class="kobospan" id="kobo.216.1">(</span><strong class="screentext"><span class="kobospan" id="kobo.217.1">A</span></strong><span class="kobospan" id="kobo.218.1">|</span><strong class="screentext"><span class="kobospan" id="kobo.219.1">B</span></strong><span class="kobospan" id="kobo.220.1">)): Using Bayes’ theorem, we can calculate</span><a id="_idIndexMarker038" class="calibre3"/><span class="kobospan" id="kobo.221.1"> the posterior probability</span><a id="_idIndexMarker039" class="calibre3"/><span class="kobospan" id="kobo.222.1"> for each candidate word based on the prior probability and the likelihood:</span></li>
</ul>
<p class="center"><span class="kobospan" id="kobo.223.1"><img alt="" role="presentation" src="../Images/B21714_01_002.png" class="calibre4"/></span></p>
<p class="center"><span class="kobospan" id="kobo.224.1"><img alt="" role="presentation" src="../Images/B21714_01_003.png" class="calibre4"/></span></p>
<p class="center"><span class="kobospan" id="kobo.225.1"><img alt="" role="presentation" src="../Images/B21714_01_004.png" class="calibre4"/></span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.226.1">Selecting the most likely word</span></strong><span class="kobospan" id="kobo.227.1">. </span><span class="kobospan" id="kobo.227.2">After calculating the posterior</span><a id="_idIndexMarker040" class="calibre3"/><span class="kobospan" id="kobo.228.1"> probabilities for each candidate word, we choose the word with the highest posterior probability as the most likely next word to complete the sentence.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.229.1">The LLM uses Bayes’ theorem and the probabilities learned during training to generate text that is contextually relevant and meaningful, capturing patterns and associations from the training data to complete sentences in a coherent manner.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.230.1">The following figure illustrates how it translates into the architectural framework of a neural network:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.231.1"><img alt="" role="presentation" src="../Images/B21714_01_07.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.232.1">Figure 1.7: Predicting the next most likely word in an LLM</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.233.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.234.1">The last layer of the ANN is typically a non-linear activation function. </span><span class="kobospan" id="kobo.234.2">In the above illustration, the function is Softmax, a mathematical function that converts a vector of real numbers into a probability distribution. </span><span class="kobospan" id="kobo.234.3">It is often used in machine learning to normalize the output of a neural network or a classifier. </span><span class="kobospan" id="kobo.234.4">The Softmax function</span><a id="_idIndexMarker041" class="calibre3"/><span class="kobospan" id="kobo.235.1"> is defined as follows:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.236.1"><img alt="" role="presentation" src="../Images/B21714_01_005.png" class="calibre4"/></span></figure>
<p class="normal1"><span class="kobospan" id="kobo.237.1">where z</span><sub class="subscript-italic"><span class="kobospan" id="kobo.238.1">i</span></sub><span class="kobospan" id="kobo.239.1"> is the </span><em class="italic"><span class="kobospan" id="kobo.240.1">i</span></em><span class="kobospan" id="kobo.241.1">-th element of the input vector, and K is the number of elements in the vector. </span><span class="kobospan" id="kobo.241.2">The Softmax function ensures that each element of the output vector is between 0 and 1 and that the sum of all elements is 1. </span><span class="kobospan" id="kobo.241.3">This makes the output vector suitable for representing probabilities of different classes or outcomes.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.242.1">Overall, ANNs are the core pillars of the development of generative AI models: thanks to their mechanisms of tokenization, embedding, and multiple hidden layers, they can capture complex patterns even in the most unstructured data, such as natural language.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.243.1">However, what we are observing</span><a id="_idIndexMarker042" class="calibre3"/><span class="kobospan" id="kobo.244.1"> today is a set of models that demonstrates incredible capabilities that have never been seen before, and this is due to a particular ANNs’ architectural framework, introduced in recent years and the main protagonist of LLM development. </span><span class="kobospan" id="kobo.244.2">This framework is called the transformer, and we are going to cover it in the following section.</span></p>
<h1 class="heading" id="_idParaDest-18"><span class="kobospan" id="kobo.245.1">Most popular LLM transformers-based architectures</span></h1>
<p class="normal"><span class="kobospan" id="kobo.246.1">ANNs, as we saw in the preceding sections, are at the heart of LLMs. </span><span class="kobospan" id="kobo.246.2">Nevertheless, in order to be </span><em class="italic"><span class="kobospan" id="kobo.247.1">generative</span></em><span class="kobospan" id="kobo.248.1">, those ANNs need to be endowed with some peculiar capabilities, such as parallel processing of textual sentences or keeping the memory of the previous context.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.249.1">These particular capabilities were at the core of generative AI research in the last decades, starting from the 80s and 90s. </span><span class="kobospan" id="kobo.249.2">However, it is only in recent years that the main drawbacks of these early models – such as the capability of text parallel processing or memory management – have been bypassed by modern generative AI frameworks. </span><span class="kobospan" id="kobo.249.3">Those</span><a id="_idIndexMarker043" class="calibre3"/><span class="kobospan" id="kobo.250.1"> frameworks are the so-called </span><strong class="screentext"><span class="kobospan" id="kobo.251.1">transformers</span></strong><span class="kobospan" id="kobo.252.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.253.1">In the following sections, we will explore the evolution of generative AI model architecture, from early developments to state-of-the-art transformers. </span><span class="kobospan" id="kobo.253.2">We will start by covering the first generative AI models that paved the way for further research, highlighting their limitations and the approaches to overcome them. </span><span class="kobospan" id="kobo.253.3">We will then explore the introduction of transformer-based architectures, covering their main components and explaining why they represent the state of the art for LLMs.</span></p>
<h2 class="heading1" id="_idParaDest-19"><span class="kobospan" id="kobo.254.1">Early experiments</span></h2>
<p class="normal"><span class="kobospan" id="kobo.255.1">The very first popular generative AI ANN architectures trace back to the 80s and 90s, including:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.256.1">Recurrent neural networks</span></strong><span class="kobospan" id="kobo.257.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.258.1">RNNs</span></strong><span class="kobospan" id="kobo.259.1">): RNNs are a type of ANN</span><a id="_idIndexMarker044" class="calibre3"/><span class="kobospan" id="kobo.260.1"> designed to handle sequential data. </span><span class="kobospan" id="kobo.260.2">They have recurrent connections that allow information to persist across time steps, making them suitable for tasks like language modeling, machine translation, and text generation. </span><span class="kobospan" id="kobo.260.3">However, RNNs have limitations in capturing long-range dependencies</span><a id="_idIndexMarker045" class="calibre3"/><span class="kobospan" id="kobo.261.1"> due to the vanishing or exploding gradient problem.</span><div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.262.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.263.1">In ANNs, the gradient is a measure of how much the model’s performance would improve if we slightly adjusted its internal parameters (weights). </span><span class="kobospan" id="kobo.263.2">During training, RNNs</span><a id="_idIndexMarker046" class="calibre3"/><span class="kobospan" id="kobo.264.1"> try to minimize the difference between their predictions and the actual targets by adjusting their weights based on the gradient of the loss function. </span><span class="kobospan" id="kobo.264.2">The problem of vanishing or exploding gradient arises in RNNs during training when the gradients become extremely small or large, respectively. </span><span class="kobospan" id="kobo.264.3">The vanishing gradient problem occurs when the gradient becomes extremely small during training. </span><span class="kobospan" id="kobo.264.4">As a result, the RNN learns very slowly and struggles to capture long-term patterns in the data. </span><span class="kobospan" id="kobo.264.5">Conversely, the exploding gradient problem happens when the gradient becomes extremely large. </span><span class="kobospan" id="kobo.264.6">This leads to unstable training and prevents the RNN from converging to a good solution.</span></p>
</div>
</li>
</ul>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.265.1">Long short-term memory</span></strong><span class="kobospan" id="kobo.266.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.267.1">LSTM</span></strong><span class="kobospan" id="kobo.268.1">): LSTMs are a variant of RNNs that address</span><a id="_idIndexMarker047" class="calibre3"/><span class="kobospan" id="kobo.269.1"> the vanishing gradient problem. </span><span class="kobospan" id="kobo.269.2">They introduce gating mechanisms that enable better preservation of important information across longer sequences. </span><span class="kobospan" id="kobo.269.3">LSTMs became popular for various sequential tasks, including text generation, speech recognition, and sentiment analysis.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.270.1">These architectures were popular and effective for various generative tasks, but they had limitations in handling long-range dependencies, scalability, and overall efficiency, especially when dealing with large-scale NLP tasks that would need massive parallel processing. </span><span class="kobospan" id="kobo.270.2">The transformer framework was introduced to overcome these limitations. </span><span class="kobospan" id="kobo.270.3">In the next section, we are going to see how a transformers-based architecture overcomes the above limitations and is at the core of modern generative AI LLMs.</span></p>
<h2 class="heading1" id="_idParaDest-20"><span class="kobospan" id="kobo.271.1">Introducing the transformer architecture</span></h2>
<p class="normal"><span class="kobospan" id="kobo.272.1">The transformer architecture is a deep learning</span><a id="_idIndexMarker048" class="calibre3"/><span class="kobospan" id="kobo.273.1"> model introduced in the paper “Attention Is All You Need” by Vaswani et al. </span><span class="kobospan" id="kobo.273.2">(2017). </span><span class="kobospan" id="kobo.273.3">It revolutionized NLP and other sequence-to-sequence tasks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.274.1">The transformer dispenses with recurrence and convolutions</span><a id="_idIndexMarker049" class="calibre3"/><span class="kobospan" id="kobo.275.1"> entirely and relies solely on </span><strong class="screentext"><span class="kobospan" id="kobo.276.1">attention mechanisms</span></strong><span class="kobospan" id="kobo.277.1"> to encode and decode sequences.</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.278.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.279.1">In the transformer architecture, “attention” is a mechanism that enables the model to focus on relevant parts of the input sequence while generating the output. </span><span class="kobospan" id="kobo.279.2">It calculates attention scores between input and output positions, applies Softmax to get weights, and takes a weighted sum of the input sequence to obtain context vectors. </span><span class="kobospan" id="kobo.279.3">Attention is crucial for capturing long-range dependencies and relationships between words in the data.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.280.1">Since transformers use attention</span><a id="_idIndexMarker050" class="calibre3"/><span class="kobospan" id="kobo.281.1"> on the same sequence that is currently</span><a id="_idIndexMarker051" class="calibre3"/><span class="kobospan" id="kobo.282.1"> being encoded, we refer to it as </span><strong class="screentext"><span class="kobospan" id="kobo.283.1">self-attention</span></strong><span class="kobospan" id="kobo.284.1">. </span><span class="kobospan" id="kobo.284.2">Self-attention layers are responsible for determining the importance of each input token in generating the output. </span><span class="kobospan" id="kobo.284.3">Those answer the question: “</span><em class="italic"><span class="kobospan" id="kobo.285.1">Which part of the input should I focus on?</span></em><span class="kobospan" id="kobo.286.1">”</span></p>
<p class="normal1"><span class="kobospan" id="kobo.287.1">In order to obtain the self-attention vector for a sentence, the elements we need are “value”, “query”, and “key.” </span><span class="kobospan" id="kobo.287.2">These matrices are used to calculate attention scores between the elements in the input sequence and are the three weight matrices that are learned during the training process (typically initialized with random values). </span><span class="kobospan" id="kobo.287.3">More specifically, their purpose is as follows:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.288.1">Query (</span><em class="italic"><span class="kobospan" id="kobo.289.1">Q</span></em><span class="kobospan" id="kobo.290.1">) is used to represent the current focus of the attention mechanism</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.291.1">Key (</span><em class="italic"><span class="kobospan" id="kobo.292.1">K</span></em><span class="kobospan" id="kobo.293.1">) is used to determine which parts of the input should be given attention</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.294.1">Value (</span><em class="italic"><span class="kobospan" id="kobo.295.1">V</span></em><span class="kobospan" id="kobo.296.1">) is used to compute the context vectors</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.297.1">They can be represented as follows:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.298.1"><img alt="" role="presentation" src="../Images/B21714_01_08.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.299.1">Figure 1.8: Decomposition of the Input matrix into Q, K, and V vectors</span></p>
<p class="normal1"><span class="kobospan" id="kobo.300.1">Those matrices are then multiplied</span><a id="_idIndexMarker052" class="calibre3"/><span class="kobospan" id="kobo.301.1"> and passed through a non-linear transformation (thanks to a Softmax function). </span><span class="kobospan" id="kobo.301.2">The output of the self-attention layer represents the input values in a transformed, context-aware manner, which allows the transformer to attend to different parts of the input depending on the task at hand.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.302.1"><img alt="Chart, box and whisker chart  Description automatically generated" src="../Images/B21714_01_09.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.303.1">Figure 1.9: Representation of Q, K, and V matrices multiplication to obtain the context vector</span></p>
<p class="normal1"><span class="kobospan" id="kobo.304.1">The mathematical formula is the following:</span></p>
<p class="center"><span class="kobospan" id="kobo.305.1"><img alt="" role="presentation" src="../Images/B21714_01_006.png" class="calibre4"/></span></p>
<p class="normal1"><span class="kobospan" id="kobo.306.1">From an architectural</span><a id="_idIndexMarker053" class="calibre3"/><span class="kobospan" id="kobo.307.1"> point of view, the transformer consists of two main components, an encoder and a decoder:</span></p>
<ul class="calibre14">
<li class="bulletlist"><span class="kobospan" id="kobo.308.1">The </span><strong class="screentext"><span class="kobospan" id="kobo.309.1">encoder</span></strong><span class="kobospan" id="kobo.310.1"> takes the input</span><a id="_idIndexMarker054" class="calibre3"/><span class="kobospan" id="kobo.311.1"> sequence and produces a sequence of hidden states, each of which is a weighted sum of all the input embeddings.</span></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.312.1">The </span><strong class="screentext"><span class="kobospan" id="kobo.313.1">decoder</span></strong><span class="kobospan" id="kobo.314.1"> takes the output </span><a id="_idIndexMarker055" class="calibre3"/><span class="kobospan" id="kobo.315.1">sequence (shifted right by one position) and produces a sequence of predictions, each of which is a weighted sum of all the encoder’s hidden states and the previous decoder’s hidden states.</span></li>
</ul>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.316.1">Note</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.317.1">The reason for shifting the output sequence right by one position in the decoder layer is to prevent the model from seeing the current token when predicting the next token. </span><span class="kobospan" id="kobo.317.2">This is because the model is trained to generate the output sequence given the input sequence, and the output sequence should not depend on itself. </span><span class="kobospan" id="kobo.317.3">By shifting the output sequence right, the model only sees the previous tokens as input and learns to predict the next token based on the input sequence and the previous output tokens. </span><span class="kobospan" id="kobo.317.4">This way, the model can learn to generate coherent and meaningful sentences without cheating.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.318.1">The following illustration from the original paper shows the transformer architecture:</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.319.1"><img alt="Diagram  Description automatically generated" src="../Images/B21714_01_10.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.320.1">Figure 1.10: Simplified transformer architecture</span></p>
<p class="normal1"><span class="kobospan" id="kobo.321.1">Let’s examine each building block, starting</span><a id="_idIndexMarker056" class="calibre3"/><span class="kobospan" id="kobo.322.1"> from the encoding part:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.323.1">Input embedding</span></strong><span class="kobospan" id="kobo.324.1">: These are the vector representations of tokenized input text.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.325.1">Positional encoding</span></strong><span class="kobospan" id="kobo.326.1">: As the transformer does not have an inherent sense of word order (unlike RNNs with their sequential nature), positional encodings are added to the input embeddings. </span><span class="kobospan" id="kobo.326.2">These encodings provide information about the positions of words in the input sequence, allowing the model to understand the order of tokens.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.327.1">Multi-head attention layer</span></strong><span class="kobospan" id="kobo.328.1">: This is a mechanism in which multiple self-attention mechanisms operate in parallel on different parts of the input data, producing multiple representations. </span><span class="kobospan" id="kobo.328.2">This allows the transformer model to attend to different parts of the input data in parallel and aggregate information from multiple perspectives.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.329.1">Add and norm layer</span></strong><span class="kobospan" id="kobo.330.1">: This combines element-wise addition and layer normalization. </span><span class="kobospan" id="kobo.330.2">It adds the output of a layer to the original input and then applies layer normalization to stabilize and accelerate training. </span><span class="kobospan" id="kobo.330.3">This technique helps mitigate gradient-related issues and improves the model’s performance on sequential data.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.331.1">Feed-forward layer</span></strong><span class="kobospan" id="kobo.332.1">: This is responsible for transforming the normalized output of attention layers into a suitable representation for the final output, using a non-linear</span><a id="_idIndexMarker057" class="calibre3"/><span class="kobospan" id="kobo.333.1"> activation function, such as the previously mentioned Softmax.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.334.1">The decoding part of the transformer starts with a similar process as the encoding part, where the target sequence (output sequence) undergoes input embedding and positional encoding. </span><span class="kobospan" id="kobo.334.2">Let’s understand these blocks:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.335.1">Output embedding</span></strong><span class="kobospan" id="kobo.336.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.337.1">shifted right</span></strong><span class="kobospan" id="kobo.338.1">): For the decoder, the target</span><a id="_idIndexMarker058" class="calibre3"/><span class="kobospan" id="kobo.339.1"> sequence is “shifted right” by one position. </span><span class="kobospan" id="kobo.339.2">This means that at each position, the model tries to predict the token that comes after the analyzed token in the original target sequence. </span><span class="kobospan" id="kobo.339.3">This is achieved by removing the last token from the target sequence and padding it with a special start-of-sequence token (start symbol). </span><span class="kobospan" id="kobo.339.4">This way, the decoder learns to generate</span><a id="_idIndexMarker059" class="calibre3"/><span class="kobospan" id="kobo.340.1"> the correct token based on the preceding context during </span><strong class="screentext"><span class="kobospan" id="kobo.341.1">autoregressive decoding</span></strong><span class="kobospan" id="kobo.342.1">.</span></li>
</ul>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.343.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.344.1">Autoregressive decoding</span><a id="_idIndexMarker060" class="calibre3"/><span class="kobospan" id="kobo.345.1"> is a technique for generating output sequences from a model that predicts each output token based on the previous output tokens. </span><span class="kobospan" id="kobo.345.2">It is often used in NLP tasks such as machine translation, text summarization, and text generation.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.346.1">Autoregressive decoding works by feeding the model an initial token, such as a start-of-sequence symbol, and then using the model’s prediction as the next input token. </span><span class="kobospan" id="kobo.346.2">This process is repeated until the model generates an end-of-sequence symbol or reaches a maximum length. </span><span class="kobospan" id="kobo.346.3">The output sequence is then the concatenation of all the predicted tokens.</span></p>
</div>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.347.1">Decoder layers</span></strong><span class="kobospan" id="kobo.348.1">: Similarly to the encoder block, here, we also</span><a id="_idIndexMarker061" class="calibre3"/><span class="kobospan" id="kobo.349.1"> have Positional Encoding, Multi-Head Attention, Add and Norm, and Feed Forward layers, whose role is the same as for the encoding part.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.350.1">Linear and Softmax</span></strong><span class="kobospan" id="kobo.351.1">: These layers apply, respectively, a linear</span><a id="_idIndexMarker062" class="calibre3"/><span class="kobospan" id="kobo.352.1"> and non-linear</span><a id="_idIndexMarker063" class="calibre3"/><span class="kobospan" id="kobo.353.1"> transformation to the output vector. </span><span class="kobospan" id="kobo.353.2">The non-linear transformation (Softmax) conveys the output vector into a probability distribution, corresponding to a set of candidate words. </span><span class="kobospan" id="kobo.353.3">The word corresponding to the greatest element of the probability vector will be the output of the whole process.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.354.1">The transformer architecture paved the way for modern LLMs, and it also saw many variations with respect to its original framework.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.355.1">Some models use </span><a id="_idIndexMarker064" class="calibre3"/><span class="kobospan" id="kobo.356.1">only the encoder part, such as </span><strong class="screentext"><span class="kobospan" id="kobo.357.1">BERT</span></strong><span class="kobospan" id="kobo.358.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.359.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="kobospan" id="kobo.360.1">), which is designed for NLU tasks such as text classification, question answering, and sentiment analysis.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.361.1">Other models use</span><a id="_idIndexMarker065" class="calibre3"/><span class="kobospan" id="kobo.362.1"> only the decoder part, such as </span><strong class="screentext"><span class="kobospan" id="kobo.363.1">GPT-3</span></strong><span class="kobospan" id="kobo.364.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.365.1">Generative Pre-trained Transformer 3</span></strong><span class="kobospan" id="kobo.366.1">), which is designed for natural language generation tasks such as text completion, summarization, and dialogue.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.367.1">Finally, there are models</span><a id="_idIndexMarker066" class="calibre3"/><span class="kobospan" id="kobo.368.1"> that use both the encoder and the decoder parts, such as </span><strong class="screentext"><span class="kobospan" id="kobo.369.1">T5</span></strong><span class="kobospan" id="kobo.370.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.371.1">Text-to-Text Transfer Transformer</span></strong><span class="kobospan" id="kobo.372.1">), which is designed for various NLP tasks that can be framed as text-to-text transformations, such as translation, paraphrasing, and text simplification.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.373.1">Regardless of the variant, the core component of a transformer – the attention mechanism – remains a constant within LLM architecture, and it also represents the reason why those frameworks gained so much popularity within the context of generative AI and NLP.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.374.1">However, the architectural variant </span><a id="_idIndexMarker067" class="calibre3"/><span class="kobospan" id="kobo.375.1">of an LLM is not the only element that features the functioning of that model. </span><span class="kobospan" id="kobo.375.2">This functioning is indeed characterized also by </span><em class="italic"><span class="kobospan" id="kobo.376.1">what the model knows</span></em><span class="kobospan" id="kobo.377.1">, depending on its training dataset, and </span><em class="italic"><span class="kobospan" id="kobo.378.1">how well it applies its knowledge upon the user’s request</span></em><span class="kobospan" id="kobo.379.1">, depending on its evaluation metrics.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.380.1">In the next section, we are going to cover both the processes of training and evaluating LLMs, also providing those metrics needed to differentiate among different LLMs and understand which one to use for specific use cases within your applications.</span></p>
<h1 class="heading" id="_idParaDest-21"><span class="kobospan" id="kobo.381.1">Training and evaluating LLMs</span></h1>
<p class="normal"><span class="kobospan" id="kobo.382.1">In the preceding sections, we saw how choosing</span><a id="_idIndexMarker068" class="calibre3"/><span class="kobospan" id="kobo.383.1"> an LLM architecture</span><a id="_idIndexMarker069" class="calibre3"/><span class="kobospan" id="kobo.384.1"> is a pivotal step in determining its functioning. </span><span class="kobospan" id="kobo.384.2">However, the quality and diversity of the output text depend largely on two factors: the training dataset and the evaluation metric.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.385.1">The training dataset determines what kind of data the LLM learns from and how well it can generalize to new domains and languages. </span><span class="kobospan" id="kobo.385.2">The evaluation metric measures how well the LLM performs on specific tasks and benchmarks, and how it compares to other models and human writers. </span><span class="kobospan" id="kobo.385.3">Therefore, choosing an appropriate training dataset and evaluation metric is crucial for developing and assessing LLMs.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.386.1">In this section, we will discuss some of the challenges and trade-offs involved in selecting and using different training datasets and evaluation metrics for LLMs, as well as some of the recent developments and future directions in this area.</span></p>
<h2 class="heading1" id="_idParaDest-22"><span class="kobospan" id="kobo.387.1">Training an LLM</span></h2>
<p class="normal"><span class="kobospan" id="kobo.388.1">By definition, LLMs are </span><em class="italic"><span class="kobospan" id="kobo.389.1">huge</span></em><span class="kobospan" id="kobo.390.1">, from a double</span><a id="_idIndexMarker070" class="calibre3"/><span class="kobospan" id="kobo.391.1"> point of view:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.392.1">Number of parameters</span></strong><span class="kobospan" id="kobo.393.1">: This is a measure of the complexity of the LLM architecture and represents the number of connections among neurons. </span><span class="kobospan" id="kobo.393.2">Complex architectures have thousands of layers, each one having multiple neurons, meaning that among layers, we will have several connections with associated parameters (or weights).</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.394.1">Training set</span></strong><span class="kobospan" id="kobo.395.1">: This refers to the unlabeled text corpus on which the LLM learns and trains its parameters. </span><span class="kobospan" id="kobo.395.2">To give an idea of how big such a text corpus for an LLM can be, let’s consider OpenAI’s GPT-3 training set:</span></li>
</ul>
<figure class="mediaobject"><span class="kobospan" id="kobo.396.1"><img alt="Table  Description automatically generated" src="../Images/B21714_01_11.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.397.1">Figure 1.11: GPT-3 knowledge base</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.398.1">Considering the assumption:</span></p>
<ul class="calibre14">
<li class="bulletlist2"><span class="kobospan" id="kobo.399.1">1 token ~= 4 characters in English</span></li>
<li class="bulletlist3"><span class="kobospan" id="kobo.400.1">1 token ~= ¾ words</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.401.1">We can conclude that GPT-3 has been trained on around </span><strong class="screentext"><span class="kobospan" id="kobo.402.1">374 billion words</span></strong><span class="kobospan" id="kobo.403.1">.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.404.1">So generally speaking, LLMs are trained</span><a id="_idIndexMarker071" class="calibre3"/><span class="kobospan" id="kobo.405.1"> using unsupervised learning on massive datasets, which often consist of billions of sentences collected from diverse sources on the internet. </span><span class="kobospan" id="kobo.405.2">The transformer architecture, with its self-attention mechanism, allows the model to efficiently process long sequences of text and capture intricate dependencies between words. </span><span class="kobospan" id="kobo.405.3">Training such models necessitates vast computational</span><a id="_idIndexMarker072" class="calibre3"/><span class="kobospan" id="kobo.406.1"> resources, typically</span><a id="_idIndexMarker073" class="calibre3"/><span class="kobospan" id="kobo.407.1"> employing distributed systems with multiple </span><strong class="screentext"><span class="kobospan" id="kobo.408.1">graphics processing units</span></strong><span class="kobospan" id="kobo.409.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.410.1">GPUs</span></strong><span class="kobospan" id="kobo.411.1">) or </span><strong class="screentext"><span class="kobospan" id="kobo.412.1">tensor processing units</span></strong><span class="kobospan" id="kobo.413.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.414.1">TPUs</span></strong><span class="kobospan" id="kobo.415.1">).</span></p>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.416.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.417.1">A tensor is a multi-dimensional array</span><a id="_idIndexMarker074" class="calibre3"/><span class="kobospan" id="kobo.418.1"> used in mathematics and computer science. </span><span class="kobospan" id="kobo.418.2">It holds numerical data and is fundamental in fields like machine learning.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.419.1">A TPU is a specialized hardware accelerator created by Google for deep learning tasks. </span><span class="kobospan" id="kobo.419.2">TPUs are optimized</span><a id="_idIndexMarker075" class="calibre3"/><span class="kobospan" id="kobo.420.1"> for tensor operations, making them highly efficient for training and running neural networks. </span><span class="kobospan" id="kobo.420.2">They offer fast processing while consuming less power, enabling faster model training and inference in data centers.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.421.1">The training process involves numerous iterations over the dataset, fine-tuning the model’s parameters using optimization algorithms backpropagation. </span><span class="kobospan" id="kobo.421.2">Through this process, transformer-based language models acquire a deep understanding of language patterns, semantics, and context, enabling them to excel in a wide range of NLP tasks, from text generation to sentiment</span><a id="_idIndexMarker076" class="calibre3"/><span class="kobospan" id="kobo.422.1"> analysis and machine translation.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.423.1">The following are the main steps</span><a id="_idIndexMarker077" class="calibre3"/><span class="kobospan" id="kobo.424.1"> involved in the training process of an LLM:</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="1"><strong class="screentext"><span class="kobospan" id="kobo.425.1">Data collection</span></strong><span class="kobospan" id="kobo.426.1">: This is the process of gathering a large amount of text data from various sources, such as the open web, books, news articles, social media, etc. </span><span class="kobospan" id="kobo.426.2">The data should be diverse, high-quality, and representative of the natural language that the LLM will encounter.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.427.1">Data preprocessing</span></strong><span class="kobospan" id="kobo.428.1">: This is the process of cleaning, filtering, and formatting the data for training. </span><span class="kobospan" id="kobo.428.2">This may include removing duplicates, noise, or sensitive information, splitting the data into sentences or paragraphs, tokenizing the text into subwords or characters, etc.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.429.1">Model architecture</span></strong><span class="kobospan" id="kobo.430.1">: This is the process of designing the structure and parameters of the LLM. </span><span class="kobospan" id="kobo.430.2">This may include choosing the type of neural network (such as transformer) and its structure (such as decoder only, encoder only, or encoder-decoder), the number and size of layers, the attention mechanism, the activation function, etc.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.431.1">Model initialization</span></strong><span class="kobospan" id="kobo.432.1">: This is the process of assigning initial values to the weights and biases of the LLM. </span><span class="kobospan" id="kobo.432.2">This may be done randomly or by using pre-trained weights from another model.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.433.1">Model pre-training</span></strong><span class="kobospan" id="kobo.434.1">: This is the process of updating the weights and biases of the LLM by feeding it batches of data and computing the loss function. </span><span class="kobospan" id="kobo.434.2">The loss function measures how well the LLM predicts the next token given the previous tokens. </span><span class="kobospan" id="kobo.434.3">The LLM tries to minimize</span><a id="_idIndexMarker078" class="calibre3"/><span class="kobospan" id="kobo.435.1"> the loss by using an </span><strong class="screentext"><span class="kobospan" id="kobo.436.1">optimization algorithm</span></strong><span class="kobospan" id="kobo.437.1"> (such as gradient descent) that adjusts the weights and biases in the direction that reduces the loss with the backpropagation mechanism. </span><span class="kobospan" id="kobo.437.2">The model training may take several epochs (iterations over the entire dataset) until it converges to a low loss value.</span></li>
</ol>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.438.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.439.1">In the context of neural networks, the optimization algorithm</span><a id="_idIndexMarker079" class="calibre3"/><span class="kobospan" id="kobo.440.1"> during training is the method used to find the best set of weights for the model that minimizes the prediction error or maximizes the accuracy of the training data. </span><span class="kobospan" id="kobo.440.2">The most common optimization</span><a id="_idIndexMarker080" class="calibre3"/><span class="kobospan" id="kobo.441.1"> algorithm for neural networks is </span><strong class="screentext"><span class="kobospan" id="kobo.442.1">stochastic gradient descent</span></strong><span class="kobospan" id="kobo.443.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.444.1">SGD</span></strong><span class="kobospan" id="kobo.445.1">), which updates the weights in small steps based on the gradient of the error function and the current input-output pair. </span><span class="kobospan" id="kobo.445.2">SGD is often combined with backpropagation, which we defined earlier in this chapter.</span></p>
</div>
<p class="normal-one"><span class="kobospan" id="kobo.446.1">The output of the pre-training</span><a id="_idIndexMarker081" class="calibre3"/><span class="kobospan" id="kobo.447.1"> phase is the so-called base model.</span></p>
<ol class="calibre15">
<li class="bulletlist1" value="6"><strong class="screentext"><span class="kobospan" id="kobo.448.1">Fine-tuning</span></strong><span class="kobospan" id="kobo.449.1">: The base model is trained in a supervised way with a dataset made of tuples of (prompt, ideal response). </span><span class="kobospan" id="kobo.449.2">This step is necessary to make the base model more in line with AI assistants, such as ChatGPT. </span><span class="kobospan" id="kobo.449.3">The output</span><a id="_idIndexMarker082" class="calibre3"/><span class="kobospan" id="kobo.450.1"> of this phase is called the </span><strong class="screentext"><span class="kobospan" id="kobo.451.1">supervised fine-tuned</span></strong><span class="kobospan" id="kobo.452.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.453.1">SFT</span></strong><span class="kobospan" id="kobo.454.1">) model.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.455.1">Reinforcement learning from human feedback</span></strong><span class="kobospan" id="kobo.456.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.457.1">RLHF</span></strong><span class="kobospan" id="kobo.458.1">): This step consists of iteratively optimizing the SFT model (by updating some of its parameters) with respect</span><a id="_idIndexMarker083" class="calibre3"/><span class="kobospan" id="kobo.459.1"> to the reward model (typically another LLM trained incorporating human preferences).</span></li>
</ol>
<div class="note">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.460.1">Definition</span></strong></p>
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.461.1">Reinforcement learning</span></strong><span class="kobospan" id="kobo.462.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.463.1">RL</span></strong><span class="kobospan" id="kobo.464.1">) is a branch of machine learning that focuses</span><a id="_idIndexMarker084" class="calibre3"/><span class="kobospan" id="kobo.465.1"> on training computers to make optimal decisions by interacting with their environment. </span><span class="kobospan" id="kobo.465.2">Instead of being given explicit instructions, the computer learns through trial and error: by exploring the environment and receiving rewards or penalties for its actions. </span><span class="kobospan" id="kobo.465.3">The goal of reinforcement learning is to find the optimal behavior or policy that maximizes the expected reward or value of a given model. </span><span class="kobospan" id="kobo.465.4">To do so, the</span><a id="_idIndexMarker085" class="calibre3"/><span class="kobospan" id="kobo.466.1"> RL process involves a </span><strong class="screentext"><span class="kobospan" id="kobo.467.1">reward model</span></strong><span class="kobospan" id="kobo.468.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.469.1">RM</span></strong><span class="kobospan" id="kobo.470.1">) that is able to provide a “preferability score” to the computer. </span><span class="kobospan" id="kobo.470.2">In the context of RLHF, the RM is trained to incorporate human preferences.</span></p>
</div>
<p class="normal1"><span class="kobospan" id="kobo.471.1">Note that RLHF is a pivotal milestone in achieving human alignment with AI systems. </span><span class="kobospan" id="kobo.471.2">Due to the rapid achievements in the field of generative AI, it is pivotal to keep endowing those powerful LLMs and, more generally, LFMs with those preferences and values that are typical</span><a id="_idIndexMarker086" class="calibre3"/><span class="kobospan" id="kobo.472.1"> of human beings.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.473.1">Once we have a trained model, the next and final step is evaluating its performance.</span></p>
<h2 class="heading1" id="_idParaDest-23"><span class="kobospan" id="kobo.474.1">Model evaluation</span></h2>
<p class="normal"><span class="kobospan" id="kobo.475.1">Evaluating traditional AI models</span><a id="_idIndexMarker087" class="calibre3"/><span class="kobospan" id="kobo.476.1"> was, in some ways, pretty intuitive. </span><span class="kobospan" id="kobo.476.2">For example, let’s think about an image classification model that has to determine whether the input image represents a dog or a cat. </span><span class="kobospan" id="kobo.476.3">So we train our model on a training dataset with a set of labeled images and, once the model is trained, we test it on unlabeled images. </span><span class="kobospan" id="kobo.476.4">The evaluation metric is simply the percentage of correctly classified images over the total number of images within the test set.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.477.1">When it comes to LLMs, the story is a bit different. </span><span class="kobospan" id="kobo.477.2">As those models are trained on unlabeled text and are not task-specific, but rather generic and adaptable given a user’s prompt, traditional evaluation metrics were not suitable anymore. </span><span class="kobospan" id="kobo.477.3">Evaluating an LLM means, among other things, measuring its language fluency, coherence, and ability to emulate different styles depending on the user’s request.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.478.1">Hence, a new set of evaluation frameworks needed to be introduced. </span><span class="kobospan" id="kobo.478.2">The following are the most popular frameworks used to evaluate LLMs:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.479.1">General Language Understanding Evaluation</span></strong><span class="kobospan" id="kobo.480.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.481.1">GLUE</span></strong><span class="kobospan" id="kobo.482.1">) and </span><strong class="screentext"><span class="kobospan" id="kobo.483.1">SuperGLUE</span></strong><span class="kobospan" id="kobo.484.1">: This benchmark is used to measure</span><a id="_idIndexMarker088" class="calibre3"/><span class="kobospan" id="kobo.485.1"> the performance of LLMs on various NLU tasks, such as sentiment analysis, natural language inference, question answering, etc. </span><span class="kobospan" id="kobo.485.2">The higher the score on the GLUE benchmark, the better the LLM is at generalizing across different tasks and domains.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.486.1">It recently evolved into a new benchmark</span><a id="_idIndexMarker089" class="calibre3"/><span class="kobospan" id="kobo.487.1"> styled after GLUE and called </span><strong class="screentext"><span class="kobospan" id="kobo.488.1">SuperGLUE</span></strong><span class="kobospan" id="kobo.489.1">, which comes with more difficult tasks. </span><span class="kobospan" id="kobo.489.2">It consists of eight challenging tasks that require more advanced reasoning skills than GLUE, such as natural language inference, question answering, coreference resolution, etc., a broad coverage diagnostic set that tests models on various linguistic capabilities and failure modes, and a leaderboard that ranks models based on their average score across all tasks.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.490.1">The difference between the GLUE and the SuperGLUE benchmark is that the SuperGLUE benchmark is more challenging and realistic than the GLUE benchmark, as it covers more complex tasks and phenomena, requires models to handle multiple domains and formats, and has higher human performance baselines. </span><span class="kobospan" id="kobo.490.2">The SuperGLUE benchmark</span><a id="_idIndexMarker090" class="calibre3"/><span class="kobospan" id="kobo.491.1"> is designed to drive research in the development</span><a id="_idIndexMarker091" class="calibre3"/><span class="kobospan" id="kobo.492.1"> of more general and robust NLU systems.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.493.1">Massive Multitask Language Understanding</span></strong><span class="kobospan" id="kobo.494.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.495.1">MMLU</span></strong><span class="kobospan" id="kobo.496.1">): This benchmark measures the knowledge</span><a id="_idIndexMarker092" class="calibre3"/><span class="kobospan" id="kobo.497.1"> of an LLM using zero-shot and few-shot settings.</span></li>
</ul>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.498.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.499.1">The concept of zero-shot evaluation</span><a id="_idIndexMarker093" class="calibre3"/><span class="kobospan" id="kobo.500.1"> is a method of evaluating a language model without any labeled data or fine-tuning. </span><span class="kobospan" id="kobo.500.2">It measures how well the language model can perform a new task by using natural language instructions or examples as prompts and computing the likelihood of the correct output given the input. </span><span class="kobospan" id="kobo.500.3">It is the probability that a trained model will produce a particular set of tokens without needing any labeled training data.</span></p>
</div>
<p class="normal-one"><span class="kobospan" id="kobo.501.1">This design adds complexity</span><a id="_idIndexMarker094" class="calibre3"/><span class="kobospan" id="kobo.502.1"> to the benchmark and aligns it more closely with the way we assess human performance. </span><span class="kobospan" id="kobo.502.2">The benchmark comprises 14,000 multiple-choice questions categorized into 57 groups, spanning STEM, humanities, social sciences, and other fields. </span><span class="kobospan" id="kobo.502.3">It covers a spectrum of difficulty levels, ranging from basic to advanced professional, assessing both general knowledge and problem-solving skills. </span><span class="kobospan" id="kobo.502.4">The subjects encompass various areas, including traditional ones like mathematics and history, as well as specialized domains like law and ethics. </span><span class="kobospan" id="kobo.502.5">The extensive range of subjects and depth of coverage make this benchmark valuable for uncovering any gaps in a model’s knowledge. </span><span class="kobospan" id="kobo.502.6">Scoring is based on subject-specific accuracy and the average accuracy across all subjects.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.503.1">HellaSwag</span></strong><span class="kobospan" id="kobo.504.1">: The HellaSwag evaluation framework</span><a id="_idIndexMarker095" class="calibre3"/><span class="kobospan" id="kobo.505.1"> is a method of evaluating LLMs on their ability to generate plausible and common sense continuations for given contexts. </span><span class="kobospan" id="kobo.505.2">It is based on the HellaSwag dataset, which is a collection of 70,000 multiple-choice questions that cover diverse domains and genres, such as books, movies, recipes, etc. </span><span class="kobospan" id="kobo.505.3">Each question consists of a context (a few sentences that describe a situation or an event) and four possible endings (one correct and three incorrect). </span><span class="kobospan" id="kobo.505.4">The endings are designed to be hard to distinguish for LLMs, as they require world knowledge, common </span><a id="_idIndexMarker096" class="calibre3"/><span class="kobospan" id="kobo.506.1">sense reasoning, and linguistic understanding.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.507.1">TruthfulQA</span></strong><span class="kobospan" id="kobo.508.1">: This benchmark evaluates a language</span><a id="_idIndexMarker097" class="calibre3"/><span class="kobospan" id="kobo.509.1"> model’s accuracy in generating responses to questions. </span><span class="kobospan" id="kobo.509.2">It includes 817 questions across 38 categories like health, law, finance, and politics. </span><span class="kobospan" id="kobo.509.3">The questions are designed to mimic those that humans might answer incorrectly due to false beliefs or misunderstandings.</span></li>
<li class="bulletlist1"><strong class="screentext"><span class="kobospan" id="kobo.510.1">AI2 Reasoning Challenge</span></strong><span class="kobospan" id="kobo.511.1"> (</span><strong class="screentext"><span class="kobospan" id="kobo.512.1">ARC</span></strong><span class="kobospan" id="kobo.513.1">): This benchmark is used to measure LLMs’ reasoning</span><a id="_idIndexMarker098" class="calibre3"/><span class="kobospan" id="kobo.514.1"> capabilities and to stimulate the development of models that can perform complex NLU tasks. </span><span class="kobospan" id="kobo.514.2">It consists of a dataset of 7,787 multiple-choice science questions, assembled to encourage research in advanced question answering. </span><span class="kobospan" id="kobo.514.3">The dataset is divided into an Easy set and a Challenge set, where the latter contains only questions that require complex reasoning or additional knowledge to answer correctly. </span><span class="kobospan" id="kobo.514.4">The benchmark also provides a corpus of over 14 million science sentences that can be used as supporting evidence for the questions.</span></li>
</ul>
<p class="normal1"><span class="kobospan" id="kobo.515.1">It is important to note that each evaluation framework has a focus on a specific feature. </span><span class="kobospan" id="kobo.515.2">Namely, the GLUE benchmark focuses on grammar, paraphrasing, and text similarity, while MMLU focuses on generalized language understanding among various domains and tasks. </span><span class="kobospan" id="kobo.515.3">Hence, while evaluating an LLM, it is important to have a clear understanding of the final goal, so that the most relevant evaluation framework can be used. </span><span class="kobospan" id="kobo.515.4">Alternatively, if the goal is that of having the best of the breed in any task, it is key not to use only one evaluation framework, but rather an average of multiple frameworks.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.516.1">In addition to that, in case no existing</span><a id="_idIndexMarker099" class="calibre3"/><span class="kobospan" id="kobo.517.1"> LLM is able to tackle your specific use cases, you still have a margin to customize those models and make them more tailored toward your application scenarios. </span><span class="kobospan" id="kobo.517.2">In the next section, we are indeed going to cover the existing techniques of LLM customization, from the lightest ones (such as prompt engineering) up to the whole training of an LLM from scratch.</span></p>
<h1 class="heading" id="_idParaDest-24"><span class="kobospan" id="kobo.518.1">Base models versus customized models</span></h1>
<p class="normal"><span class="kobospan" id="kobo.519.1">The nice thing about LLMs</span><a id="_idIndexMarker100" class="calibre3"/><span class="kobospan" id="kobo.520.1"> is that they have been trained</span><a id="_idIndexMarker101" class="calibre3"/><span class="kobospan" id="kobo.521.1"> and ready to use. </span><span class="kobospan" id="kobo.521.2">As we saw in the previous section, training an LLM requires great investment in hardware (GPUs or TPUs) and it might last for months, and these two factors might mean it is not feasible for individuals and small businesses.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.522.1">Luckily, pre-trained LLMs are generalized enough to be applicable to various tasks, so they can be consumed without further tuning directly via their REST API (we will dive deeper into model consumption in the next chapters).</span></p>
<p class="normal1"><span class="kobospan" id="kobo.523.1">Nevertheless, there might be scenarios where a general-purpose LLM is not enough, since it lacks domain-specific knowledge or doesn’t conform</span><a id="_idIndexMarker102" class="calibre3"/><span class="kobospan" id="kobo.524.1"> to a particular style and taxonomy</span><a id="_idIndexMarker103" class="calibre3"/><span class="kobospan" id="kobo.525.1"> of communication. </span><span class="kobospan" id="kobo.525.2">If this is the case, you might want to customize your model.</span></p>
<h2 class="heading1" id="_idParaDest-25"><span class="kobospan" id="kobo.526.1">How to customize your model</span></h2>
<p class="normal"><span class="kobospan" id="kobo.527.1">There are three main ways to customize your model:</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.528.1">Extending non-parametric knowledge</span></strong><span class="kobospan" id="kobo.529.1">: This allows the model to access</span><a id="_idIndexMarker104" class="calibre3"/><span class="kobospan" id="kobo.530.1"> external sources of information to integrate its parametric knowledge while responding to the user’s query.</span></li>
</ul>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.531.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.532.1">LLMs exhibit two types of knowledge: parametric and non-parametric. </span><span class="kobospan" id="kobo.532.2">The parametric knowledge</span><a id="_idIndexMarker105" class="calibre3"/><span class="kobospan" id="kobo.533.1"> is the one embedded in the LLM’s parameters, deriving from the unlabeled text corpora during the training phase. </span><span class="kobospan" id="kobo.533.2">On the other hand, non-parametric knowledge</span><a id="_idIndexMarker106" class="calibre3"/><span class="kobospan" id="kobo.534.1"> is the one we can “attach” to the model via embedded documentation. </span><span class="kobospan" id="kobo.534.2">Non-parametric knowledge doesn’t change the structure of the model, but rather, allows it to navigate through external documentation to be used as relevant context to answer the user’s query.</span></p>
</div>
<p class="normal-one"><span class="kobospan" id="kobo.535.1">This might involve connecting the model to web sources (like Wikipedia) or internal documentation with domain-specific knowledge. </span><span class="kobospan" id="kobo.535.2">The connection of the LLM </span><a id="_idIndexMarker107" class="calibre3"/><span class="kobospan" id="kobo.536.1">to external sources is called a plug-in, and we will be discussing it more deeply in the hands-on section of this book.</span></p>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.537.1">Few-shot learning</span></strong><span class="kobospan" id="kobo.538.1">: In this type of model </span><a id="_idIndexMarker108" class="calibre3"/><span class="kobospan" id="kobo.539.1">customization, the LLM is given a </span><strong class="screentext"><span class="kobospan" id="kobo.540.1">metaprompt</span></strong><span class="kobospan" id="kobo.541.1"> with a small number</span><a id="_idIndexMarker109" class="calibre3"/><span class="kobospan" id="kobo.542.1"> of examples (typically between 3 and 5) of each new task it is asked to perform. </span><span class="kobospan" id="kobo.542.2">The model must use its prior knowledge to generalize from these examples to perform the task.</span></li>
</ul>
<div class="note-one">
<p class="normal1"><strong class="screentext"><span class="kobospan" id="kobo.543.1">Definition</span></strong></p>
<p class="normal1"><span class="kobospan" id="kobo.544.1">A metaprompt is a message</span><a id="_idIndexMarker110" class="calibre3"/><span class="kobospan" id="kobo.545.1"> or instruction that can be used to improve the performance of LLMs on new tasks with a few examples.</span></p>
</div>
<ul class="calibre14">
<li class="bulletlist"><strong class="screentext"><span class="kobospan" id="kobo.546.1">Fine tuning</span></strong><span class="kobospan" id="kobo.547.1">: The fine-tuning process involves using</span><a id="_idIndexMarker111" class="calibre3"/><span class="kobospan" id="kobo.548.1"> smaller, task-specific datasets to customize the foundation models for particular applications.</span></li>
</ul>
<p class="normal-one"><span class="kobospan" id="kobo.549.1">This approach differs from the first ones because, with fine-tuning, the parameters of the pre-trained model are altered and optimized toward the specific task. </span><span class="kobospan" id="kobo.549.2">This is done by training the model on a smaller labeled dataset that is specific to the new task. </span><span class="kobospan" id="kobo.549.3">The key idea behind fine-tuning is to leverage the knowledge learned from the pre-trained model and fine-tune it to the new task, rather than training a model from scratch.</span></p>
<figure class="mediaobject"><span class="kobospan" id="kobo.550.1"><img alt="A diagram of a data processing process  Description automatically generated" src="../Images/B21714_01_12.png" class="calibre4"/></span></figure>
<p class="packt_figref"><span class="kobospan" id="kobo.551.1">Figure 1.12: Illustration of the process of fine-tuning</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.552.1">In the preceding figure, you can see a schema on how fine-tuning works on OpenAI pre-built models. </span><span class="kobospan" id="kobo.552.2">The idea is that you have available a pre-trained model with general-purpose weights or parameters. </span><span class="kobospan" id="kobo.552.3">Then, you feed your model with custom data, typically in the form of “key-value” prompts and completions:</span></p>
<pre class="programlisting2"><code class="hljs-code"><span class="kobospan" id="kobo.553.1">{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
{"prompt": "&lt;prompt text&gt;", "completion": "&lt;ideal generated text&gt;"}
...
</span></code></pre>
<p class="normal-one"><span class="kobospan" id="kobo.554.1">Once the training</span><a id="_idIndexMarker112" class="calibre3"/><span class="kobospan" id="kobo.555.1"> is done, you will have a customized model that is particularly performant for a given task, for example, the classification of your company’s documentation.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.556.1">The nice thing about fine-tuning</span><a id="_idIndexMarker113" class="calibre3"/><span class="kobospan" id="kobo.557.1"> is that you can make pre-built models tailored to your use cases, without the need to retrain them from scratch, yet leveraging smaller training datasets and hence less training time and compute. </span><span class="kobospan" id="kobo.557.2">At the same time, the model keeps its generative power and accuracy learned via the original training, the one that occurred to the massive dataset.</span></p>
<p class="normal-one"><span class="kobospan" id="kobo.558.1">In </span><em class="italic"><span class="kobospan" id="kobo.559.1">Chapter 11</span></em><span class="kobospan" id="kobo.560.1">, </span><em class="italic"><span class="kobospan" id="kobo.561.1">Fine-Tuning Large Language Models</span></em><span class="kobospan" id="kobo.562.1">, we will focus on fine-tuning your model in Python so that you can test it for your own task.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.563.1">On top of the above techniques (which you can also combine among each other), there is a fourth one, which is the most “drastic.” </span><span class="kobospan" id="kobo.563.2">It consists of training an LLM from scratch, which you might want to either build on your own or initialize from a pre-built architecture. </span><span class="kobospan" id="kobo.563.3">We will see how to approach this technique in the final chapters.</span></p>
<h1 class="heading" id="_idParaDest-26"><span class="kobospan" id="kobo.564.1">Summary</span></h1>
<p class="normal"><span class="kobospan" id="kobo.565.1">In this chapter, we explored the field of LLMs, with a technical deep dive into their architecture, functioning, and training process. </span><span class="kobospan" id="kobo.565.2">We saw the most prominent architectures, such as the transformer-based frameworks, how the training process works, and different ways to customize your own LLM.</span></p>
<p class="normal1"><span class="kobospan" id="kobo.566.1">We now have the foundation to understand what LLMs are. </span><span class="kobospan" id="kobo.566.2">In the next chapter, we will see </span><em class="italic"><span class="kobospan" id="kobo.567.1">how</span></em><span class="kobospan" id="kobo.568.1"> to use them and, more specifically, how to build intelligent applications with them.</span></p>
<h1 class="heading" id="_idParaDest-27"><span class="kobospan" id="kobo.569.1">References</span></h1>
<ul class="calibre16">
<li class="bulletlist"><span class="kobospan" id="kobo.570.1">Attention is all you need: </span><a href="https://arxiv.org" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.571.1">1706.03762.pdf</span></span><span class="kobospan" id="kobo.572.1"> (</span><span class="calibre3"><span class="kobospan" id="kobo.573.1">arxiv.org</span></span><span class="kobospan" id="kobo.574.1">)</span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.575.1">Possible End of Humanity from AI? </span><span class="kobospan" id="kobo.575.2">Geoffrey Hinton at MIT Technology Review’s EmTech Digital: </span><a href="https://www.youtube.com/watch?v=sitHS6UDMJc&amp;t=594s&amp;ab_channel=JosephRaczynski" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.576.1">https://www.youtube.com/watch?v=sitHS6UDMJc&amp;t=594s&amp;ab_channel=JosephRaczynski</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.577.1">The Glue Benchmark: </span><a href="https://gluebenchmark.com/" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.578.1">https://gluebenchmark.com/</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.579.1">TruthfulQA: </span><a href="https://paperswithcode.com/dataset/truthfulqa" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.580.1">https://paperswithcode.com/dataset/truthfulqa</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.581.1">Hugging Face Open LLM Leaderboard: </span><a href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.582.1">https://huggingface.co/spaces/optimum/llm-perf-leaderboard</span></span></a></li>
<li class="bulletlist1"><span class="kobospan" id="kobo.583.1">Think you have Solved Question Answering? </span><span class="kobospan" id="kobo.583.2">Try ARC, the AI2 Reasoning Challenge: </span><a href="https://arxiv.org/abs/1803.05457" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.584.1">https://arxiv.org/abs/1803.05457</span></span></a></li>
</ul>
<h1 class="heading"><span class="kobospan" id="kobo.585.1">Join our community on Discord</span></h1>
<p class="normal"><span class="kobospan" id="kobo.586.1">Join our community’s Discord space for discussions with the author and other readers:</span></p>
<p class="normal1"><a href="https://packt.link/llm" class="calibre3"><span class="calibre3"><span class="kobospan" id="kobo.587.1">https://packt.link/llm</span></span></a></p>
<p class="normal1"><span class="kobospan" id="kobo.588.1"><img alt="" role="presentation" src="../Images/QR_Code214329708533108046.png" class="calibre4"/></span></p>
</div>
</body></html>