<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer147">
			<h1 id="_idParaDest-158"><em class="italic"><a id="_idTextAnchor150"/>Chapter 8</em>: Programming Distance Sensors with Python</h1>
			<p>In this chapter, we look at distance sensors and how to use them to avoid objects. Avoiding obstacles is a key feature in mobile robots, as bumping into stuff is generally not good. It is also a behavior that starts to make a robot appear smart, as if it is behaving intelligently. </p>
			<p>In this chapter, we find out about the different types of sensors and choose a suitable type. We then build a layer in our robot object to access them and, in addition to this, we create a behavior to avoid walls and objects.</p>
			<p>You will learn about the following topics in this chapter:</p>
			<ul>
				<li>Choosing between optical and ultrasonic sensors</li>
				<li>Attaching and reading an ultrasonic sensor</li>
				<li>Avoiding walls – writing a script to avoid obstacles</li>
			</ul>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor151"/>Technical requirements</h1>
			<p>To complete the hands-on experiments in this chapter, you will require the following:</p>
			<ul>
				<li>The Raspberry Pi robot and the code from the previous chapters.</li>
				<li>Two HC-SR04P, RCWL-1601, or Adafruit 4007 ultrasonic sensors. They must have a 3.3 V output.</li>
				<li>A breadboard.</li>
				<li>22 AWG single-core wire or a pre-cut breadboard jumper wire kit.</li>
				<li>A breadboard-friendly <strong class="bold">single pole, double toggle (SPDT</strong>) slide switch.</li>
				<li>Male-to-female jumpers, preferably of the joined-up jumper jerky type.</li>
				<li>Two brackets for the sensor.</li>
				<li>A crosshead screwdriver.</li>
				<li>Miniature spanners or small pliers.</li>
			</ul>
			<p>The code for this chapter is available on GitHub at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter8">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter8</a>.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/2KfCkZM">https://bit.ly/2KfCkZM</a></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor152"/>Choosing between optical and ultrasonic sensors</h1>
			<p>Before we start to <a id="_idIndexMarker349"/>use distance sensors, let's find out what these sensors actually are, how they work, and some of the different types available.</p>
			<p>The most common ways in which to sense distance are to use ultrasound or light. The principle of both of these mechanisms is to fire off a pulse and then sense its reflected return, using either its timing or angle to measure a distance, as can be seen in the following diagram: </p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="Images/B15660_08_01.jpg" alt="Figure 8.1 – Using pulse timing in a distance sensor&#13;&#10;" width="1030" height="862"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Using pulse timing in a distance sensor</p>
			<p>We focus <a id="_idIndexMarker350"/>on the sensors that measure the response time, otherwise<a id="_idIndexMarker351"/> known as the <strong class="bold">time of flight</strong>. <em class="italic">Figure 8.1</em> shows how these sensors use reflection time.</p>
			<p>With this basic understanding of how sensors work, we'll now take a closer look at optical sensors and ultrasonic sensors.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor153"/>Optical sensors</h2>
			<p>Light-based sensors, like <a id="_idIndexMarker352"/>the one in <em class="italic">Figure 8.2</em>, use infrared laser light that we cannot see. These devices can be tiny; however, they can suffer in strong sunlight and fluorescent light, making them misbehave. Some objects reflect light poorly or are transparent and are undetectable by these sensors:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="Images/B15660_08_02.jpg" alt="Figure 8.2 – A VL530LOx on a carrier board&#13;&#10;" width="370" height="289"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – A VL530LOx on a carrier board</p>
			<p>In competitions where infrared beams detect course times, the beams and these sensors can interfere with each other. However, unlike ultrasonic sensors, these are unlikely to cause false detections when placed on different sides of a robot. Optical distance sensors can have higher accuracy, but over a more limited range. They can be expensive, although there are cheaper fixed range types of light sensors out there.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor154"/>Ultrasonic sensors</h2>
			<p>Many sound-based distance <a id="_idIndexMarker353"/>measuring devices use ultrasonic sound with frequencies beyond human hearing limits, although they can annoy some animals, including dogs. Mobile phone microphones and some cameras pick up their pulses as clicks. Ultrasonic devices tend to be larger than optical ones, but cheaper since sound travels slower than light and is easier to measure. Soft objects that do not reflect sound, such as fabrics, can be harder for these to detect.</p>
			<p><em class="italic">Figure 8.3</em> shows the<a id="_idIndexMarker354"/> HC-SR04, a common and inexpensive sound-based distance sensor:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="Images/B15660_08_03.jpg" alt="Figure 8.3 – The HC-SR04&#13;&#10;" width="445" height="264"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – The HC-SR04</p>
			<p>They have a range of <a id="_idIndexMarker355"/>up to 4 meters from a minimum of about 2 cm. </p>
			<p>There are a number of ultrasonic-based devices, including the common HC-SR04, but not all of them are suitable. We'll look at logic levels as this is an important factor in choosing which sensor to buy.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor155"/>Logic levels and shifting</h2>
			<p>The I/O pins on the Raspberry Pi are only suitable for inputs of 3.3 V. Many devices in the market have a 5 V logic, either for their inputs when controlling them, or from their outputs. Let's dig into what I mean by logic levels, and why it is sensible to try and stick to the native voltage level when possible.</p>
			<p><strong class="bold">Voltage</strong> is a measure of how much pushing energy there is on an electrical flow. Different electronics are built to tolerate or to respond to different voltage levels. Putting too high a voltage<a id="_idIndexMarker356"/> through a device can damage it. On the other hand, putting too low a voltage can cause your sensors or outputs to simply not respond or behave strangely. We are dealing with logic devices that output a high or low voltage to represent a true/false value. These voltages must be above a threshold to be true, and below it to be false. We must be aware of these electrical properties, or we will destroy things and fail to get them to communicate.</p>
			<p>The graph in <em class="italic">Figure 8.4</em> shows the effects that different levels have:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="Images/B15660_08_04.jpg" alt="Figure 8.4 – Voltages and logic levels&#13;&#10;" width="619" height="235"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Voltages and logic levels</p>
			<p>In <em class="italic">Figure 8.4</em>, we show a graph. On <a id="_idIndexMarker357"/>the <em class="italic">y</em>-axis (left), it shows voltage labels from 0 to 5 V. The <em class="italic">y</em>-axis shows different operating conditions. There are 4 dashed lines running through the graph. The lowest dashed line is at 0.8 V; below this, an input will consider it as logic 0. The next line, at around 2.3 V, is where many 3.3 V devices consider things at logic 1. The line at 3.3 V shows the expected input and output level for logic 1 on a Raspberry Pi. Above this line, damage may occur to a Raspberry Pi. At around 4.2 V is what some 5 V devices expect for logic 1 (although some will allow as low as 2 V for this) – the Raspberry Pi needs help to talk to those.</p>
			<p>Along the graph are 5 bars. The first labeled bar is at 0 – meaning a clear logic 0 to all devices. The next bar is<a id="_idIndexMarker358"/> a clear logic 1 for the Raspberry Pi at 3.3 V, but it is also below 4.2 V, so some 5 V devices won't recognize this. The bar labelled unclear is at 1.8 V – in this region, between the low and the high thresholds, the logic might not be clear, and this should be avoided. The bar labeled <strong class="bold">Vague logic 1</strong> is above the threshold, but only just, and could be misinterpreted or cause odd results on 3.3 V devices. The last bar is at 5 V, which 5 V devices output. This must not go to the Raspberry Pi without a level shifter or it will destroy that Raspberry Pi. </p>
			<p>There are bars in <em class="italic">Figure 8.4</em> at 1.7 V and 2.3 V. These voltages are very close to the logic threshold and can result in random data coming from the input. Avoid intermediate voltages between the required logic levels. 3 V is OK, but avoid 1.5 V as this is ambiguous.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Putting more than 3.3 V into a Raspberry Pi pin damages the Raspberry Pi. Do not use 5 V devices without logic level shifters.</p>
			<p>If you use devices that are 5 V, you require extra electronics to interface them. The electronics come with further wiring and parts, thereby increasing the cost, complexity, or size of the robot's electronics:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="Images/B15660_08_05.jpg" alt="" width="995" height="743"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Wiring the HC-SR04 sensors into the level shifters</p>
			<p><em class="italic">Figure 8.5</em> shows a <a id="_idIndexMarker359"/>wiring diagram for a robot that uses HC-SR04 5v sensors that require logic level shifting. This circuit diagram shows the Raspberry Pi GPIO pins at the top. Coming from 3 pins to the left are the 5 V, 3.3 V (written as 3v3), and ground (GND) lines. Below the GPIO pins are the 3.3 V and 5 V lines.</p>
			<p>Below the power lines (or rails) are two level shifters. Going into the right of the level shifters are connections from the Raspberry Pi GPIO pins 5, 6, 17, and 27. In this style of diagram, a black dot shows a connection, and lines that do not connect are shown with a bridge. </p>
			<p>The bottom <a id="_idIndexMarker360"/>of the diagram has a ground line from the ground pin. This is shown as it's normal that additional electronics will require access to a ground line.</p>
			<p>The left of the diagram has the two distance sensors, with connections to 5 V and GND. Each sensor has<a id="_idIndexMarker361"/> the <strong class="bold">trig</strong> and <strong class="bold">echo</strong> pins wired<a id="_idIndexMarker362"/> to the level shifters. It's not hard to see how adding more sensors that also require level shifters to this would further increase complexity.</p>
			<p>Thankfully, other options are now available. Where it is possible to use a 3.3 V native device or a device that uses its supply voltage for logic high, it is worth choosing these devices. When buying electronics for a robot, consider carefully what voltage the robot's main controller uses (like the Raspberry Pi), and check that the electronics work with the controller's voltages. </p>
			<p>The HC-SR04 has several replacement parts that have this ability. The HC-SR04P, the RCWL-1601, and Adafruit 4007 models output 3.3 V and can connect directly to the Raspberry Pi.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor156"/>Why use two sensors?</h2>
			<p>Having two sensors allows a behavior to<a id="_idIndexMarker363"/> detect which side is closer. With this, the robot can detect where open spaces are and move toward them. <em class="italic">Figure 8.6</em> shows how this works:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="Images/B15660_08_06.jpg" alt="Figure 8.6 – Using two sensors&#13;&#10;" width="1024" height="644"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.6 – Using two sensors</p>
			<p>In <em class="italic">Figure 8.6</em>, the second robot can make more interesting decisions because it has more data from the world with which to make those decisions. </p>
			<p>Considering all of these options, I recommend you use a 3.3 V variant like the HC-SR04P/RCWL-1601 or Adafruit 4007 because they are cheap and because it is easy to add two or more of these sensors.</p>
			<p>We've seen some distance<a id="_idIndexMarker364"/> sensor types and discussed the trade-offs and choices for this robot. You've learned about voltage levels, and why this is a crucial consideration for robot electronics. We've also looked at how many sensors we could use and where we could put them. Now let's look at how to add them.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor157"/>Attaching and reading an ultrasonic sensor</h1>
			<p>First, we<a id="_idIndexMarker365"/> should wire<a id="_idIndexMarker366"/> in and secure these sensors to the robot. We then write some simple test code that we can use to base our behavior code on in the next section. After completing this section, the robot block diagram should look like <em class="italic">Figure 8.7</em>:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="Images/B15660_08_07.jpg" alt="Figure 8.7 – Robot block diagram with ultrasonic sensors&#13;&#10;" width="736" height="528"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.7 – Robot block diagram with ultrasonic sensors</p>
			<p>This <a id="_idIndexMarker367"/>diagram builds on the block diagram in <em class="italic">Figure 6.33</em> from <a href="B15660_06_Final_ASB_ePub.xhtml#_idTextAnchor096"><em class="italic">Chapter 6</em></a><em class="italic">, Building Robot Basics – Wheels, Power, and Wiring</em> by adding left and right ultrasonic sensors. Both have bi-directional arrows to the<a id="_idIndexMarker368"/> Raspberry Pi, since, being an active sensor, the Raspberry Pi triggers a sensor measurement and then reads back the result. Let's attach the sensors to the robot chassis. </p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor158"/>Securing the sensors to the robot</h2>
			<p>In the <em class="italic">Technical requirements</em> section, I<a id="_idIndexMarker369"/> added an HC-SR04 bracket. Although<a id="_idIndexMarker370"/> it is possible to make a custom bracket with CAD and other part making skills, it is more sensible to use one of the stock designs. <em class="italic">Figure 8.8</em> shows the bracket I'm using:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="Images/B15660_08_08.jpg" alt="Figure 8.8 – Ultrasonic HC-SR04 sensor brackets with the screws and hardware&#13;&#10;" width="532" height="331"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Ultrasonic HC-SR04 sensor brackets with the screws and hardware</p>
			<p>These are <a id="_idIndexMarker371"/>easy to attach to your robot, assuming that your chassis is similar <a id="_idIndexMarker372"/>enough to mine, in that it has mounting holes or a slot to attach this bracket:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="Images/B15660_08_09.jpg" alt="Figure 8.9 – Steps for mounting the sensor bracket&#13;&#10;" width="814" height="939"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.9 – Steps for mounting the sensor bracket</p>
			<p>To mount the <a id="_idIndexMarker373"/>sensor bracket, use <em class="italic">Figure 8.9</em> as a guide for the<a id="_idIndexMarker374"/> following steps:</p>
			<ol>
				<li>Push the two bolts into the holes on the bracket.</li>
				<li>Push the bracket screws through the holes at the front of the robot.</li>
				<li>Thread a nut from underneath the robot on each and tighten. Repeat this for the other side.</li>
				<li>The robot should look like this with the two brackets mounted.<p><em class="italic">Figure 8.10</em> shows how to push the sensors into the brackets:</p><div id="_idContainer137" class="IMG---Figure"><img src="Images/B15660_08_10.jpg" alt="Figure 8.10 – Pushing the sensors into the brackets&#13;&#10;" width="1237" height="447"/></div><p class="figure-caption">Figure 8.10 – Pushing the sensors into the brackets</p></li>
				<li>Look at the sensor. The two transducer elements, the round cans with a gauze on top, will fit well in the holes in the brackets.</li>
				<li>The distance sensors can simply be pushed into the brackets, since they have a friction fit. The electrical connector for the sensor should be facing upward.</li>
				<li>After putting in both sensors, the robot should look like panel 7 of <em class="italic">Figure 8.10</em>. </li>
			</ol>
			<p>You've now attached the sensors to the chassis. Before we wire them, we'll take a slight detour and add a helpful power switch.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor159"/>Adding a power switch</h2>
			<p>Before we turn on the<a id="_idIndexMarker375"/> robot again, let's add a switch for the motor power. This switch is more convenient than screwing the ground wire from the battery into the terminal repeatedly. We'll see how to do this in three simple steps. Follow along:</p>
			<ol>
				<li value="1">Make sure you have the following equipment ready, as shown in <em class="italic">Figure 8.11</em>: a breadboard, some velcro, a mini breadboard-friendly SPDT switch, and one length of single-core 22 AWG wire:<div id="_idContainer138" class="IMG---Figure"><img src="Images/B15660_08_11.jpg" alt="Figure 8.11 – Items needed to add a power switch&#13;&#10;" width="831" height="834"/></div><p class="figure-caption">Figure 8.11 – Items needed to add a power switch</p></li>
				<li>Now use two strips of Velcro to stick the breadboard on top of the robot's battery, as shown in <em class="italic">Figure 8.12</em>. The velcro holds firm but is easy to remove if you need to disassemble the robot:</li>
			</ol>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="Images/B15660_08_12.jpg" alt="Figure 8.12 – Adding velcro strips&#13;&#10;" width="771" height="713"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.12 – Adding velcro strips</p>
			<p>With the<a id="_idIndexMarker376"/> breadboard in place, we can now add a switch.</p>
			<p>Take a look at <em class="italic">Figure 8.13</em> for details on how the switch is connected:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="Images/B15660_08_13.jpg" alt="Figure 8.13 – Wiring the switch &#13;&#10;" width="1425" height="424"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Wiring the switch </p>
			<p><em class="italic">Figure 8.13</em> shows a <a id="_idIndexMarker377"/>circuit diagram, a close-up of a breadboard, and a suggested way to wire the physical connections on the robot. Let's look at this in detail:</p>
			<ol>
				<li value="1">This is a circuit diagram showing the batteries, switch, and motor power input connectors. At the top is the <em class="italic">motor power in</em> terminal. From the positive (+) side of that terminal, a wire goes down the left to the batteries, shown as alternating thick and thin bars. From the batteries, the bottom terminal is their negative side. A wire goes from this around to the switch on the right of the diagram. The top of the switch is then connected via a wire to the negative (-) side of the <em class="italic">motor power in terminal</em>. This is the important diagram for making the connections.</li>
				<li>Before we physically wire the switch, it's worth talking about the rows of the breadboard. This panel shows a close-up of a breadboard, with 2 of the rows highlighted in green lines. The green lines show that the rows are connected in groups of 5. The arrangement of a breadboard has two wired groups of 5 holes (tie-points) for each of the rows (numbered 1 to 30). It has a groove in the middle separating the groups.</li>
				<li>The physical<a id="_idIndexMarker378"/> wiring uses the breadboard to make connections from wires to devices. It won't match the diagram precisely. The left shows the motor board, with a red wire from the batteries, their positive side, going into the positive (+ or VIN) terminal on the <em class="italic">motor power in terminal</em>. The batteries are in the middle. A black wire goes from the batteries into the breadboard in row 3, column <em class="italic">d</em>. In column <em class="italic">e</em>, a switch is plugged into the breadboard going across rows 1, 2, and 3. An orange precut 22 AWG wire goes from row 2 to the GND terminal, where it is screwed in. Sliding this switch turns on the power to the robot motors.</li>
			</ol>
			<p>We've now given our robot a power switch for its motor batteries, so we can turn the motor power on without needing a screwdriver. Next, we will use the same breadboard to wire up the distance sensors.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor160"/>Wiring the distance sensors</h2>
			<p>Each ultrasonic sensor<a id="_idIndexMarker379"/> has four connections:</p>
			<ul>
				<li>A trigger pin to ask for a reading</li>
				<li>An echo pin to sense the return</li>
				<li>A VCC/voltage pin that should be 3.3 V</li>
				<li>A GND or ground pin</li>
			</ul>
			<p>Ensure that the whole<a id="_idIndexMarker380"/> robot is switched off before proceeding any further. The trigger and echo pins need to go to GPIO pins on the Raspberry Pi. </p>
			<p><em class="italic">Figure 8.14</em> shows a close-up of the Raspberry Pi GPIO port to assist in making connections:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="Images/B15660_08_14.jpg" alt="Figure 8.14 – Raspberry Pi connections&#13;&#10;" width="1329" height="387"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Raspberry Pi connections</p>
			<p><em class="italic">Figure 8.14</em> is a diagram view of the GPIO connector on the Raspberry Pi. This connector is the 40 pins set in two rows at the top of the Pi. Many robots and gadgets use them. The pin numbers/names are not printed on the Raspberry Pi, but this diagram should assist in finding them.</p>
			<p>We use a breadboard for this wiring. <em class="italic">Figure 8.15</em> shows the connections n<a id="_idTextAnchor161"/>eeded for these:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="Images/B15660_08_15.jpg" alt="" width="995" height="1230"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.15 – Sensor wiring diagram</p>
			<p>Wires from the<a id="_idIndexMarker381"/> Raspberry Pi to the breadboard, and from the sensor to the breadboard, need male-to-female jumper wires. Wires on the breadboard (there are only 4 of these) use short pre-cut wires. <em class="italic">Figure 8.15</em> shows a circuit diagram above, and a breadboard wiring suggestion below.</p>
			<p>To wire the sensors, use <em class="italic">Figure 8.15</em> as a guide, along with these steps:</p>
			<ol>
				<li value="1">Start with the power connections. A wire goes from the 3.3 V (often written as 3v3 on diagrams) pin on the Raspberry Pi to the top, red-marked rail on the breadboard. We can use this red <em class="italic">rail</em> for other connections needing 3.3 V.</li>
				<li>A wire from one of the GND pins on the Pi goes to the black- or blue-marked rail on the breadboard. We can use this blue <em class="italic">rail</em> for connections requiring GND.</li>
				<li>Pull off a strip of 4 from the male-to-female jumper wires for each side.</li>
				<li>For the left-hand sensor, identify the four pins—VCC, trig, echo, and GND. For the connection from this to the breadboard, it's useful to keep the 4 wires together. Take 4 male-to-female connectors (in a joined strip if possible), from this sensor, and plug them into the board. </li>
				<li>On the breadboard, use the precut wires to make a connection from ground to the blue rail, and from VCC to the red rail.</li>
				<li>Now use some <a id="_idIndexMarker382"/>jumper wires to make the signal connections from the trig/echo pins to the Raspberry Pi GPIO pins.<p class="callout-heading">Important note</p><p class="callout">Depending on where you've placed your breadboard, the distance sensor wires may not reach. If this is the case, join two male-to-female wires back to back, and use some electrical tape to bind them together. </p></li>
			</ol>
			<p>For neatness, I like to wrap wires in spiral wrap; this is entirely optional but can reduce the clutter on the robot.</p>
			<p>Please double-check your connections before you continue. You have now installed the distance sensors into your robot's hardware, but in order to test and use them, we need to prepare the software components.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor162"/>Installing Python libraries to communicate with the sensor</h2>
			<p>To work<a id="_idIndexMarker383"/> with the GPIO sensor, and some other hardware, you need a Python library. Let's use the <strong class="source-inline">GPIOZero</strong> library, designed to help interface with hardware like this:</p>
			<p class="source-code">$ pip3 install RPi.GPIO gpiozero</p>
			<p>With the library now installed, we can write our test code.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor163"/>Reading an ultrasonic distance sensor</h2>
			<p>To write code for <a id="_idIndexMarker384"/>distance sensors, it helps to understand how they work. As suggested previously, this system works by bouncing sound pulses off of objects and measuring the pulse return times.</p>
			<p>The code on the Raspberry Pi sends an electronic<a id="_idIndexMarker385"/> pulse to the <strong class="bold">trigger</strong> pin to ask for a reading. In response to this pulse, the device makes a sound pulse and times its return. The <strong class="bold">echo</strong> pin<a id="_idIndexMarker386"/> responds using a pulse too. The length of this pulse corresponds to the sound travel time.</p>
			<p>The graph in <em class="italic">Figure 8.16</em> shows the timing of these:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="Images/B15660_08_16.jpg" alt="Figure 8.16 – Timing of a pulse and the response for an ultrasonic distance sensor&#13;&#10;" width="683" height="318"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.16 – Timing of a pulse and the response for an ultrasonic distance sensor</p>
			<p>The <strong class="source-inline">GPIOZero</strong> library can<a id="_idIndexMarker387"/> time this pulse, and convert it into a distance, which we can use in our code.</p>
			<p>The device might fail to get a return response in time if the sound didn't echo back soon enough. Perhaps the object was outside the sensor's range, or something dampened the sound.</p>
			<p>As we did with <a id="_idIndexMarker388"/>our servo motor control class previously, we should use comments and descriptive names to help us explain this part of the code. I've called this file <strong class="source-inline">test_distance_sensors.py</strong>:</p>
			<ol>
				<li value="1">Begin by importing <strong class="source-inline">time</strong> and the <strong class="source-inline">DistanceSensor</strong> library:<p class="source-code"><strong class="bold">import time</strong></p><p class="source-code"><strong class="bold">from gpiozero import DistanceSensor</strong></p></li>
				<li>Next, we set up the sensors. I've used <strong class="source-inline">print</strong> statements to show what is going on. In these lines, we create library objects for each distance sensor, registering the pins we have connected them on. Try to make sure these match your wiring:<p class="source-code"><strong class="bold">print("Prepare GPIO Pins")</strong></p><p class="source-code"><strong class="bold">sensor_l = DistanceSensor(echo=17, trigger=27, queue_len=2)</strong></p><p class="source-code"><strong class="bold">sensor_r = DistanceSensor(echo=5,  trigger=6,  queue_len=2)</strong></p><p>You'll note the extra <strong class="source-inline">queue_len</strong> parameter. The <strong class="source-inline">GPIOZero</strong> library tries to collect 30 sensor readings before giving an output, which makes it smoother, but less responsive. And what we'll need for our robot is responsive, so we take it down to 2 readings. A tiny bit of smoothing, but totally responsive.</p></li>
				<li>This test then runs in a loop until we cancel it: <p class="source-code"><strong class="bold">while True:</strong></p></li>
				<li>We then print the distance from our sensors. <strong class="source-inline">.distance</strong> is a property, as we saw with the <strong class="source-inline">.count</strong> property on our LED system earlier in the book. The sensors are continuously updating it. We multiply it by 100 since <strong class="source-inline">GPIOZero</strong> distance is in terms of a meter:<p class="source-code"><strong class="bold">    print("Left: {l}, Right: {r}".format(</strong></p><p class="source-code"><strong class="bold">        l=sensor_l.distance * 100, </strong></p><p class="source-code"><strong class="bold">        r=sensor_r.distance * 100))</strong></p></li>
				<li>A little sleep in the loop stops it flooding the output too much and prevents tight looping:<p class="source-code"><strong class="bold">    time.sleep(0.1)</strong></p></li>
				<li>Now, you can turn on your Raspberry Pi and upload this code. </li>
				<li>Put an object <a id="_idIndexMarker389"/>anywhere between 4 centimeters and 1 meter away from the sensor, as demonstrated in the following image: <div id="_idContainer144" class="IMG---Figure"><img src="Images/B15660_08_17.jpg" alt="" width="1184" height="498"/></div><p class="figure-caption">Figure 8.17 – Distance sensor with object</p><p><em class="italic">Figure 8.17</em> shows an item roughly 10.5 cm from a sensor. The object is a small toolbox. Importantly it is rigid and not fabric. </p></li>
				<li>Start the code on the Pi with <strong class="source-inline">python3 test_distance_sensors.py</strong>. As you move around the object, your Pi should start outputting distances:<p class="source-code"><strong class="bold">pi@myrobot:~ $ python3 test_distance_sensors.py </strong></p><p class="source-code"><strong class="bold">Prepare GPIO Pins</strong></p><p class="source-code"><strong class="bold">Left: 6.565688483970461, Right: 10.483658125707734</strong></p><p class="source-code"><strong class="bold">Left: 5.200715097982538, Right: 11.58136928065528</strong></p></li>
				<li>Because it is in a loop, you need to press <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop the program running.</li>
				<li>You'll see here that there are many decimal places, which isn't too helpful here. First, the devices are unlikely to be that accurate, and second, our robot does not need sub-centimeter accuracy to make decisions. We can modify the print statement in the loop to be more helpful:<p class="source-code"><strong class="bold">    print("Left: {l:.2f}, Right: {r:.2f}".format(</strong></p><p class="source-code"><strong class="bold">        l=sensor_l.distance * 100, </strong></p><p class="source-code"><strong class="bold">        r=sensor_r.distance * 100))</strong></p><p><strong class="source-inline">:.2f</strong> changes the way text is output, to state that there are always two decimal places. Because debug output can be essential to see what is going on in the robot, knowing how to refine it is a valuable skill.</p></li>
				<li>Running <a id="_idIndexMarker390"/>the code with this change gives the following output:</li>
			</ol>
			<p class="source-code">pi@myrobot:~ $ python3 test_distance_sensors.py </p>
			<p class="source-code">Prepare GPIO Pins</p>
			<p class="source-code">Left: 6.56, Right: 10.48</p>
			<p class="source-code">Left: 5.20, Right: 11.58</p>
			<p>You've demonstrated that the distance sensor is working. Added to this is exploring how you can tune the output from a sensor for debugging, something you'll do a lot more when making robots. To make sure you're on track, let's troubleshoot anything that has gone wrong.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor164"/>Troubleshooting</h2>
			<p>If this sensor isn't working as expected, try the following troubleshooting<a id="_idIndexMarker391"/> steps:</p>
			<ul>
				<li>Is anything hot in the wiring? Hold the wires to the sensor between the thumb and forefinger. <em class="italic">Nothing should be hot or even warming</em>! If so, remove the batteries, turn off the Raspberry Pi, and thoroughly check all wiring against <em class="italic">Figure 8.12</em>.</li>
				<li>If there are syntax errors, please check the code against the examples. You should have installed Python libraries with <strong class="source-inline">pip3</strong> and be running with <strong class="source-inline">python3</strong>.</li>
				<li>If you are still getting errors, or invalid values, please check the code and indentation.</li>
				<li>If the values are always <strong class="source-inline">0</strong>, or the sensor isn't returning any values, then you may have swapped trigger and echo pins. Try swapping the trigger/echo pin numbers in the code and testing it again. <em class="italic">Don't</em> swap the cables on a live Pi! Do this one device at a time.</li>
				<li>If you are still getting no values, ensure you have purchased 3.3 V-compatible systems. The HC-SR04 model will not work with the bare Raspberry Pi.</li>
				<li>If values are way out or drifting, then ensure that the surface you are testing on is hard. Soft surfaces, such as clothes, curtains, or your hand, do not respond as well as glass, wood, metal, or plastic. A wall works well!</li>
				<li>Another<a id="_idIndexMarker392"/> reason for incorrect values is the surface may be too small. Make sure that your surface is quite wide. Anything smaller than about 5 cm square may be harder to measure.</li>
				<li>As a last resort, if one sensor seems fine, and the other wrong, it's possible that a device is faulty. Try swapping the sensors to check this. If the result is different, then a sensor may be wrong. If the result is the same, it is the wiring or code that is wrong.</li>
			</ul>
			<p>You have now troubleshooted your distance sensor and made sure that it works. You have seen it output values to show that it is working and tested it with objects to see its response. Now, let's step up and write a script to avoid obstacles.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor165"/>Avoiding walls – writing a script to avoid obstacles</h1>
			<p>Now<a id="_idIndexMarker393"/> that we have tested both sensors, we can integrate them with our robot class and make obstacle avoidance behavior for them. This behavior loop reads the sensors and then chooses behavior accordingly.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor166"/>Adding the sensors to the robot class</h2>
			<p>So, before <a id="_idIndexMarker394"/>we can use the sensors in a behavior, we need to add<a id="_idIndexMarker395"/> them to the <strong class="source-inline">Robot</strong> class, assigning the correct pin numbers for each side. This way, if pin numbers change or even the interface to a sensor changes, behaviors will not need to change:</p>
			<ol>
				<li value="1">To use the <strong class="source-inline">DistanceSensor</strong> object, we need to import it from <strong class="source-inline">gpiozero</strong>; the new code is in bold:<p class="source-code">from Raspi_MotorHAT import Raspi_MotorHAT</p><p class="source-code"><strong class="bold">from gpiozero import DistanceSensor</strong></p></li>
				<li>We create an instance of one of these <strong class="source-inline">DistanceSensor</strong> objects for each side in the robot class. We need to set these up in the constructor for our robot. We use the same pin numbers and queue length as in our test:<p class="source-code">class Robot:</p><p class="source-code">    def __init__(self, motorhat_addr=0x6f):</p><p class="source-code">        # Setup the motorhat with the passed in address</p><p class="source-code">        self._mh = Raspi_MotorHAT(addr=motorhat_addr)</p><p class="source-code">        # get local variable for each motor</p><p class="source-code">        self.left_motor = self._mh.getMotor(1)</p><p class="source-code">        self.right_motor = self._mh.getMotor(2)</p><p class="source-code"><strong class="bold">        # Setup The Distance Sensors</strong></p><p class="source-code"><strong class="bold">        self.left_distance_sensor = DistanceSensor(echo=17, trigger=27, queue_len=2)</strong></p><p class="source-code"><strong class="bold">        self.right_distance_sensor = DistanceSensor(echo=5, trigger=6, queue_len=2)</strong></p><p class="source-code">        # ensure the motors get stopped when the code exits</p><p class="source-code">        atexit.register(self.stop_all)</p></li>
			</ol>
			<p>Adding this to our robot layer makes it available to behaviors. When we create our robot, the sensors will be sampling distances. Let's make a behavior that uses them.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor167"/>Making the obstacle avoid behaviors</h2>
			<p>This chapter is all about getting a <a id="_idIndexMarker396"/>behavior; how can a robot drive and avoid (most) obstacles? The sensor's specifications limit it, with smaller objects or objects with a soft/fuzzy shell, such as upholstered items, not being detected. Let's start by drawing what we mean in <em class="italic">Figure 8.18</em>:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="Images/B15660_08_18.jpg" alt="Figure 8.18 – Obstacle avoidance basics&#13;&#10;" width="751" height="873"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.18 – Obstacle avoidance basics</p>
			<p>In our example (<em class="italic">Figure 8.18</em>), a basic robot detects a wall, turns away, keeps driving until another wall is detected, and then turns away from that. We can use this to make our first attempt at wall-avoiding behavior.</p>
			<h3 id="_idParaDest-175">First attempt at obstacle avoidance</h3>
			<p>To help us understand this task, the following diagram shows a flow diagram for the behavior:</p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="Images/B15660_08_19.jpg" alt="Figure 8.19 – Obstacle avoidance flowchart&#13;&#10;" width="319" height="714"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.19 – Obstacle avoidance flowchart</p>
			<p>The flow <a id="_idIndexMarker397"/>diagram in <em class="italic">Figure 8.19</em> starts at the top. </p>
			<p>This diagram describes a loop <a id="_idIndexMarker398"/>that does the following:</p>
			<ol>
				<li value="1">The <strong class="bold">Start</strong> box goes into a <strong class="bold">Get Distances</strong> box, which gets the distances from each sensor.</li>
				<li>We test whether the left sensor reads less than 20 cm (a reasonable threshold):<p>a) If so, we set the left motor in reverse to turn the robot away from the obstacle.</p><p>b) Otherwise, we drive the left motor forward.</p></li>
				<li>We now check the right sensor, setting it backward if closer than 20 cm, or forward if not.</li>
				<li>The program waits a short time and loops around again.</li>
			</ol>
			<p>We put this <a id="_idIndexMarker399"/>loop in a <strong class="source-inline">run</strong> method. There›s a small bit of setup required in relation to this. We need to set the pan and tilt to <strong class="source-inline">0</strong> so that it won't obstruct the sensors. I've put this code in <strong class="source-inline">simple_avoid_behavior.py</strong>:</p>
			<ol>
				<li value="1">Start by importing the robot, and <strong class="source-inline">sleep</strong> for timing:<p class="source-code"><strong class="bold">from robot import Robot</strong></p><p class="source-code"><strong class="bold">from time import sleep</strong></p><p class="source-code"><strong class="bold">...</strong></p></li>
				<li>The following class is the basis of our behavior. There is a robot object stored in the behavior. A speed is set, which can be adjusted to make the robot go faster or slower. Too fast, and it has less time to react:<p class="source-code"><strong class="bold">...</strong></p><p class="source-code"><strong class="bold">class ObstacleAvoidingBehavior:</strong></p><p class="source-code"><strong class="bold">    """Simple obstacle avoiding"""</strong></p><p class="source-code"><strong class="bold">    def __init__(self, the_robot):</strong></p><p class="source-code"><strong class="bold">        self.robot = the_robot</strong></p><p class="source-code"><strong class="bold">        self.speed = 60</strong></p><p class="source-code"><strong class="bold">        ...</strong></p></li>
				<li>Now the following method <a id="_idIndexMarker400"/>chooses a speed for each motor, depending on the distance detected by the sensor. A nearer sensor distance turns away from the obstacle:<p class="source-code"><strong class="bold">    ...</strong></p><p class="source-code"><strong class="bold">    def get_motor_speed(self, distance):</strong></p><p class="source-code"><strong class="bold">        """This method chooses a speed for a motor based on the distance from a sensor"""</strong></p><p class="source-code"><strong class="bold">        if distance &lt; 0.2:</strong></p><p class="source-code"><strong class="bold">            return -self.speed</strong></p><p class="source-code"><strong class="bold">        else:</strong></p><p class="source-code"><strong class="bold">            return self.speed</strong></p><p class="source-code"><strong class="bold">    ...</strong></p></li>
				<li>The <strong class="source-inline">run</strong> method is the core, since it has the main loop. We put the pan and tilt mechanism in the middle so that it doesn't obstruct the sensors:<p class="source-code"><strong class="bold">    ...</strong></p><p class="source-code"><strong class="bold">    def run(self):</strong></p><p class="source-code"><strong class="bold">        self.robot.set_pan(0)</strong></p><p class="source-code"><strong class="bold">        self.robot.set_tilt(0)</strong></p></li>
				<li>Now, we start the main loop:<p class="source-code"><strong class="bold">        while True:</strong></p><p class="source-code"><strong class="bold">            # Get the sensor readings in meters</strong></p><p class="source-code"><strong class="bold">            left_distance = self.robot.left_distance_sensor.distance</strong></p><p class="source-code"><strong class="bold">            right_distance = self.robot.right_distance_sensor.distance</strong></p><p class="source-code"><strong class="bold">            ...</strong></p></li>
				<li>We then print out our readings on the console:<p class="source-code"><strong class="bold">            ...</strong></p><p class="source-code"><strong class="bold">            print("Left: {l:.2f}, Right: {r:.2f}".format(l=left_distance, r=right_distance))</strong></p><p class="source-code"><strong class="bold">            ...</strong></p></li>
				<li>Now, we use the distances<a id="_idIndexMarker401"/> with our <strong class="source-inline">get_motor_speed</strong> method and send this to each motor:<p class="source-code"><strong class="bold">            ...</strong></p><p class="source-code"><strong class="bold">            # Get speeds for motors from distances</strong></p><p class="source-code"><strong class="bold">            left_speed = self.get_motor_speed(left_distance)</strong></p><p class="source-code"><strong class="bold">            self.robot.set_left(left_speed)</strong></p><p class="source-code"><strong class="bold">            right_speed = self.get_motor_speed(right_distance)</strong></p><p class="source-code"><strong class="bold">            self.robot.set_right(right_speed)</strong></p></li>
				<li>Since this is our main loop, we wait a short while before we loop again. Under this is the setup and starting behavior:<p class="source-code"><strong class="bold">            ...</strong></p><p class="source-code"><strong class="bold">            # Wait a little</strong></p><p class="source-code"><strong class="bold">            sleep(0.05)</strong></p><p class="source-code"><strong class="bold">bot = Robot()</strong></p><p class="source-code"><strong class="bold">behavior = ObstacleAvoidingBehavior(bot)</strong></p><p class="source-code"><strong class="bold">behavior.run()</strong></p></li>
			</ol>
			<p>The code for this behavior is<a id="_idIndexMarker402"/> now completed and ready to run. It's time to try it out. To test this, set up a test space to be a few square meters wide. Avoid obstacles that the sensor misses, such as upholstered furniture or thin obstacles such as chair legs. I've used folders and plastic toy boxes to make courses for these.</p>
			<p>Send the code to the robot and try it out. It drives until it encounters an obstacle, and then turns away. This kind of works; you can tweak the speeds and thresholds, but the behavior gets stuck in corners and gets confused.</p>
			<p>Perhaps it's time to consider a better strategy.</p>
			<h3 id="_idParaDest-176">More sophisticated object avoidance</h3>
			<p>The previous behavior can<a id="_idIndexMarker403"/> leave the robot stuck. It appears<a id="_idIndexMarker404"/> to be indecisive with some obstacles and occasionally ends up ramming others. It may not stop in time or turn into things. Let's make a better one that drives more smoothly.</p>
			<p>So, what is our strategy? Well, let's think in terms of the sensor nearest to an obstacle, and the furthest. We can work out the speeds of the motor nearest to it, the motor further from it, and a time delay. Our code uses the time delay to be decisive about turning away from a wall, with the time factor controlling how far we turn. This reduces any jitter. Let's make some changes to the last behavior for this: </p>
			<ol>
				<li value="1">First, copy the <strong class="source-inline">simple_avoid_behavior.py</strong> file into a new file called <strong class="source-inline">avoid_behavior.py</strong>.</li>
				<li>We won't be needing <strong class="source-inline">get_motor_speed</strong>, so remove that. We replace it with a function called <strong class="source-inline">get_speeds</strong>. This takes one parameter, <strong class="source-inline">nearest_distance</strong>, which should always be the distance sensor with the lower reading:<p class="source-code"><strong class="bold">...</strong></p><p class="source-code"><strong class="bold">    def get_speeds(self, nearest_distance):</strong></p><p class="source-code"><strong class="bold">        if nearest_distance &gt;= 1.0:</strong></p><p class="source-code"><strong class="bold">            nearest_speed = self.speed</strong></p><p class="source-code"><strong class="bold">            furthest_speed = self.speed</strong></p><p class="source-code"><strong class="bold">            delay = 100</strong></p><p class="source-code"><strong class="bold">        elif nearest_distance &gt; 0.5:</strong></p><p class="source-code"><strong class="bold">            nearest_speed = self.speed</strong></p><p class="source-code"><strong class="bold">            furthest_speed = self.speed * 0.8</strong></p><p class="source-code"><strong class="bold">            delay = 100</strong></p><p class="source-code"><strong class="bold">        elif nearest_distance &gt; 0.2:</strong></p><p class="source-code"><strong class="bold">            nearest_speed = self.speed</strong></p><p class="source-code"><strong class="bold">            furthest_speed = self.speed * 0.6</strong></p><p class="source-code"><strong class="bold">            delay = 100</strong></p><p class="source-code"><strong class="bold">        elif nearest_distance &gt; 0.1:</strong></p><p class="source-code"><strong class="bold">            nearest_speed = -self.speed * 0.4</strong></p><p class="source-code"><strong class="bold">            furthest_speed = -self.speed</strong></p><p class="source-code"><strong class="bold">            delay = 100</strong></p><p class="source-code"><strong class="bold">        else: # collison</strong></p><p class="source-code"><strong class="bold">            nearest_speed = -self.speed</strong></p><p class="source-code"><strong class="bold">            furthest_speed = -self.speed</strong></p><p class="source-code"><strong class="bold">            delay = 250</strong></p><p class="source-code"><strong class="bold">        return nearest_speed, furthest_speed, delay</strong></p><p class="source-code"><strong class="bold">...</strong></p><p>These numbers are all for fine-tuning. The essential factor is that depending on the distance, we slow down the motor further from the obstacle, and if we get too close, it <a id="_idIndexMarker405"/>drives away. Based on the<a id="_idIndexMarker406"/> time delay, and knowing which motor is which, we can drive our robot. </p></li>
				<li>Most of the remaining code stays the same. This is the <strong class="source-inline">run</strong> function you've already seen:<p class="source-code"><strong class="bold">    ...</strong></p><p class="source-code"><strong class="bold">    def run(self):</strong></p><p class="source-code"><strong class="bold">        # Drive forward</strong></p><p class="source-code"><strong class="bold">        self.robot.set_pan(0)</strong></p><p class="source-code"><strong class="bold">        self.robot.set_tilt(0)</strong></p><p class="source-code"><strong class="bold">        while True:</strong></p><p class="source-code"><strong class="bold">            # Get the sensor readings in meters</strong></p><p class="source-code"><strong class="bold">            left_distance = self.robot.left_distance_sensor.distance</strong></p><p class="source-code"><strong class="bold">            right_distance = self.robot.right_distance_sensor.distance            # Display this</strong></p><p class="source-code"><strong class="bold">            self.display_state(left_distance, right_distance)</strong></p><p class="source-code"><strong class="bold">            ...</strong></p></li>
				<li>It now uses the <strong class="source-inline">get_speeds</strong> method to determine a nearest and furthest distance. Notice that we take the <strong class="source-inline">min</strong>, or minimum, of the two distances. We get back the<a id="_idIndexMarker407"/> speeds for both motors and a <a id="_idIndexMarker408"/>delay, and then print out the variables so we can see what's going on:<p class="source-code"><strong class="bold">            ...</strong></p><p class="source-code"><strong class="bold">            # Get speeds for motors from distances</strong></p><p class="source-code"><strong class="bold">            nearest_speed, furthest_speed, delay = self.get_speeds(min(left_distance, right_distance))</strong></p><p class="source-code"><strong class="bold">            print(f"Distances: l {left_distance:.2f}, r {right_distance:.2f}. Speeds: n: {nearest_speed}, f: {furthest_speed}. Delay: {delay}")</strong></p><p class="source-code"><strong class="bold">            ...</strong></p><p>We've used an <em class="italic">f-string</em> here, a further shortcut from <strong class="source-inline">.format</strong> (which we used previously). Putting the letter prefix <strong class="source-inline">f</strong> in front of a string allows us to use local variables in curly brackets in the string. We are still able to use <strong class="source-inline">.2f</strong> to control the number of decimal places.</p></li>
				<li>Now, we check which side is nearer, left or right, and set up the correct motors:<p class="source-code"><strong class="bold">            ...</strong></p><p class="source-code"><strong class="bold">            # Send this to the motors</strong></p><p class="source-code"><strong class="bold">            if left_distance &lt; right_distance:</strong></p><p class="source-code"><strong class="bold">                self.robot.set_left(nearest_speed)</strong></p><p class="source-code"><strong class="bold">                self.robot.set_right(furthest_speed)</strong></p><p class="source-code"><strong class="bold">            else:</strong></p><p class="source-code"><strong class="bold">                self.robot.set_right(nearest_speed)</strong></p><p class="source-code"><strong class="bold">                self.robot.set_left(furthest_speed)</strong></p><p class="source-code"><strong class="bold">            ...</strong></p></li>
				<li>Instead of sleeping a fixed amount of time, we sleep for the amount of time in the <strong class="source-inline">delay</strong> variable. The <a id="_idIndexMarker409"/>delay is in milliseconds, so <a id="_idIndexMarker410"/>we need to multiply it to get seconds:<p class="source-code"><strong class="bold">            ...</strong></p><p class="source-code"><strong class="bold">            # Wait our delay time</strong></p><p class="source-code"><strong class="bold">            sleep(delay * 0.001)</strong></p><p class="source-code"><strong class="bold">...</strong></p></li>
				<li>The rest of the code remains the same. You can find the full code for this file at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter8">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter8</a>.</li>
			</ol>
			<p>When you run this code, you should see smoother avoidance. You may need to tweak the timings and values. The bottom two conditions, reversing and reverse turning, might need to be tuned. Set the timings higher if the robot isn't quite pulling back enough, or lower if it turns away too far.</p>
			<p>There are still flaws in this behavior, though. It does not construct a map at all and has no reverse sensors, so while avoiding objects in front, it can quite quickly reverse into objects behind it. Adding more sensors could resolve some of these problems. Still, we cannot construct a map just yet as our robot does not have the sensors to determine how far it has turned or traveled accurately.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor168"/>Summary</h1>
			<p>In this chapter, we have added sensors to our robot. This is a major step as it makes the robot autonomous, behaving on its own and responding in some way to its environment. You've learned how to add distance sensing to our robots, along with the different kinds of sensors that are available. We've seen code to make it work and test these sensors. We then created behaviors to avoid walls and looked at how to make a simplified but flawed behavior, and how a more sophisticated and smoother behavior would make for a better system.</p>
			<p>With this experience, you can consider how other sensors could be interfaced with your robot, and some simple code to interact with them. You can output data from sensors so you can debug their behavior and create a behavior to make a robot perform some simple navigation on its own.</p>
			<p>In the next chapter, we look further into driving predetermined paths and straight lines using an encoder to make sure that the robot moves far more accurately. We use an encoder to compare our motor's output with our expected goals and get more accurate turns.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor169"/>Exercises</h1>
			<ol>
				<li value="1">Some robots get by with just a single sensor. Can you think of a way of avoiding obstacles reliably with a single sensor?</li>
				<li>We have a pan/tilt mechanism, which we use later for a camera. Consider putting a sensor on this, and how to incorporate this into a behavior.</li>
				<li>The robot behavior we created in this chapter can reverse into things. How could you remedy this? Perhaps make a plan and try to build it.</li>
			</ol>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor170"/>Further reading</h1>
			<p>Please refer to the following links for more information:</p>
			<ul>
				<li>The RCWL-1601 is still quite similar to the HC-SR04. The HC-SR04 data sheet has useful information about its range. You can find the data sheet at <a href="https://www.mouser.com/ds/2/813/HCSR04-1022824.pdf">https://www.mouser.com/ds/2/813/HCSR04-1022824.pdf</a>.</li>
				<li>ModMyPi has a tutorial with an alternative way to wire the original HC-SR04 types, and level shift their IO: <a href="https://www.modmypi.com/blog/hc-sr04-ultrasonic-range-sensor-on-the-raspberry-pi">https://www.modmypi.com/blog/hc-sr04-ultrasonic-range-sensor-on-the-raspberry-pi</a>.</li>
				<li>Raspberry Pi Tutorials also has a breadboard layout and Python script, using <strong class="source-inline">RPi.GPIO</strong> instead of <strong class="source-inline">gpiozero</strong>, at <a href="https://tutorials-raspberrypi.com/raspberry-pi-ultrasonic-sensor-hc-sr04/">https://tutorials-raspberrypi.com/raspberry-pi-ultrasonic-sensor-hc-sr04/</a>.</li>
				<li>We've started to use many pins on the Raspberry Pi. When trying to ascertain which pins to use, I highly recommend visiting the Raspberry Pi GPIO at <a href="https://pinout.xyz/">https://pinout.xyz/</a>.</li>
				<li>We briefly mentioned debug output and refining it. W3schools has an interactive guide to Python format strings at <a href="https://www.w3schools.com/python/ref_string_format.asp">https://www.w3schools.com/python/ref_string_format.asp</a>.</li>
				<li>There are many scholarly articles available on more interesting or sophisticated object behavior. I recommend reading <em class="italic">Simple, Real-Time Obstacle Avoidance Algorithm</em> (<a href="https://pdfs.semanticscholar.org/519e/790c8477cfb1d1a176e220f010d5ec5b1481.pdf">https://pdfs.semanticscholar.org/519e/790c8477cfb1d1a176e220f010d5ec5b1481.pdf</a>) for mobile robots for a more in-depth look at these behaviors.</li>
			</ul>
		</div>
	</div></body></html>