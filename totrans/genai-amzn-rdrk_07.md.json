["```py\n\"Optical character recognition (OCR) is used to convert text in images, scanned documents, or handwritten text into machine-readable and editable text format. It has several applications, including:\nDigitizing printed documents, books, and archives for easier storage and search.\nExtracting text from images or photographs for indexing or further processing.\nAutomating data entry tasks by extracting information from forms, invoices, or receipts.\nEnabling text-to-speech capabilities for visually impaired users.\nFacilitating translation of text between languages by first converting it to editable format.\"\n```", "```py\nprompt = \"\"\"You are an expert AI assistant. You will answer questions in a succinct manner. If you are unsure about the\nanswer, say 'I am not sure about this answer'\nQuestion: How can I connect my old Samsung TV with my Mac laptop?\nAnswer:\"\"\"\nparameters = {\n    \"maxTokenCount\":1024,\n    \"temperature\":0.1,\n    \"topP\":0.8,\n    \"stopSequences\":[]\n    }\n```", "```py\n\"The Arctic fox has several key adaptations that allow it to survive in the cold Arctic environments:\n1\\. Dense fur insulation to keep it warm.\n2\\. A compact body shape that limits exposure to the cold.\n3\\. A thick underfur that insulates it from the cold.\n4\\. A dense, insulating guard hair coat on top.\nThese physical adaptations, such as thick fur coats and compact body shape, enable the Arctic fox to withstand the extreme cold temperatures of the Arctic tundra region.\"\n```", "```py\n# Import the respective libraries\nimport boto3\nimport botocore\nimport os\nimport json\nimport sys\n#create bedrock runtime client\nbedrock_runtime = boto3.client('bedrock-runtime')\n#Provide the model paramters\nmodel_parameters = {\n     \"maxTokenCount\":1024,\n     \"temperature\":0,\n     \"stopSequences\":[],\n     \"topP\":0.9\n     }\n#Provide relevant context to the model\ncontext= \"\"\"Using your Apple Watch to locate a misplaced iPhone is a handy feature that can save you a lot of time and frustration. The process typically begins by opening the Control Center on your watch by swiping up from thebottom of the watch face. From there, you'll see an icon that looks like a ringing iPhone - tapping this will remotely activate a loud pinging sound on your iPhone, even if it's on silent mode. If you're within earshot, simply follow the sound to track down your missing device. Alternatively, you can use the FindMy app on your Apple Watch, which provides a map showing the last known location of your iPhone. Tap the \"Devices\" tab, select your iPhone, and it will display its location, as well as give you the option to force it to emit a sound to aid in your search. For an even quicker option, you can simply raise your wrist and ask Siri \"Hey Siri, find my iPhone,\" and the virtual assistant will attempt to pinpoint the location of your iPhone and provide directions. However, for any of these methods to work, your iPhone must be powered on, connected to a cellular or WiFi network, and have the Find My feature enabled in Settings under your Apple ID. As long as those criteria are met, your Apple Watch can be a powerful tool for tracking down a wandering iPhone.\"\"\"\n#Take the entire context/excerpt provided above and augment to the model along with the input question\nquestion = \"How can I find my iPhone from my Apple watch in case I lose my phone?\"\nprompt_data = f\"\"\" Answer the user's question solely only on the information provided between <></> XML tags. Think step by step and provide detailed instructions.\n<context>\n{context}\n</context>\nQuestion: {question}\nAnswer:\"\"\"\n#Now, you can Invoke the foundation model using boto3 to generate the output response.\nbody = json.dumps({\"inputText\": prompt_data, \"textGenerationConfig\": model_parameters})\naccept = \"application/json\"\ncontentType = \"application/json\"\n# You can change this modelID to use an alternate version from the model provider\nmodelId = \"amazon.titan-tg1-large\"\nresponse = bedrock_runtime.invoke_model(\n    body=body, modelId=modelId, accept=accept, contentType=contentType)\ngenerated_response_body = json.loads(response.get(\"body\").read())\nprint(generated_response_body.get(\"results\")[0].get(\"outputText\").strip())\n```", "```py\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n# Load the document\nloader = TextLoader('path/to/document.txt')\ndocuments = loader.load()\n# Split the documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n# Create embeddings and store in Chroma vector store\nfrom langchain_community.embeddings import BedrockEmbeddings\nembeddings = BedrockEmbeddings()\ndb = Chroma.from_documents(texts, embeddings)\n```", "```py\n#importing the respective libraries\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n!pip install chromadb\nImport boto3\nImport botocore\n#Create client side Amazon Bedrock connection with Boto3 library\nregion = os.environ.get(\"AWS_REGION\")\nbedrock_runtime = boto3.client(\n    service_name='bedrock-runtime',\n    region_name=region,\n)\n# Load the document\nloader = TextLoader('path/to/document.txt')\ndocuments = loader.load()\n# Split the documents into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ntexts = text_splitter.split_documents(documents)\n# Create embeddings and store in Chroma vector store\nfrom langchain_community.embeddings import BedrockEmbeddings\nembeddings = BedrockEmbeddings(client=boto3_bedrock, model_id=\"amazon.titan-embed-text-v1\")\ndb = Chroma.from_documents(texts, embeddings)\n# Enter a user query\nquery = \"Enter your query here\"\n#Perform Similarity search by finding relevant information from the embedded data\nretriever = db.similarity_search(query, k=3)\nfull_context = '\\n'.join([f'Document {indexing+1}: ' + i.page_content for indexing, i in enumerate(retriever)])\nprint(full_context)\n#Since we have the relevant documents identified within \"full_context\", we can use the LLM to generate an optimal answer based on the retreived documents. Prior to that, let us format our prompt template before feeding to the LLM.\nprompt_template = f\"\"\"Answer the user's question solely only on the information provided between <></> XML tags. Think step by step and provide detailed instructions.\n<context>\n{full_context}\n</context>\nQuestion: {query}\nAnswer:\"\"\"\nPROMPT = PromptTemplate.from_template(prompt_template)\n#Prompt data input creation to feed to the LLM\nprompt_data_input = PROMPT.format(human_input=query, context=context_string)\n#Now, you can Invoke the foundation model using boto3 to generate the output response.\nbody = json.dumps({\"inputText\": prompt_data_input, \"textGenerationConfig\": model_parameters})\naccept = \"application/json\"\ncontentType = \"application/json\"\n# You can change this modelID to use an alternate version from the model provider\nmodelId = \"amazon.titan-tg1-large\"\nresponse = bedrock_runtime.invoke_model(\n    body=body, modelId=modelId, accept=accept, contentType=contentType)\ngenerated_response_body = json.loads(response.get(\"body\").read())\nprint(generated_response_body.get(\"results\")[0].get(\"outputText\").strip())\n```", "```py\n    from langchain import PromptTemplate, LLMChain\n    ```", "```py\n    from langchain_community.llms import Bedrock\n    ```", "```py\n    # Define the prompt template\n    ```", "```py\n    template = \"\"\"You are a helpful travel assistant. You will be provided with information about a user's travel plans, and your task is to provide relevant suggestions and recommendations based on their preferences and requirements.\n    ```", "```py\n    Travel Details: {travel_details}\n    ```", "```py\n    Using the information provided, suggest some activities, attractions, restaurants, or any other recommendations that would enhance the user's travel experience. Provide your response in a conversational and friendly tone.\"\"\"\n    ```", "```py\n    # Create the prompt template object\n    ```", "```py\n    prompt = PromptTemplate(template=template, input_variables=[\"travel_details\"])\n    ```", "```py\n    # Sample user input\n    ```", "```py\n    user_travel_details = \"\"\"I'm planning a 5-day trip to Paris with my family (two adults and two children, ages 8 and 12). We're interested in exploring the city's history, architecture, and cultural attractions. We also enjoy trying local cuisine and engaging in family-friendly activities.\"\"\"\n    ```"]