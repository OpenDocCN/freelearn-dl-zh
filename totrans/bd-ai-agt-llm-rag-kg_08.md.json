["```py\nself.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)\nself.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\nself.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\nself.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\nself.lstm = nn.LSTMCell(32 * 6 * 6, 512)\n```", "```py\nself.critic_linear = nn.Linear(512, 1)\nself.actor_linear = nn.Linear(512, num_actions)\n```", "```py\ndef forward(self, x, hx, cx):\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x = F.relu(self.conv4(x))\n        hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))\n        return self.actor_linear(hx), self.critic_linear(hx), hx, cx\n```", "```py\nimport torch\nclass GlobalAdam(torch.optim.Adam):\n    def __init__(self, params, lr):\n        super(GlobalAdam, self).__init__(params, lr=lr)\n        for group in self.param_groups:\n            for p in group['params']:\n                state = self.state[p]\n                state['step'] = 0\n                state['exp_avg'] = torch.zeros_like(p.data)\n                state['exp_avg_sq'] = torch.zeros_like(p.data)\n                state['exp_avg'].share_memory_()\n                state['exp_avg_sq'].share_memory_()\n```", "```py\ndef train(opt):\n    torch.manual_seed(123)\n    if os.path.isdir(opt.log_path):\n        shutil.rmtree(opt.log_path)\n    os.makedirs(opt.log_path)\n    if not os.path.isdir(opt.saved_path):\n        os.makedirs(opt.saved_path)\n    mp = _mp.get_context(\"spawn\")\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n    global_model = ActorCritic(num_states, num_actions)\n    if opt.use_gpu:\n        global_model.cuda()\n    global_model.share_memory()\n    if opt.load_from_previous_stage:\n        if opt.stage == 1:\n            previous_world = opt.world - 1\n            previous_stage = 4\n        else:\n            previous_world = opt.world\n            previous_stage = opt.stage - 1\n        file_ = \"{}/A3CSuperMarioBros{}_{}\".format(opt.saved_path, previous_world, previous_stage)\n        if os.path.isfile(file_):\n            global_model.load_state_dict(torch.load(file_))\n    optimizer = GlobalAdam(global_model.parameters(), lr=opt.lr)\n    processes = []\n    for index in range(opt.num_processes):\n        if index == 0:\n            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer, True))\n        else:\n            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer))\n        process.start()\n        processes.append(process)\n    process = mp.Process(target=local_test, args=(opt.num_processes, opt, global_model))\n    process.start()\n    processes.append(process)\n    for process in processes:\n        process.join()\n```", "```py\ndef local_train(index, opt, global_model, optimizer, save=False):\n    torch.manual_seed(123 + index)\n    if save:\n        start_time = timeit.default_timer()\n    writer = SummaryWriter(opt.log_path)\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n    local_model = ActorCritic(num_states, num_actions)\n    if opt.use_gpu:\n        local_model.cuda()\n```", "```py\nlocal_model.train()\n    state = torch.from_numpy(env.reset())\n    if opt.use_gpu:\n        state = state.cuda()\n    done = True\n    curr_step = 0\n    curr_episode = 0\n    while True:\n        if save:\n            if curr_episode % opt.save_interval == 0 and curr_episode > 0:\n                torch.save(global_model.state_dict(),\n                           \"{}/a3c_super_mario_bros_{}_{}\".format(opt.saved_path, opt.world, opt.stage))\n            print(\"Process {}. Episode {}\".format(index, curr_episode))\n        curr_episode += 1\n        local_model.load_state_dict(global_model.state_dict())\n        if done:\n            h_0 = torch.zeros((1, 512), dtype=torch.float)\n            c_0 = torch.zeros((1, 512), dtype=torch.float)\n        else:\n            h_0 = h_0.detach()\n            c_0 = c_0.detach()\n        if opt.use_gpu:\n            h_0 = h_0.cuda()\n            c_0 = c_0.cuda()\n```", "```py\nfor _ in range(opt.num_local_steps):\n            curr_step += 1\n            logits, value, h_0, c_0 = local_model(state, h_0, c_0)\n            policy = F.softmax(logits, dim=1)\n            log_policy = F.log_softmax(logits, dim=1)\n            entropy = -(policy * log_policy).sum(1, keepdim=True)\n            m = Categorical(policy)\n            action = m.sample().item()\n            state, reward, done, _ = env.step(action)\n            state = torch.from_numpy(state)\n            if opt.use_gpu:\n                state = state.cuda()\n            if curr_step > opt.num_global_steps:\n                done = True\n            if done:\n                curr_step = 0\n                state = torch.from_numpy(env.reset())\n                if opt.use_gpu:\n                    state = state.cuda()\n            values.append(value)\n            log_policies.append(log_policy[0, action])\n            rewards.append(reward)\n            entropies.append(entropy)\n            if done:\n                break\n        R = torch.zeros((1, 1), dtype=torch.float)\n        if opt.use_gpu:\n            R = R.cuda()\n        if not done:\n            _, R, _, _ = local_model(state, h_0, c_0)\n```", "```py\n        gae = torch.zeros((1, 1), dtype=torch.float)\n        if opt.use_gpu:\n            gae = gae.cuda()\n        actor_loss = 0\n        critic_loss = 0\n        entropy_loss = 0\n        next_value = R\n        for value, log_policy, reward, entropy in list(zip(values, log_policies, rewards, entropies))[::-1]:\n            gae = gae * opt.gamma * opt.tau\n            gae = gae + reward + opt.gamma * next_value.detach() - value.detach()\n            next_value = value\n            actor_loss = actor_loss + log_policy * gae\n            R = R * opt.gamma + reward\n            critic_loss = critic_loss + (R - value) ** 2 / 2\n            entropy_loss = entropy_loss + entropy\n        total_loss = -actor_loss + critic_loss - opt.beta * entropy_loss\n        writer.add_scalar(\"Train_{}/Loss\".format(index), total_loss, curr_episode)\n        optimizer.zero_grad()\n        total_loss.backward()\n        for local_param, global_param in zip(local_model.parameters(), global_model.parameters()):\n            if global_param.grad is not None:\n                break\n            global_param._grad = local_param.grad\n        optimizer.step()\n        if curr_episode == int(opt.num_global_steps / opt.num_local_steps):\n            print(\"Training process {} terminated\".format(index))\n            if save:\n                end_time = timeit.default_timer()\n                print('The code runs for %.2f s ' % (end_time - start_time))\n            return\n```", "```py\ndef local_test(index, opt, global_model):\n    torch.manual_seed(123 + index)\n    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)\n    local_model = ActorCritic(num_states, num_actions)\n    local_model.eval()\n    state = torch.from_numpy(env.reset())\n    done = True\n    curr_step = 0\n    actions = deque(maxlen=opt.max_actions)\n    while True:\n        curr_step += 1\n        if done:\n            local_model.load_state_dict(global_model.state_dict())\n        with torch.no_grad():\n            if done:\n                h_0 = torch.zeros((1, 512), dtype=torch.float)\n                c_0 = torch.zeros((1, 512), dtype=torch.float)\n            else:\n                h_0 = h_0.detach()\n                c_0 = c_0.detach()\n        logits, value, h_0, c_0 = local_model(state, h_0, c_0)\n        policy = F.softmax(logits, dim=1)\n        action = torch.argmax(policy).item()\n        state, reward, done, _ = env.step(action)\n        env.render()\n        actions.append(action)\n        if curr_step > opt.num_global_steps or actions.count(actions[0]) == actions.maxlen:\n            done = True\n        if done:\n            curr_step = 0\n            actions.clear()\n            state = env.reset()\n        state = torch.from_numpy(state)\n```"]