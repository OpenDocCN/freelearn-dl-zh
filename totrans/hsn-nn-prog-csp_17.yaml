- en: GRUs Compared to LSTMs, RNNs, and Feedforward networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与LSTMs、RNNs和前馈网络相比的GRUs
- en: In this chapter, we're going to talk about **gated recurrent units** (**GRU**).
    We will also compare them to LSTMs, which we learned about in the previous chapter.
    As you know, LSTMs have been around since 1987 and are among the most widely used
    models in Deep Learning for NLP today. GRUs, however, were first presented in
    2014, are a simpler variant of LSTMs that share many of the same properties, train
    easier and faster, and typically have less computational complexity.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**门控循环单元**（**GRU**）。我们还将将其与我们在上一章中学到的LSTMs进行比较。正如您所知，LSTMs自1987年以来一直存在，并且是目前在深度学习NLP中最广泛使用的模型之一。然而，GRUs首次在2014年提出，是LSTMs的一个更简单的变体，具有许多相同的属性，训练更容易、更快，并且通常具有更少的计算复杂度。
- en: 'In this chapter, we will learn about the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下内容：
- en: GRUs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRUs
- en: How GRUs differ from LSTMs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU与LSTM的不同之处
- en: How to implement a GRU
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现GRU
- en: GRU, LTSM, RNN, and Feedforward comparisons
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU、LTSM、RNN和前馈比较
- en: Network differences
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络差异
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have a basic knowledge of .NET development using Microsoft
    Visual Studio and C#. You will need to download the code for this chapter from
    the book website.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要具备使用Microsoft Visual Studio和C#进行.NET开发的基本知识。您需要从本书网站下载本章的代码。
- en: Check out the following video to see Code in Action: [http://bit.ly/2OHd7o5](http://bit.ly/2OHd7o5).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际应用：[http://bit.ly/2OHd7o5](http://bit.ly/2OHd7o5)。
- en: QuickNN
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QuickNN
- en: 'To follow along with the code, you should have the QuickNN solution open inside
    Microsoft Visual Studio. We will be using this code to explain in detail some
    of the finer points as well as comparisons between coding the different networks.
    Here is the solution you should have loaded:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随代码，您应该在Microsoft Visual Studio中打开QuickNN解决方案。我们将使用此代码来详细解释一些细微之处以及不同网络之间的比较。以下是您应该加载的解决方案：
- en: '![](img/4b001255-96d0-4503-b3a4-d79cf7be7ca0.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b001255-96d0-4503-b3a4-d79cf7be7ca0.png)'
- en: Solution
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Solution
- en: Understanding GRUs
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解GRUs
- en: 'GRUs are a cousin to the long short-term memory recurrent neural networks.
    Both LSTM and GRU networks have additional parameters that control when and how
    their internal memory is updated. Both can capture long- and short-term dependencies
    in sequences. The GRU networks, however, involve less parameters than their LSTM
    cousins, and as a result, are faster to train. The GRU learns how to use its reset
    and forget gates in order to make longer term predictions while enforcing memory
    protection. Let''s look at a simple diagram of a GRU:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: GRUs是长短期记忆循环神经网络的一个分支。LSTM和GRU网络都有额外的参数来控制它们内部记忆何时以及如何更新。两者都可以捕捉序列中的长短期依赖关系。然而，GRU网络涉及的参数比它们的LSTM表亲要少，因此训练速度更快。GRU学习如何使用其重置和遗忘门来做出长期预测，同时执行记忆保护。让我们看看一个简单的GRU图示：
- en: '![](img/2efeafa7-af09-4338-bf4c-bba5286c6e1f.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2efeafa7-af09-4338-bf4c-bba5286c6e1f.png)'
- en: GRU
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: GRU
- en: Differences between LSTM and GRU
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LSTM和GRU之间的差异
- en: 'There are a few subtle differences between a LSTM and a GRU, although to be
    perfectly honest, there are more similarities than differences! For starters,
    a GRU has one less gate than an LSTM. As you can see in the following diagram,
    an LSTM has an input gate, a forget gate, and an output gate. A GRU, on the other
    hand, has only two gates, a reset gate and an update gate. The reset gate determines
    how to combine new inputs with the previous memory, and the update gate defines
    how much of the previous memory remains:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LSTM和GRU之间有一些细微的区别，但坦白说，它们之间的相似之处要多于不同之处！首先，GRU比LSTM少一个门。如图所示，LSTM有一个输入门、一个遗忘门和一个输出门。另一方面，GRU只有两个门，一个是重置门，一个是更新门。重置门决定了如何将新输入与之前的记忆结合，而更新门定义了之前记忆保留的程度：
- en: '![](img/2cd83c9c-b843-4099-9df8-98272cf6d59c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cd83c9c-b843-4099-9df8-98272cf6d59c.png)'
- en: LSTM vs GRU
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM与GRU的比较
- en: Another interesting fact is that if we set the reset gate to all 1s and the
    update gate to all 0s, do you know what we have? If you guessed a plain old recurrent
    neural network, you'd be right!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的事实是，如果我们把重置门设为全1，更新门设为全0，你知道我们有什么吗？如果你猜到是一个普通的循环神经网络，你就对了！
- en: 'Here are the key differences between a LSTM and a GRU:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是LSTM和GRU之间的关键区别：
- en: A GRU has two gates, a LSTM has three.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个GRU有两个门，一个LSTM有三个。
- en: GRUs do not have an internal memory cell that is different from the exposed
    hidden state. This is because the output gate that the LSTM has does.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU没有与暴露的隐藏状态不同的内部记忆单元。这是因为LSTM有输出门。
- en: The input and forget gates are coupled by an update gate that weighs the old
    and new content.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入门和遗忘门通过一个更新门耦合，该更新门权衡新旧内容。
- en: The reset gate is applied directly to the previous hidden state.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重置门直接应用于前一个隐藏状态。
- en: We do not apply a second non-linearity when computing the GRU output.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在计算GRU输出时，我们不应用第二个非线性函数。
- en: There is no output gate, the weighted sum is what becomes the output.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有输出门，加权求和就是输出。
- en: Using a GRU versus a LSTM
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GRU与LSTM的比较
- en: Since GRUs are relatively new, the question usually arises as to which network
    type to use and when. To be honest, there really is no clear winner here, as GRUs
    have yet to mature, and LSTM variants seem to pop up every month. GRUs do have
    fewer parameters, and theoretically may train a bit faster than a LSTM. It may
    also theoretically need less data than a LSTM. On the other hand, if you have
    a lot of data, the extra power of the LSTM may work better for you.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GRU相对较新，通常会出现一个问题，即何时使用哪种网络类型。说实话，这里真的没有明确的赢家，因为GRU尚未成熟，而LSTM变体似乎每个月都会出现。GRU确实有更少的参数，理论上可能训练得比LSTM快一些。它也可能理论上需要比LSTM更少的数据。另一方面，如果你有大量数据，LSTM的额外功能可能更适合你。
- en: Coding different networks
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写不同的网络
- en: 'In this section, we are going to look at the sample code we described earlier
    in this chapter. We specifically are going to look at how we build different networks.
    The `NetworkBuilder` is our main object for building the four different types
    of networks we need for this exercise. You can feel free to modify it and add
    additional networks if you so desire. Currently, it supports the following networks:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看本章前面描述的示例代码。我们将特别查看我们如何构建不同的网络。`NetworkBuilder`是我们构建本练习所需的四种不同类型网络的主要对象。你可以随意修改它并添加额外的网络，如果你愿意的话。目前，它支持以下网络：
- en: LSTM
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM
- en: RNN
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN
- en: GRU
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GRU
- en: Feedforward
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前馈
- en: The one thing that you will notice in our sample network is that the only difference
    between networks is how the network itself is created via the `NetworkBuilder`.
    All the remaining code stays the same. You will also note if you look through
    the example source code that the number of iterations or epochs is much lower
    in the GRU sample. This is because GRUs are typically easier to train and therefore
    require fewer iterations. While our normal RNN training is complete somewhere
    around 50,000 iterations (we let it go to 100,000 just in case), our GRU training
    loop completes usually in under 10,000 iterations, which is a very large computational
    saving.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在我们的示例网络中注意到的一件事是，网络之间的唯一区别在于网络本身是通过`NetworkBuilder`创建的。所有其余的代码都保持不变。如果你查看示例源代码，你还会注意到GRU示例中的迭代次数或周期数要低得多。这是因为GRU通常更容易训练，因此需要更少的迭代。虽然我们的正常RNN训练大约在50,000次迭代（我们让它达到100,000次以防万一）完成，但我们的GRU训练循环通常在10,000次迭代以下完成，这是一个非常大的计算节省。
- en: Coding an LSTM
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写LSTM
- en: 'To construct a LSTM, we simply call the `MakeLstm()` function of our `NetworkBuilder`.
    This function will ingest several input parameters and return to us a network
    object:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个LSTM，我们只需调用我们的`NetworkBuilder`的`MakeLstm()`函数。这个函数将接受几个输入参数，并返回给我们一个网络对象：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you can see, internally this calls our `MakeLSTM()` function inside the
    `NetworkBuilder` object. Here is a look at that code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，内部它调用了`NetworkBuilder`对象内部的`MakeLSTM()`函数。以下是该代码的查看：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Add all of the hidden layers:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 添加所有隐藏层：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Add the feed forward layer:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 添加前馈层：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create the network:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 创建网络：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Coding a GRU
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写GRU
- en: 'To construct a gated recurrent unit, we simply call the `MakeGru()` function
    of our `NetworkBuilder` as shown here:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个门控循环单元，我们只需像这里所示的那样调用我们的`NetworkBuilder`的`MakeGru()`函数：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `MakeGru()` function calls the same named function internally to construct
    our GRU network. Here is a look at how it does it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`MakeGru()`函数在内部调用同名的函数来构建我们的GRU网络。以下是它是如何做到这一点的：'
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Comparing LSTM, GRU, Feedforward, and RNN operations
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较LSTM、GRU、前馈和RNN操作
- en: In order to help you see the difference in both the creation and results of
    all the network objects we have been dealing with, I created the sample code that
    follows. This sample will allow you to see the difference in training times for
    all four of the network types we have here. As stated previously, the GRU is the
    easiest to train and therefore will complete faster (in less iterations) than
    the other networks. When executing the code, you will see that the GRU achieves
    the optimal error rate typically in under 10,000 iterations, while a conventional
    RNN and/or LSTM can take 50,000 or more iterations to converge properly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您看到我们一直在处理的所有网络对象在创建和结果上的差异，我创建了下面的示例代码。这个示例将让您看到我们这里四种网络类型在训练时间上的差异。正如之前所述，GRU是最容易训练的，因此将比其他网络更快完成（迭代次数更少）。在执行代码时，您将看到GRU通常在10,000次迭代以下就达到最佳错误率，而传统的RNN和/或LSTM可能需要50,000次或更多迭代才能正确收敛。
- en: 'Here is what our sample code looks like:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们的示例代码的样子：
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And here is the output from the sample running:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是示例运行的结果：
- en: '![](img/b9cbe876-a369-4124-8446-6fb6922bd871.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b9cbe876-a369-4124-8446-6fb6922bd871.png)'
- en: Output 1
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 1
- en: '![](img/5aa428f9-17b8-4150-876f-db6080f6bc73.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5aa428f9-17b8-4150-876f-db6080f6bc73.png)'
- en: Output 2
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 2
- en: Now, let's look at how we create the GRU network and run the program. In the
    following code segment, we will use our XOR Dataset generator to generate random
    data for us. For our network, we will have 2 inputs, 1 hidden layer with 3 neurons,
    and 1 output. Our learning rate is set to 0.001 and our standard deviation is
    set to 0.08.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何创建GRU网络并运行程序。在下面的代码段中，我们将使用我们的XOR数据集生成器为我们生成随机数据。对于我们的网络，我们将有2个输入，1个包含3个神经元的隐藏层，以及1个输出。我们的学习率设置为0.001，我们的标准差设置为0.08。
- en: 'We call our `NetworkBuilder`object, which is responsible for creating all our
    network variants. We pass all our described parameters to the `NetworkBuilder`.
    Once our network object is created we pass this variable to our trainer and train
    the network. Once the network training is completed we then test our network to
    ensure our results are satisfactory. When we create our Graph object for testing,
    we are sure to pass false to the constructor to let it know that we do not need
    back propagation:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用我们的`NetworkBuilder`对象，该对象负责创建我们所有的网络变体。我们将所有描述的参数传递给`NetworkBuilder`。一旦我们的网络对象创建完成，我们就将这个变量传递给我们的训练器并训练网络。一旦网络训练完成，我们就测试我们的网络以确保我们的结果是令人满意的。当我们创建用于测试的Graph对象时，我们确保将false传递给构造函数，以让它知道我们不需要反向传播：
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Network differences
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络差异
- en: 'As mentioned earlier, the only difference between our networks are the layers
    that are created and added to the network object. In an LSTM we will add LSTM
    layers, and in a GRU, unsurprisingly, we will add GRU layers, and so forth. All
    four types of creation functions are displayed as follows for you to compare:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们网络之间的唯一区别是创建并添加到网络对象中的层。在LSTM中，我们将添加LSTM层，在GRU中，不出所料，我们将添加GRU层，依此类推。以下显示了所有四种创建函数，供您比较：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Summary
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about GRUs. We showed how they compared to, and
    differed from, LSTM Networks. We also showed you an example program that tested
    all the network types we discussed and produced their outputs. We also compared
    how these networks are created.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了GRU。我们展示了它们与LSTM网络的比较和区别。我们还向您展示了一个示例程序，该程序测试了我们讨论的所有网络类型并生成了它们的输出。我们还比较了这些网络的创建方式。
- en: I hope you enjoyed your journey with me throughout this book. As we as authors
    try and better understand what readers would like to see and hear, I welcome your
    constructive comments and feedback, which will only help to make the book and
    source code better. Till the next book, happy coding!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您在这本书的旅程中过得愉快。作为作者，我们试图更好地了解读者想要看到和听到什么，我欢迎您的建设性评论和反馈，这将有助于使本书和源代码更加完善。直到下一本书，祝您编码愉快！
