- en: GRUs Compared to LSTMs, RNNs, and Feedforward networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to talk about **gated recurrent units** (**GRU**).
    We will also compare them to LSTMs, which we learned about in the previous chapter.
    As you know, LSTMs have been around since 1987 and are among the most widely used
    models in Deep Learning for NLP today. GRUs, however, were first presented in
    2014, are a simpler variant of LSTMs that share many of the same properties, train
    easier and faster, and typically have less computational complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: GRUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How GRUs differ from LSTMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement a GRU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRU, LTSM, RNN, and Feedforward comparisons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network differences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have a basic knowledge of .NET development using Microsoft
    Visual Studio and C#. You will need to download the code for this chapter from
    the book website.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see Code in Action: [http://bit.ly/2OHd7o5](http://bit.ly/2OHd7o5).
  prefs: []
  type: TYPE_NORMAL
- en: QuickNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along with the code, you should have the QuickNN solution open inside
    Microsoft Visual Studio. We will be using this code to explain in detail some
    of the finer points as well as comparisons between coding the different networks.
    Here is the solution you should have loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b001255-96d0-4503-b3a4-d79cf7be7ca0.png)'
  prefs: []
  type: TYPE_IMG
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GRUs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GRUs are a cousin to the long short-term memory recurrent neural networks.
    Both LSTM and GRU networks have additional parameters that control when and how
    their internal memory is updated. Both can capture long- and short-term dependencies
    in sequences. The GRU networks, however, involve less parameters than their LSTM
    cousins, and as a result, are faster to train. The GRU learns how to use its reset
    and forget gates in order to make longer term predictions while enforcing memory
    protection. Let''s look at a simple diagram of a GRU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2efeafa7-af09-4338-bf4c-bba5286c6e1f.png)'
  prefs: []
  type: TYPE_IMG
- en: GRU
  prefs: []
  type: TYPE_NORMAL
- en: Differences between LSTM and GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a few subtle differences between a LSTM and a GRU, although to be
    perfectly honest, there are more similarities than differences! For starters,
    a GRU has one less gate than an LSTM. As you can see in the following diagram,
    an LSTM has an input gate, a forget gate, and an output gate. A GRU, on the other
    hand, has only two gates, a reset gate and an update gate. The reset gate determines
    how to combine new inputs with the previous memory, and the update gate defines
    how much of the previous memory remains:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cd83c9c-b843-4099-9df8-98272cf6d59c.png)'
  prefs: []
  type: TYPE_IMG
- en: LSTM vs GRU
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting fact is that if we set the reset gate to all 1s and the
    update gate to all 0s, do you know what we have? If you guessed a plain old recurrent
    neural network, you'd be right!
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key differences between a LSTM and a GRU:'
  prefs: []
  type: TYPE_NORMAL
- en: A GRU has two gates, a LSTM has three.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRUs do not have an internal memory cell that is different from the exposed
    hidden state. This is because the output gate that the LSTM has does.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The input and forget gates are coupled by an update gate that weighs the old
    and new content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reset gate is applied directly to the previous hidden state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We do not apply a second non-linearity when computing the GRU output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no output gate, the weighted sum is what becomes the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a GRU versus a LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since GRUs are relatively new, the question usually arises as to which network
    type to use and when. To be honest, there really is no clear winner here, as GRUs
    have yet to mature, and LSTM variants seem to pop up every month. GRUs do have
    fewer parameters, and theoretically may train a bit faster than a LSTM. It may
    also theoretically need less data than a LSTM. On the other hand, if you have
    a lot of data, the extra power of the LSTM may work better for you.
  prefs: []
  type: TYPE_NORMAL
- en: Coding different networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to look at the sample code we described earlier
    in this chapter. We specifically are going to look at how we build different networks.
    The `NetworkBuilder` is our main object for building the four different types
    of networks we need for this exercise. You can feel free to modify it and add
    additional networks if you so desire. Currently, it supports the following networks:'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GRU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feedforward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The one thing that you will notice in our sample network is that the only difference
    between networks is how the network itself is created via the `NetworkBuilder`.
    All the remaining code stays the same. You will also note if you look through
    the example source code that the number of iterations or epochs is much lower
    in the GRU sample. This is because GRUs are typically easier to train and therefore
    require fewer iterations. While our normal RNN training is complete somewhere
    around 50,000 iterations (we let it go to 100,000 just in case), our GRU training
    loop completes usually in under 10,000 iterations, which is a very large computational
    saving.
  prefs: []
  type: TYPE_NORMAL
- en: Coding an LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To construct a LSTM, we simply call the `MakeLstm()` function of our `NetworkBuilder`.
    This function will ingest several input parameters and return to us a network
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, internally this calls our `MakeLSTM()` function inside the
    `NetworkBuilder` object. Here is a look at that code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Add all of the hidden layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the feed forward layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Coding a GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To construct a gated recurrent unit, we simply call the `MakeGru()` function
    of our `NetworkBuilder` as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `MakeGru()` function calls the same named function internally to construct
    our GRU network. Here is a look at how it does it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Comparing LSTM, GRU, Feedforward, and RNN operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to help you see the difference in both the creation and results of
    all the network objects we have been dealing with, I created the sample code that
    follows. This sample will allow you to see the difference in training times for
    all four of the network types we have here. As stated previously, the GRU is the
    easiest to train and therefore will complete faster (in less iterations) than
    the other networks. When executing the code, you will see that the GRU achieves
    the optimal error rate typically in under 10,000 iterations, while a conventional
    RNN and/or LSTM can take 50,000 or more iterations to converge properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what our sample code looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the output from the sample running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9cbe876-a369-4124-8446-6fb6922bd871.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 1
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5aa428f9-17b8-4150-876f-db6080f6bc73.png)'
  prefs: []
  type: TYPE_IMG
- en: Output 2
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how we create the GRU network and run the program. In the
    following code segment, we will use our XOR Dataset generator to generate random
    data for us. For our network, we will have 2 inputs, 1 hidden layer with 3 neurons,
    and 1 output. Our learning rate is set to 0.001 and our standard deviation is
    set to 0.08.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call our `NetworkBuilder`object, which is responsible for creating all our
    network variants. We pass all our described parameters to the `NetworkBuilder`.
    Once our network object is created we pass this variable to our trainer and train
    the network. Once the network training is completed we then test our network to
    ensure our results are satisfactory. When we create our Graph object for testing,
    we are sure to pass false to the constructor to let it know that we do not need
    back propagation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Network differences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned earlier, the only difference between our networks are the layers
    that are created and added to the network object. In an LSTM we will add LSTM
    layers, and in a GRU, unsurprisingly, we will add GRU layers, and so forth. All
    four types of creation functions are displayed as follows for you to compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about GRUs. We showed how they compared to, and
    differed from, LSTM Networks. We also showed you an example program that tested
    all the network types we discussed and produced their outputs. We also compared
    how these networks are created.
  prefs: []
  type: TYPE_NORMAL
- en: I hope you enjoyed your journey with me throughout this book. As we as authors
    try and better understand what readers would like to see and hear, I welcome your
    constructive comments and feedback, which will only help to make the book and
    source code better. Till the next book, happy coding!
  prefs: []
  type: TYPE_NORMAL
