- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conclusions and Reflections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have been on quite a journey throughout this book. I’ve learned a lot, and
    I hope you have as well. I've had the chance to revisit my love of robotics and
    spend a lot of time examining the state of the art of **artificial intelligence**
    (**AI**) and robot design to try and find a way to explain the concepts to you
    in an easily digestible form.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss finishing our robot. I’ll provide you with
    some advice on careers in AI and robotics. We will also talk a little about the
    future of AI, at least as I see it, and finish with a discussion about risk.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following main topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning when to stop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Careers in robotics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the current state of AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding risk in AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning when to stop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the last nine chapters, we’ve worked on designing a specific robot to perform
    a specific task. We designed *Albert* the robot to pick up toys in an unstructured
    environment, namely a family home. To do this, it needed to be able to recognize
    toys with a camera, pick up toys with a robot arm and hand, navigate inside a
    house, and deposit toys in a toy box. We also added interaction, teaching the
    robot to listen and react to commands. Finally, it received an artificial personality
    and simulated emotions.
  prefs: []
  type: TYPE_NORMAL
- en: So, the next question is, are we finished with designing, building, and testing
    our robot? Sometimes, the most difficult part of designing, building, and testing
    a robot is determining when it is finished. Quite often, I see that some little
    thing might be improved, or some detail added, or some feature enhanced. Oh, the
    robot needs a spotlight. It would be nice if it could remember what toys are in
    the toy box. What if it had two arms? And so on. You can continue in this vein
    forever, tinkering and adding, never quite finishing anything.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this is to set specific goals and measure against them. For
    example, we want the robot to pick up toys. How many toys? All of the ones that
    are on the floor. So, we can run several trials and, if at the end of each test,
    all of the toys have been picked up, that is finished. But what if not all of
    the toys have been picked up? What is good enough? How about we say if we can
    pick up any remaining toys in one pass by holding them all in our hands, then
    that is acceptable. What level would not be acceptable? Well, picking up no toys
    at all is not acceptable. What about half? Would we consider that OK? I’d say
    probably not. You can continue this self-conversation until you say with some
    clarity where the finish line is. Then, once you’ve crossed it, you’re done.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, we have to temper expectations when we’re trying something new or
    innovative. Expect to spend some time ironing the bugs out. I’ve had the experience
    of creating something and having the robot do something new and unexpected that
    ended up being far better than what I started with. For example, I created a robot
    that had a fairly sophisticated *follow me* function. It used body recognition
    to identify humans, and then, when commanded, followed behind one person, even
    through a crowd – so long as it could always keep the person it was following
    in its field of view. The robot was programmed to keep a six-foot distance from
    the person. This means if you walked toward it, instead of away, it backed up.
    You could then steer the robot backward in any direction simply by walking toward
    it – my *follow me* function became a *walk* *ahead* function.
  prefs: []
  type: TYPE_NORMAL
- en: Other times, an innovation just does not work, and at some point, it must be
    abandoned. Generally, you can tell this when you have to keep adding *crutches*
    or workarounds over and over again. The software becomes more complex and fragile
    with each workaround. I had this problem with a sonar-based obstacle avoidance
    system – the sonar sensor I used was just too unreliable and was very sensitive
    to the surface involved – for example, it could not see (get any echoes) from
    polished wooden doors. After a few weeks of testing, we abandoned that sensor
    and went with another light-based sensor called an **Infrared Proximity Detector**
    (**IRPD**) that worked much more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll take a look at some career paths that you could take
    if you are interested in robotics and/or AI.
  prefs: []
  type: TYPE_NORMAL
- en: Careers in robotics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I am often asked what sorts of skills or degrees robot designers need to have,
    or what courses they should take. I meet a lot of young people at robot competitions,
    student events, conferences, and recruiting trips. A lot of the advice I give
    people I have put into this book already – especially now that AI, neural networks,
    **graphics processing units** (**GPUs**), expert systems, chatbots, navigation,
    and image processing are all important. You need to understand that *robotics*
    is an umbrella term that covers a lot of industries and a lot of skill sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Who uses robotics? The range of applications is continuing to grow every day.
    These include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In the medical field, there is robot-assisted surgery, robotic prosthetic limbs,
    exoskeletons helping paraplegics to walk, and implantable devices to help people
    hear and see.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have self-driving cars and self-flying airplanes and helicopters in transportation
    (which is what I do).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have robots delivering packages on the sidewalks in San Francisco, and a
    few companies testing parcel delivery by aerial drone in Africa, Switzerland,
    Texas, and other places.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have self-driving cars being tested in several countries. Safety features
    that debuted in the *DARPA Grand Challenge* robot car race in the United States
    and were developed for autonomous cars – lane keeping, adaptive cruise control,
    driver assistance, and self-parking – are now common features on even base-level
    automobiles. There are over 80 companies currently developing some sort of electric
    **vertical takeoff and landing** (**VTOL**) manned vehicle, every one of which
    uses advanced automation and autonomy as part of its control system. In far western
    Australia, Rio Tinto Mining has developed the *Mine of the Future* in Pilbara,
    where 80 autonomous trucks are remotely operated from Perth, 1,500 kilometers
    away.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The future of robotics is just being written, and you, reading this book, may
    play a part in determining the path it takes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, what skills are required to design and make robots like the ones I just
    described? The truth is that a modern robot company would employ just about every
    skill imaginable. Let’s look more closely at some of these skills:'
  prefs: []
  type: TYPE_NORMAL
- en: Even the simplest robot takes **mechanical designers** to develop the parts,
    gears, and levers to make robots move and help package the electronics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Electrical engineers** work with batteries and motors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Radiofrequency** (**RF**) **engineers** and **technicians** work with radios
    and datalinks that are used to connect mobile robots to their control stations
    (such as an **unmanned aerial** **vehicle** (**UAV**)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cognitive specialists** design AI routines, develop robot emotions, and harness
    machine learning techniques, as well as design user interfaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Writers** and **artists** craft voice routines, write dialogue, design user
    interfaces, write manuals and documentation, and add creative touches to the inside
    and outside of the robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Managers** and **supervisors** track budgets and schedules.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supply specialists** work with suppliers, parts, build-to-print shops, electronics
    warehouses, and salesmen to get the parts to put together the assembly line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Industrial robots are managed by special types of **programmers** who use **programmable
    logic arrays** (**PLAs**) and **ladder logic** to control robot arms that paint
    and assemble components. This type of programming emulates how robots were originally
    designed using relays and switches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bookkeepers** and **accountants** make sure the bills, as well as the employees,
    are paid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Salespeople**, **marketing**, and **customer relations** teams get the product
    sold and keep the customers happy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these skills have to be present in some form in a professional roboticist,
    particularly if you think you will run your own company. I’ve been part of every
    size of robot project, from one person to thousands. Each has its strengths and
    weaknesses, but as a robot designer, you can be sure that you will be the center
    of the storm, making the magic happen, solving problems, and turning ideas into
    physical form. To me, there is no more satisfying moment than seeing my creation
    driving or flying around, doing its job, and knowing that all the hard work, research,
    coding, mechanics, sleepless nights, smashed fingers and toes, and skipped meals
    were worth this result.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk a bit about the hype surrounding robotics and AI that you have
    probably seen in the news.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the current state of AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a lot of hype going on right now in the intersecting worlds of AI and
    robotics. And a lot of it is just exaggeration.
  prefs: []
  type: TYPE_NORMAL
- en: One common myth is that robots are taking jobs away from people. In truth, robots
    and automation free up workers to do more productive tasks. The truth of this
    can be seen in job statistics – unemployment in the US is at a 50-year low ([https://www.wsj.com/articles/january-jobs-report-unemployment-rate-economy-growth-2023-11675374490](https://www.wsj.com/articles/january-jobs-report-unemployment-rate-economy-growth-2023-11675374490)),
    despite massive improvements in factory automation. However, according to *The
    Harvard Business Review*, the improved productivity of robotics creates more jobs
    than it removes ([https://hbr.org/2021/03/why-robots-wont-steal-your-job](https://hbr.org/2021/03/why-robots-wont-steal-your-job)).
    The overall level of employment has *increased*, not gone down because of automation
    and increased productivity.
  prefs: []
  type: TYPE_NORMAL
- en: I do recognize that the modern worker, even someone like myself, who works in
    technology, must be ready and willing – at any age – to retrain themselves and
    to learn and adapt to new ways of working, new economies, and new opportunities.
    I’ve had to completely retrain myself at least six times as new markets were invented
    and new technologies emerged. Sometimes, there is a *second wave* where some technology
    was invented but then disappeared when it was too expensive for the benefits it
    provided, or the proper hardware had not been invented yet. Neural networks fit
    into that category, as does virtual reality, which was a big deal in 1999, and
    has now re-emerged with very small high-resolution screens that were developed
    for cell phones.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead in AI and robotics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’m quite interested in the long-term impact of what has been called the *sharing
    economy*, where companies such as Uber, Lyft, and Airbnb create value by connecting
    suppliers and consumers on a massive scale without owning any of the capital or
    resources to provide any services. All of this is enabled and made possible by
    the ubiquitous internet, which continues to grow and evolve at a rapid pace. The
    availability of the internet has allowed the general public or an individual student
    to have access to supercomputer-level capabilities to run AI programs such as
    ChatGPT online, which are too large to fit in a home computer, smartphone, or
    tablet. I often use the term, “*but that’s a decade in internet years*” while
    referring to some idea that is maybe 24 months old to indicate the rapid turnover
    in internet tech. This trend will continue. It will be interesting to see if anyone
    owns a car in 20 years, or only a subscription to a car service.
  prefs: []
  type: TYPE_NORMAL
- en: Another trend that has become very interesting is the lowering of barriers to
    entry in a lot of businesses. You used to have to have an enormous machine shop
    and giant machines to make precision machine parts – before 3D printers came and
    put that capability on your desktop. **Generative AI** is AI that can synthesize
    writing and drawing and can also directly create music, write programs and software,
    provide advice, and assist users in writing scripts, drawing pictures, and making
    animations powered by only a text prompt. Do you want to make movies? You can
    do so on an iPhone. Do you want to start a recording studio? The parts for professional
    results (with a large amount of effort) are available for less than $200 and you
    can use AI to generate lyrics, chord progressions, arpeggios, or even song ideas.
  prefs: []
  type: TYPE_NORMAL
- en: One item that fits into that category of lowering barriers to entry is drones
    or small UAVs. When I started making UAVs, a decent **global positioning system**
    (**GPS**) and **inertial measurement unit** (**IMU**) – the things that make unstable
    quadcopters possible to control – cost tens of thousands to hundreds of thousands
    of dollars. The real breakthrough in drone technology did not come from aviation,
    but rather from our cell phones. The developments in cell phones enabled companies
    to invest billions of dollars in making the next cell phone, smartphone, hand-held
    computer pacifier, or whatever you would want to call it. The convergence of very
    small radios, very small GPSs, and very, very small accelerometers enabled an
    entire world of unmanned flying objects – quadcopters, gliders, airships, airplanes,
    and hybrid VTOL craft – to emerge. That, along with higher density batteries that
    came from (you guessed it) cell phones and laptops, allowed people to discover
    that if you put enough power on it, you can make almost anything fly, including
    you.
  prefs: []
  type: TYPE_NORMAL
- en: The secret to the flying quadcopter’s amazing success is that the tiny accelerometers
    (which measure changes in movement) and tiny gyroscopes (which measure orientation
    changes) became cheap and readily available. Without these sensors, and the robotics
    algorithms that control them, quadcopters are unstable and impossible to control.
    Another reason for the quadcopter’s success is that it uses only the throttle
    setting – the speed of the motors – to control all its aspects of flight, including
    stability. This compares with the very complicated collective controls and cyclic
    pitch controls that make a helicopter work. You can see the difference between
    a radio-controlled helicopter, which is very expensive and only a few people can
    fly, and a quadcopter, which is quite cheap and can be flown by anyone, with the
    help of a computer and some sensors. You can add a drone autopilot to a collective/cyclic
    radio-controlled helicopter and end up with a very controllable drone helicopter.
    Quadcopters and more complex flying machines use AI for stabilization, adaptive
    flight control, object recognition, and obstacle avoidance.
  prefs: []
  type: TYPE_NORMAL
- en: The other side of these advances in AI and robotics has also resulted in a backlash
    from some people and can be described as robot phobia.
  prefs: []
  type: TYPE_NORMAL
- en: Is AI phobia reasonable?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have probably seen some blazing headlines on the internet from various very
    credible sources saying some incredible things.
  prefs: []
  type: TYPE_NORMAL
- en: About a decade ago, Stephen Hawking, a leading scientist, stated that “*The
    development of full artificial intelligence could spell the end of the human race...
    It will take off on its own and re-design itself at an ever-increasing rate… Humans,
    who are limited by slow biological evolution, can’t compete, and will be superseded*”
    ([https://www.bbc.com/news/technology-30290540](https://www.bbc.com/news/technology-30290540)).
    This quote is frequently used even today by critics of AI.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, Elon Musk suggested that AI could lead to *civilization destruction*,
    although he has invested significantly in the growth of AI ([https://edition.cnn.com/2023/04/17/tech/elon-musk-ai-warning-tucker-carlson/index.html](https://edition.cnn.com/2023/04/17/tech/elon-musk-ai-warning-tucker-carlson/index.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Bill Gates, former chairman of Microsoft, takes a more middle ground, stating
    that AI presents both promise and concerns. In an open letter, he elaborated on
    AI’s potential but also discussed the risks of developing this tech. He wrote,
    “*The world needs to establish the rules of the road so that any downsides of
    artificial intelligence are far outweighed by its* *benefits*” ([https://www.forbes.com/sites/qai/2023/03/24/bill-gatess-open-letter-suggests-ais-potential-is-both-exciting-and-terrifying/](https://www.forbes.com/sites/qai/2023/03/24/bill-gatess-open-letter-suggests-ais-potential-is-both-exciting-and-terrifying/)).
  prefs: []
  type: TYPE_NORMAL
- en: First, let me clarify that I do not think I’m in the same league as these gentlemen.
    But what I am is someone who works every day at the leading edge of AI and unmanned
    vehicles and robotics, and who attends and speaks at a lot of AI conferences.
    The source of this fear of AI can be found in any bathroom, hanging on the wall
    – we are using our mirrors to reflect our motivations and desires onto our creations.
  prefs: []
  type: TYPE_NORMAL
- en: 'I would liken the state of the art of AI today to a teenager who has managed
    to strap a V-8 engine to a skateboard – an enormous amount of power with no way
    to use it. While computers, hardware, and networks have advanced and scaled at
    an amazing rate, we are still building software the same way today as we did 20
    years ago, and many of the advances of AI were invented 40 or 50 years ago. For
    instance, neural networks were invented in the 1940s ([https://link.springer.com/article/10.1007/BF02478259](https://link.springer.com/article/10.1007/BF02478259)).
    Genetic algorithms were suggested by Alan Turing in the 1950s. We have been able
    to make some advancements in applying learning theory now that we have lots of
    data (thank you, internet) and lots of bandwidth, but we have not created any
    startling new concepts or capabilities that show any sort of breakthrough in AI.
    Recent work in generative AI –such as **ChatGPT** and **Dall-E** image generation
    – has created what might be described as a computer version of an imagination
    (creating data stepwise out of noise), which indeed is a step forward, but still,
    a generative AI such as ChatGPT is doing what neural networks did before: predict
    the next word in a sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'What is generative AI? In this case, we can ask the AI itself for a definition.
    The following text was written by an AI program called ChatGPT4 in response to
    the prompt, “*Can you provide me with a definition of* *generative AI?*”:'
  prefs: []
  type: TYPE_NORMAL
- en: “Generative AI refers to a subset of artificial intelligence technologies that
    can generate new content, such as text, images, music, or code, based on the patterns
    and information it has learned from its training data. This type of AI uses machine
    learning algorithms, particularly generative models such as generative adversarial
    networks (GANs) or transformers, to understand and replicate the complexities
    of human-generated content.
  prefs: []
  type: TYPE_NORMAL
- en: “The “generative” aspect implies that the AI isn’t just analyzing or processing
    existing information but actually creating something new and previously unseen.
    For instance, in the case of text, generative AI can write essays, poems, or code;
    for images, it can create artworks or realistic photos of things that don’t exist;
    in music, it can compose new pieces in various styles.”
  prefs: []
  type: TYPE_NORMAL
- en: But is this what is referred to as **general AI** (software with human intelligence)?
    I don’t think so. While the hardware and software are advancing, I simply don’t
    see any path forward that leads to the type of problems that the esteemed Mr Gates,
    Mr Musk, or Dr Hawking suggested. Why not? We’ll focus on this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the needs of humans and AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The reason why I don’t see significant problems is because robots don’t have
    needs. Humans have needs and ambitions to exist. We are encased in a frail flesh
    cell, what William Burroughs called the *Soft Machine*. We must provide air, food,
    water, shelter, and clothing to protect our fragile shells and interact with other
    soft machines (people) to reproduce and make more of ourselves. You can argue,
    as Richard Dawkins did in his book *The Selfish Gene*, that all of this is simply
    an evolved way for our DNA to perpetuate itself, and that we are simply the product
    of our biological programming. It is impossible to separate a human from their
    needs – if we don’t, we die in a matter of minutes. It is our needs that drive
    us forward, to come out of the trees, to learn to farm, to build cities, and to
    make civilizations.
  prefs: []
  type: TYPE_NORMAL
- en: Robots, on the other hand, do not have needs as a condition of their existence.
    They are just sets of instructions we have set down in electronics – golems with
    words in their heads that make them move (as described in the book *Feet of Clay*,
    by Terry Pratchett). If we don’t provide food to them – nothing happens. If we
    don’t use them – nothing happens. If we forget them for a week and check on them
    later, they are still the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s discuss what humans’ needs are. Maslow came up with the **hierarchy
    of needs** back in 1943, and he has been quoted ever since. Maslow says that we
    not just have needs, but they form a hierarchy – the more important needs are
    at the bottom while the more abstract needs are at the top. We only worry about
    the need at any given level when all of the needs below it are satisfied, as shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Hierarchy of needs for humans](img/B19846_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Hierarchy of needs for humans
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at this pyramid of needs in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom are the *physical needs* – air, food, water, and clothing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next level is *security* – we need to feel secure from predators or from
    other humans wanting to harm us.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Above the security needs are *social needs* – to be in a group or part of society.
    Humans want to belong to a family, a community, a nation. This drive is very strong,
    as our invention of war to protect our society attests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we have *ego needs* – the need to be recognized, to be special, to stand
    out from the crowd we fought so hard to be part of. Remember, we only get to express
    this once all the other needs are taken care of, so you only worry about recognition
    once you are part of a group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our final need is called *self-actualization*, as described by Maslow – we would
    call it self-improvement, or the drive to improve one’s self. This is where we
    get athletes, artists, musicians, and people who write books.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It has been a running joke between my wife and me that every textbook we ever
    read in college contained some reference to Maslow and his hierarchy of needs.
    This was quite unusual since I studied math and engineering, and my wife’s degree
    is in human resources. I appreciate the irony that this book now adds to that
    list of books that reference Maslow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at machine intelligence and imagine for a moment what a set
    of robot needs might look like. I found this an interesting thought experiment
    – we make a baby robot that is fully capable of learning anything a baby human
    (or baby mouse, or baby cricket) can. What needs would it have? Let’s look at
    a modified version of Maslow’s hierarchy of needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Hierarchy of needs for robots](img/B19846_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Hierarchy of needs for robots
  prefs: []
  type: TYPE_NORMAL
- en: 'We can break this down as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to a robot’s needs, hunger is built into humans’ biology. However,
    in the case of an AI system, we, the creators, would need to build it in. That
    would equate, as we did in our artificial personality, to *electrical power* or
    *battery life*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next level of needs would be the *goals* and *tasks* for which the AI was
    created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next level up could potentially be *curiosity* and the need to explore –
    our AI system would have a drive to acquire more data, or to get access to more
    resources. Once a robot has data, this gives it the basis to get more data, and
    so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next level of needs we would endow to our AI would be the need for *friendship*
    or *communication*, either with other robots or with people.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we could give our robot the need for *learning*, or to acquire and
    learn new skills and techniques – to grow as a robot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may have noticed that we did not cover these subjects in this book, nor
    any other. We did not talk about giving the robot needs, only how to simulate
    emotions and some rules for conversation that make no sense to the robot at all.
    It gets no joy from telling a terrible joke to a 7-year-old because it does not
    know what any of those concepts are. It just sends electrons down one path or
    another because we tell it to. The only intelligence here is the reflection and
    imagination of us, the robot designers. Everything else is an illusion – a magician’s
    trick.
  prefs: []
  type: TYPE_NORMAL
- en: I get this sort of question quite a lot and felt that I could give you some
    of my answers if that helps arm you against the critics of AI. The bottom line
    is that I simply do not worry about AI taking over the world. I don’t mean to
    say that there will never be a general AI, just I don’t see one coming about in
    the foreseeable future.
  prefs: []
  type: TYPE_NORMAL
- en: With this context under our belt, let’s discuss how to understand and manage
    risk in AI.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding risk in AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One subject I talk about frequently at conferences and in print is the risk
    of AI in terms of *trust* and *control*. I’m not talking about AI running amok
    here, but rather how to make AI dependable. It is quite interesting that the sort
    of AI we have been considering – specifically, ANNs – does something very few
    other computer software do. Given the same inputs and conditions, the output of
    an AI system is not always the same. Given the same inputs, the AI system will
    sometimes come up with a different answer. The formal name for this behavior is
    **non-determinism**.
  prefs: []
  type: TYPE_NORMAL
- en: There is a second corollary to this. Given the same inputs, the AI process will
    sometimes take a different amount of time to complete its task. This is simply
    not normal behavior for a computer.
  prefs: []
  type: TYPE_NORMAL
- en: Admittedly, we are not using AI to get answers to math questions such as *2+2*,
    but rather how to do things such as diagnose a cancer tumor or recognize a pedestrian
    in a crosswalk for a self-driving car. How do we deal with a computer output that
    may be wrong? You can verify this for yourself – look at the examples we covered
    when we performed training on neural networks. Did we ever achieve 100% success
    from a training run, where we got all of the answers right? No, not once. This
    is because ANNs are *universal approximation functions* that map inputs – which
    can be quite complex – to outputs. They do this by dealing with probabilities
    and averages, which were developed over time. You can think of an artificial neuron
    as a probability engine that says, *45 out of the last 50 times I got this set
    of inputs, the output was supposed to be true. The odds are it will be true this
    time*. And it sets itself to true. We may have millions of little artificial neurons
    in our network, each of them making the same sort of calculation. The net result
    is making a very educated guess about the answer.
  prefs: []
  type: TYPE_NORMAL
- en: For most applications of our neural networks, this is acceptable behavior. We
    are classifying pictures, and it is acceptable if a few are wrong. We do a Google
    search for platypus, and we get one picture out of 100 Platypus brand tennis shoes.
    That is OK for a Google search, but what if we were doing something more serious,
    such as recognizing pedestrians in a self-driving car? Is it OK if we misidentify
    one pedestrian out of 100 and don’t avoid them? Of course not. That’s why, right
    now, we don’t allow AI systems in such critical functions. But people want to
    use AI in this way – in fact, quite a lot. It would be great to have an AI system
    that recognizes geese in flight and tells your airliner how to avoid them. It
    would be great to have an AI system recognize that a patient was misdiagnosed
    in the hospital and needed immediate attention. But we can’t do that until we
    come up with processes for dealing with the non-deterministic and thus non-reliable
    nature of AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, we deal with non-deterministic elements in automobiles all of the
    time. They are called drivers. It is widely believed that the vast majority of
    car crashes are caused by the human element, which is why we need self-driving
    cars with a better percentage. How do we deal with human drivers? Let’s look at
    the necessary criteria for a driver:'
  prefs: []
  type: TYPE_NORMAL
- en: We require them to be a certain age, which means they have gained experience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have to pass a test, demonstrating competency in accomplishing tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have to demonstrate compliance with rules and regulations by passing a
    knowledge test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They have to get periodically re-certified by renewing their license
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also require seat belts and airbags to partially mitigate the risk of the
    human driver making mistakes by reducing some of the resulting injuries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can apply these types of criteria to AI. We can require a certain amount
    of training cases. We can test and demonstrate a level of competency. We can predict
    the level of errors or mistakes in advance and put measures in place to mitigate
    that risk. Perhaps we can have two AI systems – one that detects obstacles and
    another that has been trained to recognize that the first AI has made a mistake.
    If we have a 90% chance of the first AI being right, and another 90% chance of
    the second AI being right, then we have a *90% + (90% of 10%) = 99%* chance of
    avoidance.
  prefs: []
  type: TYPE_NORMAL
- en: I think the key to using AI in safety-critical applications is being able to
    predict risk in advance, and designing in advance to mitigate either the cause
    of the risk or the effect.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this final chapter of this book, we summarized our journey through robotics
    and AI. We talked about robotics as a career and discussed AI robotics as a profession.
    I brought up some issues regarding the future of AI, both real and imaginary.
    Drones and self-driving cars are real; robots taking jobs from humans or taking
    over the world is imaginary, at least in my opinion. I talked about robots and
    AI not having needs, and thus lacking the motivation, pressure, or even capability
    to evolve. Finally, we talked about risk in AI and how to recognize it. I hope
    that this information gives you some guidance in your interest in robotics and
    AI and provides some *insider information* from a practitioner in this area.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re almost at the end of this book, I want to thank you for coming
    on this journey with me. I hope you have learned something along the way, even
    if it is just to know more questions to ask. I encourage you to dive in and build
    your own robot, learn about AI, and become part of the community of people who
    contribute to robotics as a hobby or a profession.
  prefs: []
  type: TYPE_NORMAL
- en: I have to acknowledge a debt of gratitude to all of the robotics and AI open
    source communities for making all of this material, knowledge, and expertise available,
    and continuing to make AI the poster child for why open source, as a model for
    the advancement of human knowledge, works and works well. ROS, which is entirely
    run by volunteers, is a case in point as it makes building robots so much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that we started the chapter on a light note and ended up talking about
    robot phobia and philosophical questions about existence, do you feel that AI
    is a threat? Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List five professions that would be necessary to turn our Albert robot into
    a product company.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we imagine a company that was going to put Albert the robot into production,
    would it need a psychologist? For the robot, or for the humans?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What components found in cell phones or smartphones are also found in quadcopters?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are AI systems, specifically ANNs, naturally non-deterministic in terms
    of results and time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What might be a practical application of an AI system that predictably makes
    mistakes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If an AI system was picking stocks for you and predicted a winning stock 43%
    of the time, and then you had a second AI that was 80% accurate at determining
    when the first AI had *not* picked a good stock, what percent of the time would
    the AI combination pick a profitable stock?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The Organization of Behavior*, by Donald Hebb, Wiley.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Computing Machinery and Intelligence*, by Alan M. Turing in the journal *Mind*.
    Vol. LIX (238).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Feet of Clay*, by Terry Pratchett, published by HarperCollins, London 2009\.
    This book discusses the fictional concept of golems, which are clay creatures
    that are *programmed* by a set of instructions written on paper and put into their
    heads, an interesting analog to robots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Theory of Human Motivation*, by A.H. Maslow in the journal *Psychological
    Review*, vol. 50 (4).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: US Dept. of Transportation. *National Motor Vehicle Crash Causation* *Survey*.
    [https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811059](https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/811059).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attacking Faulty Reasoning: A Practical Guide to Fallacy- Free Arguments*,
    by T. Edward Damer, Cengage Learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Modern Generative AI with ChatGPT and OpenAI Models*, by Valentina Alito,
    Packt Publishing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 1](B19846_01.xhtml#_idTextAnchor015)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What does the acronym PID stand for? Is this considered an AI software method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**PID** stands for **Proportional, Integral, Derivative**, and is a type of
    closed-loop controller that does not require a model (simulation) to operate.
    PID is not an AI method because there is no learning or adaptation involved in
    the decision-making. PIDs are very useful control techniques and are widely used
    to control motors and thermostats.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the Turing Test? Do you feel that this is a valid method of assessing
    an artificial intelligence system?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Turing Test**, originally called *The Imitation Game* by Alan Turing,
    is an imaginary test, or thought experiment, in which a person communicates with
    someone or something via a **teletype** (or a text message, for you younger people).
    An AI would pass the Turing Test if the person was unable to tell whether the
    entity they were communicating with was a human or a robot. The Turing Test has
    been passed by modern AI-based chatbots and generative AI engines such as ChatGPT,
    and new intelligence tests are being created ([https://www.nature.com/articles/d41586-023-02361-7](https://www.nature.com/articles/d41586-023-02361-7)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why do you think robots have a problem in general with negative obstacles such
    as stairs and potholes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is difficult to see negative obstacles (holes, drop-offs, stairs going down,
    etc.) using the robot’s sensors, which have an easier time with positive (going
    up) obstacles. Usually, cameras and LiDAR can only see part of a negative obstacle
    due to the bottom being obscured (not visible). It is sometimes easier to reason
    about negative obstacles by seeing their shadow – the area that the sensor cannot
    see. The following diagram shows how a robot perceives a hole:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.1 – How a robot perceives a hole or negative obstacle](img/B19846_11_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – How a robot perceives a hole or negative obstacle
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how a robot perceives stairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – How a robot sees stairs](img/B19846_11_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – How a robot sees stairs
  prefs: []
  type: TYPE_NORMAL
- en: In the OODA loop, what does the orient step do?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the *orient* step, all of the data is put into the same reference frame,
    which is usually the robot’s central point of view. This allows the robot to determine
    which data is relevant to decision-making. It is the most important step in decision-making.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From the discussion of Python advantages, compute the following: You have a
    program that needs 50 changes tested. Assuming each change requires one run and
    a recompile step to test. A Make compile and build in C takes 450 seconds and
    a Python `run` command takes three seconds. How much time do you sit idle waiting
    for the C compiler?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Python as an interpreted language can save a lot of time on very complex
    builds, where a C/C++ compiler and link can take 20 minutes or more. The C program
    test cycle in question would take 6.25 hours to complete, while the Python test
    program would take 2.5 minutes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What does RTOS stand for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**RTOS** stands for **real-time operating system**. This is an operating system
    that enforces time limits and processing partitions in the OS itself. An RTOS
    is a deterministic operating system that always executes a task in the exact same
    amount of time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Your robot has the following scheduled tasks: telemetry at 10 Hz, GPS at 5
    Hz, inertial measurements at 50 Hz, and motor control at 20 Hz. At what frequency
    would you schedule the base task, and what intervals would you use for the slower
    tasks (i.e., 10 Hz base, motors every three frames, telemetry every two frames,
    etc.)?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You need a number that all of the rates (10, 5, 50, 20) divide into evenly.
    The smallest number that fits is 100 Hz. I would also accept 50 Hz if the student
    assumed that the 20 Hz would update two times in one frame and three times in
    the next frame, which is cheating a little but a common adaptation for a real-time
    system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given that a frame rate scheduler has the fastest task at 20 frames per second,
    how would you schedule a task that needs to run at 7 frames per second? How about
    one that runs at 3.5 frames per second?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As given in the previous questions, there does not have to be the same number
    of samples in each frame in order to come out with a constant frame rate as long
    as there is a multiple of the base frame rate that every sample divides into.
    In this case, *20 x 7 = 140*, so the 7 Hz can run at a 20 Hz base rate, and it
    will repeat patterns every 140 frames, or 7 seconds. Half of 7 is 3.5 and can
    run at the same base rate with a pattern that repeats every 70 frames, or 3.5
    seconds.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each update would be 5.7 frames apart, which gets rounded up to 6.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is a blocking call function? Why is it bad to use blocking calls in a real-time
    system such as a robot?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A **blocking call** suspends the execution of your program until an interruption
    or event occurs, such as receiving a datagram or UDP packet. These are bad because
    you lose control of your program timing and cannot maintain a soft real-time execution.
    Use **polling calls** instead for serial ports and network interfaces. A polling
    call looks for data on the interface and then continues when no data is available.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 2](B19846_02.xhtml#_idTextAnchor032)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name three types of robot sensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A sensor is anything that conveys data from the outside world to the robot.
    Sensors mentioned in the text include the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Sonar sensors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cameras
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Microphone
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Buttons
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Analog-to-digital voltage sensors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature via thermistors
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What does the acronym PWM stand for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**PWM** stands for **pulse width modulation**, a type of digital-to-analog
    control scheme where pulses are sent out that get longer based on the amount of
    control desired. In other words, the pulse duty cycle (amount of time on/off)
    gets converted into a voltage to drive a motor. This is commonly used to control
    DC motors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is analog-to-digital conversion? What goes in and what comes out?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the name says, **analog-to-digital** (**A2D**) conversion takes in an analog
    value, typically a voltage, and converts it into a digital value or number that
    the digital part of the computer can understand. A typical application is measuring
    battery voltage to determine the state of charge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Who invented the subsumption architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As stated in *Cambrian Intelligence: The Early History of the New AI* by Rodney
    Brooks, the subsumption architecture was originally described by Dr. Rodney Brooks,
    a professor at MIT who would later help found iRobot Corporation and invent the
    Baxter Robot. Rodney was trying to develop analogs of insect brains to understand
    how to program intelligent robots.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compare my diagram of the three-layer subsumption architecture to the three
    laws of robotics postulated by Isaac Asimov. Is there a correlation? Why or why
    not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No, not really. The three laws of robotics from Isaac Asimov are fictional,
    while the **Subsumption Architecture** (**SA**) is a real architecture that is
    used to make robots in the real world.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Asimov’s three laws:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Robots will not harm a human being, or through inaction, allow a human to come
    to harm
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Robots will obey orders from humans, except when that violates the first law
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Robots will protect themselves from harm, except when that violates the first
    two laws
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s look at the three layers in the SA:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The bottom layer of the SA is the part that looks inside the robot and takes
    care of internal systems – I like to compare it to the autonomic nervous system.
    That protects the robot.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The second layer is the short-term manager – it tells the robot where to go,
    which includes obeying orders from users.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The top layer contains the strategic thinking and planning processes. The correlation
    is weak, to be truthful.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And, readers, remember the final, or zeroth, law: A robot shall not harm humanity
    or allow humanity to come to harm. That was a later addition.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Do you think I should have given our robot project *Albert* a name? Do you name
    your robots individually or by model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Roombas, being robotic vacuum cleaners, exhibit characteristics that people
    often associate with living entities, such as movement and the ability to navigate
    spaces autonomously. This behavior can trigger a human tendency to anthropomorphize
    or attribute human-like qualities to non-human entities. Naming is a natural extension
    of this anthropomorphism.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the importance of the `ROS_ROOT` environment variable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The most important variables are `ROS_ROOT` and `ROS_PACKAGE_PATH`. These variables
    are used to define the filesystem paths for ROS packages and resources. They are
    essential for the ROS system to locate and use various packages and resources
    correctly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 3](B19846_03.xhtml#_idTextAnchor043)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Describe some of the differences between a storyboard for a movie or cartoon
    and a storyboard for a software program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A storyboard for a movie is used not only for advancing the plot, but also for
    showing what point of view will be used – in other words, it is used to plan camera
    angles, directions, and movements. In that the purpose of both storyboards is
    to “tell the story” of what happens, they are the same. The point of view of a
    computer software storyboard should be the user.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What are the five *W* questions? Can you think of any more questions that would
    be relevant in examining a use case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Who*, *What*, *When*, *Where*, *Why* (with *Why* being the most important).
    More relevant questions might be: How well? How often? How many or how much?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Complete this sentence: A use case shows what the robot does but not ________.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: “How it does it.” Use cases are from the user’s perspective and never include
    implementation details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Take the storyboard in *step 9*, where the robot is driving to the toybox, and
    break it down into more sequenced steps in your own storyboard. Think about everything
    that must happen between *frames 9* and *10*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The robot has to do the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Determine a route to the toybox.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Plan a path.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Avoid obstacles along the way.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Align itself with the front of the toybox.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Drive up to the toybox.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Move the robot arm to clear the top.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Complete the reply form of the “knock-knock” joke, where the robot answers the
    user telling the joke. What do you think is the last step?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is to compliment the teller of the joke – the robot should say “That is very
    funny” or “I am sorry, I am unable to groan”. Yes, that is my opinion and not
    an official joke writer’s idea. What do you think?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Look at the teleoperate operations. Would you add any more or does this look
    like a good list?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The robot needs to send video back to the operator so that the operator can
    see where they are going.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Write a specification for a sensor that uses distance measurement to prevent
    the robot from driving downstairs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The robot shall have a sensor capable of detecting negative obstacles in the
    floor (i.e., stairs going downward, balconies) at a distance of at least six inches
    from the robot along the robot’s driving direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At which distance can a camera with 320x200 pixels and a 30-degree **field of
    view** (**FOV**) vertically and horizontally see a 6” stuffed animal, assuming
    we need 35 pixels for recognition?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To solve this problem, we first need to determine how many degrees per pixel
    we have and then use that to calculate the angular dimension of the target, which
    is 35 pixels tall:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*30 degrees / 320 pixels wide =* *0.0937 deg/pixel*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To find the number of degrees per pixel, we can perform the following calculation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*35 pixels* deg/pixel =* *3.28 degrees*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This gives us an isosceles triangle, but we need a right triangle to do the
    math. Divide the base into two to make a right triangle; thus, the base of this
    triangle is now *3 inches*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We also divide the angle in half:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*3.28/2 =* *1.64 degrees*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, to calculate the perpendicular height, we divide the length of the base
    by the value of *tan*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*3 / tan(1.64) =* *104.78 inches*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This translates to 8.73 feet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This can be illustrated by the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 11.3 – Solving for pixels needed for recognition](img/B19846_11_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Solving for pixels needed for recognition
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the required distance is 8.73 feet.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 4](B19846_04.xhtml#_idTextAnchor126)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We went through a lot in this chapter. You can use the framework provided to
    investigate the properties of neural networks. Make adjustments to the learning
    rate, batch size, number of epochs, and loss functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is an exercise for the student. You should see different curves develop
    as these parameters are changed. Some will not produce an answer at all (which
    looks like random results – the curve stays at the same level as no learning is
    taking place). Some will learn faster or slower.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Draw a diagram of an artificial neuron and label the parts. Look up a natural,
    human biological neuron and compare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See *Figure 4**.3* in the chapter. The artificial neuron has a number of inputs,
    a set of weights, one for each input, a bias, an activation, and a set of outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which features are the same in a real neuron and an artificial neuron?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Both have multiple inputs and multiple outputs and accept inputs, perform some
    processing, and then make an output. Both use some sort of activation function
    (the biological equivalent is the synapse) to determine when to *fire* or produce
    an output. Both are part of networks: it takes a lot of neurons to compose a neural
    network, and likewise an animal brain.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which features of a real neuron and an artificial neuron are different?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The natural **neuron** is an analog device that can handle many levels or degrees
    of input, with no simple on/off binary representations like the computer neuron.
    Neurons use chemical paths that make pathways and connections easier the more
    they are used, which is the learning function of a neuron. This is simulated by
    the weights in an artificial neuron. The natural neuron has an axon, or connecting
    body, that extends out to the outputs that can be at a quite distance from the
    nerve inputs. Neurons are randomly connected to other neurons, while artificial
    neurons are connected in regular patterns. Some neural networks use **dropout
    layers** that randomly disconnect neurons, providing some randomness to the output
    that helps the network estimate non-linear solutions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What relationship does the first layer of a neural network have to the input?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first layer contains the number of inputs to the network. For instance,
    if you have five inputs, then the first layer must contain five neurons.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What relationship does the last layer of a neural network have to the output?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last layer of an ANN is the output layer and has to have the same number
    of neurons as there are potential outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Look up three kinds of loss functions and describe how they work. Include mean
    square loss and the two kinds of cross-entropy loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss functions** in ANNs are the error functions that compare the expected
    output of the neuron with the actual output. Let’s look at them in detail:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Mean square loss** (**MSL**): This is the most commonly used loss function.
    It is given by the sum of the squares of the distances between the output and
    the expected output. MSL amplifies the error the farther away from the desired
    solution it is.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross entropy** (**XE**): This is also called log loss and is used mostly
    for the classification of CNNs. As the predicted value approaches 1 (no error),
    XE slowly decreases. As the values diverge, XE increases rapidly. Two types of
    cross entropy are as follows:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary (on/off, used for yes/no questions)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid cross-entropy, which can handle multiple classes
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What would you change if your network trained to 40% and got “stuck” or was
    unable to learn anything further?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are probably *overfitting* and have too small a sample size or your network
    is not wide or deep enough.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 5](B19846_05.xhtml#_idTextAnchor159)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Q-learning, what does the Q stand for (you will have to research this on
    the internet)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The origin of **Q-learning** is the doctoral thesis of Christopher John Cornish
    Hellaby Watkins from King’s College, London, May, 1989 ([https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards](https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards)).
    Evidently, the Q just stands for *Quantity*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What could we do to limit the number of states that the Q-learning algorithm
    must search through?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Only pick the Q-states that are relevant and are follow-ons to the current state.
    If one of the states is impossible to reach from the current position, or state,
    then don’t consider it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What effect does changing the learning rate have on the learning process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the learning rate is too small, the training can take a very long time. If
    the learning rate is too large, the system does not learn a path but instead overshoots
    and may miss the minimum or optimum solution. If the learning rate is too big,
    the solution may not converge or may suddenly drop off.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What function or parameter serves to penalize longer paths in the Q-learning
    equation? What effect does increasing or decreasing this function have?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The discount factor works by decreasing the reward as the path length gets longer.
    It is usually a value just short of 1.0, for example, 0.93\. Making the discount
    factor higher may cause the system to reject valid longer paths and not find a
    solution. If the discount is too small, then the paths may be very long.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the genetic algorithm, how would you go about penalizing longer paths so
    that shorter paths (fewer number of steps) would be preferred?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You would adjust the fitness function to consider path length as a factor in
    the fitness calculation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What effect does changing the learning rate in the genetic algorithm change?
    What are the upper and lower bounds of the learning rate?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generally, increasing the learning rate shortens the learning time in generations,
    up to a limit where the path jumps out of the valid range. For our example program,
    the lowest learning rate that returns a valid solution is five, and the highest
    value is 15.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the genetic algorithm, what effect does lowering the population cause?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It causes the simulation to run much faster but take many more generations to
    find a solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 6](B19846_06.xhtml#_idTextAnchor205)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Do some internet research on why an open source voice assistant was named Mycroft.
    How many stories did you find and which one did you like?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I found at least three. My favorite is that Mycroft is Sherlock Holmes’ older,
    and some say smarter, brother. Sherlock Holmes is played on TV in the UK by Benedict
    Cumberbatch, who also played Alan Turing in the movie *The Imitation Game*, the
    original name of the Turing Test, a test of AI conversation, which is what Mycroft
    does.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the discussion of intent, how would you design a neural network to predict
    command intent from natural language sentences?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One approach would be to gather a selection of commands, label the intent of
    the command, use the commands as input in a neural network, and use intent as
    the output label for the training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rewrite the “receive knock-knock jokes” program to remember the jokes told to
    the robot by adding them to the joke database used by the “tell knock-knock jokes”
    program. Is this machine learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is fairly simple to add a program to just write to the knock-knock joke program
    database. You’ll find a version of this in the GitHub Repository. Is this machine
    learning? I would say definitely! The machine has a capability that it did not
    have before. It did not have to be reprogrammed to have the new capability, so
    it learned.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Modify `KnockKnock` (the program that tells knock-knock jokes) to play sounds
    from a `WAV` file, such as a music clip, as well as do text-to-speech.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add these lines to the `KnockKnock` program:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can use this to play the audio. You can also add a tag to the joke file
    that indicates a `WAV` file `<groan.wav>`. Then, if you see this tag, call the
    `play_wav_cmdline` function above.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The sentence structure used in this chapter is all based on English grammar.
    Other languages, such as French and Japanese, have different structures. How does
    that change the parsing of sentences? Would the program we wrote be able to understand
    Yoda?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In other languages, the object or the subject appears in a different order,
    just as in Yoda’s speech patterns. “Backwards, I talk,” Yoda would say. This does
    require us to change or add new sentence patterns to our `.``voc` files.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can follow Mycroft’s instructions for changing the engine to understand
    French at [https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/languages](https://mycroft-ai.gitbook.io/docs/using-mycroft-ai/customizations/languages).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Do you think that Mycroft’s Intent Engine is actually understanding intent or
    just pulling out keywords?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I do not place Mycroft in the category of construction AI chatbots, but it is
    rather a referential type that looks up answers in a database, which makes it
    more of an expert system than an AI program. It does use AI neural networks in
    the speech-to-text section.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Describe the voice commands necessary to instruct the robot to drive to an object
    and pick it up without the robot being able to recognize the object. How many
    commands do you need?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need two commands:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Can you see* *any objects?*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Drive to the* *closest object*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From *Question 7*, work to minimize the number of commands. How many can you
    eliminate or combine?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are the voice commands to tell the robot to drive to the nearest
    object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Hey, Albert*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Drive to the* *nearest object*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: From *Question 7*, how many unique keywords are involved? How many non-unique
    keywords?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The four keywords are **see**, **objects**, **drive**, and **closest**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: All of the words are unique except **objects**, so there are three unique words.
    They have not been otherwise defined in the Mycroft word database.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 7](B19846_07.xhtml#_idTextAnchor221)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regarding SLAM, what sensor is most used to create the data that SLAM needs
    to make a map?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Light detection and ranging** (**LiDAR**) sensors are the most common SLAM
    sensors used by a wide margin. The 3D data that LiDAR provides is perfect for
    SLAM’s mapping function.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why does SLAM work better with wheel odometer data available?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The wheel odometers reduce the search space that the SLAM algorithm needs to
    look for the possible locations of the robot after moving. Thus, it increases
    information and reduces uncertainty in the map. How does it do this? By giving
    extra measurements about where the robot is located (how far it moved), we can
    then reduce our search to match where we are against our sensor readings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the Floor Finder algorithm, what does the Gaussian blur function do to improve
    the results?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Gaussian blur function** reduces noise and gets rid of stray single pixels
    in the image, making for a smoother result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The final step in Floor Finder is to trace upwards from the robot’s position
    to the first red pixel. In what other way can this step be accomplished (referring
    to *Figure 7**.3*)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instead of using radial red lines, the program can just draw upwards from the
    bottom of the screen in a series of vertical lines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why did we cut the image in half horizontally before doing our neural network
    processing?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We just want to use the upper half of the room to train the network because
    the lower half has the toys on it and is subject to change. The upper half of
    the room does not change with the addition of toys.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What advantages does using the neural network approach provide that a technique
    such as SLAM does not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We don’t have to have a map to successfully navigate the room. We are providing
    labeling of our training set by just driving the robot around and taking pictures
    at regular intervals. This approach is also far more resilient to changes in the
    room, such as the furniture being in slightly different positions. Please also
    see [https://hackaday.com/2021/10/25/fast-indoor-robot-watches-ceiling-lights-instead-of-the-road/](https://hackaday.com/2021/10/25/fast-indoor-robot-watches-ceiling-lights-instead-of-the-road/)
    for someone else’s implementation of this idea, used for indoor car racing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If we used just a random driving function (where you make random turns at random
    times) instead of the neural network, what new program or function would we have
    to add to the robot to achieve the same results?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would need to have a navigation function that determined where in the room
    we were at – this would probably mean a SLAM algorithm. We would also need something
    to detect the stairs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How did we end up avoiding the stairs in the approach presented in the chapter?
    Do you feel that this is adequate? Would you suggest any other means to accomplish
    this task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We trained the robot to navigate by looking at the upper part of the room. We
    only drove the robot in safe areas and used that information to allow the robot
    to predict its next driving command based on where it was in the room. Since we
    did not drive the robot down the stairs in this process, the robot will never
    get a command to drive toward the stairs. If there is a toy near the stairs, the
    robot will still go pick it up but will drive away from the stairs afterward when
    it goes back to navigation mode. We have to be careful to get a good training
    result before letting the robot loose, however. I used a baby gate to block the
    stairs for early testing. As an additional safety measure, we can add a lookdown
    sensor to detect stairs. I would use an **infrared proximity detector** (**IRPD**)
    for this purpose.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 8](B19846_08.xhtml#_idTextAnchor235)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the three ways to traverse a decision tree?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From beginning to end (start to goal); from goal to start; and from both ends
    at once to meet in the middle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the fishbone diagram example, how do you go about pruning the branches of
    the decision tree?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By eliminating the effect of the item on a branch. For example, using our “robot
    does not move” fault, if the branch says “Arduino-no power” and you check to see
    if the Arduino has power and it does, you can prune that branch. If the branch
    is “motor stuck”, the effect of having a motor stuck is that the robot will drive
    in circles. As the robot is not driving in circles – it is not driving at all
    – you can prune that branch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is the role of the Gini coefficient in creating a classification?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It determines the amount of *impurity* in the sample or pool. When the Gini
    coefficient = 0, all of the members of the class have the same attributes, and
    no further subdivision is possible. This minimizes misclassification. The Gini
    coefficient is given by one minus the sum of the square of the probability of
    an item being in that class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the toy classifier example using the Gini coefficient, which attributes of
    the toy were not used by the decision tree? Why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Color, Noise, Soft, and Material were not useful for dividing the categories
    by labels as the labels and the items did not correlate. It does make sense that
    color is not useful for dividing toys by type.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Which color for the toys was used as a criterion by one of the classification
    techniques we tried?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The color white was used by the decision tree that used the Gini index and one
    hot encoding to separate the stuffed animals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Give an example of label encoding and one hot encoding for menu items at a restaurant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s have three types of menu items: appetizer, entrée, and dessert. Label
    encoding would substitute 0 for appetizer, 1 for entrée, and 2 for dessert. One
    hot encoding would use 1 0 0 for appetizer, 0 1 0 for entrée, and 0 0 1 for dessert.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the A* algorithm, discuss the different ways that `G()` and `H()` are computed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `G()` function is the distance along the path from the current position
    to the start. `H()` is the distance from the current position directly to the
    goal (the Euclidean distance). Note that `G()` follows the path and `H()` is the
    straight-line distance to the goal since we have not computed a path to the goal
    yet.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the A* algorithm, why is `H()` considered heuristic and `G()` not? In the
    D* algorithm, heuristics are not used. Why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`H()` – the direct line distance to the goal – is an estimate and ignores any
    obstacles, it can’t be used directly but is just a way to compare one position
    to another. A major difference between D* and A* is that D* starts at the goal
    and works backward toward the start. This allows D* to know the exact cost to
    the target – it is using the actual path distance to the goal from the current
    position and not a heuristic approach or an estimate of the distance to go, as
    A* did.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the D* algorithm, why is there a `RAISED` and a `LOWERED` tag and not just
    a `CHANGED` flag?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `RAISED` squares or points are eliminated from consideration. The `LOWERED`
    squares may be added back into the queue for consideration to be a path. Keep
    in mind that lowering scores due to new sensor readings ripples through the path
    planner.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Chapter 9](B19846_09.xhtml#_idTextAnchor294)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is your favorite movie robot? How would you describe its personality?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is, of course, a subjective question. I’m a big R2D2 fan. R2 is feisty,
    determined, and stubborn as well as being a faithful companion and helper. R2
    will get you out of a jam, fix your starfighter, provide cover from hostile fire,
    and hack Imperial computers. He is a Swiss army knife on wheels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What techniques did the movie-makers use to express R2D2’s personality (body
    language, sounds, etc.)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: R2D2 owes his personality to a combination of his emotional beeps and squawks
    (provided by Ben Burtt) and his body movements provided by having a person inside
    his chassis (Kenny Baker). They were stuck with the not-very versatile chassis
    designed for the first *Star Wars* movie, which only has a head that moves. Most
    of R2’s persona comes through in his sounds, including his famous scream.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What are the two types of chatbots? List some of the strengths and weaknesses
    of each.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The two types of chatbots are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Retrieval-based chatbots**: Retrieval-based chatbots look up responses in
    lists of scripts and choose from a number of phrases that are written in advance
    by humans. The strengths of these chatbots are that they are easy to program,
    allow more control over the outputs, and are much smaller and faster programs.
    The weaknesses are that they have limited responses and the use of keywords gives
    them a small vocabulary.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative chatbots**: Generative chatbots use the rules of grammar and models
    of sentences to create new sentences with proper meaning, are more flexible, and
    can handle a wider range of topics, but they are much harder to program and are
    complex and slow (comparatively speaking). Generative chatbots have now taken
    over, given the success of ChatGPT and other generative AI models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In *Figure 9**.2*, the illustration on modeling custom distributions (the airport
    example), the lower picture shows two standard distributions and two uniform distributions.
    Why don’t the curves go all the way to the top of the graph?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The two distributions will add together – the standard distributions sit *on
    top* of the uniform distributions, and the two combined go to the top of the graph.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Design your own robot emotions: Pick six contrasting emotions that can express
    the entire range of your robot’s personality. Why did you pick those?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is another subjective question. My answers are in the text. I picked emotions
    that represented the range of capability of my robot and the situations it would
    be in. I kept to a friendly type of robot so that the only negative emotion was
    sadness – there was no anger, for instance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you were designing a robot to have the personality of an annoying little
    boy (think Bart Simpson, Cartman, or Dennis the Menace if you are that old), what
    traits would it have?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A small boy would be mischievous, have a short attention span, constantly change
    the subject, keep trying to bring up the same topic over and over, and repeat
    variations of the same questions. How might we represent mischievous? Perhaps
    by ignoring directives and generating random events or distractions that get the
    robot’s attention away from the task at hand.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why is it important for the robot to have a backstory or biography?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To provide consistent answers to personal questions, such as “How old are you?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the following questions, pick a persona from my list to model (from *Integrating*
    *Artificial Personality*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write six lines of dialogue for the robot to ask a human where they last went
    on vacation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: So, where did you go on vacation last?
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summertime is coming up. Where did you go on vacation last year? Do you like
    to travel? Where have you been?
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I never get to go on vacation. Where did you go last?
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: I have heard of this concept called vacation. Where do you like to go? Have
    you been to the beach?
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write six ways for the robot to express that it is tired and needs to recharge
    without sounding like a robot.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: I’m tired – have you seen my recharger?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Wow, it is getting late. I’ve been at this for a long time.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, my battery is getting low. Must be about quitting time. I am starting
    to feel a bit run down.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, look at the time! My battery needs attending to. I’m getting hungry in
    here. Can I go charge now?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 10](B19846_10.xhtml#_idTextAnchor366)'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given that we started the chapter with knock-knock jokes and ended up talking
    about robot phobia and addressing philosophical questions about existence, do
    you feel that AI is a threat? Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: I do not feel that robots or AI are a threat in any way because the necessary
    and sufficient conditions for robots to be a threat do not exist, which is to
    say that the robots have to *want* to take over the world and must have a *need*
    to take over. Currently, robots and AI have no such wants or needs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: List any five professions that would be necessary to turn our Albert robot into
    a product.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would need project managers, packaging designers, advertising and marketing
    experts, salespeople, engineers, technicians, artists, package designers, machinists,
    electricians, accountants, lawyers, a psychologist, and support staff, among others.
    You can select any five from among these options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why would we need a psychologist in our imaginary company that manufactures
    robots?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Psychologists study normal and abnormal mental states and cognitive processes,
    which is exactly what we are trying to simulate in an artificial personality.
    We want the robot to not trigger bad responses in people. I once had a robot with
    flashing red eyes that caused small children to have panic attacks. Psychologists
    would help avoid such errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What components found in cell phones or smartphones are also found in quadcopters?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPS receivers, radios, Wi-Fi, Bluetooth, accelerometers, gyroscopes, and, these
    days, applications, or apps.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why are artificial intelligence systems, specifically artificial neural networks,
    naturally non-deterministic in both result and time?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are universal approximation systems that work in probabilities and averages,
    not in discrete numbers and logic. Artificial neural networks can take a different
    amount of time because a particular bit of data may take different paths at different
    times, going through a different number of neurons and thus not taking the same
    amount of time to process. It is true that if you provide the exact same input
    to a neural network, it will give you the exact same answer every time. In the
    real world that robots live in, however, the case that two inputs are identical
    in every way can be very rare.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What might be a practical application of an AI system that made predictable
    mistakes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can use a neural network-based system to model a bad human operator for
    a driving simulation to help teach other drivers (and self-driving cars) how to
    avoid bad drivers. The desired state is an unpredictable driver, so just train
    the neural network to 60% or so. Now the network will come up with the wrong answer
    40% of the time, i.e., be statistically predictable. I actually did this on a
    project for the Navy that wanted a simulation of imperfect people misusing a system
    at a predictable level, so they could create a responsive control system that
    could handle those mistakes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If an AI system was picking stocks for you and predicted winning stock 43% of
    the time and you had a second AI system that was 80% accurate at determining when
    the first AI had *not* picked good stock, what percent of the time would the AI
    combination pick profitable stock?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have 100 stocks picked by our AI program. Of that set, an indeterminate number
    are winners and losers. There is a 43% chance the stock is correctly predicted
    to be a winner and a 57% chance it is predictably a loser. We have no way of judging
    the stocks as being winners or losers except by investing our money, which is
    what we are trying to avoid – investing in bad stocks. A 43% chance of winning
    is not good.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second AI has an 80% chance of telling you that the first AI chose bad stock.
    Eighty times out of 100, you will know that the stock was not a winner. You are
    left with an 80% chance of correctly identifying one of the 57 bad stocks, which
    eliminates 45 stocks. That leaves you with 55 stocks, of which 43 are winners
    (on average), which raises your odds to 78%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Bayes'' theorem shows the combination of two independent probabilities (probability
    of *x* occurring given *c* has occurred):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*px* = probability of *x*, and *pc* = probability of *c*)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*p(x|c) = (px* pc) / (**px*pc)+(1-px)(1-pc)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using this theorem, I recomputed the combined probabilities as 75.1%, so I’ll
    take either answer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Robotic Operating System** (**ROS**) was a framework designed to enable the
    development of software for complex robots and was developed by a company called
    *Willow Garage*, specifically for the control of the PR2 robot. The PR2 was a
    human-sized robot with two **7-degree of freedom** (**7DOF**) arms and an entire
    array of sensors. Controlling this very complex robot required the interaction
    of a multitude of sensors, motors, and communications. The ROS framework allowed
    the development of robot components to be done independently. While not an operation
    system in the traditional sense of the word, it is a **Modular Open Source** **Architecture**
    (**MOSA**).'
  prefs: []
  type: TYPE_NORMAL
- en: The primary tool of ROS is a robust *publish-subscribe* service that makes talking
    between processes — that is, **Inter-Process Communications** (**IPC**) — easy
    and flexible. It also standardized a lot of the interfaces between sensors, motors,
    and controls for robots.
  prefs: []
  type: TYPE_NORMAL
- en: We will be using ROS 2 throughout this book. ROS 2 is a new version of ROS.
    In this *Appendix*, we will discuss how to install ROS 2, use it to communicate,
    and briefly introduce some of the tools ROS provides. We will provide an in-depth
    introduction to ROS 2 and describe some of the hardware involved in the design
    of our toy-collecting robot, Albert. We will also cover some of the hardware referenced
    in the book when creating Albert as our example for this book. Albert V2, the
    robot in this second edition of this book, is my 30th or so robot design.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in the appendix include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing MOSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A brief overview of ROS 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software requirements for the robot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the hardware for the robot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robot safety tips
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing MOSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ROS is an example of a MOSA. Why is this important? Imagine if every electrical
    appliance in your house had its own plug, a different voltage, and a different
    wire. It would make life very difficult for you. But all your electrical plugs
    are the same shape and put out the same voltage. They are standardized interfaces
    that allow you to plug many different types of appliances into them. A MOSA acts
    like that for software, standardizing interfaces and allowing *plug-and-play*
    compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are its advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: A MOSA system architecture allows modularity – the ability to create software
    in sections or modules that can be developed, debugged, and operated independently.
    Before ROS, I created one major executable that ran everything on my robot. The
    problem with this is, first of all, that I could not take advantage of the multi-core
    nature of my **Single Board Computer** (**SBC**), which was the robot’s brain.
    I had all my code in one thread, in one program, and splitting out functions to
    run independently was difficult.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then there were the interactions. If I changed the timing on my motor driver,
    it messed up the sensor timing for the camera. If I changed the path planner,
    then the steering needed adjusting. This sort of interaction is typical in a **unitary
    architecture**. However, in a MOSA, each section of the robot, for example, the
    robot arm controller, is independent and runs in its own program. They can be
    developed and debugged independently, and interactions are limited to the interfaces
    we create in those programs. This frees us from a lot of problems we would otherwise
    have.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other feature we can take advantage of is the very large library of standard,
    already-created interfaces and programs that ROS provides. We don’t need to create
    a steering interface; ROS has one (the `Twist` command). We don’t need to create
    data types for camera imagery data; ROS has several to choose from. ROS also has
    USB camera drivers and viewers that we can use without writing any code at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let’s talk a bit about how ROS 2 works.
  prefs: []
  type: TYPE_NORMAL
- en: A brief overview of ROS 2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, ROS 2 is the latest version of ROS, a widely used framework
    for developing robot applications. I’ve been using ROS for some time now and appreciate
    how much simpler it makes the integration of various components, sensors, and
    capabilities into my robots. I resisted moving to ROS for some time, but now that
    I have invested the time to learn what it can do, I can’t imagine developing a
    robot without it.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss some concepts that are foundational for our
    understanding of ROS 2, how to install ROS 2, and some basic commands that we
    can use with it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please use whatever is the latest version of ROS 2.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the basic concepts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ROS 2 works a bit differently than other programming paradigms. ROS is based
    on a publish/subscribe mechanism that allows different programs or processes to
    pass information from one to another without having to know in advance who is
    receiving the data. Let’s look at how this process works:'
  prefs: []
  type: TYPE_NORMAL
- en: Each program or code that communicates in ROS is called a **node**. Nodes each
    have names that uniquely identify them to the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nodes publish data on **topics**, which represent an interface for messages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messages** are interfaces that have data types (string, float, fixed, array,
    etc.).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a joystick interface node publishes to the commanded velocity (`cmd_vel`)
    topic with a `Twist` message. On the receiver end of the message, the motor control
    interface subscribes to the `cmd_vel` topic to receive Twist messages that have
    speed and turn information for the robot.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Why is it called a Twist message? The reason for using the term “twist” is conceptual.
    Think of a robot’s movement as a combination of straightforward (linear) motion
    and rotation (twisting or turning). By combining linear and angular velocities,
    a Twist message effectively describes how the robot “twists” through space, which
    includes both translation (moving from one place to another) and rotation (changing
    orientation).
  prefs: []
  type: TYPE_NORMAL
- en: 'ROS has a few other useful features:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a systemwide `DEBUG`, `INFO`, `WARN`, `ERROR`, or `FATAL`. Log messages
    are automatically displayed on the local output (usually the command line). They
    are collected centrally, and you can parse through log messages to debug problems.
    Periodically, you do need to purge old log files, as they do tend to pile up.
    Logs are put into the `~/.``log` directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of my personal favorite features is **parameters**. These are external data
    values that can be created independently of source code and can be used to turn
    features on or off, or to set critical settings, such as image size, resolution,
    range, and other features. The ability to have external parameters goes a long
    way to make ROS portable and the interfaces reusable. Parameters can be specified
    in launch files, which are ways of starting multiple programs (nodes) all at once.
    Albert has five major subsystems (control, motor drive, arm control, vision, and
    speech), all of which have to be started together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing ROS 2 and ROS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ROS 2 introduced some very significant improvements over the original ROS.
    Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The most notable improvement is the absence of the **roscore** *traffic cop*
    application. This central program acted as the Master node in any ROS implementation
    and directed the other nodes on where to communicate via sockets. It directed
    all the traffic on the network. Therefore, every node had to communicate with
    roscore to know where it was running. If roscore died, or was turned off, then
    the entire ROS set of applications stopped. Instead, ROS 2 uses **Data Distribution
    Services** (**DDS**) as its *middleware layer*. DDS is a standard for high-performance,
    scalable, and real-time data exchange, created by the **Object Management Group**
    (**OMG**) and managed by the DDS Foundation. It provides a decentralized discovery
    mechanism for nodes to find each other without needing a central master such as
    roscore. When a node starts, it advertises its presence to other nodes and also
    discovers existing nodes and topics. This process is managed by the underlying
    DDS implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROS 2 has more features for **real-time processing**, and does not have the
    weird workaround that ROS had, where we had to do things such as post-dating messages
    into the future. These features include using DDS (which is intended for real-time
    systems, the ability to work with **real-time operating systems** (**RTOSs**),
    and to use pre-emption to control processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One big improvement is that ROS 2 will **run natively on Windows** for the first
    time. You don’t need virtual machines to run ROS on Windows – you can build your
    own control panels and interfaces directly in Windows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROS 2 also improves **discovery** (the process of finding nodes on the network)
    and has some enhancements for cyber security.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, all in all, ROS 2 was well worth taking the time to upgrade, and I feel
    that it is far less fiddly than the old ROS, easier to set up on both Windows
    and Linux, and easier to manage.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at software requirements next.
  prefs: []
  type: TYPE_NORMAL
- en: Software requirements for the robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the software requirements for the robot and
    how to install them on the robot’s CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ROS 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The version of ROS 2 I installed on Albert currently is *Foxy*. Please feel
    free to use the latest version of ROS 2\. Jetson tends to run behind Ubuntu upgrades
    and is several releases behind. We use the Jetson Nano because it has the required
    power and the **Graphics Processing Units** (**GPUs**) to run neural network software.
    My version of the Nano is running Ubuntu 20.04, but you should also be able to
    get it working with Ubuntu version 18.04.
  prefs: []
  type: TYPE_NORMAL
- en: I used the standard ROS 2 installation script that can be found at [https://github.com/jetsonhacks/installROS2](https://github.com/jetsonhacks/installROS2).
    This is a script that contains all of the steps found on the regular ROS 2 installation
    page at [https://docs.ros.org/en/foxy/Installation/Alternatives/Ubuntu-Development-Setup.html](https://docs.ros.org/en/foxy/Installation/Alternatives/Ubuntu-Development-Setup.html).
  prefs: []
  type: TYPE_NORMAL
- en: Note that we are doing an *Install from Source* setup since many of the programs
    need to be recompiled to run on the Jetson Nano’s ARM-based architecture. For
    me, this process took about 4 hours, so it does take some patience.
  prefs: []
  type: TYPE_NORMAL
- en: Installing other packages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You will also need to install the following Python packages: **Scientific Python**
    (**SciPy**), **Numeric Python** (**NumPy**), **scikit-learn**, **Open Computer
    Vision** (**OpenCV**), and **PyTorch**. Let’s look at the commands needed to install
    these packages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SciPy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'NumPy:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'scikit-learn:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'OpenCV:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For more details on OpenCV, you can refer to [https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html](https://docs.opencv.org/3.4/d6/d00/tutorial_py_root.html)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'PyTorch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now let’s look at how we can get started with ROS 2.
  prefs: []
  type: TYPE_NORMAL
- en: Basic ROS 2 commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following commands relate to starting, controlling, and monitoring nodes
    in ROS 2:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start executing a package in ROS 2, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check that this node is running, we type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us a list of currently running nodes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can get more information about the node by typing this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This results in the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we can note the following key points:'
  prefs: []
  type: TYPE_NORMAL
- en: The `turtlesim` publishes `parameter_events` (which happen when you change parameter
    values), it has a logging interface to `/rosout`, and it publishes the color of
    the turtle robot and its position (pose)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For input, it subscribes to `cmd_vel`, which uses the `Twist` command to move
    the turtle, and to `parameter_events`, which allows the program to receive parameter
    changes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The command for looking at topics is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows topics that are active. For our `turtlesim` example, we get the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you add `-t` to the end of that command, you also get the topic message
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'You can find more details and the full tutorial at [https://ros2-industrial-workshop.readthedocs.io/en/latest/_source/navigation/ROS2-Turtlebot.html](https://ros2-industrial-workshop.readthedocs.io/en/latest/_source/navigation/ROS2-Turtlebot.html).
    Also, I recommend that you refer to the topic list in the tutorials available
    from the ROS 2 website: [https://docs.ros.org/en/foxy/Tutorials.html](https://docs.ros.org/en/foxy/Tutorials.html).
    This will give a more robust introduction to ROS 2\. The rest of what you need
    to run the programs in this book is in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the hardware that makes up Albert the Robot.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the hardware for the robot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I designed Albert the Robot to perform one manual task – picking up toys. As
    such, I chose a set of motors, a speaker, and a robot arm as effectors, and a
    camera and microphone as sensors. Here is a labeled diagram of what Albert looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Albert the Robot](img/B19846_12_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Albert the Robot
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will look at how I put Albert together.
  prefs: []
  type: TYPE_NORMAL
- en: Effectors – base, motors, and wheels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The robot base is a two-layer aluminum allow frame that I purchased at [https://www.amazon.com/gp/product/B093WDD9N5](https://www.amazon.com/gp/product/B093WDD9N5).
    This base uses **Mecanum** wheels, which have the unique ability to move the chassis
    not just forward and back, but sideways and at any angle. For video game fans,
    this sideways movement is sometimes called **strafing**. You will note that the
    wheels have smaller rollers mounted at 45-degree angles. These convert various
    inputs into multiple directions. Moving all four motors forward, not surprisingly,
    results in the platform moving forward. Moving the right wheels forward and the
    left wheels backward results in turning in place to the right.
  prefs: []
  type: TYPE_NORMAL
- en: When we move the left wheels away from each other and the right wheels towards
    each other (left front forward, right front backward, left rear backward, and
    right rear forward), the vehicle moves sideways – strafes – to the right. Reverse
    these directions and you strafe to the left. These are the motions we need to
    complete the exercises in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'I made some modifications to the base: I cut off the little white connectors
    to the four motors so I could wire them to the motor controller.'
  prefs: []
  type: TYPE_NORMAL
- en: Battery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I have used a 4,200 mAh **Nickel Metal Hydride** (**NiMH**) battery– no lithium
    battery here, so there’s less risk of fire. The output is 7.2 V. This should provide
    several hours of runtime to our robot, and it fits in the chassis. You can use
    any drone-type battery with the same specs. A larger battery won’t fit in the
    chassis, which is quite small. This battery is adequate for our needs. Here is
    an example of a battery you can get for your robot: [https://www.amazon.com/gp/product/B08KXYY53G](https://www.amazon.com/gp/product/B08KXYY53G).'
  prefs: []
  type: TYPE_NORMAL
- en: DC/DC power supply
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of my favorite bits of kit is this DROK DC/DC adjustable power supply.
    This board provides the 5 V power to the main computer and the Arduino. You adjust
    the voltage level to 5 V with a small screwdriver. I really like to have this
    display on the robot to show that everything is working. Here is a link to the
    one I used: [https://www.amazon.com/Converter-DROK-Adjustable-Stabilizer-Protective/dp/B01FQH4M82?th=1](https://www.amazon.com/Converter-DROK-Adjustable-Stabilizer-Protective/dp/B01FQH4M82?th=1).'
  prefs: []
  type: TYPE_NORMAL
- en: CPU – the brains of the outfit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we’ve mentioned throughout the book, the main computer for Albert is the
    **Nvidia Jetson Nano**. This is a rugged little single-board computer with a big
    heat sink. This CPU is specifically designed to run AI code, with 128 **graphics
    processing units** (**GPUs**) and four Arm A57 **central processing units** (**CPUs**)
    running at 1.43 GHz, and packed with peripheral ports and external I/O capability.
    This is an ideal board for our needs for running a robot with perception and decision-making
    abilities. The Jetson sits on a development board, which provides an interface
    to all of its capabilities. Any of the other members of the Jetson family (TX2,
    Xavier, and AGX) will also work, but I’ve sized the code in the book to fit on
    the Nano specifically. This is a link to purchase the Jetson Nano computer: [https://developer.nvidia.com/embedded/jetson-nano-developer-kit](https://developer.nvidia.com/embedded/jetson-nano-developer-kit).'
  prefs: []
  type: TYPE_NORMAL
- en: 'I’ve added a Wi-Fi card, which is mounted underneath the CPU. This is the one
    I used: [https://www.amazon.com/gp/product/B07SGDRG34](https://www.amazon.com/gp/product/B07SGDRG34).'
  prefs: []
  type: TYPE_NORMAL
- en: Effectors – robot arm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I can’t say enough about how much of an upgrade the digital servo arm used
    in this book ([https://www.hiwonder.com.cn/store/learn/42.html](https://www.hiwonder.com.cn/store/learn/42.html))
    is over the previous analog servo arm I used in the first edition of the book.
    Some of the advantages of this arm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: This arm uses **digital servos**, and this simplifies the wiring of the arm,
    since we just plug one servo into the next in a series. Rather than being controlled
    by analog signals, these servos have a digital serial interface that gives us
    not just fine control, but the ability to determine where the motors actually
    are, rather than where they were commanded to be. Why is this important? If the
    arm hits something and can’t continue to move, it will stop. This is not a surprise,
    but then when you ask for the motor position, it tells you where it stopped. This
    means you can use the arm itself as a sensor! To determine where the floor is,
    you command the arm to a downward position, let it stop when it hits the floor,
    and read the motor positions to see where the floor actually is relative to the
    arm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This arm also lets you move the arm manually – with your hands – and then read
    the position of the arm, which is very useful when designing poses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another useful feature is the **self-programming mode**. You can put the arm
    in *program* mode using the relevant button, then move the arm and push the other
    button (labeled *run*), and the program moves into the arm without the computer.
    Then you can push *program* again to store these moves, and then hit *run* and
    it will play back what you did. This is useful for testing out moves. I used it
    to prototype grasping positions for picking up toys.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like to control a simulated robot arm instead of a real one, there
    is a tutorial at [https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot](https://community.arm.com/arm-research/b/articles/posts/do-you-want-to-build-a-robot).
  prefs: []
  type: TYPE_NORMAL
- en: Arm controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an important note on wiring up the arm – the arm draws far more electrical
    power than the USB is capable of supporting. Do *not* run the arm without a power
    supply connected to the power port. The arm will run with the power straight from
    the battery, without using the DC/DC converter. I added a master power switch
    to the robot so I can turn it on and off safely. Without the power switch, the
    only way to shut down the robot is by physically unplugging the battery, which
    is problematic.
  prefs: []
  type: TYPE_NORMAL
- en: Arduino microcontroller and motor controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Arduino UNO is integrated into the motor controller, which is plugged into
    the Arduino’s **General Purpose Input/Output** (**GPIO**) ports. This is the simplest
    way to create a computer interface to the four motors that drive the base of the
    robot. The motor controller has to have power from the batteries (again without
    going through the DC/DC converter). The Arduino uses **Pulse Width Modulation**
    (**PWM**) to control the four brushed DC motors in the drive base. The motor controller
    is from Adafruit, and is available at this website: [https://www.adafruit.com/product/1438](https://www.adafruit.com/product/1438).'
  prefs: []
  type: TYPE_NORMAL
- en: Sensor – USB camera
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main sensor of Albert is the wide **field-of-view** (**FOV**) camera that
    has a USB interface. Any number of cameras will fill this need, including many
    webcams. My camera has a 170-degree FOV and a resolution of 1,024x768 pixels.
    You can use a camera with higher resolution but make the USB ROS camera driver
    downsample the image to 1,024x768 so that we know that the rest of the software
    can handle the bandwidth. I used a color camera with an RGB (three-color) output.
  prefs: []
  type: TYPE_NORMAL
- en: Sensor and effector – audio interface
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I bought a USB audio card ([https://www.amazon.com/gp/product/B08R95XJW8](https://www.amazon.com/gp/product/B08R95XJW8))
    to support the voice input and output on the robot. This audio interface has both
    a microphone and some small speakers. This unit makes our interface to the voice
    interface for speech recognition and text-to-speech output. This unit is well
    constructed and sturdy and it does a good job with music as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a handy wiring diagram of how Albert goes together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Block wiring diagram of Albert the Robot](img/B19846_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Block wiring diagram of Albert the Robot
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have Albert the Robot assembled, we can use it for the examples
    in the book. Albert’s a very versatile platform with a lot of capability, as you
    will see.
  prefs: []
  type: TYPE_NORMAL
- en: Robot safety tips
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s quickly look at some safety tips related to working around robots:'
  prefs: []
  type: TYPE_NORMAL
- en: We are using fairly high-current batteries and drive systems. Be very careful
    with wiring and look out for shorts, where the positive and negative wires are
    touching. It is not a bad idea to put a fuse between the battery and the power
    supply of about 10 Amps to protect against accidental shorts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be careful when the robot is operating. It may suddenly change direction or
    get stuck. I have a policy of not sitting down, having my hands in my pockets,
    or using a cell phone when the robot’s motor drives are activated. You need to
    be paying attention.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beware of the pinch points in the robot arm – you can get a finger caught in
    them quite easily (as I have learned). Don’t put your fingers inside the joints
    with the power turned on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a checklist for setting up the robot, starting all the software, and turning
    on the hardware. This will stop you worrying that you have forgotten a step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, here are the steps you should be following. Turn on the robot’s
    power. Wait for the computer to boot up. Connect to the onboard computer from
    your laptop or desktop using **Virtual Network Computing** (**VNC**). Start the
    onboard software, and then start giving commands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beware when charging lithium batteries; they can catch fire. It is best to charge
    them inside a metal box. I used NiCad batteries in my Albert prototype for this
    reason – no fire hazard. Lithium batteries are lighter and stronger, but they
    have their downsides as well, such as catching fire, being a hazardous chemical,
    and permanently losing charge when frozen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
