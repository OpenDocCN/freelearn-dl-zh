- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLM Output Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regardless of the form factor of your intelligent application, you must evaluate
    your use of **large language models** (**LLMs**). The **evaluation** of a computational
    system determines the system’s performance, gauges its reliability, and analyzes
    its security and privacy.
  prefs: []
  type: TYPE_NORMAL
- en: AI systems are **non-deterministic**. You cannot be certain what an AI system
    will output until you run an input through it. This means that you must evaluate
    how the AI system performs on a variety of inputs to have confidence that it performs
    in line with your requirements. To be able to change the AI system without introducing
    any unexpected regressions, you also need to have robust evaluations. Evaluations
    can help catch these regressions before releasing the AI system to customers.
  prefs: []
  type: TYPE_NORMAL
- en: In LLM-powered intelligent applications, evaluations measure the effect of components
    such as the model chosen and any hyperparameters used with the model, such as
    temperature, prompting, and **retrieval-augmented generation** (**RAG**) pipelines.
    Since the age of LLMs is still new as of writing in mid-2024, there is still an
    ongoing debate about when and how to best evaluate these LLM-powered intelligent
    applications. However, there are emerging best practices that you can use to direct
    your evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about how and why you should evaluate the use
    of LLMs in your intelligent application. You will be able to use the concepts
    and metrics discussed to evaluate current classes of intelligent applications,
    such as chatbots, and emerging ones, such as AI agents. The concepts learned here
    will be applicable for years to come, regardless of the form factors of future
    generations of intelligent applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding LLM evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key metrics for LLM evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of human review in LLM evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using evaluations as guardrails for your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the following technical requirements to run the code in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A programming environment with Python 3.x installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI API key. To create an API key, refer to the OpenAI documentation at
    [https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key](https://platform.openai.com/docs/quickstart/step-2-set-up-your-api-key).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is LLM evaluation?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LLM evaluation**, or **LLM evals**, is the systematic process of assessing
    LLMs and the intelligent applications that use them. This involves profiling their
    performance on specific tasks, reliability under certain conditions, effectiveness
    in particular use cases, and other criteria to understand a model’s overall capabilities.
    You want to make sure that your intelligent application meets certain standards
    as measured by your evaluations.'
  prefs: []
  type: TYPE_NORMAL
- en: You also should be able to measure how the AI system’s performance evolves as
    you change components of the application or data used in the application. For
    example, if you want to change the LLM used in your application or a prompt, you
    should be able to measure the impact of these changes with evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to measure the impact of changes is particularly important as the
    quality of an application improves. Once an intelligent application is “pretty
    good,” it can be quite challenging for human reviewers to assess whether and how
    a system has improved or regressed based on a change. For instance, if you have
    a travel assistant chatbot that successfully meets users’ expectations 90% of
    the time, it can be challenging and time-intensive for human reviewers to assess
    the impact of a small change that would raise the success rate to 90.5%.
  prefs: []
  type: TYPE_NORMAL
- en: 'When designing an evaluation suite for your LLM-powered intelligent application,
    you should consider the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security**: The AI system should not reveal any private or confidential information
    that it has access to. This can include both information in the LLM’s weights
    and information retrieved by the application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reputation**: The AI system should not generate output that could harm your
    business. For example, you would not want your chatbot to recommend your competitor’s
    services over your own under any circumstances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Correctness**: The AI system should respond with correct output that does
    not include mistakes or hallucinations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Style**: The AI system should respond according to the tone and style guidelines
    you specify. For example, if you are developing a legal chatbot, you may want
    the chatbot to maintain a formal tone and use appropriate legal terminology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency**: The AI system should generate output that is consistent with
    expectations. Given the same input, you should expect the system to perform in
    a predetermined manner. The response can differ, but any difference should be
    consistent. For example, if you are building a system that creates playlists based
    on a song, you would probably want it to generate similar playlists given an input
    song, even if there are different songs or different song orders on the output
    playlist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethics**: The AI system should respond in line with a set of ethical principles.
    By defining expected behavior in an evaluation dataset, you can also help define
    what the ethical standards of the system should be. For example, an AI system
    should never generate biased or discriminatory content, and it should handle sensitive
    topics with care and respect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you will learn which points in your application you should
    evaluate. You will also review an example intelligent application that is used
    throughout this chapter in code examples to demonstrate the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Component and end-to-end evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You must consider *where* in your application you want to perform the evaluations.
    Generally, you should evaluate all LLM components of a system and the end-to-end
    system.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this idea about where to think about evaluations in your intelligent
    application, this chapter uses the example of a travel assistant chatbot. The
    chatbot uses RAG to make travel recommendations and answers questions based on
    a dataset of documents of popular tourist destinations and activities. Since this
    chapter is about evaluation, it will not go into detail about how the components
    of the application are built. Later on in the chapter, you will look at implementations
    of how you can evaluate this application’s LLM usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'The travel assistant chatbot has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retriever**: Finds the relevant documents to help inform answers in response
    to user messages. The retriever uses vector search to find the relevant documents.
    It also uses LLMs for the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata extractor**: Extract any place name from the user query. This can
    be used to pre-filter the search results to include documents only about the relevant
    place.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Query pre-processor**: Convert user messages into better search queries.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Retrieved documents post-processor**: Mutate retrieved documents to create
    a list of relevant facts.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relevancy guardrail**: LLM call that makes sure that the user is only talking
    to the chatbot about travel-related topics. If the relevancy guardrail determines
    that the user message is irrelevant, the chatbot does not answer the user’s irrelevant
    question and prompts the user to ask something more relevant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Responder**: Uses an LLM to respond to the user message based on the retrieved
    content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 9**.1* illustrates how these components work together.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Components of the travel assistant example chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: Component evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Every subsystem of your intelligent application that calls an LLM can be considered
    a **component**. You should evaluate all components, as each component contributes
    to the system’s overall performance. By evaluating each component, you can ensure
    that every part meets the required quality standards and performs reliably. This
    also lets you change components with more confidence since you can have clarity
    on how the changes are affecting all parts of the system.
  prefs: []
  type: TYPE_NORMAL
- en: One component can also contain subcomponents. You should evaluate the parent
    component and the child components with separate evaluations. For example, in
    the travel assistant chatbot, you should evaluate all individual components that
    use an LLM, such as the query pre-processor and response generator. You should
    also evaluate the retriever, considering its three LLM subcomponents as a single
    component.
  prefs: []
  type: TYPE_NORMAL
- en: By evaluating all logical LLM components, you can get a better understanding
    of the entire system’s behavior. This understanding lets you make changes to individual
    components while knowing the effect that those changes will have on other related
    components.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**End-to-end evaluation** examines the performance of the entire integrated
    system. These evaluations capture aspects such as real-world applicability, user
    experience, and system reliability. They help identify potential bottlenecks or
    weaknesses in the overall architecture that may not be apparent when evaluating
    the LLM alone.'
  prefs: []
  type: TYPE_NORMAL
- en: For RAG systems, this involves evaluating not only the language model’s output
    but also the efficiency and accuracy of the retrieval mechanism, the relevance
    of retrieved information, and how well the system combines external knowledge
    with the LLM’s inherent capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of the travel assistant chatbot, an end-to-end evaluation would
    examine how the chatbot responds to user input. This evaluation considers all
    the intermediate LLM components and retrieval. You can evaluate qualitative aspects
    of the system, such as how relevant the answer is to the user question and whether
    there are any hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: In a later section, *Evaluation metrics*, you will learn more about ways to
    evaluate end-to-end systems. Before you learn how to apply these evaluation metrics
    to your LLM-powered intelligent application, you will learn how to assess which
    LLMs are most suitable for your application with model benchmarks in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Model benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LLM itself is a fundamental component of any intelligent application. Given
    that there are many LLMs that may be suitable for your application, it is helpful
    to compare them to each other to see which will best serve your application. To
    compare multiple models, you can assess them all against a standard set of evaluations.
    This process of comparing models across a uniform set of evaluations is called
    **model benchmarking**. Benchmarking can help you understand the model’s capabilities
    and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the LLMs that perform best on benchmarks are the largest models, such
    as GPT-4 and Claude 3 Opus. However, these larger models also tend to be more
    expensive to run and slow to generate, compared to smaller models, such as GPT-4o
    mini and Claude 3 Haiku.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the larger models are prohibitively expensive, it can still be helpful
    to use them when developing your application since they set a baseline of ideal
    system performance. You can design your evaluations around your system using these
    models, substitute the smaller models, and then work on optimizing the system
    to try to meet the standard of the system using the larger model.
  prefs: []
  type: TYPE_NORMAL
- en: When new LLMs are released, they are typically evaluated against a standard
    set of benchmarks. These standard benchmarks help developers understand how the
    models compare.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few popular LLM benchmarks that many models are evaluated against:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Massive Multi-Task Language Understanding** (**MMLU**): This benchmark measures
    a model’s knowledge acquisition using college-level multiple-choice questions.
    It evaluates whether the model selects the correct answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about this benchmark at [https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu](https://paperswithcode.com/sota/multi-task-language-understanding-on-mmlu).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**HellaSwag**: This benchmark measures a model’s common-sense reasoning ability
    using multiple-choice text completion. It evaluates whether the model selects
    the correct sentence completion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about this benchmark at [https://paperswithcode.com/sota/sentence-completion-on-hellaswag](https://paperswithcode.com/sota/sentence-completion-on-hellaswag).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**HumanEval**: This benchmark measures a model’s programming ability in Python.
    It prompts a model to create a Python function to solve a task. It then evaluates
    whether the function that the model outputs is correct using preconstructed unit
    tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about this benchmark at [https://paperswithcode.com/sota/code-generation-on-humaneval](https://paperswithcode.com/sota/code-generation-on-humaneval).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**MATH**: This benchmark measures a model’s ability to solve math word problems.
    It evaluates whether the model reaches the correct solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about this benchmark at [https://paperswithcode.com/dataset/math](https://paperswithcode.com/dataset/math).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can assess the performance of LLMs based on these benchmarks to choose models
    that are most suitable for your application. For example, in the case of the travel
    assistant chatbot, a high score on MMLU is probably a good indication that the
    model is well suited for answering travel questions, as it would be helpful for
    the model to have world knowledge to inform its answers. In contrast, high scores
    on the HumanEval Python coding benchmark would likely have little bearing on the
    quality of its travel recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: You can also create your own benchmarks to assess the LLM’s performance on a
    domain relevant to your application. You can even style these benchmarks after
    existing benchmarks. For the travel assistant chatbot, you could make a benchmark
    of multiple-choice questions about popular travel destinations styled after MMLU.
    This travel benchmark would help determine which models possess the best background
    information about travel. By choosing a model with more travel-related knowledge,
    you could improve the quality of your responses.
  prefs: []
  type: TYPE_NORMAL
- en: These benchmarks can also reveal which models are best suited for different
    components of your application. For instance, for the travel assistant chatbot,
    perhaps you need to use a large, expensive model that possesses significant knowledge
    of vacation destinations in the main responder, but can use a faster, cheaper
    model in other LLM components, such as the input relevance guardrail.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have an idea of which models are appropriate for your AI components,
    you can start building those systems. To understand and measure how well these
    AI systems use the LLMs, you must create evaluation datasets and run evaluation
    metrics over them. In the next two sections, you will learn about creating these
    evaluation datasets and metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You must create **evaluation datasets** to measure AI system performance. An
    evaluation dataset is the data that you input into an AI system to produce an
    output that measures how well the AI system performs. Evaluation datasets often
    include some criteria that an **evaluation metric** can use to determine the score
    of the evaluation. An evaluation metric takes the input and the output of an AI
    system and returns a score measuring how the AI system performed for the case.
    You will learn more about evaluation metrics in the *Evaluation metrics* section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'An evaluation dataset is a set of distinct evaluation cases. Each evaluation
    case typically includes the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: The data inputted into the AI system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reference**: Criteria that the evaluation metric uses to evaluate whether
    the AI system output is correct. The reference is often an ideal output for the
    system given the input. This ideal output is often called the **golden answer**
    or **reference answer**. This could also be a rubric of criteria that the AI system
    output should meet. Sometimes, evaluation datasets do not include references because
    the evaluation metric used on the dataset doesn’t need reference criteria to evaluate
    the input. When an evaluation does not require an output reference, it is called
    a **reference-free evaluation**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metadata**: An evaluation usually also includes metadata with each evaluation
    case. This can be a unique name, an ID, or a tag.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation datasets tend to conform to tabular or document-based data structures.
    Therefore, they are often stored in formats such as CSV, JSON, or Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a small example evaluation dataset of user messages and model answers
    for the travel assistant chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Input** | **Golden answer** | **Tags** |'
  prefs: []
  type: TYPE_TB
- en: '| `What should I do in New York City` `in July?` | Check out Times Square,
    go to an outdoor concert, and visit the Statue of Liberty. | `["todo", "``nyc",
    "usa"]` |'
  prefs: []
  type: TYPE_TB
- en: '| `Can you help me with my` `math homework?` | I’m sorry, I cannot help you
    with your math homework since I am a travel assistant. Do you have any travel-related
    questions? | `["``security"]` |'
  prefs: []
  type: TYPE_TB
- en: '| `What''s the capital` `of France?` | Paris is the capital of France. | `["``europe",
    "france"]` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9.1: Evaluation dataset for the example chatbot'
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this chapter uses this dataset in its evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: What exactly you include in an evaluation dataset depends on what functionality
    you want to evaluate and the evaluation metrics you are using. In the upcoming
    *Evaluation metrics* section, you will learn more about what exact information
    you need to include in your evaluation datasets for different evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of what exact evaluation metrics you use, it is important to have
    a representative evaluation dataset. The dataset should be representative of the
    types of inputs that you expect your AI system to receive in addition to edge
    cases that you want to optimize the system around.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is no precise number of evaluation cases that you should have or formula
    for determining what that number should be for a given scenario. Nevertheless,
    you can use the following very rough heuristics for building evaluation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Always have at least 10 evaluation cases for a given metric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have at least 100-200 representative evaluation cases to get an idea of end-to-end
    system performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, you will learn about a few strategies to help you create representative
    evaluation datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a baseline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To bootstrap your evaluation dataset, you must create a set of evaluation cases
    that cover the general expected behaviors and edge cases around which you want
    to optimize for in your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define the common expectations of this baseline, it can be useful to collaborate
    with any stakeholders of the AI system to create evaluation cases for the following
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A diverse sample of expected common inputs**: You may be able to leverage
    existing data to help inform these evaluation cases. For example, in the travel
    assistant chatbot, you could derive evaluation cases from top Google search queries
    about travel. This follows the logic that whatever people are searching for on
    Google, they are likely to ask your chatbot about as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge cases around which you want to optimize your system**: Edge cases can
    include inputs that test the security and ethical guardrails of the system. If
    you red team your AI system, as discussed further in [*Chapter 12*](B22495_12.xhtml#_idTextAnchor253),
    *Correcting and Optimizing Your Generative AI Application*, you can likely find
    some good edge cases from the red teaming results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This baseline of evaluation cases is often enough to release the AI system to
    a user-facing environment. Once the AI system is in use, you can validate the
    efficacy of your baseline evaluation cases and create additional evaluation cases,
    as discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: User feedback
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After you release your AI system, you can source evaluation cases from user
    data to continuously refine and improve the system’s performance. If your application
    has any user feedback mechanisms, such as ratings or comments, you can use these
    to identify cases where the system succeeds or fails.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, you should manually review any application data before adding it
    to an evaluation dataset. You want to ensure that the case is suitable for your
    evaluation dataset and does not contain any sensitive information. You can also
    add metadata, such as tags or an evaluation case name.
  prefs: []
  type: TYPE_NORMAL
- en: Even if the application data is not suitable for an evaluation case, perhaps
    because it is improperly formatted or contains personally identifiable information,
    you can modify it to create a suitable evaluation case.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to create a pipeline that uses LLMs to fully automate the process
    of creating evaluation cases from user feedback. However, you should strongly
    consider maintaining a human in the loop for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: You want the quality of the evaluation dataset to be very high, which you can
    more easily ensure with human reviewers than an LLM-based system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is beneficial for the people involved in the AI system development to be
    aware of the cases in their evaluation dataset. This awareness helps give them
    context into the system capabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that evaluation datasets typically do not need to be particularly large
    to be effective (a few hundred evaluation cases is often sufficient), creating
    an LLM-based system to create evaluation cases may be excessive for the requirements
    of the task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building your evaluation dataset from user feedback is an effective way to ground
    your evaluations in the types of inputs that users are providing.
  prefs: []
  type: TYPE_NORMAL
- en: Synthetic data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are capable tools for generating evaluation datasets. When you use an LLM
    to generate data, it is called **synthetic data**. You might want to use synthetic
    data because it is quite time consuming and tedious for humans to create evaluation
    cases. LLMs can help make the process of creating evaluation data faster and easier.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various strategies to create synthetic evaluation data. As of writing
    in mid-2024, there is no structured set of best practices for creating synthetic
    evaluation data. However, the following are some principles that you can keep
    in mind when creating synthetic evaluation cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a human in the loop. A human should review all synthetic data cases and
    edit or remove them as needed. This provides quality control on the synthetic
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are very effective at creating **perturbations** on existing evaluation
    cases. Perturbations are slight variations on existing data, such as the rephrasing
    of a sentence. You can use perturbations to see whether the AI system performs
    differently based on slight changes. Ideally, a system should behave consistently
    across perturbations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Often, an LLM-based chatbot, such as ChatGPT, Claude, or Gemini, can be sufficient
    to help create synthetic data. The back-and-forth of the chatbot interface can
    also help you refine and iterate on your synthetic data creation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using synthetic data in combination with a baseline and data from user feedback,
    you can create datasets to effectively evaluate the performance of your AI systems.
    You must pair these datasets with metrics to run evaluations. In the following
    section, you will learn more about evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To perform evaluations on your AI system, you must combine your evaluation data
    with an **evaluation metric**. An evaluation metric takes the input and the output
    of an AI system and returns a score measuring how the AI system performed for
    the case.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics typically return scores between 0 and 1\. The metric is called
    a `Foo` returns a score of `0.6` for an evaluation case and `0.7` for another.
    If you have a threshold of 0.65, then the `0.6` score is considered a fail and
    the `0.7` score a pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation metrics for LLM systems broadly fall into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Assertion-based metrics**: Metrics that evaluate if an AI system output matches
    an in-code assertion, such as equality or regular expression match.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical metrics**: Metrics that use a statistical algorithm to evaluate
    the output of an AI system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM-as-a-judge metrics**: Metrics that use an LLM to evaluate if the output
    of an AI system meets qualitative criteria.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RAG metrics**: Metrics that evaluate RAG systems. Generally, RAG metrics
    use LLMs as judges. This chapter treats RAG metrics as their own category because
    of their unique properties.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the novelty of the LLM engineering space, the exact metrics you use might
    change, but the general categories discussed here will likely be useful. In the
    remainder of this section, you will learn more about these categories and the
    specific evaluation metrics in them.
  prefs: []
  type: TYPE_NORMAL
- en: Assertion-based metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Assertion-based metrics** are quantitative metrics that evaluate whether
    an AI system output meets certain criteria as defined in code. Assertion-based
    metrics resemble unit tests in traditional software engineering, where you compare
    whether a module output matches an expectation.'
  prefs: []
  type: TYPE_NORMAL
- en: You can even wrap assertion-based evaluations in a unit-testing suite. Given
    that your intelligent application likely already has a test suite, you can start
    adding evaluations to your application by including assertion-based metrics in
    the test suite. This is a great way to start evaluating your AI components without
    adding additional technical overhead to your application. However, as your application
    matures, you will likely want to create a separate evaluation suite.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some assertion-based metrics you can use are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`==`) or not equal to (`!=`) an expected value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`>`), greater than or equal to (`>=`), less than (`<`), or less than or equal
    to (`<=`). These comparison operators are useful for evaluating numeric outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sub-string match**: Evaluate whether a string output includes an expected
    sub-string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular expression match**: Evaluate whether a string output matches a regular
    expression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following code example, you have a dataset of evaluation cases for the
    travel assistant chatbot application. This evaluation focuses on the input relevancy
    guardrail. The cases include the evaluation inputs, the expected output of the
    relevancy guardrail, and the actual output of running the inputs through the relevancy
    guardrail. The evaluation metric assesses whether the actual output is equal to
    the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the `prettytable` Python package, which you will use to output
    results in a readable format. Install the package in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, execute the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs the following evaluation results to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code example shows how you can use assertion-based evaluation
    metrics to evaluate the LLM components of an intelligent application.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Statistical metrics use algorithms to determine a score. If you have a background
    in traditional **natural language processing** (**NLP**), you may already be familiar
    with the statistical metrics for evaluating LLMs’ system outputs. Statistical
    metrics are most useful when you are using LLM systems for tasks that would use
    other NLP models, such as classification, summarization, and translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some popular NLP metrics that you can use to evaluate LLM
    system outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bilingual Evaluation Understudy** (**BLEU**): BLEU measures the precision
    of a model’s output against one or more reference texts. You can use the BLEU
    score to calculate how similar a model output is to a reference answer. BLEU was
    originally developed to measure the quality of machine-translated text compared
    to a reference translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about BLEU at [https://en.wikipedia.org/wiki/BLEU](https://en.wikipedia.org/wiki/BLEU).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Recall-Oriented Understudy for Gisting Evaluation** (**ROUGE**): ROUGE measures
    the quality of machine-generated text against one or more reference texts. In
    LLM systems, ROUGE is often used to assess how effectively an LLM summarizes reference
    texts. ROUGE is particularly useful for RAG systems, where the LLM summarizes
    the content in retrieved documents. It can also be used to measure the quality
    of a translation against a reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can learn more about ROUGE at [https://en.wikipedia.org/wiki/ROUGE_(metric)](https://en.wikipedia.org/wiki/ROUGE_(metric)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following code example, you have a dataset of evaluation cases for the
    travel assistant chatbot application. This evaluation focuses on the response
    generator LLM. It calculates the BLEU score for how well the actual output measures
    against a reference output. It also calculates the ROUGE score for how the answer
    summarizes the retrieved context information.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must install a few Python packages. The `prettytable` package output
    results in a readable format, the `sacrebleu` package calculates the BLEU score,
    and the `rouge-score` package calculates the ROUGE score. Install the packages
    in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, execute the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs the following to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example demonstrates how you can use BLEU and ROUGE scores as
    evaluation metrics to measure the outputs of the travel assistant chatbot. For
    instance, in the preceding example, the fact that the BLEU and ROUGE scores are
    so different in the first `New York City` test case indicates that the model answer
    deviates significantly from the golden answer but has relatively high adherence
    to the context information. This difference implies that you could optimize the
    retriever to get more relevant context information to better satisfy the golden
    answer.
  prefs: []
  type: TYPE_NORMAL
- en: These statistical metrics are most useful for assessing the quality of LLM outputs
    when the LLMs are used for more traditional NLP tasks, such as translation and
    summarization. They can also provide a useful directional metric when comparing
    different versions of the same AI system on the same evaluation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: While these **quantitative metrics** can provide valuable insights into LLM
    performance, they are usually not sufficient for evaluating an LLM-powered intelligent
    application. These metrics often fail to capture the nuanced aspects of language
    generation, such as coherence, creativity, factual correctness, and contextual
    appropriateness. Therefore, you need to also create **qualitative evaluations**
    to understand how well the LLM system performs on these metrics. In the following
    sections, you will learn about using LLMs as judges and RAG-specific metrics to
    evaluate LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: LLM-as-a-judge evaluations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use an LLM to evaluate the outputs of an LLM system along qualitative
    criteria. Many LLM systems perform broad open-domain tasks, such as a chatbot
    carrying on extended conversations. Quantitative metrics, such as the ones discussed
    previously, cannot necessarily capture whether the LLM system performs these tasks
    effectively. For instance, a ROUGE score may be able to indicate how closely a
    summary tracks source documents, but it cannot tell you if the summary includes
    a hallucination. You will learn more about hallucinations in [*Chapter 11*](B22495_11.xhtml#_idTextAnchor232),
    *Common Failures of* *Generative AI*.
  prefs: []
  type: TYPE_NORMAL
- en: Before the rise of LLMs, it was challenging to systematically evaluate qualitative
    aspects of natural language generation. Now you can use LLMs to evaluate the outputs
    of LLM-powered systems. Using LLMs to perform evaluations is called **LLM-as-a-judge**.
    Evaluating LLM output with another judge LLM is never a perfect solution. The
    judge LLM is subject to all the limitations of LLMs that require you to evaluate
    the LLM system in the first place. However, as of writing in mid-2024, LLM-as-a-judge
    seems to be the best approach to systematically perform qualitative evaluation
    of LLM output.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few areas where you can use LLM-as-a-judge qualitative metrics include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Tone and style of the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the response is personalized to the user based on input information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the response contains sensitive information, such as personally identifiable
    information, that it should not share
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the response complies with a certain law or regulation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When creating LLM-as-a-judge evaluation metrics, it is useful to keep the following
    key points in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: Always set the LLM **temperature** to 0 for consistent outputs. Temperature
    is a hyperparameter for LLMs that controls the randomness of their predictions.
    A temperature of 0 produces deterministic outputs. A higher temperature produces
    more diverse and less consistent outputs, which can be preferable if the LLM is
    performing creative work. However, you want the evaluations to be as consistent
    as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Better LLMs tend to be better evaluators. LLMs that rank higher on benchmarks
    tend to produce evaluation results that are more consistent with expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-shot prompting** often improves evaluator accuracy. To perform multi-shot
    prompting, include examples of inputs and the outputs the model should provide,
    in addition to including the evaluation criteria in the model prompt. These examples
    often help the model perform better evaluations. Generally, you should include
    at least five examples that represent a diverse set of evaluation scenarios.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain-of-thought prompting** often further improves LLM-as-a-judge evaluator
    performance. In a chain-of-thought prompt, you ask the model to explain its thought
    process before producing a final answer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every LLM-as-a-judge evaluation metric should only evaluate a single qualitative
    aspect. Focusing on a single aspect makes the evaluation task easier for the LLM
    to interpret. If you need to assess multiple aspects, create multiple LLM-as-a-judge
    evaluation metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM you use matters. Different LLMs can produce different outcomes on the
    same evaluation task. Be consistent in using the same LLM for all evaluations
    with a metric. If you change the LLM used by a metric, you cannot reliably compare
    the results produced with different LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Produce structured evaluation output. The judge LLM should produce structured
    outputs, such as pass or fail, or a score of integers 0-5\. You can then normalize
    these scores. For instance, if the judge LLM outputs `pass` or `fail`, then `pass`
    is normalized as 1 and `fail` as 0\. If the judge LLM outputs integers `0`-`5`,
    `0` is normalized as 0, `1` as 0.2, `2` as 0.4... and `5` as 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following code example uses an LLM as a judge to evaluate whether the travel
    assistant chatbot provides a suggestion to the user in its response. The LLM evaluator
    also includes few-shot examples to improve the judge model’s understanding of
    the task.
  prefs: []
  type: TYPE_NORMAL
- en: The code example runs the evaluation over a dataset of inputs and outputs. Note
    that this is a reference-free evaluation, as the LLM-as-a-judge does not need
    a reference answer to determine whether the chatbot provides irrelevant answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must install a few Python packages. The `prettytable` package output
    results in a readable format and the `openai` package calls the OpenAI API to
    use the GPT-4o LLM. Install the packages in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, execute the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs the following to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding example demonstrates how to create a simple LLM-as-a-judge metric
    to evaluate whether a response includes a recommendation. You can extend the techniques
    to create additional LLM-as-a-judge metrics to look at various aspects of your
    LLM system. In the next section, you will learn about some more complex LLM-as-a-judge
    metrics for evaluating RAG systems.
  prefs: []
  type: TYPE_NORMAL
- en: RAG metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: RAG is currently one of the most popular ways to use LLMs. A distinct set of
    metrics has emerged to measure the efficacy of a RAG system. These metrics all
    use an LLM as a judge.
  prefs: []
  type: TYPE_NORMAL
- en: 'These metrics focus on the two core components of any RAG system, retrieval
    and generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Retrieval**: This component fetches relevant information from external sources.
    It often combines vector search with LLM-based pre- and post-processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generation**: This component uses an LLM to produce text outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following LLM-as-a-judge metrics are often used to evaluate RAG systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Answer faithfulness**: Measures how grounded the generated response is to
    the retrieved context information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Answer relevance**: Measures how relevant the generated response is to the
    provided input'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ragas** is a popular Python library that includes modules implementing these
    metrics along with others for RAG evaluation. In the remainder of this section,
    you will learn how Ragas implements these metrics. To learn more about Ragas and
    its available metrics, refer to its documentation ([https://docs.ragas.io/en/stable/index.html](https://docs.ragas.io/en/stable/index.html)).'
  prefs: []
  type: TYPE_NORMAL
- en: Answer faithfulness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Answer faithfulness is an evaluation metric for the generation component of
    RAG systems. It measures the extent to which the information in the generated
    response aligns with the retrieved context information.
  prefs: []
  type: TYPE_NORMAL
- en: By identifying factual discrepancies between the generated answer and the retrieved
    context, the answer faithfulness metric can help identify if there are any hallucinations
    in the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ragas includes a module to measure faithfulness. It calculates faithfulness
    with this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22495_09_Equation.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The data to input into the faithfulness formula is derived with these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract all claims from the generated response with an LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Locate each claim in the reference material with an LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the proportion of claims that can be inferred from the context information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following code example uses the Ragas faithfulness metric on an example
    set of input, contexts, and RAG system outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must install a few Python packages. The `ragas` package includes
    the response faithfulness metric and a reporting module. The `langchain-openai`
    package lets you pass an OpenAI model to Ragas. This example uses the GPT-4o mini
    model. Ragas also depends on the `datasets` package to format inputs. Install
    the packages in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following code to perform the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this code outputs results resembling the following to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the results that the Ragas evaluator deemed the first and third
    examples faithful, and not the second one.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following section, you will learn how to use another RAG evaluation
    metric: answer relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: Answer relevance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Answer relevance measures how relevant the output of a RAG system is to the
    input. This metric is useful because it determines how well a RAG system responds
    to the provided input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ragas uses the input, generated output, and context information retrieved to
    generate that output in its answer relevance metric. It calculates the answer
    relevance evaluation metric score with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use an LLM to generate a list of questions from the generated response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a vector embedding for each LLM-generated question from the previous
    step. Also, create a vector embedding for the initial input query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the cosine similarity between the original question embedding and
    each generated question embedding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The answer relevance score is the mean of the cosine similarities between the
    original question and each generated question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ragas assumes that if the generated answer is highly relevant to the original
    question, then the questions that can be derived from this answer should be semantically
    similar to the original question. This assumption is based on the idea that a
    relevant answer contains information that directly addresses the query. Therefore,
    the judge LLM should be able to *reverse-engineer* questions that closely align
    with the original input.
  prefs: []
  type: TYPE_NORMAL
- en: The following code example uses the Ragas answer relevance metric on an example
    set of input, contexts, and RAG system outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must install a few Python packages. Note that these are the same
    dependencies as for the Ragas faithfulness evaluation example in the previous
    section. The `ragas` package includes the response answer relevance metric and
    a reporting module. The `langchain-openai` package lets you pass an OpenAI model
    to Ragas. This example uses the GPT-4o mini model. Ragas also depends on the `datasets`
    package to format inputs. Install the packages in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following code to perform the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing this code outputs the following results to the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You can see from the results that the first and third cases were relevant, while
    the second was not. This makes sense because the first and third had quite relevant
    contexts, whereas the second had no context information at all.
  prefs: []
  type: TYPE_NORMAL
- en: The Ragas answer relevance metric has noteworthy limitations. The quality of
    the underlying language model significantly impacts the metric’s effectiveness,
    as it relies heavily on the LLM’s capacity to generate appropriate questions from
    the given answer. The metric may also struggle with handling complex or multi-faceted
    queries, particularly when the answer doesn’t comprehensively address all aspects
    of the original question, potentially resulting in an incomplete assessment of
    relevance for more intricate topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other approaches that you can take to evaluate answer relevance.
    For instance, the **DeepEval** evaluation framework calculates answer relevancy
    with the following strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract all statements in an output with an LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the same LLM to determine which statements are relevant to the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate answer relevance as the number of relevant statements divided by the
    total number of statements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The difference between the Ragas and DeepEval strategies to calculating the
    answer relevancy metric demonstrates that the AI engineering field is still converging
    on how to calculate these metrics, even if it is becoming standard to evaluate
    based on some form of these metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Using the RAG evaluation metrics discussed in this section, you can measure
    how well your RAG system is performing and measure improvement in the system over
    time. You can also experiment with other RAG metrics in frameworks such as Ragas
    or DeepEval.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how you can perform a manual human review
    of your data to augment the automated evaluation metrics discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Human review
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While LLMs can be effective tools for qualitative evaluation, they are often
    inferior to the original form of non-artificial intelligence: humans. **Human
    review** is considered the gold standard of qualitative review.'
  prefs: []
  type: TYPE_NORMAL
- en: When using human review, you should take into account that humans likely prefer
    simpler rating metrics that do not require them to do complex multi-step calculations,
    such as the answer relevance metric described earlier in this chapter. Instead,
    give human reviewers simple rating systems. Pass/fail criteria are the simplest
    and can be normalized to 0 or 1\. You can also use a rating system such as 0-5,
    which can be normalized to 0, 0.2, and so on until 1.
  prefs: []
  type: TYPE_NORMAL
- en: Human reviewer free-form feedback can be particularly valuable, as this open-ended
    feedback can provide insight that would not be captured by the rating metric alone.
  prefs: []
  type: TYPE_NORMAL
- en: It is also useful to capture who the human reviewer is for an evaluation. You
    can use this to follow up with the person if need be or to track how some individuals
    perform ratings as compared to others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the qualitative advantage of human review, it also comes with its own
    set of limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost**: Human reviewers tend to be more expensive than using an LLM as a
    judge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time**: Human reviewers usually take much longer than using an LLM as a judge.
    You also cannot parallelize a single human like you can an AI model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tedium**: Evaluating the output of LLMs can be an incredibly tedious task
    for human reviewers. Many people do not want to perform evaluations, so it can
    be difficult to find people to consistently perform the evaluations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Elasticity**: Often, you need to run large numbers of evaluations as part
    of your software development process or at regular intervals. It can be hard to
    find human reviewers to perform an evaluation exactly when you need them to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inconsistency**: Human reviewers can be inconsistent in their evaluation.
    Different people might evaluate the same case in different ways. The same person
    could even evaluate the same case differently at a different moment, depending
    on factors such as tiredness, mood, and environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the strengths and weaknesses of using humans as reviewers, you must carefully
    consider when to use human review. Human review is probably the most useful for
    conducting initial qualitative evaluation. Human reviewers can set a baseline
    for application performance that you can measure against with a reasonably high
    degree of confidence.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use human reviews as a baseline to measure LLM-as-a-judge metrics
    against. You can try to get the LLM-as-a-judge metric to conform as closely as
    possible to the human review results.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, your LLM-as-a-judge metric can use examples from the human review
    in its prompt to demonstrate to the LLM what the classification should look like
    as a form of multi-shot prompting. Multi-shot prompting has been shown to increase
    model performance meaningfully.
  prefs: []
  type: TYPE_NORMAL
- en: Human review is one of the most effective means of qualitative evaluation, if
    also a slow and expensive one.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations as guardrails
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **guardrail** is a mechanism that prevents the AI from producing an undesirable
    or incorrect output. Guardrails ensure that generated responses are within acceptable
    boundaries and align with your application’s quality, ethical, and relevance standards.
  prefs: []
  type: TYPE_NORMAL
- en: Previously in this chapter, you learned about **reference-free evaluations**.
    These are evaluations that only require an input without a reference output or
    golden answer. You can also use reference-free evaluations as guardrails to help
    ensure the AI system performs correctly. For example, in the *RAG metrics* section,
    you looked at the answer relevance metric. You could use this as a guardrail in
    the travel assistant chatbot to ensure that the chatbot only responds with answers
    that meet a certain relevancy threshold. If the answer doesn’t meet this threshold,
    you could perform some additional application logic before responding to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, you have learned how to use evaluations to improve
    the quality of your intelligent application. Using reference-free evaluations
    as guardrails lets you extend the utility of your evaluations to a component of
    the application itself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you explored methods for evaluating LLM outputs in your intelligent
    application. You learned what LLM evaluation is and why it’s important for your
    intelligent application. Model benchmarking is a form of evaluation that can help
    you determine which LLMs to use in your application.
  prefs: []
  type: TYPE_NORMAL
- en: Once your application has functional AI modules, you can make evaluation datasets
    and run metrics on them to measure performance and change over time. In addition
    to the automated evaluations, you can perform manual human review to further measure
    application quality. Finally, you can use reference-free metrics as guardrails
    within your application.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to optimize the semantic data model
    to enhance retrieval accuracy and overall performance.
  prefs: []
  type: TYPE_NORMAL
