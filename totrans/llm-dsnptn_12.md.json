["```py\nimport torch\nimport torch.nn.utils.prune as prune\n# Assume model is an instance of a pre-trained LLM\nmodel = ...  # Load or define your LLM model\n# Prune 30% of the lowest magnitude weights in all Linear layers\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear):\n        prune.l1_unstructured(module, name='weight', amount=0.3)\n# Remove the pruning reparameterization\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear):\n        prune.remove(module, 'weight')\n```", "```py\n    def prune.l1_unstructured(module, name, amount):\n    \"\"\"Prunes weights with lowest L1 norm magnitude in a module's tensor\"\"\"\n    # Get the parameter to prune\n    tensor = getattr(module, name)\n    # Calculate number of parameters to prune\n    n_params_to_prune = int(amount * tensor.numel())\n    # Get magnitude threshold (kth smallest absolute value)\n    threshold = torch.kthvalue(\n        tensor.abs().view(-1), n_params_to_prune\n    ).values\n    # Create and apply mask (zeros out weights below threshold)\n    mask = tensor.abs() > threshold\n    pruned_tensor = tensor.clone() * mask\n    # Update parameter and register mask\n    setattr(module, name, torch.nn.Parameter(pruned_tensor))\n    module.register_buffer(f'{name}_mask', mask)\n    # Add hook to maintain pruning during updates\n    module.register_forward_pre_hook(\n        lambda m, _: setattr(\n            m, name,\n            torch.nn.Parameter(\n                getattr(m, name) * getattr(m, f'{name}_mask')\n            )\n        )\n    )\n    return mask\n```", "```py\nimport torch.nn.utils.prune as prune\n# Structured pruning of entire neurons in a layer\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear):\n        prune.ln_structured(\n            module, name='weight', amount=0.3, n=2, dim=0\n        )\n```", "```py\n# Iteratively prune 10% of the model after every 10 epochs\nfor epoch in range(1, num_epochs+1):\n    train(model, train_loader, optimizer)  # Regular training step\n    if epoch % 10 == 0:\n        for name, module in model.named_modules():\n            if isinstance(module, torch.nn.Linear):\n                prune.l1_unstructured(module, name='weight',\n                    amount=0.1)\n                prune.remove(module, 'weight')  # Remove pruning mask after each step\n    validate(model, val_loader)\n```", "```py\n    import torch\n    import torch.nn.utils.prune as prune\n    # Assuming model is a pre-trained LLM\n    model = ...  # Load or define your LLM model\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    criterion = torch.nn.CrossEntropyLoss()\n    def train(model, train_loader, optimizer):\n        model.train()\n        for batch in train_loader:\n            inputs, targets = batch\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n    # Prune 20% of the weights every 5 epochs during training\n    for epoch in range(1, 20):\n        train(model, train_loader, optimizer)\n        # Apply pruning every 5 epochs\n        if epoch % 5 == 0:\n            for name, module in model.named_modules():\n                if isinstance(module, torch.nn.Linear):\n                    prune.l1_unstructured(module, name='weight',\n                        amount=0.2)\n                    prune.remove(module, 'weight')  # Remove reparameterization after each pruning\n    ```", "```py\n    # Assuming the model has already been fully trained\n    model = ...  # Load or define your trained LLM model\n    # Prune 30% of the weights in all Linear layers after training\n    for name, module in model.named_modules():\n        if isinstance(module, torch.nn.Linear):\n            prune.l1_unstructured(module, name='weight', amount=0.3)\n    # Optionally, fine-tune the model after pruning\n    fine_tune_epochs = 3\n    for epoch in range(fine_tune_epochs):\n        train(model, train_loader, optimizer)  # Fine-tuning the pruned model\n    ```", "```py\nimport torch.nn.utils.prune as prune\n# Assuming model has been trained and pruned\nmodel = ...  # Pruned LLM model\n# Apply fine-tuning to restore performance after pruning\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Lower learning rate for fine-tuning\nfine_tune_epochs = 5\nfor epoch in range(fine_tune_epochs):\n    train(model, train_loader, optimizer)  # Reuse the train function from earlier\n    validate(model, val_loader)  # Validation step to monitor performance\n```", "```py\nimport torch\nimport torch.nn.utils.prune as prune\nimport torch.quantization as quant\n# Prune the model first\nmodel = ...  # Pre-trained LLM\nfor name, module in model.named_modules():\n    if isinstance(module, torch.nn.Linear):\n        prune.l1_unstructured(module, name='weight', amount=0.4)\n        prune.remove(module, 'weight')\n# Apply dynamic quantization after pruning\nquantized_model = quant.quantize_dynamic(\n    model, {torch.nn.Linear}, dtype=torch.qint8\n)\n# Check the size reduction\nprint(\"Original model size:\", torch.cuda.memory_allocated())\nprint(\"Quantized model size:\", torch.cuda.memory_allocated())\n```", "```py\n# Teacher and student models for knowledge distillation\nteacher_model = ...  # Larger, fully trained model\nstudent_model = ...  # Smaller model to be distilled and pruned\ndef distillation_loss(student_outputs, teacher_outputs, temperature):\n    return torch.nn.KLDivLoss()(\n        torch.nn.functional.log_softmax(\n            student_outputs / temperature\n        ),\n        torch.nn.functional.softmax(\n            teacher_outputs / temperature\n        )\n    )\n# Train the smaller, pruned model using knowledge distillation\ntemperature = 2.0\noptimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)\nfor batch in train_loader:\n    inputs, _ = batch\n    teacher_outputs = teacher_model(inputs)\n    student_outputs = student_model(inputs)\n    loss = distillation_loss(\n        student_outputs, teacher_outputs, temperature\n    )\n    loss.backward()\n    optimizer.step()\n```"]