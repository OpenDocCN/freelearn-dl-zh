- en: Decision Trees and Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees and random forests are powerful techniques that you can use to
    add power to your applications. Let's walk through some concepts and some code
    and hopefully have you up and running in no time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to learn about decision trees and random forests.
    We will:'
  prefs: []
  type: TYPE_NORMAL
- en: Work through a lot of code samples to show you how you can add this powerful
    functionality to your applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discuss random forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be required to have Microsoft Visual Studio installed in your system.
    You may also need to refer to open source SharpLearning framework's GitHub repository
    at [https://github.com/mdabros/SharpLearning](https://github.com/mdabros/SharpLearning).
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see Code in Action: [http://bit.ly/2O1Lbhr](http://bit.ly/2O1Lbhr).
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees can be used for both classification and regression. Decision
    trees answer sequential questions with a yes/no, true/false response. Based upon
    those responses, the tree follows predetermined paths to reach its goal. Trees
    are more formally a version of what is known as a directed acyclic graph. Finally,
    a decision tree is built using the entire dataset and all features.
  prefs: []
  type: TYPE_NORMAL
- en: Here is an example of a decision tree. You may not know it as a decision tree,
    but for sure you know the process. Anyone for a doughnut?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a372cb1a-d005-4c00-b232-ada277d070fa.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the flow of a decision tree starts at the top and works its
    way downward until a specific result is achieved. The root of the tree is the
    first decision that splits the dataset. The tree recursively splits the dataset
    according to what is known as the **splitting metric** at each node. Two of the
    most popular metrics are **Gini Impurity** and **Information Gain**.
  prefs: []
  type: TYPE_NORMAL
- en: Here is another depiction of a decision tree, albeit without the great donuts!
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee594bda-9688-4c4f-976b-acbd2cce21cc.png)'
  prefs: []
  type: TYPE_IMG
- en: The *depth* of a decision tree represents how many questions have been asked
    so far. This is the deepest that the tree can go (the total number of questions
    that may be asked), even if some results can be achieved using fewer questions.
    For instance, using the preceding diagram, some results can be obtained after
    1 question, some after 2\. Therefore, the *depth* of that decision tree would
    be 2.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some advantages from using decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy interpretation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Straightforward, self-explanatory visualizations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be easily reproduced.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can handle both numeric and categorical data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform well on very large datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normally are very fast.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth-wise, the location of the tree allows easy visualization of which features
    are important. The importance is denoted by the depth of the tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree disadvantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some disadvantages to using decision trees:'
  prefs: []
  type: TYPE_NORMAL
- en: At each node, the algorithm needs to determine the correct choice. The best
    choice at one node may not necessarily be the best choice for the entire tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a tree is deep, it can be prone to what is known as overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees can memorize the training set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When should we use a decision tree?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some examples of when to use a decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: When you want a simple and explainable model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When your model should be non-parametric
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you don’t want to worry about feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have talked about decision trees, and now it’s time to discuss random forests.
    Very basically, a random forest is a collection of decision trees. In random forests,
    a fraction of the number of total rows and features are selected at random to
    train on. A decision tree is then built upon this subset. This collection will
    then have the results aggregated into a single result.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests can also reduce bias and variance. How do they do this? By training
    on different data samples, or by using a random subset of features. Let’s take
    an example. Let’s say we have 30 features. A random forest might only use 10 of
    these features. That leaves 20 features unused, but some of those 20 features
    might be important. Remember that a random forest is a collection of decision
    trees. Therefore, in each tree, if we utilize 10 features, over time most if not
    all of our features would have been included anyway simply because of the law
    of averages. So, it is this inclusion that helps limit our error due to bias and
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: For large datasets, the number of trees can grow quite large, sometimes into
    the tens of thousands and more, depending on the number of features you are using,
    so you need to be careful regarding performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a diagram of what a random forest might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fef193f7-c28b-492e-838e-7352b2518c59.png)'
  prefs: []
  type: TYPE_IMG
- en: Random forest advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some advantages from using random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: More robust than just a single decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests contain many decision trees and are therefore able to limit overfitting
    and error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depth-wise, the location shows which features contribute to the classification
    or regression as well as their relative importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can be used for both regression and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Default parameters can be sufficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast to train
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest disadvantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some disadvantages to using random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: Random forests need to be done in parallel in order to increase speed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions can be slow to create once trained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More accuracy requires more trees, and this can result in a slower model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When should we use a random forest?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are some examples of when to use random forests:'
  prefs: []
  type: TYPE_NORMAL
- en: When model interpretation is not the most important criterion. Interpretation
    will not be as easy as a single tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When model accuracy is most important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want robust classification, regression, and feature selection analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To prevent overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image classification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation engines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SharpLearning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s now turn our attention to an incredible open source package, SharpLearning.
    **SharpLearning** is an excellent machine learning framework for individuals to
    learn about many aspects of machine learning, including decision trees and random
    forests as we described in the preceding sections. Let’s spend a few minutes getting
    familiar with a few things before we dive into some code samples and example applications.
  prefs: []
  type: TYPE_NORMAL
- en: Terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this chapter you are going to see the following terms used. Here
    is the context for what each of them means:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Learner**: This refers to a machine learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: This refers to a machine learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyper-parameters**: These are the parameters used to adjust and regulate
    (hopefully) the machine learning model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Targets**: These are more commonly referred to as a dependent variable. In
    most notations, this will be *y*. These are the values that we are attempting
    to model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observations**: These are the feature matrix, which contains all the information
    we currently have about the targets. In most notations, this will be *x*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Throughout most of our examples we will be focusing on two namespaces within
    SharpLearning. They are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SharpLearning.DecisionTrees`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SharpLearning.RandomForest`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that behind us, let’s start digging into SharpLearning and show you a few
    concepts relative to how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and saving models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SharpLearning makes it very easy to load and save models to disk. This is a
    very important part of a machine learning library and SharpLearning is among the
    easiest to implement.
  prefs: []
  type: TYPE_NORMAL
- en: All models in SharpLearning have a `Save` and a `Load` method. These methods
    do the heavy lifting of saving and loading a model for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, here we will save a model that we learned to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to load this model back in, we simply use the `Load` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Yep, it’s that easy and simple to load and save your data models. It is also
    possible for you to save models using serialization. This will allow us to choose
    between XML and a Binary format. Another very nice design feature of SharpLearning
    is that serializing models allows us to serialize to the `IPredictorModel` interface.
    This makes replacing your models much easier, if each conforms to that interface.
    Here’s how we would do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '| **Algorithm** | **Train Error** | **Test Error** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RegressionDecisionTreeLearner(default) | 0.0518 | 0.4037 |'
  prefs: []
  type: TYPE_TB
- en: And there you have it, instant training and testing errors.
  prefs: []
  type: TYPE_NORMAL
- en: When reporting the performance of your model, you should always use the test
    error even if the training error is lower, since that is an estimate of how well
    the model generalizes to new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s talk for a second about **hyperparameters**. Hyperparameters are
    parameters that affect the learning process of the machine learning algorithm.
    You can adjust them to tune the process and improve performance and reliability.
    At the same time, you can also incorrectly adjust parameters and have something
    that does not work as intended. Let''s look at a few things that can happen with
    an incorrectly tuned hyperparameter:'
  prefs: []
  type: TYPE_NORMAL
- en: If the model is too complex, you can end up with what is known as high variance,
    or **Overfitting**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the model ends up being too simple, you will end up with what is known as
    high bias, or **Underfitting**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For those who have not done so, manually tuning hyperparameters, a process
    that happens in almost every use case, can take a considerable amount of your
    time. As the number of hyperparameters increases with the model, the tuning time
    and effort increase as well. The best way around this is to use an optimizer and
    let the work happen for you. To this end, SharpLearning can be a huge help to
    us due to the numerous optimizers that are available for it. Here is a list of
    just some of them:'
  prefs: []
  type: TYPE_NORMAL
- en: Grid search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Particle swarm (which we will talk about in [Chapter 7](34e7d6b8-85b9-42ff-865b-86f158138320.xhtml),
    *Replacing Back Propagation with PSO*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Globalized bounded nelder mead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a learner and use the default parameters, which, more than likely,
    will be good enough. Once we find our parameters and create the learner, we need
    to create the model. We then will predict the training and test set. Once all
    of that is complete, we will measure the error on the test set and record it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: And here is our test set error
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Test Error** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RegressionSquareLossGradientBoostLearner (default) | 0.4984 |'
  prefs: []
  type: TYPE_TB
- en: 'With that part complete, we now have our baseline established. Let’s use a
    `RandomSearchOptimizer` to tune the hyperparameters to see if we can get any better
    results. To do this we need to establish the bounds of the hyperparameters, so
    our optimizer knows how to tune. Let’s look at how we do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Did you notice that we used a Logarithmic transform for the learning rate?
    Do you know why we did this? The answer is: to ensure that we had a more even
    distribution across the entire range of values. We have a large range difference
    between our minimum and maximum values (0.02 -> 0.2), so the logarithmic transform
    will be best.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need a *validation set* to help us measure how well the model generalizes
    to unseen data during our optimization. To do this, we will need to further split
    the training data. To do this, we are going to leave our current test set out
    of the optimization process. If we don’t, we risk getting a positive bias on our
    final error estimate, and that will not be what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'One more thing that the optimizer will need is an objective function. The function
    will take a double array as input (containing the set of hyperparameters) and
    return an `OptimizerResult` that contains the validation error and the corresponding
    set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this `objective` function has been defined, we can now create and run
    the optimizer to find the best set of parameters. Let’s start out by running our
    optimizer for 30 iterations and trying out 30 different sets of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run this, our optimizer should find the best set of hyperparameters.
    Let’s see what it finds:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Trees`: 277'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learningRate`: 0.035'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maximumTreeDepth`: 15'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subSampleRatio`: 0.838'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`featuresPrSplit`: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Progress. Now that we have a set of best hyperparameters, which were measured
    on our validation set, we can create a learner with these parameters and learn
    a new model using the entire dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With our final set of hyperparameters now intact, we pass these to our learner
    and are able to reduce the test error significantly. For us to do that manually
    would have taken us an eternity and beyond!
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Test Error** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RegressionSquareLossGradientBoostLearner (default) | 0.4984 |'
  prefs: []
  type: TYPE_TB
- en: '| RegressionSquareLossGradientBoostLearner (Optimizer) | 0.3852 |'
  prefs: []
  type: TYPE_TB
- en: Example code and applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the next few sections, we are going to look at some code samples without
    all the verbosity. This will be pure C# code so it should be something easily
    understood by all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a quick look at how we can use SharpLearning to predict observations.
    I’ll show you an entire code sample without the verbosity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Saving a model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is a code sample that will show you how easy it is to save a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Mean squared error regression metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mean squared error** (**MSE**) is a metric that measures the average of the
    squares of the errors. More concretely, it measures the average distance between
    the estimated values and what is estimated. A mean squared error is always non-negative,
    and values that are closer to zero are considered more acceptable. SharpLearning
    makes it incredibly easy to calculate this error metric, as depicted in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: F1 score
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To talk about an f1 score we must first talk about *Precision* and *Recall*.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** is the ratio of correctly predicted positive observations divided
    by the total predicted positive observations. Less formally, of all the people
    that said they were coming, how many came?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** (sensitivity) is the ratio of correctly predicted positive observations
    to all observations in total.'
  prefs: []
  type: TYPE_NORMAL
- en: '**F1 score** is then the weighted average of Precision and Recall.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how we calculate an f1 score using SharpLearning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Optimizations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is how you can use the *Particle Swarm Optimizer* to return the
    result that best minimizes the provided function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is how you can use the *Grid Search Optimizer* to optimize by
    trying all combinations of the provided parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is how you can use the *Random Search Optimizer* to initialize
    random parameters between the min and max provided. The result that best minimizes
    the provided function will be returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Sample application 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With all this knowledge under our belt, let’s go ahead and write our first
    sample program. The program itself is very simple and meant to show how easy it
    is to implement such techniques in your applications with a minimal amount of
    code. To show you exactly what I mean, here is what the output of the program
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a5da7152-6cf1-4d74-a8e3-939463ceb358.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the code that implements our sample program and produces the previous
    output. As you can see, the code is very simple and everything in here (save the
    shuffling of indices) is code we have already walked through before. We’ll keep
    the verbosity to a minimum so that you can concentrate on the code itself. This
    sample will read in our data, parse it into observations and target samples, and
    then create a learner using 1,000 trees. From there we will use the learner to
    learn and create our model. Once this is complete we will calculate our mean squared
    error metric and display it on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Sample application 2 – wine quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our next application, we are going to use our knowledge to determine the
    most important features for wine based upon the model that is created. Here is
    what our output will look like when we complete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe7ac6eb-9a76-4d5c-b02a-ad46e51e49a0.png)'
  prefs: []
  type: TYPE_IMG
- en: The code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here is the code for our application. As always, we first load and parse our
    data into observations and target sample sets. Since this is a regression sample,
    we’ll use a 70/30 split of our data sample: 70% for training, 30% for testing.
    From there, we create our random forest learner and create our model. After this,
    we calculate our training and test errors and print out the feature importance
    in order of importance, as found by our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about decision trees and random forests. We also
    learned how to use the open source framework, SharpLearn, to add these powerful
    features to our applications. In the next chapter, we are going to learn about
    facial and motion detection and show you how you can enable your application with
    this exciting technology! You’ll meet Frenchie, my pet French Bulldog, who will
    demonstrate most of the samples we will show. Also, we have a guest poser you
    will just have to see!
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://github.com/mdabros/SharpLearning](https://github.com/mdabros/SharpLearning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
