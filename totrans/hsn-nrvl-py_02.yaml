- en: Overview of Neuroevolution Methods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经进化方法概述
- en: The concept of **artificial neural networks** (**ANN**) was inspired by the
    structure of the human brain. There was a strong belief that, if we were able
    to imitate this intricate structure in a very similar way, we would be able to
    create artificial intelligence. We are still on the road to achieving this. Although
    we can implement Narrow AI agents, we are still far from creating a Generic AI
    agent.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）的概念灵感来源于人脑的结构。人们坚信，如果我们能够以非常相似的方式模仿这种复杂的结构，我们就能创造出人工智能。我们仍在通往这一目标的道路上。尽管我们可以实现窄AI代理，但我们离创建通用AI代理还远着呢。'
- en: 'This chapter introduces you to the concept of ANNs and the two methods that
    we can use to train them (the gradient descent with error backpropagation and neuroevolution)
    so that they learn how to approximate the objective function. However, we will
    mainly focus on discussing the neuroevolution-based family of algorithms. You
    will learn about the implementation of the evolutionary process that''s inspired
    by natural evolution and become familiar with the most popular neuroevolution
    algorithms: NEAT, HyperNEAT, and ES-HyperNEAT. We will also discuss the methods
    of optimization that we can use to search for final solutions and make a comparison
    between objective-based search and Novelty Search algorithms. By the end of this
    chapter, you will have a complete understanding of the internals of neuroevolution
    algorithms and be ready to apply this knowledge in practice.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您介绍了人工神经网络的概念以及我们可以用来训练它们的两种方法（带有误差反向传播的梯度下降和神经进化），以便它们学会如何逼近目标函数。然而，我们将主要关注讨论基于神经进化的算法系列。您将了解受自然进化启发的进化过程的实现，并熟悉最流行的神经进化算法：NEAT、HyperNEAT和ES-HyperNEAT。我们还将讨论我们可以用来搜索最终解决方案的优化方法，并在基于目标搜索和新颖性搜索算法之间进行比较。到本章结束时，您将对神经进化算法的内部结构有一个完整的理解，并准备好将此知识应用于实践。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Evolutionary algorithms and neuroevolution-based methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化算法和基于神经进化的方法
- en: NEAT algorithm overview
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NEAT算法概述
- en: Hypercube-based NEAT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于超立方体的NEAT
- en: Evolvable-Substrate HyperNEAT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可进化基座HyperNEAT
- en: Novelty Search optimization method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新颖性搜索优化方法
- en: Evolutionary algorithms and neuroevolution-based methods
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化算法和基于神经进化的方法
- en: The term artificial neural networks stands for a graph of nodes connected by
    links where each of the links has a particular weight. The neural node defines
    a kind of threshold operator that allows the signal to pass only after a specific
    activation function has been applied. It remotely resembles the way in which neurons
    in the brain are organized. Typically, the ANN training process consists of selecting
    the appropriate weight values for all the links within the network. Thus, ANN
    can approximate any function and can be considered as a universal approximator,
    which is established by the Universal Approximation Theorem.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络这一术语代表由链接连接的节点图，其中每个链接都有一个特定的权重。神经网络节点定义了一种阈值运算符，它允许信号在应用特定的激活函数之后才通过。它远程地类似于大脑中神经元的组织方式。通常，ANN的训练过程包括选择网络中所有链接的适当权重值。因此，ANN可以逼近任何函数，可以被认为是通用逼近器，这是由通用逼近定理所确立的。
- en: 'For more information on the proof of the Universal Approximation Theorem, take
    a look at the following papers:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于通用逼近定理证明的信息，请参阅以下论文：
- en: Cybenko, G. (1989)* Approximations by Superpositions of Sigmoidal Functions,*
    Mathematics of Control, Signals, and Systems, 2(4), 303–314.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cybenko, G. (1989)*通过Sigmoid函数的叠加逼近*，《控制、信号与系统数学》，2(4)，303–314。
- en: 'Leshno, Moshe; Lin, Vladimir Ya.; Pinkus, Allan; Schocken, Shimon (January
    1993). *Multilayer feedforward networks with a nonpolynomial activation function
    can approximate any function*. Neural Networks. 6 (6): 861–867\. doi:10.1016/S0893-6080(05)80131-5\.
    ([https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315?via%3Dihub))'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leshno, Moshe; Lin, Vladimir Ya.; Pinkus, Allan; Schocken, Shimon (1993年1月)。*多层前馈网络具有非多项式激活函数可以逼近任何函数*。神经网络。6（6）：861–867。doi:10.1016/S0893-6080(05)80131-5。（[https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315?via%3Dihub)）
- en: Kurt Hornik (1991) *Approximation Capabilities of Multilayer Feedforward Networks*,
    Neural Networks, 4(2), 251–257\. doi:10.1016/0893-6080(91)90009-T ([https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub))
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurt Hornik (1991) *多层前馈网络的逼近能力*, 神经网络，4(2), 251–257\. doi:10.1016/0893-6080(91)90009-T
    ([https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub))
- en: Hanin, B. (2018). *Approximating Continuous Functions by ReLU Nets of Minimal
    Width*. arXiv preprint arXiv:1710.11278\. ([https://arxiv.org/abs/1710.11278](https://arxiv.org/abs/1710.11278))
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hanin, B. (2018). *通过最小宽度的ReLU网络逼近连续函数*. arXiv预印本 arXiv:1710.11278\. ([https://arxiv.org/abs/1710.11278](https://arxiv.org/abs/1710.11278))
- en: Over the past 70 years, many ANN training methods have been proposed. However,
    the most popular technique that gained fame in this decade was proposed by Jeffrey
    Hinton. It is based on the backpropagation of prediction error through the network,
    with various optimization techniques built around the gradient descent of the
    loss function with respect to connection weights between the network nodes. It
    demonstrates the outstanding performance of training deep neural networks for
    tasks related mainly to pattern recognition. However, despite its inherent powers,
    it has significant drawbacks. One of these drawbacks is that a vast amount of
    training samples are required to learn something useful from a specific dataset.
    Another significant disadvantage is the fixed network architecture that's created
    manually by the experimenter, which results in inefficient use of computational
    resources. This is due to a significant amount of network nodes not participating
    in the inference process. Also, backpropagation-based methods have problems with
    transferring the acquired knowledge to other similar domains.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的70年里，提出了许多人工神经网络训练方法。然而，在本十年中获得声誉的最流行技术是由Jeffrey Hinton提出的。它基于通过网络反向传播预测误差，并在网络节点之间的连接权重上围绕损失函数相对于梯度下降的各种优化技术构建。它展示了训练深度神经网络在主要与模式识别相关的任务上的卓越性能。然而，尽管它具有内在的力量，但它有显著的缺点。其中一个缺点是需要大量的训练样本才能从特定的数据集中学习到有用的东西。另一个显著的缺点是实验者手动创建的固定网络架构，这导致计算资源的低效使用。这是由于大量的网络节点没有参与推理过程。此外，基于反向传播的方法在将获得的知识转移到其他类似领域时存在问题。
- en: Alongside backpropagation methods, there are very promising evolutionary algorithms
    that can address the aforementioned problems. These bio-inspired techniques draw
    inspiration from Darwin's theory of evolution and use natural evolution abstractions
    to create artificial neural networks. The basic idea behind neuroevolution is
    to produce the ANNs by using stochastic, population-based search methods. It is
    possible to evolve optimal architectures of neural networks, which accurately
    address the specific tasks using the evolutionary process. As a result, compact
    and energy-efficient networks with moderate computing power requirements can be
    created. The evolutionary process is executed by applying genetic operators (*mutation*,
    *crossover*) to the population of chromosomes (genetically encoded representations
    of ANNs/solutions) over many generations. The central belief is that since this
    is in biological systems, subsequent generations will be suited to withstand the
    generational pressure that's expressed by the objective function, that is, they
    will become better approximators of the objective function.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了反向传播方法之外，还有一些非常有前途的进化算法可以解决上述问题。这些生物启发技术从达尔文的进化论中汲取灵感，并使用自然进化的抽象来创建人工神经网络。神经进化的基本思想是通过使用基于群体的随机搜索方法来产生人工神经网络。有可能通过进化过程进化出神经网络的优化架构，这些架构能够准确完成特定任务。因此，可以创建出紧凑且节能的网络，同时具有适中的计算能力需求。进化过程通过在许多代中对染色体群体（ANN/solutions的遗传编码表示）应用遗传算子（*变异*，*交叉*）来执行。核心信念是，由于这是在生物系统中，后续的代将能够承受由目标函数表达出的代际压力，也就是说，它们将成为目标函数更好的近似器。
- en: Next, we will discuss the basic concepts of genetic algorithms. You will need
    to have a moderate level of understanding of genetic algorithms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论遗传算法的基本概念。你需要对遗传算法有一个中等水平以上的理解。
- en: Genetic operators
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遗传算子
- en: 'Genetic operators are at the very heart of every evolutionary algorithm, and
    the performance of any neuroevolutionary algorithm depends on them. There are
    two major genetic operators: mutation and crossover (recombination).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算子是每个进化算法的核心，任何神经进化算法的性能都取决于它们。主要有两种遗传算子：变异和交叉（重组）。
- en: In this chapter, you will learn about the basics of genetic algorithms and how
    they differ from conventional algorithms, which use error backpropagation-based
    methods for training the ANN.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习遗传算法的基本知识以及它们与使用基于误差反向传播方法的常规算法的不同之处。
- en: Mutation operator
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变异算子
- en: The *mutation* operator serves the essential role of preserving the genetic
    diversity of the population during evolution and prevents stalling in the local
    minima when the chromosomes of organisms in a population become too similar. This
    mutation alters one or more genes in the chromosome, according to the mutation
    probability defined by the experimenter. By introducing random changes to the
    solver's chromosome, mutation allows the evolutionary process to explore new areas
    in the search space of possible solutions and find better and better solutions
    over generations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 变异算子起着在进化过程中保持种群遗传多样性的基本作用，并在种群中生物体的染色体变得过于相似时防止局部最小值停滞。这种变异根据实验者定义的变异概率改变染色体中的一个或多个基因。通过向求解器的染色体引入随机变化，变异允许进化过程在可能解的搜索空间中探索新的区域，并在代际之间找到更好和更好的解。
- en: 'The following diagram shows the common types of mutation operators:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了常见的变异算子类型：
- en: '![](img/1a5c1fb7-54b2-4d24-8878-807d27020f44.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1a5c1fb7-54b2-4d24-8878-807d27020f44.png)'
- en: Types of mutation operators
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 变异算子类型
- en: 'The exact type of mutation operator depends on the kind of genetic encoding
    that''s used by a specific genetic algorithm. Among the various mutation types
    we come across, we can distinguish the following:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 具体的变异算子类型取决于特定遗传算法使用的遗传编码类型。在遇到的多种变异类型中，我们可以区分以下几种：
- en: '**Bit inversion**: The randomly selected bit, which is inverted (*binary encoding*).'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**位反转**：随机选择的位被反转（*二进制编码*）。'
- en: '**Order change**: Two genes are randomly selected and their position is flipped
    in the genome (*permutation encoding*).'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序变化**：随机选择两个基因，并在基因组中翻转它们的位置（*排列编码*）。'
- en: '**Value change**: A small value is added to the expressed gene at a random
    position (*value encoding*).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**值变化**：在表达基因的随机位置添加一个小的值（*值编码*）。'
- en: '**Gene expression change**: A random gene is selected and added/removed from
    the genotype (*structural encoding*).'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基因表达变化**：随机选择一个基因并从基因型（*结构编码*）中添加/移除。'
- en: Genotypes can be encoded using genetic encoding schemes with fixed and variable
    chromosomal lengths. The first three mutations can be applied to both types of
    encoding schemes. The last mutation can only be expressed in genotypes that have
    been encoded using a variable-length encoding.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基因型可以使用具有固定和可变染色体长度的遗传编码方案进行编码。前三种变异可以应用于两种类型的编码方案。最后一种变异只能表达在已使用可变长度编码编码的基因型中。
- en: Crossover operator
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交叉算子
- en: The *crossover* (recombination) operator allows us to stochastically generate
    new generations (solutions) from existing populations by recombining genetic information
    from two parents to generate offspring. Thus, the portions of good solutions from
    parent organisms can be combined and can potentially lead to better offspring.
    Typically, after a crossover, the produced offspring are mutated before being
    added to the population of the next generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉（重组）算子允许我们通过重新组合两个父母的遗传信息以生成后代来随机生成新的一代（解）。因此，来自父母生物体的良好解的部分可以结合起来，并可能产生更好的后代。通常，在交叉之后，产生的后代在添加到下一代种群之前会被变异。
- en: 'The following diagram shows the various crossover operators:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了各种交叉算子：
- en: '![](img/d7d251a0-ed00-4a83-b961-b912bb4b40c7.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7d251a0-ed00-4a83-b961-b912bb4b40c7.png)'
- en: Types of crossover operators
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉算子类型
- en: 'The different types of crossover operators also depend on the genetic encoding
    that''s used by particular algorithms, but the following are the most common:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的交叉算子类型也取决于特定算法使用的遗传编码，但以下是最常见的：
- en: '**Single-point crossover**: The random crossover point is selected, the genome
    part from the beginning to the crossover point is copied to the offspring from
    one parent, and the rest are copied from another parent.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单点交叉**：随机选择交叉点，从开始到交叉点的基因组部分复制到来自一个亲本的子代，其余部分来自另一个亲本。'
- en: '**Two-point crossover**: The two crossover points are chosen randomly, the
    part of the genome from the beginning to the first point is copied from the first
    parent, the part between the first and second crossover point is copied from the
    second parent, and the rest are copied from the first parent.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**两点交叉**：两个交叉点随机选择，从第一个点到开始的部分基因组来自第一个亲本，第一个和第二个交叉点之间的部分来自第二个亲本，其余部分来自第一个亲本。'
- en: '**Uniform crossover**: The genes are copied from the first or second parent
    randomly.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均匀交叉**：基因从第一个或第二个亲本随机复制。'
- en: Genome encoding schemes
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基因组编码方案
- en: One of the most crucial choices when designing the neuroevolution algorithm
    is to determine the genetic representation of the neural network, which can be
    evolved in the following ways
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 设计神经进化算法时最重要的选择之一是确定神经网络的遗传表示，这可以通过以下方式进化
- en: Standard mutation (see the preceding *Mutation operator* subsection)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准突变（参见前面的 *突变算子* 子节）
- en: Combination operators (see the preceding *Crossover operator* subsection)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组合算子（参见前面的 *交叉算子* 子节）
- en: 'At the moment, two major schemes for genome encoding exist: direct and indirect.
    Let''s consider each schema in more detail.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，存在两种主要的基因组编码方案：直接和间接。让我们更详细地考虑每个方案。
- en: Direct genome encoding
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接基因组编码
- en: Direct genome encoding attempts were used in neuroevolution methods to create
    ANNs that were related to neural networks with a fixed topology; that is, the
    network topology was determined solely by the experimenter. Here, genetic encoding
    (*genotype*) is implemented as a vector of real numbers, representing the strength
    (*weights*) of the connections between the network nodes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经进化方法中，直接基因组编码尝试用于创建与具有固定拓扑的神经网络相关的 ANNs；也就是说，网络拓扑完全由实验者决定。在这里，遗传编码（*基因型*）实现为一个表示网络节点之间连接强度（*权重*）的实数向量。
- en: The evolutionary operators modify the values of the weights vector with the
    mutation operator and combine the vectors of the parent organisms with the recombination
    (crossover) operator to produce offspring. While allowing evolutionary operators
    to be applied with ease, the described encoding method has some significant drawbacks.
    One of its main drawbacks is that the network topology is determined by the experimenter
    from the very beginning and fixed through all the generations during the execution
    of the algorithm. This approach contradicts the natural evolutionary process,
    in which not only the properties but also the physical structure of the organisms
    change during the evolutionary process. This allows us to explore the broadest
    possible search space and find optimal solutions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 进化算子通过突变算子修改权重向量的值，并通过重组（交叉）算子结合亲本有机体的向量以产生后代。虽然允许进化算子轻松应用，但所描述的编码方法有一些显著的缺点。其主要缺点之一是网络拓扑从一开始就由实验者决定，并在算法执行的所有代中固定。这种方法与自然进化过程相矛盾，在自然进化过程中，不仅有机体的属性，而且其物理结构在进化过程中也会发生变化。这使我们能够探索最广泛的搜索空间并找到最优解。
- en: 'The following diagram shows the evolutionary process:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图显示了进化过程：
- en: '![](img/6a987ef3-6b60-49af-9a0b-38ce6b5b1eda.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6a987ef3-6b60-49af-9a0b-38ce6b5b1eda.png)'
- en: The evolutionary process
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 进化过程
- en: To address the drawbacks of the fixed topology methods, Kenneth O. Stanley proposed
    the **NeuroEvolution of Augmenting Topologies** (**NEAT**) method. The primary
    idea behind this algorithm is that the evolutionary operators are applied not
    only to the vector with the weights of all the connections but also to the topology
    of the created neural network. Thus, through generating the populations of the organisms,
    various topologies with a variety of connection weights are tested. We will discuss
    the particulars of the NEAT algorithm later in this chapter.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决固定拓扑方法的缺点，Kenneth O. Stanley 提出了 **增强拓扑的神经进化**（**NEAT**）方法。该算法背后的主要思想是，进化算子不仅应用于所有连接权重的向量，还应用于创建的神经网络拓扑。因此，通过生成有机体的种群，测试了具有各种连接权重的各种拓扑。我们将在本章后面讨论
    NEAT 算法的具体细节。
- en: The NEAT algorithm demonstrates outstanding performance in a variety of tasks
    – from traditional reinforcement learning to the control of sophisticated non-player
    characters in computer games – and has become one of the most popular neuroevolution
    algorithms ever. However, it belongs to the family of direct encoding algorithms,
    which limits its use to evolving only modest-sized ANNs, where parameter space
    is limited to a maximum of thousands of connections. This is because each connection
    is directly encoded in the genotype, and with a large number of encoded connections,
    the computational requirements increase significantly. This makes it impossible
    to use the algorithm to evolve large neural networks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT算法在各种任务中表现出卓越的性能——从传统的强化学习到控制计算机游戏中的复杂非玩家角色——并已成为最受欢迎的神经进化算法之一。然而，它属于直接编码算法的家族，这限制了其只能用于进化适度规模的ANN，其中参数空间限制在最多数千个连接。这是因为每个连接都直接编码在基因型中，随着编码连接数量的增加，计算需求显著增加。这使得无法使用该算法进化大型神经网络。
- en: Indirect genome encoding
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 间接基因组编码
- en: To overcome size issues with direct encoding, Kenneth O. Stanley proposed an
    *indirect* encoding method, which is inspired by how the phenotype is encoded
    by the genome in the DNA. It is based on the fact that the physical world is built
    around geometry and regularities (structural patterns), where natural symmetries
    are found everywhere. Thus, the encoding size of any physical process can be significantly
    reduced through the reuse of a specific set of encoding blocks for the same structure
    that repeats many times. The proposed method, called **Hypercube-based NeuroEvolution
    of Augmenting Topologies** (**HyperNEAT**), is designed to build large-scale neural
    networks by exploiting geometrical regularities. HyperNEAT employs a connective
    **Compositional Pattern Producing Network** (**CPPN**) to represent node connections
    as a function of Cartesian space. We will discuss HyperNEAT in more detail later
    in this chapter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服直接编码的大小问题，Kenneth O. Stanley提出了一个**间接**编码方法，该方法受到DNA中基因组如何编码表型的启发。它基于这样一个事实，即物理世界是围绕几何和规律性（结构模式）构建的，其中自然对称性无处不在。因此，任何物理过程的编码大小可以通过重复使用一组特定的编码块来显著减少，这些编码块用于重复多次的结构。提出的方法，称为**基于超立方体的增强拓扑神经进化**（**HyperNEAT**），旨在通过利用几何规律性来构建大规模神经网络。HyperNEAT采用一个连接的**组合模式生成网络**（**CPPN**）来表示节点连接作为笛卡尔空间中的函数。我们将在本章后面更详细地讨论HyperNEAT。
- en: Coevolution
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协同进化
- en: 'In nature, populations of different species often simultaneously evolve in
    mutual interaction with each other. This type of inter-species relationship is
    called **coevolution**. Coevolution is a powerful tool of natural evolution, and
    it is no surprise that it attracted the attention of the neuroevolution community.
    There are three main types of coevolution:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中，不同物种的种群常常在相互作用的共同进化中同时进化。这种种间关系被称为**协同进化**。协同进化是自然进化的强大工具，它吸引了神经进化社区的注意也就不足为奇了。协同进化主要有三种类型：
- en: '**Mutualism**, which is when two or more species coexist and mutually benefit
    from each other.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**互利共生**，即两种或更多物种共存并相互受益。'
- en: '**Competitive coevolution**:'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**竞争协同进化**：'
- en: '**Predation**, which is when one organism kills another and consumes its resources.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**捕食**，即一种生物杀死另一种生物并消耗其资源的行为。'
- en: '**Parasitism**, which is when one organism exploits the resources of another
    but does not kill it.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**寄生**，即一种生物利用另一种生物的资源，但并不杀死它。'
- en: '**Commensalism**, which is when the members of one species gain benefits without
    causing harm or gaining benefits from other species.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**共生**，即一种物种的成员在没有造成伤害或从其他物种中获得利益的情况下获得利益。'
- en: 'The preceding coevolution strategies were explored by researchers and their
    pros and cons were revealed. In this book, we will introduce a neuroevolution
    algorithm that employs the commensalistic principle to maintain two coevolving
    populations: the population of candidate solutions and the population of candidate
    objective functions. We will discuss the **Solution and Fitness Evolution** (**SAFE**)
    algorithm later in [Chapter 9](048be1ce-8b6a-48c7-9d13-cb34c8482eb4.xhtml), *Co-Evolution
    and the SAFE Method*.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员已经探讨了先前的协同进化策略，并揭示了它们的优缺点。在这本书中，我们将介绍一种采用共生原理的神经进化算法，以维持两个协同进化的种群：候选解决方案的种群和候选目标函数的种群。我们将在第9章[协同进化和SAFE方法](048be1ce-8b6a-48c7-9d13-cb34c8482eb4.xhtml)中讨论**解决方案和适应度进化**（**SAFE**）算法。
- en: Modularity and hierarchy
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模块化和层次结构
- en: Another crucial aspect of how natural cognitive systems are organized is modularity
    and hierarchy. While studying the human brain, neuroscientists have found that
    it is not a monolithic system with a uniform structure, but rather a complex hierarchy
    of modular structures. Also, due to the speed limitations of signal propagation
    in the biological tissues, the structure of the brain enforces the principle of
    locality when related tasks are processed by geometrically adjacent structures
    in the brain. This aspect of natural systems did not escape the attention of researchers
    of neuroevolution and they are implemented in many evolutionary algorithms. We
    will discuss how modular ANNs can be created using a neuroevolution-based algorithm
    in [Chapter 8](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml), *ES-HyperNEAT and
    the Retina Problems*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 自然认知系统组织的另一个关键方面是模块化和层次结构。在研究人脑时，神经科学家发现它不是一个具有统一结构的单一系统，而是一个复杂的模块化结构层次。此外，由于生物组织中信号传播速度的限制，大脑的结构强制执行局部性原则，当大脑中几何相邻的结构处理相关任务时。这一自然系统的方面没有逃过神经进化研究者的注意，他们已经在许多进化算法中实现了这一点。我们将在第8章[ES-HyperNEAT和视网膜问题](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml)中讨论如何使用基于神经进化的算法创建模块化人工神经网络。
- en: NEAT algorithm overview
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NEAT算法概述
- en: The method of NEAT for evolving complex ANNs was designed to reduce the dimensionality
    of the parameter search space through the gradual elaboration of the ANN's structure
    during evolution. The evolutionary process starts with a population of small,
    simple genomes (seeds) and gradually increases their complexity over generations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT用于进化复杂人工神经网络的方法旨在通过在进化过程中逐步细化人工神经网络的结构来减少参数搜索空间的维度。进化过程从一个小型、简单的基因组（种子）种群开始，并在每一代中逐渐增加其复杂性。
- en: 'The seed genomes have a very simple topology: only input, output, and bias
    neurons are expressed. No hidden nodes are introduced into the seed from the beginning
    to guarantee that the search for a solution starts in the lowest-dimensional parameter
    space (connection weights) possible. With each new generation, additional genes
    are introduced, expanding the solution search space by presenting a new dimension
    that previously did not exist. Thus, evolution begins by searching in a small
    space that can be easily optimized and adds new dimensions when necessary. With
    this approach, complex phenotypes (solutions) can be discovered gradually, step
    by step, which is much more efficient than launching the search directly in the
    vast space of the final solutions. Natural evolution utilizes a similar strategy
    by occasionally adding new genes that make phenotypes more complex. In biology,
    this process of incremental elaboration is called **complexification**.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 种子基因组具有一个非常简单的拓扑结构：仅表达输入、输出和偏置神经元。从一开始就没有引入隐藏节点，以确保解决方案的搜索从可能的最低维参数空间（连接权重）开始。在每一代中，都会引入新的基因，通过呈现一个之前不存在的新维度来扩展解决方案搜索空间。因此，进化开始于一个小型空间，可以轻松优化，并在必要时添加新维度。采用这种方法，可以逐步、逐步地发现复杂的表型（解决方案），这比直接在最终解决方案的广阔空间中启动搜索要高效得多。自然进化通过偶尔添加使表型更复杂的基因来利用类似的策略。在生物学中，这个过程被称为**复杂化**。
- en: The primary goal of the NEAT method is to minimize the complexity of the genome
    structure – not only the final product, but of all the intermediate generations
    of the organisms as well. Thus, the evolution of the network topology results
    in a significant performance advantage by reducing the overall solutions for the
    search space. For example, the high-dimensional space of the final solution is
    only encountered at the end of the evolutionary process. Another essential feature
    of the algorithm is that each structure that's introduced to the genome is the
    subject of subsequent fitness evaluations in the future generations. Also, only
    useful structures will survive during the evolutionary process. In other words,
    the structural complexity of the genome is always goal-justified.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT方法的主要目标是最小化基因组结构的复杂性——不仅是最終產品，还包括所有中间代有机体的结构。因此，网络拓扑结构的进化通过减少搜索空间的整体解决方案来带来显著的性能优势。例如，最终解决方案的高维空间仅在进化过程的最后阶段遇到。算法的另一个基本特征是，引入基因组的每个结构都将成为未来代际中后续适应性评估的主题。此外，在进化过程中，只有有用的结构才能生存下来。换句话说，基因组的结构复杂性始终是目标合理的。
- en: NEAT encoding scheme
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NEAT编码方案
- en: 'The genetic encoding scheme of NEAT is designed to allow easy matching between
    corresponding genes during the mating process when a crossover operator is applied
    to the two parent genomes. The NEAT genome is a linear representation of the connectivity
    pattern of the encoded neural network, as shown in the following NEAT genome scheme:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT的遗传编码方案设计用于在交叉操作应用于两个父代基因组时，允许在配对过程中轻松匹配相应的基因。NEAT基因组是编码神经网络连接模式的线性表示，如下所示NEAT基因组方案：
- en: '![](img/79b7b32f-0c4e-4871-9773-006c9c4ff857.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79b7b32f-0c4e-4871-9773-006c9c4ff857.png)'
- en: The NEAT genome scheme
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT基因组方案
- en: 'Each genome is represented as a list of connection genes that encode connections
    between the nodes of the neural network. Also, there are node genes that encode
    information about network nodes, such as the node identifier, node type, and type
    of activation function. The connection gene encodes the following connection parameters
    of the network link:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个基因组表示为连接基因的列表，这些连接基因编码神经网络节点之间的连接。此外，还有节点基因，这些基因编码有关网络节点信息，例如节点标识符、节点类型和激活函数类型。连接基因编码网络链接的以下连接参数：
- en: The identifier of the input network node
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入网络节点的标识符
- en: The identifier of the output network node
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出网络节点的标识符
- en: The strength (weight) of the connection
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接的强度（权重）
- en: A bit, which indicates whether the connection is enabled (expressed) or not
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个位，表示连接是否启用（表达）
- en: An innovation number, which allows matching genes during recombination
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个创新号，允许在重组过程中匹配基因
- en: The bottom part of the preceding diagram represents a scheme of the same genome
    in the form of a directed graph.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个图的下部表示同一基因组以有向图形式呈现的方案。
- en: Structural mutations
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结构突变
- en: 'The mutation operator that''s specific to NEAT can change a connection''s strength
    (weight) and the network''s structure. There are two main types of structural
    mutations:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 特定于NEAT的突变操作可以改变连接的强度（权重）和网络的结构。主要有两种结构突变类型：
- en: Adding a new connection between nodes
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在节点之间添加新的连接
- en: Adding a new node to the network
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在网络中添加新的节点
- en: 'The following diagram shows the structural mutations of the NEAT algorithm:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了NEAT算法的结构突变：
- en: '![](img/0db168be-5731-4f21-804b-b16860e4f654.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0db168be-5731-4f21-804b-b16860e4f654.png)'
- en: The structural mutations of the NEAT algorithm
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT算法的结构突变
- en: When the mutation operator is applied to the NEAT genome, the newly added gene
    (connection gene or node gene) is assigned with an increasingly incremented innovation
    number. During the evolutionary process, the genomes of organisms within the population
    gradually get larger and genomes of varying sizes are produced. This process results
    in different connection genes being in the same positions within a genome, making
    the matching process between same-origin genes extremely complicated.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当突变操作应用于NEAT基因组时，新添加的基因（连接基因或节点基因）被分配一个不断增加的创新号。在进化过程中，种群中生物的基因组逐渐变大，产生了不同大小的基因组。这个过程导致不同的连接基因在基因组中的相同位置，使得同源基因之间的匹配过程极其复杂。
- en: Crossover with an innovation number
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有创新号的交叉
- en: There is a piece of unexploited information in the evolutionary process that
    tells us exactly which genes to match between the genomes of any organism in the
    topologically diverse population. This is where each gene tells us which ancestor
    that gene was derived from. The connection genes with the same historical origin
    represent the same structure, despite possibly having different connection weight
    values. The historical origins of genes in the NEAT algorithm are represented
    by incrementally assigned innovation numbers, which allow us to track the chronology
    of structural mutations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在进化过程中存在一些未被充分利用的信息，它告诉我们如何精确匹配任何在拓扑多样性种群中生物的基因组之间的基因。这正是每个基因告诉我们该基因是从哪个祖先那里衍生出来的。具有相同历史起源的连接基因代表相同的结构，尽管可能具有不同的连接权重值。NEAT算法中基因的历史起源由递增分配的创新号表示，这使我们能够追踪结构突变的年代学。
- en: At the same time, during the crossover, the offspring inherit the innovation
    numbers of genes from parent genomes. Thus, the innovation number of specific
    genes never change, allowing similar genes from different genomes to be matched
    during the crossover. The innovation numbers of matched genes are the same. If
    the innovation numbers do not match, the gene belongs to the *disjoint* or *excess*
    part of the genome, depending on whether its innovation number lies inside of,
    or outside of, the range of other parent innovation numbers. The disjoint or excess
    genes represent structures that are not present in the genome of the other parent
    and require special handling during the crossover phase. Thus, the offspring inherits
    genes that have the same innovation number. These are randomly chosen from one
    of the parents. The offspring always inherit the disjoint or excess genes from
    the parent with the highest fitness. This feature allows a NEAT algorithm to efficiently
    perform gene recombination using linear genome encoding, without the need for
    complex topological analysis.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在交叉过程中，后代继承了来自父母基因组的基因创新编号。因此，特定基因的创新编号永远不会改变，这使得来自不同基因组的相似基因在交叉过程中可以匹配。匹配基因的创新编号是相同的。如果创新编号不匹配，该基因属于基因组的*不连续*或*多余*部分，这取决于其创新编号是否位于其他父母创新编号的范围之内或之外。不连续或多余的基因代表在另一父母基因组中不存在的结构，在交叉阶段需要特殊处理。因此，后代继承了具有相同创新编号的基因。这些基因是从父母之一随机选择的。后代总是从适应度最高的父母那里继承不连续或多余的基因。这一特性允许NEAT算法使用线性基因组编码有效地执行基因重组，而无需进行复杂的拓扑分析。
- en: 'The following diagram shows the crossover (recombination) in the NEAT algorithm:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了NEAT算法中的交叉（重组）：
- en: '![](img/3e0a3210-228f-46c1-a3f7-3d4baeff1635.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3e0a3210-228f-46c1-a3f7-3d4baeff1635.png)'
- en: Crossover (recombination) in the NEAT algorithm
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT算法中的交叉（重组）
- en: 'The preceding diagram shows an example of a crossover between two parents using
    the NEAT algorithm. The genomes of both parents are aligned using the innovation
    numbers (the number at the top of the connection gene cell). After that, the offspring
    is produced by randomly choosing connection genes from either of parents when
    the innovation numbers are the same: the genes with innovation numbers from one
    to five. Finally, the disjoint and excess genes are added from either of the parents
    unconditionally and ordered by innovation number.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表展示了使用NEAT算法的两个父母之间的交叉示例。两个父母的基因组通过创新编号（连接基因细胞顶部的数字）对齐。之后，当创新编号相同时，通过从任一父母随机选择连接基因来产生后代：编号为一到五的基因。最后，无条件地从任一父母那里添加不连续和多余的基因，并按创新编号排序。
- en: Speciation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物种分化
- en: In the evolutionary process, organisms can create diverse topologies through
    generations, but they fail to produce and maintain topological innovations of
    their own. The smaller network structures optimize faster than larger ones, which
    artificially reduces the chances of survival of a descendant genome after adding
    a new node or connection to it. Thus, the freshly augmented topologies experience
    negative evolutionary pressure due to the temporary decrease of fitness of the
    organisms within the population. At the same time, novel topologies can introduce
    innovations that lead to a winning solution in the long run. To address the temporal
    drop of fitness, the concept of speciation was introduced in the NEAT algorithm.
    The speciation limits the range of organisms that can mate by introducing narrow
    niches where only organisms that belong to the same niche compete with each other
    during the crossover, instead of competing with all the organisms in the population.
    Speciation is implemented by dividing the population so that organisms with a
    similar topology belong to the same species.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在进化过程中，生物体可以通过代际繁衍创造出多样的拓扑结构，但它们无法产生并维持自身的拓扑创新。较小的网络结构比较大的网络结构优化得更快，这人为地减少了在基因组中添加新节点或连接后后代基因组的生存机会。因此，新增加的拓扑结构由于种群中生物体适应度的暂时下降而承受着负面的进化压力。同时，新的拓扑结构可以引入创新，最终导致长期的成功解决方案。为了解决适应度的暂时下降，NEAT算法中引入了物种分化的概念。物种分化通过引入狭窄的生态位来限制可以交配的生物体范围，在这些生态位中，只有属于同一生态位的生物体在交叉过程中相互竞争，而不是与种群中的所有生物体竞争。物种分化通过将种群分割，使得具有相似拓扑结构的生物体属于同一物种来实现。
- en: 'Let''s refer to the following speciation algorithm:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们参考以下物种分化算法：
- en: '![](img/d5150acd-027d-4972-b813-c4137022868f.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5150acd-027d-4972-b813-c4137022868f.png)'
- en: The speciation algorithm
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 物种分化算法
- en: The NEAT method permits the creation of complex ANNs that are capable of solving
    a variety of control optimization problems, as well as other unsupervised learning
    problems. Due to the introduced specifics of ANN topology augmentation through
    complexification and speciation, the solutions tend to optimize the performance
    of training and inference. The resulting ANN topology grows to match the problem
    that needs to be solved, without any excess layers of hidden units being introduced
    by the conventional methods of ANN's topology design for backpropagation-based
    training.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: NEAT方法允许创建复杂的ANN，能够解决各种控制优化问题，以及其他无监督学习问题。由于引入了通过复杂化和物种分化来增强ANN拓扑结构的特定细节，解决方案往往优化了训练和推理的性能。结果，ANN拓扑结构增长以匹配需要解决的问题，而没有通过传统的ANN拓扑设计方法引入任何多余的隐藏层。
- en: 'More details about the NEAT algorithm can be found in the original paper: [http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 关于NEAT算法的更多详细信息，请参阅原始论文：[http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf)。
- en: Hypercube-based NEAT
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于超立方体的NEAT
- en: Intelligence is a product of the brain, and the human brain as a structure is itself
    a product of natural evolution. Such an intricate structure has evolved over millions
    of years, under pressure from harsh environments, and while competing with other
    living beings for survival. As a result, an extremely complex structure has evolved,
    with many layers, modules, and trillions of connections between neurons. The structure
    of the human brain is our guiding star and is aiding our efforts in creating artificial
    intelligence systems. However, how can we address all the complexity of the human
    brain with our imperfect instruments?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 智力是大脑的产物，而人类大脑作为一种结构，本身也是自然进化的产物。这样一个复杂的结构在数百万年的演变过程中，在恶劣环境的压力下，以及在与其他生物为生存而竞争的过程中逐渐形成。因此，一个极其复杂的结构已经形成，具有许多层次、模块以及神经元之间数万亿的连接。人类大脑的结构是我们的指南星，正帮助我们努力创造人工智能系统。然而，我们如何用我们不完美的工具来应对人类大脑的复杂性呢？
- en: By studying the human brain, neuroscientists have found that its spatial structure
    plays an essential role in all perceiving and cognitive tasks – from vision to
    abstract thinking. Many intricate geometric structures have been found, such as
    the grid cells that help us with inertial navigation, and the cortical columns
    that are connected to the eye's retina to process visual stimuli. It has been
    demonstrated that the structure of the brain allows us to effectively respond
    to the patterns in signals that are received from the sensorium by using designated
    neural structures that are activated by specific patterns in the inputs. This
    feature of the brain allows it to use an extremely efficient way of representing
    and processing the entire diversity of the input data that's obtained from the
    environment. Our brains have evolved to be effective pattern recognition and pattern
    processing engines that actively reuse specific neural modules to process particular
    patterns, thus dramatically reducing the number of different neural structures
    required. This only became possible due to the complex modular hierarchy and the
    spatial integration of its various parts.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过研究人类大脑，神经科学家发现，其空间结构在所有感知和认知任务中起着至关重要的作用——从视觉到抽象思维。已经发现了许多复杂的几何结构，例如帮助我们进行惯性导航的网格细胞，以及与眼睛视网膜相连的皮层柱，用于处理视觉刺激。已经证明，大脑的结构使我们能够通过由输入中的特定模式激活的特定神经网络结构，有效地对从感官接收到的信号中的模式做出反应。这种大脑的特性允许它以极其高效的方式表示和处理从环境中获得的所有输入数据的多样性。我们的头脑已经进化成有效的模式识别和模式处理引擎，积极重用特定的神经网络模块来处理特定的模式，从而大大减少了所需的不同神经网络结构的数量。这仅由于复杂的模块化层次和其各个部分的空間整合才成为可能。
- en: 'As we mentioned previously, the biological brain incorporates complex hierarchical
    and spatially-aware data processing routines. This has inspired the researchers
    of neuroevolution to introduce similar data processing methods in the field of
    artificial neural networks. When designing such systems, it is necessary to address
    the following problems:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，生物大脑包含了复杂的分层和空间感知数据处理程序。这激发了神经进化研究人员在人工神经网络领域引入类似的数据处理方法。在设计此类系统时，必须解决以下问题：
- en: The vast number of input features and training parameters that require large-scale
    ANNs
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要大规模ANN的大量输入特征和训练参数
- en: The effective representation of natural geometrical regularities and symmetries
    that are observed in the physical world
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效表示在物理世界中观察到的自然几何规律和对称性
- en: The effective processing of input data through the introduction of the locality
    principle, that is, when spatially/semantically adjacent data structures are processed
    by the modules of interconnected neural units, which occupy the same compact area
    of the entire network structure
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过引入局部性原理有效地处理输入数据，即当空间/语义相邻的数据结构由相互连接的神经单元模块处理时，这些模块占据整个网络结构的相同紧凑区域
- en: In this section, you learned about the** Hypercube-based NeuroEvolution of Augmenting
    Topologies** (**HyperNEAT**) method, which was proposed by Kenneth O. Stanley
    to solve various problems by exploiting geometrical regularities. In the next
    section, we will look at **Compositional Pattern Producing Networks** (**CPPNs**).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解了**基于超立方的神经进化拓扑增强**（**HyperNEAT**）方法，该方法由Kenneth O. Stanley提出，通过利用几何规律来解决各种问题。在下一节中，我们将探讨**组合模式生成网络**（**CPPNs**）。
- en: Compositional Pattern Producing Networks
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组合模式生成网络
- en: HyperNEAT extends the original NEAT algorithm by introducing a new type of indirect
    genome encoding scheme called CPPNs. This type of encoding makes it possible to
    represent the connectivity patterns of a phenotype's ANN as a function of its
    geometry.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: HyperNEAT通过引入一种新的间接基因组编码方案CPPNs扩展了原始的NEAT算法。这种编码方式使得将表型ANN的连接模式表示为其几何形状的函数成为可能。
- en: 'HyperNEAT stores the connectivity pattern of the phenotype neural network as
    a four-dimensional hypercube, where each point encodes the connection between
    two nodes (that is, the coordinates of the source and target neurons) and the
    connective CPPN paints various patterns within it. In other words, CPPN computes
    the four-dimensional function, which is defined as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: HyperNEAT将表型神经网络的连接模式存储为一个四维超立方体，其中每个点编码两个节点之间的连接（即源神经元和目标神经元的坐标）以及连接的CPPN在其内部绘制各种图案。换句话说，CPPN计算一个四维函数，其定义如下：
- en: '![](img/0bff394f-591a-4f1f-a0ee-5ad0267c4329.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0bff394f-591a-4f1f-a0ee-5ad0267c4329.png)'
- en: Here, the source node is at (*x[1]*, *y[1]*) and the target node is at (*x[2]*,
    *y[2]*). At this stage, CPPN returns a weight for every connection between every
    node in the phenotype network, which is represented as a grid. By convention,
    the connection between the two nodes is not expressed if the magnitude of the
    connection weight that's computed by CPPN is less than a minimum threshold (*w[min]*).
    That way, the connectivity pattern that's produced by CPPN can represent any network
    topology. The connectivity pattern can be used to encode large-scale ANNs by discovering
    regularities in the training data and can reuse the same set of genes to encode
    repetitions. By convention, the connectivity pattern that's produced by CPPN is
    called the **substrate**.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，源节点位于(*x[1]*, *y[1]*)，目标节点位于(*x[2]*, *y[2]*)。在这个阶段，CPPN为表型网络中每个节点之间的每个连接返回一个权重，这以网格的形式表示。按照惯例，如果CPPN计算出的连接权重的大小小于一个最小阈值(*w[min]*)，则不表示两个节点之间的连接。这样，CPPN产生的连接模式可以表示任何网络拓扑。连接模式可以通过在训练数据中发现规律来编码大规模ANN，并且可以重用同一组基因来编码重复。按照惯例，CPPN产生的连接模式被称为**基质**。
- en: 'The following diagram shows the interpretation of the Hypercube-based Geometric
    Connectivity Pattern:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了基于超立方的几何连接模式的解释：
- en: '![](img/4823fb2d-40e2-4eb0-b06b-a49250416730.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4823fb2d-40e2-4eb0-b06b-a49250416730.png)'
- en: Hypercube-based Geometric Connectivity Pattern interpretation
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于超立方的几何连接模式解释
- en: Unlike traditional ANN architectures, CPPN employs a set of various activation
    functions for its hidden nodes to explore a variety of geometrical regularities.
    For example, the trigonometric sine can be used to represent repetitions, while
    Gaussian can be used to enforce locality at a specific part of the network (that
    is, symmetry along the coordinate axis). Thus, the CPPN encoding scheme can represent
    patterns with different geometrical regularities such as symmetry, repetition,
    repetition with regularities, and so on in a compact manner.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的ANN架构不同，CPPN为其隐藏节点使用一组各种激活函数来探索各种几何规律。例如，三角函数的正弦可以用来表示重复，而高斯函数可以用来在网络的特定部分强制局部性（即沿坐标轴的对称性）。因此，CPPN编码方案可以以紧凑的方式表示具有不同几何规律的图案，如对称性、重复、具有规律的重复等。
- en: Substrate configuration
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 底板配置
- en: The layout of the network nodes in the substrate that CPPN connects to can take
    various forms, which are best suited to different kinds of problems. It is the
    responsibility of the experimenter to select the appropriate layout to achieve
    optimal performance. For example, the output nodes that control a radial entity
    such as a six-leg crawler may be best laid out with radial geometry so that a
    connectivity pattern can be expressed with polar coordinates.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: CPPN连接到底板中的网络节点的布局可以采取各种形式，最适合不同类型的问题。选择合适的布局以实现最佳性能是实验者的责任。例如，控制六腿爬行器等径向实体的输出节点可能最好采用径向几何布局，以便可以用极坐标表示连接模式。
- en: 'The following diagram shows some examples of substrate layout configurations:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了底板布局配置的一些示例：
- en: '![](img/bd6be905-d416-463e-98b2-9f26beb88416.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bd6be905-d416-463e-98b2-9f26beb88416.png)'
- en: Examples of substrate layout configurations
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 底板布局配置示例
- en: 'There are several common types of substrate layout that are typically used
    with HyperNEAT (see the preceding diagram), some of which are as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: HyperNEAT通常使用几种常见的底板布局类型（参见前面的图），以下是一些例子：
- en: '**Two-dimensional grid**: A regular grid of network nodes in a two-dimensional
    Cartesian space centered at (0, 0)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二维网格**：以(0, 0)为中心的二维笛卡尔空间中的网络节点规则网格'
- en: '**Three-dimensional grid**: A regular grid of network nodes in a three-dimensional
    Cartesian space centered at (0, 0, 0)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**三维网格**：以(0, 0, 0)为中心的三维笛卡尔空间中的网络节点规则网格'
- en: '**State-Space Sandwich**: Two 2D planar grids with source and target nodes
    in which one layer can send connections in the direction of the other one'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态空间三明治**：两个二维平面网格，其中源节点和目标节点可以相互发送连接'
- en: '**Circular**: The regular radial structure, which is suited to define regularities
    in radial geometry-based polar coordinates'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**圆形**：适合定义基于极坐标的径向几何规律的规则径向结构'
- en: Evolving connective CPPNs and the HyperNEAT algorithm
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进化连接CPPN和HyperNEAT算法
- en: The method is called **HyperNEAT** because it uses a modified NEAT to evolve
    CPPNs that represent spatial patterns in the hyperspace. Each expressed point
    of the pattern, which is bounded by a hypercube, represents a connection between
    two nodes in the lower-dimensional graph (substrate). Thus, the dimensionality
    of the hyperspace is twice as big as the dimensionality of the underlying lower-dimensional
    graph. Later in [Chapter 8](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml), *ES-HyperNEAT
    and the Retina Problem*, we will look at some examples that use two-dimensional
    connectivity patterns.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**HyperNEAT**，因为它使用修改后的NEAT来进化表示超空间中空间模式的CPPNs。每个由超立方体界定的模式表达点，代表低维图中两个节点之间的连接（底板）。因此，超空间的维度是底层低维图维度的两倍。在[第8章](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml)，*ES-HyperNEAT和视网膜问题*中，我们将探讨一些使用二维连接模式示例。
- en: 'The HyperNEAT algorithm can be seen in the following diagram:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: HyperNEAT算法如下图所示：
- en: '![](img/35641a49-7e0d-4127-999d-623852a4f8e8.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/35641a49-7e0d-4127-999d-623852a4f8e8.png)'
- en: The general form of the HyperNEAT algorithm
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: HyperNEAT算法的一般形式
- en: Any connection gene or node gene that's added to the connective CPPN during
    its evolution leads to the discovery of a new global dimension of variation in
    the connectivity patterns across the phenotype substrate (novel traits). Each
    modification that's made to the CPPN genome represents a new way that an entire
    connectivity pattern can vary. Also, previously evolved connective CPPNs can be
    queried to produce connectivity patterns for the substrate at a higher resolution
    than what was used for its training. This allows us to produce a working solution
    to the same problem at any resolution, potentially without an upper limit. Thus,
    the aforementioned properties have made HyperNEAT a powerful instrument in evolving
    large-scale bio-inspired artificial neural networks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在其进化过程中添加到连接CPPN中的任何连接基因或节点基因都会导致在表型基板上的连接模式中找到新的全局变化维度（新特性）。对CPPN基因组所做的任何修改都代表了一种全新的整个连接模式可以变化的方式。此外，先前进化的连接CPPN可以被查询以产生比用于其训练的更高分辨率的基板连接模式。这使得我们能够在任何分辨率下产生相同问题的有效解决方案，可能没有上限。因此，上述特性使HyperNEAT成为进化大规模生物启发式人工神经网络的有力工具。
- en: 'For more information on the HyperNEAT method, you can refer to the following
    link: [https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf](https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解更多关于HyperNEAT方法的信息，您可以参考以下链接：[https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf](https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf).
- en: Evolvable-Substrate HyperNEAT
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可进化基板HyperNEAT
- en: The HyperNEAT method exposes the fact that geometrical regularities of the natural
    world can be adequately represented by artificial neural networks with nodes placed
    at specific spatial locations. That way, the neuroevolution gains significant
    benefits and it allows large-scale ANNs to be trained for high dimensional problems,
    which was impossible with the ordinary NEAT algorithm. At the same time, the HyperNEAT
    approach is inspired by the structure of a natural brain, which still lacks the
    plasticity of the natural evolution process. While allowing the evolutionary process
    to elaborate on a variety of connectivity patterns between network nodes, the
    HyperNEAT approach exposes a hard limitation on where the network nodes are placed.
    The experimenter must define the layout of the network nodes from the very beginning,
    and any incorrect assumption that's made by the researcher will lower the performance
    of the evolutionary process.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: HyperNEAT方法揭示了自然世界的几何规律可以通过放置在特定空间位置的人工神经网络节点得到充分表示。这样，神经进化获得了显著的好处，并允许大规模ANN用于高维问题，这在普通的NEAT算法中是不可能的。同时，HyperNEAT方法受到自然大脑结构的启发，但仍然缺乏自然进化过程的可塑性。虽然允许进化过程在节点之间阐述各种连接模式，但HyperNEAT方法在节点放置位置上暴露了一个硬限制。实验者必须从一开始就定义网络节点的布局，任何研究人员做出的错误假设都会降低进化过程的表现。
- en: By placing the network node at a specific location in the substrate, the experimenter
    creates an unintentional constraint on the pattern of weights that are produced
    by the CPPN. This restriction then interferes with the CPPN when it attempts to
    encode the geometrical regularities of the natural world into the topography of
    solution-producing ANN (phenotype). Here, the connectivity pattern produced by
    CPPN must perfectly align with the layout of the substrate that is defined by
    the experimenter; connections only are possible between given network nodes. Such
    limitation leads to unnecessary approximation errors, which spoil the outcome.
    It may be more effective for the CPPN to elaborate connectivity patterns over
    nodes that have been placed at slightly different locations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将网络节点放置在基板上的特定位置，实验者对由CPPN产生的权重模式施加了无意中的约束。这种限制随后干扰了CPPN，当它试图将自然世界的几何规律编码到产生解决方案的ANN（表型）的地形时。在这里，由CPPN产生的连接模式必须与实验者定义的基板布局完美对齐；只有给定网络节点之间才能建立连接。这种限制导致不必要的近似误差，从而破坏了结果。对于CPPN来说，在放置位置略有不同的节点上详细阐述连接模式可能更有效。
- en: Information patterns in the hypercube
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超立方体中的信息模式
- en: Why should such limitations on the location of nodes be imposed in the first
    place? Wouldn't it be nice if the implicit clues that had been drawn from the
    connectivity patterns became the guidelines of where to place the next node to
    represent the natural regularities of the physical world better?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么一开始就要对节点的位置施加这样的限制？如果从连接模式中提取的隐含线索成为放置下一个节点以更好地表示物理世界自然规律的位置指南，那岂不是很好？
- en: The areas with uniform connection weights encode a small amount of information
    and hence have little functional value. At the same time, the areas with vast
    gradients of weight values are extremely information-intensive. Such areas can
    benefit from additional network nodes being placed to represent a much finer encoding
    of the natural process. As you may recall from our discussion of the HyperNEAT
    algorithm, it is possible to represent the connection between two nodes in the
    substrate as a point in a four-dimensional hypercube. Thus, the main feature of
    the proposed ES-HyperNEAT algorithm is to express more hyper-points in the areas
    of the hypercube, where the high variation of connection weights are detected.
    At the same time, the fewer hyper-points are placed in the areas with a lower
    variation of connection weights.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 具有均匀连接权重的区域编码的信息量很少，因此功能价值不大。同时，具有巨大权重值梯度的区域信息密集度极高。这些区域可以通过放置额外的网络节点来受益，以表示对自然过程的更精细编码。如您从我们对HyperNEAT算法的讨论中回忆的那样，可以在四维超立方体中用一个点来表示基板中两个节点之间的连接。因此，所提出的ES-HyperNEAT算法的主要特征是在检测到连接权重高变动的超立方体区域中表达更多的超点。同时，在连接权重变动较低的区域中放置较少的超点。
- en: The placement of nodes and the exposed connections between them can be dictated
    by the variation in the weights of connections that are produced by the evolving
    CPPN for a given region of a substrate. In other words, there is no need for additional
    information to decide on the next node placement in the substrate, other than
    what we are already receiving from the CPPN that encodes the connectivity patterns
    of the network. Information density becomes the main guiding principle for the
    algorithm to determine the topography of the substrate.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的放置以及它们之间暴露的连接可以由进化CPPN为基板给定区域产生的连接权重变化来决定。换句话说，除了从编码网络连接模式的CPPN中接收到的信息之外，不需要额外的信息来决定基板中下一个节点的放置。信息密度成为算法确定基板地形的主要指导原则。
- en: Node placement in the phenotype ANN signifies where the information is encoded
    in the connectivity patterns that are created by the CPPN.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表型ANN中的节点放置表示信息编码在由CPPN创建的连接模式中。
- en: Quadtree as an effective information extractor
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 四叉树作为有效的信息提取器
- en: To represent hyper-points that are encoding connection weights within the hypercube,
    the ES-HyperNEAT algorithm employs a *quadtree*. A quadtree is a tree data structure
    in which each internal node has exactly four children nodes. This data structure
    was selected due to its inherent properties, allowing it to represent two-dimensional
    areas at different levels of granularity. With a quadtree, it is possible to organize
    an effective search through the two-dimensional space by splitting any area of
    interest into four subareas, and each of them becomes a leaf of the tree, with
    root (parent) node representing the original (decomposed) region.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示在超立方体中编码连接权重的超点，ES-HyperNEAT算法采用了一个**四叉树**。四叉树是一种树形数据结构，其中每个内部节点恰好有四个子节点。这种数据结构被选中是因为其固有的特性，允许它在不同粒度级别上表示二维区域。使用四叉树，可以通过将任何感兴趣的区域分割成四个子区域来有效地组织二维空间中的搜索，每个子区域成为树的叶子节点，根（父）节点代表原始（分解）区域。
- en: Using the quadtree-based information extraction method, the ES-HyperNEAT approach
    iteratively looks for new connections between nodes in the two-dimensional space
    of the substrate ANN, starting from the input and output nodes that have been
    predefined by the experimenter. This method is much more computationally effective
    than searching directly in the four-dimensional hypercube space.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于四叉树的信息提取方法，ES-HyperNEAT方法迭代地在基板ANN的二维空间中寻找节点之间新的连接，从实验者预先定义的输入和输出节点开始。这种方法比直接在四维超立方体空间中搜索计算上更有效。
- en: 'The following diagram shows an example of extracting information using quadtree
    data structures:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用四叉树数据结构提取信息的一个示例：
- en: '![](img/6f64241b-221e-403a-806a-41c94354d8af.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f64241b-221e-403a-806a-41c94354d8af.png)'
- en: Quadtree information extraction example
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于四叉树的信息提取示例
- en: 'The quadtree-based search algorithm operates in two main stages:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于四叉树的搜索算法在两个主要阶段运行：
- en: '**Division and initialization**: At this stage, the quadtree is created by
    recursively subdividing the initial substrate space occupying the area from (-1,
    -1) to (1, 1). The subdivision stops when the desired tree depth is reached. This
    implicitly determines how many subspaces fit into the initial space of the substrate
    (initialization resolution). After that, for every quadtree node with the center
    at ![](img/81ca5f1c-7d61-4d46-9d54-0f9a1671bf80.png), the CPPN is queried with ![](img/b192864b-1e82-4114-8c13-2d7cd7fe6310.png) arguments
    to find connection weights. When connection weights for ![](img/eb93f7bf-1498-4e2d-8d1a-b63f2b397304.png)
    leaf nodes of a particular quadtree node ![](img/70e29e27-edfd-4fca-b88f-a0a9e0012f3f.png)
    are found, the variance of this node can be calculated by the following formula:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**划分和初始化**：在这个阶段，通过递归细分初始底物空间（从(-1, -1)到(1, 1)的区域）来创建四叉树。细分在达到所需的树深度时停止。这隐式地确定了有多少子空间适合底物的初始空间（初始化分辨率）。之后，对于每个以
    ![](img/81ca5f1c-7d61-4d46-9d54-0f9a1671bf80.png) 为中心的四叉树节点，使用 ![](img/b192864b-1e82-4114-8c13-2d7cd7fe6310.png) 个参数查询CPPN以找到连接权重。当找到特定四叉树节点 ![](img/70e29e27-edfd-4fca-b88f-a0a9e0012f3f.png) 的叶节点的连接权重时，可以使用以下公式计算该节点的方差：'
- en: '![](img/a4e2dc7c-beee-49c5-af45-0886c166b8f3.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a4e2dc7c-beee-49c5-af45-0886c166b8f3.png)'
- en: Here ![](img/06860371-dbc5-4efc-9a50-b958b744c471.png) is the mean connection
    weight among ![](img/531eab51-4128-4d34-a8ac-d128a4699802.png) leaf nodes and ![](img/ddba058a-ccaa-4c47-adbd-25886edeb8ae.png) is
    a connection weight to a specific leaf node. The calculated variance value is
    a heuristic indicator of the presence of information in the specific substrate
    area. If this value is higher than the particular division threshold (defining
    desired information density), then the division stage can be repeated for the
    corresponding square of the substrate. This way, the desired information density
    can be enforced by the algorithm. Take a look at the top part of the preceding
    diagram for a visual insight into how division and initialization are done using
    the quadtree data structures.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![](img/06860371-dbc5-4efc-9a50-b958b744c471.png) 是叶节点之间平均连接权重，而 ![](img/531eab51-4128-4d34-a8ac-d128a4699802.png) 是到特定叶节点的连接权重。计算出的方差值是特定底物区域存在信息的启发式指标。如果这个值高于特定的划分阈值（定义所需信息密度），则可以对底物的相应平方区域重复划分阶段。这样，算法可以通过这种方式强制执行所需的信息密度。查看前一个图的顶部部分，以了解如何使用四叉树数据结构进行划分和初始化的视觉洞察。
- en: '**Pruning and extraction**: To guarantee that more connections (and nodes in
    the substrate) become expressed in the regions with high information density (high
    weights variance), the pruning and extraction procedure is executed over the quadtree
    that was generated during the previous stage. The quadtree traverses depth-first
    until the variance of the current node is smaller than a variance threshold ![](img/9213a181-6ab8-46bf-9a55-4d1be8b0ba88.png) or
    until the node has no children (the zero variance). For every qualified node,
    the connection is expressed between its center ![](img/7d06e8bd-2222-4af3-ab99-45da675f2121.png) and
    each parent node is already defined, either by the experimenter or found at the
    previous run of these two stages (that is, from hidden nodes that have already
    been created by ES-HyperNEAT method). Refer to the bottom part of the preceding
    diagram for a visual insight into how the pruning and extraction phase works.'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修剪和提取**：为了保证更多连接（以及底物中的节点）在信息密度高（权重方差高）的区域表达出来，修剪和提取过程是在前一个阶段生成的四叉树上执行的。四叉树深度优先遍历，直到当前节点的方差小于方差阈值
    ![](img/9213a181-6ab8-46bf-9a55-4d1be8b0ba88.png) 或者直到节点没有子节点（零方差）。对于每个合格的节点，连接在其中心
    ![](img/7d06e8bd-2222-4af3-ab99-45da675f2121.png) 和每个父节点之间表达，父节点要么由实验者定义，要么在前两个阶段的运行中找到（即，从ES-HyperNEAT方法已经创建的隐藏节点）。参考前一个图的底部部分，以了解修剪和提取阶段的工作原理。'
- en: ES-HyperNEAT algorithm
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ES-HyperNEAT算法
- en: The ES-HyperNEAT algorithm starts with user-defined input nodes and elaborates
    on exploring connections from them and sending them to the newly expressed hidden
    nodes. The expression of outgoing connectivity patterns and hidden nodes placement
    within the substrate space is done using the quadtree information extraction method,
    which we described previously. The information extraction process is iteratively
    applied until the desired level of information expression density is achieved,
    or until no more information can be discovered in the hypercube. After that, the
    resulting network is connected to the user-defined output nodes by expressing
    the incoming connectivity patterns to the outputs. We use quadtree information
    extraction for this as well. Only those hidden nodes are kept in the final network,
    which has a path to both the input and output nodes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ES-HyperNEAT算法从用户定义的输入节点开始，并详细探索从它们到新表达的隐藏节点的连接。在基板空间内表达输出连接模式和隐藏节点位置使用的是我们之前描述的四叉树信息提取方法。信息提取过程是迭代应用的，直到达到所需的信息表达密度水平，或者直到在超立方体中不能再发现更多信息。之后，通过表达到输出的输入连接模式，将得到的网络连接到用户定义的输出节点。我们也为此使用了四叉树信息提取。只有那些有路径连接到输入和输出节点的隐藏节点被保留在最终的网络中。
- en: Now, we have defined a multitude of nodes and connections within the substrate
    of the phenotype ANN. It can be beneficial to remove some nodes from the network
    by introducing an additional band pruning processing stage. At this stage, we
    keep only the points within a specific band and remove points on the edge of the
    band. By making bands broader or narrower, the CPPN can manage the density of
    the encoded information. For more details about band pruning, please refer to
    the *ES-HyperNEAT paper* ([https://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在表型ANN的基板中定义了许多节点和连接。引入一个额外的带修剪处理阶段来从网络中移除一些节点可能是有益的。在这个阶段，我们只保留特定带内的点，并移除带边缘的点。通过使带变宽或变窄，CPPN可以管理编码信息的密度。有关带修剪的更多详细信息，请参阅*ES-HyperNEAT论文*
    ([https://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf))。
- en: 'Take a look at the following ES-HyperNEAT algorithm:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的ES-HyperNEAT算法：
- en: '![](img/6f194296-5fae-4524-a44c-4fffe328680e.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f194296-5fae-4524-a44c-4fffe328680e.png)'
- en: The ES-HyperNEAT algorithm
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ES-HyperNEAT算法
- en: 'The ES-HyperNEAT algorithm takes all the advantages of the NEAT and HyperNEAT
    methods and introduces even more powerful new features, including the following:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ES-HyperNEAT算法继承了NEAT和HyperNEAT方法的所有优点，并引入了更多更强大的新特性，包括以下内容：
- en: Automatic placement of the hidden nodes within the substrate to precisely match
    the connectivity patterns that are expressed by evolved CPPNs.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动在基板内放置隐藏节点，以精确匹配由进化出的CPPN所表达的联系模式。
- en: Allows us to produce modular phenotype ANNs much more easily due to its inherent
    capabilities to start the evolutionary search with a bias toward locality (by
    the specific design of the initial CPPN architectures).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其固有的能力，即通过初始CPPN架构的具体设计，以局部性偏向开始进化搜索，这使得我们能够更容易地产生模块化表型ANN。
- en: With ES-HyperNEAT, it is possible to elaborate on the existing phenotype ANN
    structure by increasing the number of nodes and connections in the substrate during
    evolution. This is the opposite of HyperNEAT, where the number of substrate nodes
    is predefined.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ES-HyperNEAT，可以在进化过程中通过增加基板中的节点和连接数量来详细阐述现有的表型ANN结构。这与HyperNEAT相反，在HyperNEAT中，基板节点的数量是预定义的。
- en: The ES-HyperNEAT algorithm allows us to use the original HyperNEAT architecture
    without altering the genetic structure of the NEAT part. It allows us to address
    problems that are hard to solve with the HyperNEAT algorithm due to difficulties
    with creating an appropriate substrate configuration in advance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ES-HyperNEAT算法允许我们使用原始的HyperNEAT架构，而不改变NEAT部分的遗传结构。它使我们能够解决由于在事先创建适当的基板配置方面的困难，而难以用HyperNEAT算法解决的问题。
- en: More details about the ES-HyperNEAT algorithm and the motivations behind it
    can be found at [h](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)[ttps://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 关于ES-HyperNEAT算法及其背后的动机的更多详细信息可以在[h](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)[ttps://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)找到。
- en: Novelty Search optimization method
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新颖性搜索优化方法
- en: Most of the machine learning methods, including evolutionary algorithms, base
    their training on the optimization of the objective function. The main focus underlying
    the methods of optimization of the objective function is that the best way to
    improve the performance of a solver is to reward them for getting closer to the
    goal. In most evolutionary algorithms, the closeness to the goal is measured by
    the fitness of the solver. The measure of an organism's performance is defined
    by the fitness function, which is a metaphor for evolutionary pressure on the
    organism to adapt to its environment. According to that paradigm, the fittest
    organism is better adapted to its environment and best suited to find a solution.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习方法，包括进化算法，都是基于目标函数的优化进行训练的。目标函数优化方法背后的主要关注点是，提高求解器性能的最佳方式是奖励它们接近目标。在大多数进化算法中，接近目标是通过求解器的适应度来衡量的。一个生物体的性能是通过适应度函数来定义的，这是生物体适应其环境的进化压力的隐喻。根据这一范式，最适应的生物体更适合其环境，并且最适合找到解决方案。
- en: While direct fitness function optimization methods work well in many simple
    cases, for more complex tasks, it often falls victim to the local optima trap.
    Convergence to the local optima means that no local step in the search space provides
    any improvements during the fitness function optimization process. The traditional
    genetic algorithms use mutation and island mechanisms to escape from such local
    optima. However, as we will find out by doing experiments later in this book,
    it may not always help with deceptive problems, or it can take too long to find
    a successful solution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然直接适应度函数优化方法在许多简单情况下效果良好，但对于更复杂的任务，它往往陷入局部最优的陷阱。收敛到局部最优意味着在搜索空间中的任何局部步骤在适应度函数优化过程中都不会提供任何改进。传统的遗传算法使用变异和岛屿机制来逃离这种局部最优。然而，正如我们在本书后面的实验中所发现的那样，它可能并不总是有助于欺骗性问题，或者可能需要太长时间才能找到成功的解决方案。
- en: Many real-world problems have such deceptive fitness function landscapes that
    cannot be solved by an optimization process that is based solely on measuring
    how close the current solution is to the goal. As an example, we can consider
    the task of navigating through an unknown city with an irregular street pattern.
    In such a task, heading toward the destination often means traveling along deceptive
    roads that move you further away, only to bring you to the destination after several
    twists. But if you decide to start with roads that have been aligned in direction
    to the destination, it often leads you to a dead end, while the destination is
    just behind the wall but unreachable.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的问题具有这样的欺骗性适应度函数景观，无法通过仅基于测量当前解决方案与目标接近程度的优化过程来解决。例如，我们可以考虑在具有不规则街道模式的未知城市中导航的任务。在这样的任务中，朝着目的地前进通常意味着沿着欺骗性的道路行驶，这些道路只会让你离目的地越来越远，直到经过几次转弯后才到达。但如果你决定从指向目的地的道路开始，这通常会让你走到死胡同，而目的地就在墙的另一边，却无法触及。
- en: Novelty Search and natural evolution
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新颖性搜索与自然进化
- en: By looking at how natural selection works in the physical world, we can see
    that the enabling force behind evolutionary diversity is a search for novelty.
    In other words, any evolving species gains immediate evolutionary advantages over
    its rivals by finding new behavior patterns. This allows them to exploit the environment
    more efficiently. The natural evolution has no defined goals, and it broadens
    the solution search space by rewarding the exploration and exploitation of novel
    behaviors. This novelty can be considered as a proxy for many hidden creative
    forces in the natural world, which allows evolution to elaborate on even more
    complex behaviors and biological structures.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察自然选择在物理世界中的运作方式，我们可以看到，进化多样性的背后推动力是对新颖性的追求。换句话说，任何正在进化的物种通过发现新的行为模式，都能立即获得相对于其竞争对手的进化优势。这使得它们能够更有效地利用环境。自然进化没有明确的目标，它通过奖励对新行为的探索和利用来扩大解决方案的搜索空间。这种新颖性可以被视为自然界中许多隐藏的创造力的代理，这使得进化能够进一步细化更复杂的行为和生物结构。
- en: Taking inspiration from the natural evolution, *Joel Lehman* proposed a new
    method of search optimization for an artificial evolutionary process called **Novelty
    Search**. With this method, no particular fitness function is defined or used
    for solution search; instead, the novelty of each found solution is directly rewarded
    during the neuroevolution process. Thus, the novelty of the solutions that are found guide
    the neuroevolution toward the final goal. Such an approach gives us a chance to
    exploit the creative forces of evolution independent of the adaptive pressure
    to fit the solution into a particular niche.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 受自然进化的启发，*乔尔·莱曼*提出了一种用于人工进化过程的新颖性搜索优化方法。使用这种方法，没有定义或使用特定的适应度函数来搜索解决方案；相反，在神经进化过程中，每个找到的解决方案的新颖性直接得到奖励。因此，找到的解决方案的新颖性指导神经进化达到最终目标。这种方法使我们有机会利用进化的创造力，而无需适应压力将解决方案适应到特定的生态位。
- en: The effectiveness of a Novelty Search can be demonstrated with the *maze navigation*
    experiment, where an objective-based search finds the solution for the simple
    maze in many more steps (generations) than a Novelty Search. Furthermore, for
    the hard maze with deceptive configuration, the objective-based search fails to
    find any solution at all. We will discuss maze navigation experiments later in
    [Chapter 5](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml), *Autonomous Maze Navigation*.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖性搜索的有效性可以通过迷宫导航实验来证明，其中基于目标的搜索在简单迷宫中找到解决方案所需的步骤（代数）比新颖性搜索多得多。此外，对于具有欺骗性配置的困难迷宫，基于目标的搜索甚至无法找到任何解决方案。我们将在第5章
    [自主迷宫导航](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml)中讨论迷宫导航实验。
- en: Novelty metric
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新颖性度量
- en: The Novelty Search method employs a novelty metric for tracking the uniqueness
    of the behavior of each new individual. That is, the novelty metric is a measure
    of how far the new organism is from the rest of the population in the behavior
    space. An effective novelty metric implementation should allow us to compute sparseness
    at any point of the behavior space. Any area with a denser cluster of visited
    points is less novel and produces less evolutionary rewards.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖性搜索方法使用新颖性度量来跟踪每个新个体的行为独特性。也就是说，新颖性度量是衡量新生物体在行为空间中相对于其他个体的距离的度量。一个有效的新颖性度量实现应该允许我们在行为空间的任何点上计算稀疏性。任何有更密集的访问点集群的区域都相对不那么新颖，并产生较少的进化奖励。
- en: 'The most straightforward measure of sparseness at a point is an average distance
    to the k-nearest neighbors of that point in the behavior space. When this distance
    is high, the point of interest is in the sparse area. At the same time, the denser
    areas are marked by lower values of distance. Thus, sparseness ![](img/98e5abd5-4d97-40fa-94bf-a17c4b8f18e7.png) at
    the point ![](img/61d4f9eb-a296-4600-91c6-cd1617a1a894.png) is given by the following
    formula:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在行为空间中，一个点稀疏性的最直接度量是该点k个最近邻的平均距离。当这个距离较高时，感兴趣的点位于稀疏区域。同时，密集区域以较低的距离值标记。因此，点
    ![](img/61d4f9eb-a296-4600-91c6-cd1617a1a894.png) 的稀疏性 ![](img/98e5abd5-4d97-40fa-94bf-a17c4b8f18e7.png)
    由以下公式给出：
- en: '![](img/f16535a8-dde3-463e-9c03-05103fddcc13.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f16535a8-dde3-463e-9c03-05103fddcc13.png)'
- en: Here, ![](img/6efa9269-ed7d-4d38-bda1-f5d646dd229a.png) is the i-th nearest
    neighbor of  ![](img/bfabb96e-2cfd-4ce2-8d58-feb46d60152a.png), as calculated
    by the distance metric  ![](img/66e6b90d-f6ea-4705-b8d1-97bf89a965f4.png). The
    distance metric is a domain-specific measure of the behavioral difference between
    the two individuals.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/6efa9269-ed7d-4d38-bda1-f5d646dd229a.png) 是根据距离度量 ![](img/66e6b90d-f6ea-4705-b8d1-97bf89a965f4.png)
    计算的 ![](img/bfabb96e-2cfd-4ce2-8d58-feb46d60152a.png) 的第i个最近邻。距离度量是两个个体之间行为差异的特定领域度量。
- en: The candidate individuals from sparse areas receive higher novelty scores. When
    this score exceeds some minimum threshold ![](img/092df855-c373-4f67-b897-1150f7effe54.png),
    the individual at that location is added to the archive of best performers that
    characterize the distribution of prior solutions in the behavior space. The current
    generation of the population, combined with the archive, defines where the search
    has already been and where it is now. Thus, by maximizing the novelty metric,
    the gradient of search is directed toward new behavior, without any explicit objective.
    However, Novelty Search is still driven by meaningful information because exploring
    new behaviors requires comprehensive exploitation of the search domain.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 来自稀疏区域的候选个体获得更高的新颖性评分。当这个评分超过某个最小阈值时！[](img/092df855-c373-4f67-b897-1150f7effe54.png)，该位置的个体将被添加到表现最佳者的存档中，这些最佳表现者表征了行为空间中先前解决方案的分布。当前种群代与存档一起定义了搜索已经进行过的地方以及现在所在的位置。因此，通过最大化新颖性指标，搜索的梯度被引导向新的行为，而不需要任何明确的目标。然而，新颖性搜索仍然由有意义的信息驱动，因为探索新的行为需要全面利用搜索域。
- en: 'The following image shows the Novelty Search algorithm:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了新颖性搜索算法：
- en: '![](img/112dc5f7-ef90-4a8a-bb28-aeecab886517.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/112dc5f7-ef90-4a8a-bb28-aeecab886517.png)'
- en: The Novelty Search algorithm
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖性搜索算法
- en: The Novelty Search optimization method allows evolution to search for solutions
    in any deceptive space and find optimal solutions. With this method, it is possible
    to implement divergent evolution when the population is forced not to converge
    in a particular niche solution (local optima) and have to explore the whole solution
    space. It seems like a very effective search optimization method, despite its
    counterintuitive approach, which completely ignores the explicit objective during
    the search. Moreover, it can find the final solution in most cases even faster
    than a traditional objective-based search that's measuring fitness as a distance
    from the final solution.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 新颖性搜索优化方法允许进化在任意欺骗空间中搜索解决方案并找到最优解。使用这种方法，当种群被迫不在特定利基解决方案（局部最优）中收敛，而必须探索整个解决方案空间时，可以实现发散进化。尽管其方法反直觉，完全忽略了搜索过程中的明确目标，但它似乎是一种非常有效的搜索优化方法。此外，它可以在大多数情况下比测量适应度作为最终解决方案距离的传统基于目标的搜索更快地找到最终解决方案。
- en: For more details, you can refer to the following link: [http://joellehman.com/lehman-dissertation.pdf](http://joellehman.com/lehman-dissertation.pdf).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多详细信息，请参阅以下链接： [http://joellehman.com/lehman-dissertation.pdf](http://joellehman.com/lehman-dissertation.pdf)。
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we began by discussing the different methods that are used
    to train artificial neural networks. We considered how traditional gradient descent-based
    methods differ from neuroevolution-based ones. Then, we presented one of the most
    popular neuroevolution algorithms (NEAT) and the two ways we can extend it (HyperNEAT
    and ES-HyperNEAT). Finally, we described the search optimization method (Novelty
    Search), which can find solutions to a variety of deceptive problems that cannot
    be solved by conventional objective-based search methods. Now, you are ready to
    put this knowledge into practice after setting up the necessary environment, which
    we will discuss in the next chapter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先讨论了用于训练人工神经网络的多种方法。我们考虑了基于传统梯度下降的方法与基于神经进化的方法之间的区别。然后，我们介绍了一种最流行的神经进化算法（NEAT）以及我们可以扩展它的两种方式（HyperNEAT和ES-HyperNEAT）。最后，我们描述了搜索优化方法（新颖性搜索），它可以找到传统基于目标的搜索方法无法解决的多种欺骗问题的解决方案。现在，在设置必要的环境之后，你就可以将所学知识付诸实践了，我们将在下一章中讨论这一点。
- en: In the next chapter, we will cover the libraries that are available so that
    we can experiment with neuroevolution in Python. We will also demonstrate how
    to set up a working environment and what tools are available to manage dependencies
    in the Python ecosystem.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍可用的库，以便我们可以在Python中进行神经进化的实验。我们还将演示如何设置工作环境以及Python生态系统中可用于管理依赖项的工具。
- en: Further reading
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For a deeper understanding of the topics that we discussed in this chapter,
    take a look at the following links:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入地理解本章讨论的主题，请查看以下链接：
- en: '**NEAT**: [http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NEAT**: [http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf)'
- en: '**HyperNEAT**: [https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf](https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HyperNEAT**: [https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf](https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf)'
- en: '**ES-HyperNEAT**: [https://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ES-HyperNEAT**: [https://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)'
- en: '**Novelty Search**: [http://joellehman.com/lehman-dissertation.pdf](http://joellehman.com/lehman-dissertation.pdf)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新颖搜索**: [http://joellehman.com/lehman-dissertation.pdf](http://joellehman.com/lehman-dissertation.pdf)'
