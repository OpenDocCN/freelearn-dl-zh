- en: Overview of Neuroevolution Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of **artificial neural networks** (**ANN**) was inspired by the
    structure of the human brain. There was a strong belief that, if we were able
    to imitate this intricate structure in a very similar way, we would be able to
    create artificial intelligence. We are still on the road to achieving this. Although
    we can implement Narrow AI agents, we are still far from creating a Generic AI
    agent.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter introduces you to the concept of ANNs and the two methods that
    we can use to train them (the gradient descent with error backpropagation and neuroevolution)
    so that they learn how to approximate the objective function. However, we will
    mainly focus on discussing the neuroevolution-based family of algorithms. You
    will learn about the implementation of the evolutionary process that''s inspired
    by natural evolution and become familiar with the most popular neuroevolution
    algorithms: NEAT, HyperNEAT, and ES-HyperNEAT. We will also discuss the methods
    of optimization that we can use to search for final solutions and make a comparison
    between objective-based search and Novelty Search algorithms. By the end of this
    chapter, you will have a complete understanding of the internals of neuroevolution
    algorithms and be ready to apply this knowledge in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary algorithms and neuroevolution-based methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NEAT algorithm overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypercube-based NEAT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolvable-Substrate HyperNEAT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Novelty Search optimization method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary algorithms and neuroevolution-based methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term artificial neural networks stands for a graph of nodes connected by
    links where each of the links has a particular weight. The neural node defines
    a kind of threshold operator that allows the signal to pass only after a specific
    activation function has been applied. It remotely resembles the way in which neurons
    in the brain are organized. Typically, the ANN training process consists of selecting
    the appropriate weight values for all the links within the network. Thus, ANN
    can approximate any function and can be considered as a universal approximator,
    which is established by the Universal Approximation Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the proof of the Universal Approximation Theorem, take
    a look at the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: Cybenko, G. (1989)* Approximations by Superpositions of Sigmoidal Functions,*
    Mathematics of Control, Signals, and Systems, 2(4), 303–314.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Leshno, Moshe; Lin, Vladimir Ya.; Pinkus, Allan; Schocken, Shimon (January
    1993). *Multilayer feedforward networks with a nonpolynomial activation function
    can approximate any function*. Neural Networks. 6 (6): 861–867\. doi:10.1016/S0893-6080(05)80131-5\.
    ([https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/S0893608005801315?via%3Dihub))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurt Hornik (1991) *Approximation Capabilities of Multilayer Feedforward Networks*,
    Neural Networks, 4(2), 251–257\. doi:10.1016/0893-6080(91)90009-T ([https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub](https://www.sciencedirect.com/science/article/abs/pii/089360809190009T?via%3Dihub))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hanin, B. (2018). *Approximating Continuous Functions by ReLU Nets of Minimal
    Width*. arXiv preprint arXiv:1710.11278\. ([https://arxiv.org/abs/1710.11278](https://arxiv.org/abs/1710.11278))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the past 70 years, many ANN training methods have been proposed. However,
    the most popular technique that gained fame in this decade was proposed by Jeffrey
    Hinton. It is based on the backpropagation of prediction error through the network,
    with various optimization techniques built around the gradient descent of the
    loss function with respect to connection weights between the network nodes. It
    demonstrates the outstanding performance of training deep neural networks for
    tasks related mainly to pattern recognition. However, despite its inherent powers,
    it has significant drawbacks. One of these drawbacks is that a vast amount of
    training samples are required to learn something useful from a specific dataset.
    Another significant disadvantage is the fixed network architecture that's created
    manually by the experimenter, which results in inefficient use of computational
    resources. This is due to a significant amount of network nodes not participating
    in the inference process. Also, backpropagation-based methods have problems with
    transferring the acquired knowledge to other similar domains.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside backpropagation methods, there are very promising evolutionary algorithms
    that can address the aforementioned problems. These bio-inspired techniques draw
    inspiration from Darwin's theory of evolution and use natural evolution abstractions
    to create artificial neural networks. The basic idea behind neuroevolution is
    to produce the ANNs by using stochastic, population-based search methods. It is
    possible to evolve optimal architectures of neural networks, which accurately
    address the specific tasks using the evolutionary process. As a result, compact
    and energy-efficient networks with moderate computing power requirements can be
    created. The evolutionary process is executed by applying genetic operators (*mutation*,
    *crossover*) to the population of chromosomes (genetically encoded representations
    of ANNs/solutions) over many generations. The central belief is that since this
    is in biological systems, subsequent generations will be suited to withstand the
    generational pressure that's expressed by the objective function, that is, they
    will become better approximators of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the basic concepts of genetic algorithms. You will need
    to have a moderate level of understanding of genetic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Genetic operators are at the very heart of every evolutionary algorithm, and
    the performance of any neuroevolutionary algorithm depends on them. There are
    two major genetic operators: mutation and crossover (recombination).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the basics of genetic algorithms and how
    they differ from conventional algorithms, which use error backpropagation-based
    methods for training the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: Mutation operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *mutation* operator serves the essential role of preserving the genetic
    diversity of the population during evolution and prevents stalling in the local
    minima when the chromosomes of organisms in a population become too similar. This
    mutation alters one or more genes in the chromosome, according to the mutation
    probability defined by the experimenter. By introducing random changes to the
    solver's chromosome, mutation allows the evolutionary process to explore new areas
    in the search space of possible solutions and find better and better solutions
    over generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the common types of mutation operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a5c1fb7-54b2-4d24-8878-807d27020f44.png)'
  prefs: []
  type: TYPE_IMG
- en: Types of mutation operators
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact type of mutation operator depends on the kind of genetic encoding
    that''s used by a specific genetic algorithm. Among the various mutation types
    we come across, we can distinguish the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bit inversion**: The randomly selected bit, which is inverted (*binary encoding*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Order change**: Two genes are randomly selected and their position is flipped
    in the genome (*permutation encoding*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value change**: A small value is added to the expressed gene at a random
    position (*value encoding*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gene expression change**: A random gene is selected and added/removed from
    the genotype (*structural encoding*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genotypes can be encoded using genetic encoding schemes with fixed and variable
    chromosomal lengths. The first three mutations can be applied to both types of
    encoding schemes. The last mutation can only be expressed in genotypes that have
    been encoded using a variable-length encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Crossover operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *crossover* (recombination) operator allows us to stochastically generate
    new generations (solutions) from existing populations by recombining genetic information
    from two parents to generate offspring. Thus, the portions of good solutions from
    parent organisms can be combined and can potentially lead to better offspring.
    Typically, after a crossover, the produced offspring are mutated before being
    added to the population of the next generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the various crossover operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7d251a0-ed00-4a83-b961-b912bb4b40c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Types of crossover operators
  prefs: []
  type: TYPE_NORMAL
- en: 'The different types of crossover operators also depend on the genetic encoding
    that''s used by particular algorithms, but the following are the most common:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Single-point crossover**: The random crossover point is selected, the genome
    part from the beginning to the crossover point is copied to the offspring from
    one parent, and the rest are copied from another parent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Two-point crossover**: The two crossover points are chosen randomly, the
    part of the genome from the beginning to the first point is copied from the first
    parent, the part between the first and second crossover point is copied from the
    second parent, and the rest are copied from the first parent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uniform crossover**: The genes are copied from the first or second parent
    randomly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Genome encoding schemes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most crucial choices when designing the neuroevolution algorithm
    is to determine the genetic representation of the neural network, which can be
    evolved in the following ways
  prefs: []
  type: TYPE_NORMAL
- en: Standard mutation (see the preceding *Mutation operator* subsection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combination operators (see the preceding *Crossover operator* subsection)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the moment, two major schemes for genome encoding exist: direct and indirect.
    Let''s consider each schema in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: Direct genome encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Direct genome encoding attempts were used in neuroevolution methods to create
    ANNs that were related to neural networks with a fixed topology; that is, the
    network topology was determined solely by the experimenter. Here, genetic encoding
    (*genotype*) is implemented as a vector of real numbers, representing the strength
    (*weights*) of the connections between the network nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The evolutionary operators modify the values of the weights vector with the
    mutation operator and combine the vectors of the parent organisms with the recombination
    (crossover) operator to produce offspring. While allowing evolutionary operators
    to be applied with ease, the described encoding method has some significant drawbacks.
    One of its main drawbacks is that the network topology is determined by the experimenter
    from the very beginning and fixed through all the generations during the execution
    of the algorithm. This approach contradicts the natural evolutionary process,
    in which not only the properties but also the physical structure of the organisms
    change during the evolutionary process. This allows us to explore the broadest
    possible search space and find optimal solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the evolutionary process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a987ef3-6b60-49af-9a0b-38ce6b5b1eda.png)'
  prefs: []
  type: TYPE_IMG
- en: The evolutionary process
  prefs: []
  type: TYPE_NORMAL
- en: To address the drawbacks of the fixed topology methods, Kenneth O. Stanley proposed
    the **NeuroEvolution of Augmenting Topologies** (**NEAT**) method. The primary
    idea behind this algorithm is that the evolutionary operators are applied not
    only to the vector with the weights of all the connections but also to the topology
    of the created neural network. Thus, through generating the populations of the organisms,
    various topologies with a variety of connection weights are tested. We will discuss
    the particulars of the NEAT algorithm later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The NEAT algorithm demonstrates outstanding performance in a variety of tasks
    – from traditional reinforcement learning to the control of sophisticated non-player
    characters in computer games – and has become one of the most popular neuroevolution
    algorithms ever. However, it belongs to the family of direct encoding algorithms,
    which limits its use to evolving only modest-sized ANNs, where parameter space
    is limited to a maximum of thousands of connections. This is because each connection
    is directly encoded in the genotype, and with a large number of encoded connections,
    the computational requirements increase significantly. This makes it impossible
    to use the algorithm to evolve large neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Indirect genome encoding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To overcome size issues with direct encoding, Kenneth O. Stanley proposed an
    *indirect* encoding method, which is inspired by how the phenotype is encoded
    by the genome in the DNA. It is based on the fact that the physical world is built
    around geometry and regularities (structural patterns), where natural symmetries
    are found everywhere. Thus, the encoding size of any physical process can be significantly
    reduced through the reuse of a specific set of encoding blocks for the same structure
    that repeats many times. The proposed method, called **Hypercube-based NeuroEvolution
    of Augmenting Topologies** (**HyperNEAT**), is designed to build large-scale neural
    networks by exploiting geometrical regularities. HyperNEAT employs a connective
    **Compositional Pattern Producing Network** (**CPPN**) to represent node connections
    as a function of Cartesian space. We will discuss HyperNEAT in more detail later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Coevolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In nature, populations of different species often simultaneously evolve in
    mutual interaction with each other. This type of inter-species relationship is
    called **coevolution**. Coevolution is a powerful tool of natural evolution, and
    it is no surprise that it attracted the attention of the neuroevolution community.
    There are three main types of coevolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mutualism**, which is when two or more species coexist and mutually benefit
    from each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Competitive coevolution**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predation**, which is when one organism kills another and consumes its resources.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parasitism**, which is when one organism exploits the resources of another
    but does not kill it.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Commensalism**, which is when the members of one species gain benefits without
    causing harm or gaining benefits from other species.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding coevolution strategies were explored by researchers and their
    pros and cons were revealed. In this book, we will introduce a neuroevolution
    algorithm that employs the commensalistic principle to maintain two coevolving
    populations: the population of candidate solutions and the population of candidate
    objective functions. We will discuss the **Solution and Fitness Evolution** (**SAFE**)
    algorithm later in [Chapter 9](048be1ce-8b6a-48c7-9d13-cb34c8482eb4.xhtml), *Co-Evolution
    and the SAFE Method*.'
  prefs: []
  type: TYPE_NORMAL
- en: Modularity and hierarchy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another crucial aspect of how natural cognitive systems are organized is modularity
    and hierarchy. While studying the human brain, neuroscientists have found that
    it is not a monolithic system with a uniform structure, but rather a complex hierarchy
    of modular structures. Also, due to the speed limitations of signal propagation
    in the biological tissues, the structure of the brain enforces the principle of
    locality when related tasks are processed by geometrically adjacent structures
    in the brain. This aspect of natural systems did not escape the attention of researchers
    of neuroevolution and they are implemented in many evolutionary algorithms. We
    will discuss how modular ANNs can be created using a neuroevolution-based algorithm
    in [Chapter 8](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml), *ES-HyperNEAT and
    the Retina Problems*.
  prefs: []
  type: TYPE_NORMAL
- en: NEAT algorithm overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The method of NEAT for evolving complex ANNs was designed to reduce the dimensionality
    of the parameter search space through the gradual elaboration of the ANN's structure
    during evolution. The evolutionary process starts with a population of small,
    simple genomes (seeds) and gradually increases their complexity over generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The seed genomes have a very simple topology: only input, output, and bias
    neurons are expressed. No hidden nodes are introduced into the seed from the beginning
    to guarantee that the search for a solution starts in the lowest-dimensional parameter
    space (connection weights) possible. With each new generation, additional genes
    are introduced, expanding the solution search space by presenting a new dimension
    that previously did not exist. Thus, evolution begins by searching in a small
    space that can be easily optimized and adds new dimensions when necessary. With
    this approach, complex phenotypes (solutions) can be discovered gradually, step
    by step, which is much more efficient than launching the search directly in the
    vast space of the final solutions. Natural evolution utilizes a similar strategy
    by occasionally adding new genes that make phenotypes more complex. In biology,
    this process of incremental elaboration is called **complexification**.'
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal of the NEAT method is to minimize the complexity of the genome
    structure – not only the final product, but of all the intermediate generations
    of the organisms as well. Thus, the evolution of the network topology results
    in a significant performance advantage by reducing the overall solutions for the
    search space. For example, the high-dimensional space of the final solution is
    only encountered at the end of the evolutionary process. Another essential feature
    of the algorithm is that each structure that's introduced to the genome is the
    subject of subsequent fitness evaluations in the future generations. Also, only
    useful structures will survive during the evolutionary process. In other words,
    the structural complexity of the genome is always goal-justified.
  prefs: []
  type: TYPE_NORMAL
- en: NEAT encoding scheme
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The genetic encoding scheme of NEAT is designed to allow easy matching between
    corresponding genes during the mating process when a crossover operator is applied
    to the two parent genomes. The NEAT genome is a linear representation of the connectivity
    pattern of the encoded neural network, as shown in the following NEAT genome scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b7b32f-0c4e-4871-9773-006c9c4ff857.png)'
  prefs: []
  type: TYPE_IMG
- en: The NEAT genome scheme
  prefs: []
  type: TYPE_NORMAL
- en: 'Each genome is represented as a list of connection genes that encode connections
    between the nodes of the neural network. Also, there are node genes that encode
    information about network nodes, such as the node identifier, node type, and type
    of activation function. The connection gene encodes the following connection parameters
    of the network link:'
  prefs: []
  type: TYPE_NORMAL
- en: The identifier of the input network node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The identifier of the output network node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The strength (weight) of the connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bit, which indicates whether the connection is enabled (expressed) or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An innovation number, which allows matching genes during recombination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom part of the preceding diagram represents a scheme of the same genome
    in the form of a directed graph.
  prefs: []
  type: TYPE_NORMAL
- en: Structural mutations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The mutation operator that''s specific to NEAT can change a connection''s strength
    (weight) and the network''s structure. There are two main types of structural
    mutations:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding a new connection between nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a new node to the network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows the structural mutations of the NEAT algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0db168be-5731-4f21-804b-b16860e4f654.png)'
  prefs: []
  type: TYPE_IMG
- en: The structural mutations of the NEAT algorithm
  prefs: []
  type: TYPE_NORMAL
- en: When the mutation operator is applied to the NEAT genome, the newly added gene
    (connection gene or node gene) is assigned with an increasingly incremented innovation
    number. During the evolutionary process, the genomes of organisms within the population
    gradually get larger and genomes of varying sizes are produced. This process results
    in different connection genes being in the same positions within a genome, making
    the matching process between same-origin genes extremely complicated.
  prefs: []
  type: TYPE_NORMAL
- en: Crossover with an innovation number
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is a piece of unexploited information in the evolutionary process that
    tells us exactly which genes to match between the genomes of any organism in the
    topologically diverse population. This is where each gene tells us which ancestor
    that gene was derived from. The connection genes with the same historical origin
    represent the same structure, despite possibly having different connection weight
    values. The historical origins of genes in the NEAT algorithm are represented
    by incrementally assigned innovation numbers, which allow us to track the chronology
    of structural mutations.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, during the crossover, the offspring inherit the innovation
    numbers of genes from parent genomes. Thus, the innovation number of specific
    genes never change, allowing similar genes from different genomes to be matched
    during the crossover. The innovation numbers of matched genes are the same. If
    the innovation numbers do not match, the gene belongs to the *disjoint* or *excess*
    part of the genome, depending on whether its innovation number lies inside of,
    or outside of, the range of other parent innovation numbers. The disjoint or excess
    genes represent structures that are not present in the genome of the other parent
    and require special handling during the crossover phase. Thus, the offspring inherits
    genes that have the same innovation number. These are randomly chosen from one
    of the parents. The offspring always inherit the disjoint or excess genes from
    the parent with the highest fitness. This feature allows a NEAT algorithm to efficiently
    perform gene recombination using linear genome encoding, without the need for
    complex topological analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the crossover (recombination) in the NEAT algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e0a3210-228f-46c1-a3f7-3d4baeff1635.png)'
  prefs: []
  type: TYPE_IMG
- en: Crossover (recombination) in the NEAT algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram shows an example of a crossover between two parents using
    the NEAT algorithm. The genomes of both parents are aligned using the innovation
    numbers (the number at the top of the connection gene cell). After that, the offspring
    is produced by randomly choosing connection genes from either of parents when
    the innovation numbers are the same: the genes with innovation numbers from one
    to five. Finally, the disjoint and excess genes are added from either of the parents
    unconditionally and ordered by innovation number.'
  prefs: []
  type: TYPE_NORMAL
- en: Speciation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the evolutionary process, organisms can create diverse topologies through
    generations, but they fail to produce and maintain topological innovations of
    their own. The smaller network structures optimize faster than larger ones, which
    artificially reduces the chances of survival of a descendant genome after adding
    a new node or connection to it. Thus, the freshly augmented topologies experience
    negative evolutionary pressure due to the temporary decrease of fitness of the
    organisms within the population. At the same time, novel topologies can introduce
    innovations that lead to a winning solution in the long run. To address the temporal
    drop of fitness, the concept of speciation was introduced in the NEAT algorithm.
    The speciation limits the range of organisms that can mate by introducing narrow
    niches where only organisms that belong to the same niche compete with each other
    during the crossover, instead of competing with all the organisms in the population.
    Speciation is implemented by dividing the population so that organisms with a
    similar topology belong to the same species.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s refer to the following speciation algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5150acd-027d-4972-b813-c4137022868f.png)'
  prefs: []
  type: TYPE_IMG
- en: The speciation algorithm
  prefs: []
  type: TYPE_NORMAL
- en: The NEAT method permits the creation of complex ANNs that are capable of solving
    a variety of control optimization problems, as well as other unsupervised learning
    problems. Due to the introduced specifics of ANN topology augmentation through
    complexification and speciation, the solutions tend to optimize the performance
    of training and inference. The resulting ANN topology grows to match the problem
    that needs to be solved, without any excess layers of hidden units being introduced
    by the conventional methods of ANN's topology design for backpropagation-based
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'More details about the NEAT algorithm can be found in the original paper: [http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Hypercube-based NEAT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Intelligence is a product of the brain, and the human brain as a structure is itself
    a product of natural evolution. Such an intricate structure has evolved over millions
    of years, under pressure from harsh environments, and while competing with other
    living beings for survival. As a result, an extremely complex structure has evolved,
    with many layers, modules, and trillions of connections between neurons. The structure
    of the human brain is our guiding star and is aiding our efforts in creating artificial
    intelligence systems. However, how can we address all the complexity of the human
    brain with our imperfect instruments?
  prefs: []
  type: TYPE_NORMAL
- en: By studying the human brain, neuroscientists have found that its spatial structure
    plays an essential role in all perceiving and cognitive tasks – from vision to
    abstract thinking. Many intricate geometric structures have been found, such as
    the grid cells that help us with inertial navigation, and the cortical columns
    that are connected to the eye's retina to process visual stimuli. It has been
    demonstrated that the structure of the brain allows us to effectively respond
    to the patterns in signals that are received from the sensorium by using designated
    neural structures that are activated by specific patterns in the inputs. This
    feature of the brain allows it to use an extremely efficient way of representing
    and processing the entire diversity of the input data that's obtained from the
    environment. Our brains have evolved to be effective pattern recognition and pattern
    processing engines that actively reuse specific neural modules to process particular
    patterns, thus dramatically reducing the number of different neural structures
    required. This only became possible due to the complex modular hierarchy and the
    spatial integration of its various parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned previously, the biological brain incorporates complex hierarchical
    and spatially-aware data processing routines. This has inspired the researchers
    of neuroevolution to introduce similar data processing methods in the field of
    artificial neural networks. When designing such systems, it is necessary to address
    the following problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The vast number of input features and training parameters that require large-scale
    ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effective representation of natural geometrical regularities and symmetries
    that are observed in the physical world
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effective processing of input data through the introduction of the locality
    principle, that is, when spatially/semantically adjacent data structures are processed
    by the modules of interconnected neural units, which occupy the same compact area
    of the entire network structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, you learned about the** Hypercube-based NeuroEvolution of Augmenting
    Topologies** (**HyperNEAT**) method, which was proposed by Kenneth O. Stanley
    to solve various problems by exploiting geometrical regularities. In the next
    section, we will look at **Compositional Pattern Producing Networks** (**CPPNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Compositional Pattern Producing Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HyperNEAT extends the original NEAT algorithm by introducing a new type of indirect
    genome encoding scheme called CPPNs. This type of encoding makes it possible to
    represent the connectivity patterns of a phenotype's ANN as a function of its
    geometry.
  prefs: []
  type: TYPE_NORMAL
- en: 'HyperNEAT stores the connectivity pattern of the phenotype neural network as
    a four-dimensional hypercube, where each point encodes the connection between
    two nodes (that is, the coordinates of the source and target neurons) and the
    connective CPPN paints various patterns within it. In other words, CPPN computes
    the four-dimensional function, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0bff394f-591a-4f1f-a0ee-5ad0267c4329.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the source node is at (*x[1]*, *y[1]*) and the target node is at (*x[2]*,
    *y[2]*). At this stage, CPPN returns a weight for every connection between every
    node in the phenotype network, which is represented as a grid. By convention,
    the connection between the two nodes is not expressed if the magnitude of the
    connection weight that's computed by CPPN is less than a minimum threshold (*w[min]*).
    That way, the connectivity pattern that's produced by CPPN can represent any network
    topology. The connectivity pattern can be used to encode large-scale ANNs by discovering
    regularities in the training data and can reuse the same set of genes to encode
    repetitions. By convention, the connectivity pattern that's produced by CPPN is
    called the **substrate**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the interpretation of the Hypercube-based Geometric
    Connectivity Pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4823fb2d-40e2-4eb0-b06b-a49250416730.png)'
  prefs: []
  type: TYPE_IMG
- en: Hypercube-based Geometric Connectivity Pattern interpretation
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional ANN architectures, CPPN employs a set of various activation
    functions for its hidden nodes to explore a variety of geometrical regularities.
    For example, the trigonometric sine can be used to represent repetitions, while
    Gaussian can be used to enforce locality at a specific part of the network (that
    is, symmetry along the coordinate axis). Thus, the CPPN encoding scheme can represent
    patterns with different geometrical regularities such as symmetry, repetition,
    repetition with regularities, and so on in a compact manner.
  prefs: []
  type: TYPE_NORMAL
- en: Substrate configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The layout of the network nodes in the substrate that CPPN connects to can take
    various forms, which are best suited to different kinds of problems. It is the
    responsibility of the experimenter to select the appropriate layout to achieve
    optimal performance. For example, the output nodes that control a radial entity
    such as a six-leg crawler may be best laid out with radial geometry so that a
    connectivity pattern can be expressed with polar coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows some examples of substrate layout configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd6be905-d416-463e-98b2-9f26beb88416.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of substrate layout configurations
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several common types of substrate layout that are typically used
    with HyperNEAT (see the preceding diagram), some of which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-dimensional grid**: A regular grid of network nodes in a two-dimensional
    Cartesian space centered at (0, 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Three-dimensional grid**: A regular grid of network nodes in a three-dimensional
    Cartesian space centered at (0, 0, 0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State-Space Sandwich**: Two 2D planar grids with source and target nodes
    in which one layer can send connections in the direction of the other one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Circular**: The regular radial structure, which is suited to define regularities
    in radial geometry-based polar coordinates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolving connective CPPNs and the HyperNEAT algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The method is called **HyperNEAT** because it uses a modified NEAT to evolve
    CPPNs that represent spatial patterns in the hyperspace. Each expressed point
    of the pattern, which is bounded by a hypercube, represents a connection between
    two nodes in the lower-dimensional graph (substrate). Thus, the dimensionality
    of the hyperspace is twice as big as the dimensionality of the underlying lower-dimensional
    graph. Later in [Chapter 8](9f3dce4d-2cc7-4307-a704-bfcfe4ad56b4.xhtml), *ES-HyperNEAT
    and the Retina Problem*, we will look at some examples that use two-dimensional
    connectivity patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The HyperNEAT algorithm can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35641a49-7e0d-4127-999d-623852a4f8e8.png)'
  prefs: []
  type: TYPE_IMG
- en: The general form of the HyperNEAT algorithm
  prefs: []
  type: TYPE_NORMAL
- en: Any connection gene or node gene that's added to the connective CPPN during
    its evolution leads to the discovery of a new global dimension of variation in
    the connectivity patterns across the phenotype substrate (novel traits). Each
    modification that's made to the CPPN genome represents a new way that an entire
    connectivity pattern can vary. Also, previously evolved connective CPPNs can be
    queried to produce connectivity patterns for the substrate at a higher resolution
    than what was used for its training. This allows us to produce a working solution
    to the same problem at any resolution, potentially without an upper limit. Thus,
    the aforementioned properties have made HyperNEAT a powerful instrument in evolving
    large-scale bio-inspired artificial neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the HyperNEAT method, you can refer to the following
    link: [https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf](https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Evolvable-Substrate HyperNEAT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The HyperNEAT method exposes the fact that geometrical regularities of the natural
    world can be adequately represented by artificial neural networks with nodes placed
    at specific spatial locations. That way, the neuroevolution gains significant
    benefits and it allows large-scale ANNs to be trained for high dimensional problems,
    which was impossible with the ordinary NEAT algorithm. At the same time, the HyperNEAT
    approach is inspired by the structure of a natural brain, which still lacks the
    plasticity of the natural evolution process. While allowing the evolutionary process
    to elaborate on a variety of connectivity patterns between network nodes, the
    HyperNEAT approach exposes a hard limitation on where the network nodes are placed.
    The experimenter must define the layout of the network nodes from the very beginning,
    and any incorrect assumption that's made by the researcher will lower the performance
    of the evolutionary process.
  prefs: []
  type: TYPE_NORMAL
- en: By placing the network node at a specific location in the substrate, the experimenter
    creates an unintentional constraint on the pattern of weights that are produced
    by the CPPN. This restriction then interferes with the CPPN when it attempts to
    encode the geometrical regularities of the natural world into the topography of
    solution-producing ANN (phenotype). Here, the connectivity pattern produced by
    CPPN must perfectly align with the layout of the substrate that is defined by
    the experimenter; connections only are possible between given network nodes. Such
    limitation leads to unnecessary approximation errors, which spoil the outcome.
    It may be more effective for the CPPN to elaborate connectivity patterns over
    nodes that have been placed at slightly different locations.
  prefs: []
  type: TYPE_NORMAL
- en: Information patterns in the hypercube
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why should such limitations on the location of nodes be imposed in the first
    place? Wouldn't it be nice if the implicit clues that had been drawn from the
    connectivity patterns became the guidelines of where to place the next node to
    represent the natural regularities of the physical world better?
  prefs: []
  type: TYPE_NORMAL
- en: The areas with uniform connection weights encode a small amount of information
    and hence have little functional value. At the same time, the areas with vast
    gradients of weight values are extremely information-intensive. Such areas can
    benefit from additional network nodes being placed to represent a much finer encoding
    of the natural process. As you may recall from our discussion of the HyperNEAT
    algorithm, it is possible to represent the connection between two nodes in the
    substrate as a point in a four-dimensional hypercube. Thus, the main feature of
    the proposed ES-HyperNEAT algorithm is to express more hyper-points in the areas
    of the hypercube, where the high variation of connection weights are detected.
    At the same time, the fewer hyper-points are placed in the areas with a lower
    variation of connection weights.
  prefs: []
  type: TYPE_NORMAL
- en: The placement of nodes and the exposed connections between them can be dictated
    by the variation in the weights of connections that are produced by the evolving
    CPPN for a given region of a substrate. In other words, there is no need for additional
    information to decide on the next node placement in the substrate, other than
    what we are already receiving from the CPPN that encodes the connectivity patterns
    of the network. Information density becomes the main guiding principle for the
    algorithm to determine the topography of the substrate.
  prefs: []
  type: TYPE_NORMAL
- en: Node placement in the phenotype ANN signifies where the information is encoded
    in the connectivity patterns that are created by the CPPN.
  prefs: []
  type: TYPE_NORMAL
- en: Quadtree as an effective information extractor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To represent hyper-points that are encoding connection weights within the hypercube,
    the ES-HyperNEAT algorithm employs a *quadtree*. A quadtree is a tree data structure
    in which each internal node has exactly four children nodes. This data structure
    was selected due to its inherent properties, allowing it to represent two-dimensional
    areas at different levels of granularity. With a quadtree, it is possible to organize
    an effective search through the two-dimensional space by splitting any area of
    interest into four subareas, and each of them becomes a leaf of the tree, with
    root (parent) node representing the original (decomposed) region.
  prefs: []
  type: TYPE_NORMAL
- en: Using the quadtree-based information extraction method, the ES-HyperNEAT approach
    iteratively looks for new connections between nodes in the two-dimensional space
    of the substrate ANN, starting from the input and output nodes that have been
    predefined by the experimenter. This method is much more computationally effective
    than searching directly in the four-dimensional hypercube space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows an example of extracting information using quadtree
    data structures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f64241b-221e-403a-806a-41c94354d8af.png)'
  prefs: []
  type: TYPE_IMG
- en: Quadtree information extraction example
  prefs: []
  type: TYPE_NORMAL
- en: 'The quadtree-based search algorithm operates in two main stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Division and initialization**: At this stage, the quadtree is created by
    recursively subdividing the initial substrate space occupying the area from (-1,
    -1) to (1, 1). The subdivision stops when the desired tree depth is reached. This
    implicitly determines how many subspaces fit into the initial space of the substrate
    (initialization resolution). After that, for every quadtree node with the center
    at ![](img/81ca5f1c-7d61-4d46-9d54-0f9a1671bf80.png), the CPPN is queried with ![](img/b192864b-1e82-4114-8c13-2d7cd7fe6310.png) arguments
    to find connection weights. When connection weights for ![](img/eb93f7bf-1498-4e2d-8d1a-b63f2b397304.png)
    leaf nodes of a particular quadtree node ![](img/70e29e27-edfd-4fca-b88f-a0a9e0012f3f.png)
    are found, the variance of this node can be calculated by the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a4e2dc7c-beee-49c5-af45-0886c166b8f3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here ![](img/06860371-dbc5-4efc-9a50-b958b744c471.png) is the mean connection
    weight among ![](img/531eab51-4128-4d34-a8ac-d128a4699802.png) leaf nodes and ![](img/ddba058a-ccaa-4c47-adbd-25886edeb8ae.png) is
    a connection weight to a specific leaf node. The calculated variance value is
    a heuristic indicator of the presence of information in the specific substrate
    area. If this value is higher than the particular division threshold (defining
    desired information density), then the division stage can be repeated for the
    corresponding square of the substrate. This way, the desired information density
    can be enforced by the algorithm. Take a look at the top part of the preceding
    diagram for a visual insight into how division and initialization are done using
    the quadtree data structures.
  prefs: []
  type: TYPE_NORMAL
- en: '**Pruning and extraction**: To guarantee that more connections (and nodes in
    the substrate) become expressed in the regions with high information density (high
    weights variance), the pruning and extraction procedure is executed over the quadtree
    that was generated during the previous stage. The quadtree traverses depth-first
    until the variance of the current node is smaller than a variance threshold ![](img/9213a181-6ab8-46bf-9a55-4d1be8b0ba88.png) or
    until the node has no children (the zero variance). For every qualified node,
    the connection is expressed between its center ![](img/7d06e8bd-2222-4af3-ab99-45da675f2121.png) and
    each parent node is already defined, either by the experimenter or found at the
    previous run of these two stages (that is, from hidden nodes that have already
    been created by ES-HyperNEAT method). Refer to the bottom part of the preceding
    diagram for a visual insight into how the pruning and extraction phase works.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ES-HyperNEAT algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ES-HyperNEAT algorithm starts with user-defined input nodes and elaborates
    on exploring connections from them and sending them to the newly expressed hidden
    nodes. The expression of outgoing connectivity patterns and hidden nodes placement
    within the substrate space is done using the quadtree information extraction method,
    which we described previously. The information extraction process is iteratively
    applied until the desired level of information expression density is achieved,
    or until no more information can be discovered in the hypercube. After that, the
    resulting network is connected to the user-defined output nodes by expressing
    the incoming connectivity patterns to the outputs. We use quadtree information
    extraction for this as well. Only those hidden nodes are kept in the final network,
    which has a path to both the input and output nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have defined a multitude of nodes and connections within the substrate
    of the phenotype ANN. It can be beneficial to remove some nodes from the network
    by introducing an additional band pruning processing stage. At this stage, we
    keep only the points within a specific band and remove points on the edge of the
    band. By making bands broader or narrower, the CPPN can manage the density of
    the encoded information. For more details about band pruning, please refer to
    the *ES-HyperNEAT paper* ([https://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following ES-HyperNEAT algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f194296-5fae-4524-a44c-4fffe328680e.png)'
  prefs: []
  type: TYPE_IMG
- en: The ES-HyperNEAT algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 'The ES-HyperNEAT algorithm takes all the advantages of the NEAT and HyperNEAT
    methods and introduces even more powerful new features, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic placement of the hidden nodes within the substrate to precisely match
    the connectivity patterns that are expressed by evolved CPPNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows us to produce modular phenotype ANNs much more easily due to its inherent
    capabilities to start the evolutionary search with a bias toward locality (by
    the specific design of the initial CPPN architectures).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With ES-HyperNEAT, it is possible to elaborate on the existing phenotype ANN
    structure by increasing the number of nodes and connections in the substrate during
    evolution. This is the opposite of HyperNEAT, where the number of substrate nodes
    is predefined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ES-HyperNEAT algorithm allows us to use the original HyperNEAT architecture
    without altering the genetic structure of the NEAT part. It allows us to address
    problems that are hard to solve with the HyperNEAT algorithm due to difficulties
    with creating an appropriate substrate configuration in advance.
  prefs: []
  type: TYPE_NORMAL
- en: More details about the ES-HyperNEAT algorithm and the motivations behind it
    can be found at [h](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)[ttps://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Novelty Search optimization method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the machine learning methods, including evolutionary algorithms, base
    their training on the optimization of the objective function. The main focus underlying
    the methods of optimization of the objective function is that the best way to
    improve the performance of a solver is to reward them for getting closer to the
    goal. In most evolutionary algorithms, the closeness to the goal is measured by
    the fitness of the solver. The measure of an organism's performance is defined
    by the fitness function, which is a metaphor for evolutionary pressure on the
    organism to adapt to its environment. According to that paradigm, the fittest
    organism is better adapted to its environment and best suited to find a solution.
  prefs: []
  type: TYPE_NORMAL
- en: While direct fitness function optimization methods work well in many simple
    cases, for more complex tasks, it often falls victim to the local optima trap.
    Convergence to the local optima means that no local step in the search space provides
    any improvements during the fitness function optimization process. The traditional
    genetic algorithms use mutation and island mechanisms to escape from such local
    optima. However, as we will find out by doing experiments later in this book,
    it may not always help with deceptive problems, or it can take too long to find
    a successful solution.
  prefs: []
  type: TYPE_NORMAL
- en: Many real-world problems have such deceptive fitness function landscapes that
    cannot be solved by an optimization process that is based solely on measuring
    how close the current solution is to the goal. As an example, we can consider
    the task of navigating through an unknown city with an irregular street pattern.
    In such a task, heading toward the destination often means traveling along deceptive
    roads that move you further away, only to bring you to the destination after several
    twists. But if you decide to start with roads that have been aligned in direction
    to the destination, it often leads you to a dead end, while the destination is
    just behind the wall but unreachable.
  prefs: []
  type: TYPE_NORMAL
- en: Novelty Search and natural evolution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By looking at how natural selection works in the physical world, we can see
    that the enabling force behind evolutionary diversity is a search for novelty.
    In other words, any evolving species gains immediate evolutionary advantages over
    its rivals by finding new behavior patterns. This allows them to exploit the environment
    more efficiently. The natural evolution has no defined goals, and it broadens
    the solution search space by rewarding the exploration and exploitation of novel
    behaviors. This novelty can be considered as a proxy for many hidden creative
    forces in the natural world, which allows evolution to elaborate on even more
    complex behaviors and biological structures.
  prefs: []
  type: TYPE_NORMAL
- en: Taking inspiration from the natural evolution, *Joel Lehman* proposed a new
    method of search optimization for an artificial evolutionary process called **Novelty
    Search**. With this method, no particular fitness function is defined or used
    for solution search; instead, the novelty of each found solution is directly rewarded
    during the neuroevolution process. Thus, the novelty of the solutions that are found guide
    the neuroevolution toward the final goal. Such an approach gives us a chance to
    exploit the creative forces of evolution independent of the adaptive pressure
    to fit the solution into a particular niche.
  prefs: []
  type: TYPE_NORMAL
- en: The effectiveness of a Novelty Search can be demonstrated with the *maze navigation*
    experiment, where an objective-based search finds the solution for the simple
    maze in many more steps (generations) than a Novelty Search. Furthermore, for
    the hard maze with deceptive configuration, the objective-based search fails to
    find any solution at all. We will discuss maze navigation experiments later in
    [Chapter 5](22365f85-3003-4b67-8e1e-cc89fa5e259b.xhtml), *Autonomous Maze Navigation*.
  prefs: []
  type: TYPE_NORMAL
- en: Novelty metric
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Novelty Search method employs a novelty metric for tracking the uniqueness
    of the behavior of each new individual. That is, the novelty metric is a measure
    of how far the new organism is from the rest of the population in the behavior
    space. An effective novelty metric implementation should allow us to compute sparseness
    at any point of the behavior space. Any area with a denser cluster of visited
    points is less novel and produces less evolutionary rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most straightforward measure of sparseness at a point is an average distance
    to the k-nearest neighbors of that point in the behavior space. When this distance
    is high, the point of interest is in the sparse area. At the same time, the denser
    areas are marked by lower values of distance. Thus, sparseness ![](img/98e5abd5-4d97-40fa-94bf-a17c4b8f18e7.png) at
    the point ![](img/61d4f9eb-a296-4600-91c6-cd1617a1a894.png) is given by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f16535a8-dde3-463e-9c03-05103fddcc13.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/6efa9269-ed7d-4d38-bda1-f5d646dd229a.png) is the i-th nearest
    neighbor of  ![](img/bfabb96e-2cfd-4ce2-8d58-feb46d60152a.png), as calculated
    by the distance metric  ![](img/66e6b90d-f6ea-4705-b8d1-97bf89a965f4.png). The
    distance metric is a domain-specific measure of the behavioral difference between
    the two individuals.
  prefs: []
  type: TYPE_NORMAL
- en: The candidate individuals from sparse areas receive higher novelty scores. When
    this score exceeds some minimum threshold ![](img/092df855-c373-4f67-b897-1150f7effe54.png),
    the individual at that location is added to the archive of best performers that
    characterize the distribution of prior solutions in the behavior space. The current
    generation of the population, combined with the archive, defines where the search
    has already been and where it is now. Thus, by maximizing the novelty metric,
    the gradient of search is directed toward new behavior, without any explicit objective.
    However, Novelty Search is still driven by meaningful information because exploring
    new behaviors requires comprehensive exploitation of the search domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the Novelty Search algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/112dc5f7-ef90-4a8a-bb28-aeecab886517.png)'
  prefs: []
  type: TYPE_IMG
- en: The Novelty Search algorithm
  prefs: []
  type: TYPE_NORMAL
- en: The Novelty Search optimization method allows evolution to search for solutions
    in any deceptive space and find optimal solutions. With this method, it is possible
    to implement divergent evolution when the population is forced not to converge
    in a particular niche solution (local optima) and have to explore the whole solution
    space. It seems like a very effective search optimization method, despite its
    counterintuitive approach, which completely ignores the explicit objective during
    the search. Moreover, it can find the final solution in most cases even faster
    than a traditional objective-based search that's measuring fitness as a distance
    from the final solution.
  prefs: []
  type: TYPE_NORMAL
- en: For more details, you can refer to the following link: [http://joellehman.com/lehman-dissertation.pdf](http://joellehman.com/lehman-dissertation.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began by discussing the different methods that are used
    to train artificial neural networks. We considered how traditional gradient descent-based
    methods differ from neuroevolution-based ones. Then, we presented one of the most
    popular neuroevolution algorithms (NEAT) and the two ways we can extend it (HyperNEAT
    and ES-HyperNEAT). Finally, we described the search optimization method (Novelty
    Search), which can find solutions to a variety of deceptive problems that cannot
    be solved by conventional objective-based search methods. Now, you are ready to
    put this knowledge into practice after setting up the necessary environment, which
    we will discuss in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the libraries that are available so that
    we can experiment with neuroevolution in Python. We will also demonstrate how
    to set up a working environment and what tools are available to manage dependencies
    in the Python ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For a deeper understanding of the topics that we discussed in this chapter,
    take a look at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NEAT**: [http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HyperNEAT**: [https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf](https://eplex.cs.ucf.edu/papers/stanley_alife09.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ES-HyperNEAT**: [https://eplex.cs.ucf.edu/papers/risi_alife12.pdf](https://eplex.cs.ucf.edu/papers/risi_alife12.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Novelty Search**: [http://joellehman.com/lehman-dissertation.pdf](http://joellehman.com/lehman-dissertation.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
