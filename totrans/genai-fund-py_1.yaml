- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: 'Understanding Generative AI: An Introduction'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解生成式AI：入门
- en: In his influential book *The Singularity Is Near* (2005), renowned inventor
    and futurist Ray Kurzweil asserted that we were on the precipice of an exponential
    acceleration in technological advancements. He envisioned a future where technological
    innovation would continue to accelerate, eventually leading to a **singularity**—a
    point where **artificial intelligence** (**AI**) could transcend human intelligence,
    blurring the lines between humans and machines. Fast-forward to today and we find
    ourselves advancing along the trajectory Kurzweil outlined, with generative AI
    marking a significant stride along this path. Today, we are experiencing state-of-the-art
    generative models can behave as collaborators capable of synthetic understanding
    and generating sophisticated responses that mirror human intelligence.. The rapid
    and exponential growth of generative approaches is propelling Kurzweil’s vision
    forward, fundamentally reshaping how we interact with technology.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在他影响深远的著作《奇点临近》（2005年）中，著名发明家和未来学家雷·库兹韦尔断言，我们正处于技术进步指数级加速的边缘。他设想了一个未来，其中技术创新将继续加速，最终导致一个**奇点**——一个**人工智能**（AI）能够超越人类智能、模糊人类与机器之间界限的点。快进到今天，我们发现自己在沿着库兹韦尔描绘的轨迹前进，生成式AI标志着这一路径上的重大进步。今天，我们正经历着最先进的生成模型能够作为协作伙伴，具备合成理解和生成反映人类智能的复杂响应。生成方法的快速和指数级增长正在推动库兹韦尔愿景的实现，从根本上重塑了我们与技术互动的方式。
- en: In this chapter, we lay the conceptual groundwork for anyone hoping to apply
    generative AI to their work, research, or field of study, broadening a fundamental
    understanding of what this technology does, how it was derived, and how it can
    be used. It establishes how generative models differ from classical **machine
    learning** (**ML**) paradigms and elucidates how they discern complex relationships
    and idiosyncrasies in data to synthesize human-like text, audio, and video. We
    will explore critical foundational generative methods, such as generative adversarial
    networks (GANs), diffusion models, and transformers, with a particular emphasis
    on their real-world applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为任何希望将生成式AI应用于其工作、研究或研究领域的人打下概念基础，扩展了对这项技术做什么、它是如何产生的以及如何使用的根本理解。它阐述了生成模型与经典**机器学习**（ML）范式之间的区别，并阐明了它们如何识别数据中的复杂关系和独特性，以合成类似人类的文本、音频和视频。我们将探讨关键的基础生成方法，如生成对抗网络（GANs）、扩散模型和变换器，并特别强调它们的实际应用。
- en: Additionally, this chapter hopes to dispel some common misunderstandings surrounding
    generative AI and provides guidelines to adopt this emerging technology ethically,
    considering its environmental footprint and advocating for responsible development
    and adoption. We will also highlight scenarios where generative models are apt
    for addressing business challenges. By the conclusion of this chapter, we will
    better understand the potential of generative AI and its applications across a
    wide array of sectors and have critically assessed the risks, limitations, and
    long-term considerations.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本章还希望消除围绕生成式AI的一些常见误解，并提供采用这项新兴技术的道德指南，考虑到其环境影响，并倡导负责任的发展和采用。我们还将强调生成模型在解决商业挑战方面的适用场景。到本章结束时，我们将更好地理解生成式AI的潜力及其在广泛领域的应用，并批判性地评估了风险、局限性和长期考虑。
- en: Whether your interest is casual, you are a professional transitioning from a
    different field, or you are an established practitioner in the fields of data
    science or ML, this chapter offers a contextual understanding to make informed
    decisions regarding the responsible adoption of generative AI.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你的兴趣是业余的，你是在从不同领域过渡的专业人士，还是数据科学或ML领域的资深从业者，本章都提供了对生成式AI负责任采用的有益理解，以做出明智的决定。
- en: Ultimately, we aim to establish a foundation through an introductory exploration
    of generative AI and **large language models** (**LLMs**), dissected into two
    parts.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们希望通过介绍生成式AI和**大型语言模型**（LLMs）的入门探索来建立一个基础，分为两部分。
- en: The beginning of the book will introduce the fundamentals and history of generative
    AI, surveying various types, such as GANs, diffusers, and transformers, tracing
    the foundations of **natural language generation** (**NLG**), and demonstrating
    the basic steps to implement generative models from prototype to production. Moving
    forward, we will focus on slightly more advanced application fundamentals, including
    fine-tuning generative models, prompt engineering, and addressing ethical considerations
    toward the responsible adoption of generative AI. Let’s get started.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 书的开头将介绍生成式AI的基础和历史，概述各种类型，例如生成对抗网络（GANs）、扩散器（diffusers）和转换器（transformers），追溯**自然语言生成**（**NLG**）的基础，并展示从原型到生产的实现生成模型的基本步骤。接下来，我们将关注稍微更高级的应用基础，包括微调生成模型、提示工程以及针对生成AI负责任采用的伦理考量。让我们开始吧。
- en: Generative AI
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式AI
- en: In recent decades, AI has made incredible strides. The origins of the field
    stem from classical statistical models meticulously designed to help us analyze
    and make sense of data. As we developed more robust computational methods to process
    and store data, the field shifted—intersecting computer science and statistics
    and giving us ML. ML systems could learn complex relationships and surface latent
    insights from vast amounts of data, transforming our approach to statistical modeling.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近几十年里，人工智能取得了惊人的进步。该领域的起源可以追溯到精心设计的经典统计模型，旨在帮助我们分析和理解数据。随着我们开发了更强大的计算方法来处理和存储数据，该领域发生了转变——交汇于计算机科学和统计学，为我们带来了机器学习（ML）。ML系统可以从大量数据中学习复杂关系并揭示潜在见解，从而改变我们统计建模的方法。
- en: This shift laid the groundwork for the rise of deep learning, a substantial
    step forward that introduced multi-layered neural networks (i.e., a system of
    interconnected functions) to model complex patterns. Deep learning enabled powerful
    discriminative models that became pivotal for advancements in diverse fields of
    research, including image recognition, voice recognition, and natural language
    processing.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转变为深度学习的兴起奠定了基础，这是一次实质性的进步，它引入了多层神经网络（即相互连接的函数系统）来模拟复杂模式。深度学习使强大的判别式模型成为各个研究领域进步的关键，包括图像识别、语音识别和自然语言处理。
- en: However, the journey continues with the emergence of generative AI. Generative
    AI harnesses the power of deep learning to accomplish a broader objective. Instead
    of classifying and discriminating data, generative AI seeks to learn and replicate
    data distributions to “create” entirely new and seemingly original data, mirroring
    human-like output.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着生成式AI的出现，旅程仍在继续。生成式AI利用深度学习的力量来实现更广泛的目标。它不是对数据进行分类和区分，而是寻求学习和复制数据分布，以“创建”全新且看似原创的数据，类似于人类的输出。
- en: Distinguishing generative AI from other AI models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 区分生成式AI与其他AI模型
- en: Again, the critical distinction between discriminative and generative models
    lies in their objectives. Discriminative models aim to predict target outputs
    given input data. Classification algorithms, such as logistic regression or support
    vector machines, find decision boundaries in data to categorize inputs as belonging
    to one or more class. Neural networks learn input-output mappings by optimizing
    weights through backpropagation (or tracing back to resolve errors) to make accurate
    predictions. Advanced gradient boosting models, such as XGBoost or LightGBM, further
    enhance these discriminative models by employing decision trees and incorporating
    the principles of gradient boosting (or the strategic ensembling of models) to
    make highly accurate predictions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，判别式模型和生成式模型之间的关键区别在于它们的目标。判别式模型旨在根据输入数据预测目标输出。例如，分类算法，如逻辑回归或支持向量机，在数据中找到决策边界，将输入分类为一类或多类。神经网络通过反向传播（或回溯以解决错误）优化权重，学习输入输出映射，以做出准确的预测。高级梯度提升模型，如XGBoost或LightGBM，通过采用决策树和结合梯度提升（或模型的战略集成）的原则，进一步增强了这些判别式模型，以做出高度准确的预测。
- en: Generative methods learn complex relationships through expansive training in
    order to generate new data sequences enabling many downstream applications. Effectively,
    these models create synthetic outputs by replicating the statistical patterns
    and properties discovered in training data, capturing nuances and idiosyncrasies
    that closely reflect human behaviors.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式方法通过广泛的训练学习复杂关系，以生成新的数据序列，从而实现许多下游应用。实际上，这些模型通过复制在训练数据中发现的数据的统计模式和属性来创建合成输出，捕捉细微差别和独特性，这些与人类行为紧密相关。
- en: In practice, a discriminative image classifier labels images containing a cat
    or a dog. In contrast, a generative model can synthesize diverse, realistic cat
    or dog images by learning the distributions of pixels and implicit features from
    existing images. Moreover, generative models can be trained across modalities
    to unlock new possibilities in synthesis-focused applications to generate human-like
    photographs, videos, music, and text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，判别性图像分类器为包含猫或狗的图像贴标签。相比之下，生成模型可以通过学习现有图像中的像素分布和隐含特征来合成多样化的、逼真的猫或狗图像。此外，生成模型可以在不同模态上进行训练，以在以合成为重点的应用中解锁新的可能性，生成类似人类的照片、视频、音乐和文本。
- en: There are several key methods that have formed the foundation for many of the
    recent advancements in Generative AI, each with unique approaches and strengths.
    In the next section, we survey generative advancements over time, including adversarial
    networks, variational autoencoders, diffusion models, and autoregressive transformers,
    to better understand their impact and influence.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种关键方法构成了许多最近在生成式人工智能领域进步的基础，每种方法都有独特的方法和优势。在下一节中，我们将回顾生成式进步的历史，包括对抗网络、变分自编码器、扩散模型和自回归转换器，以更好地理解它们的影响和影响。
- en: Briefly surveying generative approaches
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简要概述生成方法
- en: 'Modern generative modeling encompasses diverse architectures suited to different
    data types and distinct tasks. Here, we briefly introduce some of the key approaches
    that have emerged over the years, bringing us to the state-of-the-art models:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现代生成式建模涵盖了适用于不同数据类型和不同任务的多样化架构。在此，我们简要介绍了一些年来涌现的关键方法，使我们达到了最先进的模型：
- en: '**Generative adversarial networks (GANs)** involve two interconnected neural
    networks—one acting as a generator to create realistic synthetic data and the
    other acting as a discriminator that distinguishes between real and synthetic
    (fake) data points. The generator and discriminator are adversaries in a **zero-sum
    game**, each fighting to outperform the other. This adversarial relationship gradually
    improves the generator’s capacity to produce vividly realistic synthetic data,
    making GANs adept at creating intricate image distributions and achieving photo-realistic
    image synthesis.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络（GANs**）涉及两个相互连接的神经网络——一个作为生成器以创建逼真的合成数据，另一个作为判别器以区分真实和合成（伪造）数据点。生成器和判别器在**零和游戏**中是竞争对手，每个都在努力超越对方。这种对抗关系逐渐提高了生成器产生生动逼真合成数据的能力，使GANs擅长创建复杂的图像分布并实现照片级图像合成。'
- en: '**Variational autoencoders** (**VAEs**) employ a unique learning method to
    compress data into a simpler form (or latent representation). This process involves
    an encoder and a decoder that work conjointly (Kingma & Welling, 2013). While
    VAEs may not be the top choice for image quality, they are unmatched in efficiently
    separating and understanding complex data patterns.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自编码器（VAEs**）采用独特的学习方法将数据压缩成更简单的形式（或潜在表示）。这个过程涉及一个编码器和一个解码器共同工作（Kingma &
    Welling, 2013）。虽然VAEs可能不是图像质量的最佳选择，但它们在有效地分离和理解复杂数据模式方面是无与伦比的。'
- en: '**Diffusion models** continuously add Gaussian noise to data over multiple
    steps to corrupt it. Gaussian noise can be thought of as random variations applied
    to a signal to distort it, creating “noise”. Diffusion models are trained to eliminate
    the added noise to recover the original data distribution. This type of reverse
    engineering process equips diffusion models to generate diverse, high-quality
    samples that closely replicate the original data distribution, producing diverse
    high-fidelity images (Ho et al., 2020).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩散模型**在多个步骤中持续向数据添加高斯噪声以破坏它。高斯噪声可以被视为应用于信号以扭曲它的随机变化，从而产生“噪声”。扩散模型经过训练以消除添加的噪声以恢复原始数据分布。这种逆向工程过程使扩散模型能够生成多样化、高质量且与原始数据分布紧密相似的样本，产生多样化的高保真图像（Ho
    et al., 2020）。'
- en: '**Autoregressive transformers** leverage parallelizable self-attention to model
    complex sequential dependencies, showing exceptional performance in language-related
    tasks (Vaswani et al., 2017). Pretrained models such as GPT-4 or Claude have demonstrated
    the capability for generalizations in natural language tasks and impressive human-like
    text generation. Despite ethical issues and misuse concerns, transformers have
    emerged as the frontrunners in language modeling and multimodal generation.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collectively, these methodologies paved the way for advanced generative modeling
    across a wide array of domains, including images, videos, audio, and text. While
    architectural and engineering innovations progress daily, generative methods showcase
    unparalleled synthesis capabilities across diverse modalities. Throughout the
    book, we will explore and apply generative methods to simulate real-world scenarios.
    However, before diving in, we further distinguish generative methods from traditional
    ML methods by addressing some common misconceptions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Clarifying misconceptions between discriminative and generative paradigms
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To better understand the distinctive capabilities and applications of traditional
    ML models (often referred to as discriminative) and generative methods, here,
    we clear up some common misconceptions and myths:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Myth 1**: Generative models cannot recognize patterns as effectively as discriminative
    models.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '**Truth**: State-of-the-art generative models are well-known for their impressive
    abilities to recognize and trace patterns, rivaling some discriminative models.
    Despite primarily focusing on creative synthesis, generative models display classification
    capabilities. However, the classes output from a generative model can be difficult
    to explain as generative models are not explicitly trained to learn decision boundaries
    or predetermined relationships. Instead, they may only learn to simulate classification
    based on labels learned implicitly (or organically) during training. In short,
    in cases where the explanation of model outcomes is important, classification
    using a discriminative model may be the better choice.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Consider GPT-4\. In addition to synthesizing human-like text,
    it can understand context, capture long-range dependencies, and detect patterns
    in texts. GPT-4 uses these intrinsic language processing capabilities to discriminate
    between classes, such as traditional classifiers. However, because GPT learns
    semantic relationships through extensive training, explaining its decision-making
    cannot be accomplished using any established methods.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '**Myth 2**: Generative AI will eventually replace discriminative AI.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Truth**: This is a common misunderstanding. Discriminative models have consistently
    been the option for high-stakes prediction tasks because they focus directly on
    learning the decision boundary between classes, ensuring high precision and reliability.
    More importantly, discriminative models can be explained post-hoc, making them
    the ultimate choice for critical applications in sectors such as healthcare, finance,
    and security. However, generative models may increasingly become more popular
    for high-stakes modeling as explainability techniques emerge.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Consider a discriminative model trained specifically for disease
    prediction in healthcare. A specialized model can classify data points (e.g.,
    images of skin) as healthy or unhealthy, giving healthcare professionals a tool
    for early intervention and treatment plans. Post-hoc explanation methods, such
    as SHAP, can be employed to identify and analyze the key features that influence
    classification outcomes. This approach offers clear insights into the specific
    results (i.e., feature attribution).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '**Myth 3**: Generative models continuously learn from user input.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '**Truth**: Not exactly. Generative LLMs are trained using a static approach.
    This means they learn from a vast training data corpora, and their knowledge is
    limited to the information contained within that training window. While models
    can be augmented with additional data or in-context information to help them contextualize,
    giving the impression of real-time learning, the underlying model itself is essentially
    frozen and does not learn in real time.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: GPT-3 was trained in 2020 and only contained information up to
    that date until its successor GPT-3.5, released in March of 2023\. Naturally,
    GPT-4 was trained on more recent data, but due to training limitations (including
    diminishing performance returns), it is reasonable to expect that subsequent training
    checkpoints will be released periodically and not continuously.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: While generative and discriminative models have distinct strengths and limitations,
    knowing when to apply each paradigm requires evaluating several key factors. As
    we have clarified some common myths about their capabilities, let’s turn our attention
    to guidelines for selecting the right approach for a given task or problem.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right paradigm
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice between generative and discriminative models depends on various
    factors, such as the task or problem at hand, the quality and quantity of data
    available, the desired output, and the level of performance required. The following
    is a list of key considerations:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '**Task specificity**: Discriminative models are more suitable for high-stakes
    applications, such as disease diagnosis, fraud detection, or credit risk assessment,
    where precision is crucial. However, generative models are more adept at creative
    tasks such as synthesizing images, text, music, or video.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data availability**: Discriminative models tend to overfit (or memorize examples)
    when trained on small datasets, which may lead to poor generalization. On the
    other hand, because generative models are often pretrained on vast amounts of
    data, they can produce a diverse output even with minimal input, making them a
    viable choice when data are scarce.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据可用性**：判别模型在训练于小数据集时往往会过拟合（或记忆示例），这可能导致泛化能力差。另一方面，由于生成模型通常在大量数据上预训练，即使输入最小，它们也能产生多样化的输出，这使得它们在数据稀缺时成为一个可行的选择。'
- en: '**Model performance**: Discriminative models outperform generative models in
    tasks where it is crucial to learn and explain a decision boundary between classes
    or where expected relationships in the data are well understood. Generative models
    usually excel in less constrained tasks that require a measure of perceived creativity
    and flexibility.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型性能**：在需要学习和解释类别之间的决策边界或数据中的预期关系被充分理解的任务中，判别模型优于生成模型。生成模型通常在不太受约束的任务中表现出色，这些任务需要一定程度的感知创造性和灵活性。'
- en: '**Model explainability**: While both paradigms can include models that are
    considered “black boxes” or not intrinsically interpretable, generative models
    can be more difficult, or at times, impossible to explain, as they often involve
    complex data generation processes that rely on understanding the underlying data
    distribution. Alternatively, discriminative models often focus on learning the
    boundary between classes. In use cases where model explainability is a key requirement,
    discriminative models may be more suitable. However, generative explainability
    research is gaining traction.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可解释性**：虽然两种范式都可以包括被认为是“黑盒”或本质上不可解释的模型，但生成模型可能更难以解释，有时甚至不可能解释，因为它们通常涉及复杂的数据生成过程，这些过程依赖于理解潜在的数据分布。另一方面，判别模型通常专注于学习类别之间的边界。在使用模型可解释性是关键要求的用例中，判别模型可能更合适。然而，生成模型的可解释性研究正在取得进展。'
- en: '**Model complexity**: Generally, discriminative models require less computational
    power because they learn to directly predict some output given a well-defined
    set of inputs.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型复杂性**：通常，判别模型需要的计算能力更少，因为它们学会直接预测给定一组明确定义的输入的一些输出。'
- en: Alternatively, generative models may consume more computational resources, as
    their training objective is to jointly capture the intricate hidden relationships
    between both inputs and presumed outputs. Accurately learning these intricacies
    requires vast amounts of data and large computations. Computational efficiency
    in generative LLM training (e.g., quantization) is a vibrant area of research.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一方面，生成模型可能消耗更多的计算资源，因为它们的训练目标是同时捕捉输入和假设输出之间复杂的隐藏关系。准确学习这些复杂性需要大量的数据和大量的计算。在生成型大型语言模型训练中的计算效率（例如，量化）是一个充满活力的研究领域。
- en: Ultimately, the choice between generative and discriminative models should be
    made by considering the trade-offs involved. Moreover, the adoption of these paradigms
    requires different levels of infrastructure, data curation, and other prerequisites.
    Occasionally, a hybrid approach that combines the strengths of both models can
    serve as an ideal solution. For example, a pretrained generative model can be
    fine-tuned as a classifier. We will learn about task-specific fine-tuning in [*Chapter
    5*](B21773_05.xhtml#_idTextAnchor180).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在生成模型和判别模型之间做出选择，应该考虑到所涉及的权衡。此外，采用这些范式需要不同层次的基础设施、数据整理和其他先决条件。偶尔，结合两种模型优势的混合方法可以作为一个理想的解决方案。例如，一个预训练的生成模型可以被微调为一个分类器。我们将在[*第五章*](B21773_05.xhtml#_idTextAnchor180)中学习关于特定任务微调的内容。
- en: Now that we have explored the key distinctions between traditional ML (i.e.,
    discriminative) and generative paradigms, including their distinct risks, we can
    look back at how we arrived at this paradigm shift. In the next section, we take
    a brief look at the evolution of generative AI.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了传统机器学习（即判别范式）和生成范式之间的关键区别，包括它们各自的风险，我们可以回顾一下我们是如何到达这个范式转变的。在下一节中，我们将简要回顾生成人工智能的演变。
- en: Looking back at the evolution of generative AI
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾生成人工智能的演变
- en: The field of generative AI has experienced an unprecedented acceleration, leading
    to a surge in the development and adoption of foundation models such as GPT. However,
    this momentum has been building for several decades, driven by continuous and
    significant advancements in ML and natural language generation research. These
    developments have brought us to the current generation of state-of-the-art models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: To fully appreciate the current state of generative AI, it is important to understand
    its evolution, beginning with traditional language processing techniques and moving
    through to more recent advancements.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Overview of traditional methods in NLP
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) technology has enabled machines to
    understand, interpret, and generate human language. It emerged from traditional
    statistical techniques such as n-grams and **hidden Markov models** (**HMMs**),
    which converted linguistic structures into mathematical models that machines could
    understand.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Initially, n-grams and HMMs were the primary methods used in NLP. N-grams predicted
    the next word in a sequence based on the last “*n*” words, while HMMs modeled
    sequences by considering every word as a state in a Markov process. These early
    methods were good at capturing local patterns and short-range dependencies in
    language.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: As computational power and data availability grew, more sophisticated techniques
    for natural language processing emerged. Among these was the **recurrent neural
    network** (**RNN**), which managed relationships across extended sequences and
    was proven to be effective in tasks where prior context influenced future predictions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, **long short-term memory networks** (**LSTMs**) were developed.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional RNNs, LSTMs had a unique ability to retain relevant long-term
    information while disregarding irrelevant data, maintaining semantic relationships
    across prolonged sequences.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Further advancements led to the introduction of sequence-to-sequence models,
    often utilizing LSTMs as their underlying structure. These models revolutionized
    fields such as machine translation and text summarization by dramatically improving
    efficiency and effectiveness.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Overall, NLP evolved from traditional statistical methods to advanced neural
    networks, transforming how we interacted with machines and enabling countless
    applications, such as machine translation and information retrieval (IR) (or finding
    relevant text based on a query). As the NLP field matured, incorporating the strengths
    of traditional statistical methods and advanced neural networks, a renaissance
    was forming. The next generation of NLP advancements would introduce transformer
    architectures, starting with the seminal paper *Attention is All You Need* and
    later the release of models such as BERT and eventually GPT.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Arrival and evolution of transformer-based models
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The release of the research paper titled *Attention is All You Need* in 2017
    served as a paradigm shift in natural language processing. This pivotal paper
    introduced the transformer model, an architectural innovation that provided an
    unprecedented approach to sequential language tasks such as translation. The transformer
    model contrasted with prior models that processed sequences serially. Instead,
    it simultaneously processed different segments of an input sequence, determining
    its relevance based on the task. This innovative processing addressed the complexity
    of long-range dependencies in sequences, enabling the model to draw out the critical
    semantic information needed for a task. The transformer was such a critical advancement
    that nearly every state-of-the-art generative LLM applies some derivation of the
    original architecture. Its importance and influence motivate our detailed exploration
    and implementation of the original transformer in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: With the transformer came significant advancements in natural language processing,
    including GPT-1 or Generative Pretrained Transformer 1 (Radford et al., 2018).
    GPT-1 introduced a novel directional architecture to tackle diverse NLP tasks.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Coinciding with GPT-1 was **BERT**, or **bidirectional encoder representations
    from transformers**, a pioneering work in the family of transformer-based models.
    BERT stood out among its predecessors, analyzing sentences forward and backward
    (or bi-directionally). This bidirectional analysis allowed BERT to capture semantic
    and syntactic nuances more effectively. At the time, BERT achieved unprecedented
    results when applied to complex natural language tasks such as named entity recognition,
    question answering, and sentiment analysis (Devlin et al., 2018).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Later, GPT-2, the much larger successor to GPT-1, attracted immense attention,
    as it greatly outperformed any of its predecessors across various tasks. In fact,
    GPT-2 was so unprecedented in its ability to generate human-like output that concerns
    about potential implications led to a delay in its initial release (Hern, 2019).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Amid early concerns, OpenAI followed up with the development of GPT-3, signaling
    a leap in the potential of LLMs. Developers demonstrated the potential of training
    at a massive scale, reaching 175 billion parameters (or adjustable variables learned
    during training), surpassing its two predecessors. GPT-3 was a “general-purpose”
    learner, capable of performing a wide range of natural language tasks learned
    implicitly from its training corpus instead of through task-specific fine-tuning.
    This capability sparked the exploration of foundation model development for general
    use across various domains and tasks. GPT-3’s distinct design and unprecedented
    scale led to a generation of generative models that could perform an indefinite
    number of increasingly complex downstream tasks learned implicitly through its
    extensive training.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Development and impact of GPT-4
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI’s development of GPT-4 marked a significant advance in the potential
    of large-scale, multimodal models. GPT-4, capable of processing image and text
    inputs and producing text outputs, represented yet another giant leap ahead of
    predecessors.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的发展标志着在大型、多模态模型潜力方面取得了重大进步。GPT-4能够处理图像和文本输入并生成文本输出，代表了在先前的模型之上的又一次巨大飞跃。
- en: GPT-4 exhibited human-level performance on various professional and academic
    benchmarks. For instance, it passed a simulated bar exam with a score falling
    into the top 10% of test-takers (OpenAI, 2023).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4在各种专业和学术基准测试中展现了人类水平的表现。例如，它通过了一场模拟的律师资格考试，得分位于测试者前10%（OpenAI，2023）。
- en: A key distinction of GPT-4 is what happens after pretraining. Open AI applied
    **reinforcement learning with human feedback** (**RLHF**)—a type of risk/reward
    training derived from the same technique used to teach autonomous vehicles to
    make decisions based on the environment they encounter. In the case of GPT-4,
    the model learned to respond appropriately to a myriad of scenarios, incorporating
    human feedback along the way. This novel refinement strategy drastically improved
    the model’s propensity for factuality and its adherence to desired behaviors.
    The integration of RLHF demonstrated how models could be better aligned with human
    judgment toward the goal of responsible AI.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4的一个关键区别在于预训练之后发生的事情。OpenAI应用了**带有人类反馈的强化学习**（**RLHF**）——这是一种从用于教授自动驾驶汽车根据其遇到的环境做出决策的技术中衍生出来的风险/奖励训练。在GPT-4的情况下，模型学会了适当地对各种场景做出反应，并在过程中结合了人类反馈。这种新颖的改进策略极大地提高了模型的事实性和对期望行为的遵守。RLHF的集成展示了模型如何更好地与人类的判断相一致，以实现负责任的AI的目标。
- en: However, despite demonstrating groundbreaking abilities, GPT-4 had similar limitations
    to earlier GPT models. It was not entirely reliable and had a limited context
    window (or input size). Meaning it could not receive large texts or documents
    as input. It was also prone to hallucination. As discussed, Hallucination is an
    anthropomorphized way of describing the model’s tendency to generate content that
    is not grounded in fact or reality. A hallucination occurs because generative
    language models (without augmentation) synthesize content purely based on semantic
    context and don’t perform any logical processing to verify factuality. This weakness
    presented meaningful risks, particularly in contexts where fact-based outcomes
    are paramount.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管展示了开创性的能力，GPT-4与早期的GPT模型具有类似的局限性。它并不完全可靠，并且具有有限的上下文窗口（或输入大小）。这意味着它不能接收大文本或文档作为输入。它也容易产生幻觉。正如讨论的那样，幻觉是一种拟人化的描述，指模型倾向于生成不基于事实或现实的內容。幻觉的发生是因为生成式语言模型（未经增强）纯粹基于语义上下文合成内容，而不进行任何逻辑处理以验证事实性。这种弱点带来了有意义的风险，尤其是在基于事实的结果至关重要的环境中。
- en: Despite limitations, GPT-4 made significant strides in language model performance.
    As with prior models, GPT-4’s development and potential use underscored the importance
    of safety and ethical considerations for future AI applications. As a result,
    the rise of GPT-4 accentuated the ongoing discussions and research into the potential
    implications of deploying such powerful models. In the next section, we briefly
    survey some of the known risks that are unique to generative AI.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在局限性，GPT-4在语言模型性能上取得了重大进展。与之前的模型一样，GPT-4的开发和潜在用途强调了未来AI应用中安全和伦理考虑的重要性。因此，GPT-4的兴起加剧了关于部署如此强大模型潜在影响的持续讨论和研究。在下一节中，我们将简要概述一些仅与生成式AI相关的已知风险。
- en: Looking ahead at risks and implications
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展望风险和影响
- en: 'Both generative and discriminative AI introduce unique risks and benefits that
    must be weighed carefully. However, generative methods can not only carry forward
    but also exacerbate many risks associated with traditional ML while also introducing
    new risks. Consequently, before we can adopt generative AI in the real world and
    at scale, it is essential to understand the risks and establish responsible governance
    principles to help mitigate them:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式和判别式AI都引入了独特的风险和收益，这些必须仔细权衡。然而，生成式方法不仅能够继承，还能加剧许多与传统机器学习相关的风险，同时也会引入新的风险。因此，在我们能够在现实世界和大规模上采用生成式AI之前，理解这些风险并建立负责任的治理原则以帮助减轻这些风险是至关重要的：
- en: '**Hallucination**: This is a term widely used to describe when models generate
    factually inaccurate information. Generative models are adept at producing plausible-sounding
    output without basis in fact. As such, it is critical to ground generative models
    with factual information. The term “grounding” refers to appending model inputs
    with additional information that is known to be factual. We explore grounding
    techniques in [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225). Additionally, it
    is essential to have a strategy for evaluating model outputs that includes human
    review.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幻觉**：这是一个广泛使用的术语，用来描述模型生成事实不准确信息的情况。生成模型擅长在没有事实依据的情况下产生听起来合理的输出。因此，用事实信息来定位生成模型至关重要。术语“定位”指的是在模型输入中附加已知为事实的额外信息。我们在[*第七章*](B21773_07.xhtml#_idTextAnchor225)中探讨了定位技术。此外，制定一个包括人工审查在内的评估模型输出的策略也是必不可少的。'
- en: '**Plagiarism**: Since generative models are sometimes trained on uncrated datasets,
    some training corpora may have included data without explicit permissions. Models
    may produce information that is subject to copyright protections or can be claimed
    as intellectual property.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剽窃**：由于生成模型有时是在未经许可的数据集上训练的，一些训练语料库可能包括未经明确许可的数据。模型可能会产生受版权保护的信息或可以声称为知识产权的信息。'
- en: '**Accidental memorization**: As with many ML models that train on immense corpora,
    generative models tend to memorize parts of the training data. In particular,
    they are prone to memorizing sparse examples that do not fit neatly into a broader
    pattern. In some cases, models could memorize sensitive information that can be
    extracted and exposed (Brundage et al., 2020; Carlini et al., 2020). Consequently,
    whether consuming a pretrained model or fine-tuning (i.e., continued model training),
    training data curation is essential.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**意外记忆**：与许多在大量语料库上训练的机器学习模型一样，生成模型倾向于记住训练数据的一部分。特别是，它们容易记住那些不适合更广泛模式的不常见示例。在某些情况下，模型可能会记住可以提取和暴露的敏感信息（Brundage等人，2020年；Carlini等人，2020年）。因此，无论是消费预训练模型还是微调（即继续模型训练），训练数据的选择至关重要。'
- en: '**Toxicity and bias**: Another byproduct of large-scale model training is that
    the model will inevitably learn any societal biases embedded in the training data.
    Biases can manifest as gender, racial, or socioeconomic biases in generated text
    or images, often replicating or amplifying stereotypes. We detail mitigations
    for this risk in [*Chapter 8*](B21773_08.xhtml#_idTextAnchor251).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**毒性和偏见**：大规模模型训练的另一个副作用是模型不可避免地会学习训练数据中嵌入的社会偏见。偏见可能表现为生成文本或图像中的性别、种族或社会经济偏见，通常复制或放大刻板印象。我们在[*第八章*](B21773_08.xhtml#_idTextAnchor251)中详细介绍了这种风险的缓解措施。'
- en: 'With an understanding of some of the risks, we turn our focus to the nuanced
    implications of adopting generative AI:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些风险的理解，我们将关注采用生成式AI的微妙影响：
- en: '**Ethical**: As discussed, these models inevitably learn and reproduce the
    biases inherent in the training data, raising serious ethical questions. Similarly,
    concerns about data privacy and security have emerged due to the model’s susceptibility
    to memorizing and exposing its training data. This has led to calls for robust
    ethical guidelines and data privacy regulations (Gebru et al., 2018).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伦理**：正如所讨论的，这些模型不可避免地会学习和复制训练数据中固有的偏见，引发严重的伦理问题。同样，由于模型容易记住和暴露其训练数据，数据隐私和安全问题也出现了。这导致了对于强有力的伦理指南和数据隐私法规的呼吁（Gebru等人，2018年）。'
- en: '**Environmental**: LLMs are computational giants, demanding unprecedented resources
    for training and implementation. Thus, they inevitably present environmental impacts.
    The energy consumption required to train an LLM produces substantial carbon dioxide
    emissions—roughly the equivalent lifetime emissions of five vehicles. Consequently,
    multiple efforts are underway to increase model efficiency and reduce carbon footprints.
    For example, techniques such as reduced bit precision training (or quantization)
    and parameter efficient fine-tuning (discussed in [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180))
    reduce overall training time, helping to shrink carbon footprints.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：大型语言模型（LLM）是计算巨人，需要前所未有的资源进行训练和实施。因此，它们不可避免地会产生环境影响。训练一个LLM所需的能源消耗会产生大量的二氧化碳排放——大约相当于五辆汽车终身的排放量。因此，正在进行多项工作以提高模型效率并减少碳足迹。例如，如减少位精度训练（或量化）和参数高效微调（在第[*第五章*](B21773_05.xhtml#_idTextAnchor180)中讨论）等技术可以减少整体训练时间，有助于缩小碳足迹。'
- en: '**Social**: Along with environmental impacts, LLMs also have social implications.
    As these models become proficient at generating text, simulating intelligent conversation,
    and automating fundamental tasks, they present an unparalleled opportunity for
    job automation. Due to various complex factors, this potential for large-scale
    automation in the US may disproportionately affect marginalized or underrepresented
    communities. Thus, this amplifies prior concerns regarding labor rights and the
    need for additional protections to minimize harm.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社会**： 除了环境影响，大型语言模型（LLMs）也具有社会影响。随着这些模型在生成文本、模拟智能对话和自动化基本任务方面变得熟练，它们为自动化工作提供了前所未有的机会。由于各种复杂因素，这种在美国大规模自动化的潜力可能会不成比例地影响边缘化或代表性不足的社区。因此，这加剧了关于劳动权利和需要额外保护以最小化伤害的先前担忧。'
- en: '**Business and labor**: Along with broader socio-economic implications, we
    must examine more direct impacts on the business sector. While generative AI opens
    up new opportunities, changes in the labor market could bring about immense disruption
    if not addressed responsibly. Beyond labor impacts, AI advancements also significantly
    affect various business sectors. They can result in the creation of new roles,
    business models, and opportunities, requiring ongoing governance strategy and
    explorative frameworks that center on inclusivity, ethics, and responsible adoption.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**商业与劳动力**： 除了更广泛的社会经济影响，我们还必须考察对商业部门的直接影响。虽然生成式人工智能开辟了新的机会，但如果处理不当，劳动力市场的变化可能会带来巨大的破坏。除了劳动力影响之外，人工智能的进步也显著影响了各个商业部门。它们可能导致新角色的创造、商业模式的改变和机会的产生，需要持续的治理策略和以包容性、道德和负责任采用为中心的探索框架。'
- en: Addressing these challenges will require technical and scientific improvements,
    data-specific regulations and laws, ethical guidelines, and human-centered AI
    governance strategies. These are integral to building an equitable, secure, and
    inclusive AI-driven future.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 应对这些挑战需要技术、科学上的改进，针对特定数据的法规和法律，道德规范，以及以人为本的人工智能治理策略。这些都是构建一个公平、安全、包容的人工智能驱动未来的关键。
- en: Having discussed the history, risks, and limitations of generative AI, we are
    now better equipped to explore the vast opportunities and applications of such
    transformative technology.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了生成式人工智能的历史、风险和限制之后，我们现在更有能力探索这种变革性技术的广泛机会和应用。
- en: Introducing use cases of generative AI
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍生成式人工智能的应用案例
- en: 'Generative AI has already begun to disrupt various sectors. The technology
    is making waves across many disciplines, from enhancing language-based tasks to
    reshaping digital art. The following section offers examples of real-world applications
    of generative AI across different sectors:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能已经开始颠覆各个行业。这项技术正在许多学科中掀起波澜，从增强基于语言的任务到重塑数字艺术。以下部分提供了生成式人工智能在不同行业中的实际应用示例：
- en: '**Traditional natural language processing**: LLMs, such as Open AI’s GPT series,
    have elevated traditional NLP and NLG. As discussed, these models have a unique
    ability to generate coherent, relevant, and human-like text. The potential of
    these models was demonstrated when GPT-3 outperformed classical and modern approaches
    in several language tasks, displaying an unprecedented understanding of human
    language. The release of GPT-4 and Claude 3 marked another milestone, raising
    the standard even further for state-of-the-art models.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**传统自然语言处理**： 如Open AI的GPT系列等大型语言模型（LLMs）已经提升了传统的自然语言处理（NLP）和自然语言生成（NLG）。正如所讨论的，这些模型具有生成连贯、相关且类似人类文本的独特能力。这些模型的可能性在GPT-3在多个语言任务中优于经典和现代方法时得到了证明，展示了人类语言前所未有的理解。GPT-4和Claude
    3的发布标志着另一个里程碑，将最先进模型的标准进一步提高。'
- en: '**Digital art creation**: The advent of “generative art” is evidence of the
    radical impact of generative AI in the field of digital art. For instance, artists
    can use AI generative models to create intricate designs, allowing them to focus
    on the conceptual aspect of art. It simplifies the process, reducing the need
    for high-level technical acumen.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数字艺术创作**： “生成艺术”的出现是生成式人工智能在数字艺术领域产生激进影响的证据。例如，艺术家可以使用人工智能生成模型来创建复杂的设计，从而让他们专注于艺术的概念性方面。这简化了过程，减少了高级技术专长的需求。'
- en: '**Music creation**: In the music industry, generative AI can enhance the composition
    process. Several platforms offer high-quality AI-driven music creation tools that
    can generate long-form musical compositions combining different music styles across
    various eras and genres.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**音乐创作**: 在音乐产业中，生成式人工智能可以增强创作过程。几个平台提供了高质量的AI驱动音乐创作工具，可以生成结合不同时代和流派的音乐风格的长期音乐作品。'
- en: '**Streamlining business processes**: Several businesses have started employing
    generative AI to enable faster and more efficient processes. Generative AI-enabled
    operational efficiencies allow employees to focus on more strategic tasks. For
    example, fully integrated LLM email clients can organize emails and (combined
    with other technologies) learn to prioritize critical emails over time.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化业务流程**: 一些企业已经开始采用生成式人工智能来使流程更快、更高效。生成式人工智能带来的运营效率使员工能够专注于更具战略性的任务。例如，完全集成的LLM电子邮件客户端可以组织电子邮件，并且（与其他技术结合）随着时间的推移学会优先处理关键电子邮件。'
- en: '**Entertainment**: While still largely experimental, LLMs show promising potential
    to disrupt creative writing and storytelling, particularly in the gaming industry.
    For example, procedural games could apply LLMs to enhance dynamic storytelling
    and create more engaging, personalized user experiences. As technology advances,
    we may see more mainstream adoption of LLMs in gaming, opening up new possibilities
    for interactive narratives.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**娱乐**: 尽管仍处于实验阶段，但大型语言模型（LLMs）在颠覆创意写作和叙事方面展现出有希望的潜力，尤其是在游戏行业。例如，程序化游戏可以利用LLMs来增强动态叙事，并创造更具吸引力和个性化的用户体验。随着技术的进步，我们可能会看到LLMs在游戏领域的更广泛采用，为互动叙事开辟新的可能性。'
- en: '**Fashion**: In the fashion industry, generative models help designers innovate.
    By using a state-of-the-art generative AI model, designers can create and visualize
    new clothing styles by simply tweaking a few configurations.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时尚**: 在时尚产业中，生成模型帮助设计师进行创新。通过使用最先进的生成式人工智能模型，设计师可以通过简单地调整几个配置来创建和可视化新的服装风格。'
- en: '**Architecture and construction**: In the architectural world, generative-enhanced
    tools can help architects and urban planners optimize and generate design solutions,
    leading to more efficient and sustainable architectural designs.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建筑和施工**: 在建筑领域，生成增强工具可以帮助建筑师和城市规划师优化和生成设计方案，从而实现更高效和可持续的建筑设计。'
- en: '**Food industry**: Emerging AI-driven cooking assistants can generate unique
    food combinations, novel recipes, and modified recipes for highly specific dietary
    needs.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**食品行业**: 新兴的AI驱动烹饪助手可以生成独特的食物组合、新颖的食谱以及针对高度特定饮食需求的修改食谱。'
- en: '**Education**: Generative AI-enhanced educational platforms offer the automatic
    creation of study aids that can facilitate personalized learning experiences and
    can automatically generate tailored content to accommodate specific and diverse
    learning styles.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**教育**: 生成式人工智能增强的教育平台可以自动创建学习辅助工具，这些工具可以促进个性化的学习体验，并自动生成定制内容以适应特定的和多样化的学习风格。'
- en: However, we must balance the breadth of opportunities with sophisticated guardrails
    and the continued promotion of ethical use. As data scientists, policymakers,
    and industry leaders, we must continue to work towards fostering an environment
    conducive to responsible AI deployment. That said, as generative AI continues
    to evolve, it presents a future replete with novel innovations and applications.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须在机会的广度与复杂的约束之间取得平衡，并持续推广道德使用。作为数据科学家、政策制定者和行业领导者，我们必须继续努力营造有利于负责任AI部署的环境。尽管如此，随着生成式人工智能的持续发展，它带来了充满新颖创新和应用的未来。
- en: The future of generative AI applications
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能应用的未来
- en: The relentless advancement of generative AI presents a future filled with both
    possibilities and complex challenges. Imagine a future where a generative model
    trained on the world’s leading climate change research can offer practical yet
    groundbreaking counteractive strategies with precise details about their application.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式人工智能不懈的进步预示着一个充满可能性和复杂挑战的未来。想象一个未来，一个基于世界领先的气候变化研究训练的生成模型可以提供实际且具有突破性的应对策略，并精确地提供其应用的详细信息。
- en: However, as we embrace an increasingly AI-centered future, we should not overlook
    the existing challenges. These involve the potential misuse of AI tools, unpredictable
    implications, and the profound ethical considerations underlying AI adoption.
    Additionally, sustainable and eco-conscious development is key, as training large-scale
    models can be resource-intensive
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: In an age of accelerated progress, collaboration across all stakeholders—from
    data scientists, AI enthusiasts, and policymakers to industry leaders—is essential.
    By being equipped with comprehensive oversight, robust guidelines, and strategic
    education initiatives, concerted efforts can safeguard a future where generative
    AI is ubiquitous.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Despite these hurdles, the transformative potential of generative AI remains
    unquestionable. With its capacity to reshape industries, redefine societal infrastructures,
    and alter our ways of living, learning, and working, generative AI serves as a
    reminder that we are experiencing a pivotal moment—one propelled by decades of
    scientific research and computational ingenuity that are coalescing to bring us
    forward as a society.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we traced the evolution of generative AI, distinguished it
    from traditional ML, explored its evolution, discussed its risks and implications,
    and, hopefully, dispelled some common misconceptions. We contemplated some of
    the possibilities anchored by consideration for its responsible adoption.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: As we move on to the next chapter, we will examine the fundamental architectures
    behind generative AI, giving us a foundational understanding of the key generative
    methods, including GANs, diffusion models, and transformers. These ML methods
    form the backbone of generative AI and have been instrumental in bringing about
    the remarkable advancements we see today.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[https://doi.org/10.1007/s11023-020-09526-7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction](https://doi.org/10.1007/s11023-020-09526-7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B.,
    Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen,
    G. C., Steinhardt, J., Flynn, C., Ó hÉigeartaigh, S., Beard, S., Belfield, H.,
    Farquhar, S., & Amodei, D. (2018). *The malicious use of artificial intelligence:
    Forecasting, prevention, and mitigation*. arXiv [cs.AI]. [http://arxiv.org/abs/1802.07228](http://arxiv.org/abs/1802.07228).'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee,
    K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., & Raffel, C.
    (2020). *Extracting training data from large language models*. arXiv [cs.CR].
    [http://arxiv.org/abs/2012.07805](http://arxiv.org/abs/2012.07805).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training
    of deep bidirectional transformers for language understanding*. arXiv [cs.CL].
    [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805).'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hagendorff, T. (2020). Publisher correction to *The ethics of AI ethics: An
    evaluation of guidelines*. *Minds and Machines*, 30(3), 457–461\. [https://doi.org/10.1007/s11023-020-09526-7](https://doi.org/10.1007/s11023-020-09526-7).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hern, A. (2019, February 14). *New AI fake text generator may be too dangerous
    to release, say creators*. *The* *Guardian*. [https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising diffusion probabilistic models*.
    arXiv [cs.LG]. [http://arxiv.org/abs/2006.11239](http://arxiv.org/abs/2006.11239).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R.,
    Gray, S., Kingma, D. P., & Welling, M. (2013). *Auto-encoding variational bayes*.
    arXiv [stat.ML]. [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Muhammad, T., Aftab, A. B., Ahsan, M. M., Muhu, M. M., Ibrahim, M., Khan, S.
    I., & Alam, M. S. (2022). *Transformer-based deep learning model for stock price
    prediction: A case study on Bangladesh stock market*. arXiv [q-fin.ST]. [http://arxiv.org/abs/2208.08300](http://arxiv.org/abs/2208.08300).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI. (2023). *GPT-4 technical report*. arXiv [cs.CL]. [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018).
    *Language models are unsupervised* *multitask learners*.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, L., & Polosukhin, I. (2017). *Attention Is All You Need*. arXiv [cs.CL].
    [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
