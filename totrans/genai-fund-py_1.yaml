- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Understanding Generative AI: An Introduction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In his influential book *The Singularity Is Near* (2005), renowned inventor
    and futurist Ray Kurzweil asserted that we were on the precipice of an exponential
    acceleration in technological advancements. He envisioned a future where technological
    innovation would continue to accelerate, eventually leading to a **singularity**—a
    point where **artificial intelligence** (**AI**) could transcend human intelligence,
    blurring the lines between humans and machines. Fast-forward to today and we find
    ourselves advancing along the trajectory Kurzweil outlined, with generative AI
    marking a significant stride along this path. Today, we are experiencing state-of-the-art
    generative models can behave as collaborators capable of synthetic understanding
    and generating sophisticated responses that mirror human intelligence.. The rapid
    and exponential growth of generative approaches is propelling Kurzweil’s vision
    forward, fundamentally reshaping how we interact with technology.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we lay the conceptual groundwork for anyone hoping to apply
    generative AI to their work, research, or field of study, broadening a fundamental
    understanding of what this technology does, how it was derived, and how it can
    be used. It establishes how generative models differ from classical **machine
    learning** (**ML**) paradigms and elucidates how they discern complex relationships
    and idiosyncrasies in data to synthesize human-like text, audio, and video. We
    will explore critical foundational generative methods, such as generative adversarial
    networks (GANs), diffusion models, and transformers, with a particular emphasis
    on their real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this chapter hopes to dispel some common misunderstandings surrounding
    generative AI and provides guidelines to adopt this emerging technology ethically,
    considering its environmental footprint and advocating for responsible development
    and adoption. We will also highlight scenarios where generative models are apt
    for addressing business challenges. By the conclusion of this chapter, we will
    better understand the potential of generative AI and its applications across a
    wide array of sectors and have critically assessed the risks, limitations, and
    long-term considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Whether your interest is casual, you are a professional transitioning from a
    different field, or you are an established practitioner in the fields of data
    science or ML, this chapter offers a contextual understanding to make informed
    decisions regarding the responsible adoption of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, we aim to establish a foundation through an introductory exploration
    of generative AI and **large language models** (**LLMs**), dissected into two
    parts.
  prefs: []
  type: TYPE_NORMAL
- en: The beginning of the book will introduce the fundamentals and history of generative
    AI, surveying various types, such as GANs, diffusers, and transformers, tracing
    the foundations of **natural language generation** (**NLG**), and demonstrating
    the basic steps to implement generative models from prototype to production. Moving
    forward, we will focus on slightly more advanced application fundamentals, including
    fine-tuning generative models, prompt engineering, and addressing ethical considerations
    toward the responsible adoption of generative AI. Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent decades, AI has made incredible strides. The origins of the field
    stem from classical statistical models meticulously designed to help us analyze
    and make sense of data. As we developed more robust computational methods to process
    and store data, the field shifted—intersecting computer science and statistics
    and giving us ML. ML systems could learn complex relationships and surface latent
    insights from vast amounts of data, transforming our approach to statistical modeling.
  prefs: []
  type: TYPE_NORMAL
- en: This shift laid the groundwork for the rise of deep learning, a substantial
    step forward that introduced multi-layered neural networks (i.e., a system of
    interconnected functions) to model complex patterns. Deep learning enabled powerful
    discriminative models that became pivotal for advancements in diverse fields of
    research, including image recognition, voice recognition, and natural language
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: However, the journey continues with the emergence of generative AI. Generative
    AI harnesses the power of deep learning to accomplish a broader objective. Instead
    of classifying and discriminating data, generative AI seeks to learn and replicate
    data distributions to “create” entirely new and seemingly original data, mirroring
    human-like output.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing generative AI from other AI models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, the critical distinction between discriminative and generative models
    lies in their objectives. Discriminative models aim to predict target outputs
    given input data. Classification algorithms, such as logistic regression or support
    vector machines, find decision boundaries in data to categorize inputs as belonging
    to one or more class. Neural networks learn input-output mappings by optimizing
    weights through backpropagation (or tracing back to resolve errors) to make accurate
    predictions. Advanced gradient boosting models, such as XGBoost or LightGBM, further
    enhance these discriminative models by employing decision trees and incorporating
    the principles of gradient boosting (or the strategic ensembling of models) to
    make highly accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Generative methods learn complex relationships through expansive training in
    order to generate new data sequences enabling many downstream applications. Effectively,
    these models create synthetic outputs by replicating the statistical patterns
    and properties discovered in training data, capturing nuances and idiosyncrasies
    that closely reflect human behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, a discriminative image classifier labels images containing a cat
    or a dog. In contrast, a generative model can synthesize diverse, realistic cat
    or dog images by learning the distributions of pixels and implicit features from
    existing images. Moreover, generative models can be trained across modalities
    to unlock new possibilities in synthesis-focused applications to generate human-like
    photographs, videos, music, and text.
  prefs: []
  type: TYPE_NORMAL
- en: There are several key methods that have formed the foundation for many of the
    recent advancements in Generative AI, each with unique approaches and strengths.
    In the next section, we survey generative advancements over time, including adversarial
    networks, variational autoencoders, diffusion models, and autoregressive transformers,
    to better understand their impact and influence.
  prefs: []
  type: TYPE_NORMAL
- en: Briefly surveying generative approaches
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modern generative modeling encompasses diverse architectures suited to different
    data types and distinct tasks. Here, we briefly introduce some of the key approaches
    that have emerged over the years, bringing us to the state-of-the-art models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative adversarial networks (GANs)** involve two interconnected neural
    networks—one acting as a generator to create realistic synthetic data and the
    other acting as a discriminator that distinguishes between real and synthetic
    (fake) data points. The generator and discriminator are adversaries in a **zero-sum
    game**, each fighting to outperform the other. This adversarial relationship gradually
    improves the generator’s capacity to produce vividly realistic synthetic data,
    making GANs adept at creating intricate image distributions and achieving photo-realistic
    image synthesis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variational autoencoders** (**VAEs**) employ a unique learning method to
    compress data into a simpler form (or latent representation). This process involves
    an encoder and a decoder that work conjointly (Kingma & Welling, 2013). While
    VAEs may not be the top choice for image quality, they are unmatched in efficiently
    separating and understanding complex data patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diffusion models** continuously add Gaussian noise to data over multiple
    steps to corrupt it. Gaussian noise can be thought of as random variations applied
    to a signal to distort it, creating “noise”. Diffusion models are trained to eliminate
    the added noise to recover the original data distribution. This type of reverse
    engineering process equips diffusion models to generate diverse, high-quality
    samples that closely replicate the original data distribution, producing diverse
    high-fidelity images (Ho et al., 2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoregressive transformers** leverage parallelizable self-attention to model
    complex sequential dependencies, showing exceptional performance in language-related
    tasks (Vaswani et al., 2017). Pretrained models such as GPT-4 or Claude have demonstrated
    the capability for generalizations in natural language tasks and impressive human-like
    text generation. Despite ethical issues and misuse concerns, transformers have
    emerged as the frontrunners in language modeling and multimodal generation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collectively, these methodologies paved the way for advanced generative modeling
    across a wide array of domains, including images, videos, audio, and text. While
    architectural and engineering innovations progress daily, generative methods showcase
    unparalleled synthesis capabilities across diverse modalities. Throughout the
    book, we will explore and apply generative methods to simulate real-world scenarios.
    However, before diving in, we further distinguish generative methods from traditional
    ML methods by addressing some common misconceptions.
  prefs: []
  type: TYPE_NORMAL
- en: Clarifying misconceptions between discriminative and generative paradigms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To better understand the distinctive capabilities and applications of traditional
    ML models (often referred to as discriminative) and generative methods, here,
    we clear up some common misconceptions and myths:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Myth 1**: Generative models cannot recognize patterns as effectively as discriminative
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Truth**: State-of-the-art generative models are well-known for their impressive
    abilities to recognize and trace patterns, rivaling some discriminative models.
    Despite primarily focusing on creative synthesis, generative models display classification
    capabilities. However, the classes output from a generative model can be difficult
    to explain as generative models are not explicitly trained to learn decision boundaries
    or predetermined relationships. Instead, they may only learn to simulate classification
    based on labels learned implicitly (or organically) during training. In short,
    in cases where the explanation of model outcomes is important, classification
    using a discriminative model may be the better choice.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Consider GPT-4\. In addition to synthesizing human-like text,
    it can understand context, capture long-range dependencies, and detect patterns
    in texts. GPT-4 uses these intrinsic language processing capabilities to discriminate
    between classes, such as traditional classifiers. However, because GPT learns
    semantic relationships through extensive training, explaining its decision-making
    cannot be accomplished using any established methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Myth 2**: Generative AI will eventually replace discriminative AI.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Truth**: This is a common misunderstanding. Discriminative models have consistently
    been the option for high-stakes prediction tasks because they focus directly on
    learning the decision boundary between classes, ensuring high precision and reliability.
    More importantly, discriminative models can be explained post-hoc, making them
    the ultimate choice for critical applications in sectors such as healthcare, finance,
    and security. However, generative models may increasingly become more popular
    for high-stakes modeling as explainability techniques emerge.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: Consider a discriminative model trained specifically for disease
    prediction in healthcare. A specialized model can classify data points (e.g.,
    images of skin) as healthy or unhealthy, giving healthcare professionals a tool
    for early intervention and treatment plans. Post-hoc explanation methods, such
    as SHAP, can be employed to identify and analyze the key features that influence
    classification outcomes. This approach offers clear insights into the specific
    results (i.e., feature attribution).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Myth 3**: Generative models continuously learn from user input.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Truth**: Not exactly. Generative LLMs are trained using a static approach.
    This means they learn from a vast training data corpora, and their knowledge is
    limited to the information contained within that training window. While models
    can be augmented with additional data or in-context information to help them contextualize,
    giving the impression of real-time learning, the underlying model itself is essentially
    frozen and does not learn in real time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Example**: GPT-3 was trained in 2020 and only contained information up to
    that date until its successor GPT-3.5, released in March of 2023\. Naturally,
    GPT-4 was trained on more recent data, but due to training limitations (including
    diminishing performance returns), it is reasonable to expect that subsequent training
    checkpoints will be released periodically and not continuously.'
  prefs: []
  type: TYPE_NORMAL
- en: While generative and discriminative models have distinct strengths and limitations,
    knowing when to apply each paradigm requires evaluating several key factors. As
    we have clarified some common myths about their capabilities, let’s turn our attention
    to guidelines for selecting the right approach for a given task or problem.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right paradigm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The choice between generative and discriminative models depends on various
    factors, such as the task or problem at hand, the quality and quantity of data
    available, the desired output, and the level of performance required. The following
    is a list of key considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Task specificity**: Discriminative models are more suitable for high-stakes
    applications, such as disease diagnosis, fraud detection, or credit risk assessment,
    where precision is crucial. However, generative models are more adept at creative
    tasks such as synthesizing images, text, music, or video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data availability**: Discriminative models tend to overfit (or memorize examples)
    when trained on small datasets, which may lead to poor generalization. On the
    other hand, because generative models are often pretrained on vast amounts of
    data, they can produce a diverse output even with minimal input, making them a
    viable choice when data are scarce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance**: Discriminative models outperform generative models in
    tasks where it is crucial to learn and explain a decision boundary between classes
    or where expected relationships in the data are well understood. Generative models
    usually excel in less constrained tasks that require a measure of perceived creativity
    and flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model explainability**: While both paradigms can include models that are
    considered “black boxes” or not intrinsically interpretable, generative models
    can be more difficult, or at times, impossible to explain, as they often involve
    complex data generation processes that rely on understanding the underlying data
    distribution. Alternatively, discriminative models often focus on learning the
    boundary between classes. In use cases where model explainability is a key requirement,
    discriminative models may be more suitable. However, generative explainability
    research is gaining traction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model complexity**: Generally, discriminative models require less computational
    power because they learn to directly predict some output given a well-defined
    set of inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternatively, generative models may consume more computational resources, as
    their training objective is to jointly capture the intricate hidden relationships
    between both inputs and presumed outputs. Accurately learning these intricacies
    requires vast amounts of data and large computations. Computational efficiency
    in generative LLM training (e.g., quantization) is a vibrant area of research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Ultimately, the choice between generative and discriminative models should be
    made by considering the trade-offs involved. Moreover, the adoption of these paradigms
    requires different levels of infrastructure, data curation, and other prerequisites.
    Occasionally, a hybrid approach that combines the strengths of both models can
    serve as an ideal solution. For example, a pretrained generative model can be
    fine-tuned as a classifier. We will learn about task-specific fine-tuning in [*Chapter
    5*](B21773_05.xhtml#_idTextAnchor180).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the key distinctions between traditional ML (i.e.,
    discriminative) and generative paradigms, including their distinct risks, we can
    look back at how we arrived at this paradigm shift. In the next section, we take
    a brief look at the evolution of generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the evolution of generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of generative AI has experienced an unprecedented acceleration, leading
    to a surge in the development and adoption of foundation models such as GPT. However,
    this momentum has been building for several decades, driven by continuous and
    significant advancements in ML and natural language generation research. These
    developments have brought us to the current generation of state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: To fully appreciate the current state of generative AI, it is important to understand
    its evolution, beginning with traditional language processing techniques and moving
    through to more recent advancements.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of traditional methods in NLP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Natural language processing** (**NLP**) technology has enabled machines to
    understand, interpret, and generate human language. It emerged from traditional
    statistical techniques such as n-grams and **hidden Markov models** (**HMMs**),
    which converted linguistic structures into mathematical models that machines could
    understand.'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, n-grams and HMMs were the primary methods used in NLP. N-grams predicted
    the next word in a sequence based on the last “*n*” words, while HMMs modeled
    sequences by considering every word as a state in a Markov process. These early
    methods were good at capturing local patterns and short-range dependencies in
    language.
  prefs: []
  type: TYPE_NORMAL
- en: As computational power and data availability grew, more sophisticated techniques
    for natural language processing emerged. Among these was the **recurrent neural
    network** (**RNN**), which managed relationships across extended sequences and
    was proven to be effective in tasks where prior context influenced future predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, **long short-term memory networks** (**LSTMs**) were developed.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional RNNs, LSTMs had a unique ability to retain relevant long-term
    information while disregarding irrelevant data, maintaining semantic relationships
    across prolonged sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Further advancements led to the introduction of sequence-to-sequence models,
    often utilizing LSTMs as their underlying structure. These models revolutionized
    fields such as machine translation and text summarization by dramatically improving
    efficiency and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, NLP evolved from traditional statistical methods to advanced neural
    networks, transforming how we interacted with machines and enabling countless
    applications, such as machine translation and information retrieval (IR) (or finding
    relevant text based on a query). As the NLP field matured, incorporating the strengths
    of traditional statistical methods and advanced neural networks, a renaissance
    was forming. The next generation of NLP advancements would introduce transformer
    architectures, starting with the seminal paper *Attention is All You Need* and
    later the release of models such as BERT and eventually GPT.
  prefs: []
  type: TYPE_NORMAL
- en: Arrival and evolution of transformer-based models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The release of the research paper titled *Attention is All You Need* in 2017
    served as a paradigm shift in natural language processing. This pivotal paper
    introduced the transformer model, an architectural innovation that provided an
    unprecedented approach to sequential language tasks such as translation. The transformer
    model contrasted with prior models that processed sequences serially. Instead,
    it simultaneously processed different segments of an input sequence, determining
    its relevance based on the task. This innovative processing addressed the complexity
    of long-range dependencies in sequences, enabling the model to draw out the critical
    semantic information needed for a task. The transformer was such a critical advancement
    that nearly every state-of-the-art generative LLM applies some derivation of the
    original architecture. Its importance and influence motivate our detailed exploration
    and implementation of the original transformer in [*Chapter 3*](B21773_03.xhtml#_idTextAnchor081).
  prefs: []
  type: TYPE_NORMAL
- en: With the transformer came significant advancements in natural language processing,
    including GPT-1 or Generative Pretrained Transformer 1 (Radford et al., 2018).
    GPT-1 introduced a novel directional architecture to tackle diverse NLP tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Coinciding with GPT-1 was **BERT**, or **bidirectional encoder representations
    from transformers**, a pioneering work in the family of transformer-based models.
    BERT stood out among its predecessors, analyzing sentences forward and backward
    (or bi-directionally). This bidirectional analysis allowed BERT to capture semantic
    and syntactic nuances more effectively. At the time, BERT achieved unprecedented
    results when applied to complex natural language tasks such as named entity recognition,
    question answering, and sentiment analysis (Devlin et al., 2018).
  prefs: []
  type: TYPE_NORMAL
- en: Later, GPT-2, the much larger successor to GPT-1, attracted immense attention,
    as it greatly outperformed any of its predecessors across various tasks. In fact,
    GPT-2 was so unprecedented in its ability to generate human-like output that concerns
    about potential implications led to a delay in its initial release (Hern, 2019).
  prefs: []
  type: TYPE_NORMAL
- en: Amid early concerns, OpenAI followed up with the development of GPT-3, signaling
    a leap in the potential of LLMs. Developers demonstrated the potential of training
    at a massive scale, reaching 175 billion parameters (or adjustable variables learned
    during training), surpassing its two predecessors. GPT-3 was a “general-purpose”
    learner, capable of performing a wide range of natural language tasks learned
    implicitly from its training corpus instead of through task-specific fine-tuning.
    This capability sparked the exploration of foundation model development for general
    use across various domains and tasks. GPT-3’s distinct design and unprecedented
    scale led to a generation of generative models that could perform an indefinite
    number of increasingly complex downstream tasks learned implicitly through its
    extensive training.
  prefs: []
  type: TYPE_NORMAL
- en: Development and impact of GPT-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI’s development of GPT-4 marked a significant advance in the potential
    of large-scale, multimodal models. GPT-4, capable of processing image and text
    inputs and producing text outputs, represented yet another giant leap ahead of
    predecessors.
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 exhibited human-level performance on various professional and academic
    benchmarks. For instance, it passed a simulated bar exam with a score falling
    into the top 10% of test-takers (OpenAI, 2023).
  prefs: []
  type: TYPE_NORMAL
- en: A key distinction of GPT-4 is what happens after pretraining. Open AI applied
    **reinforcement learning with human feedback** (**RLHF**)—a type of risk/reward
    training derived from the same technique used to teach autonomous vehicles to
    make decisions based on the environment they encounter. In the case of GPT-4,
    the model learned to respond appropriately to a myriad of scenarios, incorporating
    human feedback along the way. This novel refinement strategy drastically improved
    the model’s propensity for factuality and its adherence to desired behaviors.
    The integration of RLHF demonstrated how models could be better aligned with human
    judgment toward the goal of responsible AI.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite demonstrating groundbreaking abilities, GPT-4 had similar limitations
    to earlier GPT models. It was not entirely reliable and had a limited context
    window (or input size). Meaning it could not receive large texts or documents
    as input. It was also prone to hallucination. As discussed, Hallucination is an
    anthropomorphized way of describing the model’s tendency to generate content that
    is not grounded in fact or reality. A hallucination occurs because generative
    language models (without augmentation) synthesize content purely based on semantic
    context and don’t perform any logical processing to verify factuality. This weakness
    presented meaningful risks, particularly in contexts where fact-based outcomes
    are paramount.
  prefs: []
  type: TYPE_NORMAL
- en: Despite limitations, GPT-4 made significant strides in language model performance.
    As with prior models, GPT-4’s development and potential use underscored the importance
    of safety and ethical considerations for future AI applications. As a result,
    the rise of GPT-4 accentuated the ongoing discussions and research into the potential
    implications of deploying such powerful models. In the next section, we briefly
    survey some of the known risks that are unique to generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: Looking ahead at risks and implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Both generative and discriminative AI introduce unique risks and benefits that
    must be weighed carefully. However, generative methods can not only carry forward
    but also exacerbate many risks associated with traditional ML while also introducing
    new risks. Consequently, before we can adopt generative AI in the real world and
    at scale, it is essential to understand the risks and establish responsible governance
    principles to help mitigate them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hallucination**: This is a term widely used to describe when models generate
    factually inaccurate information. Generative models are adept at producing plausible-sounding
    output without basis in fact. As such, it is critical to ground generative models
    with factual information. The term “grounding” refers to appending model inputs
    with additional information that is known to be factual. We explore grounding
    techniques in [*Chapter 7*](B21773_07.xhtml#_idTextAnchor225). Additionally, it
    is essential to have a strategy for evaluating model outputs that includes human
    review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plagiarism**: Since generative models are sometimes trained on uncrated datasets,
    some training corpora may have included data without explicit permissions. Models
    may produce information that is subject to copyright protections or can be claimed
    as intellectual property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accidental memorization**: As with many ML models that train on immense corpora,
    generative models tend to memorize parts of the training data. In particular,
    they are prone to memorizing sparse examples that do not fit neatly into a broader
    pattern. In some cases, models could memorize sensitive information that can be
    extracted and exposed (Brundage et al., 2020; Carlini et al., 2020). Consequently,
    whether consuming a pretrained model or fine-tuning (i.e., continued model training),
    training data curation is essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toxicity and bias**: Another byproduct of large-scale model training is that
    the model will inevitably learn any societal biases embedded in the training data.
    Biases can manifest as gender, racial, or socioeconomic biases in generated text
    or images, often replicating or amplifying stereotypes. We detail mitigations
    for this risk in [*Chapter 8*](B21773_08.xhtml#_idTextAnchor251).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With an understanding of some of the risks, we turn our focus to the nuanced
    implications of adopting generative AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ethical**: As discussed, these models inevitably learn and reproduce the
    biases inherent in the training data, raising serious ethical questions. Similarly,
    concerns about data privacy and security have emerged due to the model’s susceptibility
    to memorizing and exposing its training data. This has led to calls for robust
    ethical guidelines and data privacy regulations (Gebru et al., 2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environmental**: LLMs are computational giants, demanding unprecedented resources
    for training and implementation. Thus, they inevitably present environmental impacts.
    The energy consumption required to train an LLM produces substantial carbon dioxide
    emissions—roughly the equivalent lifetime emissions of five vehicles. Consequently,
    multiple efforts are underway to increase model efficiency and reduce carbon footprints.
    For example, techniques such as reduced bit precision training (or quantization)
    and parameter efficient fine-tuning (discussed in [*Chapter 5*](B21773_05.xhtml#_idTextAnchor180))
    reduce overall training time, helping to shrink carbon footprints.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social**: Along with environmental impacts, LLMs also have social implications.
    As these models become proficient at generating text, simulating intelligent conversation,
    and automating fundamental tasks, they present an unparalleled opportunity for
    job automation. Due to various complex factors, this potential for large-scale
    automation in the US may disproportionately affect marginalized or underrepresented
    communities. Thus, this amplifies prior concerns regarding labor rights and the
    need for additional protections to minimize harm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business and labor**: Along with broader socio-economic implications, we
    must examine more direct impacts on the business sector. While generative AI opens
    up new opportunities, changes in the labor market could bring about immense disruption
    if not addressed responsibly. Beyond labor impacts, AI advancements also significantly
    affect various business sectors. They can result in the creation of new roles,
    business models, and opportunities, requiring ongoing governance strategy and
    explorative frameworks that center on inclusivity, ethics, and responsible adoption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing these challenges will require technical and scientific improvements,
    data-specific regulations and laws, ethical guidelines, and human-centered AI
    governance strategies. These are integral to building an equitable, secure, and
    inclusive AI-driven future.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the history, risks, and limitations of generative AI, we are
    now better equipped to explore the vast opportunities and applications of such
    transformative technology.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing use cases of generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative AI has already begun to disrupt various sectors. The technology
    is making waves across many disciplines, from enhancing language-based tasks to
    reshaping digital art. The following section offers examples of real-world applications
    of generative AI across different sectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traditional natural language processing**: LLMs, such as Open AI’s GPT series,
    have elevated traditional NLP and NLG. As discussed, these models have a unique
    ability to generate coherent, relevant, and human-like text. The potential of
    these models was demonstrated when GPT-3 outperformed classical and modern approaches
    in several language tasks, displaying an unprecedented understanding of human
    language. The release of GPT-4 and Claude 3 marked another milestone, raising
    the standard even further for state-of-the-art models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Digital art creation**: The advent of “generative art” is evidence of the
    radical impact of generative AI in the field of digital art. For instance, artists
    can use AI generative models to create intricate designs, allowing them to focus
    on the conceptual aspect of art. It simplifies the process, reducing the need
    for high-level technical acumen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Music creation**: In the music industry, generative AI can enhance the composition
    process. Several platforms offer high-quality AI-driven music creation tools that
    can generate long-form musical compositions combining different music styles across
    various eras and genres.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlining business processes**: Several businesses have started employing
    generative AI to enable faster and more efficient processes. Generative AI-enabled
    operational efficiencies allow employees to focus on more strategic tasks. For
    example, fully integrated LLM email clients can organize emails and (combined
    with other technologies) learn to prioritize critical emails over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Entertainment**: While still largely experimental, LLMs show promising potential
    to disrupt creative writing and storytelling, particularly in the gaming industry.
    For example, procedural games could apply LLMs to enhance dynamic storytelling
    and create more engaging, personalized user experiences. As technology advances,
    we may see more mainstream adoption of LLMs in gaming, opening up new possibilities
    for interactive narratives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fashion**: In the fashion industry, generative models help designers innovate.
    By using a state-of-the-art generative AI model, designers can create and visualize
    new clothing styles by simply tweaking a few configurations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architecture and construction**: In the architectural world, generative-enhanced
    tools can help architects and urban planners optimize and generate design solutions,
    leading to more efficient and sustainable architectural designs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Food industry**: Emerging AI-driven cooking assistants can generate unique
    food combinations, novel recipes, and modified recipes for highly specific dietary
    needs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Education**: Generative AI-enhanced educational platforms offer the automatic
    creation of study aids that can facilitate personalized learning experiences and
    can automatically generate tailored content to accommodate specific and diverse
    learning styles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we must balance the breadth of opportunities with sophisticated guardrails
    and the continued promotion of ethical use. As data scientists, policymakers,
    and industry leaders, we must continue to work towards fostering an environment
    conducive to responsible AI deployment. That said, as generative AI continues
    to evolve, it presents a future replete with novel innovations and applications.
  prefs: []
  type: TYPE_NORMAL
- en: The future of generative AI applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The relentless advancement of generative AI presents a future filled with both
    possibilities and complex challenges. Imagine a future where a generative model
    trained on the world’s leading climate change research can offer practical yet
    groundbreaking counteractive strategies with precise details about their application.
  prefs: []
  type: TYPE_NORMAL
- en: However, as we embrace an increasingly AI-centered future, we should not overlook
    the existing challenges. These involve the potential misuse of AI tools, unpredictable
    implications, and the profound ethical considerations underlying AI adoption.
    Additionally, sustainable and eco-conscious development is key, as training large-scale
    models can be resource-intensive
  prefs: []
  type: TYPE_NORMAL
- en: In an age of accelerated progress, collaboration across all stakeholders—from
    data scientists, AI enthusiasts, and policymakers to industry leaders—is essential.
    By being equipped with comprehensive oversight, robust guidelines, and strategic
    education initiatives, concerted efforts can safeguard a future where generative
    AI is ubiquitous.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these hurdles, the transformative potential of generative AI remains
    unquestionable. With its capacity to reshape industries, redefine societal infrastructures,
    and alter our ways of living, learning, and working, generative AI serves as a
    reminder that we are experiencing a pivotal moment—one propelled by decades of
    scientific research and computational ingenuity that are coalescing to bring us
    forward as a society.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we traced the evolution of generative AI, distinguished it
    from traditional ML, explored its evolution, discussed its risks and implications,
    and, hopefully, dispelled some common misconceptions. We contemplated some of
    the possibilities anchored by consideration for its responsible adoption.
  prefs: []
  type: TYPE_NORMAL
- en: As we move on to the next chapter, we will examine the fundamental architectures
    behind generative AI, giving us a foundational understanding of the key generative
    methods, including GANs, diffusion models, and transformers. These ML methods
    form the backbone of generative AI and have been instrumental in bringing about
    the remarkable advancements we see today.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This reference section serves as a repository of sources referenced within
    this book; you can explore these resources to further enhance your understanding
    and knowledge of the subject matter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://doi.org/10.1007/s11023-020-09526-7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction](https://doi.org/10.1007/s11023-020-09526-7https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B.,
    Dafoe, A., Scharre, P., Zeitzoff, T., Filar, B., Anderson, H., Roff, H., Allen,
    G. C., Steinhardt, J., Flynn, C., Ó hÉigeartaigh, S., Beard, S., Belfield, H.,
    Farquhar, S., & Amodei, D. (2018). *The malicious use of artificial intelligence:
    Forecasting, prevention, and mitigation*. arXiv [cs.AI]. [http://arxiv.org/abs/1802.07228](http://arxiv.org/abs/1802.07228).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee,
    K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., & Raffel, C.
    (2020). *Extracting training data from large language models*. arXiv [cs.CR].
    [http://arxiv.org/abs/2012.07805](http://arxiv.org/abs/2012.07805).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). *BERT: Pre-training
    of deep bidirectional transformers for language understanding*. arXiv [cs.CL].
    [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hagendorff, T. (2020). Publisher correction to *The ethics of AI ethics: An
    evaluation of guidelines*. *Minds and Machines*, 30(3), 457–461\. [https://doi.org/10.1007/s11023-020-09526-7](https://doi.org/10.1007/s11023-020-09526-7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hern, A. (2019, February 14). *New AI fake text generator may be too dangerous
    to release, say creators*. *The* *Guardian*. [https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction](https://www.theguardian.com/technology/2019/feb/14/elon-musk-backed-ai-writes-convincing-news-fiction).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho, J., Jain, A., & Abbeel, P. (2020). *Denoising diffusion probabilistic models*.
    arXiv [cs.LG]. [http://arxiv.org/abs/2006.11239](http://arxiv.org/abs/2006.11239).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R.,
    Gray, S., Kingma, D. P., & Welling, M. (2013). *Auto-encoding variational bayes*.
    arXiv [stat.ML]. [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Muhammad, T., Aftab, A. B., Ahsan, M. M., Muhu, M. M., Ibrahim, M., Khan, S.
    I., & Alam, M. S. (2022). *Transformer-based deep learning model for stock price
    prediction: A case study on Bangladesh stock market*. arXiv [q-fin.ST]. [http://arxiv.org/abs/2208.08300](http://arxiv.org/abs/2208.08300).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI. (2023). *GPT-4 technical report*. arXiv [cs.CL]. [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018).
    *Language models are unsupervised* *multitask learners*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, L., & Polosukhin, I. (2017). *Attention Is All You Need*. arXiv [cs.CL].
    [http://arxiv.org/abs/1706.03762](http://arxiv.org/abs/1706.03762).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
