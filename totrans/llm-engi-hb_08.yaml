- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Inference Optimization
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理优化
- en: Deploying LLMs is challenging due to their significant computational and memory
    requirements. Efficiently running these models necessitates the use of specialized
    accelerators, such as GPUs or TPUs, which can parallelize operations and achieve
    higher throughput. While some tasks, like document generation, can be processed
    in batches overnight, others require low latency and fast generation, such as
    code completion. As a result, optimizing the inference process – how these models
    make predictions based on input data – is critical for many practical applications.
    This includes reducing the time it takes to generate the first token (latency),
    increasing the number of tokens generated per second (throughput), and minimizing
    the memory footprint of LLMs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM具有显著的计算和内存需求，部署LLM具有挑战性。高效运行这些模型需要使用专门的加速器，如GPU或TPU，这些加速器可以并行化操作并实现更高的吞吐量。虽然一些任务，如文档生成，可以在夜间批量处理，但其他任务，如代码补全，则需要低延迟和快速生成。因此，优化推理过程——即这些模型如何根据输入数据进行预测——对于许多实际应用至关重要。这包括减少生成第一个标记所需的时间（延迟），增加每秒生成的标记数量（吞吐量），以及最小化LLM的内存占用。
- en: Indeed, naive deployment approaches lead to poor hardware utilization and underwhelming
    throughput and latency. Fortunately, a variety of optimization techniques have
    emerged to dramatically speed up inference. This chapter will explore key methods
    like speculative decoding, model parallelism, and weight quantization, demonstrating
    how thoughtful implementations can achieve speedups of 2–4X or more. We will also
    introduce three popular inference engines (Text Generation Inference, vLLM, and
    TensorRT-LLM) and compare their features in terms of inference optimization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，简单的部署方法会导致硬件利用率低，吞吐量和延迟不尽如人意。幸运的是，各种优化技术已经出现，可以显著加快推理速度。本章将探讨关键方法，如推测性解码、模型并行化和权重量化，展示精心设计的实现如何实现2-4倍或更高的加速。我们还将介绍三种流行的推理引擎（文本生成推理、vLLM和TensorRT-LLM），并比较它们在推理优化方面的特性。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Model optimization strategies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型优化策略
- en: Model parallelism
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型并行化
- en: Model quantization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型量化
- en: By the end of this chapter, you will understand the core challenges in LLM inference
    and be familiar with state-of-the-art optimization techniques, including model
    parallelism and weight quantization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解LLM推理的核心挑战，并熟悉最先进的优化技术，包括模型并行化和权重量化。
- en: All the code examples from this chapter can be found on GitHub at [https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在GitHub上找到，链接为[https://github.com/PacktPublishing/LLM-Engineering](https://github.com/PacktPublishing/LLM-Engineering)。
- en: Model optimization strategies
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化策略
- en: Most of the LLMs used nowadays, like GPT or Llama, are powered by a decoder-only
    Transformer architecture. The *decoder-only* architecture is designed for text-generation
    tasks. It predicts the next word in a sequence based on preceding words, making
    it effective for generating contextually appropriate text continuations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数使用的LLM，如GPT或Llama，都由仅包含解码器的Transformer架构驱动。仅包含解码器的架构是为文本生成任务设计的。它根据前面的单词预测序列中的下一个单词，这使得它在生成上下文适当的文本续写方面非常有效。
- en: In contrast, an *encoder-only* architecture, like BERT, focuses on understanding
    and representing the input text with detailed embeddings. It excels in tasks that
    require comprehensive context understanding, such as text classification and named
    entity recognition. Finally, the encoder-decoder architecture, like T5, combines
    both functionalities. The encoder processes the input text to generate a context-rich
    representation, which the decoder then uses to produce the output text. This dual
    structure is particularly powerful for sequence-to-sequence tasks like translation
    and summarization, where understanding the input context and generating a relevant
    output are equally important.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，仅包含编码器架构，如BERT，专注于通过详细的嵌入来理解和表示输入文本。它在需要全面上下文理解的任务中表现出色，例如文本分类和命名实体识别。最后，编码器-解码器架构，如T5，结合了这两种功能。编码器处理输入文本以生成丰富的上下文表示，解码器随后使用这些表示来生成输出文本。这种双重结构对于翻译和摘要等序列到序列任务特别强大，在这些任务中，理解输入上下文和生成相关输出同等重要。
- en: In this book, we only focus on the decoder-only architecture, which dominates
    the LLM field.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们只关注仅包含解码器的架构，该架构主导了LLM领域。
- en: '![](img/B31105_08_01.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_08_01.png)'
- en: Figure 8.1 – Inference process with decoder-only models. We provide “I have
    a dream” as input and obtain “of” as output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 仅使用解码器模型的推理过程。我们以“我有一个梦想”作为输入，得到“of”作为输出。
- en: 'As shown in *Figure 8.1*, the basic inference process for a decoder-only model
    involves:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.1*所示，仅解码器模型的基本推理过程包括：
- en: '**Tokenizing** the input prompt and passing it through an embedding layer and
    positional encoding.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**标记化**输入提示并将其通过嵌入层和位置编码。'
- en: '**Computing** key and value pairs for each input token using the multi-head
    attention mechanism.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多头注意力机制为每个输入标记计算键和值对。
- en: '**Generating** output tokens sequentially, one at a time, using the computed
    keys and values.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逐个、顺序地**生成**输出标记，使用计算出的键和值。
- en: While *Steps 1* and *2* are computationally expensive, they consist of highly
    parallelizable matrix multiplication that can achieve high hardware utilization
    on accelerators like GPUs and TPUs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*步骤1*和*步骤2*在计算上很昂贵，但它们由高度可并行化的矩阵乘法组成，可以在GPU和TPU等加速器上实现高硬件利用率。
- en: The real challenge is that the token generation in *Step 3* is inherently sequential
    – to generate the next token, you need to have generated all previous tokens.
    This leads to an iterative process where the output sequence is grown one token
    at a time, failing to leverage the parallel computing capabilities of the hardware.
    Addressing this bottleneck is one of the core focuses of inference optimization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的挑战在于*步骤3*中的标记生成本质上是顺序的 – 要生成下一个标记，你需要生成所有之前的标记。这导致了一个迭代过程，其中输出序列逐个标记地增长，未能利用硬件的并行计算能力。解决这个瓶颈是推理优化的核心重点之一。
- en: In this section, we will detail several optimization strategies that are commonly
    used to speed up inference and reduce **Video Random-Access Memory** (**VRAM**)
    usage, such as implementing a (static) KV cache, continuous batching, speculative
    decoding, and optimized attention mechanisms.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细介绍几种常用的优化策略，这些策略可以加快推理速度并减少**视频随机存取存储器**（**VRAM**）的使用，例如实现（静态）KV缓存、连续批处理、推测解码和优化的注意力机制。
- en: KV cache
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KV缓存
- en: We saw that LLMs generate text token by token, which is slow because each new
    prediction depends on the entire previous context. For example, to predict the
    100^(th) token in a sequence, the model needs the context of tokens 1 through
    99\. When predicting the 101^(st) token, it again needs the information from tokens
    1 through 99, plus token 100\. This repeated computation is particularly inefficient.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，大型语言模型（LLMs）是逐个生成文本标记的，这很慢，因为每个新的预测都依赖于整个先前的上下文。例如，要预测序列中的第100个标记，模型需要1到99个标记的上下文。在预测第101个标记时，它又需要1到99个标记的信息，加上标记100。这种重复计算特别低效。
- en: The **key-value** (**KV**) cache addresses this issue by storing key-value pairs
    produced by self-attention layers. Instead of recalculating these pairs for each
    new token, the model retrieves them from the cache, significantly speeding up
    the generation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**键值对**（**KV**）缓存通过存储自注意力层生成的键值对来解决此问题。模型不再为每个新标记重新计算这些对，而是从缓存中检索它们，从而显著加快了生成速度。'
- en: 'You can see an illustration of this technique in *Figure 8.2*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图8.2*中看到这种技术的说明：
- en: '![An illustration of KV caching depicted in Prefill and Decode phases. Prefill
    is a highly parallelized operation where the KV tensors of all input tokens can
    be computed simultaneously. During decode, new KV tensors and subsequently the
    output token at each step is computed autoregressively. ](img/B31105_08_02.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![在预填充和解码阶段展示的KV缓存的说明图。预填充是一个高度并行化的操作，其中所有输入标记的KV张量可以同时计算。在解码过程中，新的KV张量和随后每个步骤的输出标记是自回归地计算的。](img/B31105_08_02.png)'
- en: Figure 8.2 – Illustration of the KV cache
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – KV缓存的说明
- en: When a new token is generated, only the key and value for that single token
    need to be computed and added to the cache. The KV cache is an immediate optimization
    that is implemented in every popular tool and library. Some implementations maintain
    a separate KV cache for each layer of the model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当生成新标记时，只需要计算并添加到缓存中该单个标记的键和值。KV缓存是每个流行工具和库中实施的即时优化。一些实现为模型的每一层维护一个单独的KV缓存。
- en: 'The size of the KV cache scales with the number of tokens (![](img/B31105_08_001.png))
    and several model dimensions, like the number of layers (![](img/B31105_08_002.png)),
    the number of attention heads (![](img/B31105_08_003.png)), their dimension (![](img/B31105_08_004.png)),
    and the precision of the parameters in bytes (![](img/B31105_08_005.png)):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存的大小与标记数（![](img/B31105_08_001.png)）和几个模型维度成比例，如层数（![](img/B31105_08_002.png)）、注意力头数（![](img/B31105_08_003.png)）、它们的维度（![](img/B31105_08_004.png)）以及参数的精度（以字节为单位）（![](img/B31105_08_005.png)）：
- en: '![](img/B31105_08_006.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_08_006.png)'
- en: For a typical 7B parameter model using 16-bit precision, this exceeds 2 GB for
    high sequence lengths (higher than 2,048 tokens). Larger models with more layers
    and higher embedding dimensions will see even greater memory requirements.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个典型的7B参数模型，使用16位精度，对于高序列长度（高于2,048个标记），这超过了2 GB。具有更多层和更高嵌入维度的更大模型将看到更大的内存需求。
- en: Since the KV cache grows with each generation step and is dynamic, it prevents
    you from taking advantage of torch.compile, a powerful optimization tool that
    fuses PyTorch code into fast and optimized kernels. The *static KV cache* solves
    this issue by pre-allocating the KV cache size to a maximum value, which allows
    you to combine it with `torch.compile` for up to a 4x speedup in the forward pass.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于KV缓存随着每个生成步骤的增长而增长且是动态的，它阻止了你利用torch.compile，这是一个强大的优化工具，可以将PyTorch代码融合到快速和优化的内核中。*静态KV缓存*通过预先分配KV缓存大小到最大值来解决这个问题，这允许你将其与`torch.compile`结合使用，在正向传递中实现高达4倍的速度提升。
- en: 'To configure a model to use a static KV cache with the transformers library,
    follow these steps:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置模型使用transformers库中的静态KV缓存，请按照以下步骤操作：
- en: 'We import the tokenizer and the model we want to optimize:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入标记化器和我们要优化的模型：
- en: '[PRE0]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To implement the static cache, we change the cache implementation in the model’s
    generation config to `static`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要实现静态缓存，我们将模型生成配置中的缓存实现更改为`static`：
- en: '[PRE1]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that our KV cache is static, we can compile the model using torch.compile:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在KV缓存是静态的，我们可以使用torch.compile编译模型：
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We tokenize an input question, “`What is 2+2?`", and store it on a GPU if available
    (if not, we store it on the CPU):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对输入问题进行标记化，“`2+2等于多少？`"，并在可用的GPU上存储它（如果不可用，我们将其存储在CPU上）：
- en: '[PRE3]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let’s use the `generate()` method to get the model’s output and decode it with
    `batch_decode()` to print its answer:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用`generate()`方法获取模型的输出，并用`batch_decode()`解码以打印其答案：
- en: '[PRE4]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This returns a list containing both the input and output, correctly answering
    our question.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回一个包含输入和输出的列表，正确回答了我们的问题。
- en: Note that the static cache doesn’t work with all architectures. For details
    on which architectures are supported, check out the transformers documentation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，静态缓存并不适用于所有架构。有关支持哪些架构的详细信息，请查看transformers文档。
- en: Efficiently managing the KV cache is essential, as it can quickly exhaust available
    GPU memory and limit the batch sizes that can be processed. This has motivated
    the development of memory-efficient attention mechanisms and other techniques,
    which we will cover in the last section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有效管理KV缓存至关重要，因为它可以快速耗尽可用的GPU内存并限制可以处理的批处理大小。这促使开发了内存高效的注意力机制和其他技术，我们将在最后一节中介绍。
- en: Continuous batching
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连续批处理
- en: Batching, or processing multiple inference requests simultaneously, is a standard
    approach to achieve high throughput. Larger batch sizes spread out the memory
    cost of model weights and transfer more data to the GPU at once, better saturating
    its parallel compute capacity.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理，或同时处理多个推理请求，是实现高吞吐量的标准方法。较大的批处理大小可以分散模型权重的内存成本，并一次性传输更多数据到GPU，更好地饱和其并行计算能力。
- en: However, decoder-only models pose a particular challenge due to the high variability
    in input prompt lengths and desired output lengths. Some requests may have short
    prompts and only need a one-word answer, while others may input a lengthy context
    and expect a multi-paragraph response.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅解码器模型由于输入提示长度和期望输出长度的极高可变性而带来特定的挑战。一些请求可能有简短的提示并且只需要一个单词的回答，而其他请求可能输入一个冗长的上下文并期望多段落的响应。
- en: With traditional batching, we would have to wait for the longest request in
    a batch to complete before starting a new batch. This leads to under-utilization
    as the accelerator sits partly idle waiting for a straggling request to finish.
    *Continuous batching*, also known as `in-flight` batching, aims to prevent idle
    time by immediately feeding a new request into the batch as soon as one completes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的批处理中，我们必须等待批处理中请求的最长时间完成，然后才能开始新的批处理。这导致加速器部分空闲，等待拖沓的请求完成，从而造成低效。**连续批处理**，也称为**飞行中批处理**，旨在通过在请求完成时立即将新的请求喂入批处理来防止空闲时间。
- en: The batching process begins the same – by filling the batch with initial requests.
    But as soon as a request completes its generation, it is evicted from the batch
    and a new request takes its place. This way, the accelerator is always processing
    a full batch, leading to maximally efficient hardware utilization. An additional
    consideration is the need to periodically pause the generation process to run
    prefill, or the embedding and encoding of waiting requests. Finding the optimal
    balance between generation and prefill requires some tuning of the waiting-served
    ratio hyperparameter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理过程开始时相同——通过填充初始请求。但是，一旦一个请求完成其生成，它就会被从批处理中移除，一个新的请求取而代之。这样，加速器始终在处理一个完整的批处理，从而实现最大化的硬件利用率。另一个需要考虑的因素是需要定期暂停生成过程以运行预填充，即等待请求的嵌入和编码。在生成和预填充之间找到最佳平衡需要调整等待-服务比率超参数。
- en: Continuous batching is natively implemented in most inference frameworks, like
    Hugging Face’s **Text Generation Inference** (**TGI**), vLLM, and NVIDIA TensorRT-LLM.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 连续批处理在大多数推理框架中是原生实现的，例如Hugging Face的**文本生成推理**（**TGI**）、vLLM和NVIDIA TensorRT-LLM。
- en: Speculative decoding
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推测解码
- en: Another powerful optimization technique is *speculative decoding*, also called
    assisted generation. The key insight is that even with continuous batching, the
    token-by-token generation process fails to fully saturate the parallel processing
    capabilities of the accelerator. Speculative decoding aims to use this spare compute
    capacity to predict multiple tokens simultaneously, using a smaller proxy model
    (see *Figure 8.3*).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种强大的优化技术是**推测解码**，也称为辅助生成。关键洞察是，即使在连续批处理中，按标记逐个生成的过程也无法充分利用加速器的并行处理能力。推测解码旨在使用这种额外的计算能力同时预测多个标记，使用较小的代理模型（见*图8.3*）。
- en: '![](img/B31105_08_03.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_08_03.png)'
- en: Figure 8.3 – Illustration of traditional decoding (left) and speculative decoding
    (right)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 传统解码（左）和推测解码（右）的示意图
- en: 'The general approach is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通用方法是：
- en: Apply a smaller model, like a distilled or pruned version of the main model,
    to predict multiple token completions in parallel. This could be 5–10 tokens predicted
    in a single step.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用一个较小的模型，例如主模型的蒸馏或剪枝版本，以并行预测多个标记补全。这可能是单步预测5-10个标记。
- en: Feed these speculative completions into the full model to validate which predictions
    match what the large model would have generated.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些推测补全输入到完整模型中，以验证哪些预测与大型模型生成的结果相匹配。
- en: Retain the longest matching prefix from the speculative completions and discard
    any incorrect tokens.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留推测补全中最长的匹配前缀，并丢弃任何错误的标记。
- en: The result is that, if the small model approximates the large model well, multiple
    tokens can be generated in a single step. This avoids running the expensive large
    model for several iterations. The degree of speedup depends on the quality of
    the small model’s predictions – a 90% match could result in a 3–4X speedup.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，如果小模型很好地近似大模型，可以在单步中生成多个标记。这避免了多次迭代运行昂贵的大型模型。加速的程度取决于小模型预测的质量——90%的匹配可能导致3-4倍的速度提升。
- en: 'It is crucial that both models use the same tokenizer. If this is not the case,
    the tokens generated by the draft model will not align with those produced by
    the large model, making them incompatible. Let’s implement this using the transformers
    library. In this example, we will use two Qwen1.5 models from Alibaba Cloud: a
    1.8B version as the main model, and a 0.5B version as the draft model. Note that,
    if you have enough VRAM, you can use much larger models like 14B, 32B, 72B, or
    110B as the main model.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 确保两个模型使用相同的分词器至关重要。如果不是这样，草稿模型生成的标记将不会与大型模型产生的标记对齐，使它们不兼容。让我们使用transformers库来实现这一点。在这个例子中，我们将使用来自阿里巴巴云的两个Qwen1.5模型：一个1.8B版本作为主模型，一个0.5B版本作为草稿模型。注意，如果您有足够的VRAM，您可以使用更大的模型，如14B、32B、72B或110B作为主模型。
- en: Here, we’re limited by the VRAM of the T4 GPU in Google Colab, but to get the
    maximum speedup, the assistant model should be much smaller than the large model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们受限于 Google Colab 中 T4 GPU 的 VRAM，但要获得最大的加速效果，辅助模型应该远小于大型模型。
- en: 'Here’s a step-by-step guide to implement speculative decoding:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是实现推测性解码的逐步指南：
- en: 'We load the tokenizer and both models:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了分词器和两个模型：
- en: '[PRE5]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We then tokenize the same input and store it in the accelerator, if available:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将相同的输入进行分词并存储在加速器中，如果有的话：
- en: '[PRE6]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now use `model.generate()` with the argument `assistant_model` to enable
    speculative decoding:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用 `model.generate()` 并带有 `assistant_model` 参数来启用推测性解码：
- en: '[PRE7]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The speedup in this small example is not significant, but it is clearly noticeable
    with bigger models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个小型示例中，加速效果并不显著，但使用更大的模型时，这种加速效果是明显可感知的。
- en: '*Prompt lookup decoding* is a variant of speculative decoding, tailored to
    input-grounded tasks like summarization where there is often overlap between the
    prompt and output. Shared n-grams are used as the LLM candidate tokens. We can
    enable prompt lookup decoding by using the `prompt_lookup_num_tokens` parameter
    in `model.generate()`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*提示查找解码* 是一种推测性解码的变体，适用于输入与输出有重叠的任务，如摘要。在这里，共享的 n-gram 被用作 LLM 的候选标记。我们可以通过在
    `model.generate()` 中使用 `prompt_lookup_num_tokens` 参数来启用提示查找解码：'
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By combining the static KV cache with torch.compile, implementing continuous
    batching, and leveraging speculative decoding techniques, LLMs can see inference
    speedups of 2–4x or more with no loss in quality.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合静态 KV 缓存、使用 torch.compile 实现连续批处理以及利用推测性解码技术，LLMs 可以在质量不受损失的情况下看到 2-4 倍或更多的推理速度提升。
- en: Another approach to creating a small proxy model consists of jointly fine-tuning
    a small model alongside a large model for maximum fidelity. A representative technique
    here is Medusa, which inserts dedicated speculation heads into the main model.
    The Medusa-1 approach fine-tunes these speculation heads while freezing the large
    model, while the Medusa-2 approach jointly fine-tunes both the speculation heads
    and the large model. The Medusa method has demonstrated impressive results, enabling
    a 70M parameter model to closely approximate the performance of a 7B parameter
    model on a range of tasks. Speculative decoding is natively supported by TGI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 创建小型代理模型的另一种方法是与大型模型联合微调一个小型模型，以实现最大的保真度。这里的一个代表性技术是 Medusa，它将专门的推测头插入到主模型中。Medusa-1
    方法在冻结大型模型的同时微调这些推测头，而 Medusa-2 方法联合微调推测头和大型模型。Medusa 方法已经展示了令人印象深刻的结果，使一个 70M
    参数的模型能够在一系列任务上接近一个 7B 参数模型的性能。推测性解码在 TGI 中原生支持。
- en: Optimized attention mechanisms
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化的注意力机制
- en: The Transformer architecture is based on the attention mechanism, which scales
    quadratically with the number of input tokens (or sequence length). This is particularly
    inefficient for longer sequences, where the size of the KV cache can blow up.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构基于注意力机制，其复杂度与输入标记数（或序列长度）的平方成正比。这对于较长的序列来说尤其低效，因为 KV 缓存的大小可能会急剧增加。
- en: Introduced by Kwon, Li, et al. (2023), *PagedAttention* addresses these memory
    challenges by drawing inspiration from virtual memory and paging in operating
    systems. It partitions the KV cache into blocks, eliminating the need for contiguous
    memory allocation. Each block contains the keys and values for a fixed number
    of tokens. During attention computation, the PagedAttention kernel efficiently
    fetches these blocks, regardless of their physical memory location.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Kwon、Li 等人（2023）提出的 *PagedAttention* 通过从操作系统的虚拟内存和分页中汲取灵感来解决这些内存挑战。它将 KV 缓存划分为块，消除了对连续内存分配的需求。每个块包含一定数量的标记的键和值。在注意力计算过程中，PagedAttention
    内核有效地检索这些块，无论它们的物理内存位置如何。
- en: This partitioning allows for near-optimal memory utilization. This is useful
    for batching more sequences together, which increases throughput and GPU utilization.
    Moreover, `PagedAttention`'s block-based approach naturally supports memory sharing
    across multiple output sequences generated from the same prompt. This is particularly
    advantageous in parallel sampling and beam search, where the same prompt is used
    to generate multiple outputs. The shared memory blocks reduce redundant computations
    and memory usage, cutting the memory overhead by up to 55% and improving throughput
    by up to 2.2x, according to the authors. The vLLM library received the first implementation
    of PagedAttention. Since then, PagedAttention has also been implemented in TGI
    and TensorRT-LLM.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Another popular option is *FlashAttention-2*. Developed by Tri Dao (2023), it
    introduced several key innovations that are designed to address the quadratic
    runtime and memory constraints in traditional attention. By dividing input and
    output matrices into smaller blocks, FlashAttention-2 ensures that these blocks
    can fit into the GPU’s on-chip SRAM, which is much faster than high-bandwidth
    memory. This approach significantly reduces the frequency of data transfers between
    the GPU’s main memory and its processing units.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: This is combined with online softmax, which computes the softmax function independently
    for each block of the attention scores matrix, rather than for the entire matrix
    at once. By maintaining a running maximum and a running sum of exponentials, FlashAttention-2
    can calculate attention probabilities without needing to store large intermediate
    matrices.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, FlashAttention-2’s online softmax computation enables block-wise
    processing, maintaining accuracy while significantly reducing memory requirements.
    This is particularly important for training, where the recomputation of intermediate
    values (instead of storing them) in the backward pass reduces memory usage from
    quadratic to linear, in relation to sequence length.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike PagedAttention, FlashAttention-2 can easily be used with the transformers
    library through the `attn_implementation` parameter:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the `flash-attn` library with `--no-build-isolation` so that we don’t
    install the dependencies:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'To use FlashAttention-2 for inference, specify `flash_attention_2` in the `attn_implementation`
    parameter when loading a model. For example, this is how to load Mistral-7B-Instruct-v0.3
    with FlashAttention-2:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The techniques presented in this section focused on improving the model’s efficiency
    in processing tokens. In the next section, we will discuss how to distribute our
    model and calculations across multiple GPUs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model parallelism allows you to distribute the memory and compute requirements
    of LLMs across multiple GPUs. This enables the training and inference of models
    too large to fit on a single device, while also improving performance in terms
    of throughput (tokens per second).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main approaches to model parallelism, each involving splitting
    the model weights and computation in different ways: *data parallelism*, *pipeline
    parallelism*, and *tensor parallelism*.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行有三种主要方法，每种方法都涉及以不同的方式分割模型权重和计算：*数据并行*、*流水线并行*和*张量并行*。
- en: Although these approaches were originally developed for training, we can reuse
    them for inference by focusing on the forward pass only.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法最初是为训练开发的，但我们可以通过仅关注正向传递来重新用于推理。
- en: Data parallelism
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行
- en: '**Data parallelism** (**DP**) is the simplest type of model parallelism. It
    involves making copies of the model and distributing these replicas across different
    GPUs (see *Figure 8.4*). Each GPU processes a subset of the data simultaneously.
    During training, the gradients calculated on each GPU are averaged and used to
    update the model parameters, ensuring that each replica remains synchronized.
    This approach is particularly beneficial when the batch size is too large to fit
    into a single machine or when aiming to speed up the training process.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据并行**（**DP**）是最简单的一种模型并行方式。它涉及复制模型并在不同的GPU上分配这些副本（参见*图8.4*）。每个GPU同时处理数据的一个子集。在训练过程中，每个GPU上计算的梯度被平均并用于更新模型参数，确保每个副本保持同步。这种方法在批大小太大而无法适应单个机器或旨在加快训练过程时特别有益。'
- en: '![](img/B31105_08_04.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_08_04.png)'
- en: Figure 8.4 – Illustration of data parallelism with four GPUs
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 使用四个GPU的数据并行示意图
- en: During inference, DP can be useful for processing concurrent requests. By distributing
    the workload across multiple GPUs, this approach helps reduce latency, as multiple
    requests can be handled simultaneously. This concurrent processing also increases
    throughput, since a higher number of requests can be processed at the same time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，DP（数据并行）可以用于处理并发请求。通过将工作负载分配到多个GPU上，这种方法有助于降低延迟，因为可以同时处理多个请求。这种并发处理也提高了吞吐量，因为可以同时处理更多的请求。
- en: However, the effectiveness of DP is limited by the model size and the communication
    overhead between GPUs. Indeed, replicating the model’s parameters on each GPU
    is inefficient. This means that this technique only works when the model is small
    enough to fit into a single GPU, leaving less room for input data and thus limiting
    the batch size. For larger models or when memory is a constraint, this can be
    a significant drawback.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，DP的有效性受到模型大小和GPU之间通信开销的限制。实际上，在每个GPU上复制模型参数是不高效的。这意味着这种技术仅在模型足够小，可以适应单个GPU时才有效，从而为输入数据留下较少的空间，从而限制了批大小。对于更大的模型或当内存成为限制时，这可能是一个重大的缺点。
- en: Typically, DP is mainly used for training, while pipeline and tensor parallelism
    are preferred for inference.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，DP主要用于训练，而流水线和张量并行则更适用于推理。
- en: Pipeline parallelism
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流水线并行
- en: Introduced by Huang et al. in the GPipe paper (2019), **pipeline parallelism**
    (**PP**) is a strategy for distributing the computational load of training and
    running large neural networks across multiple GPUs.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由Huang等人于2019年在GPipe论文中引入的**流水线并行**（**PP**）是一种将训练和运行大型神经网络的计算负载分配到多个GPU的策略。
- en: Unlike traditional DP, which replicates the entire model on each GPU, pipeline
    parallelism partitions the model’s layers across different GPUs. This approach
    allows each GPU to handle a specific portion of the model, thereby reducing the
    memory burden on individual GPUs.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 与在传统DP中在每个GPU上复制整个模型不同，流水线并行将模型的层分配到不同的GPU上。这种方法允许每个GPU处理模型的一个特定部分，从而减轻了单个GPU的内存负担。
- en: '![](img/B31105_08_05.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_08_05.png)'
- en: Figure 8.5 – Illustration of pipeline parallelism with four GPUs
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 使用四个GPU的流水线并行示意图
- en: As shown in *Figure 8.5*, in a typical four-way pipeline parallel split, the
    model is divided into four segments, with each segment assigned to a different
    GPU. The first 25% of the model’s layers might be processed by GPU 1, the next
    25% by GPU 2, and so on. During the forward pass, activations are computed and
    then passed along to the next GPU. For training, the backward pass follows a similar
    sequence in reverse, with gradients being propagated back through the GPUs. The
    number of GPUs is often referred to as the degree of parallelism.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图8.5*所示，在典型的四路管道并行分割中，模型被分为四个部分，每个部分分配给不同的GPU。模型的前25%层可能由GPU 1处理，接下来的25%由GPU
    2处理，依此类推。在正向传递过程中，激活被计算然后传递到下一个GPU。对于训练，反向传递遵循类似的顺序，但方向相反，梯度通过GPU反向传播。GPU的数量通常被称为并行度。
- en: The primary advantage of pipeline parallelism is its ability to significantly
    reduce the memory requirements per GPU. However, this approach introduces new
    challenges, particularly related to the sequential nature of the pipeline. One
    of the main issues is the occurrence of “pipeline bubbles.” These bubbles arise
    when some GPUs are idle, waiting for activations from preceding layers. This idle
    time can reduce the overall efficiency of the process.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行化的主要优势在于其能够显著降低每个GPU的内存需求。然而，这种方法引入了新的挑战，尤其是与管道的顺序性质相关的问题。其中一个主要问题是“管道气泡”的出现。这些气泡发生在某些GPU空闲，等待前一层激活时。这种空闲时间可能会降低整个过程的效率。
- en: Micro-batching was developed to mitigate the impact of pipeline bubbles. By
    splitting the input batch into smaller sub-batches, micro-batching ensures that
    GPUs remain busier, as the next sub-batch can begin processing before the previous
    one is fully completed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 微批处理是为了减轻管道气泡的影响而开发的。通过将输入批次分成更小的子批次，微批处理确保GPU保持忙碌，因为下一个子批次可以在前一个子批次完全完成之前开始处理。
- en: '![](img/B31105_08_06.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_08_06.png)'
- en: Figure 8.6 – Illustration of pipeline parallelism with micro-batching.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 微批处理管道并行的示意图。
- en: '*Figure 8.6* shows an example of pipeline parallelism with micro-batching.
    In this example, the pipeline has four stages (**F0**, **F1**, **F2**, **F3**),
    and the input batch is divided into four micro-batches. GPU 0 will process forward
    paths **F0**,0, **F0**,1, **F0**,2, and **F0**,3, sequentially. Once **F0**,0
    is complete, GPU 1 can immediately start processing **F1**,0 and so on. After
    completing these forward passes, GPU 0 waits for the other GPUs to finish their
    respective forward computations before starting the backward paths (**B0**,3,
    **B0**,2, **B0**,1, and **B0**,0).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*图8.6*展示了带有微批处理的管道并行化示例。在这个例子中，管道有四个阶段（**F0**、**F1**、**F2**、**F3**），输入批次被分为四个微批次。GPU
    0将依次处理正向路径**F0**，0、**F0**，1、**F0**，2和**F0**，3。一旦**F0**，0完成，GPU 1可以立即开始处理**F1**，0，依此类推。完成这些正向传递后，GPU
    0等待其他GPU完成各自的正向计算，然后开始反向路径（**B0**，3、**B0**，2、**B0**，1和**B0**，0）。'
- en: Pipeline parallelism is implemented in distributed training frameworks like
    Megatron-LM, DeepSpeed (ZeRO), and PyTorch through the dedicated **Pipeline Parallelism
    for PyTorch** (**PiPPy**) library. At the time of writing, only certain inference
    frameworks like TensorRT-LLM support pipeline parallelism.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行化在分布式训练框架如Megatron-LM、DeepSpeed (ZeRO)和PyTorch中通过专门的**PyTorch管道并行化库**（**PiPPy**）实现。在撰写本文时，只有某些推理框架如TensorRT-LLM支持管道并行化。
- en: Tensor parallelism
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 张量并行化
- en: Introduced by Shoeby, Patwary, Puri et al. in the Megatron-LM paper (2019),
    **tensor parallelism** (**TP**) is another popular technique to distribute the
    computation of LLM layers across multiple devices. In contrast to pipeline parallelism,
    TP splits the weight matrices found in individual layers. This enables simultaneous
    computations, significantly reducing memory bottlenecks and increasing processing
    speed.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由Shoeby、Patwary、Puri等人于2019年在Megatron-LM论文中提出，**张量并行化**（**TP**）是另一种流行的技术，用于将LLM层的计算分布在多个设备上。与管道并行化不同，TP将单个层中找到的权重矩阵分割。这允许同时计算，显著减少内存瓶颈并提高处理速度。
- en: In TP, large matrices, such as the weight matrices in MLPs or the attention
    heads in self-attention layers, are partitioned across several GPUs. Each GPU
    holds a portion of these matrices and performs computations on its respective
    slice.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在TP中，大型矩阵，如MLP中的权重矩阵或自注意力层中的注意力头，被分配到多个GPU上。每个GPU持有这些矩阵的一部分，并在其相应的切片上进行计算。
- en: '![](img/B31105_08_07.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B31105_08_07.png)'
- en: Figure 8.7 – Illustration of column-wise tensor parallelism in an MLP layer
    (W)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – MLP层中列向张量并行化的示意图（W）
- en: For instance, in an MLP layer, the weight matrix is divided so that each GPU
    processes only a subset of the weights (see *Figure 8.7*). The inputs are broadcast
    to all GPUs, which then independently compute their respective outputs. The partial
    results are then aggregated through an all-reduce operation, combining them to
    form the final output.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个MLP层中，权重矩阵被分割，使得每个GPU只处理权重的一个子集（参见*图8.7*）。输入被广播到所有GPU，然后它们独立地计算各自的输出。然后通过all-reduce操作聚合部分结果，将它们组合成最终的输出。
- en: In the context of self-attention layers, TP is particularly efficient due to
    the inherent parallelism of attention heads. Each GPU can compute a subset of
    these heads independently, allowing the model to process large sequences more
    effectively. This makes TP more efficient than pipeline parallelism, which requires
    waiting for the completion of previous layers.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力层的上下文中，TP由于注意力头固有的并行性而特别高效。每个GPU可以独立计算这些头的一部分，这使得模型能够更有效地处理长序列。这使得TP比需要等待前一层完成的管道并行化更高效。
- en: Despite its advantages, TP is not universally applicable to all layers of a
    neural network. Layers like LayerNorm and Dropout, which have dependencies spanning
    the entire input, cannot be efficiently partitioned and are typically replicated
    across devices instead. However, these operations can be split on the sequence
    dimension of the input instead (sequence parallelism). Different GPUs can compute
    these layers on different slices of the input sequence, avoiding replication of
    weights. This technique is limited to a few specific layers, but it can provide
    additional memory savings, especially for very large input sequence lengths.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管TP具有优势，但它并不适用于神经网络的所有层。像LayerNorm和Dropout这样的层，其依赖关系跨越整个输入，无法有效地分区，通常会在设备之间复制。然而，这些操作可以在输入序列的序列维度上拆分（序列并行化）。不同的GPU可以在输入序列的不同切片上计算这些层，避免权重的复制。这项技术仅限于少数特定层，但它可以为非常大的输入序列长度提供额外的内存节省。
- en: Moreover, TP necessitates high-speed interconnects between devices to minimize
    communication overhead, making it impractical to implement across nodes with insufficient
    interconnect bandwidth.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，TP需要在设备之间实现高速互连以最小化通信开销，这使得在互连带宽不足的节点上实现变得不切实际。
- en: TP is also implemented in distributed training frameworks like Megatron-LM,
    DeepSpeed (ZeRO), and PyTorch (FSDP). It is available in most inference frameworks,
    like TGI, vLLM, and TensorRT-LLM.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: TP也被实现于分布式训练框架中，如Megatron-LM、DeepSpeed (ZeRO)和PyTorch (FSDP)。它在大多数推理框架中都是可用的，如TGI、vLLM和TensorRT-LLM。
- en: Combining approaches
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组合方法
- en: 'Data, tensor, and pipeline parallelisms are orthogonal techniques that can
    be combined. *Figure 8.8* illustrates how a given model can be split according
    to each approach:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据、张量和平行化是可组合的正交技术。*图8.8*展示了如何根据每种方法将给定的模型进行拆分：
- en: '![A screenshot of a computer  Description automatically generated](img/B31105_08_08.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  描述自动生成](img/B31105_08_08.png)'
- en: Figure 8.8 – Illustration of the different model parallelism techniques
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 不同模型并行化技术的示意图
- en: Combining these techniques can mitigate their respective issues. Pipeline parallelism
    provides the greatest memory reduction but sacrifices efficiency, due to pipeline
    bubbles. This may be ideal if the primary constraint fits the model in the GPU
    memory. In contrast, if low latency is paramount, then prioritizing tensor parallelism
    and accepting a larger memory footprint may be the better trade-off. In practice,
    a model may be split depth-wise into a few pipeline stages, with tensor parallelism
    used within each stage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些技术可以缓解它们各自的问题。管道并行化提供了最大的内存减少，但牺牲了效率，因为存在管道气泡。如果主要约束适合GPU内存中的模型，这可能是一个理想的选择。相反，如果低延迟至关重要，那么优先考虑张量并行化并接受更大的内存占用可能是一个更好的权衡。在实践中，模型可能被深度拆分为几个管道阶段，每个阶段使用张量并行化。
- en: Balancing these tradeoffs and mapping a given model architecture onto available
    hardware accelerators is a key challenge in deploying LLMs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡这些权衡并映射给定的模型架构到可用的硬件加速器是部署大型语言模型（LLMs）的一个关键挑战。
- en: Model quantization
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型量化
- en: Quantization refers to the process of representing the weights and activations
    of a neural network using lower-precision data types. In the context of LLMs,
    quantization primarily focuses on reducing the precision of the model’s weights
    and activations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是指使用低精度数据类型来表示神经网络权重和激活的过程。在LLMs的上下文中，量化主要关注降低模型权重和激活的精度。
- en: By default, weights are typically stored in a 16-bit or 32-bit floating-point
    format (FP16 or FP32), which provides high precision but comes at the cost of
    increased memory usage and computational complexity. Quantization is a solution
    to reduce the memory footprint and accelerate the inference of LLMs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，权重通常以16位或32位浮点格式（FP16或FP32）存储，这提供了高精度，但代价是增加了内存使用和计算复杂性。量化是一种减少内存占用并加速LLMs推理的解决方案。
- en: In addition to these benefits, larger models with over 30 billion parameters
    can outperform smaller models (7B–13B LLMs) in terms of quality when quantized
    to 2- or 3-bit precision. This means they can achieve superior performance while
    maintaining a comparable memory footprint.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些好处外，当量化到2位或3位精度时，超过300亿参数的大型模型在质量方面可以优于较小的模型（7B–13B LLMs）。这意味着它们可以在保持可比内存占用的情况下实现更优越的性能。
- en: In this section, we will introduce the concepts of quantization, GGUF with `llama.cpp`,
    GPTQ, and EXL2, along with an overview of additional techniques. In addition to
    the code provided in this section, you can refer to AutoQuant ([bit.ly/autoquant](https://bit.ly/autoquant))
    to quantize their models using a Google Colab notebook.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍量化的概念、GGUF与`llama.cpp`、GPTQ和EXL2，以及额外技术的概述。除了本节中提供的代码外，您还可以参考AutoQuant
    ([bit.ly/autoquant](https://bit.ly/autoquant))，使用Google Colab笔记本量化他们的模型。
- en: Introduction to quantization
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化简介
- en: 'There are two main approaches to weight quantization: **Post-Training Quantization**
    (**PTQ**) and **Quantization-Aware Training** (**QAT**). PTQ is a straightforward
    technique where the weights of a pre-trained model are directly converted to a
    lower precision format without any retraining. While PTQ is easy to implement,
    it may result in some performance degradation. Conversely, QAT performs quantization
    during the training or fine-tuning stage, allowing the model to adapt to the lower
    precision weights. QAT often yields better performance compared to PTQ but requires
    additional computational resources and representative training data.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 权重量化的主要有两种方法：**训练后量化**（**PTQ**）和**量化感知训练**（**QAT**）。PTQ是一种直接将预训练模型的权重转换为低精度格式而不进行任何重新训练的直接技术。虽然PTQ易于实现，但它可能会导致一些性能下降。相反，QAT在训练或微调阶段进行量化，允许模型适应低精度权重。与PTQ相比，QAT通常能提供更好的性能，但需要额外的计算资源和代表性的训练数据。
- en: The choice of data type plays a crucial role in quantization. Floating-point
    numbers, such as **FP32**, **FP16** (half-precision), and **BF16** (brain floating-point),
    are commonly used in deep learning. These formats allocate a fixed number of bits
    to represent the `sign`, `exponent`, and `significand` (mantissa) of a number.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型的选择在量化中起着至关重要的作用。在深度学习中，浮点数，如**FP32**、**FP16**（半精度）和**BF16**（脑浮点），被广泛使用。这些格式为表示数字的`符号`、`指数`和`尾数`（尾数）分配了固定数量的位。
- en: '![A screenshot of a computer program  Description automatically generated](img/B31105_08_09.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![计算机程序截图  自动生成的描述](img/B31105_08_09.png)'
- en: Figure 8.9 – Comparison the between FP32, FP16, and BF16 formats
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – FP32、FP16和BF16格式的比较
- en: 'A sign of 0 represents a positive number, while 1 indicates a negative number.
    Conversely, the exponent controls the range that is represented (big or small).
    Finally, the significand controls the precision of the number (the number of digits).
    The formula used to convert these representations into real numbers is:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 0的符号表示正数，而1表示负数。相反，指数控制表示的范围（大或小）。最后，尾数控制数字的精度（数字的位数）。将这些表示转换为实数的公式是：
- en: '![](img/B31105_08_007.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B31105_08_007.png)'
- en: The data types shown in *Figure 7.7* display different tradeoffs, as illustrated
    with different representations of ![](img/B31105_08_008.png) (![](img/B31105_08_009.png)).
    FP32 uses 32 bits, providing high precision but also requiring more memory. Conversely,
    FP16 and BF16 use 16 bits, lowering the memory footprint at the cost of a lower
    precision. In general, neural networks prefer a bigger range than better precision,
    which is why BF16 is the most popular data type when the hardware supports it.
    For example, NVIDIA’s Ampere architecture (A100, A30, etc.) supports BF16, but
    previous generations like Turing (T4, T40, etc.) do not.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we are not restricted to these three data types. Lower-precision data
    types, such as INT8 (8-bit integers), can be employed for quantization, further
    reducing the memory footprint. Naïve quantization techniques, such as *absolute
    maximum (absmax) quantization* and *zero-point quantization*, can be applied to
    convert `FP32`, `FP16`, or `BF16` weights to `INT8`, as illustrated in *Figure
    8.10*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a video game  Description automatically generated](img/B31105_08_10.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Quantization of 0.1 in a [-3.0, 3.2] range with absmax quantization
    and zero-point quantization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Absmax quantization maps the original weights ![](img/B31105_08_010.png) to
    the range [-127, 127] by dividing them by the absolute maximum value of ![](img/B31105_08_010.png)
    and scaling them:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31105_08_012.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'For example, if our absolute maximum value is 3.2 (see *Figure 8.8*), a weight
    of 0.1 would be quantized to ![](img/B31105_08_013.png). To dequantize it, we
    do the inverse operation:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31105_08_014.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'This means that if we dequantize our weight, we obtain ![](img/B31105_08_015.png).
    We can see a rounding error of ![](img/B31105_08_016.png) in this example. In
    Python, we can implement it as follows with the PyTorch library:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Zero-point quantization, on the other hand, considers asymmetric input distributions
    and maps the weights ![](img/B31105_08_0161.png)to the range [-128, 127] by introducing
    a zero-point offset:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31105_08_018.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B31105_08_019.png) and ![](img/B31105_08_020.png).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: If we take the same example with a weight of 0.1, we get a scale of ![](img/B31105_08_021.png)
    and a zero-point value of ![](img/B31105_08_022.png). The weight of 0.1 would
    be quantized to ![](img/B31105_08_023.png), unlike the value of ![](img/B31105_08_024.png)
    provided by absmax.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily get the dequantization by applying the inverse operation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B31105_08_025.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'In Python, zero-point quantization can be implemented as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: However, naïve quantization methods have limitations, particularly when dealing
    with *outlier features* in LLMs. Outlier features are extreme weight values (about
    0.1% of total values) that can significantly impact the quantization process,
    leading to reduced precision for other values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'Discarding these outliers is not feasible, as it would degrade a model’s performance.
    You can see an example of outliers in *Figure 8.11*:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 丢弃这些异常值是不可行的，因为这会降低模型的表现。您可以在*图8.11*中看到异常值的示例：
- en: '![A screenshot of a computer  Description automatically generated](img/B31105_08_11.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  自动生成的描述](img/B31105_08_11.png)'
- en: Figure 8.11 – Example of outliers in a weight matrix
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 – 权重矩阵中异常值的示例
- en: To address the outlier problem, more advanced quantization techniques have been
    proposed. One notable example is `LLM.int8()`, introduced by Dettmers et al. (2022).
    `LLM.int8()` employs a mixed-precision quantization scheme, where outlier features
    are processed using FP16, while the remaining values are quantized to INT8\. This
    approach effectively reduces the memory footprint of LLMs by nearly 2x while minimizing
    performance degradation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决异常值问题，已经提出了更先进的量化技术。一个值得注意的例子是Dettmers等人（2022）引入的`LLM.int8()`。`LLM.int8()`采用混合精度量化方案，其中异常特征使用FP16处理，而其余值量化为INT8。这种方法有效地将LLM的内存占用减少了近2倍，同时最大限度地减少了性能下降。
- en: '`LLM.int8()` works by performing matrix multiplication in three steps. First,
    it extracts columns containing outlier features from the input hidden states using
    a custom threshold. Second, it performs separate matrix multiplications for the
    outliers (in `FP16`) and non-outliers (in `INT8`) using vector-wise quantization.
    Finally, it dequantizes the non-outlier results and combines them with the outlier
    results to obtain the final output in *FP16*.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`LLM.int8()`通过三个步骤执行矩阵乘法。首先，它使用自定义阈值从输入隐藏状态中提取包含异常特征列。其次，它对异常值（在`FP16`中）和非异常值（在`INT8`中）执行单独的矩阵乘法，使用向量量化。最后，它对非异常结果进行反量化，并将它们与异常结果合并，以获得最终的*FP16*输出。'
- en: 'The effectiveness of `LLM.int8()` has been demonstrated empirically, showing
    negligible performance degradation (<1%) compared to the original `FP32` models.
    However, it does introduce an additional computational overhead, resulting in
    around 20% slower inference for large models. Models can be directly loaded in
    8-bit precision with the transformer library, using `LLM.int8()`, as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`LLM.int8()`的有效性已经通过实证研究得到证明，与原始的`FP32`模型相比，性能下降可忽略不计（<1%）。然而，它确实引入了额外的计算开销，导致大型模型的推理速度大约慢20%。可以使用transformer库直接以8位精度加载模型，使用`LLM.int8()`，如下所示：'
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Introduced by *Dettmers et al*. (2023), NF4 is a 4-bit precision format designed
    for QLoRA (discussed in *Chapter 5*). It is also integrated into the transformers
    library but requires the bitsandbytes library as a dependency. To load a model
    in NF4 (4-bit precision), you can use the `load_in_4bit` parameter, as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由*Dettmers等人*（2023）引入的NF4是一种4位精度格式，专为QLoRA（在第5章中讨论）设计。它还集成到transformers库中，但需要bitsandbytes库作为依赖项。要加载NF4（4位精度）模型，可以使用`load_in_4bit`参数，如下所示：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Quantization with GGUF and llama.cpp
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GGUF和llama.cpp进行量化
- en: The llama.cpp project is an open-source C++ software library created by Georgi
    Gerganov, designed to perform inference with various LLMs. It is the most popular
    quantization technique, with many quantized models available on the Hugging Face
    Hub.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: llama.cpp项目是由Georgi Gerganov创建的开源C++软件库，旨在使用各种LLM进行推理。它是最受欢迎的量化技术，Hugging Face
    Hub上有许多量化模型可用。
- en: Compared to other libraries that rely on hardware-specific closed-source libraries
    like CUDA, llama.cpp can run on a broader range of hardware. It has gained significant
    popularity, particularly among users without specialized hardware, as it can operate
    on CPUs and Android devices. Moreover, llama.cpp can also offload layers to the
    GPU, accelerating inference speed. It is compatible with different inference optimization
    techniques, such as FlashAttention-2 and speculative decoding.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于特定硬件的封闭源库（如CUDA）的其他库相比，llama.cpp可以在更广泛的硬件上运行。它在没有专用硬件的用户中获得了显著的流行，因为它可以在CPU和Android设备上运行。此外，llama.cpp还可以将层卸载到GPU，加速推理速度。它与不同的推理优化技术兼容，如FlashAttention-2和推测性解码。
- en: 'This project features its own quantization format, GGUF, designed to simplify
    and speed up model loading. GGUF files store tensors and metadata, supporting
    various formats, from 1-bit to 8-bit precision. It follows a naming convention
    based on the number of bits used and specific variants, such as:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目具有自己的量化格式，GGUF，旨在简化并加快模型加载。GGUF文件存储张量和元数据，支持从1位到8位精度的各种格式。它遵循基于使用的位数和特定变体的命名约定，例如：
- en: '`IQ1_S` and `IQ1_M`: 1-bit precision – very low quality'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IQ2_XXS/XS/S/M` and `Q2_K`: 2-bit precision – generally low quality but IQ2
    can be usable for large models'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IQ3_XXS/XS/S/M` and `Q3_K_S/M/L`: 3-bit precision – low quality but usable
    for large models'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`IQ4_XS/NL` and `Q4_K_S/M, Q4_0/1`: 4-bit precision – good quality and usable
    for most models'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q5_K_S/M` and `Q5_0/1`: 5-bit precision – high quality'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q6_K`: 6-bit precision –very high quality'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q8_0`: 8-bit precision – highest quality'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To provide a brief overview of GGUF quantization, llama.cpp groups values into
    blocks and rounds them to a lower precision. For instance, the legacy Q4_0 format
    handles 32 values per block, scaling and quantizing them based on the largest
    weight value in the block (![](img/B31105_08_026.png)). In Q4_1, the smallest
    Lvalue in the block is also added (![](img/B31105_08_027.png) ). In Q4_K, weights
    are divided into super-blocks, containing 8 blocks with 32 values. Block scales
    and minimum values are also quantized in higher precision with 6 bits (![](img/B31105_08_028.png)).
    Finally, i-quants like IQ4_XS are inspired by another quantization technique called
    QuIP#. This ensures an even number of positive (or negative) quant signs in groups
    of eight and implements the ![](img/B31105_08_029.png) lattice to store their
    magnitude.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a practical example of how to quantize a model in the GGUF format.
    The following steps can be executed on a free T4 GPU in Google Colab:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Install llama.cpp and the required libraries:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Download the model to convert. We will provide the model ID from the Hugging
    Face Hub – for example, `mistralai/Mistral-7B-Instruct-v0.2`:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'First, we convert the model into FP16\. This is an intermediary artifact that
    will be used for every GGUF quantization type. Note that different conversion
    scripts exist in llama.cpp and are compatible with different models:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We select a format (here, `Q4_K_M`) and start the quantization. This process
    can take an hour on a T4 GPU:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once it’s done, your quantized model is ready. You can download it locally,
    or upload it to the Hugging Face Hub using the following code:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: GGUF models can be used with backends such as llama-cpp-python and frameworks
    like LangChain. This is useful if you want to integrate a quantized model into
    a broader system. You can also directly chat with the model using frontends, like
    llama.cpp’s lightweight server, LM Studio, and the Text Generation Web UI. These
    tools enable easy interaction with the GGUF models, providing an experience similar
    to ChatGPT.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Quantization with GPTQ and EXL2
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While GGUF and llama.cpp offer CPU inference with GPU offloading, GPTQ and EXL2
    are two quantization formats dedicated to GPUs. This makes them both faster than
    llama.cpp during inference. In particular, EXL2 offers the highest throughput
    with its dedicated library, ExLlamaV2.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: GPTQ and EXL2 quants are based on the GPTQ algorithm, introduced by Frantar
    et al. (2023). It optimizes weight quantization for LLMs by refining **the Optimal
    Brain Quantization** (**OBQ**) approach to handle extensive matrices efficiently.
    It begins with a Cholesky decomposition of the Hessian inverse, ensuring numerical
    stability. Instead of quantizing weights in a strict order, GPTQ processes them
    in batches, updating columns and associated blocks iteratively. This method leverages
    lazy batch updates, reducing computational redundancy and memory bottlenecks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ和EXL2量化器基于Frantar等人（2023年）提出的GPTQ算法。它通过细化**最优脑量化**（**OBQ**）方法来优化LLM的权重量化，以高效处理大量矩阵。它从Hessian逆矩阵的Cholesky分解开始，确保数值稳定性。GPTQ不是严格按照顺序量化权重，而是将它们分批处理，迭代更新列和相关块。这种方法利用了懒惰批更新，减少了计算冗余和内存瓶颈。
- en: While GPTQ is limited to 4-bit precision, EXL2 offers more flexibility with
    a highly customizable precision that can mix different quantization levels. This
    allows for precise bitrates between 2 and 8 bits per weight, such as `2.3`, `3.5`,
    or `6.0`. It can also apply multiple quantization levels to each linear layer,
    prioritizing more important weights with higher bit quantization. Parameters are
    selected automatically, by quantizing each matrix multiple times and choosing
    a combination that minimizes the quantization error while meeting a target bitrate.
    In practice, this allows 70B models to run on a single 24 GB GPU with 2.55-bit
    precision.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GPTQ 限制在 4 位精度，但EXL2提供了高度可定制的精度，可以混合不同的量化级别。这允许每个权重之间精确的比特率在2到8位之间，例如`2.3`、`3.5`或`6.0`。它还可以将多个量化级别应用于每个线性层，优先考虑更高位量化的重要权重。参数通过多次量化每个矩阵并选择一个组合来选择，该组合在满足目标比特率的同时最小化量化错误。在实践中，这允许70B模型在单个24GB
    GPU上以2.55位精度运行。
- en: The inference itself is handled by the ExLlamaV2 library, which supports both
    the GPTQ and EXL2 models.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 推理本身由支持 GPTQ 和 EXL2 模型的 ExLlamaV2 库处理。
- en: 'In the following example, let’s quantize a model in the EXL2 format using ExLlamaV2\.
    These steps can be executed on a free T4 GPU in Google Colab:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，让我们使用 ExLlamaV2 以 EXL2 格式量化一个模型。这些步骤可以在 Google Colab 中的免费 T4 GPU 上执行：
- en: 'Install the ExLlamaV2 library from source:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从源代码安装ExLlamaV2库：
- en: '[PRE20]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We download the model to quantize by cloning its repo from the Hugging Face
    Hub:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从Hugging Face Hub克隆其repo来下载用于量化的模型：
- en: '[PRE21]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Download the calibration dataset used to measure the quantization error. In
    this case, we will use WikiText-103, a standard calibration dataset with high-quality
    articles from Wikipedia:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载用于测量量化错误的校准数据集。在这种情况下，我们将使用WikiText-103，这是一个包含来自维基百科高质量文章的标准校准数据集：
- en: '[PRE22]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Quantize the model at a given precision (for example, 4.5):'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在给定的精度下量化模型（例如，4.5）：
- en: '[PRE23]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The quantized model can then be uploaded to the Hugging Face Hub, as seen previously.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 量化模型然后可以上传到 Hugging Face Hub，如前所述。
- en: GPTQ and EXL2 quants are not as widely supported as GGUF. For example, frontends
    like LM Studio do not currently integrate them. You can use other tools instead,
    like oobabooga’s Text Generation Web UI. It is also directly integrated into the
    transformers library and supported by TGI. GPTQ models are also supported in TensorRT-LLM.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ和EXL2量化器不像GGUF那样得到广泛支持。例如，LM Studio等前端目前尚未集成它们。您可以使用其他工具，如oobabooga的文本生成Web
    UI。它也直接集成到transformers库中，并得到TGI的支持。GPTQ模型也支持在TensorRT-LLM中。
- en: While less popular than GGUF, you can find a lot of GPTQ and EXL2 models on
    the Hugging Face Hub.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 GGUF 更受欢迎，但您可以在 Hugging Face Hub 上找到许多 GPTQ 和 EXL2 模型。
- en: Other quantization techniques
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他量化技术
- en: There is a variety of quantization techniques beyond GGUF, GPTQ, and EXL2\.
    This subsection will briefly introduce **Activate-aware Weight Quantization**
    (**AWQ**) as well as extreme quantization techniques, like QuIP# (Quantization
    with Incoherence Processing) and HQQ (Half-Quadratic Quantization).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GGUF、GPTQ和EXL2之外，还有许多其他量化技术。本小节将简要介绍**激活感知权重量化**（**AWQ**）以及极端量化技术，如QuIP#（具有失真处理的量化）和HQQ（半二次量化）。
- en: Introduced by Lin et al. (2023), AWQ is another popular quantization algorithm.
    It identifies and protects the most important weights, which are determined based
    on activation magnitude instead of weight magnitude. This approach involves applying
    optimal per-channel scaling to these salient weights, without relying on backpropagation
    or reconstruction, ensuring that the LLM does not overfit the calibration set.
    While it relies on a different paradigm, AWQ is quite close to the GPTQ and EXL2
    versions, although slightly slower. They are well-supported by inference engines
    and integrated into TGI, vLLM, and TensorRT-LLM.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由Lin等人（2023年）引入的AWQ是另一种流行的量化算法。它识别并保护最重要的权重，这些权重是基于激活幅度而不是权重幅度确定的。这种方法涉及对这些显著的权重应用最优的每通道缩放，而不依赖于反向传播或重建，确保LLM不会过度拟合校准集。尽管它依赖于不同的范式，但AWQ与GPTQ和EXL2版本相当接近，尽管速度略慢。它们得到了推理引擎的良好支持，并集成到了TGI、vLLM和TensorRT-LLM中。
- en: An interesting trend is the quantization of models into 1- or 2-bit precision.
    While some formats, like EXL2, allow extreme quantization, the quality of the
    models often suffers significantly. However, recent algorithms like QuIP# and
    HQQ have targeted this regime and offer quantization methods that better preserve
    the performance of the original models. This is particularly true for large models
    (over 30B parameters), which can end up taking less space than 7B or 13B parameter
    models while providing higher-quality outputs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的趋势是将模型量化为1位或2位精度。虽然一些格式，如EXL2，允许极端量化，但模型的品质通常会显著下降。然而，最近的算法如QuIP#和HQQ针对这一领域，并提供了更好地保留原始模型性能的量化方法。这对于大型模型（超过30B参数）尤其如此，这些模型最终可能比7B或13B参数模型占用更少的空间，同时提供更高品质的输出。
- en: This trend is expected to continue, further optimizing these quantization methods.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 预计这一趋势将持续下去，进一步优化这些量化方法。
- en: 'To conclude this chapter, here is a table summarizing the features of the three
    main inference engines we covered in the previous sections:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本章内容，以下是一个表格，总结了我们在前几节中介绍的三种主要推理引擎的特征：
- en: '| **Technique** | **TGI** | **vLLM** | **TensorRT-LLM** |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **技术** | **TGI** | **vLLM** | **TensorRT-LLM** |'
- en: '| Continuous batching | ✓ | ✓ | ✓ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 连续批处理 | ✓ | ✓ | ✓ |'
- en: '| Speculative decoding | ✓ |  |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 推测解码 | ✓ |  |  |'
- en: '| FlashAttention2 | ✓ | ✓ | ✓ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| FlashAttention2 | ✓ | ✓ | ✓ |'
- en: '| PagedAttention | ✓ | ✓ | ✓ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| PagedAttention | ✓ | ✓ | ✓ |'
- en: '| Pipeline parallelism |  |  | ✓ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 管道并行性 |  |  | ✓ |'
- en: '| Tensor parallelism | ✓ | ✓ | ✓ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 张量并行性 | ✓ | ✓ | ✓ |'
- en: '| GPTQ | ✓ |  | ✓ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | ✓ |  | ✓ |'
- en: '| EXL2 | ✓ |  |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| EXL2 | ✓ |  |  |'
- en: '| AWQ | ✓ | ✓ | ✓ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | ✓ | ✓ | ✓ |'
- en: Table 8.1 – Summary of features for TGI, vLLM, and TensorRT-LLM
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 – TGI、vLLM和TensorRT-LLM的特征总结
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In summary, inference optimization is a critical aspect of deploying LLMs effectively.
    This chapter explored various optimization techniques, including optimized generation
    methods, model parallelism, and weight quantization. Significant speedups can
    be achieved by leveraging techniques like predicting multiple tokens in parallel
    with speculative decoding, or using an optimized attention mechanism with FlashAttention-2\.
    Additionally, we discussed how model parallelism methods, including data, pipeline,
    and tensor parallelism, distribute the computational load across multiple GPUs
    to increase throughput and reduce latency. Weight quantization, with formats like
    GGUF and EXL2, further reduces the memory footprint and accelerates inference,
    with some calculated tradeoff in output quality.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，推理优化是有效部署大型语言模型（LLMs）的关键方面。本章探讨了各种优化技术，包括优化的生成方法、模型并行性和权重量化。通过利用预测多个标记与推测解码并行进行等技术，可以实现显著的加速。此外，我们还讨论了模型并行性方法，包括数据、管道和张量并行性，这些方法如何将计算负载分配到多个GPU上以提高吞吐量和降低延迟。使用GGUF和EXL2等格式的权重量化进一步减少了内存占用并加速了推理，尽管在输出质量上有所权衡。
- en: Understanding and applying these optimization strategies are essential for achieving
    high performance in practical applications of LLMs, such as chatbots and code
    completion. The choice of techniques and tools depends on specific requirements,
    including available hardware, desired latency, and throughput. By combining various
    approaches, such as continuous batching and speculative decoding, along with advanced
    attention mechanisms and model parallelism, users can tailor their deployment
    strategies to maximize efficiency.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Way back in *Chapter 4*, we focused only on implementing the ingestion pipeline,
    which is just one component of a standard RAG application. In the next chapter,
    we will conclude the RAG system by implementing the retrieval and generation components
    and integrating them into the inference pipeline.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hugging Face, Text Generation Inference, [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference),
    2022.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W. Kwon*, *Z. Li*, *S. Zhuang*, *Y. Sheng*, *L. Zheng*, *C.H. Yu*, *J.E. Gonzalez*,
    *H. Zhang*, *I. Stoica*, *Efficient Memory Management for Large Language Model
    Serving with PagedAttention*, *2023*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nvidia, *TensorRT-LLM*, [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2023.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y. Leviathan, M. Kalman, Y. Matias, Fast Inference from Transformers via Speculative
    Decoding, 2023*.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T. Cai, Y. Li, Z. Geng, H. Peng, J.D. Lee, D. Chen, T. Dao, Medusa: Simple
    LLM Inference Acceleration Framework with Multiple Decoding Heads, 2024*.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C.H. Yu, J.E. Gonzalez, H.
    Zhang, I. Stoica, Efficient Memory Management for Large Language Model Serving
    with PagedAttention, 2023*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R.Y. Aminabadi, S. Rajbhandari, M. Zhang, A.A. Awan, C. Li, D. Li, E. Zheng,
    J. Rasley, S. Smith, O. Ruwase, Y. He, DeepSpeed Inference: Enabling Efficient
    Inference of Transformer Models at Unprecedented Scale, 2022*.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Y. Huang, Y. Cheng, A. Bapna, O. Firat, M.X. Chen, D. Chen, H. Lee, J. Ngiam,
    Q.V. Le, Y. Wu, Z. Chen, GPipe: Efficient Training of Giant Neural Networks using
    Pipeline Parallelism, 2019*.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*K. James Reed, PiPPy: Pipeline Parallelism for PyTorch*, [https://github.com/pytorch/PiPPy](https://github.com/pytorch/PiPPy),
    *2022*.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, B. Catanzaro, Megatron-LM:
    Training Multi-Billion Parameter Language Models Using Model Parallelism, 2020.*'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Verma and Vaidya, Mastering LLM Techniques: Inference Optimization, NVIDIA
    Developer Technical Blog*, [https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/),
    *2023*.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*T. Dettmers, M. Lewis, Y. Belkada, L. Zettlemoyer, LLM.int8(): 8-bit Matrix
    Multiplication for Transformers at Scale, 2022.*'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G. Gerganov, llama.cpp*, [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp),
    *2023*.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training
    Quantization for Generative Pre-trained Transformers, 2023*.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*E. Frantar, S. Ashkboos, T. Hoefler, D. Alistarh, GPTQ: Accurate Post-Training
    Quantization for Generative Pre-trained Transformers, 2023*.'
- en: '*Tuboderp, exllamav2*, [https://github.com/turboderp/exllamav2](https://github.com/turboderp/exllamav2),
    *2023*.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tuboderp, exllamav2*, [https://github.com/turboderp/exllamav2](https://github.com/turboderp/exllamav2),
    *2023*.'
- en: '*J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang,
    C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration, 2024*.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang,
    C. Gan, S. Han, AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration, 2024*.'
- en: Join our book’s Discord space
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/llmeng](https://packt.link/llmeng)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/llmeng](https://packt.link/llmeng)'
- en: '![](img/QR_Code79969828252392890.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code79969828252392890.png)'
