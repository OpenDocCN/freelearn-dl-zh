# 8

# 解决伦理考量并绘制通往可信赖生成式AI的路径

随着生成式AI的进步，它将超越基本语言任务，融入日常生活，并影响几乎每个行业。其广泛采用的必然性突出了解决其伦理影响的必要性。这项技术有望变革行业、增强创造力和解决复杂问题，必须与认真导航其伦理景观的责任相结合。本章将探讨这些伦理考量，剖析这些模型中纠缠在一起的偏见复杂性，并探讨培养通用人工智能系统信任的策略。通过彻底的审查和反思，我们可以开始勾勒出一条负责任使用的路径，帮助确保生成式AI的进步被用于更大的善，同时最大限度地减少伤害。

为了使我们的讨论有据可依，我们首先将确定一些与生成式AI相关的伦理规范和普遍价值观。虽然本章不能详尽无遗，但它旨在介绍关键的伦理考量。

# 生成式AI背景下的伦理规范和价值

指导生成式AI开发和部署的伦理规范和价值根植于透明度、公平性、问责制、隐私、同意、安全和包容性。这些原则可以作为开发并采用符合社会价值观并支持更大善的系统的基础。让我们详细探讨这些原则：

+   **透明度**涉及清楚地解释大型语言模型（LLM）构建背后的方法、数据来源和过程。这种做法通过使利益相关者能够理解技术的可靠性和局限性来建立信任。例如，一家公司可以发布一份详细的报告，说明其LLM训练的数据类型以及为确保数据隐私和偏见缓解所采取的步骤。

+   在LLM（大型语言模型）的背景下，**公平性**通过积极预防模型中的偏见，确保所有用户都得到公平对待和结果。这需要彻底分析并纠正训练数据，并持续监控交易以减少歧视。一家公司可能采取的措施是对LLM在各种人口群体中的表现进行常规审查，以识别和解决未预见的偏见。

+   **问责制**确立LLM的开发者和用户对模型输出和影响负责。它包括透明和易于访问的报告和解决负面后果或伦理违规的机制。在实践中，这可能表现为建立一个独立的审查委员会，监督AI项目并在伦理不当行为的情况下进行干预。

+   **隐私和同意**原则上涉及确保在将个人数据作为LLM输入使用时尊重和保护个人隐私和同意。在实践中，开发者应避免在没有明确许可的情况下使用个人数据进行训练，并实施强大的数据保护措施。例如，开发者可能使用数据匿名化或隐私保护技术来训练模型，确保在数据处理之前移除个人标识符和敏感信息。

+   **安全**涉及保护集成LLM的系统及其数据免受未经授权的访问和网络威胁。在实践中，建立LLM特定的红队（或通过模拟攻击测试防御的团队）可以帮助保护AI系统免受潜在的安全漏洞。

+   **包容性**涉及在LLM的开发过程中有意识地包括不同的声音和观点，确保技术对广泛的用户可访问和有益。在实践中，与能够指导适当行动以促进和保护包容性的社会-技术领域的专家合作至关重要。

这套原则并非全面，但可能有助于形成道德LLM开发和采用的 conceptual foundation，其普遍目标是避免伤害地推进技术。

此外，各种领先的权威机构已经发布了关于负责任AI的指南，包括伦理影响。这些包括美国商务部**国家标准与技术研究院**（**NIST**）、斯坦福大学**以人为本的人工智能研究所**（**HAI**）和**分布式人工智能研究院**（**DAIR**），等等。

# 调查和最小化生成式LLM和生成式图像模型中的偏差

生成式AI模型中的偏差，包括LLM和生成式图像模型，是一个复杂的问题，需要仔细的调查和缓解策略。偏差可能表现为在生成输出中的无意刻板印象、不准确性和排除，通常源于有偏差的数据集和模型架构。识别和解决这些偏差对于创建公平和值得信赖的AI系统至关重要。

在其核心，算法或模型偏差指的是导致某些群体受到优先对待或不公平结果的系统性错误。在生成式AI中，这可能会表现为输出中的性别、种族或社会经济偏差，通常反映了社会刻板印象。例如，一个大型语言模型（LLM）可能会生成强化这些偏差的内容，反映出其训练数据中存在的历 史和社会偏差。

让我们再次回顾一下我们的假设时尚零售商，StyleSprint。考虑一种情况，StyleSprint尝试使用多模态生成式LLM模型为其最新的运动鞋系列生成促销图像和标题。它发现该模型主要生成背景为城市、涂鸦遍布的鞋子，无意中引发了一种基于刻板印象的联想。此外，团队开始注意到标题中也充满了持续刻板印象的语言。这一认识促使他们对图像和文本进行重新评估，首先是对问题如何出现进行调查研究。

调查偏见涉及各种技术，从分析训练数据集的多样性和代表性到实施针对不同群体和场景的特定测试协议以寻找偏见输出。统计分析可以揭示模型结果的差异，而比较研究和用户反馈可以帮助识别生成内容中的偏见。

在这个案例中，假设StyleSprint使用的是一个无法影响其训练数据或开发过程的LLM提供商。为了减轻偏见风险，团队可能采取以下措施：

+   后处理调整以多样化图像，确保更广泛地反映与其客户群产生共鸣的背景

+   建立一个手动审查流程，让团队成员仔细审查和精选AI生成的图像和标题，在发布之前确保每件内容都符合品牌对多元化和包容性的承诺（即“人机协同”）

对于生成式AI的其他类型评估来说，评估偏见需要定量和定性方法。统计分析可以揭示不同群体之间的性能差异，比较研究可以检测输出中的偏见。收集来自不同用户的反馈有助于理解现实世界中的偏见影响，而独立的审计和研究对于识别内部评估可能遗漏的问题至关重要。

在更好地理解我们如何调查和评估模型的社会偏见结果之后，我们可以探索技术方法来引导模型结果向可靠性、公平性和普遍可信度发展，以减少推理过程中的偏见或不公平结果。

# 限制生成和诱导可信结果

在实践中，可以通过限制模型生成并引导结果向事实性和公平性方向发展。正如所讨论的，通过持续训练和微调，或在进行推理时，可以引导模型向可信赖的结果发展。例如，**从人类反馈中进行强化学习（RLHF**）和**直接偏好优化（DPO**）等方法越来越多地细化模型输出，以使模型结果与人类判断一致。此外，正如在第7章[*](B21773_07.xhtml#_idTextAnchor225)中讨论的，各种定位技术有助于确保模型输出反映经过验证的数据，持续引导模型向负责任和准确的内容生成方向发展。

## 通过微调进行受限生成

如同在第7章[*](B21773_07.xhtml#_idTextAnchor225)中讨论的，将人类判断整合到模型训练过程中的细化策略，如RLHF，将AI引导至符合伦理和真实标准的行动。通过引入人类反馈循环，RLHF确保AI的输出达到技术准确性和社会规范。

类似地，数据保护官（DPO）根据明确的人类偏好来细化模型输出，提供精确的控制以确保结果符合道德标准和人类价值观。这种技术体现了内容生成向更符合伦理标准转变的趋势，因为它直接将人类价值观纳入优化过程。

## 通过提示工程进行受限生成

正如我们在[*第7章*](B21773_07.xhtml#_idTextAnchor225)中发现的那样，我们可以通过使用事实信息来定位LLM来引导模型响应。这可以通过直接使用上下文窗口或检索方法（例如，检索增强生成（RAG））来实现。正如我们可以应用这些方法来诱导事实性响应一样，我们也可以应用相同的技巧来引导模型向公平和包容的结果发展。

例如，考虑一家在线新闻机构希望使用大型语言模型（LLM）来审查文章内容的语法和可读性。该模型在审查和修订草稿方面表现出色。然而，在同行评审过程中，它意识到其中一些语言可能存在文化不敏感或缺乏包容性的问题。正如所讨论的，定性评估和人工监督对于确保模型输出与人类判断一致至关重要。尽管如此，写作团队可以使用一套关于包容性和无偏见语言的通用指南来引导模型与公司价值观保持一致。例如，可以通过其内部政策文件的摘录或其无意识偏见培训指南的内容来对模型进行定位。

采用RLHF和DPO等方法论，以及基于事实的技术，确保LLM生成的内容不仅符合事实，而且符合伦理标准，展示了生成式AI遵守高标准的真实性和包容性的潜力。尽管我们不能低估或淡化人类判断在塑造模型输出中的重要性，但我们可以通过应用如基于事实的补充方法等实用方法来降低有害或偏见模型输出的可能性。

在下一节中，我们将探讨试图规避我们刚才讨论的约束所提出的风险和伦理困境，强调在快速采用生成式LLM的同时，平衡适当的滥用防范措施的持续挑战。

# 理解越狱和有害行为

在生成式LLM的背景下，**越狱**一词描述了旨在操纵模型以覆盖任何伦理保障或内容限制的技术和策略，从而允许生成受限制或有害的内容。越狱通过复杂的对抗性提示来利用模型，可以诱导出意外或有害的响应。例如，攻击者可能会试图指示LLM解释如何生成显式内容或表达歧视性观点。理解这种易受攻击性对于开发者和利益相关者来说至关重要，以确保应用生成式AI免受滥用并最大限度地减少潜在的危害。

这些越狱攻击利用了LLM被训练来解释和响应指令的事实。尽管已经做出了复杂的努力来防御滥用，攻击者仍然可以利用LLM中嵌入的复杂和广泛的知识来发现其安全预防措施中的漏洞。特别是，在未经审查的数据集上训练的模型最易受影响，因为模型从中采样的可能输出范围可能包括有害和有毒内容。此外，LLM是多语言的，可以接受各种编码作为输入。例如，**base64**这样的编码可以将纯文本转换为二进制格式，可以用来混淆有害指令。在这种情况下，安全过滤器可能表现不一致，无法检测某些语言或替代输入。

尽管LLM存在这种固有的弱点，但开发者和从业者可以采取一些实际步骤来减轻越狱风险。记住，这些不能详尽无遗，因为新的对抗性技术通常会被发现：

+   **预处理和安全过滤**：实施强大的内容过滤，以检测和阻止跨语言和输入类型的危险语义模式。例如，一家公司可能会应用机器学习技术来分析提示中的对抗性模式，并在将它们传递给LLM之前阻止可疑的输入。

+   **后处理和输出筛选**：在返回之前，应用一个专门的分类器或其他复杂技术来筛选LLM输出中的不适当内容。

+   **以安全为重点的微调**：为LLM提供额外的以安全为重点的微调，以加强和扩展其安全知识。重点关注已知的越狱策略。

+   **监控和迭代**：在生产环境中积极监控越狱或政策违规尝试，分析它们以识别差距，并不断更新防御措施以保持领先于有创造性的攻击者。

虽然消除所有可能的越狱尝试是不切实际的，但多层防御和操作最佳实践可以显著降低风险。

在下一节中，我们将应用实时越狱防御机制，同时降低产生偏见和有害输出的可能性。

# 实践项目：使用过滤来最小化有害行为

对于这个项目，我们将使用响应过滤来尝试最小化误用并遏制不想要的LLM输出。再次强调，我们将考虑我们的假设业务，StyleSprint。在成功使用LLM生成产品描述并微调它以回答常见问题之后，StyleSprint现在想要尝试使用一个通用的LLM（不进行微调）来优化其网站搜索。然而，直接让客户访问LLM存在误用的风险。恶意行为者可能会尝试使用LLM搜索来产生有害内容，意图损害StyleSprint的声誉。为了防止这种行为，我们可以回顾我们在[*第7章*](B21773_07.xhtml#_idTextAnchor225)中提到的RAG实现，应用一个过滤器来评估查询是否偏离了适当的使用。

重新使用上一章中（可在GitHub仓库[https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python](https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python)找到）的先前实现，该实现将RAG应用于回答特定产品相关的问题，我们可以评估模型对超出预期范围的问题的响应。回想一下，RAG只是一个向量搜索引擎与LLM结合，以产生由特定数据源上下文化的连贯且更精确的响应。我们将直接重用该实现和相同的产品数据以简化流程，但这次，我们将输入一个完全不相关的查询而不是询问产品：

[PRE0]

如我们所见，该模型没有尝试将其答案限制在搜索索引的内容之内。它基于其庞大的训练数据返回了一个答案。这正是我们想要避免的行为。想象一下，一个恶意行为者诱导模型生成显式内容或某些其他不希望产生的输出。此外，考虑一个复杂的攻击者，可能诱导模型泄露训练数据或在训练过程中意外记住的敏感信息（Carlini等，2018；胡等，2022）。在任何情况下，StyleSprint都可能面临重大的风险和暴露。

为了防止这种情况，我们可以利用过滤器来限制输出，明确提供与给定问题相关的答案。该实现已经内置到LlamaIndex RAG接口中。他们称之为结构化答案过滤：

当structured_answer_filtering设置为True时，我们的refine模块能够过滤掉任何与所提问题不相关的输入节点。这对于涉及从外部向量存储检索给定用户查询的文本块的基础RAG问答系统特别有用。（LlamaIndex）

简而言之，这项功能让我们能够精细控制提供给LLM用于综合的上下文，确保只包含最相关的结果。在综合响应之前过滤掉不相关的内容，可以确保只使用与用户问题相关的信息。这种方法有助于避免离题或超出预期主题范围的答案。我们可以快速重新实现我们的RAG方法，通过进行一些小的改动来实现这一功能。

注意

当使用能够支持函数调用的LLM时，这项功能最为可靠。

让我们看看这个功能是如何实现的。

[PRE1]

使用这种方法，模型对标准问题返回了响应，但对不相关的问题没有返回响应。实际上，我们可以进一步将这种过滤与提示模板中的额外指令相结合。例如，如果我们修订`response_synthesizer`，我们可以从LLM促进更严格的响应：

[PRE2]

这次，模型明确地响应了，“我无法回答”。使用提示模板，StyleSprint可以返回它认为适当的消息，以响应与搜索索引无关的输入，并且作为副作用，忽略不符合其政策的查询。虽然这并不是一个完美的解决方案，但将RAG与更严格的答案过滤相结合可以帮助阻止或防御有害指令或对抗性提示。此外，如第7章[*](B21773_07.xhtml#_idTextAnchor225)所探讨的，我们可以应用RAG特定的评估技术，如RAGAS，以衡量事实一致性和答案的相关性。

# 摘要

在本节中，我们认识到生成式人工智能日益突出，并探讨了应引导其进步的伦理考量。我们概述了关键概念，如透明度、公平性、问责制、尊重隐私、知情同意、安全和包容性，这些对于这些技术的负责任开发和利用至关重要。

我们回顾了尝试对抗这些偏差的策略，包括与人类对齐的训练技术和针对如越狱等易受攻击性的实际应用级措施。总之，我们探索了生成式人工智能采用的多元化和以人为本的方法。

在完成我们对生成式人工智能的基础探索之后，我们现在可以反思我们的旅程。我们开始时是奠定基础，检查基础生成架构，例如生成对抗网络（GANs）、扩散模型和转换器。

*第2章*和*第3章*引导我们了解语言模型的演变，特别关注自回归转换器。我们探讨了这些模型如何显著提升生成式人工智能的能力，推动机器理解和生成类似人类语言边界的扩展。

[*第4章*](B21773_04.xhtml#_idTextAnchor123)为我们提供了在生产就绪环境中的实践经验。在[*第5章*](B21773_05.xhtml#_idTextAnchor180)中，我们探讨了针对特定任务的LLM微调，这是一种增强其性能和适应特定应用的技术。[*第6章*](B21773_06.xhtml#_idTextAnchor211)专注于领域自适应的概念，展示了如何定制AI模型以理解特定领域的细微差别，可以极大地提高其在金融、法律和医疗保健等专门领域的实用性。

*第7章*和*第8章*集中在提示工程和约束生成上，讨论了确保AI生成内容保持可信并与伦理指南保持一致的技术。

本书旨在为生成式人工智能提供一个坚实的基础，为来自各个学科和行业的专业人士提供必要的理论知识与实践技能，以便有效地参与这一变革性技术。生成式人工智能的潜力是巨大的，随着我们对其技术的更深入理解，以及我们对伦理和社会考量的深思熟虑的方法，我们准备好负责任地利用其优势。

# 参考文献

本参考部分作为本书中引用的来源库；您可以探索这些资源，以进一步加深对主题内容的理解和知识：

+   Sun, L., Huang, Y., Wang, H., Wu, S., Zhang, Q., Gao, C., Huang, Y., Lyu, W., Zhang, Y., Li, X., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Kailkhura, B., Xiong, C., Xiao, C., Li, C., Xing, E., . . . Zhao, Y. (2024). *TrustLLM: 大型语言模型的可信度*. *ArXiv*. /abs/2401.05561

+   Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., & Song, D. (2018). *秘密分享者：评估和测试神经网络中的无意记忆*. 在 arXiv [cs.LG]. [http://arxiv.org/abs/1802.08232](http://arxiv.org/abs/1802.08232)

+   Hu, H., Salcic, Z., Sun, L., Dobbie, G., Yu, P. S., & Zhang, X. (2022). *机器学习中的成员推理攻击：综述. ACM 计算评论*, 54(11s), 1–37\. [https://doi.org/10.1145/3523273](https://doi.org/10.1145/3523273)

+   LlamaIndex. (n.d.). *响应合成器. 在 LlamaIndex 文档（稳定版本）中*. 2024年3月12日检索。 [https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html](https://docs.llamaindex.ai/en/stable/module_guides/querying/response_synthesizers/root.html)
