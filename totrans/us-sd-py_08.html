<html><head></head><body>
		<div id="_idContainer054">
			<h1 id="_idParaDest-97" class="chapter-number"><a id="_idTextAnchor153"/>8</h1>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor154"/>Using Community-Shared LoRAs</h1>
			<p>To meet specific needs and generate higher fidelity images, we may need to fine-tune a pre-trained Stable Diffusion model, but the fine-tuning process is extremely slow without powerful GPUs. Even if you have all the hardware or resources on hand, the fine-tuned model is large, usually the same size as the original <span class="No-Break">model file.</span></p>
			<p>Fortunately, researchers<a id="_idIndexMarker244"/> from the Large Language Model (LLM) neighbor community developed an efficient fine-tuning method, <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong> — “Low” is why the “o” is in lowercase) [1]. With LoRA, the original checkpoint is frozen without any modification, and the tuned weight changes are stored in an independent file, which we usually call the LoRA file. Additionally, there are countless community-shared LoRAs on sites such as CIVITAI [4] <span class="No-Break">and HuggingFace.</span></p>
			<p>In this chapter, we are going to delve into the theory of LoRA, and then introduce the Python way to load up LoRA into a Stable Diffusion model. We will also dissect a LoRA model to understand the LoRA model structure internally and create a custom function to load up a Stable Diffusion <span class="No-Break">V1.5 LoRA.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>How does <span class="No-Break">LoRA work?</span></li>
				<li>Using LoRA <span class="No-Break">with Diffusers</span></li>
				<li>Applying LoRA weight <span class="No-Break">during loading</span></li>
				<li>Diving <span class="No-Break">into LoRA</span></li>
				<li>Making a function to <span class="No-Break">load LoRA</span></li>
				<li>Why <span class="No-Break">LoRA works</span></li>
			</ul>
			<p>By the end of this chapter, we will be able to use any community LoRA programmatically and also understand how and why LoRA works in <span class="No-Break">Stable Diffusion.</span><a id="_idTextAnchor155"/></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor156"/>Technical requirements</h1>
			<p>If you have <strong class="source-inline">Diffusers</strong> package running in your computer, you should be able to execute all code in this chapter as well as the code used to load LoRA <span class="No-Break">with Diffusers.</span></p>
			<p>Diffusers use <strong class="bold">PEFT</strong> (<strong class="bold">Parameter-Efficient Fine-Tuning</strong>) [10] to manage the LoRA loading and offloading. PEFT is a<a id="_idIndexMarker245"/> library developed by Hugging Face that provides parameter-efficient ways to adapt large pre-trained models for specific downstream applications. The key idea behind PEFT is to fine-tune only a small fraction of a model’s parameters instead of fine-tuning all of them, resulting in significant savings in terms of computation and memory usage. This makes it possible to fine-tune very large models even on consumer hardware with limited resources. Turn to <a href="B21263_21.xhtml#_idTextAnchor405"><span class="No-Break"><em class="italic">Chapter 21</em></span></a> for more <span class="No-Break">about LoRA.</span></p>
			<p>We will need to install the PEFT package to enable Diffusers’ PEFT <span class="No-Break">LoRA loading:</span></p>
			<pre class="source-code">
pip install PEFT</pre>
			<p>You can also refer to <a href="B21263_02.xhtml#_idTextAnchor037"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, if you encounter other execution errors from <span class="No-Break">the code<a id="_idTextAnchor157"/>.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor158"/>How does LoRA work?</h1>
			<p>LoRA<span class="Annotation-reference"> </span>is<a id="_idIndexMarker246"/> a technique for quickly fine-tuning diffusion models, first introduced by Microsoft researchers in a paper by Edward J. Hu et al [1]. It works by creating a small, low-rank model that is adapted for a specific concept. This small model can be merged with the main checkpoint model to generate images similar to the ones used to <span class="No-Break">train LoRA.</span></p>
			<p>Let’s use <span class="_-----MathTools-_Math_Variable">W</span> to denote the original UNet attention weights (<span class="_-----MathTools-_Math_Variable">Q</span>,<span class="_-----MathTools-_Math_Variable">K</span>,<span class="_-----MathTools-_Math_Variable">V</span>), <span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span> to denote the fine-tuned weights from LoRA, and <span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span> as the merged weights. The process of adding LoRA to a model can be expressed <span class="No-Break">like this:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Δ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">W</span></span></p>
			<p>If we want to control the scale of LoRA weights, we denote the scale as <span class="_-----MathTools-_Math_Variable">α</span>. Adding LoRA to a model can be expressed like <span class="No-Break">this now:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">α</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Δ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">W</span></span></p>
			<p>The range of <span class="_-----MathTools-_Math_Variable">α</span> can be from <strong class="source-inline">0</strong> to <strong class="source-inline">1.0</strong> [2]. It should be fine if we set <span class="_-----MathTools-_Math_Variable">α</span> slightly larger than <strong class="source-inline">1.0</strong>. The reason why LoRA is so small is that <span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span> can be represented by two small matrices <span class="_-----MathTools-_Math_Variable">A</span> and <span class="_-----MathTools-_Math_Variable">B</span>, <span class="No-Break">such that:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span></p>
			<p>Where <span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ℝ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Variable">d</span> is an <span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span> matrix, and <span class="_-----MathTools-_Math_Variable">B</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_Extended">ℝ</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Variable">d</span> is an <span class="_-----MathTools-_Math_Variable">m</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span> matrix. The transpose of <span class="_-----MathTools-_Math_Variable">B</span> denoted as <span class="_-----MathTools-_Math_Variable">B</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">T</span> is a <span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">m</span></span><span class="No-Break"> matrix.</span></p>
			<p>For example, if <span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span> is a <span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">8</span> matrix, there are a total of <strong class="source-inline">48</strong> weight numbers. Now, in the LoRA file, the <span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">8</span> matrix can be represented by two matrices – one <span class="_-----MathTools-_Math_Number">6</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span> matrix, <strong class="source-inline">12</strong> numbers in total, and another <span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">×</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">8</span> matrix, making it <strong class="source-inline">16</strong> numbers <span class="No-Break">in total.</span></p>
			<p>The total <a id="_idIndexMarker247"/>number of weights is reduced from <strong class="source-inline">48</strong> to <strong class="source-inline">28</strong>. This is why the LoRA file can be so small compared to the <span class="No-Break">checkpoint mod<a id="_idTextAnchor159"/>el.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor160"/>Using LoRA with Diffusers</h2>
			<p>With the contributions from the <a id="_idIndexMarker248"/>open source community, loading LoRA with Python has never been easier. In this section, we are going to cover the solutions to load a LoRA model <span class="No-Break">with Diffusers.</span></p>
			<p>In the following steps, we will first load the base Stable Diffusion V1.5, generate an image without LoRA, and then load a LoRA model called <strong class="source-inline">MoXinV1</strong> into the base model. We will clearly see the difference with and without the <span class="No-Break">LoRA model:</span></p>
			<ol>
				<li><strong class="bold">Prepare a Stable Diffusion pipeline</strong>: The following code will load up the Stable Diffusion pipeline and move the pipeline instance <span class="No-Break">to VRAM:</span><pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
pipeline = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre></li>
				<li><strong class="bold">Generate an image without LoRA</strong>: Now, let’s generate an image without LoRA loaded. Here, I am going to use the Stable Diffusion default v1.5 model to generate “a branch of flower” in a “traditional Chinese ink <span class="No-Break">painting” style:</span><pre class="source-code">
prompt = """</pre><pre class="source-code">
shukezouma, shuimobysim, a branch of flower, traditional chinese ink painting</pre><pre class="source-code">
"""</pre><pre class="source-code">
image = pipeline(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda:0").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(image)</pre><p class="list-inset">The<a id="_idIndexMarker249"/> preceding code uses a non-cherry-picked generator with <strong class="source-inline">default seed:1</strong>. The result is shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B21263_08_01.jpg" alt="Figure 8.1: A branch of flower without LoRA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: A branch of flower without LoRA</p>
			<p class="list-inset">To be <a id="_idIndexMarker250"/>honest, the preceding image isn’t that good, and the “flower” is more like black <span class="No-Break">ink dots.</span></p>
			<ol>
				<li value="3"><strong class="bold">Generate an image with LoRA with default settings</strong>: Next, let’s load up the LoRA model to the pipeline and see what the MoXin LoRA can do to help image generation. Loading LoRA with default settings is just one line <span class="No-Break">of code:</span><pre class="source-code">
# load LoRA to the pipeline</pre><pre class="source-code">
pipeline.load_lora_weights(</pre><pre class="source-code">
    "andrewzhu/MoXinV1",</pre><pre class="source-code">
    weight_name   = "MoXinV1.safetensors",</pre><pre class="source-code">
    adapter_name  = "MoXinV1"</pre><pre class="source-code">
)</pre><p class="list-inset">Diffusers downloads the LoRA model file automatically if the model does not exist in your <span class="No-Break">model cache.</span></p><p class="list-inset">Now, run the<a id="_idIndexMarker251"/> inference again with the following code (the same code used in <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">):</span></p><pre class="source-code">
image = pipeline(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda:0").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(image)</pre><p class="list-inset">We will have a new image with a better “flower” in the ink-painting style, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B21263_08_02.jpg" alt="Figure 8.2: A branch of flower with LoRA using the default settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: A branch of flower with LoRA using the default settings</p>
			<p>This <a id="_idIndexMarker252"/>time, the “flower” is more like a flower and, overall, better than the one without applying LoRA. However, the code in this section loads LoRA without a “weight” applied to it. In the next section, we will load a LoRA model with an arbitrary weight (or <span class="No-Break"><span class="_-----MathTools-_Math_Variable">α</span></span><span class="No-Break"> ).</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor161"/>Applying a LoRA weight during loading</h2>
			<p>In the <em class="italic">How does LoRA work?</em> section, we<a id="_idIndexMarker253"/> mentioned the <span class="_-----MathTools-_Math_Variable">α</span> value used to define the portion of LoRA weight added to the main model. We can easily achieve this using Diffusers with <span class="No-Break">PEFT [10].</span></p>
			<p>What is PEFT? PEFT <a id="_idIndexMarker254"/>is a library developed by Hugging Face to efficiently adapt pre-trained models, such as <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) and Stable Diffusion models, to new tasks without needing to<a id="_idIndexMarker255"/> fine-tune the whole model. PEFT is a broader concept representing a collection of methods aimed at efficiently fine-tuning LLMs. LoRA, conversely, is a specific technique that falls under the umbrella <span class="No-Break">of PEFT.</span></p>
			<p>Before the integration of PEFT, loading and managing LoRAs in Diffusers required a lot of custom code and hacking. To make it easier to manage multiple LoRAs with weight loading and offloading, Diffusers uses the PEFT library to help manage different adapters for inference. In PEFT, the fine-tuned parameters are called adapters, which is why you will see some parameters are named <strong class="source-inline">adapters</strong>. LoRA is one of the main adapter techniques; you can take <a id="_idIndexMarker256"/>LoRA and adapter as referring to the same thing through <span class="No-Break">this chapter.</span></p>
			<p>Loading a LoRA model with weight is simple, as shown in the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
pipeline.set_adapters(
    ["MoXinV1"],
    adapter_weights=[0.5]
)
image = pipeline(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
display(image)</pre>
			<p>In the preceding code, we gave the LoRA weight as <strong class="source-inline">0.5</strong> to replace the default <strong class="source-inline">1.0</strong>. Now, you will see the generated image, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B21263_08_03.jpg" alt="Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight</p>
			<p>From <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em>, we can <a id="_idIndexMarker257"/>observe the difference after applying the <strong class="source-inline">0.5</strong> weight to the <span class="No-Break">LoRA model.</span></p>
			<p>The PEFT-integrated Diffusers can also load another LoRA by reusing the same code we used to load the first <span class="No-Break">LoRA model:</span></p>
			<pre class="source-code">
# load another LoRA to the pipeline
pipeline.load_lora_weights(
    "andrewzhu/civitai-light-shadow-lora",
    weight_name   = "light_and_shadow.safetensors",
    adapter_name  = "light_and_shadow"
)</pre>
			<p>Then, add the <a id="_idIndexMarker258"/>weight for the second LoRA model by calling the <span class="No-Break"><strong class="source-inline">set_adapters</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
pipeline.set_adapters(
    ["MoXinV1", "light_and_shadow"],
    adapter_weights=[0.5,1.0]
)
prompt = """
shukezouma, shuimobysim ,a branch of flower, traditional chinese ink painting,STRRY LIGHT,COLORFUL
"""
image = pipeline(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
display(image)</pre>
			<p>We will get a new image with style added from the second LoRA, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B21263_08_04.jpg" alt="Figure 8.4: A branch of flower with two LoRA models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: A branch of flower with two LoRA models</p>
			<p>We can also use the <a id="_idIndexMarker259"/>same code to load LoRA for Stable Diffusion <span class="No-Break">XL pipelines.</span></p>
			<p>With PEFT, we don’t need to restart the pipeline to disable LoRA; we can disable all LoRAs with simply one line <span class="No-Break">of code:</span></p>
			<pre class="source-code">
pipeline.disable_lora()</pre>
			<p>Note that the implementation of LoRA loading is somewhat different compared with other tools, such as A1111 Stable Diffusion WebUI. Using the same prompt, the same settings, and the same LoRA weight, you may get a <span class="No-Break">different result.</span></p>
			<p>Don’t worry – in the next section, we are going to dive into the LoRA model internally and implement a solution to use LoRA that outputs the same result, with tools such as A1111 Stable <span class="No-Break">Diffu<a id="_idTextAnchor162"/>sion WebUI.</span></p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor163"/>Diving into the internal structure of LoRA</h1>
			<p>Understanding how LoRA works <a id="_idIndexMarker260"/>internally will help us to implement our own LoRA-related features based on specific needs. In this section, we are going to dive into the internals of LoRA’s structure and its weights schema, and then manually load the LoRA model into the Stable Diffusion model step <span class="No-Break">by step.</span></p>
			<p>As we discussed at the beginning of the chapter, applying LoRA is as simple as <span class="No-Break">the following:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">α</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">Δ</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">W</span></span></p>
			<p>And <span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span> can be broken down into <span class="_-----MathTools-_Math_Variable">A</span> <span class="No-Break">and </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break">:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span></p>
			<p>So, the overall idea of merging LoRA weights to the checkpoint model works <span class="No-Break">like this:</span></p>
			<ol>
				<li>Find the <span class="_-----MathTools-_Math_Variable">A</span> and <span class="_-----MathTools-_Math_Variable">B</span> weight matrix from the <span class="No-Break">LoRA file.</span></li>
				<li>Match the LoRA module layer name to the checkpoint module layer name so that we know which matrix <span class="No-Break">to merge.</span></li>
				<li>Produce <span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break">.</span></li>
				<li>Update the checkpoint <span class="No-Break">model weights.</span></li>
			</ol>
			<p>If you have prior experience training a LoRA model, you might be aware that a hyperparameter, <strong class="source-inline">alpha</strong>, can be set to a value greater than <strong class="source-inline">1</strong>, such as <strong class="source-inline">4</strong>. This is often done in conjunction with setting another parameter, <strong class="source-inline">rank</strong>, to <strong class="source-inline">4</strong> as well. However, <span class="_-----MathTools-_Math_Variable">α</span> used in this context is typically less than 1. The actual value of <span class="_-----MathTools-_Math_Variable">α</span> is generally computed using the <span class="No-Break">following formula:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">l</span><span class="_-----MathTools-_Math_Variable">p</span><span class="_-----MathTools-_Math_Variable">h</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">k</span><span class="_-----MathTools-_Math_Variable"> </span></p>
			<p>During the training phase, setting both <strong class="source-inline">alpha</strong> and <strong class="source-inline">rank</strong> to <strong class="source-inline">4</strong> will yield an <span class="_-----MathTools-_Math_Variable">α</span> value of <strong class="source-inline">1</strong>. This concept<a id="_idIndexMarker261"/> may seem confusing if not <span class="No-Break">properly understood.</span></p>
			<p>Next, let’s explore the internals of a LoRA model <a id="_idTextAnchor164"/>step <span class="No-Break">by step.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor165"/>Finding the <span class="_-----MathTools-_Math_Variable_v-bold-italic">A</span> and <span class="_-----MathTools-_Math_Variable_v-bold-italic">B</span> weight matrix from the LoRA file</h2>
			<p>Before start <a id="_idIndexMarker262"/>exploring the internals of a LoRA structure, you will need to download a LoRA file. You can download the <strong class="source-inline">MoXinV1.safetensors</strong> from the following <span class="No-Break">URL: </span><a href="https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors"><span class="No-Break">https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors</span></a><span class="No-Break">.</span></p>
			<p>After setting up the LoRA file in the <strong class="source-inline">.safetensors</strong> format, load it using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# load lora file
from safetensors.torch import load_file
lora_path = "MoXinV1.safetensors"
state_dict = load_file(lora_path)
for key in state_dict:
    print(key)</pre>
			<p>When LoRA weights are applied to the text encoder, the key names start <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">lora_te_</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
...
lora_te_text_model_encoder_layers_7_mlp_fc1.alpha
lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight
lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight
...</pre>
			<p>When LoRA weights are applied to UNet, key names start <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">lora_unet_</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
...
lora_unet_down_blocks_0_attentions_1_proj_in.alpha
lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight
lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight
...</pre>
			<p>The output<a id="_idIndexMarker263"/> is of the <strong class="source-inline">string</strong> type. Here are the meanings of the terms that appeared in the output LoRA <span class="No-Break">weight keys:</span></p>
			<ul>
				<li>The <strong class="source-inline">lora_te_</strong> prefix says that the weights are applied to the text encoder; <strong class="source-inline">lora_unet_</strong> says that the weights aim at updating the Stable Diffusion <strong class="source-inline">unet</strong> <span class="No-Break">module layers.</span></li>
				<li><strong class="source-inline">down_blocks_0_attentions_1_proj_in</strong> is the layer name, which should exist in the checkpoint model <strong class="source-inline">unet</strong> <span class="No-Break">modules too.</span></li>
				<li><strong class="source-inline">.alpha</strong> is the trained weight set to denote how much of the LoRA weight will be applied to the main checkpoint model. It holds a float value that is denoted as <span class="_-----MathTools-_Math_Variable">α</span> in <span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Operator_Extended_v-normal">′</span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span>. Since the value will be replaced by user input, we can skip <span class="No-Break">this value.</span></li>
				<li><strong class="source-inline">lora_down.weight</strong> denotes the value of this layer that <span class="No-Break">represents </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">A</span></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">lora_up.weight</strong> denotes the value of this layer that <span class="No-Break">represents </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break">.</span></li>
				<li>Note that <strong class="source-inline">down</strong> in <strong class="source-inline">down_blocks</strong> denotes the downside (the left side of UNet) of the <span class="No-Break"><strong class="source-inline">unet</strong></span><span class="No-Break"> model.</span></li>
			</ul>
			<p>The following Python code will get the LoRA layer info and also have the model <span class="No-Break">object handler:</span></p>
			<pre class="source-code">
# find the layer name
LORA_PREFIX_UNET = 'lora_unet'
LORA_PREFIX_TEXT_ENCODER = 'lora_te'
for key in state_dict:
    if 'text' in key:
        layer_infos = key.split('.')[0].split(
            LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')
        curr_layer = pipeline.text_encoder
    else:
        layer_infos = key.split('.')[0].split(
            LORA_PREFIX_UNET+'_')[-1].split('_')
        curr_layer = pipeline.unet</pre>
			<p><strong class="source-inline">key</strong> holds the<a id="_idIndexMarker264"/> LoRA module layer name, and <strong class="source-inline">layer_infos</strong> holds the checkpoint model layer name extracted from the LoRA layers. The reason we do this is that not all layers from the checkpoint model have LoRA weights to adjust, which is why we need to get the list of layers that <a id="_idTextAnchor166"/>will <span class="No-Break">be updated.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor167"/>Finding the corresponding checkpoint model layer name</h2>
			<p>Print out the <a id="_idIndexMarker265"/>structure of the checkpoint model <span class="No-Break"><strong class="source-inline">unet</strong></span><span class="No-Break"> structure:</span></p>
			<pre class="source-code">
unet = pipeline.unet
modules = unet.named_modules()
for child_name, child_module in modules:
    print("child_module:",child_module)</pre>
			<p>We can see that the module is stored in a tree structure <span class="No-Break">like this:</span></p>
			<pre class="source-code">
...
(down_blocks): ModuleList(
    (0): CrossAttnDownBlock2D(
        (attentions): ModuleList(
        (0-1): 2 x Transformer2DModel(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (attn1): Attention(
                (to_q): Linear(in_features=320, out_features=320, bias=False)
                (to_k): Linear(in_features=320, out_features=320, bias=False)
                (to_v): Linear(in_features=320, out_features=320, bias=False)
                (to_out): ModuleList(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                )
...</pre>
			<p>Each line is <a id="_idIndexMarker266"/>composed of a module name (<strong class="source-inline">down_blocks</strong>), and the module content can be <strong class="source-inline">ModuleList</strong> or a specific neural network layer, <strong class="source-inline">Conv2d</strong>. These are the components of the UNet. For now, applying LoRA to a specific UNet module isn't required. However, it's important to understand the UNet's <span class="No-Break">internal structure:</span></p>
			<pre class="source-code">
# find the layer name
for key in state_dict:
    # find the LoRA layer name (the same code shown above)
    for key in state_dict:
    if 'text' in key:
        layer_infos = key.split('.')[0].split(
            "lora_unet_")[-1].split('_')
        curr_layer = pipeline.text_encoder
    else:
        layer_infos = key.split('.')[0].split(
            "lora_te_")[-1].split('_')
        curr_layer = pipeline.unet
    # loop through the layers to find the target layer
    temp_name = layer_infos.pop(0)
    while len(layer_infos) &gt; -1:
        try:
            curr_layer = curr_layer.__getattr__(temp_name)
            # no exception means the layer is found
            if len(layer_infos) &gt; 0:
                temp_name = layer_infos.pop(0)
            # all names are pop out, break out from the loop
            elif len(layer_infos) == 0:
                break
        except Exception:
            # no such layer exist, pop next name and try again
            if len(temp_name) &gt; 0:
                temp_name += '_'+layer_infos.pop(0)
            else:
                # temp_name is empty
                temp_name = layer_infos.pop(0)</pre>
			<p>The loop-through part is a bit tricky. When looking back to the checkpoint model structure, which is layered as a tree, we can’t simply use a <strong class="source-inline">for</strong> loop to loop through the list. Instead, we need to use a <strong class="source-inline">while</strong> loop to navigate every leaf of the tree. The overall process is <span class="No-Break">as follows:</span></p>
			<ol>
				<li><strong class="source-inline">layer_infos.pop(0)</strong> will return the first name of the list in the <strong class="source-inline">string</strong> type and remove it from the list such as <strong class="source-inline">up</strong> from the <strong class="source-inline">layer_infos</strong> list – <strong class="source-inline">['up', 'blocks', '3', 'attentions', '2', 'transformer', 'blocks', '0', 'ff', '</strong><span class="No-Break"><strong class="source-inline">net', '2']</strong></span></li>
				<li>Use <strong class="source-inline">curr_layer.__getattr__(temp_name)</strong> to check whether the layer exists or not. If it does not exist, an exception will be thrown, and the program will move to the <strong class="source-inline">exception </strong>section to continue outputting names from the <strong class="source-inline">layer_infos</strong> list and <span class="No-Break">check again.</span></li>
				<li>If the layer is found but some names are still left in the <strong class="source-inline">layer_infos</strong> list, they will keep on <span class="No-Break">popping out.</span></li>
				<li>The names will continue to pop out until no exception is thrown out and we meet the <strong class="source-inline">len(layer_infos) == 0</strong> condition, which means that the layer is <span class="No-Break">fully matched.</span></li>
			</ol>
			<p>At this point, the <strong class="source-inline">curr_layer</strong> object <a id="_idIndexMarker267"/>points to the checkpoint model weight data and can be reference<a id="_idTextAnchor168"/>d in the <span class="No-Break">next step.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor169"/>Updating the checkpoint model weights</h2>
			<p>For easier key value <a id="_idIndexMarker268"/>referencing, let’s make a <strong class="source-inline">pair_keys = []</strong> list, in which <strong class="source-inline">pair_keys[0]</strong> returns the <span class="_-----MathTools-_Math_Variable">A</span> matrix and <strong class="source-inline">pair_keys[1]</strong> returns the <span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break"> matrix:</span></p>
			<pre class="source-code">
# ensure the sequence of lora_up(A) then lora_down(B)
pair_keys = []
if 'lora_down' in key:
    pair_keys.append(key.replace('lora_down', 'lora_up'))
    pair_keys.append(key)
else:
    pair_keys.append(key)
    pair_keys.append(key.replace('lora_up', 'lora_down'))</pre>
			<p>Then, we <a id="_idIndexMarker269"/>update <span class="No-Break">the weights:</span></p>
			<pre class="source-code">
alpha = 0.5
# update weight
if len(state_dict[pair_keys[0]].shape) == 4:
    # squeeze(3) and squeeze(2) remove dimensions of size 1 
    #from the tensor to make the tensor more compact
    weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).\
        to(torch.float32)
    weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).\
        to(torch.float32)
    curr_layer.weight.data += alpha * torch.mm(weight_up, 
        weight_down).unsqueeze(2).unsqueeze(3)
else:
    weight_up = state_dict[pair_keys[0]].to(torch.float32)
    weight_down = state_dict[pair_keys[1]].to(torch.float32)
    curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)</pre>
			<p>The <strong class="source-inline">alpha * torch.mm(weight_up, weight_down)</strong> code is the core code used to <a id="_idIndexMarker270"/>implement <span class="_-----MathTools-_Math_Variable">α</span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break">.</span></p>
			<p>And that’s it! Now, the pipeline’s text encoder and <strong class="source-inline">unet</strong> model weights are updated by LoRA. Next, let’s put all the parts together to create a full-featured function that can load a LoRA model into the Stab<a id="_idTextAnchor170"/>le <span class="No-Break">Diffusion pipeline.</span></p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor171"/>Making a function to load LoRA</h1>
			<p>Let’s add one<a id="_idIndexMarker271"/> more list to store keys that have been visited and put all the preceding code together into a function <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">load_lora</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
def load_lora(
    pipeline,
    lora_path,
    lora_weight = 0.5,
    device = 'cpu'
):
    state_dict = load_file(lora_path, device=device)
    LORA_PREFIX_UNET = 'lora_unet'
    LORA_PREFIX_TEXT_ENCODER = 'lora_te'
    alpha = lora_weight
    visited = []
    # directly update weight in diffusers model
    for key in state_dict:
        # as we have set the alpha beforehand, so just skip
        if '.alpha' in key or key in visited:
            continue
        if 'text' in key:
            layer_infos = key.split('.')[0].split(
                LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')
            curr_layer = pipeline.text_encoder
        else:
            layer_infos = key.split('.')[0].split(
                LORA_PREFIX_UNET+'_')[-1].split('_')
            curr_layer = pipeline.unet
        # find the target layer
        # loop through the layers to find the target layer
        temp_name = layer_infos.pop(0)
        while len(layer_infos) &gt; -1:
            try:
                curr_layer = curr_layer.__getattr__(temp_name)
                # no exception means the layer is found
                if len(layer_infos) &gt; 0:
                    temp_name = layer_infos.pop(0)
                # layer found but length is 0,
                # break the loop and curr_layer keep point to the 
                # current layer
                elif len(layer_infos) == 0:
                    break
            except Exception:
                # no such layer exist, pop next name and try again
                if len(temp_name) &gt; 0:
                    temp_name += '_'+layer_infos.pop(0)
                else:
                    # temp_name is empty
                    temp_name = layer_infos.pop(0)
        # org_forward(x) + lora_up(lora_down(x)) * multiplier
        # ensure the sequence of lora_up(A) then lora_down(B)
        pair_keys = []
        if 'lora_down' in key:
            pair_keys.append(key.replace('lora_down', 'lora_up'))
            pair_keys.append(key)
        else:
            pair_keys.append(key)
            pair_keys.append(key.replace('lora_up', 'lora_down'))
        # update weight
        if len(state_dict[pair_keys[0]].shape) == 4:
            # squeeze(3) and squeeze(2) remove dimensions of size 1 
            # from the tensor to make the tensor more compact
            weight_up = state_dict[pair_keys[0]].squeeze(3).\
                squeeze(2).to(torch.float32)
            weight_down = state_dict[pair_keys[1]].squeeze(3).\
                squeeze(2).to(torch.float32)
            curr_layer.weight.data += alpha * torch.mm(weight_up, 
                weight_down).unsqueeze(2).unsqueeze(3)
        else:
            weight_up = state_dict[pair_keys[0]].to(torch.float32)
            weight_down = state_dict[pair_keys[1]].to(torch.float32)
            curr_layer.weight.data += alpha * torch.mm(weight_up, 
                weight_down)
        # update visited list, ensure no duplicated weight is 
        # processed.
        for item in pair_keys:
            visited.append(item)</pre>
			<p>To use the function is easy; simply provide the <strong class="source-inline">pipeline</strong> object, the LoRA path, <strong class="source-inline">lora_path</strong>, and the <a id="_idIndexMarker272"/>LoRA weight number, <strong class="source-inline">lora_weight</strong>, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
pipeline = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.bfloat16
).to("cuda:0")
lora_path = r"MoXinV1.safetensors"
load_lora(
    pipeline = pipeline,
    lora_path = lora_path,
    lora_weight = 0.5,
    device = "cuda:0"
)</pre>
			<p>Now, let’s <a id="_idIndexMarker273"/>try <span class="No-Break">it out:</span></p>
			<pre class="source-code">
prompt = """
shukezouma, shuimobysim ,a branch of flower, traditional chinese ink painting
"""
image = pipeline(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
display(image)</pre>
			<p>It works, and works well; see the result shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B21263_08_05.jpg" alt="Figure 8.5: A branch of flower using the custom LoRA loader"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: A branch of flower using the custom LoRA loader</p>
			<p>You might<a id="_idIndexMarker274"/> be wondering, “Why does a small LoRA file possess such formidable capabilities?” Let’s delve deeper into the reasons why a<a id="_idTextAnchor172"/> LoRA model <span class="No-Break">is effective.</span></p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor173"/>Why LoRA works</h1>
			<p>The paper <em class="italic">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</em> [8] by Armen et al. found that the pre-trained representations’ intrinsic dimension is way lower than expected, stated by them <span class="No-Break">as follows:</span></p>
			<p class="author-quote">“We empirically show that common NLP tasks within the context of pre-trained representations have an intrinsic dimension several orders of magnitudes less than the full parameterization.”</p>
			<p>The intrinsic <a id="_idIndexMarker275"/>dimension of a matrix is a concept used to determine the effective number of dimensions required to represent the important information contained within <span class="No-Break">that matrix.</span></p>
			<p>Let’s suppose we have a matrix, <strong class="source-inline">M</strong>, with five rows and three columns, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
M =  1  2  3
     4  5  6
     7  8  9
     10 11 12
     13 14 15</pre>
			<p>Each row of this matrix represents a data point or a vector with three values. We can think of these vectors as points in a three-dimensional space. However, if we visualize these points, we might find that they lie approximately on a two-dimensional plane, rather than occupying the full <span class="No-Break">three-dimensional space.</span></p>
			<p>In this case, the intrinsic dimension of the matrix, <strong class="source-inline">M</strong>, would be <strong class="source-inline">2</strong>, indicating that the essential structure of the data can be captured effectively using two dimensions. The third dimension doesn’t provide much <span class="No-Break">additional information.</span></p>
			<p>A low intrinsic dimension matrix can be represented by two low-rank matrices because the data in the matrix can be compressed into a few key features. These features can then be represented by two smaller matrices, each of which has a rank that is equal to the intrinsic dimension of the <span class="No-Break">original matrix.</span></p>
			<p>The paper <em class="italic">LoRA: Low-Rank Adaptation of Large Language Models</em> [1] by Edward J. Hu et al goes a step further, introducing the concept of LoRA to leverage the low intrinsic dimension nature, boosting the fine-tuning process by breaking down the delta weights to two low-rank parts, <span class="_-----MathTools-_Math_Variable">Δ</span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">B</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"> </span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">T</span></span><span class="No-Break">.</span></p>
			<p>The effectiveness of <a id="_idIndexMarker276"/>LoRA was soon discovered to extend beyond LLM models, also yielding good results with diffusion models. Simo Ryu published the LoRA [2] code and was the first one to try out LoRA training for Stable Diffusion. That was in July 2023 and there are now more than 40,000 LoRA models shared <a id="_idTextAnchor174"/><span class="No-Break">at </span><a href="https://www.civitai.com"><span class="No-Break">https://www.civitai.com</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor175"/>Summary</h1>
			<p>In this chapter, we discussed how to enhance the Stable Diffusion model using LoRA, understood what LoRA is, and why it is good for fine-tuning <span class="No-Break">and inference.</span></p>
			<p>Then, we began loading LoRA using the experimental functions from the <strong class="source-inline">Diffusers</strong> package and provided LoRA weights through a custom implementation. We used simple code to quickly understand what LoRA can bring to <span class="No-Break">the table.</span></p>
			<p>Then, we dived into the internal structure of a LoRA model, walked through the detailed steps to extract LoRA weights, and understood how to merge those weights into the <span class="No-Break">checkpoint model.</span></p>
			<p>Further, we implemented a function in Python that can load a LoRA safetensors file and perform <span class="No-Break">weight merges.</span></p>
			<p>Finally, we briefly explored why LoRA works, based on the most recent papers <span class="No-Break">from researchers.</span></p>
			<p>In the next chapter, we are going to explore another powerful technique – textual inversion – to teach a model new “words,” and then use the pre-trained “words” to add new concep<a id="_idTextAnchor176"/>ts to the <span class="No-Break">generated images.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor177"/>References</h1>
			<ol>
				<li>Edward J. et al, LoRA: Low-Rank Adaptation of Large Language <span class="No-Break">Models: </span><a href="https://arxiv.org/abs/2106.09685"><span class="No-Break">https://arxiv.org/abs/2106.09685</span></a><a href="https://arxiv.org/abs/2106.09685&#13;"/></li>
				<li>Simo Ryu (cloneofsimo), <span class="No-Break"><strong class="source-inline">lora</strong></span><span class="No-Break">: </span><a href="https://github.com/cloneofsimo/lora"><span class="No-Break">https://github.com/cloneofsimo/lora</span></a><a href="https://github.com/cloneofsimo/lora&#13;"/></li>
				<li><span class="No-Break"><strong class="source-inline">kohya_lora_loader</strong></span><span class="No-Break">: </span><a href="https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44"><span class="No-Break">https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44</span></a></li>
				<li><span class="No-Break">CIVITAI: </span><a href="https://www.civitai.com"><span class="No-Break">https://www.civitai.com</span></a></li>
				<li>Rinon Gal et al, An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion: <a href="https://textual-inversion.github.io/">https://textual-inversion.github.io/</a> </li>
				<li>Diffusers’ <strong class="source-inline">lora_state_dict</strong> <span class="No-Break">function: </span><a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py"><span class="No-Break">https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py</span></a></li>
				<li>Andrew Zhu, Improving Diffusers Package for High-Quality Image <span class="No-Break">Generation: </span><a href="https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4"><span class="No-Break">https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4</span></a></li>
				<li>Armen et al, Intrinsic Dimensionality Explains the Effectiveness of Language Model <span class="No-Break">Fine-Tuning: </span><a href="https://arxiv.org/abs/2012.13255"><span class="No-Break">https://arxiv.org/abs/2012.13255</span></a></li>
				<li>Hugging Face, <span class="No-Break">LoRA: </span><a href="https://huggingface.co/docs/diffusers/training/lora"><span class="No-Break">https://huggingface.co/docs/diffusers/training/lora</span></a></li>
				<li>Hugging Face, <span class="No-Break">PEFT: </span><a href="https://huggingface.co/docs/peft/en/index"><span class="No-Break">https://huggingface.co/docs/peft/en/index</span></a></li>
			</ol>
		</div>
	</body></html>