<html><head></head><body>
		<div><h1 id="_idParaDest-97" class="chapter-number"><a id="_idTextAnchor153"/>8</h1>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor154"/>Using Community-Shared LoRAs</h1>
			<p>To meet specific needs and generate higher fidelity images, we may need to fine-tune a pre-trained Stable Diffusion model, but the fine-tuning process is extremely slow without powerful GPUs. Even if you have all the hardware or resources on hand, the fine-tuned model is large, usually the same size as the original model file.</p>
			<p>Fortunately, researchers<a id="_idIndexMarker244"/> from the Large Language Model (LLM) neighbor community developed an efficient fine-tuning method, <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong> — “Low” is why the “o” is in lowercase) [1]. With LoRA, the original checkpoint is frozen without any modification, and the tuned weight changes are stored in an independent file, which we usually call the LoRA file. Additionally, there are countless community-shared LoRAs on sites such as CIVITAI [4] and HuggingFace.</p>
			<p>In this chapter, we are going to delve into the theory of LoRA, and then introduce the Python way to load up LoRA into a Stable Diffusion model. We will also dissect a LoRA model to understand the LoRA model structure internally and create a custom function to load up a Stable Diffusion V1.5 LoRA.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>How does LoRA work?</li>
				<li>Using LoRA with Diffusers</li>
				<li>Applying LoRA weight during loading</li>
				<li>Diving into LoRA</li>
				<li>Making a function to load LoRA</li>
				<li>Why LoRA works</li>
			</ul>
			<p>By the end of this chapter, we will be able to use any community LoRA programmatically and also understand how and why LoRA works in Stable Diffusion.<a id="_idTextAnchor155"/></p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor156"/>Technical requirements</h1>
			<p>If you have <code>Diffusers</code> package running in your computer, you should be able to execute all code in this chapter as well as the code used to load LoRA with Diffusers.</p>
			<p>Diffusers use <strong class="bold">PEFT</strong> (<strong class="bold">Parameter-Efficient Fine-Tuning</strong>) [10] to manage the LoRA loading and offloading. PEFT is a<a id="_idIndexMarker245"/> library developed by Hugging Face that provides parameter-efficient ways to adapt large pre-trained models for specific downstream applications. The key idea behind PEFT is to fine-tune only a small fraction of a model’s parameters instead of fine-tuning all of them, resulting in significant savings in terms of computation and memory usage. This makes it possible to fine-tune very large models even on consumer hardware with limited resources. Turn to <a href="B21263_21.xhtml#_idTextAnchor405"><em class="italic">Chapter 21</em></a> for more about LoRA.</p>
			<p>We will need to install the PEFT package to enable Diffusers’ PEFT LoRA loading:</p>
			<pre class="source-code">
pip install PEFT</pre>
			<p>You can also refer to <a href="B21263_02.xhtml#_idTextAnchor037"><em class="italic">Chapter 2</em></a>, if you encounter other execution errors from the code<a id="_idTextAnchor157"/>.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor158"/>How does LoRA work?</h1>
			<p>LoRA is<a id="_idIndexMarker246"/> a technique for quickly fine-tuning diffusion models, first introduced by Microsoft researchers in a paper by Edward J. Hu et al [1]. It works by creating a small, low-rank model that is adapted for a specific concept. This small model can be merged with the main checkpoint model to generate images similar to the ones used to train LoRA.</p>
			<p>Let’s use W to denote the original UNet attention weights (Q,K,V), ΔW to denote the fine-tuned weights from LoRA, and W′ as the merged weights. The process of adding LoRA to a model can be expressed like this:</p>
			<p>W′= W + ΔW</p>
			<p>If we want to control the scale of LoRA weights, we denote the scale as α. Adding LoRA to a model can be expressed like this now:</p>
			<p>W′= W + αΔW</p>
			<p>The range of α can be from <code>0</code> to <code>1.0</code> [2]. It should be fine if we set α slightly larger than <code>1.0</code>. The reason why LoRA is so small is that ΔW can be represented by two small matrices A and B, such that:</p>
			<p>ΔW = A B T</p>
			<p>Where A ∈ ℝ n×d is an n × d matrix, and B ∈ ℝ m×d is an m × d matrix. The transpose of B denoted as B T is a d × m matrix.</p>
			<p>For example, if ΔW is a 6 × 8 matrix, there are a total of <code>48</code> weight numbers. Now, in the LoRA file, the 6 × 8 matrix can be represented by two matrices – one 6 × 2 matrix, <code>12</code> numbers in total, and another 2 × 8 matrix, making it <code>16</code> numbers in total.</p>
			<p>The total <a id="_idIndexMarker247"/>number of weights is reduced from <code>48</code> to <code>28</code>. This is why the LoRA file can be so small compared to the checkpoint mod<a id="_idTextAnchor159"/>el.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor160"/>Using LoRA with Diffusers</h2>
			<p>With the contributions from the <a id="_idIndexMarker248"/>open source community, loading LoRA with Python has never been easier. In this section, we are going to cover the solutions to load a LoRA model with Diffusers.</p>
			<p>In the following steps, we will first load the base Stable Diffusion V1.5, generate an image without LoRA, and then load a LoRA model called <code>MoXinV1</code> into the base model. We will clearly see the difference with and without the LoRA model:</p>
			<ol>
				<li><strong class="bold">Prepare a Stable Diffusion pipeline</strong>: The following code will load up the Stable Diffusion pipeline and move the pipeline instance to VRAM:<pre class="source-code">
import torch</pre><pre class="source-code">
from diffusers import StableDiffusionPipeline</pre><pre class="source-code">
pipeline = StableDiffusionPipeline.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    torch_dtype = torch.float16</pre><pre class="source-code">
).to("cuda:0")</pre></li>
				<li><strong class="bold">Generate an image without LoRA</strong>: Now, let’s generate an image without LoRA loaded. Here, I am going to use the Stable Diffusion default v1.5 model to generate “a branch of flower” in a “traditional Chinese ink painting” style:<pre class="source-code">
prompt = """</pre><pre class="source-code">
shukezouma, shuimobysim, a branch of flower, traditional chinese ink painting</pre><pre class="source-code">
"""</pre><pre class="source-code">
image = pipeline(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda:0").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(image)</pre><p class="list-inset">The<a id="_idIndexMarker249"/> preceding code uses a non-cherry-picked generator with <code>default seed:1</code>. The result is shown in <em class="italic">Figure 8</em><em class="italic">.1</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_08_01.jpg" alt="Figure 8.1: A branch of flower without LoRA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1: A branch of flower without LoRA</p>
			<p class="list-inset">To be <a id="_idIndexMarker250"/>honest, the preceding image isn’t that good, and the “flower” is more like black ink dots.</p>
			<ol>
				<li value="3"><strong class="bold">Generate an image with LoRA with default settings</strong>: Next, let’s load up the LoRA model to the pipeline and see what the MoXin LoRA can do to help image generation. Loading LoRA with default settings is just one line of code:<pre class="source-code">
# load LoRA to the pipeline</pre><pre class="source-code">
pipeline.load_lora_weights(</pre><pre class="source-code">
    "andrewzhu/MoXinV1",</pre><pre class="source-code">
    weight_name   = "MoXinV1.safetensors",</pre><pre class="source-code">
    adapter_name  = "MoXinV1"</pre><pre class="source-code">
)</pre><p class="list-inset">Diffusers downloads the LoRA model file automatically if the model does not exist in your model cache.</p><p class="list-inset">Now, run the<a id="_idIndexMarker251"/> inference again with the following code (the same code used in <em class="italic">step 2</em>):</p><pre class="source-code">
image = pipeline(</pre><pre class="source-code">
    prompt = prompt,</pre><pre class="source-code">
    generator = torch.Generator("cuda:0").manual_seed(1)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
display(image)</pre><p class="list-inset">We will have a new image with a better “flower” in the ink-painting style, as shown in <em class="italic">Figure 8</em><em class="italic">.2</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_08_02.jpg" alt="Figure 8.2: A branch of flower with LoRA using the default settings"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2: A branch of flower with LoRA using the default settings</p>
			<p>This <a id="_idIndexMarker252"/>time, the “flower” is more like a flower and, overall, better than the one without applying LoRA. However, the code in this section loads LoRA without a “weight” applied to it. In the next section, we will load a LoRA model with an arbitrary weight (or α ).</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor161"/>Applying a LoRA weight during loading</h2>
			<p>In the <em class="italic">How does LoRA work?</em> section, we<a id="_idIndexMarker253"/> mentioned the α value used to define the portion of LoRA weight added to the main model. We can easily achieve this using Diffusers with PEFT [10].</p>
			<p>What is PEFT? PEFT <a id="_idIndexMarker254"/>is a library developed by Hugging Face to efficiently adapt pre-trained models, such as <strong class="bold">Large Language Models</strong> (<strong class="bold">LLMs</strong>) and Stable Diffusion models, to new tasks without needing to<a id="_idIndexMarker255"/> fine-tune the whole model. PEFT is a broader concept representing a collection of methods aimed at efficiently fine-tuning LLMs. LoRA, conversely, is a specific technique that falls under the umbrella of PEFT.</p>
			<p>Before the integration of PEFT, loading and managing LoRAs in Diffusers required a lot of custom code and hacking. To make it easier to manage multiple LoRAs with weight loading and offloading, Diffusers uses the PEFT library to help manage different adapters for inference. In PEFT, the fine-tuned parameters are called adapters, which is why you will see some parameters are named <code>adapters</code>. LoRA is one of the main adapter techniques; you can take <a id="_idIndexMarker256"/>LoRA and adapter as referring to the same thing through this chapter.</p>
			<p>Loading a LoRA model with weight is simple, as shown in the following code:</p>
			<pre class="source-code">
pipeline.set_adapters(
    ["MoXinV1"],
    adapter_weights=[0.5]
)
image = pipeline(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
display(image)</pre>
			<p>In the preceding code, we gave the LoRA weight as <code>0.5</code> to replace the default <code>1.0</code>. Now, you will see the generated image, as shown in <em class="italic">Figure 8</em><em class="italic">.3</em>:</p>
			<div><div><img src="img/B21263_08_03.jpg" alt="Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3: A branch of flower with LoRA by applying a 0.5 LoRA weight</p>
			<p>From <em class="italic">Figure 8</em><em class="italic">.3</em>, we can <a id="_idIndexMarker257"/>observe the difference after applying the <code>0.5</code> weight to the LoRA model.</p>
			<p>The PEFT-integrated Diffusers can also load another LoRA by reusing the same code we used to load the first LoRA model:</p>
			<pre class="source-code">
# load another LoRA to the pipeline
pipeline.load_lora_weights(
    "andrewzhu/civitai-light-shadow-lora",
    weight_name   = "light_and_shadow.safetensors",
    adapter_name  = "light_and_shadow"
)</pre>
			<p>Then, add the <a id="_idIndexMarker258"/>weight for the second LoRA model by calling the <code>set_adapters</code> function:</p>
			<pre class="source-code">
pipeline.set_adapters(
    ["MoXinV1", "light_and_shadow"],
    adapter_weights=[0.5,1.0]
)
prompt = """
shukezouma, shuimobysim ,a branch of flower, traditional chinese ink painting,STRRY LIGHT,COLORFUL
"""
image = pipeline(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
display(image)</pre>
			<p>We will get a new image with style added from the second LoRA, as shown in <em class="italic">Figure 8</em><em class="italic">.4</em>:</p>
			<div><div><img src="img/B21263_08_04.jpg" alt="Figure 8.4: A branch of flower with two LoRA models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4: A branch of flower with two LoRA models</p>
			<p>We can also use the <a id="_idIndexMarker259"/>same code to load LoRA for Stable Diffusion XL pipelines.</p>
			<p>With PEFT, we don’t need to restart the pipeline to disable LoRA; we can disable all LoRAs with simply one line of code:</p>
			<pre class="source-code">
pipeline.disable_lora()</pre>
			<p>Note that the implementation of LoRA loading is somewhat different compared with other tools, such as A1111 Stable Diffusion WebUI. Using the same prompt, the same settings, and the same LoRA weight, you may get a different result.</p>
			<p>Don’t worry – in the next section, we are going to dive into the LoRA model internally and implement a solution to use LoRA that outputs the same result, with tools such as A1111 Stable Diffu<a id="_idTextAnchor162"/>sion WebUI.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor163"/>Diving into the internal structure of LoRA</h1>
			<p>Understanding how LoRA works <a id="_idIndexMarker260"/>internally will help us to implement our own LoRA-related features based on specific needs. In this section, we are going to dive into the internals of LoRA’s structure and its weights schema, and then manually load the LoRA model into the Stable Diffusion model step by step.</p>
			<p>As we discussed at the beginning of the chapter, applying LoRA is as simple as the following:</p>
			<p>W′= W + αΔW</p>
			<p>And ΔW can be broken down into A and B:</p>
			<p>ΔW = A B T</p>
			<p>So, the overall idea of merging LoRA weights to the checkpoint model works like this:</p>
			<ol>
				<li>Find the A and B weight matrix from the LoRA file.</li>
				<li>Match the LoRA module layer name to the checkpoint module layer name so that we know which matrix to merge.</li>
				<li>Produce ΔW = A B T.</li>
				<li>Update the checkpoint model weights.</li>
			</ol>
			<p>If you have prior experience training a LoRA model, you might be aware that a hyperparameter, <code>alpha</code>, can be set to a value greater than <code>1</code>, such as <code>4</code>. This is often done in conjunction with setting another parameter, <code>rank</code>, to <code>4</code> as well. However, α used in this context is typically less than 1. The actual value of α is generally computed using the following formula:</p>
			<p>α =  alpha _ rank </p>
			<p>During the training phase, setting both <code>alpha</code> and <code>rank</code> to <code>4</code> will yield an α value of <code>1</code>. This concept<a id="_idIndexMarker261"/> may seem confusing if not properly understood.</p>
			<p>Next, let’s explore the internals of a LoRA model <a id="_idTextAnchor164"/>step by step.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor165"/>Finding the A and B weight matrix from the LoRA file</h2>
			<p>Before start <a id="_idIndexMarker262"/>exploring the internals of a LoRA structure, you will need to download a LoRA file. You can download the <code>MoXinV1.safetensors</code> from the following URL: <a href="https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors">https://huggingface.co/andrewzhu/MoXinV1/resolve/main/MoXinV1.safetensors</a>.</p>
			<p>After setting up the LoRA file in the <code>.safetensors</code> format, load it using the following code:</p>
			<pre class="source-code">
# load lora file
from safetensors.torch import load_file
lora_path = "MoXinV1.safetensors"
state_dict = load_file(lora_path)
for key in state_dict:
    print(key)</pre>
			<p>When LoRA weights are applied to the text encoder, the key names start with <code>lora_te_</code>:</p>
			<pre class="source-code">
...
lora_te_text_model_encoder_layers_7_mlp_fc1.alpha
lora_te_text_model_encoder_layers_7_mlp_fc1.lora_down.weight
lora_te_text_model_encoder_layers_7_mlp_fc1.lora_up.weight
...</pre>
			<p>When LoRA weights are applied to UNet, key names start with <code>lora_unet_</code>:</p>
			<pre class="source-code">
...
lora_unet_down_blocks_0_attentions_1_proj_in.alpha
lora_unet_down_blocks_0_attentions_1_proj_in.lora_down.weight
lora_unet_down_blocks_0_attentions_1_proj_in.lora_up.weight
...</pre>
			<p>The output<a id="_idIndexMarker263"/> is of the <code>string</code> type. Here are the meanings of the terms that appeared in the output LoRA weight keys:</p>
			<ul>
				<li>The <code>lora_te_</code> prefix says that the weights are applied to the text encoder; <code>lora_unet_</code> says that the weights aim at updating the Stable Diffusion <code>unet</code> module layers.</li>
				<li><code>down_blocks_0_attentions_1_proj_in</code> is the layer name, which should exist in the checkpoint model <code>unet</code> modules too.</li>
				<li><code>.alpha</code> is the trained weight set to denote how much of the LoRA weight will be applied to the main checkpoint model. It holds a float value that is denoted as α in W′= W + αΔW. Since the value will be replaced by user input, we can skip this value.</li>
				<li><code>lora_down.weight</code> denotes the value of this layer that represents A.</li>
				<li><code>lora_up.weight</code> denotes the value of this layer that represents B.</li>
				<li>Note that <code>down</code> in <code>down_blocks</code> denotes the downside (the left side of UNet) of the <code>unet</code> model.</li>
			</ul>
			<p>The following Python code will get the LoRA layer info and also have the model object handler:</p>
			<pre class="source-code">
# find the layer name
LORA_PREFIX_UNET = 'lora_unet'
LORA_PREFIX_TEXT_ENCODER = 'lora_te'
for key in state_dict:
    if 'text' in key:
        layer_infos = key.split('.')[0].split(
            LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')
        curr_layer = pipeline.text_encoder
    else:
        layer_infos = key.split('.')[0].split(
            LORA_PREFIX_UNET+'_')[-1].split('_')
        curr_layer = pipeline.unet</pre>
			<p><code>key</code> holds the<a id="_idIndexMarker264"/> LoRA module layer name, and <code>layer_infos</code> holds the checkpoint model layer name extracted from the LoRA layers. The reason we do this is that not all layers from the checkpoint model have LoRA weights to adjust, which is why we need to get the list of layers that <a id="_idTextAnchor166"/>will be updated.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor167"/>Finding the corresponding checkpoint model layer name</h2>
			<p>Print out the <a id="_idIndexMarker265"/>structure of the checkpoint model <code>unet</code> structure:</p>
			<pre class="source-code">
unet = pipeline.unet
modules = unet.named_modules()
for child_name, child_module in modules:
    print("child_module:",child_module)</pre>
			<p>We can see that the module is stored in a tree structure like this:</p>
			<pre class="source-code">
...
(down_blocks): ModuleList(
    (0): CrossAttnDownBlock2D(
        (attentions): ModuleList(
        (0-1): 2 x Transformer2DModel(
            (norm): GroupNorm(32, 320, eps=1e-06, affine=True)
            (proj_in): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))
            (transformer_blocks): ModuleList(
            (0): BasicTransformerBlock(
                (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
                (attn1): Attention(
                (to_q): Linear(in_features=320, out_features=320, bias=False)
                (to_k): Linear(in_features=320, out_features=320, bias=False)
                (to_v): Linear(in_features=320, out_features=320, bias=False)
                (to_out): ModuleList(
                    (0): Linear(in_features=320, out_features=320, bias=True)
                    (1): Dropout(p=0.0, inplace=False)
                )
...</pre>
			<p>Each line is <a id="_idIndexMarker266"/>composed of a module name (<code>down_blocks</code>), and the module content can be <code>ModuleList</code> or a specific neural network layer, <code>Conv2d</code>. These are the components of the UNet. For now, applying LoRA to a specific UNet module isn't required. However, it's important to understand the UNet's internal structure:</p>
			<pre class="source-code">
# find the layer name
for key in state_dict:
    # find the LoRA layer name (the same code shown above)
    for key in state_dict:
    if 'text' in key:
        layer_infos = key.split('.')[0].split(
            "lora_unet_")[-1].split('_')
        curr_layer = pipeline.text_encoder
    else:
        layer_infos = key.split('.')[0].split(
            "lora_te_")[-1].split('_')
        curr_layer = pipeline.unet
    # loop through the layers to find the target layer
    temp_name = layer_infos.pop(0)
    while len(layer_infos) &gt; -1:
        try:
            curr_layer = curr_layer.__getattr__(temp_name)
            # no exception means the layer is found
            if len(layer_infos) &gt; 0:
                temp_name = layer_infos.pop(0)
            # all names are pop out, break out from the loop
            elif len(layer_infos) == 0:
                break
        except Exception:
            # no such layer exist, pop next name and try again
            if len(temp_name) &gt; 0:
                temp_name += '_'+layer_infos.pop(0)
            else:
                # temp_name is empty
                temp_name = layer_infos.pop(0)</pre>
			<p>The loop-through part is a bit tricky. When looking back to the checkpoint model structure, which is layered as a tree, we can’t simply use a <code>for</code> loop to loop through the list. Instead, we need to use a <code>while</code> loop to navigate every leaf of the tree. The overall process is as follows:</p>
			<ol>
				<li><code>layer_infos.pop(0)</code> will return the first name of the list in the <code>string</code> type and remove it from the list such as <code>up</code> from the <code>layer_infos</code> list – <code>['up', 'blocks', '3', 'attentions', '2', 'transformer', 'blocks', '0', 'ff', '</code><code>net', '2']</code></li>
				<li>Use <code>curr_layer.__getattr__(temp_name)</code> to check whether the layer exists or not. If it does not exist, an exception will be thrown, and the program will move to the <code>exception </code>section to continue outputting names from the <code>layer_infos</code> list and check again.</li>
				<li>If the layer is found but some names are still left in the <code>layer_infos</code> list, they will keep on popping out.</li>
				<li>The names will continue to pop out until no exception is thrown out and we meet the <code>len(layer_infos) == 0</code> condition, which means that the layer is fully matched.</li>
			</ol>
			<p>At this point, the <code>curr_layer</code> object <a id="_idIndexMarker267"/>points to the checkpoint model weight data and can be reference<a id="_idTextAnchor168"/>d in the next step.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor169"/>Updating the checkpoint model weights</h2>
			<p>For easier key value <a id="_idIndexMarker268"/>referencing, let’s make a <code>pair_keys = []</code> list, in which <code>pair_keys[0]</code> returns the A matrix and <code>pair_keys[1]</code> returns the B matrix:</p>
			<pre class="source-code">
# ensure the sequence of lora_up(A) then lora_down(B)
pair_keys = []
if 'lora_down' in key:
    pair_keys.append(key.replace('lora_down', 'lora_up'))
    pair_keys.append(key)
else:
    pair_keys.append(key)
    pair_keys.append(key.replace('lora_up', 'lora_down'))</pre>
			<p>Then, we <a id="_idIndexMarker269"/>update the weights:</p>
			<pre class="source-code">
alpha = 0.5
# update weight
if len(state_dict[pair_keys[0]].shape) == 4:
    # squeeze(3) and squeeze(2) remove dimensions of size 1 
    #from the tensor to make the tensor more compact
    weight_up = state_dict[pair_keys[0]].squeeze(3).squeeze(2).\
        to(torch.float32)
    weight_down = state_dict[pair_keys[1]].squeeze(3).squeeze(2).\
        to(torch.float32)
    curr_layer.weight.data += alpha * torch.mm(weight_up, 
        weight_down).unsqueeze(2).unsqueeze(3)
else:
    weight_up = state_dict[pair_keys[0]].to(torch.float32)
    weight_down = state_dict[pair_keys[1]].to(torch.float32)
    curr_layer.weight.data += alpha * torch.mm(weight_up, weight_down)</pre>
			<p>The <code>alpha * torch.mm(weight_up, weight_down)</code> code is the core code used to <a id="_idIndexMarker270"/>implement αA B T.</p>
			<p>And that’s it! Now, the pipeline’s text encoder and <code>unet</code> model weights are updated by LoRA. Next, let’s put all the parts together to create a full-featured function that can load a LoRA model into the Stab<a id="_idTextAnchor170"/>le Diffusion pipeline.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor171"/>Making a function to load LoRA</h1>
			<p>Let’s add one<a id="_idIndexMarker271"/> more list to store keys that have been visited and put all the preceding code together into a function named <code>load_lora</code>:</p>
			<pre class="source-code">
def load_lora(
    pipeline,
    lora_path,
    lora_weight = 0.5,
    device = 'cpu'
):
    state_dict = load_file(lora_path, device=device)
    LORA_PREFIX_UNET = 'lora_unet'
    LORA_PREFIX_TEXT_ENCODER = 'lora_te'
    alpha = lora_weight
    visited = []
    # directly update weight in diffusers model
    for key in state_dict:
        # as we have set the alpha beforehand, so just skip
        if '.alpha' in key or key in visited:
            continue
        if 'text' in key:
            layer_infos = key.split('.')[0].split(
                LORA_PREFIX_TEXT_ENCODER+'_')[-1].split('_')
            curr_layer = pipeline.text_encoder
        else:
            layer_infos = key.split('.')[0].split(
                LORA_PREFIX_UNET+'_')[-1].split('_')
            curr_layer = pipeline.unet
        # find the target layer
        # loop through the layers to find the target layer
        temp_name = layer_infos.pop(0)
        while len(layer_infos) &gt; -1:
            try:
                curr_layer = curr_layer.__getattr__(temp_name)
                # no exception means the layer is found
                if len(layer_infos) &gt; 0:
                    temp_name = layer_infos.pop(0)
                # layer found but length is 0,
                # break the loop and curr_layer keep point to the 
                # current layer
                elif len(layer_infos) == 0:
                    break
            except Exception:
                # no such layer exist, pop next name and try again
                if len(temp_name) &gt; 0:
                    temp_name += '_'+layer_infos.pop(0)
                else:
                    # temp_name is empty
                    temp_name = layer_infos.pop(0)
        # org_forward(x) + lora_up(lora_down(x)) * multiplier
        # ensure the sequence of lora_up(A) then lora_down(B)
        pair_keys = []
        if 'lora_down' in key:
            pair_keys.append(key.replace('lora_down', 'lora_up'))
            pair_keys.append(key)
        else:
            pair_keys.append(key)
            pair_keys.append(key.replace('lora_up', 'lora_down'))
        # update weight
        if len(state_dict[pair_keys[0]].shape) == 4:
            # squeeze(3) and squeeze(2) remove dimensions of size 1 
            # from the tensor to make the tensor more compact
            weight_up = state_dict[pair_keys[0]].squeeze(3).\
                squeeze(2).to(torch.float32)
            weight_down = state_dict[pair_keys[1]].squeeze(3).\
                squeeze(2).to(torch.float32)
            curr_layer.weight.data += alpha * torch.mm(weight_up, 
                weight_down).unsqueeze(2).unsqueeze(3)
        else:
            weight_up = state_dict[pair_keys[0]].to(torch.float32)
            weight_down = state_dict[pair_keys[1]].to(torch.float32)
            curr_layer.weight.data += alpha * torch.mm(weight_up, 
                weight_down)
        # update visited list, ensure no duplicated weight is 
        # processed.
        for item in pair_keys:
            visited.append(item)</pre>
			<p>To use the function is easy; simply provide the <code>pipeline</code> object, the LoRA path, <code>lora_path</code>, and the <a id="_idIndexMarker272"/>LoRA weight number, <code>lora_weight</code>, like this:</p>
			<pre class="source-code">
pipeline = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype = torch.bfloat16
).to("cuda:0")
lora_path = r"MoXinV1.safetensors"
load_lora(
    pipeline = pipeline,
    lora_path = lora_path,
    lora_weight = 0.5,
    device = "cuda:0"
)</pre>
			<p>Now, let’s <a id="_idIndexMarker273"/>try it out:</p>
			<pre class="source-code">
prompt = """
shukezouma, shuimobysim ,a branch of flower, traditional chinese ink painting
"""
image = pipeline(
    prompt = prompt,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
display(image)</pre>
			<p>It works, and works well; see the result shown in <em class="italic">Figure 8</em><em class="italic">.5</em>:</p>
			<div><div><img src="img/B21263_08_05.jpg" alt="Figure 8.5: A branch of flower using the custom LoRA loader"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5: A branch of flower using the custom LoRA loader</p>
			<p>You might<a id="_idIndexMarker274"/> be wondering, “Why does a small LoRA file possess such formidable capabilities?” Let’s delve deeper into the reasons why a<a id="_idTextAnchor172"/> LoRA model is effective.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor173"/>Why LoRA works</h1>
			<p>The paper <em class="italic">Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning</em> [8] by Armen et al. found that the pre-trained representations’ intrinsic dimension is way lower than expected, stated by them as follows:</p>
			<p class="author-quote">“We empirically show that common NLP tasks within the context of pre-trained representations have an intrinsic dimension several orders of magnitudes less than the full parameterization.”</p>
			<p>The intrinsic <a id="_idIndexMarker275"/>dimension of a matrix is a concept used to determine the effective number of dimensions required to represent the important information contained within that matrix.</p>
			<p>Let’s suppose we have a matrix, <code>M</code>, with five rows and three columns, like this:</p>
			<pre class="source-code">
M =  1  2  3
     4  5  6
     7  8  9
     10 11 12
     13 14 15</pre>
			<p>Each row of this matrix represents a data point or a vector with three values. We can think of these vectors as points in a three-dimensional space. However, if we visualize these points, we might find that they lie approximately on a two-dimensional plane, rather than occupying the full three-dimensional space.</p>
			<p>In this case, the intrinsic dimension of the matrix, <code>M</code>, would be <code>2</code>, indicating that the essential structure of the data can be captured effectively using two dimensions. The third dimension doesn’t provide much additional information.</p>
			<p>A low intrinsic dimension matrix can be represented by two low-rank matrices because the data in the matrix can be compressed into a few key features. These features can then be represented by two smaller matrices, each of which has a rank that is equal to the intrinsic dimension of the original matrix.</p>
			<p>The paper <em class="italic">LoRA: Low-Rank Adaptation of Large Language Models</em> [1] by Edward J. Hu et al goes a step further, introducing the concept of LoRA to leverage the low intrinsic dimension nature, boosting the fine-tuning process by breaking down the delta weights to two low-rank parts, ΔW = A B T.</p>
			<p>The effectiveness of <a id="_idIndexMarker276"/>LoRA was soon discovered to extend beyond LLM models, also yielding good results with diffusion models. Simo Ryu published the LoRA [2] code and was the first one to try out LoRA training for Stable Diffusion. That was in July 2023 and there are now more than 40,000 LoRA models shared <a id="_idTextAnchor174"/>at <a href="https://www.civitai.com">https://www.civitai.com</a>.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor175"/>Summary</h1>
			<p>In this chapter, we discussed how to enhance the Stable Diffusion model using LoRA, understood what LoRA is, and why it is good for fine-tuning and inference.</p>
			<p>Then, we began loading LoRA using the experimental functions from the <code>Diffusers</code> package and provided LoRA weights through a custom implementation. We used simple code to quickly understand what LoRA can bring to the table.</p>
			<p>Then, we dived into the internal structure of a LoRA model, walked through the detailed steps to extract LoRA weights, and understood how to merge those weights into the checkpoint model.</p>
			<p>Further, we implemented a function in Python that can load a LoRA safetensors file and perform weight merges.</p>
			<p>Finally, we briefly explored why LoRA works, based on the most recent papers from researchers.</p>
			<p>In the next chapter, we are going to explore another powerful technique – textual inversion – to teach a model new “words,” and then use the pre-trained “words” to add new concep<a id="_idTextAnchor176"/>ts to the generated images.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor177"/>References</h1>
			<ol>
				<li>Edward J. et al, LoRA: Low-Rank Adaptation of Large Language Models: <a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a><a href="https://arxiv.org/abs/2106.09685&#13;"/></li>
				<li>Simo Ryu (cloneofsimo), <code>lora</code>: <a href="https://github.com/cloneofsimo/lora">https://github.com/cloneofsimo/lora</a><a href="https://github.com/cloneofsimo/lora&#13;"/></li>
				<li><code>kohya_lora_loader</code>: <a href="https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44">https://gist.github.com/takuma104/e38d683d72b1e448b8d9b3835f7cfa44</a></li>
				<li>CIVITAI: <a href="https://www.civitai.com">https://www.civitai.com</a></li>
				<li>Rinon Gal et al, An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion: <a href="https://textual-inversion.github.io/">https://textual-inversion.github.io/</a> </li>
				<li>Diffusers’ <code>lora_state_dict</code> function: <a href="https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py">https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/modeling_utils.py</a></li>
				<li>Andrew Zhu, Improving Diffusers Package for High-Quality Image Generation: <a href="https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4">https://towardsdatascience.com/improving-diffusers-package-for-high-quality-image-generation-a50fff04bdd4</a></li>
				<li>Armen et al, Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning: <a href="https://arxiv.org/abs/2012.13255">https://arxiv.org/abs/2012.13255</a></li>
				<li>Hugging Face, LoRA: <a href="https://huggingface.co/docs/diffusers/training/lora">https://huggingface.co/docs/diffusers/training/lora</a></li>
				<li>Hugging Face, PEFT: <a href="https://huggingface.co/docs/peft/en/index">https://huggingface.co/docs/peft/en/index</a></li>
			</ol>
		</div>
	</body></html>