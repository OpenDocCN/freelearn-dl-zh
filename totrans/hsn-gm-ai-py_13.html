<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Exploiting ML-Agents</h1>
                </header>
            
            <article>
                
<p class="mce-root">At some point, we need to move beyond building and training agent algorithms and explore building our own environments. Building your own environments will also give you more experience in making good reward functions. We have virtually omitted this important question in <strong>Reinforcement Learning</strong> (<strong>RL</strong>) and <strong>Deep Reinforcement Learning</strong> (<strong>DRL</strong>) and that is what makes a good reward function.  </p>
<p>In this chapter, we will look to answer the question of what makes a good reward function or what a reward function is. We will talk about reward functions by building new environments with the Unity game engine. We will start by installing and setting up Unity ML-Agents, an advanced DRL kit for building agents and environments. From there, we <span>will</span> look at how to build one of the standard Unity demo environments for our use with our PyTorch models. Conveniently, this leads us to working with the ML-Agents toolkit for using a Unity environment from Python and PyTorch with our previously explored Rainbow DQN model. After that, we <span>will </span>look at creating a new environment, and then finish this chapter by looking at advances Unity has developed for furthering RL.</p>
<p>Here are the main topics we will cover in this chapter:</p>
<ul>
<li>Installing ML-Agents</li>
<li>Building a Unity environment</li>
<li>Training a Unity environment with Rainbow</li>
<li>Creating a new environment</li>
<li>Advancing RL with ML-Agents</li>
</ul>
<p>Unity is the largest and most frequently used game engine for game development. You likely already know this if you are a game developer. The game engine itself is developed with C++ but it provides a scripting interface in C# that 99% of its game developers use. As such, we will need to expose you to some C# code in this chapter, but just a tiny amount.  </p>
<p>In the next section, we'll install Unity and the ML-Agents toolkit.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing ML-Agents</h1>
                </header>
            
            <article>
                
<p>Installing Unity, the game engine itself, is not very difficult, but when working with ML-Agents, you need to be careful when you pick your version. As such, the next exercise is intended to be more configurable, meaning you may need to ask/answer questions while performing the exercise. We did this to make this exercise longer lasting since this toolkit has been known to change frequently with many breaking changes.</p>
<p>Unity will run on any major desktop computer (Windows, Mac, or Linux), so open your development computer and follow along with the next exercise to install Unity and the ML-Agents toolkit:</p>
<ol>
<li>Before installing Unity, check the ML-Agents GitHub installation page (<a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md">https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md</a>) and confirm which version of Unity is currently supported. At the time of writing, this is 2017.4, and we will prefer to use only that version even though the <span>documentation </span>suggests later versions are supported.</li>
</ol>
<div class="packt_tip">You can download and install Unity directly or through the Unity Hub. Since managing multiple versions of Unity is so common, Unity built a management app, the Unity Hub, for this purpose.</div>
<ol start="2">
<li>Download and install the required minimum version of Unity. If you have never installed Unity, you will need to create a user account and verify their license agreement. After you create a user account, you will be able to run the Unity editor.</li>
<li>Open a Python/Anaconda command shell and make sure to activate your virtual environment with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>conda activate gameAI</strong><br/>--- or ---<br/><strong>activate gameAI</strong></pre>
<ol start="4">
<li>Install the Unity Gym wrapper with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install gym_unity    </strong></pre>
<ol start="5">
<li>Change to a root working folder, preferably <kbd>C:</kbd> or <kbd>/</kbd>, and create a directory for cloning the ML-Agents toolkit into with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd /</strong><br/><strong>mkdir mlagents</strong><br/><strong>cd mlagents</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="6">
<li>Then, assuming you have <kbd>git</kbd> installed, use <kbd>git</kbd> to pull down the ML-Agents toolkit with this:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/Unity-Technologies/ml-agents.git</strong></pre>
<div class="packt_tip">The reason we prefer a root folder is that the ML-Agents directory structure can get quite deep and this may cause too long filename errors on some operating systems.</div>
<ol start="7">
<li>Testing the entire installation is best done by consulting the current Unity docs and using their most recent guide. A good place to start is the first example environment, the 3D Balance Ball. You can find this document at <a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md">https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Getting-Started-with-Balance-Ball.md</a>.</li>
</ol>
<p>Take some time and explore the ML-Agents toolkit on your own. It is meant to be quite accessible and if your only experience in DRL is this book, you should have plenty of background by now to understand the general gist of running Unity environments. We will review some of these procedures but there are plenty of other helpful guides out there that can help you run ML-Agents in Unity. Our priority here will be using Unity to build environments and possibly new environments we can use to test our models on. While we won't use the ML-Agents toolkit to train agents, we will use the Gym wrappers, which do require knowledge of what a brain or academy is.  </p>
<div class="packt_infobox">Adam Kelly has an excellent blog, Immersive limit (<a href="http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents">http://www.immersivelimit.com/tutorials/tag/Unity+ML+Agents</a>), devoted to machine learning and DRL with a specialization of creating very cool ML-Agents environments and projects.  </div>
<p>ML-Agents currently uses PPO and Soft Actor-Critic methods to train agents. It also provides several helpful modules for state encoding using convolutional and recurrent networks, hence allowing for visual observation encoding and memory or context. Additionally, it provides methods for defining discrete or continuous action spaces, as well as enabling mixing action or observation types. The toolkit is extremely well done but, with the rapidly changing landscape of ML, it has become quickly outdated and/or perhaps just out-hyped. In the end, it also appears that most researchers or serious practitioners of DRL just want to build their own frameworkll for now. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_tip">While DRL is quite complicated, the amount of code to make something powerful is still quite small. Therefore, we will likely see a plethora of RL frameworks trying to gain a foothold on the space. Whether you decide to use one of these frameworks or build your own is up to you. Just remember that frameworks come and go, but the more underlying knowledge you have on a topic, the better your ability to guide future decisions.</div>
<p>Regardless of whether you decide to use the ML-Agents framework for training DRL agents, use another framework, or build your own, Unity provides you with an excellent opportunity to build new and more exciting environments. We learn how to build a Unity environment we can train with DRL in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Unity environment</h1>
                </header>
            
            <article>
                
<p>The ML-Agents toolkit provides not only a DRL training framework but also a mechanism to quickly and easily set up AI agents within a Unity game. Those agents can then be externally controlled through a Gym interface—yes, that same interface we used to train most of our previous agent/algorithms. One of the truly great things about this platform is that Unity provides several new demo environments that we can explore. Later, we will look at how to build our own environments for training agents.</p>
<div class="packt_tip">The exercises in this section are meant to summarize the setup steps required to build an executable environment to train with Python. They are intended for newcomers to Unity who don't want to learn all about Unity to just build a training environment. If you encounter issues using these exercises, it is likely the SDK may have changed. If that is the case, then just revert back and consult the full online documentation.</div>
<p>Building a Unity environment for agent training requires a few specialized steps we will cover in this exercise:</p>
<ol>
<li>First, open the Unity editor, either through the Unity Hub or just Unity itself. Remember to use a version that supports the ML-Agents toolkit.</li>
<li>Using the Unity Hub, we can add the project using the <strong>Add</strong> button, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-649 image-border" src="assets/d317976a-e439-4a1f-b61f-a33e2ea27a23.png" style="width:89.75em;height:58.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Adding a project in the Unity Hub</span></div>
<ol start="3">
<li>After you click <strong>Add</strong>, you will be prompted to locate the project folder. Use the dialog to find and select the <kbd>UnitySDK</kbd> project folder we pulled down with <kbd>git</kbd> in the previous exercise. This folder should be located in your <kbd>/mlagents/ml-agents/UnitySDK</kbd> folder.</li>
<li>When the project has been added, it will also be added to the top of the list of projects. You will likely see a warning icon indicating you need to select the version number. Select a version of Unity that coincides with the ML-Agents toolkit and then select the project to launch it in the editor.</li>
<li>You may be prompted to <strong>Upgrade</strong> the project. If you are prompted, then select <strong>Yes</strong> to do so. If the upgrade fails or the project won't run right, then you can just delete all of the old files and try again with a different version of Unity. Loading this project may take some time, so be patient, grab a drink, and wait.</li>
</ol>
<ol start="6">
<li>After the project finishes loading and the editor opens, open the scene for the <kbd>3DBall</kbd> environment located in the <kbd>Assets/ML-Agents/Examples/Hallway/Scenes</kbd> folder by double-clicking on the <kbd>3DBall</kbd> scene file.</li>
<li>We need to set the Academy to control the brain, that is, allow the brain to be trained. To do that, select the <strong>Academy</strong>, then locate the <strong>Hallway Academy</strong> component in the <strong>Inspector</strong> window, and select the <strong>Control</strong> option, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-871 image-border" src="assets/d5f6d096-ee5f-4a4a-ae6e-bd4c2e820467.png" style="width:156.25em;height:75.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Setting the academy to control the brain</div>
<ol start="8">
<li>Next, we need to modify the run parameters for the environment. The idea here is that we will build the Unity environment as an executable game that we can then use the wrappers on to train an agent to play. However, to do that, we need to make some assumptions about the game:<br/>
<ul>
<li>The game is windowless and runs in the background.</li>
<li>Any player actions need to be controlled by the agent. A dialog prompts for warnings, errors, or anything else that must be avoided.</li>
<li>Make sure that the training scene is loaded first.  </li>
</ul>
</li>
<li>Before finishing that though, turn the <strong>Control</strong> option back off or on for the academy and run the scene by pressing the <strong>Play</strong> button at the top of the interface. You will be able to observe an already trained agent play through the scene. Make sure to turn the <strong>Control</strong> option back on when you are done viewing the agent play.</li>
</ol>
<p>Now, the ML-Agents toolkit will allow you to train directly from here by just running a separate Python command shell and script controlling the editor. As of yet, this is not possible and our only way to run an environment is with wrappers. In the next section, we will finish setting up the environment by setting some final parameters and building them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building for Gym wrappers</h1>
                </header>
            
            <article>
                
<p>Configuring the setup of an environment just requires setting a few additional parameters. We will learn how to do this in the following exercise:</p>
<ol>
<li>From the editor menu, select <strong>Edit | Project Settings... </strong>to open the <strong>Project Settings</strong> window. You can anchor this window or close it after you've finished editing.</li>
<li>Select the <strong>Player</strong> option. Player, in this case, denotes the player or game runner—not to be confused with an actual human player. Change the text in the <strong>Company Name</strong> and <strong>Product Name</strong> fields to <kbd>GameAI</kbd>.</li>
</ol>
<ol start="3">
<li>Open the <strong>Resolution and Presentation</strong> section and then be sure that <strong>Run In Background*</strong> is checked and <strong>Display Resolution Dialog</strong> is set to <strong>Disabled</strong>, as shown in the following screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-651 image-border" src="assets/61c81f5c-cebf-46de-82d9-f7e36ab8e030.png" style="width:39.33em;height:44.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Setting the Player settings</div>
<ol start="4">
<li>Close the dialog or anchor it and then, from the menu, select <strong>File | Build Settings</strong>.</li>
<li>Click the <strong>Add Open Scene</strong> button and be sure to select your default platform for training. This should be a desktop environment you can easily run with Python. The following screenshot shows the Windows option by default:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-652 image-border" src="assets/8ebf8c6e-436c-42ef-8df8-901d5cdd67c5.png" style="width:39.42em;height:38.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Building the scene into a game environment</div>
<ol start="6">
<li>Click the <span class="packt_screen">Build</span> button at the bottom of the dialog to build the executable environment.</li>
</ol>
<ol start="7">
<li>You will be prompted to save the output to a folder. Be sure to note the location of this folder and/or save it someplace accessible. A good suggested location is the <kbd>mlagents</kbd> root folder in <kbd>/mlagents</kbd>. Create a new folder called <kbd>desktop</kbd><strong> </strong>and save the output there.</li>
</ol>
<p>The environment should be built and runnable now as a Gym environment. We will set up this environment and start training it as an environment in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a Unity environment with Rainbow</h1>
                </header>
            
            <article>
                
<p><span>Training an agent to learn a Unity environment is not unlike much of the training we have already done. There are a few slight changes to the way we interact and set up the environment but overall it is much the same, which makes it a further plus for us because now we can go back and train several different agents/algorithms on completely new environments that we can even design. Furthermore, we now can use other DRL frameworks to train agents with Python— outside the ML-Agents agents, that is. We will cover more on using other frameworks in <a href="6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml">Chapter 12</a>, <em>DRL Frameworks</em>.</span></p>
<p>In the next exercise, we see how to convert one of our latest and most state-of-the-art samples, <kbd>Chapter_10_Rainbow.py</kbd>, and turn it into <kbd>Chapter_11_Unity_Rainbow.py</kbd>. Open <kbd>Chapter_11_Unity_Rainbow.py</kbd> and follow the next exercise:</p>
<ol>
<li>We first need to copy the output folder from the last build, the desktop folder, and place it in the same folder as this chapter's source code. This will allow us to launch that build as the environment our agent will train on.  </li>
<li>Since you will likely want to convert a few of our previous samples to run Unity environments, we will go through the required changes step by step, starting first with the new import, as follows:</li>
</ol>
<pre style="padding-left: 60px">from unityagents import UnityEnvironment</pre>
<ol start="3">
<li>This imports the <kbd>UnityEnviroment</kbd> class, which is a Gym adapter to Unity. We next use this class to instantiate the <kbd>env</kbd> environment, like so; note that we have placed commented lines for other operating systems:</li>
</ol>
<pre style="padding-left: 60px">env = UnityEnvironment(file_name="desktop/gameAI.exe") # Windows<br/>#env = UnityEnvironment(file_name="desktop/gameAI.app") # Mac<br/>#env = UnityEnvironment(file_name="desktop/gameAI.x86") # Linux x86<br/>#env = UnityEnvironment(file_name="desktop/gameAI.x86_64") # Linux x86_64</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="4">
<li>Next, we get <kbd>brain</kbd> and <kbd>brain_name</kbd> from the environment. Unity uses the concept of a brain to control agents. We will explore agent brains in a later section. For now, realize that we just take the first available brain with the following code:</li>
</ol>
<pre style="padding-left: 60px">brain_name = env.brain_names[0]<br/>brain = env.brains[brain_name]</pre>
<ol start="5">
<li>Then, we extract the action (<kbd>action_size</kbd>) and state size (<kbd>state_size</kbd>) from the brain and use these as inputs to construct our <kbd>RainbowDQN</kbd> models, like so:</li>
</ol>
<pre style="padding-left: 60px">action_size = brain.vector_action_space_size<br/>state_size = brain.vector_observation_space_size<br/><br/>current_model = RainbowDQN(state_size, action_size, num_atoms, Vmin, Vmax)<br/>target_model = RainbowDQN(state_size, action_size, num_atoms, Vmin, Vmax)</pre>
<ol start="6">
<li>The last part we need to worry about is down in the training code and has to do with how the environment is reset. Unity allows for multiple agents/brains to run in concurrent environments concurrently, either as a way mechanism for A2C/A3C or other mechanisms. As such, it requires a bit more care as to which specific brain and mode we want to reset. The following code shows how we reset the environment when training Unity:</li>
</ol>
<pre style="padding-left: 60px">env_info = env.reset(train_mode=True)[brain_name]<br/>state = env_info.vector_observations[0]</pre>
<ol start="7">
<li>As mentioned, the purpose of the slightly confusing indexing has to do with which brain/agent you want to pull the state from. Unity may have multiple brains training multiple agents in multiple sub-environments, all either working together or against each other. We will cover more about training multiple agent environments in <a href="a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml">Chapter 14</a>, <em><span>From DRL to AGI</span></em>.</li>
<li>We also have to change any other occurrences of when the environment may reset itself like in the following example when the algorithm checks whether the episode is done, with the following code:</li>
</ol>
<pre style="padding-left: 60px">if done:<br/>        #state = env.reset()<br/>        env_info = env.reset(train_mode=True)[brain_name] <br/>        state = env_info.vector_observations[0]<br/>        all_rewards.append(episode_reward)<br/>        episode_reward = 0</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="9">
<li>Run the code and watch the agent train. You won't see any visuals other than TensorBoard output, assuming you go through the steps and run TB in another shell, which you can likely do on your own by now.</li>
</ol>
<div class="packt_infobox">This example may be problematic to run due to API compatibility issues. If you encounter problems when running the sample, then try and set up a whole new virtual environment and install everything again. If the issue continues, then check online for help in places such as Stack Overflow or GitHub. Be sure to also refer to the latest Unity documentation on ML-Agents.</div>
<p>The real benefit of plugging in and using Unity is the ability to construct your own environments and then use those new environments with your own or another RL framework. In the next section, we will look at the basics of building your own RL environment with Unity ML-Agents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new environment</h1>
                </header>
            
            <article>
                
<p>The great thing about the ML-Agents toolkit is the ability it provides for creating new agent environments quickly and simply. You can even transform existing games or game projects into training environments for a range of purposes, from building full robotic simulations to simple game agents or even game agents that play as non-player characters. There is even potential to use DRL agents for game quality assurance testing. Imagine building an army of game testers that learn to play your game with just trial and error. The possibilities are endless and Unity is even building a full cloud-based simulation environment for running or training these agents in the future.</p>
<p>In this section, we will walk through using a game project as a new training environment. Any environment you create in Unity would be best tested with the ML-Agents toolkit before you set up your own Python code. DRL agents are masters at finding bugs and/or cheats. As such, you will almost always want to test the environment first with ML-Agents before training it with your own code. I already recommended that you go through the process of setting up and running the ML-Agents Python code to train agents. Remember that once you export an environment for Gym training, it becomes windowless and you will not have any knowledge of how well the agent trains or performs in the environment. If there are any cheats or bugs to be found, the agent will most assuredly find them. After all, your agent will attempt millions of different trial and error combinations trying to find how to play the game.</p>
<p class="mce-root"/>
<p class="mce-root">We are going to look at the <span class="packt_screen">Basic</span> ML-Agents environment as a way of understanding how to build our own extended or new environment. The ML-Agents documentation is an excellent source to fall back on if the information here is lacking. This exercise is intended to get you up to speed building your own environments quickly:</p>
<ol>
<li>Open up the <strong>Unity Editor</strong> to the <strong>UnitySDK</strong> ML-Agents project we previously had open. Locate and open (double-click) the <strong>Basic</strong> scene at <kbd>Assets/ML-Agents/Examples/Basic/Scenes</kbd>.</li>
<li>At the center of any environment is the Academy. Locate and select the <strong>Academy</strong> object in the <strong>Hierarchy</strong> window and then view the properties in the <strong>Inspector</strong> window, as shown in the screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-653 image-border" src="assets/8bfa360d-b17b-416c-9a01-b67c806967ba.png" style="width:84.50em;height:64.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inspecting the Academy </div>
<ol start="3">
<li>Click on and select the <strong>BasicLearning (LearningBrain)</strong> brain in the <strong>Basic Academy | Broadcast Hub | Brains</strong> entry. This will highlight the entry in the <strong>Project</strong> window. Select the <strong>BasicLearning</strong> brain in the <strong>Project</strong> window and view the brain setup in the <strong>Inspector</strong> window, as shown in the screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-654 image-border" src="assets/db8d17df-1a65-491d-adf8-4c14fb9f2e8b.png" style="width:86.92em;height:63.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inspecting the Learning Brain</div>
<ol start="4">
<li>We can see a few things about the brain here. A brain controls an agent so the brain's observation and action space effectively become the same as the agent's. In the <strong>Inspector</strong> window, you can see there are 20 vector observations and an action space of three discrete actions. For this environment, the actions are left or right and null. The 0 action becomes a null or pause action.</li>
</ol>
<ol start="5">
<li>Next, we want to inspect the agent itself. Click on and expand the <strong>Basic</strong> object in the <strong>Hierarchy</strong> window. Select the <strong>BasicAgent</strong> object and then review the <strong>Inspector</strong> window, as the screenshot shows:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-655 image-border" src="assets/6eefe3a4-eb8a-4652-bf53-e35f3c2bb995.png" style="width:90.83em;height:49.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inspecting the Basic Agent</div>
<ol start="6">
<li>Inspect the Basic Agent component and you can see the <strong>Brain</strong> is set to the <strong>BasicLearning</strong> brain and there are other properties displayed here. Note how the <span class="packt_screen">Reset On Done</span> and <span class="packt_screen">On Demand Decisions</span> are both checked. <strong>Reset On Done</strong> enables the environment to reset itself when an episode is complete—what you would expect is the default behavior but is not. <strong>On Demand Decisions</strong> equate to using on- versus off-policy models and is more relevant when training with ML-Agents toolkit.</li>
<li>Pressing <span class="packt_screen">Play</span> will show you the agent playing the game. Watch how the agent plays and while the agent moves around, be sure to select and inspect objects in the editor. Unity is great for seeing how your game mechanics work and this comes in especially handy when building your own agent environments.</li>
</ol>
<p class="mce-root"/>
<p>The academy, brain, and agent are the main elements you will need to consider when building any new environment. As long as you follow this basic example, you should be able to construct a simple working environment quickly. The other tricky part of building your own environment is the special coding you may have to do and we will cover that in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding an agent/environment</h1>
                </header>
            
            <article>
                
<p>Unity provides an excellent interface for prototyping and building commercial games. You can actually get quite far with very little coding. Unfortunately, that is currently not the case when building new ML-Agents environments. </p>
<p>As such, we will explore the important coding parts in the next exercise:</p>
<ol>
<li>Next, locate and open the <strong>Scripts</strong> folder under <kbd>Assets/ML-Agents/Examples/Basic</kbd> and inside that double-click to open <kbd>BasicAgent.cs</kbd>. This is a C# (CSharp) file and it will open in the default editor.</li>
<li>At the top of the file, you will note that this <kbd>BasicAgent</kbd> class is extended from <kbd>Agent</kbd> and not <kbd>MonoBehaviour</kbd>, which is the Unity default. <kbd>Agent</kbd> is a special class in Unity, which as you likely guessed, defines an agent that is capable of exploring the environment. However, in this case, agent refers more to a worker as in a worker in asynchronous or synchronous actor-critic. This means a single brain may control multiple agents, which is often the case:</li>
</ol>
<pre style="padding-left: 60px">using UnityEngine;<br/>using MLAgents;<br/><br/>public class BasicAgent : Agent</pre>
<ol start="3">
<li>Skipping down past the fields, we will jump to the method definitions starting with <kbd>CollectObservations</kbd>, shown here:</li>
</ol>
<pre style="padding-left: 60px">public override void CollectObservations()<br/>{<br/>  AddVectorObs(m_Position, 20);<br/>}</pre>
<ol start="4">
<li>Inside this method, we can see how the agent/brain collects observations from the environment. In this case, the observation is added using <kbd>AddVectorObs</kbd>, which adds the observation as a one-hot encoded vector of the required size. In this case, the vector size is 20, the same as the brain's state size.</li>
</ol>
<div class="packt_infobox">One-hot encoding is a method by which we encode can encode class information in terms of binary values inside a vector. Hence, if a one-hot encoded vector denoting class or position 1 was active, it would be written as [0,1,0,0].</div>
<ol start="5">
<li>The main action method is the <kbd>AgentAction</kbd> method. This where the agent performs actions in the environment, be these actions moving or something else:</li>
</ol>
<pre style="padding-left: 60px"> public override void AgentAction(float[] vectorAction, string textAction)<br/> {<br/>  var movement = (int)vectorAction[0];<br/>  var direction = 0;<br/>  switch (movement)<br/>  {<br/>    case 1:<br/>      direction = -1;<br/>      break;<br/>    case 2:<br/>      direction = 1;<br/>      break;<br/>  }<br/><br/>  m_Position += direction;<br/>  if (m_Position &lt; m_MinPosition) { m_Position = m_MinPosition; }<br/>  if (m_Position &gt; m_MaxPosition) { m_Position = m_MaxPosition; }<br/><br/>  gameObject.transform.position = new Vector3(m_Position - 10f, 0f, 0f);<br/><br/>  AddReward(-0.01f);<br/><br/>  if (m_Position == m_SmallGoalPosition)<br/>  {<br/>    Done();<br/>    AddReward(0.1f);<br/>  }<br/><br/>  if (m_Position == m_LargeGoalPosition)<br/>  {<br/>    Done();<br/>    AddReward(1f);<br/>  }<br/>}</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="6">
<li>The first part of this code just determines how the agent moves based on the action it has taken. You can see how the code adjusts the agent's position based on its move. Then, we see the following line of code:</li>
</ol>
<pre style="padding-left: 60px">AddReward(-0.01f);</pre>
<ol start="7">
<li>This line adds a step reward, meaning it always adds this reward every step. It does this as a way of limiting the agent's moves. Hence, the longer the agent takes to make the wrong decisions, the less the reward. We sometimes use a step reward but it can also have negative effects and it often makes sense to eliminate a step reward entirely.</li>
<li>At the bottom of the <kbd>AgentAction</kbd> method, we can see what happens when the agent reaches the small or large goal. If the agent reaches the large goal it gets a reward of 1 and .1 if it makes the small goal. With that, we can also see that, when it reaches a goal, the episode terminates using a call to <kbd>Done()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">Done();<br/>AddReward(0.1f);  //small goal<br/>// or<br/>Done();<br/>AddReward(1f);  //large goal</pre>
<ol start="9">
<li>Reverse the numbers for the rewards, save the code, and return to the editor. Set the <strong>Academy</strong> to <span class="packt_screen">Control the brain</span> and then train the agent with the ML-Agents or the code we developed earlier. You should very clearly see the agent having a preference for the smaller goal.</li>
</ol>
<p>Extending these concepts and building your own environment now will be up to you. The sky <span>really</span><span> </span><span>is the limit and Unity provides several excellent examples to work with and learn from. In the next section, we will take the opportunity to look at the advances ML-Agents provides as mechanisms to enhance your agents or even explore new ways to train.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advancing RL with ML-Agents</h1>
                </header>
            
            <article>
                
<p>The ML-Agents toolkit, the part that allows you to train DRL agents, is considered one of the more serious and top-end frameworks for training agents. Since the framework was developed on top of Unity, it tends to perform better on Unity-like environments. However, not unlike many others who spend time training agents, the Unity developers realized early on that some environments present such difficult challenges as to require us to assist our agents.</p>
<p class="mce-root"/>
<p>Now, this assistance is not so much direct but rather indirect and often directly relates to how easy or difficult it is for an agent to find rewards. This, in turn, directly relates to how well the environment designer can build a reward function that an agent can use to learn an environment. There are also the times when an environment's state space is so large and not obvious that creating a typical reward function is just not possible. With all that in mind, Unity has gone out of its way to enhance the RL inside ML-Agents with the following new forms of learning:</p>
<ul>
<li>Curriculum learning</li>
<li>Behavioral cloning (imitation learning)</li>
<li>Curiosity learning</li>
<li>Training generalized RL agents</li>
</ul>
<p>We will cover each form of learning in a quick example using the Unity environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Curriculum learning</h1>
                </header>
            
            <article>
                
<p>Curriculum learning allows you to train an agent by increasing the complexity of the task as the agent learns. This is fairly intuitive and likely very similar to the way we learn various tasks from math to programming. </p>
<p>Follow the exercise to quickly see how you can set up for curriculum learning:</p>
<ol>
<li>Open the <kbd>WallJump</kbd> scene located in the <kbd>Assets/ML-Agents/Examples/WallJump/Scenes</kbd> folder.</li>
</ol>
<ol start="2">
<li>Select the <strong>Academy</strong> in the scene and review the settings of the <strong>Wall Jump Academy</strong> component in the <strong>Inspector</strong> window and as shown in the screenshot:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-656 image-border" src="assets/3c4c96dc-e061-470a-ac33-738fb0274f84.png" style="width:91.33em;height:68.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Inspecting the WallJump Academy</div>
<ol start="3">
<li>Inside the academy is an expanded section called <strong>Reset Parameters</strong>. These parameters represent training level parameters for various training states we want to put the agent through.</li>
<li>These parameters now need to be configured in a configuration file the ML-Agents toolkit will use to train the agent with curriculum. The contents of this file can be found or created at <span><kbd>config/curricula/wall-jump/</kbd> and consist of the following:</span></li>
</ol>
<pre style="padding-left: 60px">{
    <span class="pl-s"><span class="pl-pds">"</span>measure<span class="pl-pds">"</span></span> : <span class="pl-s"><span class="pl-pds">"</span>progress<span class="pl-pds">"</span></span>,
    <span class="pl-s"><span class="pl-pds">"</span>thresholds<span class="pl-pds">"</span></span> : [<span class="pl-c1">0.1</span>, <span class="pl-c1">0.3</span>, <span class="pl-c1">0.5</span>],
    <span class="pl-s"><span class="pl-pds">"</span>min_lesson_length<span class="pl-pds">"</span></span> : <span class="pl-c1">100</span>,
    <span class="pl-s"><span class="pl-pds">"</span>signal_smoothing<span class="pl-pds">"</span></span> : <span class="pl-c1">true</span>,
    <span class="pl-s"><span class="pl-pds">"</span>parameters<span class="pl-pds">"</span></span> :
    {
        <span class="pl-s"><span class="pl-pds">"</span>big_wall_min_height<span class="pl-pds">"</span></span> : [<span class="pl-c1">0.0</span>, <span class="pl-c1">4.0</span>, <span class="pl-c1">6.0</span>, <span class="pl-c1">8.0</span>],
        <span class="pl-s"><span class="pl-pds">"</span>big_wall_max_height<span class="pl-pds">"</span></span> : [<span class="pl-c1">4.0</span>, <span class="pl-c1">7.0</span>, <span class="pl-c1">8.0</span>, <span class="pl-c1">8.0</span>]
    }
}</pre>
<ol start="5">
<li>Understanding these parameters can be best done by referring back to the ML-Agents docs. Basically, the idea here is that these parameters control the wall height which is increased over time. Hence, the agent needs to learn to move the block over to jump over the wall as it gets harder and harder.</li>
<li>Set the <strong>Control</strong> flag on the <strong>Academy</strong> brains and then run an ML-Agents session in a Python shell with the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --curriculum=config/curricula/wall-jump/ --run-id=wall-jump-curriculum --train</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="7">
<li>Assuming the configuration files are in the correct place, you will be prompted to run the editor and watch the agent train in the environment. The results of this example are shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-657 image-border" src="assets/308194a0-9602-4a54-aae2-4c80548cb9ee.png" style="width:91.08em;height:68.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The output of curriculum training example</div>
<p>Curriculum learning solves the problem of an environment not having an obvious answer in a novel way. In this case, the agent's goal is to make the target square. However, if the wall started out very high and the agent needed to move the block there to jump over it, it likely won't even understand it needs to get to a block. Therefore, we help it to train by first allowing it to get to the goal but then make it gradually harder to do so. As the difficulty increases, the agent learns how to use the block to jump over the wall.</p>
<p>In the next section, we look to another method that helps agents to solve tasks with difficult to find or what we call sparse rewards.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Behavioral cloning</h1>
                </header>
            
            <article>
                
<p>Behavioral cloning is sometimes also referred to as imitation learning. While not exactly both the same, we will use the terms interchangeably here. In RL, we use the term sparse rewards or rewards sparsity for any environment where it is difficult for an agent to just finish a task by trial and error and perhaps luck. The larger an environment is, the more sparse the rewards and in many cases, the observation space can be so large that any hope of training an agent at all is extremely difficult. Fortunately, a method called behavioral cloning or imitation learning can solve the problem of sparse rewards by using the observations of humans as previous sampled observations. Unity provides three methods to generate previous observations and they are as follows:</p>
<ul>
<li><strong>Generative Adversarial Imitation Learning</strong> (<strong>GAIL</strong>): You can use something called the GAIL reward signal to enhance learning rewards from a few observations.</li>
<li><strong>Pretraining</strong>: This allows you to use prerecorded demonstrations likely from a human and use those to bootstrap the learning of the agent. If you use pretraining, you also need to provide a configuration section in your ML-Agents config file like so:</li>
</ul>
<pre style="padding-left: 60px">pretraining:
        demo_path: ./demos/Tennis.demo
        strength: 0.5
        steps: 10000</pre>
<ul>
<li><strong>Behavioral Cloning</strong> (<strong>BC</strong>): In this training, setup happens directly in the Unity editor. This is great for environments where small demonstrations can help to increase an agent's learning. BC does not work so well on larger environments with a large observation state space. </li>
</ul>
<p>These three methods can be combined in a variety of configurations and used together in the case of pretraining and GAIL with other methods such as curiosity learning, which we will see later.</p>
<p>It can be especially entertaining to train an agent in real time with BC, as we'll see in the next exercise. Follow the next exercise to explore using the BC method of demonstrating to an agent:</p>
<ol>
<li>Open the <strong>TennisIL</strong> scene located in the <kbd>Assets/ML-Agents/Examples/Tennis/Scenes</kbd> folder.</li>
<li>This environment is an example of a sparse rewards environment, whereby the agent needs to find and hit the ball back to its opponent. This environment makes for an excellent example to test BC with on.</li>
</ol>
<ol start="3">
<li>Select the <strong>Academy</strong> object in the <strong>Hierarchy</strong> window and then check the <strong>Control</strong> option of <strong>TennisLearning (LearningBrain)</strong> in the <strong>Inspector</strong> window, as shown in the screenshot here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-658 image-border" src="assets/f4b71a0c-c0d9-4e85-bad5-6437fccc68c2.png" style="width:39.75em;height:30.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Turning on the learning brain to control</div>
<ol start="4">
<li>As you can see, there are two brains in this scene: one student brain— the learning brain, and one teacher brain—the player brain. The teacher brain, controlled by the human player, won't control an actual agent but rather just take direct inputs from the player. The student brain observes the teacher's actions and uses those as samples in its policy. In a basic sense, this becomes the teacher working from the human policy that the target policy, the agent, needs to learn. This is really no different than us having current and target networks.</li>
<li>The next thing we have to do is customize the ML-Agents hyperparameters config file. We customize the file by adding the following entry for <kbd>StudentBrain</kbd>:</li>
</ol>
<pre style="padding-left: 60px">StudentBrain:<br/>  <strong>trainer: imitation</strong><br/>  max_steps: 10000<br/>  summary_freq: 1000<br/>  <strong>brain_to_imitate: TeacherBrain</strong><br/>  batch_size: 16<br/>  batches_per_epoch: 5<br/>  num_layers: 4<br/>  hidden_units: 64<br/>  use_recurrent: false<br/>  sequence_length: 16<br/>  buffer_size: 128</pre>
<ol start="6">
<li>The highlighted elements in the preceding configuration show the <kbd>trainer:</kbd> set to <kbd>imitation</kbd> and <kbd>brain_to_imitate:</kbd> as <kbd>TeacherBrain</kbd>. Plenty of more information about setting up the configuration for ML-Agents can be found with the online docs.</li>
<li>Next, you need to open a Python/Anaconda shell and change to the <kbd>mlagents</kbd> folder. After that, run the following command to start training:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --run-id=tennis1 --train</strong></pre>
<ol start="8">
<li>This will start the trainer and in short while you will be prompted to start the Unity editor in <strong>Play</strong> mode.  </li>
</ol>
<ol start="9">
<li>Press <strong>Play</strong> to put the editor in play mode and use the <em>WASD</em> controls to maneuver the paddle to play tennis against the agent. Assuming you do well, the agent will also improve. A screenshot of this training is shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-659 image-border" src="assets/e6f26fd0-b5df-429d-a9cc-5962d065294e.png" style="width:43.50em;height:19.42em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Training the tennis agent with BC</div>
<p>Imitation learning was a key ingredient in training the agent, AlphaStar. AlphaStar was shown to beat human players at a very complex real-time strategy game called <em>StarCraft 2</em>. It has many keen benefits in getting agents past the sparse rewards problem. However, there are many in the RL community that want to avoid IL or BC because it can introduce human bias. Human bias has been shown to decrease agent performance when compared to agents trained entirely without BC. In fact, AlphaStar was trained to a sufficient enough level of playability before it was trained on itself. It was this self-training that is believed to be responsible for the innovation that allowed it to beat human players.</p>
<p>In the next section, we look at another exciting way Unity has tried to capture a method to counter sparse reward problems.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Curiosity learning</h1>
                </header>
            
            <article>
                
<p>Up until now, we have only ever considered external rewards given to the agent from the environment. Yet, we and other animals receive a wide variety of external and internal rewards. Internal rewards are often characterized by emotion or feeling. An agent could have an internal reward that gives it +1 every time it looks to some face, perhaps denoting some internal love or infatuation reward. These types of rewards are called intrinsic rewards and they represent rewards that are internal or self-derived by the agent. This has some powerful capabilities for everything from creating interesting motivated agents to enhancing an agent's learning ability.</p>
<p>It is the second way in which Unity introduced curiosity learning or the internal curiosity reward system as a way of letting agents explore more when they get surprised. That is, whenever an agent is surprised by an action, its curiosity increases and hence it needs to explore the state actions in the space that surprised it.</p>
<p>Unity has produced a very powerful example of curiosity learning in an environment called Pyramids. It is the goal of the agent in this environment to find a pile of yellow blocks with a gold block on top. Knock over the pile of blocks and then get the gold block. The problem is that there are piles of boxes in many rooms at the start but none start yellow. To turn the blocks yellow, that agent needs to find and press a button. Finding this sequence of tasks using straight RL could be problematic and/or time-consuming. Fortunately, with CL, we can improve this performance dramatically. We will look at how to use CL in the next section to train the Pyramids environment:</p>
<ol>
<li>Open the <strong>Pyramids</strong> scene located in the <kbd>Assets/ML-Agents/Examples/Pyramids/Scenes</kbd> folder.</li>
</ol>
<ol start="2">
<li>Press <span class="packt_screen">Play</span> to run the default agent; this will be one trained with Unity. When you run the agent, watch it play through the environment and you will see the agent first find the button, press it, then locate the pile of blocks it needs to knock over. It will knock over the boxes as shown in the sequence of screenshots here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-660 image-border" src="assets/bd8c5703-0567-418a-aa63-667979789af6.png" style="width:15.83em;height:34.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Pyramids agent playing the environment</div>
<ol start="3">
<li>Training an agent with curiosity just requires setting the Academy to control the brain and running the ML-Agents trainer with the proper configuration. This documentation to drive CL has changed several times over the course of ML-Agents development. Therefore, it is recommended you consult the ML-Agents docs for the most recent documentation.</li>
</ol>
<p>CL can be very powerful and the whole concept of intrinsic rewards has some fun and interesting application towards games. Imagine being able to power internal reward systems for enemy agents that may play to greed, power, or some other evil trait. In the next section, we finish out this chapter with a look at training generalized reinforcement learning agents.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training generalized reinforcement learning agents</h1>
                </header>
            
            <article>
                
<p>We often have to remind ourselves that RL is just a derivation of data science best practices and we often have to consider how could you fix a training issue with data science. In the case of RL, we see the same issues we see in data science and machine learning only at different scales and exposed in another manner. One example of this is when an agent is overfitted to an environment that we then try to apply to other general variations of that environment. For instance, imagine the Frozen Lake environment that could be various sizes or even provide random starting points or other variations. By introducing these types of variations, we allow our agent to better generalize to a wider variety of similar environments. It is this generalization that we want to introduce into our environment.</p>
<div class="packt_infobox"><strong>AGI</strong> or <strong>Artificial General Intelligence</strong> is the concept of generalized training agents to the <em>n</em><sup>th</sup> degree. It is expected that a truly AGI agent would be able to be placed in any environment and learn to solve the task. This could take an amount of training but ideally, no other hyperparameters or other human intervention should be required.</div>
<p>By making the environment stochastic, we are essentially increasing the likelihood of our methods that use distributional RL and noisy networks will also become more powerful. Unfortunately, enabling these types of parameters with other training code, or our PyTorch code, is not currently available. In the next exercise, we'll learn how to set up a generalized training environment:</p>
<ol>
<li>Open the <strong>WallJump</strong> scene located in the <kbd>Assets/ML-Agents/Examples/WallJump/Scenes</kbd> folder.</li>
<li><strong>WallJump</strong> is already set up and configured with several reset parameters we looked at earlier when we reviewed curriculum learning. This time, instead of progressively changing those parameters, we are going to have the environment sample them randomly.</li>
</ol>
<ol start="3">
<li>The parameters we want to resample are based on this sample. We can create a new generalized YAML file called <kbd>walljump_generalize.yaml</kbd> in the config folder. </li>
<li>Open and put the following text in this file and then save it:</li>
</ol>
<pre style="padding-left: 60px"><span class="pl-ent">resampling-interval</span>: <span class="pl-c1">5000</span>

<span class="pl-ent">big_wall_min_height</span>:
    <span class="pl-ent">sampler-type</span>: <span class="pl-s"><span class="pl-pds">"</span>uniform<span class="pl-pds">"</span></span>
    <span class="pl-ent">min_value</span>: <span class="pl-c1">5</span>
    <span class="pl-ent">max_value</span>: <span class="pl-c1">8<br/><br/></span><span class="pl-ent">big_wall_max_height</span>:
    <span class="pl-ent">sampler-type</span>: <span class="pl-s"><span class="pl-pds">"</span>uniform<span class="pl-pds">"</span></span>
    <span class="pl-ent">min_value</span>: <span class="pl-c1">8</span>
    <span class="pl-ent">max_value</span>: <span class="pl-c1">10<br/><br/></span><span class="pl-ent">small_wall_height</span>:
    <span class="pl-ent">sampler-type</span>: <span class="pl-s"><span class="pl-pds">"</span>uniform<span class="pl-pds">"</span></span>
    <span class="pl-ent">min_value</span>: <span class="pl-c1">2</span>
    <span class="pl-ent">max_value</span>: <span class="pl-c1">5<br/><br/></span><span class="pl-ent">no_wall_height</span>:
    <span class="pl-ent">sampler-type</span>: <span class="pl-s"><span class="pl-pds">"</span>uniform<span class="pl-pds">"</span></span>
    <span class="pl-ent">min_value</span>: <span class="pl-c1">0</span>
    <span class="pl-ent">max_value</span>: <span class="pl-c1">3</span></pre>
<ol start="5">
<li>This sets up the sampling distributions for how we will sample the values. The values for the environment can then be sampled with the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>SamplerFactory.register_sampler(*custom_sampler_string_key*, *custom_sampler_object*)</span></pre>
<ol start="6">
<li>We can also define new sampler types or ways of sampling data values using a custom sampler that our classes place in the <kbd>sample_class.py</kbd> file in the ML-Agents code. The following is an example of a custom sampler:</li>
</ol>
<pre style="padding-left: 60px"><span class="pl-k">class</span> <span class="pl-en">CustomSampler</span>(<span class="pl-e">Sampler</span>):
    <span class="pl-k">def</span> <span class="pl-c1">__init__</span>(<span class="pl-smi">self</span>, <span class="pl-smi">argA</span>, <span class="pl-smi">argB</span>, <span class="pl-smi">argC</span>):
        <span class="pl-c1">self</span>.possible_vals <span class="pl-k">=</span> [argA, argB, argC]

    <span class="pl-k">def</span> <span class="pl-en">sample_all</span>(<span class="pl-smi">self</span>):
        <span class="pl-k">return</span> np.random.choice(<span class="pl-c1">self</span>.possible_vals)</pre>
<p class="mce-root"/>
<ol start="7">
<li>Then, you can configure the config file to run this sampler like so:</li>
</ol>
<pre style="padding-left: 60px"><span class="pl-ent">height</span>:
    <span class="pl-ent">sampler-type</span>: <span class="pl-s"><span class="pl-pds">"</span>custom-sampler<span class="pl-pds">"</span></span>
    <span class="pl-ent">argB</span>: <span class="pl-c1">1</span>
    <span class="pl-ent">argA</span>: <span class="pl-c1">2</span>
    <span class="pl-ent">argC</span>: <span class="pl-c1">3</span></pre>
<ol start="8">
<li>Remember that you still need to sample the values and modify the environment's configuration when the agent resets. This will require modifying the code to sample the inputs using the appropriate samplers.</li>
<li>You can then run the Unity ML-Agents trainer code with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>mlagents-learn config/trainer_config.yaml --sampler=config/walljump_generalize.yaml
--run-id=walljump-generalization --train</strong></pre>
<p>Being able to train agents in this manner allows your agents to be more robust and able to tackle various incarnations of your environments. If you are building a game that needs a practical agent, you will most likely need to train your agents in a generalized manner. Generalized agents will generally be able to adapt to unforeseen changes in the environment far better than an agent trained otherwise.</p>
<p>That about does it for this chapter and, in the next section, we'll look at gaining further experience with the sample exercises for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>The exercises in this section are intended to introduce you to Unity ML-Agents in more detail. If your preference is not to use ML-Agents as a training framework, then move on to the next section and the end of this chapter. For those of you still here, ML-Agents on its own is a powerful toolkit for quickly exploring DRL agents. The toolkit hides most of the details of DRL but that should not be a problem for you to figure out by now:</p>
<ol>
<li>Set up and run one of the Unity ML-Agents sample environments in the editor to train an agent. This will require that you consult the Unity ML-Agents documentation.</li>
<li>Tune the hyperparameters of a sample Unity environment.</li>
<li>Start TensorBoard and run it so that it collects logs from the Unity runs folder. This will allow you to watch the training performance of the agents being trained with ML-Agents.</li>
</ol>
<ol start="4">
<li>Build a Unity environment and train it with the Rainbow DQN example.</li>
<li>Customize one of the existing Unity environments by changing the setup, parameters, reset parameters, and/or reward function. That is, change the reward feedback the agent receives when completing actions or tasks.</li>
<li>Set up and train an agent with pretrained data. This will require you to set up a player brain to record demonstrations. Play the game to record those demonstrations and then set the game for training with a learning brain.</li>
<li>Train an agent with behavioral cloning using the tennis environment.</li>
<li>Train an agent with curiosity learning using the Pyramids scene.</li>
<li>Set up and run a Unity environment for generalized training. Use the sampling to pull stochastic values from distributions for the environment. What effect do different distributions have on the agent's training performance?</li>
<li>Convert a PG method example such as PPO so that you can run a Unity environment. How does the performance compare with Rainbow DQN?</li>
</ol>
<p>Use these examples to familiarize yourself with Unity ML-Agents and more advanced concepts in RL. In the next section, we will summarize and complete this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a diversion and built our own DRL environments for training with our own code, or another framework, or using the ML-Agents framework from Unity. At first, we looked at the basics of installing the ML-Agents toolkit for the development of environments, training, and training with our own code. Then, we looked at how to build a basic Unity environment for training from a Gym interface like we have been doing throughout this whole book. After that, we learned how our RainbowDQN sample could be customized to train an agent. From there, we looked at how we can create a brand new environment from the basics. We finished this chapter by looking at managing rewards in environments and the set of tools ML-Agents uses to enhance environments with sparse rewards. There, we looked at several methods Unity has added to ML-Agents to assist with difficult environments and sparse rewards.</p>
<p>Moving on from this chapter, we will continue to explore other DRL frameworks that can be used to train agents. ML-Agents is one of many powerful frameworks that can be used to train agents, as we will soon see.</p>


            </article>

            
        </section>
    </body></html>