<html><head></head><body><div><div><div><h1 id="_idParaDest-286" class="chapter-number"><a id="_idTextAnchor355"/>25</h1>
			<h1 id="_idParaDest-287"><a id="_idTextAnchor356"/>Automatic Multi-Step Reasoning and Tool Use</h1>
			<p>Multi-step reasoning and tool use in LLMs involve the model’s ability to break down complex tasks into manageable steps and leverage external resources or APIs to accomplish these tasks. This capability significantly extends the problem-solving potential of LLMs, allowing them to tackle more complex, real-world scenarios. Its key characteristics include the following:</p>
			<ul>
				<li><strong class="bold">Task decomposition</strong>: This refers to the model’s ability to take a complex input or <a id="_idIndexMarker1105"/>goal and divide it into smaller, more manageable sub-tasks that can be solved sequentially or hierarchically. Instead of trying to solve an entire problem in one step, the model creates a structured plan or sequence of reasoning steps that progressively leads to a solution. This process mimics the way humans often approach complex problems by identifying dependencies, sequencing actions, and breaking large goals into intermediate objectives. Techniques such as chain-of-thought prompting explicitly encourage this behavior by prompting the model to articulate each reasoning step before arriving at an answer.</li>
				<li><strong class="bold">External tools</strong>: The capabilities of LLMs can be enhanced by integrating additional <a id="_idIndexMarker1106"/>resources, such as databases, APIs, or specialized services, that LLMs cannot access directly due to limitations in their training environment. These tools enable the LLMs to interact with real-time data, perform specific tasks beyond their built-in knowledge, or offer enhanced functionalities such as web browsing, file handling, or executing external scripts. For example, an LLM can use an external tool to query up-to-date weather data, retrieve specific information from a live API, or run computations that require specialized algorithms. This integration allows LLMs to offer more dynamic, relevant, and specialized responses, particularly for applications requiring real-time information or complex multi-step processes.</li>
				<li><strong class="bold">Reasoning about tool applicability</strong>: This involves the model’s judgment in recognizing <a id="_idIndexMarker1107"/>when an external capability is required to solve a particular sub-task. The model must assess the nature of the sub-task and determine whether internal reasoning suffices or whether delegating part of the task to a tool would yield better or even necessary results.</li>
				<li><strong class="bold">Tool selection and invocation</strong>: This refers to the model’s ability to identify which tool <a id="_idIndexMarker1108"/>is appropriate for a given sub-task and to formulate the correct input to trigger its use. This requires the model to understand the functionality and input requirements of each available tool and to match these against the demands of the current step in the reasoning process. For example, if the task requires accessing up-to-date weather information, the model must choose a weather API and generate a syntactically correct and semantically relevant query to that API. This phase includes formatting inputs, calling the tool, and ensuring that the request aligns with both the current problem context and the tool’s capabilities.</li>
				<li><strong class="bold">Integration of tool outputs</strong>: This describes the model’s capability to interpret <a id="_idIndexMarker1109"/>the results returned by the external tool and incorporate them into the ongoing reasoning process. After a tool is invoked and responds with data—such as a numerical value, a structured object, or a text snippet—the model must parse the result, extract relevant elements, and update its understanding or intermediate outputs accordingly. This step often involves interpreting heterogeneous output formats, managing type mismatches, and maintaining continuity in the reasoning chain. Effective integration ensures that tool use is not isolated but meaningfully contributes to solving the broader task.</li>
				<li><strong class="bold">Iterative problem solving</strong>: This <a id="_idIndexMarker1110"/>refers to the model’s recursive application of the previous stages—decomposition, tool reasoning, selection, invocation, and integration—in a loop until the task is resolved or further steps become unproductive. The model continuously reassesses its progress, determines whether additional sub-tasks remain, and decides whether further tool use is necessary. This iterative behavior <a id="_idIndexMarker1111"/>enables the model to handle tasks with dynamic structure, uncertainty, or errors from prior steps by adjusting the plan or refining previous actions. In agent-based architectures, this process may be explicitly managed by a planner or controller, while in prompt-based settings, it often emerges through recursive self-queries and prompt augmentation.</li>
			</ul>
			<p>In this chapter, we’ll delve into advanced techniques for enabling LLMs to perform complex multi-step reasoning and utilize external tools.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Designing prompts for complex task decomposition</li>
				<li>Integrating external tools</li>
				<li>Implementing automatic tool selection and use</li>
				<li>Complex problem solving</li>
				<li>Evaluating multi-step reasoning and tool use</li>
				<li>Challenges and future directions</li>
			</ul>
			<h1 id="_idParaDest-288"><a id="_idTextAnchor357"/>Designing prompts for complex task decomposition</h1>
			<p>To enable effective multi-step reasoning, prompts should guide the LLM to break down complex <a id="_idIndexMarker1112"/>tasks into smaller, manageable steps. Here’s an example of a task decomposition prompt:</p>
			<pre class="source-code">
def task_decomposition_prompt(task, available_tools):
    prompt = f"""Given the following complex task:
{task}
And the following available tools:
{' '.join(f'- {tool}' for tool in available_tools)}
Please break down the task into smaller, logical steps. For each step, indicate if a specific tool should be used. If no tool is needed, explain the reasoning required.
Your task decomposition:
Step 1:
Step 2:
Step 3:
...
Ensure that the steps are in a logical order and cover all aspects of the task.
"""
    return prompt
# Example usage
task = "Analyze the sentiment of tweets about a new product launch and create a summary report with visualizations."
available_tools = ["Twitter API", "Sentiment Analysis Model",
    "Data Visualization Library"]
prompt = task_decomposition_prompt(task, available_tools)
print(prompt)</pre>			<p>This <a id="_idIndexMarker1113"/>function generates a prompt that guides the LLM to decompose a complex task into steps, considering the available tools.</p>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor358"/>Integrating external tools</h1>
			<p>To enable LLMs to use external tools such as search, calculations, API calls, and so on, we need to <a id="_idIndexMarker1114"/>create an interface between the model and the tools. Here’s a simple implementation:</p>
			<ol>
				<li>Perform the necessary imports and define the <code>ToolKit</code> class:<pre class="source-code">
import requests
from textblob import TextBlob
import matplotlib.pyplot as plt
class ToolKit:
    def __init__(self):
        self.tools = {
            "Twitter API": self.fetch_tweets,
            "Sentiment Analysis": self.analyze_sentiment,
            "Data Visualization": self.create_visualization
        }</pre><p class="list-inset">The preceding code defines a <code>ToolKit</code> class that organizes and offers access to different functionalities through its methods. In the <code>__init__</code> method, a dictionary named <code>tools</code> is initialized with keys representing tool names such as <code>"Twitter API"</code>, <code>"Sentiment Analysis"</code>, and <code>"Data Visualization"</code>, and values that reference the corresponding methods for fetching tweets, performing sentiment analysis using the TextBlob library, and creating data visualizations using Matplotlib. The <code>requests</code> library is imported for making HTTP requests, while <code>TextBlob</code> is used for natural language processing tasks such as sentiment analysis, and <code>matplotlib.pyplot</code> is imported for <a id="_idIndexMarker1115"/>generating visualizations. The code sets up the structure for these tools but is incomplete as the <code>fetch_tweets</code>, <code>analyze_sentiment</code>, and <code>create_visualization</code> methods are not defined, leaving room for further implementation of these functionalities.</p></li>				<li>Define three methods: <code>fetch_tweets</code> for generating mock tweets based on a query, <code>analyze_sentiment</code> for computing sentiment polarity scores for a list of texts using TextBlob, and <code>create_visualization</code> for creating and saving a histogram of the sentiment data with a specified title:<pre class="source-code">
    def fetch_tweets(self, query, count=100):
        return [f"Tweet about {query}" for _ in range(count)]
    def analyze_sentiment(self, texts):
        sentiments = [TextBlob(text).sentiment.polarity
            for text in texts]
        return sentiments
    def create_visualization(self, data, title):
        plt.figure(figsize=(10, 6))
        plt.hist(data, bins=20)
        plt.title(title)
        plt.xlabel("Sentiment")
        plt.ylabel("Frequency")
        plt.savefig("sentiment_visualization.png")
        return "sentiment_visualization.png"</pre></li>				<li>Define the <code>use_tool</code> method to execute a specified tool with given arguments if it exists <a id="_idIndexMarker1116"/>in the tools dictionary; otherwise, return an error message:<pre class="source-code">
    def use_tool(self, tool_name, *args, kwargs):
        if tool_name in self.tools:
            return self.tools[tool_name](*args, kwargs)
        else:
            return f"Error: Tool '{tool_name}' not found."</pre></li>				<li>The following example demonstrates using a <code>ToolKit</code> class to fetch tweets about a product launch, analyze their sentiments, create a sentiment visualization, and print the path to the generated visualization file:<pre class="source-code">
toolkit = ToolKit()
tweets = toolkit.use_tool(
    "Twitter API", "new product launch", count=50
)
sentiments = toolkit.use_tool("Sentiment Analysis", tweets)
visualization = toolkit.use_tool(
    "Data Visualization", sentiments,
    "Sentiment Analysis of Product Launch Tweets"
)
print(f"Generated visualization: {visualization}")</pre></li>			</ol>
			<p>This <code>ToolKit</code> class provides an interface for the LLM to interact with external tools, simulating API calls and data processing tasks.</p>
			<h1 id="_idParaDest-290"><a id="_idTextAnchor359"/>Implementing automatic tool selection and use</h1>
			<p>To enable <a id="_idIndexMarker1117"/>LLMs to automatically select and use tools, we can create a system that interprets the model’s output and executes the appropriate tools. Here’s an example:</p>
			<ol>
				<li>First, we define a function, <code>auto_tool_use</code>, that uses a pre-trained language model and tokenizer from Hugging Face’s Transformers library to decompose a task into executable steps using a prompt, parses the decomposition into steps, executes tools as needed using a toolkit, and collects the results:<pre class="source-code">
from transformers import AutoModelForCausalLM, AutoTokenizer
def auto_tool_use(model, tokenizer, task, toolkit):
    # Generate task decomposition
    decomposition_prompt = task_decomposition_prompt(
        task, toolkit.tools.keys()
    )
    inputs = tokenizer(decomposition_prompt,
        return_tensors="pt")
    outputs = model.generate(
        inputs, max_length=1000, num_return_sequences=1
    )
    decomposition = tokenizer.decode(outputs[0],
        skip_special_tokens=True)
    # Parse decomposition and execute tools
    steps = parse_steps(decomposition)
    results = []
    for step in steps:
        if step['tool']:
            result = toolkit.use_tool(step['tool'],
                *step['args'])
        else:
            result = f"Reasoning: {step['reasoning']}"
        results.append(result)</pre></li>				<li>Then we <a id="_idIndexMarker1118"/>generate a final report. The generated report contains the task description, a breakdown of each step along with its result, and a concluding summary. The model uses the provided steps and results to generate a more cohesive and comprehensive narrative of the task:<pre class="source-code">
    report_prompt = f"Task: {task}\n\nSteps and Results:\n"
    for i, (step, result) in enumerate(zip(steps, results), 1):
        report_prompt += (
            f"Step {i}: {step['description']}\n"
            f"Result: {result}\n\n"
        )
    report_prompt += "Please provide a comprehensive report summarizing the results and insights."
    inputs = tokenizer(report_prompt, return_tensors="pt")
    outputs = model.generate(
        inputs, max_length=1500, num_return_sequences=1
    )
    report = tokenizer.decode(outputs[0],
        skip_special_tokens=True)
    return report</pre></li>				<li>Then, we implement <a id="_idIndexMarker1119"/>logic to parse the decomposition into structured steps. This is a simplified placeholder implementation:<pre class="source-code">
def parse_steps(decomposition):
    steps = []
    for line in decomposition.split('\n'):
        if line.startswith("Step"):
            tool = "Twitter API" if "Twitter" in line else \
                   "Sentiment Analysis" if "sentiment" in line else \
                   "Data Visualization" if "visualization" in line else None
            steps.append({
                'description': line,
                'tool': tool,
                'args': [],
                'reasoning': line if not tool else ""
            })
    return steps</pre></li>				<li>The following example usage demonstrates loading a language model and tokenizer using <code>AutoModelForCausalLM</code> and <code>AutoTokenizer</code>, defining a task to <a id="_idIndexMarker1120"/>analyze tweet sentiments and generate a summary report with visualizations, and using an <code>auto_tool_use</code> function to automate the task via <code>ToolKit</code>, with the final report being printed:<pre class="source-code">
model_name = "llama3.3"  # Replace with your preferred model
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
task = "Analyze the sentiment of tweets about a new product launch and create a summary report with visualizations."
toolkit = ToolKit()
report = auto_tool_use(model, tokenizer, task, toolkit)
print(report)</pre></li>			</ol>
			<p>This code snippet shows at a high level how to enable the LLM to automatically decompose tasks, select appropriate tools, and generate a final report based on the results.</p>
			<p>The first three sections of this chapter laid the groundwork by covering prompt design, integrating external tools, and implementing automatic tool selection to enhance AI functionality. In the following section, we will explore how to design prompts for complex problem solving.</p>
			<h1 id="_idParaDest-291"><a id="_idTextAnchor360"/>Complex problem solving</h1>
			<p>Multi-step <a id="_idIndexMarker1121"/>reasoning and tool use can be applied to various complex problem-solving scenarios. Here’s an example of how to use this approach for market analysis:</p>
			<pre class="source-code">
def market_analysis(model, tokenizer, toolkit, product_name):
    task = f"""Conduct a comprehensive market analysis for the product: {product_name}.
    Include competitor analysis, sentiment analysis of customer reviews, and market trends visualization."""
    analysis_report = auto_tool_use(model, tokenizer, task, toolkit)
    return analysis_report
# Example usage
product_name = "SmartHome AI Assistant"
market_report = market_analysis(model, tokenizer, toolkit,
        product_name)
print(market_report)</pre>			<p>The <code>market_analysis</code> function <a id="_idIndexMarker1122"/>automates the generation of a market research report for a given product by constructing a structured task prompt and passing it to an external utility, <code>auto_tool_use</code>, which is assumed to orchestrate tool-augmented responses from a language model. The prompt requests a multi-part analysis—covering competitors, sentiment analysis of customer feedback, and visualization of market trends—targeted to the specific <code>product_name</code> supplied. This design leverages the model and toolkit to produce a consolidated report without manual intervention, enabling a consistent and repeatable approach to product market research through prompt-driven execution.</p>
			<h1 id="_idParaDest-292"><a id="_idTextAnchor361"/>Evaluating multi-step reasoning and tool use</h1>
			<p>To assess the effectiveness of multi-step reasoning and tool use, we need to evaluate both <a id="_idIndexMarker1123"/>the process and the outcome. Here’s a simple evaluation framework:</p>
			<pre class="source-code">
def evaluate_multistep_tooluse(
    task, generated_report, ground_truth, criteria
):
    scores = {}
    for criterion in criteria:
        scores[criterion] = evaluate_criterion(generated_report,
            ground_truth, criterion)
    # Evaluate tool use effectiveness
    tool_use_score = evaluate_tool_use(task, generated_report)
    scores['Tool Use Effectiveness'] = tool_use_score
    return scores
def evaluate_criterion(generated_report, ground_truth, criterion):
    # Implement criterion-specific evaluation logic
    # This is a placeholder implementation
    return 0.0  # Return a score between 0 and 1
def evaluate_tool_use(task, generated_report):
    # Implement logic to evaluate how effectively tools were used
    # This could involve checking for specific tool outputs or insights
    # This is a placeholder implementation
    return 0.0  # Return a score between 0 and 1
# Example usage
criteria = ['Accuracy', 'Comprehensiveness', 'Insight Quality',
    'Logical Flow']
ground_truth = "Ideal market analysis report content..."  # This would be a benchmark report
evaluation_scores = evaluate_multistep_tooluse(task, market_report,
    ground_truth, criteria)
print("Evaluation Scores:", evaluation_scores)</pre>			<p>This <a id="_idIndexMarker1124"/>evaluation framework assesses both the quality of the generated report and the effectiveness of tool use in the process.</p>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor362"/>Challenges and future directions</h1>
			<p>While <a id="_idIndexMarker1125"/>powerful, multi-step reasoning and tool use in LLMs face several challenges:</p>
			<ul>
				<li><strong class="bold">Tool selection accuracy</strong>: Ensure LLMs choose the most appropriate tools for each task</li>
				<li><strong class="bold">Error propagation</strong>: Mitigate the impact of errors in the early steps of the reasoning process; keep in mind that error propagation across multiple steps can be a major risk in complex tool chains if not mitigated early</li>
				<li><strong class="bold">Scalability</strong>: Manage the complexity of integrating a large number of diverse tools</li>
				<li><strong class="bold">Adaptability</strong>: Enable LLMs to work with new, unseen tools without retraining</li>
			</ul>
			<p>To address <a id="_idIndexMarker1126"/>some of these challenges, consider implementing a self-correction mechanism:</p>
			<pre class="source-code">
def self_correcting_tooluse(
    model, tokenizer, task, toolkit, max_attempts=3
):
    for attempt in range(max_attempts):
        report = auto_tool_use(model, tokenizer, task, toolkit)
        # Prompt the model to evaluate its own work
        evaluation_prompt = f"""Task: {task}
Generated Report:
{report}
Please evaluate the quality and completeness of this report. Identify any errors, omissions, or areas for improvement. If necessary, suggest specific steps to enhance the analysis.
Your evaluation:
"""
        inputs = tokenizer(evaluation_prompt, return_tensors="pt")
        outputs = model.generate(
            inputs, max_length=1000, num_return_sequences=1
        )
        evaluation = tokenizer.decode(outputs[0],
            skip_special_tokens=True)
        if "satisfactory" in evaluation.lower() and "no major issues" in evaluation.lower():
            break
        # If issues were identified, use the evaluation to improve the next attempt
        task += f"\n\nPrevious attempt evaluation: {evaluation}\nPlease address these issues in your next attempt."
    return report
# Example usage
final_report = self_correcting_tooluse(model, tokenizer, task,
    toolkit)
print(final_report)</pre>			<p>Self-correcting <a id="_idIndexMarker1127"/>in this context refers to a method where a language model iteratively refines its output by evaluating and improving <a id="_idIndexMarker1128"/>its own previous responses without external feedback. In the <code>self_correcting_tooluse</code> function, this is implemented by first generating a report using <code>auto_tool_use</code> and then prompting the model to assess the quality of that report. If the model’s self-evaluation does not include indicators of adequacy—such as “satisfactory” and “no major issues”—the evaluation is appended to the task description, effectively guiding the next iteration to address identified shortcomings. This loop continues for a set number of attempts (<code>max_attempts</code>) until the output meets the model’s own acceptance criteria, allowing self-guided refinement across multiple passes.</p>
			<p>We can identify the following three promising research areas for overcoming the challenges from some research conducted by AI/ML communities:</p>
			<ul>
				<li><strong class="bold">Enhanced tool learning and discovery</strong>: Future LLMs will be able to dynamically learn about and integrate new tools without explicit programming. This involves mechanisms for understanding tool documentation and API specifications and even experimenting with tools to infer their functionality. This will allow LLMs to adapt to a constantly evolving landscape of software and services, expanding their capabilities beyond a fixed set of pre-defined tools. This will involve techniques such as meta-learning, reinforcement learning from tool interactions, and semantic understanding of tool descriptions (<a href="https://arxiv.org/abs/2305.17126">https://arxiv.org/abs/2305.17126</a>).</li>
				<li><strong class="bold">Robust and adaptive reasoning with uncertainty</strong>: Future LLMs will incorporate probabilistic models to handle uncertainty in multi-step tasks. This means <a id="_idIndexMarker1129"/>assigning probabilities to different reasoning paths, outcomes, and tool effectiveness. Bayesian methods, Monte Carlo simulations, and other probabilistic techniques will be integrated into the reasoning process. This will enable LLMs to make more robust decisions in complex scenarios with incomplete or noisy information and to better manage the inherent uncertainty of real-world problems. LLMs will be better equipped to handle unexpected situations, recover from errors, and provide more reliable solutions when faced with ambiguity (<a href="https://arxiv.org/abs/2310.04406">https://arxiv.org/abs/2310.04406</a>).</li>
				<li><strong class="bold">Human-in-the-loop multi-step reasoning with explainability</strong>: Future systems will involve closer collaboration between humans and LLMs in multi-step problem solving. This means creating interfaces that allow humans to understand the LLM’s reasoning process, provide guidance, correct errors, and work together on complex tasks. Explainability will be key, with LLMs able to articulate their reasoning steps, justify tool choices, and present alternative solution paths. This will foster trust and allow for more effective human-AI collaboration, especially in critical domains such as healthcare, finance, and scientific research. This could involve visualizations of reasoning graphs, natural language explanations, and interactive debugging tools: <a href="https://www.microsoft.com/en-us/research/blog/guidance-for-developing-with-large-language-models-llms/">https://www.microsoft.com/en-us/research/blog/guidance-for-developing-with-large-language-models-llms/</a>.</li>
			</ul>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor364"/>Summary</h1>
			<p>Automatic multi-step reasoning and tool use significantly expand the problem-solving capabilities of LLMs, enabling them to tackle complex, real-world tasks.</p>
			<p>In this chapter, you learned how to design prompts for complex task decomposition and implement systems that allow LLMs to interact with external tools and APIs. We looked at strategies for automatic tool selection and use and explored applications in complex problem-solving scenarios. You also learned how to evaluate the effectiveness of multi-step reasoning and tool use in LLMs. By implementing the techniques and considerations discussed in this chapter, you can create sophisticated AI systems that can decompose problems, leverage external tools, and generate comprehensive solutions to multi-faceted challenges.</p>
			<p>As we move forward, the next part of the book will focus on retrieval and knowledge integration. This will build upon the tool use capabilities we’ve discussed here, exploring how LLMs can be enhanced with external knowledge, improving their ability to access and utilize information effectively.</p>
		</div>
	</div></div>
<div><div><p>&#13;
			<h1 id="_idParaDest-295" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor365"/>Part 5: Retrieval and Knowledge Integration in Large Language Models</h1>&#13;
			<p>We conclude this book by examining techniques that enhance LLMs with external knowledge through retrieval-augmented generation (RAG) methods. You will learn how to design retrieval systems that efficiently access relevant information, integrate structured knowledge into model outputs, and leverage graph-based retrieval to enrich responses with contextual relationships. Advanced RAG patterns, such as iterative and adaptive retrieval, will be explored, helping you create models capable of dynamic knowledge integration. We also discuss evaluation methodologies to measure retrieval quality and effectiveness. The final chapter introduces agentic patterns, enabling you to build autonomous systems that combine reasoning, planning, and decision-making. By mastering these techniques, you will be able to create LLMs that are not only informed but also capable of goal-directed behavior.</p>&#13;
			<p>This part has the following chapters:</p>&#13;
			<ul>&#13;
				<li><a href="B31249_26.xhtml#_idTextAnchor366"><em class="italic">Chapter 26</em></a>, <em class="italic">Retrieval-Augmented Generation</em></li>&#13;
				<li><a href="B31249_27.xhtml#_idTextAnchor378"><em class="italic">Chapter 27</em></a>, <em class="italic">Graph-Based RAG</em></li>&#13;
				<li><a href="B31249_28.xhtml#_idTextAnchor389"><em class="italic">Chapter 28</em></a>, <em class="italic">Advanced RAG</em></li>&#13;
				<li><a href="B31249_29.xhtml#_idTextAnchor400"><em class="italic">Chapter 29</em></a>, <em class="italic">Evaluating RAG Systems</em></li>&#13;
				<li><a href="B31249_30.xhtml#_idTextAnchor469"><em class="italic">Chapter 30</em></a>, <em class="italic">Agentic Patterns</em></li>&#13;
			</ul>&#13;
		</p>&#13;
		<p>&#13;
			<div>&#13;
			</p>&#13;
		</div>&#13;
		<p>&#13;
			<div>&#13;
			</p>&#13;
		</div>&#13;
	</div></div></body></html>