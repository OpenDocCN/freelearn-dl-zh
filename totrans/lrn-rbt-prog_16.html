<html><head></head><body><div><div><h1 id="_idParaDest-303"><em class="italic"><a id="_idTextAnchor283"/>Chapter 13</em>: Robot Vision – Using a Pi Camera and OpenCV</h1>
			<p>Giving a robot the ability to see things allows it to behave in ways to which humans relate well. Computer vision is still actively being researched, but some of the basics are already available for use in our code, with a Pi Camera and a little work. </p>
			<p>In this chapter, we will use the robot and camera to drive to objects and follow faces with our pan-and-tilt mechanism. We'll be using the PID algorithm some more and streaming camera output to a web page, giving you a way to see what your robot is seeing. </p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Setting up the Raspberry Pi camera</li>
				<li>Setting up computer vision software</li>
				<li>Building a Raspberry Pi camera stream app</li>
				<li>Running background tasks when streaming</li>
				<li>Following colored objects with Python</li>
				<li>Tracking faces with Python</li>
			</ul>
			<h1 id="_idParaDest-304"><a id="_idTextAnchor284"/>Technical requirements</h1>
			<p>For this chapter, you will need the following:</p>
			<ul>
				<li>The robot with the pan-and-tilt mechanism from <a href="B15660_11_Final_ASB_ePub.xhtml#_idTextAnchor219"><em class="italic">Chapter 11</em></a>, <em class="italic">Programming Encoders with Python</em>.</li>
				<li>Code for the robot up to <a href="B15660_11_Final_ASB_ePub.xhtml#_idTextAnchor219"><em class="italic">Chapter 11</em></a>, <em class="italic">Programming Encoders with Python</em>, which you can download from GitHub at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter11">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter11</a>. We will be extending and modifying this for new functionality.</li>
				<li>A Raspberry Pi camera.</li>
				<li>A 300 mm-long Pi Camera cable, as the cable included with the camera is too short. Be sure that the cable is not for a Pi Zero (which has different connectors).</li>
				<li>Two M2 bolts and an M2 nut.</li>
				<li>A small square of thin cardboard—a cereal box will do.</li>
				<li>A small jeweler's screwdriver.</li>
				<li>A pencil.</li>
				<li>A kids' bowling set—the type with differently colored pins (plain, with no pictures).</li>
				<li>A well-lit space for the robot to drive in.</li>
				<li>Internet access.</li>
			</ul>
			<p>The code for this chapter is on GitHub, available at <a href="https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter13">https://github.com/PacktPublishing/Learn-Robotics-Programming-Second-Edition/tree/master/chapter13</a>.</p>
			<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/39xfDJ9">https://bit.ly/39xfDJ9</a></p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor285"/>Setting up the Raspberry Pi camera</h1>
			<p>Before we can get into <a id="_idIndexMarker747"/>computer vision, we need to prepare the camera on your robot. There is hardware installation and software installation involved.</p>
			<p>When we have completed this installation, our robot block diagram will look like <em class="italic">Figure 13.1</em>:</p>
			<div><div><img src="img/B15660_13_01.jpg" alt="" width="792" height="742"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.1 – Our robot block diagram with the camera added</p>
			<p><em class="italic">Figure 13.1</em> continues the block diagrams we have shown throughout the book, with the camera's addition and its connection to the Raspberry Pi highlighted on the left.</p>
			<p>We will first attach <a id="_idIndexMarker748"/>the camera to the pan-and-tilt assembly. We can then use a longer cable to wire the camera into the Pi. Let's start preparing the camera to be attached.</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor286"/>Attaching the camera to the pan-and-tilt mechanism</h2>
			<p>In <a href="B15660_10_Final_ASB_ePub.xhtml#_idTextAnchor192"><em class="italic">Chapter 10</em></a>, <em class="italic">Using Python to Control Servo Motors</em>, you added a pan-and-tilt mechanism to <a id="_idIndexMarker749"/>your robot. You will mount the camera <a id="_idIndexMarker750"/>onto the front plate of this mechanism. There are brackets and kits, but they are not universally available. Feel free to use one of these if you can adapt it to the pan-and-tilt mechanism; if not, I have a few plans.</p>
			<p>Building a robot requires creative thinking and being adaptable, as well as the necessary technical skills. I frequently look through the materials I have for possible solutions before I go and buy something. Sometimes, the first thing you attempt will not work, and you'll need a plan B. My plan A was to use a hook-and-loop fastener (such as Velcro) stuck directly to the camera, but it does not adhere well to the back of the camera. So I had to move to plan B, that is, using a square of cardboard, making holes for 2 mm screws in it, bolting the camera to the cardboard, and then using the hook-and-loop fastener to attach the camera assembly to the Pi. Another possibility is to drill additional holes in the pan-and-tilt mechanism to line up with the camera screw holes.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Could I glue this? Yes, like most of our robot build, some glue—even crazy glue—could be used to adhere the camera to the pan and tilt. It would probably be an easier build. However, I can easily foresee that you would need to replace or remove the camera at some point. Reasons for that might be to reverse the camera cable or swap the camera out for another sensor, or even a newer camera with better features. It is for this reason that I generally avoid glue in my robot builds, looking for modular and replaceable solutions.</p>
			<p>The <a id="_idIndexMarker751"/>parts needed are shown <a id="_idIndexMarker752"/>in <em class="italic">Figure 13.2</em>:</p>
			<div><div><img src="img/B15660_13_02.jpg" alt="" width="1013" height="485"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.2 – The parts needed for our plan to fit the camera module</p>
			<p><em class="italic">Figure 13.2</em> shows the tools and materials laid out: some thin card, 2 mm bolts and screws, the Pi Camera module, some scissors, a small spanner (or pliers), hook-and-loop tape, and a small screwdriver. You will also need a pencil.</p>
			<p>While making this, please try not to touch the camera's lens. So let's begin. The following figure shows you the steps to attach the camera:</p>
			<div><div><img src="img/B15660_13_03.jpg" alt="" width="1356" height="451"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.3 – Fitting the camera, steps 1–2</p>
			<p>Here's how to use these parts to mount the camera:</p>
			<ol>
				<li>First, cut a small amount for one side of the hook-and-loop fastener and adhere it to the pan-and-tilt mechanism, as shown in <em class="italic">Figure 13.3</em>.</li>
				<li>Mark <a id="_idIndexMarker753"/>and cut out a small square of <a id="_idIndexMarker754"/>cardboard a little larger than the camera:<div><img src="img/B15660_13_04.jpg" alt="" width="521" height="484"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 13.4 – Using a pen to mark the screw positions</p></li>
				<li>Then use a pen or pencil to poke through the camera screw holes to mark a dot, as shown in <em class="italic">Figure 13.4</em>. Then take a pointed tool (such as the point of a cross-headed jeweler's <a id="_idIndexMarker755"/>screwdriver or a math set <a id="_idIndexMarker756"/>compass), and on a firm surface, punch a hole where you made the mark:<div><img src="img/B15660_13_05.jpg" alt="" width="1044" height="492"/></div><p class="figure-caption">Figure 13.5 – Bolting the camera to the cardboard</p></li>
				<li>Use a couple of M2 bolts and nuts to fasten the camera onto the cardboard carrier, as shown in <em class="italic">Figure 13.5</em>. Note that the bolt-facing side is at the back—this is so any protruding threads won't interfere with the hook and loop:<div><img src="img/B15660_13_06.jpg" alt="" width="1069" height="493"/></div><p class="figure-caption"> </p><p class="figure-caption">Figure 13.6 – The back of the cardboard/camera assembly with our hook-and-loop fastener</p></li>
				<li>Now cut a small amount of the hook-and-loop fabric, to which the material on the pan-and-tilt mechanism will fasten, and stick it to the back of the cardboard, as shown in <em class="italic">Figure 13.6</em>.</li>
			</ol>
			<p>Note that the <a id="_idIndexMarker757"/>camera may have a film covering the <a id="_idIndexMarker758"/>lens—please remove this.</p>
			<p>The camera is ready to be stuck to the robot. Don't attach the camera just yet, as we need to wire in the cable first. Let's see how in the next section.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor287"/>Wiring in the camera</h2>
			<p>With the camera <a id="_idIndexMarker759"/>ready to attach, we'll need to use the Raspberry Pi camera cable to connect it to the Pi. We'll need to move some parts to get to the Raspberry Pi connector and thread the ribbon connector through.</p>
			<p>The sequence of images in <em class="italic">Figure 13.7</em> shows how we will wire this:</p>
			<div><div><img src="img/B15660_13_07.jpg" alt="" width="940" height="1250"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.7 – The camera connector slot and the motor board</p>
			<p>The steps in <em class="italic">Figure 13.7</em> show <a id="_idIndexMarker760"/>how we'll prepare the cable connector:</p>
			<ol>
				<li value="1">The Raspberry Pi has a slot specifically for the camera—the camera cable fits into this. We will be wiring our camera into this slot, but the motor board covers the slot on our robot.</li>
				<li>To get around the slot being covered, we will need to lift the other boards above the Pi. You'll temporarily <a id="_idIndexMarker761"/>need to unbolt the <strong class="bold">Inertial Measurement Unit</strong> (<strong class="bold">IMU</strong>), so the motor board isn't covered by it. Loosen the nut on top of the IMU; then you can turn the lower spacer post by hand to remove the IMU, leaving the IMU and standoff assembly complete.</li>
				<li>Disconnect the motor wires (note how you'd previously connected them, or take a photo for later reference).</li>
				<li>Now gently lift the motor board off the Raspberry Pi. </li>
				<li>When you <a id="_idIndexMarker762"/>connect the camera to the Pi, the long cable will need to pass through the motor board. Keep this in mind as you perform the next step.</li>
			</ol>
			<p>I recommend following <em class="italic">Connect ribbon cable to camera</em> in <em class="italic">The Official Raspberry Pi Camera Guide</em> (<a href="https://magpi.raspberrypi.org/books/camera-guide">https://magpi.raspberrypi.org/books/camera-guide</a>) for attaching the camera using <a id="_idIndexMarker763"/>the long 300 mm cable. After following the guide, you should have the ribbon installed the correct way around in the camera, then going through the slot in the motor board and into the port the right way around on the Raspberry Pi. </p>
			<p>Double-checking that your connections are the right way around before replacing the motor board will save you a lot of time. </p>
			<p>To complete the reassembly, take a look at <em class="italic">Figure 13.8</em>:</p>
			<div><div><img src="img/B15660_13_08.jpg" alt="" width="924" height="817"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.8 – Completing the camera interface</p>
			<p>Follow these steps, using <em class="italic">Figure 13.8</em> as a reference:</p>
			<ol>
				<li value="1">Gently replace the motor board, pushing its header down onto the Raspberry Pi GPIO header and the holes onto spacers. </li>
				<li>Bolt the IMU <a id="_idIndexMarker764"/>back in.</li>
				<li>Reconnect the motor cables based on your reference.</li>
				<li>Push the camera onto the hook-and-loop attachment on the pan-and-tilt head, with the cable facing upward.</li>
			</ol>
			<p>You've seen how to wire in Raspberry Pi cameras. This camera is now wired and ready to use. Next, we will start preparing the software to get images from the camera.</p>
			<h1 id="_idParaDest-308"><a id="_idTextAnchor288"/>Setting up computer vision software</h1>
			<p>Before we can <a id="_idIndexMarker765"/>start writing code, we'll need to set up drivers, tools, and libraries to interact with the camera and software to assist with computer vision.</p>
			<p>In this section, we will activate the camera in Raspberry Pi OS Raspberry Pi OS and get a test picture. Then we will add the libraries to start interacting with the camera for visual processing.</p>
			<p>We will then build our first app with the tool to demonstrate that the parts are in place and give us a starting point for the behaviors. Let's get into setting up the software.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor289"/>Setting up the Pi Camera software</h2>
			<p>So <a id="_idIndexMarker766"/>that the camera is ready to use, we <a id="_idIndexMarker767"/>need to enable it:</p>
			<ol>
				<li value="1">Power up the Pi on external power (that is, plugged into a USB wall adapter) for this operation, leaving the motors powered down for now.</li>
				<li>Log in via SSH. At the terminal, type the following:<pre><strong class="bold">pi@myrobot:~ $ sudo raspi-config</strong></pre></li>
				<li>You should now see <code>raspi-config</code>. Select the<strong class="bold"> Interfacing Options</strong> menu item by using the cursor keys and <em class="italic">Enter</em>.</li>
				<li>Select <code>raspi-config</code> will then ask whether you would like the camera interface to be enabled. Select <strong class="bold">Yes</strong> and <strong class="bold">Ok</strong>, then <strong class="bold">Finish</strong>.</li>
				<li>You will need to reboot for this to take effect:<pre><strong class="bold">pi@myrobot:~ $ sudo reboot</strong></pre></li>
			</ol>
			<p>To verify that we can get pictures, we'll need the <code>picamera</code> package. At the time of writing, there is a copy of <code>picamera</code> already installed in Raspberry Pi OS.</p>
			<p>Now that the camera is enabled, let's try getting our first picture.</p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor290"/>Getting a picture from the Raspberry Pi</h2>
			<p>The first <a id="_idIndexMarker768"/>thing we need to <a id="_idIndexMarker769"/>do, to confirm that our setup was successful, is to ask the Pi Camera to take a picture. If the camera isn't detected, please go back and check that the cable connection is correct, that you have installed <code>picamera</code>, and that you have enabled the Raspberry Pi camera in <code>raspi-config</code>:</p>
			<ol>
				<li value="1">Reconnect to the Raspberry Pi and type the following to get a picture:<pre><code>raspistill</code> command takes a still image, and the <code>-o</code> parameter tells it to store that image in <code>test.jpg</code>. This command may take a while; taking a still can be slow if light conditions are poor.</p></li>
				<li>You can then use your SFTP client (which we set up in <a href="B15660_04_Final_ASB_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a>, <em class="italic">Preparing a Headless Raspberry Pi for a Robot</em>) to download this image and verify it on your computer. You will notice that the picture is upside down, due to how the camera is mounted. Don't worry—we will correct this with our software.</li>
			</ol>
			<p>With a picture taken, you know that the camera works. Now we can install the rest of the software needed to use the camera in visual processing applications.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor291"/>Installing OpenCV and support libraries</h2>
			<p>We will <a id="_idIndexMarker770"/>need a few helper <a id="_idIndexMarker771"/>libraries to do the heavy lifting of visual processing and display the output in a useful <a id="_idIndexMarker772"/>way. <strong class="bold">Open Computer Vision</strong> (<strong class="bold">OpenCV</strong>) is a library with a collection <a id="_idIndexMarker773"/>of tools for manipulating pictures and <a id="_idIndexMarker774"/>extracting information. Code can use these OpenCV tools together to make useful behaviors and pipelines for processing images. </p>
			<p>To run our code on the Raspberry Pi, we will need to install the Python OpenCV library there: </p>
			<ol>
				<li value="1">OpenCV has some dependencies that are needed first:<pre><strong class="bold">pi@myrobot:~ $ sudo apt install -y libavcodec58 libilmbase23 libgtk-3-0 libatk1.0-0 libpango-1.0-0 libavutil56 libavformat58 libjasper1 libopenexr23 libswscale5 libpangocairo-1.0-0 libtiff5 libcairo2 libwebp6 libgdk-pixbuf2.0-0 libcairo-gobject2 libhdf5-dev</strong>
<strong class="bold">pi@myrobot:~ $ sudo pip3 install "opencv_python_headless&lt;4.5" "opencv_contrib_python_headless&lt;4.5"</strong></pre></li>
				<li>Raspberry Pi OS requires a library to be identified for OpenCV to work. This line identifies the library every time you log in to the Pi. We should also prepare it for this session:<pre><strong class="bold">pi@myrobot:~ $ echo export LD_PRELOAD=/usr/lib/arm-linux-gnueabihf/libatomic.so.1 &gt;&gt;.bashrc</strong>
<strong class="bold">pi@myrobot:~ $ source .bashrc</strong></pre></li>
				<li><strong class="bold">Flask</strong> is a library <a id="_idIndexMarker775"/>for creating web servers that we'll use to <a id="_idIndexMarker776"/>stream the video data to a browser:<pre><strong class="bold">pi@myrobot:~ $ sudo pip3 install flask</strong></pre></li>
				<li><strong class="bold">NumPy</strong>, the <a id="_idIndexMarker777"/>numeric Python library, is excellent for the <a id="_idIndexMarker778"/>manipulation of large blocks of numbers. An image stored on a computer is essentially a large block of numbers, with each tiny dot having similar content to the three-color numbers we sent to the LEDs in <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB LED Strips in Python</em>:<pre><strong class="bold">pi@myrobot:~ $ sudo apt install -y libgfortran5 libatlas3-base</strong>
<strong class="bold">pi@myrobot:~ $ sudo pip3 install numpy</strong></pre></li>
				<li>We will need to install <a id="_idIndexMarker779"/>the large array <a id="_idIndexMarker780"/>extension for <code>picamera</code>. This will help us convert it's data for use in NumPy and OpenCV:<pre><strong class="bold">pi@myrobot:~ $ sudo pip3 install picamera[array]</strong></pre></li>
			</ol>
			<p>We will continue testing on external power for the next few operations.</p>
			<p>You've now prepared the software libraries and verified that the camera can take pictures. Next, we'll build an app to stream video from the camera to your browser.</p>
			<h1 id="_idParaDest-312"><a id="_idTextAnchor292"/>Building a Raspberry Pi camera stream app</h1>
			<p>Downloading one <a id="_idIndexMarker781"/>picture at a time is fine, but we need to do things with those pictures on our robot. We also need a handy way to see what the robot is doing with the camera data. For that, we will learn how to use a Flask web server to serve up our pictures so we can view the output on a phone or laptop. We can use the core of this app to make a few different behaviors. We'll keep the base app around for them.</p>
			<p>A video or video stream is a sequence of <a id="_idIndexMarker782"/>images, usually known as <strong class="bold">frames</strong>.</p>
			<p>Let's design our streaming server.</p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor293"/>Designing the OpenCV camera server</h2>
			<p>The diagram in <em class="italic">Figure 13.9</em> shows an image <a id="_idIndexMarker783"/>data pipeline, going from the camera, through the processing, and out to our web browser: </p>
			<div><div><img src="img/B15660_13_09.jpg" alt="" width="1360" height="538"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.9 – The image server app</p>
			<p>The image server app in <em class="italic">Figure 13.9</em> starts with the camera. The camera feeds image data to a <strong class="bold">convert to OpenCV</strong> step, with the raw photo given. Image data needs some processing for OpenCV to be able to manipulate it.</p>
			<p><strong class="bold">convert to OpenCV</strong> feeds data to <strong class="bold">process a frame</strong>, which can be anything we require; for this example, we'll apply a color mask, which we explore in more depth in the next section. Above the <strong class="bold">process a frame</strong> step is an example of an image after using a red color mask.</p>
			<p>The raw frame and processed frame go into the next step, <strong class="bold">join with original</strong>, which creates a compound image with both images. Above the step are the two images joined into a single longer frame.</p>
			<p>The joined images go into the <code>jpeg</code>, an image encoding that a browser can show, and <a id="_idIndexMarker784"/>importantly, display as a sequence of frames, a streaming movie. </p>
			<p>The encoded data goes to <strong class="bold">serve over HTTP</strong>, getting the data into a system you can view with a web browser. It uses a template (some layout and text for the browser) to serve this. </p>
			<p>The image output then goes from <strong class="bold">serve over HTTP</strong>, via the network, to the users, browser. Finally, the browser shows the image to the user. The browser could be on a laptop or a phone.</p>
			<p>It's time to start building the code. We'll break it down into two major parts: first, a <code>CameraStream</code> object, which will send our frames to the second part of our code project, an <code>image_server.py</code> script.</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor294"/>Writing the CameraStream object</h2>
			<p>As part of <a id="_idIndexMarker785"/>our system, we will create a helper library to set up the camera and get data streams from it: </p>
			<ol>
				<li value="1">Start the <code>camera_stream.py</code> file with the following imports:<pre><code>PiCamera</code> code needed to access our camera. <code>cv2</code> is OpenCV, the computer vision library used to process the images. Here, NumPy is <em class="italic">aliased</em>, or nicknamed, <code>np</code>. </p></li>
				<li>The next few lines set up parameters for the capture size and image quality:<pre><code>encode</code> parameter. </p></li>
				<li>Add <a id="_idIndexMarker786"/>a function to set up the camera:<pre><strong class="bold">def setup_camera():</strong>
<strong class="bold">    camera = PiCamera()</strong>
<strong class="bold">    camera.resolution = size</strong>
<strong class="bold">    camera.rotation = 180</strong>
<strong class="bold">    return camera</strong></pre><p>After initializing the camera, we set its resolution to the size. I mentioned that the camera is the wrong way up, so we set its rotation to 180 degrees to turn the pictures around. </p></li>
				<li>We will need a function to start capturing a stream of images (a video, but a frame at a time):<pre><code>PiRGBArray</code> instance, a type for storing RGB images. We then set up the stream of data with <code>capture_continuous</code>, a <code>picamera</code> method to take photos repeatedly. We pass it to the image store and tell it to format the output data as <code>bgr</code> (blue, green, red), which is how OpenCV stores color data. The last parameter to this is <code>use_video_port</code>, which, when set to <code>true</code>, results in a reduction in image quality in exchange for faster production of frames. </p></li>
				<li>We can loop through <code>cam_stream</code> for frames until we choose to stop. Python has a <a id="_idIndexMarker787"/>concept of <code>for</code> loop is a <a id="_idIndexMarker788"/>generator. Every cycle will yield the raw <code>.array</code> from the frame that the stream captured. What this means is that a loop can use the output of the <code>start_stream</code> function, so when looped over, the code in this <code>for</code> loop will run just enough to produce one raw frame, then the next, and so on. Python generators are a way to construct processing pipelines.</p><p>The last line of the loop calls <code>truncate</code> to reset <code>image_storage</code> ready to hold another image. <code>PiRGBArray</code> can store many images in sequence, but we only want the latest one. More than one image may have arrived while we were processing a frame, so we must throw them away.</p></li>
				<li>The final thing we add to the <code>camera_stream.py</code> script is a function to encode an image as <code>jpeg</code> and then into bytes for sending, as shown here:<pre><strong class="bold">def get_encoded_bytes_for_frame(frame):</strong>
<strong class="bold">    result, encoded_image = cv2.imencode('.jpg', frame, encode_param)</strong>
<strong class="bold">    return encoded_image.tostring()</strong></pre></li>
			</ol>
			<p>We will use the <code>camera_stream</code> library for a few of our behaviors, giving us the ability to fetch and encode <a id="_idIndexMarker789"/>camera frames, both ready for input and encoded for display. With that ready, let's use it in a test app to serve frames in a browser.</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor295"/>Writing the image server main app</h2>
			<p>This <a id="_idIndexMarker790"/>part of the app will set up Flask, start our camera stream, and link them together. We will put this in a new script named <code>image_server.py</code>: </p>
			<ol>
				<li value="1">We need to import all of these components and set up a Flask app:<pre><code>Flask</code> app object, which handles routing; a way to render templates into output; and a way to make our web app response. We import the <code>camera_stream</code> library we've just made, and we import <code>time</code> so we can limit the frame rate to something sensible. After the imports, we create a Flask <code>app</code> object for us to register everything with.</p></li>
				<li>Flask works in routes, which are links between an address you hit a web server at and a registered handler function. A matching address asked for at our server app will run the corresponding function. Let's set up the most basic route:<pre><code>'/'</code> route will be the <code>index</code> page, what you get by default if you just land on the robot's app server. Our function renders a template, which we'll write in the next section. </p></li>
				<li>Now we get to the tricky bit, the video feed. Although <code>camera_stream</code> does some of the encoding, we need to turn the frames into an HTTP stream of data, that is, data that your browser expects to be continuous. I'll put this in a <code>frame_generator</code> function, which we'll need to break down a little. Let's start by setting up the camera stream:<pre><code>time.sleep</code> is here because we need to let the camera warm up after turning it on. Otherwise, we may not get usable frames from it. </p></li>
				<li>Next, we need to loop over the frames from <code>camera_stream</code>:<pre><code>start_stream</code>, encoding each frame to JPG. </p></li>
				<li>To send the encoded frame bytes back to the browser, we use another generator with <code>yield</code>, so Flask considers this a multipart stream—a response made of multiple chunks of data, with parts deferred for later—which many frames of the same video would be. Note that HTTP content declarations prefix the encoded bytes:<pre><code>b</code> in front of this string to tell Python to treat this as raw bytes and not perform further encoding on the information. The <code>\r</code> and <code>\n</code> items are raw line-ending characters. That completes the <code>frame_generator</code> function. </p></li>
				<li>The next function, named <code>display</code>, routes from Flask to a loopable stream of HTTP frames from <code>frame_generator</code>:<pre><code>display</code> route generates a response from <code>frame_generator</code>. As that is a generator, Flask will keep consuming items from that generator and sending those parts to the browser. </p><p>The response also specifies a content type with a boundary between items. This boundary must be a <a id="_idIndexMarker792"/>string of characters. We have used <code>frame</code>. The boundary must match in <code>mimetype</code> and the boundary (<code>--frame</code>) in the content (<em class="italic">step 5</em>).</p></li>
				<li>Now we can just add the code to start Flask. I've put this app on port <code>5001</code>:<pre><strong class="bold">app.run(host="0.0.0.0", debug=True, port=5001)</strong></pre></li>
			</ol>
			<p>The app is nearly ready, but we mentioned a template—let's use this to describe what will go on the web page with the camera stream.</p>
			<h2 id="_idParaDest-316"><a id="_idTextAnchor296"/>Building a template</h2>
			<p>Flask makes <a id="_idIndexMarker793"/>web pages using HTML templates, which route functions render into the output, replacing some elements at runtime if necessary. Create a <code>templates</code> folder, then make a file in that folder named <code>image_server.html</code>:</p>
			<ol>
				<li value="1">Our template starts with the HTML tag, with a title and a level 1 heading:<pre><strong class="bold">&lt;html&gt;</strong>
<strong class="bold">    &lt;head&gt;</strong>
<strong class="bold">        &lt;title&gt;Robot Image Server&lt;/title&gt;</strong>
<strong class="bold">    &lt;/head&gt;</strong>
<strong class="bold">    &lt;body&gt;</strong>
<strong class="bold">        &lt;h1&gt;Robot Image Server&lt;/h1&gt;</strong></pre></li>
				<li>Now, we add the image link that will display the output of our server:<pre><code>url_for</code> here. Flask can use a template renderer, Jinja, to insert the URL from a route in Flask by its function name.</p></li>
				<li>Finally, we just close the tags in the template:<pre><strong class="bold">    &lt;/body&gt;</strong>
<strong class="bold">&lt;/html&gt;</strong></pre></li>
			</ol>
			<p>We can serve this <a id="_idIndexMarker794"/>template up in our main server app.</p>
			<p>Now we can upload all three of these parts, ensuring that you upload the template into the <code>templates</code> directory on the Pi. </p>
			<p>With the server code and templates ready, you should be able to run the image server.</p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor297"/>Running the server</h2>
			<p>Start the <a id="_idIndexMarker795"/>app with <code>python3 image_server.py</code>.</p>
			<p>Point your browser at the app by going to <code>http://myrobot.local:5001</code> (or your robot's address), and you should see a video served, as shown in <em class="italic">Figure 13.10</em>:</p>
			<div><div><img src="img/B15660_13_10.jpg" alt="" width="428" height="436"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.10 – Screen capture of the robot image server</p>
			<p>The screenshot in <em class="italic">Figure 13.10</em> shows our robot image server output in a browser. The top shows the <a id="_idIndexMarker796"/>browser search bar, with the <code>myrobot.local:5001</code> address in it. Below this is the <strong class="bold">Robot Image Server</strong> heading from the template. Below the heading is an image capture of a kids' red bowling pin taken from a robot camera—served up with the video stream code.</p>
			<h2 id="_idParaDest-318"><a id="_idTextAnchor298"/>Troubleshooting</h2>
			<p>If you have problems running the <a id="_idIndexMarker797"/>server and seeing the picture, try the following steps:</p>
			<ul>
				<li>If you see errors while running the code, do the following:<p>a) Ensure you can capture images with raspistill.</p><p>b) Ensure you have installed all the required dependencies.</p><p>c) If it's about <code>libatomic</code>, please ensure that you have performed the previous <code>LD_PRELOAD</code> exports.</p><p>d) Check that the code is correct.</p></li>
				<li>If the image is black, check your lighting. The Raspberry Pi camera is susceptible to light conditions and needs a well-lit space to operate. Note that none of the following tracking will work if the camera is not getting enough light.</li>
				<li>Expect the <a id="_idIndexMarker798"/>rate to be slow—this is not a fast or high-quality capture.</li>
			</ul>
			<p>Now you can stream images from a Raspberry Pi into a browser. Next, we will add a background worker task and control mechanism to the app, as this whole server depends on the slow browser request cycle.</p>
			<h1 id="_idParaDest-319"><a id="_idTextAnchor299"/>Running background tasks when streaming</h1>
			<p>Our image <a id="_idIndexMarker799"/>service works but has a significant flaw. Currently it <a id="_idIndexMarker800"/>will wait between requests before taking each action, but what if we want our robot to be doing something? To do this, we need to be able to run a behavior in parallel with the server. That behavior and the server both need access to the image data.</p>
			<p>We will approach this by making the Flask web app a secondary process, with the behavior as the primary process for the robot when it is running. Python has a handy tool for precisely this kind of <a id="_idIndexMarker801"/>structure, called multiprocessing. Find out more at <a href="https://docs.python.org/3/library/multiprocessing.html">https://docs.python.org/3/library/multiprocessing.html</a>.</p>
			<p>Communicating between multiple processes is tricky. If two processes try to access (read or write) the same data simultaneously, the results can be unpredictable and cause strange behavior. So, to save them trying to access data simultaneously, we will use the multiprocessing queue object. A queue allows one process to put data in at one end and another process to consume it safely at the other—it is a one-way flow of information. We will use one queue to send images to the server and another to get control data from user interactions in the browser.</p>
			<p>The diagram in <em class="italic">Figure 13.11</em> shows the way data will flow through these behaviors:</p>
			<div><div><img src="img/B15660_13_11.jpg" alt="" width="1573" height="502"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.11 – Data flow between a browser, server process, and robot behavior</p>
			<p>In <em class="italic">Figure 13.11</em>, we abridge some of the sections in <em class="italic">Figure 13.9</em>. First, there is data from the camera going into a visual processing behavior (for example, tracking an object). This behavior will output image frames to an image queue. The output will be the fully processed and joined image. </p>
			<p>A server process, the web app, will take the images from the image queue to serve them to a browser via the network. However, the web app will also handle commands from user interaction in the browser. The app puts them in the control queue as messages. The visual processing behavior will read any messages from the control queue and act on them.</p>
			<p>A few caveats: the <a id="_idIndexMarker802"/>visual processing behavior will only place <a id="_idIndexMarker803"/>images in the image queue when it's empty, so the queue will only ever contain one image. Allowing only one prevents the visual processing behavior from trying to overwrite an image in shared memory when a server tries to output it. The control queue has no such restriction; we'll just expect that user interactions will not produce control messages faster than the behavior loop can consume them.</p>
			<p>We will separate the web app as a core and then write a behavior based on it. We can use the web app core multiple times. Let's write this code.</p>
			<h3 id="_idParaDest-320">Writing a web app core</h3>
			<p>In this design, the web app core will handle setting up the queues, running the server process, and the Flask-based <a id="_idIndexMarker804"/>routing. We will write the library in Flask style, using plain Python functions in a module. </p>
			<p>As an interface to the core, our other behaviors will be able to do the following:</p>
			<ul>
				<li><code>start_server_process(template_name)</code> will start the web app server, using the named template.</li>
				<li><code>put_output_image(encoded_bytes)</code> will put images into the display queue.</li>
				<li><code>get_control_instruction()</code> is used to check and return instructions from the control queue. This function returns a dictionary of instruction data.</li>
			</ul>
			<p>The Flask/web server part of the app is slightly independent of the behavior, allowing the user to <em class="italic">tune in</em> to see its display, but it should not stop the app running when a user is not present or a browser stalls:</p>
			<ol>
				<li value="1">Let's start with some imports. We'll put this code in <code>image_app_core.py</code>:<pre><code>Queue</code> and <code>Process</code> to create the process and communicate with it. We then use the same imports for Flask that we used previously. Note—we are <em class="italic">not</em> importing any of the camera parts in this module.</p></li>
				<li>Next, we define our Flask app and the queues. We only really want one frame queued, but we put in one in case of hiccups while transmitting—although we can check whether a <code>Queue</code> instance is empty, this is not 100% reliable, and we don't want one part of the app waiting for the other:<pre><strong class="bold">app = Flask(__name__)</strong>
<strong class="bold">control_queue = Queue()</strong>
<strong class="bold">display_queue = Queue(maxsize=2)</strong></pre></li>
				<li>We will also define a global <code>display_template</code> here, in which we'll store the main app template:<pre><strong class="bold">display_template = 'image_server.html'</strong></pre></li>
				<li>Now we add routes for this Flask app. The index route is only different in that it uses <code>display_template</code>:<pre><strong class="bold">@app.route('/')</strong>
<strong class="bold">def index():</strong>
<strong class="bold">    return render_template(display_template)</strong></pre></li>
				<li>Next, we will create the loop for getting frames: a modified version of <code>frame_generator</code>. This function is our main video feed. So that it doesn't <em class="italic">spin</em> (that is, run very <a id="_idIndexMarker805"/>quickly in a tight loop), we put in a sleep of 0.05 to limit the frame rate to 20 frames per second:<pre><strong class="bold">def frame_generator():</strong>
<strong class="bold">    while True:</strong>
<strong class="bold">        time.sleep(0.05)</strong></pre></li>
				<li>After the sleep, we should try to get data from <code>display_queue</code> (we'll put frames into the queue later). Like we did in <code>image_server</code>, this loop also turns our data into multi-part data:<pre><strong class="bold">        encoded_bytes = display_queue.get()</strong>
<strong class="bold">        yield (b'--frame\r\n'</strong>
<strong class="bold">                b'Content-Type: image/jpeg\r\n\r\n' + encoded_bytes + b'\r\n')</strong></pre></li>
				<li>Now make that available through a display block:<pre><strong class="bold">@app.route('/display')</strong>
<strong class="bold">def display():</strong>
<strong class="bold">    return Response(frame_generator(),</strong>
<strong class="bold">        mimetype='multipart/x-mixed-replace; boundary=frame')</strong></pre></li>
				<li>We need a way to post control messages to our app. The <code>control</code> route accepts these, takes their form data (a dictionary with instructions), and uses <code>control_queue.put</code> to pass that along to the robot behavior:<pre><strong class="bold">@app.route('/control', methods=['POST'])</strong>
<strong class="bold">def control():</strong>
<strong class="bold">    control_queue.put(request.form)</strong>
<strong class="bold">    return Response('queued')</strong></pre></li>
				<li>That gives us all the core internals, but we also need to start the server process. The part of the app from earlier that started our server, we've now put into a function named <code>start_server_process</code>:<pre><code>template_name</code> in the global <code>display_template</code>. The preceding <code>index</code> route uses the template. Instead of calling <code>app.run</code>, we create a <code>Process</code> object. The <code><a id="_idIndexMarker806"/></code><code>Process</code> parameter <code>target</code> is a function to run (<code>app.run</code>), and some parameters need to be given to that function (the host and port settings). We then start the server process and return the process handle so our code can stop it later.</p></li>
				<li>The next interface task is putting an image into the queue we created in <em class="italic">step 1</em>. To ensure that we don't run up a lot of memory, we only intend the queue to have a length of one. That means that the first frame will be stale, but the next frame will arrive soon enough for it not to affect the user:<pre><strong class="bold">def put_output_image(encoded_bytes):</strong>
<strong class="bold">    if display_queue.empty():</strong>
<strong class="bold">        display_queue.put(encoded_bytes)</strong></pre></li>
				<li>Finally, for this interface, we need a function to get the control messages out. This function will not wait and will return a message if there is one or <code>None</code> for <em class="italic">no message</em>:<pre><strong class="bold">def get_control_instruction():</strong>
<strong class="bold">    if control_queue.empty():</strong>
<strong class="bold">        return None</strong>
<strong class="bold">    else:</strong>
<strong class="bold">        return control_queue.get()</strong></pre></li>
			</ol>
			<p>The <code>image_app_core.py</code> file establishes a controllable base for us to build visual processing robot behaviors with, or <a id="_idIndexMarker807"/>indeed any behavior with a web interface, control instructions, an output stream, and background process. Next, let's test this core with a simple behavior.</p>
			<h3 id="_idParaDest-321">Making a behavior controllable</h3>
			<p>We can try out our core <a id="_idIndexMarker808"/>with a behavior that sends images to the web service and accepts a simple <code>exit</code> control message:</p>
			<ol>
				<li value="1">Let's make a new file called <code>control_image_behavior.py</code>, starting with imports for the <code>image_app_core</code> interface and <code>camera_stream</code>:<pre><strong class="bold">import time</strong>
<strong class="bold">from image_app_core import start_server_process, get_control_instruction, put_output_image</strong>
<strong class="bold">import camera_stream</strong></pre></li>
				<li>We then add a function that runs our simple behavior with the main loop. I've broken this function down as it's a little complicated. First, we'll set up the camera and use a sleep to give the camera warm-up time: <pre><strong class="bold">def controlled_image_server_behavior():</strong>
<strong class="bold">    camera = camera_stream.setup_camera()</strong>
<strong class="bold">    time.sleep(0.1)</strong></pre></li>
				<li>Next, we get frames from a camera stream in a <code>for</code> loop and put those as encoded bytes on the output queue:<pre><strong class="bold">    for frame in camera_stream.start_stream(camera):</strong>
<strong class="bold">        encoded_bytes = camera_stream.get_encoded_bytes_for_frame(frame)</strong>
<strong class="bold">        put_output_image(encoded_bytes)</strong></pre></li>
				<li>While still in the loop, we will try accepting a control instruction to exit. Normally the instruction will be <code>None</code>, signalling there are no control instructions waiting. But if we have a message, we should match the command in it to exit:<pre><code>return</code> to stop the behavior when it receives the <code>exit</code> instruction from the control queue. </p></li>
				<li>We then need to start the <a id="_idIndexMarker809"/>server and start our behavior. We always want to stop the web server process. By surrounding the behavior with <code>try</code> and <code>finally</code>, it will <em class="italic">always</em> run anything in the <code>finally</code> part, in this case, making sure the process is terminated (stopped):<pre><strong class="bold">process = start_server_process('control_image_behavior.</strong><strong class="bold">html')</strong>
<strong class="bold">try:</strong>
<strong class="bold">    controlled_image_server_behavior()</strong>
<strong class="bold">finally:</strong>
<strong class="bold">    process.terminate()</strong></pre></li>
			</ol>
			<p>We now have a simple controllable behavior; however, it mentions the <code>control_image_behavior.html</code> template. We need to provide that.</p>
			<h3 id="_idParaDest-322">Making the control template</h3>
			<p>This template, in <code>templates/control_image_behavior.html</code>, is the <a id="_idIndexMarker810"/>same as the one before, but with two important differences, shown here in bold:</p>
			<pre>&lt;html&gt;
    &lt;head&gt;
        <strong class="bold">&lt;script src="img/jquery-3.3.1.min.js"&gt;&lt;/script&gt;</strong>
        &lt;title&gt;Robot Image Server&lt;/title&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;h1&gt;Robot Image Server&lt;/h1&gt;
        &lt;img src="img/{{ url_for('display') }}"&gt;&lt;br&gt;
<strong class="bold">        &lt;a href="#" onclick="$.post('/control', {'command': 'exit'}); "&gt;Exit&lt;/a&gt;</strong>
    &lt;/body&gt;
&lt;/html&gt;</pre>
			<p>The differences are as follows:</p>
			<ul>
				<li>In this template, we load a library in our browser called <code>jquery</code>, which is handy for interactive web pages. There is great documentation <a id="_idIndexMarker811"/>for jQuery at <a href="https://api.jquery.com/">https://api.jquery.com/</a>. </li>
				<li>We have the image and header that we saw before, but new to this code is an <code>a</code> tag (for anchor), which when clicked will post the <code>exit</code> command to the <code>'/control'</code> route on <a id="_idIndexMarker812"/>our web app. <code>&lt;br&gt;</code> just creates a line break to show the exit link below the image.</li>
			</ul>
			<p>If you wanted to run this where internet access is difficult, you would need the server to serve the <code>jquery</code> library. This template tells the browser to download <code>jquery</code> directly from the internet. </p>
			<p>Now we have the components, we should try running our controllable behavior.</p>
			<h3 id="_idParaDest-323">Running the controllable image server</h3>
			<p>Now we have the <a id="_idIndexMarker813"/>components, let's get this running and try out the commands:</p>
			<ol>
				<li value="1">To run the image server, you need to upload all three files: <p>a) <code>image_app_core.py</code></p><p>b) <code>control_image_behavior.py</code></p><p>c) <code>templates/control_image_behavior.html</code>. </p></li>
				<li>On your Pi, use <code>python3 control_image_behavior.py</code> to start the process.</li>
				<li>Point your browser at <code>http://myrobot.local:5001</code> (or the address of your robot). You will see the pictures again. </li>
				<li>If you click on the <strong class="bold">Exit</strong> link below the image, this will send a control instruction to your app, which should gracefully quit.</li>
			</ol>
			<p>You've now seen how to get image data from a behavior while sending control data back to the behavior. With the <a id="_idIndexMarker814"/>control and streaming technique tested and ready, and a framework to use for it, we can build a more interesting behavior. In the next section, we'll make the robot follow an object with a specific color.</p>
			<h1 id="_idParaDest-324"><a id="_idTextAnchor300"/>Following colored objects with Python</h1>
			<p>Now we have <a id="_idIndexMarker815"/>some basics ready; we can use this to build <a id="_idIndexMarker816"/>some more interesting behaviors. </p>
			<p>We will create a behavior that will chase, but not get too close to, a colored object. This behavior will make the robot seem very intelligent. We will revisit color models, covered in <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em>. We'll add color masking and filtering and use the OpenCV contours tools to detect the largest blob of color in an image and point the robot at it.</p>
			<p>Building the color-chasing behavior requires a few steps. Let's start with a diagram showing an overview of this whole behavior in <em class="italic">Figure 13.12</em>:</p>
			<div><div><img src="img/B15660_13_12.jpg" alt="" width="1598" height="1001"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.12 – The color-tracking behavior</p>
			<p>The flow of data in <em class="italic">Figure 13.12</em> starts from <strong class="bold">camera images</strong>. These go through <strong class="bold">visual processing</strong> to <strong class="bold">get object info from image</strong>. <strong class="bold">get object info from image</strong> outputs the <a id="_idIndexMarker817"/>object's size (based on the radius of a circle around it) and the object's position (the middle of the enclosing circle) and puts frames on the image queue for the web app/browser.</p>
			<p>The <a id="_idIndexMarker818"/>object size goes <a id="_idIndexMarker819"/>into a speed <strong class="bold">Proportional Integral Derivative</strong> (<strong class="bold">PID</strong>) controller, which <a id="_idIndexMarker820"/>also has an object size reference as its set point. Depending on the difference between the expected size and actual size, this PID will output a speed for the motors, optimizing the radius to be the same as the reference size. That way, the robot will maintain a distance from an object of a known size. This is a base speed for both motors.</p>
			<p>The object position has an <code>x</code> component and a <code>y</code> component. This behavior will turn to center the object, so we are interested in the <code>x</code> coordinate. The <code>x</code> coordinate goes into a PID for controlling the direction/heading. This PID takes a reference position—the center of the camera viewport. This direction PID will produce an output to try and get the difference between these coordinates to zero. By adding to one motor's speed and reducing the other's speed, the robot will turn to face the object (or, if you swap them for fun, it'll turn away!).</p>
			<p>The images are <a id="_idIndexMarker821"/>sent, via an image queue using the app core, to <a id="_idIndexMarker822"/>the browser. A detail not shown in the diagram is the control queue with messages to start the motors, stop the motors, and exit the behavior.</p>
			<p>The final part <a id="_idIndexMarker823"/>of this system, and probably the most interesting, is the color tracking. The box labeled <strong class="bold">get object info from image</strong> performs the tracking. Let's see how that works next.</p>
			<h2 id="_idParaDest-325"><a id="_idTextAnchor301"/>Turning a picture into information</h2>
			<p>We are using <a id="_idIndexMarker824"/>colored pins from a kids' bowling set. They come in nice, bright, primary colors. I will use green as an example. We start with just a picture. However, a set of transformations to the data is needed to turn the picture into information the robot can use to make decisions. </p>
			<p>A pipeline is a good way to design a set of transformations. Let's look at the color tracking as an image processing pipeline in <em class="italic">Figure 13.13</em>:</p>
			<div><div><img src="img/B15660_13_13.jpg" alt="" width="1062" height="700"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.13 – Getting color object information from a camera</p>
			<p>As with other pipelines, <em class="italic">Figure 13.13</em> starts from the camera. This is converted to a low resolution to keep things fast. The figure shows a camera image above the step.</p>
			<p>The process converts the output from the image capture to HSV, the colorspace we mentioned in <a href="B15660_09_Final_ASB_ePub.xhtml#_idTextAnchor171"><em class="italic">Chapter 9</em></a>, <em class="italic">Programming RGB Strips in Python</em>. We use HSV because it means the process can filter colors in a specific range of hues, by their light (very dark objects may confuse us), and by <a id="_idIndexMarker825"/>saturation, so it won't include almost-gray objects. RGB (or BGR) images are tricky to filter, as getting the different light and saturation levels of a particular hue (say, the blues) is not viable. The figure shows the hue color wheel above this step.</p>
			<p>OpenCV has a function, <code>cv2.cvtColor</code>, to convert whole images between colorspaces. Note that OpenCV uses 0–179 for the hue range, instead of 0–359. This is so it fits in a byte (0–255), but you can convert hue values by simply dividing by 2 if you know the value you want.</p>
			<p>After converting to HSV, we then filter the colors in the image with a mask, highlighting pixels in a particular range. It will output white if the object is in the range, or black if it's not. Above this step, the unshaded region on the hue color wheel shows the range, with the masked output next to it. There is a function in OpenCV to do this: <code>cv2.inRange</code>. This gives us a very easy binary output, a masked image, to draw around for our system.</p>
			<p>Our pipeline then uses the contours system to draw around our masked image. The contour specifies only the boundary points of our object. OpenCV provides a <code>cv2.findContours</code> function to do exactly this, which returns a list of shapes, each defined by its outlines. The preceding figure shows the contours (taken from the mask) drawn onto the raw image. Note how light and shade have made the bottom of the bowling pin a bit rough as it doesn't quite fit the mask.</p>
			<p>The processing pipeline then takes the contours (outlines) and uses <code>cv2.minEnclosingCircle</code> to draw circles around them. We will then have some circles, described by a center <code>x</code>, <code>y</code> coordinate, and radius. The preceding figure shows these circles projected on the raw image.</p>
			<p>Our object may <a id="_idIndexMarker826"/>have highlights, producing more than one circle, and other objects may also produce smaller circles. We are only interested in one, the largest of these, so we can loop through the circles, and keep only the largest. Above the <strong class="bold">get the largest circle</strong> step is the raw <a id="_idIndexMarker827"/>image with only the largest circle drawn.</p>
			<p>This largest circle's coordinates and radius give us enough information for our robot to start chasing an object. Above this last step is just the circle, with crosshairs showing its position.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A caveat about red objects: we will use green because red is slightly tricky, as it requires two masks. The hues for red cross a boundary between 179 (the upper limit of our hue range) and 0 (the lower limit), so we would have to mask the image twice and then combine these with an <code>or</code> operation. You could use the <code>cv2.bitwise_or</code> function to try masking red.</p>
			<p>Now we have examined how the pipeline will work and its caveats. We've seen how this pipeline will fit with PID controllers to create an interesting behavior. Let's build this code.</p>
			<h2 id="_idParaDest-326"><a id="_idTextAnchor302"/>Enhancing the PID controller</h2>
			<p>We are going to be using <a id="_idIndexMarker828"/>more PID controllers. We still don't require the differential component, but we will develop an issue with our integral component building up while the motors take time to move. The integral has a sum that starts to grow if there is a constant error. It is good to correct for that error but it can result in a large overshoot. This overshoot, due to the integral still growing after the robot has started to <a id="_idIndexMarker829"/>react, albeit slowly, is called <strong class="bold">integral windup</strong>.</p>
			<p>We can prevent this sum from getting too large by introducing a windup limit to our PID:</p>
			<ol>
				<li value="1">Open up the <code>pid_controller.py</code> file and make the changes in bold in the following snippet. First, add the <code>windup_limit</code> parameter, which defaults to <code>None</code> if you don't set a limit:<pre>class PIController(object):
    def __init__(self, proportional_constant=0, integral_constant=0, <strong class="bold">windup_limit=None</strong>):
        self.proportional_constant = proportional_constant
        self.integral_constant = integral_constant
        <strong class="bold">self.windup_limit = windup_limit</strong>
        self.integral_sum = 0</pre></li>
				<li>We want to prevent our integral growth if we have a limit and hit it. Our integral will change if any of the following occurs:<p>a) There is no windup limit (you set it to <code>None</code>).</p><p>b) The absolute value of the sum is below the windup limit.</p><p>c) The sign of the error would reduce the sum (by being opposed to it).</p><p>This prevents us from going above the limit if there is one.</p><p>Let's see this in code—this code will replace the previous <code>handle_integral </code>method:</p><pre>    <strong class="bold">def handle_integral(self, error):</strong>
<strong class="bold">        if self.windup_limit is None or \</strong>
<strong class="bold">                (abs(self.integral_sum) &lt; self.windup_limit) or \</strong>
<strong class="bold">                ((error &gt; 0) != (self.integral_sum &gt; 0)):</strong>
<strong class="bold">            self.integral_sum += error</strong>
<strong class="bold">        return self.integral_constant * self.integral_sum</strong></pre></li>
				<li>We can <code>start</code> and <code>stop</code> this behavior from the web page. If we start moving again, we won't want <a id="_idIndexMarker830"/>the PIDs to carry old values. Let's add a <code>reset</code> function to zero out the integral sum:<pre><strong class="bold">    def reset(self):</strong>
<strong class="bold">        self.integral_sum = 0</strong></pre></li>
			</ol>
			<p>The PID controller is now able to reset and has a windup limit to stop big overshoots. Let's build the other behavior components that use it.</p>
			<h2 id="_idParaDest-327"><a id="_idTextAnchor303"/>Writing the behavior components</h2>
			<p>This behavior <a id="_idIndexMarker831"/>has two files—a template to pass to our app core with the <a id="_idIndexMarker832"/>control buttons, and then the main behavior code. Let's start by writing the template.</p>
			<h3 id="_idParaDest-328">Writing the control template</h3>
			<p>This template is for the <a id="_idIndexMarker833"/>stream app, with some different controls:</p>
			<ol>
				<li value="1">Copy the template from <code>templates/control_image_behavior.html</code> to <code>templates/color_track_behavior.html</code>. </li>
				<li>We will add two further controls to this, <code>start</code> and <code>stop</code>, displayed here in bold:<pre>        &lt;img src="img/{{ url_for('display') }}"&gt;&lt;br&gt;
<strong class="bold">        &lt;a href="#" onclick="$.post('/control', {'command': 'start'});"&gt;Start&lt;/a&gt;</strong>
<strong class="bold">        &lt;a href="#" onclick="$.post('/control', {'command': 'stop'})"&gt;Stop&lt;/a&gt;&lt;br&gt;</strong>
        &lt;a href="#" onclick="$.post('/control', {'command': 'exit'});"&gt;Exit&lt;/a&gt;</pre><p>We intend to run the <a id="_idIndexMarker834"/>program with the robot stopped first, so we can tune in with our phone or browser, see what the robot is detecting, and click the <strong class="bold">Start</strong> button to get it moving.</p></li>
			</ol>
			<p>With the template modified, we will need to write the behavior code next.</p>
			<h3 id="_idParaDest-329">Writing the behavior code</h3>
			<p>We'll put this new behavior <a id="_idIndexMarker835"/>in a file called <code>color_track_behavior.py</code>: </p>
			<ol>
				<li value="1">It's no surprise that we start with the imports. Because we are bringing together many elements, there are quite a few, but we have seen them all before:<pre><strong class="bold">import time</strong>
<strong class="bold">from image_app_core import start_server_process, get_control_instruction, put_output_image</strong>
<strong class="bold">import cv2</strong>
<strong class="bold">import numpy as np</strong>
<strong class="bold">import camera_stream</strong>
<strong class="bold">from pid_controller import PIController</strong>
<strong class="bold">from robot import Robot</strong></pre></li>
				<li>Now, we add the <code>Behavior</code> class to find and get close to a colored object. We pass this the <code>robot</code> object:<pre><strong class="bold">class ColorTrackingBehavior:</strong>
<strong class="bold">    def __init__(self, robot):</strong>
<strong class="bold">        self.robot = robot</strong></pre></li>
				<li>These values are intended to be tuned for the color mask and object size:<pre><code>low_range</code> and <code>high_range</code> values for the color filter (as seen in <em class="italic">Figure 13.13</em>). Colors that lie between these HSV ranges would be white in the masked image. Our hue is 25 to 80, which correspond to 50 to 160 degrees on a hue wheel. Saturation is 70 to 255—any lower and we'd start to detect washed out or gray colors. Light is 25 (very dark) to 255 (fully lit).</p><p>The <code>correct_radius</code> value sets the size we intend to keep the object at and behaves as a <a id="_idIndexMarker836"/>distance setting. <code>center</code> should be half the horizontal resolution of the pictures we capture.</p></li>
				<li>The last member variable set here is <code>running</code>. This will be set to <code>True</code> when we want the robot to be moving. When set to <code>False</code>, the processing still occurs, but the motors and PIDs will stop:<pre><strong class="bold">        self.running = False</strong></pre></li>
				<li>The next bit of code is to process any control instructions from the web app:<pre><code>start</code>, <code>stop</code>, and <code>exit</code> buttons. It uses the <code>running</code> variable to start or stop the robot moving. </p></li>
				<li>Next, we have the code that will find an object from a frame. This implements the pipeline shown in <em class="italic">Figure 13.13</em>. We'll break this function down a bit, though:<pre><strong class="bold">    def find_object(self, original_frame):</strong>
<strong class="bold">        """Find the largest enclosing circle for all contours in a masked image.</strong>
<strong class="bold">        Returns: the masked image, the object coordinates, the object radius"""</strong></pre><p>Because <a id="_idIndexMarker837"/>this code is complex, we have a documentation string or <strong class="bold">docstring</strong> explaining what it does and what it returns.</p></li>
				<li>Next, the method converts the frame to HSV, so it can be filtered using <code>inRange</code> to leave only the <code>masked</code> pixels from our frame:<pre><strong class="bold">        frame_hsv = cv2.cvtColor(original_frame, cv2.COLOR_BGR2HSV)</strong>
<strong class="bold">        masked = cv2.inRange(frame_hsv, self.low_range, self.high_range)</strong></pre></li>
				<li>Now that we have the masked image, we can draw contours (outline points) around it:<pre><code>RETR_LIST</code>. OpenCV is capable of more detailed types, but they take more time to process. </p><p>The last parameter is the method used to find the contours. We use the <code>CHAIN_APPROX_SIMPLE</code> method to simplify the outline to an approximate chain of points, such as four points for a rectangle. Note the <code>_</code> in the return values; there is optionally a hierarchy returned here, but we neither want nor use it. The <code>_</code> means ignore the hierarchy return value.</p></li>
				<li>The next thing is to find all the enclosing circles for each contour. We use a tiny loop to do this. The <code>minEnclosingCircle</code> method gets the smallest circle that entirely encloses all points in a contour:<pre><code>cv2</code> returns each circle as a radius and coordinates—exactly what we want. </p></li>
				<li>However, we <a id="_idIndexMarker838"/>only want the biggest one. Let's filter for it:<pre><code>largest</code> value of <code>0</code>, and then we loop through the circles. If the circle has a radius larger than the circle we last stored, we replace the stored circle with the current circle. We also convert the values to <code>int</code> here, as <code>minEnclosingCircle</code> produces non-integer floating-point numbers.</p></li>
				<li>We end this method by returning the masked image, the largest coordinates, and the largest radius:<pre><strong class="bold">        return masked, largest[0], largest[1]</strong></pre></li>
				<li>Our next method will take an original frame and processed frame, then turn them into a dual-screen display (two images of the same scale joined together horizontally) on the output queue through to the web app:<pre><code>np.concatenate</code> function to join the two images, which are equivalent to NumPy arrays. You could change the <code>axis</code> parameter to <code>0</code> if you wanted screens stacked vertically instead of horizontally.</p></li>
				<li>The next method processes a frame of data through the preceding functions, finding the <a id="_idIndexMarker839"/>objects and setting the display. It then returns the object info as follows:<pre><code>cvtColor</code> to change the masked image to a three-channel image—the original frame and processed frame must use the same color system to join them into a display. We use <code>cv2.circle</code> to draw a circle around the tracked object on the original frame so we can see what our robot has tracked on the web app, too.</p></li>
				<li>The next method is the actual behavior, turning the preceding coordinates and radius into robot movements. When we start our behavior, the pan-and-tilt mechanism may not be pointing straight forward. We should ensure that the mechanism is facing forward by setting both servos to <code>0</code>, then start the camera:<pre><strong class="bold">    def run(self):</strong>
<strong class="bold">        self.robot.set_pan(0)</strong>
<strong class="bold">        self.robot.set_tilt(0)</strong>
<strong class="bold">        camera = camera_stream.setup_camera()</strong></pre></li>
				<li>While the servos are moving and the camera is warming up, we can prepare the two PID controllers we need for speed (based on radius) and direction (based on distance from the horizontal middle):<pre><strong class="bold">        speed_pid = PIController(proportional_constant=0.8, </strong>
<strong class="bold">            integral_constant=0.1, windup_limit=100)</strong>
<strong class="bold">        direction_pid = PIController(proportional_constant=0.25, </strong>
<strong class="bold">            integral_constant=0.05, windup_limit=400)</strong></pre><p>These values I arrived at through much tuning; you may find you need to tune these further. The <em class="italic">Tuning the PID controller settings </em>section will cover how to tune the PIDs.</p></li>
				<li>Now we wait a <a id="_idIndexMarker840"/>little while for the camera and pan-and-tilt servos to settle, and then we turn off the servos in the center position:<pre><strong class="bold">        time.sleep(0.1)</strong>
<strong class="bold">        self.robot.servos.stop_all()</strong></pre></li>
				<li>We let the user know, with a <code>print</code> statement, and output some debug headers:<pre><strong class="bold">        print("Setup Complete")</strong>
<strong class="bold">        print('Radius, Radius error, speed value, direction error, direction value')</strong></pre></li>
				<li>We can then enter the main loop. First, we get the processed data from the frame. Notice we use brackets to unpack <code>coordinates</code> into <code>x</code> and <code>y</code>:<pre><strong class="bold">        for frame in camera_stream.start_stream(camera):</strong>
<strong class="bold">            (x, y), radius = self.process_frame(frame)</strong></pre></li>
				<li>We should check our control messages at this point. We then check whether we are allowed to move, or whether there is any object big enough to be worth looking for. If there is, we can start as follows:<pre><strong class="bold">            self.process_control()</strong>
<strong class="bold">            if self.running and radius &gt; 20:</strong></pre></li>
				<li>Now we know the robot should be moving, so let's calculate error values to feed the PID controllers. We get the size error and feed it into the speed PID to get speed values:<pre><strong class="bold">               radius_error = self.correct_radius - radius</strong>
<strong class="bold">                speed_value = speed_pid.get_value(radius_error)</strong></pre></li>
				<li>We use the center coordinate and current object, <code>x</code>, to calculate a direction error, feeding that into the direction PID:<pre><strong class="bold">                direction_error = self.center - x</strong>
<strong class="bold">                direction_value = direction_pid.get_value(direction_error)</strong></pre></li>
				<li>So we can <a id="_idIndexMarker841"/>debug this; we print a debug message here matching with the headers shown before:<pre><strong class="bold">                print(f"{radius}, {radius_error}, {speed_value:.2f}, {direction_error}, {direction_value:.2f}")</strong></pre></li>
				<li>We can use the speed and direction values to produce left and right motor speeds:<pre><strong class="bold">                self.robot.set_left(speed_value - direction_value)</strong>
<strong class="bold">                self.robot.set_right(speed_value + direction_value)</strong></pre></li>
				<li>We've handled what to do when the motors are running. If they are not, or there is no object worth examining, then we should stop the motors. If we have hit the <strong class="bold">Stop</strong> button, we should also reset the PIDs, so they do not accumulate odd values:<pre><strong class="bold">            else:</strong>
<strong class="bold">                self.robot.stop_motors()</strong>
<strong class="bold">                if not self.running:</strong>
<strong class="bold">                    speed_pid.reset()</strong>
<strong class="bold">                    direction_pid.reset()</strong></pre></li>
				<li>We have now finished that function and the <code>ColorTrackingBehavior</code> class. Now, all that is left is to set up our behavior and web app core, then start them:<pre><strong class="bold">print("Setting up")</strong>
<strong class="bold">behavior = ColorTrackingBehavior(Robot())</strong>
<strong class="bold">process = start_server_process('color_track_behavior.html')</strong>
<strong class="bold">try:</strong>
<strong class="bold">    behavior.run()</strong>
<strong class="bold">finally:</strong>
<strong class="bold">    process.terminate()</strong></pre></li>
			</ol>
			<p>This behavior code is <a id="_idIndexMarker842"/>built and ready to run. You've seen how to convert the image, then mask it for a particular color, and how to draw around the blobs in the mask, and then find the largest one. I've also shown you how to turn this visual processing into robot moving behavior by feeding this data through PIDs and using their output to control motor movements. Let's try it out!</p>
			<h2 id="_idParaDest-330"><a id="_idTextAnchor304"/>Running the behavior</h2>
			<p>I'm sure you are keen to see this <a id="_idIndexMarker843"/>working and fix any problems that there are. Let's get into it:</p>
			<ol>
				<li value="1">To run this behavior, you will need to upload <code>color_track_behavior.py</code>, the modified <code>pid_controller.py</code> file, and the template at <code>templates/color_track_behavior.html</code>. I'll assume that you already have <code>robot.py</code> and the other supporting files uploaded.</li>
				<li>Start the app with <code>python3 color_track_behavior.py</code>, which will start the web server and wait. </li>
				<li>At this point, you should use your browser to connect to <code>http://myrobot.local:5001</code>, and you should be able to see your robot's image feed.<p>You can see the object <a id="_idIndexMarker844"/>and its circle, along with links to control the robot, as shown in the screenshot in <em class="italic">Figure 13.14</em>:</p><div><img src="img/B15660_13_14.jpg" alt="" width="818" height="486"/></div><p class="figure-caption">Figure 13.14 – The color-tracking web app</p><p><em class="italic">Figure 13.14</em> shows a screenshot of our app server running the code to track a colored object. Under the address bar and heading is a dual-screen type output. The left has the direct feed from the camera, with a kids' green bowling pin close to the middle and a blue circle outlining the pin, generated by the behavior to show it's tracking the largest matching object. On the right is the mask's output, so we can see what aspects of the image match and tune the mask values if we need to. Under this are <strong class="bold">Start</strong>, <strong class="bold">Stop</strong>, and <strong class="bold">Exit</strong> links, to start the motors, stop the motors, and exit the program.</p></li>
				<li>To make the robot start moving, press the <strong class="bold">Start</strong> button on the web page.<p>When the robot starts moving, you will see the PID debug output in the console (PuTTY). This will only show when the robot is running. </p></li>
				<li>You can press the <strong class="bold">Stop</strong> button <a id="_idIndexMarker845"/>on the web page to stop the robot moving or the <strong class="bold">Exit</strong> button to exit the behavior.</li>
			</ol>
			<p>The robot won't be moving quite right; the movements may be understeering or overshooting. You'll need to tune the PID controllers to get this right, as shown in the next section.</p>
			<p>Tuning the PID controller settings</p>
			<p>I start with a proportional <a id="_idIndexMarker846"/>constant of 0.1, and raise it, using <code>nano</code> to make quick edits on the Pi, until the robot starts to overshoot—that is, it goes past its target, then returns far back—then I halve this proportional constant value.</p>
			<p>It may then have a constant error, so I start raising the integral constant by about 0.01 to counter this error. Tuning PIDs is a slow process: start by getting the object close to dead center and tuning <code>direction_pid</code> until it's pretty good, then come back for <code>speed_pid</code>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Do not try to tweak all the values at once—rather, change one thing and retry.</p>
			<p>For a deeper look at this, see <em class="italic">Tuning a PID controller</em> in the <em class="italic">Further reading</em> section.</p>
			<h2 id="_idParaDest-331"><a id="_idTextAnchor305"/>Troubleshooting</h2>
			<p>Color <a id="_idIndexMarker847"/>tracking is a tricky behavior, and there are some things that can go wrong:</p>
			<ul>
				<li>If the motors stop or slow down, the simplest fix is to use fresh batteries. </li>
				<li>If there are syntax errors, please check your code carefully.</li>
				<li>Ensure that the web app examples work with the camera and that you troubleshoot any problems there.</li>
				<li>You will need good lighting, as the mask may not pick up poorly lit objects.  </li>
				<li>Beware of other objects in the view that may match; the mask may pick up things other than your intended items.</li>
				<li>Use the web app to check your object is in view and that the mask shows your object mostly in white. If not, then you may need to tune the upper and lower HSV ranges. The hue is the factor most likely to cause problems, as the saturation and value ranges are quite permissive.</li>
				<li>If the robot starts weaving from side to side, you may need to tune the direction PID. Reduce the proportional element somewhat. </li>
				<li>If the robot barely turns, you can increase the proportional element a little.</li>
				<li>If the robot is stopped but not facing the detected object, then increase the integral element for the direction PID by about 0.01. If you see the same problems moving back and forward, try applying the same tweaks.</li>
			</ul>
			<p>You've seen how to track a brightly colored object with a camera, a technique you can use to spot objects in a room, or by industrial robots to detect ripe fruit. It is quite impressive to watch. However, some objects are more subtle than just a color, for example, a human face. In the next section, we look at how to use cascading feature matches to pick out objects.</p>
			<h1 id="_idParaDest-332"><a id="_idTextAnchor306"/>Tracking faces with Python</h1>
			<p>Detecting <a id="_idIndexMarker848"/>faces (or other objects) by features is a smart behavior. Once our <a id="_idIndexMarker849"/>robot is detecting faces, it will point the pan-and-tilt mechanism at the nearest (well, largest) face.</p>
			<p>Using <strong class="bold">Haar cascades</strong> is a common <a id="_idIndexMarker850"/>technique, well documented in a paper by Paul Viola and Michael Jones (known as <em class="italic">Viola Jones</em>). In essence, it means using a cascade of feature matches to search for a matching object. We will give an overview of this technique, then put it into use on our robot to create a fun behavior. Using different cascade model files, we could pick out faces or other objects. </p>
			<h2 id="_idParaDest-333"><a id="_idTextAnchor307"/>Finding objects in an image</h2>
			<p>We will be using an <a id="_idIndexMarker851"/>algorithm implemented in OpenCV as a single and useful function, which makes it very easy to use. It provides a simple way to detect objects. More advanced and complex methods involve machine learning, but many systems use Haar cascades, including camera apps on phones. Our code will convert the images into grayscale (black through gray to white) for this detection method. Each pixel here holds a number for the intensity of light.</p>
			<p>First, let's dig into a way of representing these images: integral images. </p>
			<h3 id="_idParaDest-334">Converting to integral images</h3>
			<p>There are two <a id="_idIndexMarker852"/>stages applied in the function. The first is to produce an <strong class="bold">integral</strong> image, or <strong class="bold">summed-area table</strong>, as shown in <em class="italic">Figure 13.15</em>:</p>
			<div><div><img src="img/B15660_13_15.jpg" alt="" width="1137" height="521"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.15 – Integral images and summed-area tables</p>
			<p>The left side of <em class="italic">Figure 13.15</em> shows a <em class="italic">smiling face</em> type image, with numeric pixels representing shades, with larger numbers making for a lighter color. Every shade has a number. </p>
			<p>On the right of <em class="italic">Figure 13.15</em> is the integral image. Each pixel in the integral image is the sum or <strong class="bold">integral</strong> of the previous pixels. It <a id="_idIndexMarker853"/>adds itself to the original pixels above and left of it. The coordinate 2,2 is circled. It is the last in a 3x3 grid. The cell here has the value 44. 44 is the sum of the pixels in the highlighted box (9 + 9 + 5 + 9 + 5 + 1 + 5 + 1 + 0). </p>
			<p>When the code sums the pixels, the integral process can use a shortcut and use the previous sums. The new sum is equal to the pixel to the left plus the pixel above. For example, for a pixel much further down (8,8), also circled in the image, we could add all the numbers, but it will be faster to reuse the results we already have. We can take the pixel value (1), add the sum above (166), and add the sum to the left (164). This sum will have included the middle pixels twice, so we need to subtract those, so take away the value up and to the left (146). The sum for this would be 1 + 164 + 166 – 146 = 185. The computer can do this pretty quickly.</p>
			<p>This creates an array of numbers with the same dimensions as the image. Each coordinate is the sum of all the pixels' intensities between the current coordinate and 0,0.</p>
			<p>Code can use the integral image to quickly find the intensity sum of any rectangle in it, of any size. You can start with the bottom-right pixel of the image, then subtract the top-right one, leaving the sum of pixels below the top-right pixel. We also then want to subtract the bottom-left pixel. This nearly constrains the sum to only the rectangle's pixels, but we would have taken away sections above the top-left pixel twice. To correct this, add back the value of the top-left pixel:</p>
			<div><div><img src="img/B15660_13_001.jpg" alt="" width="1650" height="67"/>
				</div>
			</div>
			<p>The equation <a id="_idIndexMarker854"/>works for a small rectangle of 2x2 or a large 300x200 rectangle. See the Viola Jones paper in the <em class="italic">Further reading</em> section for more details. The good news is, you don't need to write this code as it's already part of the OpenCV classifier. The cascade stage can use this integral image to perform its next potent trick quickly.</p>
			<h3 id="_idParaDest-335">Scanning for basic features</h3>
			<p>The <a id="_idIndexMarker855"/>next part of this puzzle is scanning the image for features. The features are extremely simple, involving looking for the difference between two rectangles, so they are quick to apply. <em class="italic">Figure 13.16</em> shows a selection of these basic features:</p>
			<div><div><img src="img/B15660_13_16.jpg" alt="" width="477" height="434"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.16 – Simple rectangular feature types</p>
			<p>The top left of <em class="italic">Figure 13.16</em> shows a left/right feature, where the left pixels are set to <strong class="bold">1</strong> and the right set to <strong class="bold">0</strong> (and shaded). This will match a vertical contrast feature. The figure's top right has two rows of <strong class="bold">0</strong>s (shaded), two rows of <strong class="bold">1</strong>s, and then two further rows of shaded <strong class="bold">0</strong>s; this will match a horizontal bar feature. The middle left has the top three rows set to <strong class="bold">1</strong>s and the lower three rows shaded and set to <strong class="bold">0</strong>s, matching a horizontal contrast <a id="_idIndexMarker856"/>feature. The figure's middle right has two columns of shaded <strong class="bold">0</strong>s, followed by two columns of <strong class="bold">1</strong>s, and then two further rows of shaded <strong class="bold">0</strong>s; this will match a vertical bar feature. </p>
			<p>The bottom image shows a feature with the first few rows as three <strong class="bold">1</strong>s followed by three <strong class="bold">0</strong>s. It follows these rows with three rows of three <strong class="bold">0</strong>s andt three <strong class="bold">1</strong>s. This makes a small checkerboard pattern that will match a feature with diagonal contrast.</p>
			<p>The algorithm will apply rectangles like those from <em class="italic">Figure 13.16</em> in a particular order and relative locations, then each match will <em class="italic">cascade</em> to a further attempt to match another feature. Files describe objects as a set of features. There are face cascades with 16,000 features to apply. Applying every single one to every part of an image would take a long time. So they are applied in groups, starting perhaps with just one. If a feature check fails, that part of the image is not subject to further feature tests. Instead, they cascade into later group tests. The groups include weighting and applying groups of these features at different angles.</p>
			<p>If all the feature checks pass, then the checked region is taken as a match. For this to work, we need to find the feature cascade that will identify our object. Luckily, OpenCV has such a file designed for face recognition, and we have already installed it on our Raspberry Pi.</p>
			<p>This whole operation of applying the summed area, then using the cascade file to look for potential matches, is all available through two OpenCV operations:</p>
			<ul>
				<li><code>cv2.CascadeClassifier(cascade_filename)</code> will open the given cascade file, which describes the features to test. The file only needs to be loaded once and can be used on all the frames. This is a constructor and returns a <code>CascadeClassifier</code> object.</li>
				<li><code>CascadeClassifier.detectMultiScale(image)</code> applies the classifier check to an image.</li>
			</ul>
			<p>You <a id="_idIndexMarker857"/>now have a basic understanding of a common face (and object) recognition technique. Let's use cascade classifier visual processing with our existing behavior experience to plan the face-tracking behavior. </p>
			<h2 id="_idParaDest-336"><a id="_idTextAnchor308"/>Planning our behavior</h2>
			<p>We can use code fairly <a id="_idIndexMarker858"/>similar to our color-tracking behavior to track faces. We'll set our robot up to use the pan-and-tilt mechanism to follow the largest face seen in the camera. The block diagram in <em class="italic">Figure 13.17</em> shows an overview of the face behavior:</p>
			<div><div><img src="img/B15660_13_17.jpg" alt="" width="1416" height="897"/>
				</div>
			</div>
			<p class="figure-caption">Figure 13.17 – The face-tracking behavior</p>
			<p>The flow in <em class="italic">Figure 13.17</em> will look very familiar. We have the same camera to visual behavior to image queue we've seen before. This time, the <a id="_idIndexMarker859"/>visual processing is <code>x</code> and <code>y</code> coordinate for the item. We feed position <code>x</code> into a PID with center <code>x</code> to get a pan position, which is then used by the pan servo <a id="_idIndexMarker860"/>motor. Position <code>y</code> is fed to a PID with center <code>y</code> and outputs a tilt position to the tilt servos. The servos move the camera, creating a feedback loop where the view moves.</p>
			<p>The differences are in the data we are sending to the PID controllers, and that each PID controls a different servo motor.</p>
			<p>Now we have a plan; let's write the code.</p>
			<h2 id="_idParaDest-337"><a id="_idTextAnchor309"/>Writing face-tracking code</h2>
			<p>The code for this <a id="_idIndexMarker861"/>behavior will seem very familiar—adapting the previous behavior code for this purpose. It's possible that refactoring could yield more common code, but it is currently simpler to work with a copy for now. This code will go into the <code>face_track_behavior.py</code> file. I've not even created a new template, as the color track template will work just fine for this:</p>
			<ol>
				<li value="1">The imports are nearly the same as our <code>color_track_behavior</code>:<pre><strong class="bold">import time</strong>
<strong class="bold">from image_app_core import start_server_process, get_control_instruction, put_output_image</strong>
<strong class="bold">import cv2</strong>
<strong class="bold">import os</strong>
<strong class="bold">import camera_stream</strong>
<strong class="bold">from pid_controller import PIController</strong>
<strong class="bold">from robot import Robot</strong></pre></li>
				<li>The <code>init</code> function for the behavior class is slightly different, starting with loading the Haar cascade. There are many other cascade files in the same directory, with which <a id="_idIndexMarker862"/>you could try to track things other than a face. This code uses <code>assert</code> to verify that the file exists at the location here because OpenCV will instead return cryptic errors in <code>detectMultiscale</code> if it cannot find it:<pre><strong class="bold">class FaceTrackBehavior:</strong>
<strong class="bold">    def __init__(self, robot):</strong>
<strong class="bold">        self.robot = robot</strong>
<strong class="bold">        cascade_path = "/usr/local/lib/python3.7/dist-packages/cv2/data/haarcascade_frontalface_default.xml"</strong>
<strong class="bold">        assert os.path.exists(cascade_path), f"File {cascade_path} not found"</strong>
<strong class="bold">        self.cascade = cv2.CascadeClassifier(cascade_path)</strong></pre></li>
				<li>The tuning parameters have center positions and a minimum face size. I've also brought the PID controllers out to the class, so they can be tuned here, and then reset in the control handler (you could add the reset to the previous behavior too):<pre><strong class="bold">        self.center_x = 160</strong>
<strong class="bold">        self.center_y = 120</strong>
<strong class="bold">        self.min_size = 20</strong>
<strong class="bold">        self.pan_pid = PIController(proportional_constant=0.1, integral_constant=0.03)</strong>
<strong class="bold">        self.tilt_pid = PIController(proportional_constant=-0.1, integral_constant=-0.03)</strong></pre></li>
				<li>Our constructor still tracks whether the behavior is running motors or not:<pre><strong class="bold">        self.running = False</strong></pre></li>
				<li>The process control here differs; when the <code>stop</code> instruction is received, it stops the <a id="_idIndexMarker863"/>motors and resets the PIDs:<pre><strong class="bold">    def process_control(self):</strong>
<strong class="bold">        instruction = get_control_instruction()</strong>
<strong class="bold">        if instruction:</strong>
<strong class="bold">            command = instruction['command']</strong>
<strong class="bold">            if command == "start":</strong>
<strong class="bold">                self.running = True</strong>
<strong class="bold">            elif command == "stop":</strong>
<strong class="bold">                self.running = False</strong>
<strong class="bold">                self.pan_pid.reset()</strong>
<strong class="bold">                self.tilt_pid.reset()</strong>
<strong class="bold">                self.robot.servos.stop_all()</strong>
<strong class="bold">            elif command == "exit":</strong>
<strong class="bold">                print("Stopping")</strong>
<strong class="bold">                exit()</strong></pre></li>
				<li>This behavior still has a <code>find_object</code> method, taking the original frame. First, we convert the image to grayscale to reduce the amount of data to search:<pre><strong class="bold">    def find_object(self, original_frame):</strong>
<strong class="bold">        gray_img = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)</strong></pre></li>
				<li>Next, we use the grayscale image with the cascade <code>detectMultiScale</code> method to get a list of matches:<pre><code>detectMultiScale</code> method creates the integral image and applies the Haar cascade algorithm. It will return several objects as rectangles, with <code>x</code>, <code>y</code>, width, and height values. </p></li>
				<li>We can use a loop similar to the color-tracking behavior to find the largest rectangle by area. First, we need to set up a store for the current largest rectangle, in <a id="_idIndexMarker864"/>a data structure holding the area, then a sub-list containing the <code>x</code>, <code>y</code>, width, and height:<pre><strong class="bold">        largest = 0, (0, 0, 0, 0) </strong>
<strong class="bold">        for (x, y, w, h) in objects:</strong>
<strong class="bold">            item_area = w * h</strong>
<strong class="bold">            if item_area &gt; largest[0]:</strong>
<strong class="bold">                largest = item_area, (x, y, w, h)</strong></pre></li>
				<li>We return the position and dimensions of that largest rectangle:<pre><strong class="bold">        return largest[1]</strong></pre></li>
				<li>The <code>make_display</code> method is simpler than the color-tracking behavior, as there is only one image. It must still encode the image, though:<pre><strong class="bold">    def make_display(self, display_frame):</strong>
<strong class="bold">        encoded_bytes = camera_stream.get_encoded_bytes_for_frame(display_frame)</strong>
<strong class="bold">        put_output_image(encoded_bytes)</strong></pre></li>
				<li>The <code>process_frame</code> method finds the object and then draws a rectangle on the frame for output. The <code>cv2.rectangle</code> function takes two coordinates: a starting <code>x</code>,<code>y</code> and an ending <code>x</code>,<code>y</code>, along with a color value. To get the ending coordinates, we need to add the width and height back in:<pre><strong class="bold">    def process_frame(self, frame):</strong>
<strong class="bold">        (x, y, w, h) = self.find_object(frame)</strong>
<strong class="bold">        cv2.rectangle(frame, (x, y), (x + w, y + w), [255, 0, 0])</strong>
<strong class="bold">        self.make_display(frame)</strong>
<strong class="bold">        return x, y, w, h</strong></pre></li>
				<li>Now comes the <code>run</code> function. We start with the camera setup and warm-up time:<pre><strong class="bold">    def run(self):</strong>
<strong class="bold">        camera = camera_stream.setup_camera()</strong>
<strong class="bold">        time.sleep(0.1)</strong>
<strong class="bold">        print("Setup Complete")</strong></pre></li>
				<li>Like the color-tracking behavior, we start the main loop by processing the frame and checking for control instructions:<pre><strong class="bold">        for frame in camera_stream.start_stream(camera):</strong>
<strong class="bold">            (x, y, w, h) = self.process_frame(frame)</strong>
<strong class="bold">            self.process_control()</strong></pre></li>
				<li>We <a id="_idIndexMarker865"/>only want to be moving if we've detected a large enough object (using height, as faces tend to be bigger in this dimension) and if the robot is running:<pre><strong class="bold">            if self.running and h &gt; self.min_size:</strong></pre></li>
				<li>When we know the robot is running, we feed the PIDs and send the output values straight to the servo motors for both pan and tilt. Note that to find the middle of the object, we take the coordinate and add half its width or height:<pre><strong class="bold">                pan_error = self.center_x - (x + (w / 2))</strong>
<strong class="bold">                pan_value = self.pan_pid.get_value(pan_error)</strong>
<strong class="bold">                self.robot.set_pan(int(pan_value))</strong>
<strong class="bold">                tilt_error = self.center_y - (y + (h /2))</strong>
<strong class="bold">                tilt_value = self.tilt_pid.get_value(tilt_error)</strong>
<strong class="bold">                self.robot.set_tilt(int(tilt_value))</strong></pre></li>
				<li>So that we can track what is going on here, a debug <code>print</code> statement is recommended:<pre><strong class="bold">                print(f"x: {x}, y: {y}, pan_error: {pan_error}, tilt_error: {tilt_error}, pan_value: {pan_value:.2f}, tilt_value: {tilt_value:.2f}")</strong></pre></li>
				<li>Finally, we need to add the code to set up and run our behavior. Notice that we still use the <a id="_idIndexMarker866"/>color-tracking template:<pre><strong class="bold">print("Setting up")</strong>
<strong class="bold">behavior = FaceTrackBehavior(Robot())</strong>
<strong class="bold">process = start_server_process('color_track_behavior.html')</strong>
<strong class="bold">try:</strong>
<strong class="bold">    behavior.run()</strong>
<strong class="bold">finally:</strong>
<strong class="bold">    process.terminate()</strong></pre></li>
			</ol>
			<p>With the code ready, including the setup functions, we can try it out and see the behavior running.</p>
			<h2 id="_idParaDest-338"><a id="_idTextAnchor310"/>Running the face-tracking behavior</h2>
			<p>To run this <a id="_idIndexMarker867"/>behavior, you will need to have uploaded the color-tracking behavior files already: </p>
			<ol>
				<li value="1">Upload the <code>face_track_behavior.py</code> file.</li>
				<li>Start using <code>$ python3 face_track_behavior.py</code>.</li>
				<li>Send your browser to <code>http://myrobot.local:5001</code>. You should see a single frame of the camera, with a rectangular outline around the largest face.</li>
				<li>You must press the <strong class="bold">Start</strong> button for the robot to move.</li>
			</ol>
			<p>The servo motors on the pan-and-tilt mechanism should move to try and put your face in the middle of the screen, which will mean the camera is pointed right at you. If you move your head around, the camera will (slowly) follow you. If you have someone stand behind you, the behavior <a id="_idIndexMarker868"/>won't pick them up, but if you cover half your face with your hand, it will stop recognizing you, and turn to their face instead.</p>
			<h2 id="_idParaDest-339"><a id="_idTextAnchor311"/>Troubleshooting</h2>
			<p>Start <a id="_idIndexMarker869"/>with the troubleshooting steps that we covered for the previous behavior—that should get you most of the way—then try these if you need to:</p>
			<ul>
				<li>If the app fails to find the Haar cascade file, check the location for the files there. These files have moved between OpenCV packaging versions and may do so again. Check that you haven't mistyped it. If not, then try the following command:<pre><strong class="bold">$ find /usr/ -iname "haarcas*"</strong></pre><p>This command should show the location of the files on the Raspberry Pi.</p></li>
				<li>If the camera fails to detect faces in the picture, try making sure the area is well lit. </li>
				<li>The detection algorithm is only for faces that face the camera head-on, and anything obscuring a part of the face will fool it. It is a little picky, so glasses and hats may confuse it.</li>
				<li>Faces only partially in the frame are also likely to be missed. Faces that are too far away or small are filtered. Reducing the minimum parameter will pick up more objects but generate false positives from tiny face-like objects.</li>
				<li>Please check the indentation matches, as this can change the meaning of where things happen in Python.</li>
			</ul>
			<p>You have now made code that will detect and track faces in a camera view. Face-tracking behavior is sure to be impressive. Let's summarize what we've seen in this chapter.</p>
			<h1 id="_idParaDest-340"><a id="_idTextAnchor312"/>Summary</h1>
			<p>In this chapter, you saw how to set up the Raspberry Pi Camera module. You then used it to see what your robot sees—the robot's view of the world.</p>
			<p>You got the robot to display its camera as a web app on a phone or desktop, and then used the camera to drive smart color- and face-tracking behaviors. I've suggested ways the behaviors could be enhanced and hopefully given you a taste of what computer vision can do.</p>
			<p>In the next chapter, we will extend our object-tracking visual processing to follow lines with the camera, seeing further ways to use the camera.</p>
			<h1 id="_idParaDest-341"><a id="_idTextAnchor313"/>Exercises</h1>
			<p>This code is fun, but there are many ways you could improve the behaviors. Here are some suggested ways to extend this code and deepen your learning:</p>
			<ul>
				<li>Use the control pipeline to allow a user to tune the color filters, correct radius, and PID values from the web page. Perhaps the initial PID values should be close to the other tunable values?</li>
				<li>There is quite a lot of setup code. Could you put this into a function/method?</li>
				<li>Could the queues to the web page be used to send the debug data to the page, instead of printing them in the console? Could the data be plotted in a graph?</li>
				<li>The field of view for tracking with the Pi Camera is pretty narrow. A wide-angle lens would improve the field of view a lot, letting the robot see more.</li>
				<li>The camera doesn't perform too well when it's dark. The robot has an LED strip, but it's not illuminating much. Could you add a bright LED as a headlamp for the camera?</li>
				<li>You could track other objects by trying the other cascade files found in the <code>/usr/share/opencv/haarcascades</code> folder on the Raspberry Pi. </li>
				<li>Perhaps you could try swapping features of the two behaviors to use the servo motors to track the colored object, or chase the faces?</li>
				<li>Could you combine the pan-and-tilt mechanism with the main wheels to track an object, then engage the main wheels to chase the matching face and aim to center the pan while keeping the object in view? This may require some fancy PID controller thinking.</li>
			</ul>
			<p>With these ideas, you should have plenty of ways to dig further into this type of visual processing.</p>
			<h1 id="_idParaDest-342"><a id="_idTextAnchor314"/>Further reading</h1>
			<p>Visual processing is a deep topic, so this is only a small selection of places where you can read more about using a camera for visual processing:</p>
			<ul>
				<li><em class="italic">The Official Raspberry Pi Camera Guide</em> at <a href="https://magpi.raspberrypi.org/books/camera-guide">https://magpi.raspberrypi.org/books/camera-guide</a> is an excellent resource for getting to know the camera, with many practical projects for it.</li>
				<li>To delve in far greater depth into using the Raspberry Pi Camera, I recommend the PiCamera documentation, available at <a href="https://picamera.readthedocs.io/">https://picamera.readthedocs.io/</a>.</li>
				<li>To gain insight into further techniques, the PyImageSearch website, at <a href="https://www.pyimagesearch.com">https://www.pyimagesearch.com</a>, has great resources.</li>
				<li>OpenCV and visual processing is a complex topic, only briefly covered here. I recommend <em class="italic">OpenCV 3 Computer Vision with Python Cookbook</em>, by <em class="italic">Alexey Spizhevoy</em> and <em class="italic">Aleksandr Rybnikov</em>, from <em class="italic">Packt Publishing</em>, available at <a href="https://www.packtpub.com/application-development/opencv-3-computer-vision-python-cookbook">https://www.packtpub.com/application-development/opencv-3-computer-vision-python-cookbook</a>, for more information.</li>
				<li>Streaming video through Flask is a neat trick and is explored further in <em class="italic">Video Streaming with Flask</em>, at <a href="https://blog.miguelgrinberg.com/post/video-streaming-with-flask">https://blog.miguelgrinberg.com/post/video-streaming-with-flask</a>.</li>
				<li>I recommend <a href="https://flaskbook.com/">https://flaskbook.com/</a> for other neat ways to use Flask to manage your robot from your phone or laptop.</li>
				<li>Tuning a PID controller—we touched on this in <a href="B15660_11_Final_ASB_ePub.xhtml#_idTextAnchor219"><em class="italic">Chapter 11</em></a>, <em class="italic">Programming Encoders with Python</em>, and needed more in this chapter. <em class="italic">Robots For Roboticists</em> | <em class="italic">PID Control</em>, available at <a href="http://robotsforroboticists.com/pid-control/">http://robotsforroboticists.com/pid-control/</a>, is a little heavy on the math but has an excellent section on manually tuning a PID.</li>
				<li><em class="italic">Rapid Object Detection Using a Boosted Cascade of Simple Features</em>, by Paul Viola and Michael Jones, available at <a href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf">https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf</a>. This paper, from 2001, discusses in more detail the Haar cascade object-finding technique that we used.</li>
				<li>A good video introducing face tracking is <em class="italic">Detecting Faces (Viola Jones Algorithm) – Computerphile</em>, available at <a href="https://www.youtube.com/watch?v=uEJ71VlUmMQ">https://www.youtube.com/watch?v=uEJ71VlUmMQ</a>, which dives into the combination of techniques used.</li>
				<li>The cascade classification OpenCV documentation, at <a href="https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html">https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html</a>, gives a reference for the library functions used in the face-tracking behavior.</li>
				<li>OpenCV also has a tutorial on face tracking (for version 3.0), called <em class="italic">OpenCV: Face Detection using Haar Cascades</em>, which is available at <a href="https://docs.opencv.org/3.3.0/d7/d8b/tutorial_py_face_detection.html">https://docs.opencv.org/3.3.0/d7/d8b/tutorial_py_face_detection.html</a>.</li>
			</ul>
		</div>
	</div></body></html>