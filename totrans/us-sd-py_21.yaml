- en: '21'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diffusion Model Transfer Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This book is mainly focused on using Stable Diffusion with Python, and when
    doing so, we will need to fine-tune a model for our specific needs. As we discussed
    in previous chapters, there are many ways to customize the model, such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlocking UNet to fine-tune all parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a textual inversion to add new keyword embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locking UNet and training a LoRA model for customized styles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a ControlNet model to guide image generation with control guidance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an adaptor to use the image as one of the guidance embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is impossible to cover all the model training topics in simply one chapter.
    Another book would be needed to discuss the details of model training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, we still want to use this chapter to drill down to the core concepts
    of model training. Instead of listing sample code on how to fine-tune a diffusion
    model, or using the scripts from the `Diffusers` package, we want to introduce
    you to the core concepts of training so that you fully understand the common training
    process. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the foundations of training a model by training a linear model from
    scratch using PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the Hugging Face Accelerate package to train a model in multiple
    GPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building code to train a Stable Diffusion V1.5 LoRA model using PyTorch and
    Accelerator step by step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be familiar with the overall training process
    and key concepts, and you’ll be able to read sample code from other repositories
    and build your own training code to customize a model from a pre-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Writing code to train one model is the best way to learn how to train a model.
    Let’s start work on it.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a model requires more GPU power and VRAM than model inference. Prepare
    a GPU with at least 8 GB of VRAM – the more, the better. You can also train a
    model using multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is recommended to install the latest version of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the specified packages with the versions I used to write the code
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The training code was tested in the Ubuntu 22.04 x64 version.
  prefs: []
  type: TYPE_NORMAL
- en: Training a neural network model with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The target of this section is to build and train one simple neural network model
    using PyTorch. The model will be a simple one-layer model, with no additional
    fancy layers. It is simple but with all the elements required to train a Stable
    Diffusion LoRA, as we will see later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to skip this section if you are familiar with PyTorch model training.
    If it is your first time to start training a model, this simple model training
    will help you thoroughly understand the process of model training.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting, make sure you have installed all the required packages mentioned
    in the *Technical* *requirements* section.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s assume we want to train a model with four weights and output one digital
    result show, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: y = w 1 × x 1 + w 2 × x 2 + w 3 × x 3 + w 4 × x 4
  prefs: []
  type: TYPE_NORMAL
- en: 'The four weights, w 1, w 2, w 3, w 4, are the model weights we want to have
    from the training data (Think of these weights as the Stable Diffusion model weight).
    Because we need to have some real data to train the model, I will use the weights
    `[2,3,4,7]` to generate some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create 10 groups of input sample data, `x_sample`; each `x_sample` is
    an array with four elements, the same length as the weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the following section, we will use a neural network model to predict a list
    of weights; for the sake of training, let’s assume that the true weights are unknown
    after generating the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code snippet, we utilize `numpy` to leverage its dot product
    operator, `@`, to compute the output, `y`. Now, let’s generate `y_list` containing
    `10` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can print `x_list` and `y_list` to take a look at the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Our training data is ready; there’s no need to download anything else. Next,
    let’s define the model itself and prepare for training.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our model could be the world’s simplest model ever, a simple linear dot product,
    as defined in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `torch.randn(4)` code is to generate a tensor with a four-weight number.
    No other code is needed; our NN model is ready now, named `MyLinear`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a model, we will need to initialize it, similar to initializing random
    weights in an LLM or diffusion model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Almost all neural network model training follows these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Forward a pass to predict the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the difference between the predicted result and the ground truth, known
    as the loss value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform backpropagation to calculate the gradient loss value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before kicking off the training, define a loss function and an optimizer. The
    loss function, `loss_fn`, will help calculate a loss value based on the predicted
    result and ground truth result. `optimizer` will be used to update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`lr` represents the learning rate, a crucial hyperparameter to set. Determining
    the best **learning rate** (**lr**) often involves trial and error, depending
    on the characteristics of your model, dataset, and problem. To find a reasonable
    learning rate, you need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start with a small learning rate**: A common practice is to start with a
    small learning rate, such as 0.001, and gradually increase or decrease it based
    on the observed convergence behavior.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use learning rate schedules**: You can use learning rate schedules to adjust
    the learning rate dynamically during training. One common approach is step decay,
    where the learning rate decreases after a fixed number of epochs. Another popular
    method is exponential decay, in which the learning rate decreases exponentially
    over time. (We won’t use it in the world’s simplest model.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, don’t forget to convert the input and output to the torch Tensor object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: All the preparations are done, so let’s start training a model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will set the epoch number to 100, which means looping through our training
    data 100 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`y_pred = model(x)`: This line applies the model to the current input data
    sample, `x`, generating a prediction, `y_pred`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss = loss_fn(y_pred,y_output[i])`: This line calculates the loss (also known
    as the error or cost) by comparing the predicted output, `y_pred`, with the actual
    output, `y_output[i]` , using a specified loss function, `loss_fn`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer.zero_grad()`: This line resets the gradients calculated during the
    backward pass to zero. This is important because it prevents gradient values from
    carrying over between different samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss.backward()`: This line performs the backpropagation algorithm, computing
    gradients for all parameters with respect to the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer.step()`: This line updates the model’s parameters based on the computed
    gradients and the chosen optimization method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Putting all the code together and running it, we will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The loss value converges quickly and approaches `0` after `100` epochs. Execute
    the following code to see the current weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that the weights update as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is quite close to `[2,3,4,7]`! The model was successfully trained to find
    the right weight numbers.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Stable Diffusion and multiple GPU training, we can get help from
    the Hugging Face Accelerate package [4]. Let’s start using `Accelerate` next.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model with Hugging Face’s Accelerate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hugging Face’s `Accelerate` is a library that provides a high-level API over
    different PyTorch distributed frameworks, aiming to simplify the process of distributed
    and mixed-precision training. It is designed to keep changes to your training
    loop to a minimum and allow the same functions to work for any distributed setup.
    Let’s see what `Accelerate` can bring to the table.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Hugging Face’s Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s apply `Accelerate` to our simple but working model. Accelerate is designed
    to be used together with PyTorch, so we don’t need to change too much code. Here
    are the steps to use `Accelerate` to train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate the default configuration file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize an `Accelerate` instance, and send the model instance and data to
    the device managed by Accelerate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace `loss.backward` with `accelerator.backward(loss)` :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we will update the training code using Accelerate.
  prefs: []
  type: TYPE_NORMAL
- en: Putting code together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will keep all the other code the same; here is the complete training code
    except for the data preparations and model initializing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code, we should get the same output as when we run the
    training model without the Hugging Face `Accelerate` library. And the loss value
    converges as well.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model with multiple GPUs using Accelerate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many types of multiple GPU training; in our case, we will use the
    data parallel style [1]. Simply put, we will load the whole model data into each
    GPU and split the training data across multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In PyTorch, we can achieve this with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For the world’s simplest model, we will load the whole model to each GPU and
    split the 10 groups’ training data into 5 groups each. Each GPU will take five
    groups of data at the same time. After each step, all loss gradient numbers will
    be merged using the `allreduce` operation. The `allreduce` operation simply added
    all the loss data from all GPUs, added it up, and then sent it back to each GPU
    to update the weights, as shown in the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Accelerate will launch two independent processes to train. To avoid creating
    two training datasets, let’s generate one dataset and save it to local storage
    using the `pickle` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, wrap the whole model and training code in a `main` function and save
    it in a new Python file named `train_model_in_2gpus.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, start the training using this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If so, congratulations! You just successfully trained an AI model in two GPUs.
    With the knowledge you’ve learned, let’s now start to train a Stable Diffusion
    V1.5 LoRA.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Stable Diffusion V1.5 LoRA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Hugging Face document provides complete guidance on training a LoRA by calling
    a pre-defined script [2] provided by Diffusers. However, we don’t want to stop
    at “using” the script. The training code from Diffusers includes a lot of edge-case
    handling and additional code that is hard to read and learn. In this section,
    we will write up each line of the training code to fully understand what happens
    in each step.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sample, we will use eight images with associated captions to
    train a LoRA. The image and image captions are provided in the `train_data` folder
    of the code for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our training code structure will be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Right below the `main()` function, we initialize the `accelerate` instance.
    The `Accelerator` instance is initialized with two hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gradient_accumulation_steps`: This is the number of training steps to accumulate
    gradients before we update the model parameters. Gradient accumulation allows
    you to effectively train with a larger batch size than would be possible with
    a single GPU, while still fitting the model parameters in memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mixed_precision`: This specifies the precision to use during training. The
    `"fp16"` value means that half-precision floating point values will be used for
    the intermediate computations, which can lead to faster training times and lower
    memory usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `Accelerator` instance also has an attribute device, which is the device
    (GPU or CPU) on which the model will be trained. The device attribute can be used
    to move the model and tensors to the appropriate device before training.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s start defining hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Defining training hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyperparameters are parameters that are not learned from the data but, instead,
    are set before the commencement of the learning process. They are user-defined
    settings that govern the training process of a machine learning algorithm. In
    our LoRA training case, we will have the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the preceding settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`output_dir`: This is the directory where the model outputs will be saved.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pretrained_model_name_or_path`: This is the name or path of the pretrained
    model to be used as the starting point for training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_rank`: This is the number of layers in the `32` might not be effective
    enough, while ranks above `256` might be overkill for most tasks. In our case,
    since we use only eight images to train the LoRA, setting the rank to `4` is enough.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lora_alpha`: This, conversely, controls the strength of the updates made to
    the pretrained model’s weights during fine-tuning. Specifically, the weight changes
    generated during fine-tuning are multiplied by a scaling factor equal to Alpha
    divided by Rank, before being added back to the original model weights. Therefore,
    increasing Alpha relative to Rank. Setting Alpha equal to Rank is a common starting
    practice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: This parameter controls how quickly the model learns from
    its mistakes during training. Specifically, it sets the step size for each iteration,
    determining how aggressively the model adjusts its parameters to minimize the
    `loss` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adam_beta1` and `adam_beta2`: These are the parameters used in the Adam optimizer
    to control the decay rates of the moving averages of the gradient and squared
    gradient, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adam_weight_decay`: This is the weight decay used in the Adam optimizer to
    prevent overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`adam_epsilon`: This is a small value added to the denominator for numerical
    stability in the Adam optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_name`: This is the name of the dataset to be used for training. Particularly,
    this is the Hugging Face dataset ID, such as `lambdalabs/pokemon-blip-captions`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_data_dir`: This is the directory where the training data is stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`top_rows`: This is the number of rows used for training. It is used to select
    the top rows for training; if you have a dataset with 1,000 rows, set it to 8
    to train the training code with the top 8 rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_dir`: This is the directory where the outputs will be saved during
    training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resolution`: This is the resolution of the input images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`center_crop`: This is a Boolean flag indicating whether to perform center
    cropping on the input images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`random_flip`: This is a Boolean flag indicating whether to perform random
    horizontal flipping on the input images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_batch_size`: This is the batch size used during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gradient_accumulation_steps`: This is the number of training steps to accumulate
    gradients before updating the model parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_train_epochs`: This is the number of training epochs to perform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_scheduler_name`: This is the name of the learning rate scheduler to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_grad_norm`: This is the maximum norm of the gradients to clip to prevent
    exploding gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diffusion_scheduler`: This is the name of the diffusion scheduler to use.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the Stable Diffusion components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When training a LoRA, the process involves inference, adding the loss value,
    and backpropagation - a procedure reminiscent of the inference process. To facilitate
    this, let’s use the `StableDiffusionPipeline` from `Diffusers` package to get
    `tokenizer`, `text_encoder`, `vae`, and `unet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'During LoRA training, those components will facilitate the forward pass, but
    their weights won’t be updated during backpropagation, so we need to set `requires_grad_`
    to `False`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The LoRA weights are the part we want to train; let’s use PEFT’s [3] `LoraConfig`
    to initialize the LoRA configurations.
  prefs: []
  type: TYPE_NORMAL
- en: '`PEFT` is a library developed by Hugging Face that provides parameter-efficient
    ways to adapt large pre-trained models to specific downstream applications. The
    key idea behind PEFT is to fine-tune only a small fraction of a model’s parameters
    instead of fine-tuning all of them, resulting in significant savings in terms
    of computation and memory usage. This makes it possible to fine-tune very large
    models even on consumer hardware with limited resources.'
  prefs: []
  type: TYPE_NORMAL
- en: LoRA is one of the PEFT methods supported by the PEFT library. With LoRA, instead
    of updating all the weights of a given layer during fine-tuning, only a low-rank
    approximation of the weight updates is learned, reducing the number of additional
    parameters required per layer. This approach allows you to fine-tune just 0.16%
    of the total parameters of a model while achieving similar performance to full
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use LoRA with a pre-trained transformer model, you need to instantiate a
    `LoraConfig` object and pass it to the appropriate component of your model. The
    `LoraConfig` class has several attributes that control its behavior, including
    the dimension/rank of the decomposition, dropout rates, and other hyperparameters.
    Once configured, you can then train your model using standard techniques, such
    as gradient descent. Here is the code to create a LoRA configuration object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s add the LoRA adapter to the UNet model using the `unet_lora_config`
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Inside the `for` loop, if the parameters require gradients (i.e., they are trainable),
    their data type is explicitly cast to `torch.float32`. This ensures that only
    the trainable parameters are in the `float32` format for efficient training.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s load up some data using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`if dataset_name:`: If `dataset_name` is provided, the code tries to load a
    dataset from Hugging Face’s dataset hub using the `load_dataset` function. If
    no `dataset_name` is provided, it assumes that the dataset is stored locally and
    loads it using the `imagefolder` dataset type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_data = dataset["train"]`: The train split of the dataset is assigned
    to the `train_data` variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset["train"] = train_data.select(range(top_rows))`: The first top rows
    of the train dataset are selected and assigned back to the train split of the
    dataset. This is useful when working with a small subset of the dataset for faster
    experimentation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataset_columns = list(dataset["train"].features.keys())`: The keys of the
    `dataset["train"]` feature dictionary are extracted and assigned to the `dataset_columns`
    variable. These keys represent the image and caption columns in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_column, caption_column = dataset_columns[0], dataset_columns[1]`: The
    first and second columns are assigned to the `image_column` and `caption_column`
    variables, respectively. This assumes that the dataset has exactly two columns
    – the first for images and the second for captions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will need a function to convert the input text to token IDs; we define the
    function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we train the data transform pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code defines a set of image transformations that will be applied
    to the training dataset during the training of a machine learning or deep learning
    model. These transformations are defined using the `transforms` module from the
    `PyTorch` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of what each line does:'
  prefs: []
  type: TYPE_NORMAL
- en: '`transforms.Compose()`: This is a function that “chains” multiple transformations
    together. It takes a list of transformation functions as input and applies them
    in order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR)`:
    This line resizes the image to the given resolution pixels while keeping the aspect
    ratio. The interpolation method used is bilinear interpolation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.CenterCrop(resolution) if center_crop else transforms.RandomCrop(resolution)`:
    This line crops the image to a square of resolution x resolution. If `center_crop`
    is `True`, the crop is taken from the center of the image. If `center_crop` is
    `False`, the crop is taken randomly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.RandomHorizontalFlip() if random_flip else transforms.Lambda(lambda
    x: x)`: This line horizontally flips the image randomly with a probability of
    0.5\. If `random_flip` is `False`, it leaves the image unchanged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.ToTensor()`: This line converts the image from a PIL image or NumPy
    array to a PyTorch tensor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transforms.Normalize([0.5], [0.5])`: This line scales the pixel values of
    the image between -1 and 1\. It is commonly used to normalize image data before
    passing it to a neural network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By chaining these transformations together using `transforms.Compose`, you can
    easily preprocess your image data and apply multiple transformations to your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need the following code to use the chained transformation object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code first defines a function called `preprocess_train`, which
    preprocesses the train data. It first converts the images to the RGB format, and
    then it applies a series of image transformations (resize, center/random crop,
    random horizontal flip, and normalization) to them using the `train_transforms`
    object. It then tokenizes the input captions using the `tokenize_captions` function.
    The resulting preprocessed data is added to the `examples` dictionary as the `pixel_values`
    and `input_ids` keys.
  prefs: []
  type: TYPE_NORMAL
- en: The with `accelerator.main_process_first()` line is used to ensure that the
    code inside the block is executed only in the main process. In this case, it sets
    the training transforms for `train_dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: The `collate_fn` function is used to collate the dataset examples into a batch
    to be fed to the model. It takes a list of examples and stacks `pixel_values`
    and `input_ids` together. The resulting tensors are then converted to the `float32`
    format and returned as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `train_dataloader` is created using the `torch.utils.data.DataLoader`
    class, which loads `train_dataset` with the specified batch size, shuffle, and
    collate functions.
  prefs: []
  type: TYPE_NORMAL
- en: In PyTorch, DataLoader is a utility class that abstracts the process of loading
    data in batches for training or evaluation. It is used to load data in batches,
    which are sequences of data points used to train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: In the provided code, `train_dataloader` is an instance of PyTorch’s `DataLoader`
    class. It is used to load the training data in batches. More specifically, it
    loads the data from `train_dataset` in batches of a predefined batch size, shuffles
    the data for each epoch, and applies a user-defined `collate_fn` function to preprocess
    the data before feeding it to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '`train_dataloader` is necessary for the efficient training of the model. By
    loading data in batches, it allows the model to process multiple data points in
    parallel, which can significantly reduce training time. Additionally, shuffling
    the data for each epoch helps prevent overfitting by ensuring that the model sees
    different data points in each epoch.'
  prefs: []
  type: TYPE_NORMAL
- en: In the provided code, the `collate_fn` function is used to preprocess the data
    before it is fed to the model. It takes a list of examples and returns a dictionary
    containing the pixel values and input IDs for each example. The `collate_fn` function
    is applied to each batch of data by `DataLoader` before it is fed to the model.
    This allows for more efficient processing of the data by applying the same preprocessing
    steps to each batch of data.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the training components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To prepare and define the training components, let’s first initialize an `AdamW`
    optimizer. `AdamW` is an optimization algorithm to train machine learning models.
    It is a variant of the popular `Adam` optimizer, which uses adaptive learning
    rates for each model parameter. The `AdamW` optimizer is similar to the `Adam`
    optimizer, but it includes an additional weight decay term in the gradient update
    step. This weight decay term is added to the gradient of the loss function during
    optimization, which helps to prevent overfitting by adding a regularization term
    to the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can initialize an `AdamW` optimizer using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `filter` function is used to iterate through all the parameters of the `unet`
    model and selects only those parameters that require gradient computation. The
    `filter` function returns a generator object that contains the parameters that
    require gradient computation. This generator object is assigned to the `lora_layers`
    variable, which will be used to optimize the model parameters during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `AdamW` optimizer is initialized with the following hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lr`: The learning rate, which controls the step size at each iteration while
    moving toward a minimum of a loss function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`betas`: A tuple containing the exponential decay rates for the moving average
    of the gradient (β1) and the squared gradient (β2)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_decay`: The weight decay term added to the gradient of the loss function
    during optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eps`: A small value added to the denominator to improve numerical stability'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, we define a learning rate scheduler – `lr_scheduler`. Instead of defining
    one manually, we can use the `get_scheduler` function provided by the `Diffusers`
    package (`from diffusers.optimization` `import get_scheduler`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This code creates a learning rate scheduler object using the `get_scheduler`
    function from the `Diffusers` library. The learning rate scheduler determines
    how the learning rate (i.e., the step size in gradient descent) changes during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get_scheduler` function takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`lr_scheduler_name`: The name of the learning rate scheduler algorithm to use.
    In our sample, the name is `constant`, defined at the beginning of the code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`optimizer`: The PyTorch optimizer object that the learning rate scheduler
    will be applied to. This is the `AdamW` optimizer we just initialized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have just prepared all the elements to kick off training and we have written
    lots of code to prepare the dataset, although the actual training code isn’t that
    long. Let’s write the training code next.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Stable Diffusion V1.5 LoRA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training a LoRA will usually take a while, and we’d better create a progress
    bar to track the training progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the core training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is a typical training loop for Stable Diffusion model training.
    Here’s a breakdown of what each part of the code does:'
  prefs: []
  type: TYPE_NORMAL
- en: The outer loop (`for epoch in range(num_train_epochs)`) iterates over the number
    of training epochs. An epoch is one complete pass through the entire training
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet.train()` sets the model to training mode. This is important because some
    layers, such as dropout and batch normalization, behave differently during training
    and testing. In the training phase, these layers behave differently than in the
    evaluation phase. For example, dropout layers will drop out nodes with a certain
    probability during training to prevent overfitting, but they will not drop out
    any nodes during evaluation. Similarly, `BatchNorm` layers will use batch statistics
    during training, but will use accumulated statistics during evaluation. So, if
    you don’t call `unet.train()`, these layers will not behave correctly for the
    training phase, which could lead to incorrect results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inner loop (`for step, batch in enumerate(train_dataloader)`) iterates over
    the training data. `train_dataloader` is a `DataLoader` object that provides batches
    of training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 1*, the model encodes the input images into a latent space using a
    `latents`), which are scaled by a factor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 2*, random noise is added to the latent vectors. This noise is sampled
    from a standard normal distribution and has the same shape as the latent vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 3*, random timesteps are sampled for each image in the batch. This
    is part of a time-dependent noise addition process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 4*, the text encoder is used to get the text embedding for conditioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 5*, noise is added to the latent vectors according to the noise magnitude
    at each timestep.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 6*, the target for the loss calculation is determined based on the
    prediction type. It can be either the noise or the velocity of the noise.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *steps 7* and *8*, The model makes a prediction using the noisy latent vectors,
    the timesteps, and the text embeddings. The loss is then calculated as the mean
    squared error between the model’s prediction and the target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 9*, the loss is gathered across all processes for logging. This is
    necessary in the case of distributed training, where the model is trained on multiple
    GPUs. So that we can see the loss value changes in the middle of a training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 10*, the gradients of the loss with respect to the model parameters
    are computed (`accelerator.backward(loss)`), and the gradients are clipped if
    necessary. This is to prevent the gradients from becoming too large, which can
    cause numerical instability. The optimizer updates the model parameters based
    on the gradients (`optimizer.step()`), and the learning rate scheduler updates
    the learning rate (`lr_scheduler.step()`). The gradients are then reset to zero
    (`optimizer.zero_grad()`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In *step 11*, if the gradients are synchronized, the training loss is reset
    to zero and the progress bar is updated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the training loss, learning rate, and current epoch are logged to monitor
    the training process. The progress bar is updated with these logs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you understand the preceding steps, you can not only train a Stable Diffusion
    LoRA but also train any other models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will need to save the LoRA we just trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break down the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accelerator.wait_for_everyone()`: This line is used in distributed training
    to make sure all processes have reached this point in the code. It’s a synchronization
    point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`if accelerator.is_main_process:`: This checks whether the current process
    is the main one. In distributed training, you typically only want to save the
    model once, not once for each process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet = unet.to(torch.float32)`: This line converts the data type of the model’s
    weights to `float32`. This is typically done to save memory, as `float32` uses
    less memory than `float64` but still provides sufficient precision for most deep
    learning tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unwrapped_unet = accelerator.unwrap_model(unet)`: This unwraps the model from
    the accelerator, which is a wrapper used for distributed training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))`:
    This line gets the state dictionary of the model, which contains the weights of
    the model, and then converts it to a format suitable for Diffusers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weight_name = f"lora_{pretrained_model_name_or_path.split(''/'')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{diffusion_scheduler.__name__}_{formatted_date}.safetensors"`:
    This line creates a name for the file where the weights will be saved. The name
    includes various details about the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StableDiffusionPipeline.save_lora_weights(...)`: This line saves the weights
    of the model to a file. The `save_directory` argument specifies the directory
    where the file will be saved, `unet_lora_layers` is the state dictionary of the
    model, `safe_serialization` indicates that the weights should be saved in a way
    that is safe to load later, and `weight_name` is the name of the file.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accelerator.end_training()`: This line signals the end of the training process.
    This is typically used to clean up resources used during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have the complete training code in the associated code folder for this chapter,
    named `train_sd16_lora.py`. We are not done yet; we still need to kick off the
    training using the `accelerator` command instead of entering `python` `py_file.py`
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: Kicking off the training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have one GPU, simply run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'For two GPUs, increase `--num_processes` to `2`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have more than two GPUs and want to train on assigned GPUs (e.g., you
    have three GPUs and want the training code run on the second and third GPUs),
    use this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the first and third GPUs, simply update the `CUDA_VISIBLE_DEVICES` settings
    to `0,2`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Verifying the result
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the most exciting moment to witness the power of model training. First,
    let’s load up the LoRA but set its weight to `0.0` with `adapter_weights = [``0.0]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code, we will get the images shown in *Figure 21**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 21.1: A toy bike, a macro photo, a 3D game asset, and an image generated
    without using LoRA](img/B21263_21_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.1: A toy bike, a macro photo, a 3D game asset, and an image generated
    without using LoRA'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is not that good. Now, let’s enable the trained LoRA with `adapter_weights
    = [1.0]`. Run the code again, and you should see the images shown in *Figure 21**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 21.2: A toy bike, a macro photo, a 3D game asset, and an image generated\
    \ with LoRA training using eight images\uFEFF](img/B21263_21_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.2: A toy bike, a macro photo, a 3D game asset, and an image generated
    with LoRA training using eight images'
  prefs: []
  type: TYPE_NORMAL
- en: The result is way better than the images without using the LoRA! If you see
    similar results, congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been a long chapter, but learning about the power of model training
    is worth the length. Once we have mastered the training skill, we can train any
    models based on our needs. The whole training process isn’t easy, as there are
    so many details and trivial things to deal with. However, writing the training
    code is the only way to fully understand how model training works; considering
    the fruitful outcome, it is worth spending time to figure it out from the bottom
    up.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the length limitation of one chapter, I can only cover the entire LoRA
    training process, but once you succeed with LoRA training, you can find more training
    samples from Diffusers, change the code based on your specific needs, or simply
    write your training code, especially if you are working on a new model’s architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we began by training one simple model; the model itself isn’t
    that interesting, but it helped you to understand the core steps of model training
    using PyTorch. Then, we moved on to leverage the Accelerator package to train
    a model in multiple GPUs. Finally, we touched on the real Stable Diffusion model
    and trained a full-functioning LoRA, using simply eight images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we’ll discuss something less technical, AI, and
    its relationship with us, privacy, and how to keep pace with its fast-changing
    advancements.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What is **Distributed Data Parallel** (**DDP**): [https://pytorch.org/tutorials/beginner/ddp_series_theory.html](https://pytorch.org/tutorials/beginner/ddp_series_theory.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Launch the LoRA training script: [https://huggingface.co/docs/diffusers/en/training/lora#launch-the-script](https://huggingface.co/docs/diffusers/en/training/lora#launch-the-script'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face PEFT: [https://huggingface.co/docs/peft/en/index](https://huggingface.co/docs/peft/en/index)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hugging Face Accelerate: [https://huggingface.co/docs/accelerate/en/index](https://huggingface.co/docs/accelerate/en/index'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
