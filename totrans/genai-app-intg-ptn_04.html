<html><head></head><body>
<div><div><h1 class="chapterNumber">4</h1>
<h1 class="chapterTitle" id="_idParaDest-46">Generative AI Batch and Real-Time Integration Patterns</h1>
<p class="normal">This chapter covers the two primary patterns for designing systems around <strong class="keyWord">large language models </strong>(<strong class="keyWord">LLMs</strong>) – batch and real-time. The decision to architect a batch or real-time application will depend on the use case you are working on. In general, batch use cases are formulated around generating data to be consumed later. For example, you will leverage LLMs to extract data points from a large corpus of data and then have a step to generate summaries to be consumed by business analysts on a daily basis. In the case of real-time use cases, data will be used as it becomes available. For example, you will leverage LLMs as online agents to answer questions from your customers or employees through a chat or voice interface.</p>
<p class="normal">Diving deeper into batch mode, it involves sending queries in bulk for higher throughput at the cost of latency. This is better suited for long, time-consuming production workloads and large data consumption. Additionally, batch-generated results are not immediately exposed to the end user, which allows the option to review the content through additional pipelines. These pipelines can be integrated before the prompt is sent to the LLM where data cleansing and prompt engineering can happen, or after the response from the LLM is received, where you can augment the reply to match a specific format or add additional data from other data sources.</p>
<p class="normal">The real-time design pattern offers a back-and-forth querying experience at a faster rate. Despite lower throughput, the real-time design pattern provides faster feedback and could be enhanced with a multi-turn memory mechanism to increase awareness of previous requests to the LLM. Real-time inferences are subject to latency requirements and the user experience can be diminished, hence the opportunities to review results are reduced. </p>
<p class="normal">In this case, you can apply some filtering at inference time and apply a <strong class="keyWord">retrieval-augmented generation</strong> (<strong class="keyWord">RAG</strong>) pipeline to augment the user query before it is sent to the LLM. Additionally, keeping a layer of filtering at the entry and exit points can help you keep your application safe.</p>
<p class="normal">We’ll provide example use cases of both batch and real-time querying to highlight the trade-offs. Readers will learn when to use each approach based on factors like scale, cost, and latency requirements.</p>
<p class="normal">We are going to cover the following main topics in this chapter: </p>
<ul>
<li class="bulletList">Batch and real-time integration patterns:<ul>
<li class="bulletList level-2">Batch mode involves sending queries in bulk for higher throughput but higher latency. It is better suited for long-running workloads and large data consumption.</li>
<li class="bulletList level-2">Real-time mode offers back-and-forth querying at a faster rate, providing quicker feedback but with lower throughput. It is better for low-latency requirements.</li>
</ul>
</li>
<li class="bulletList">Different pipeline architectures:<ul>
<li class="bulletList level-2">The implications of batch versus real-time on different components of the integration pipeline, such as entry points, pre-processing, inference, post-processing, and result presentation.</li>
</ul>
</li>
<li class="bulletList">Application integration patterns in the integration framework:<ul>
<li class="bulletList level-2">How the nuances of batch and real-time patterns map to the different stages of the integration framework, including entry point, prompt pre-processing, inference, result post-processing, and result presentation.</li>
</ul>
</li>
<li class="bulletList">Use case example – search enhanced by generative AI (GenAI):<ul>
<li class="bulletList level-2">An example use case of using GenAI to enhance a website search, with document ingestion happening in batch mode and search/response generation in real-time mode.</li>
</ul>
</li>
</ul>
<h1 class="heading-1" id="_idParaDest-47">Batch and real-time integration patterns</h1>
<p class="normal">The first key decision when evaluating batch<a id="_idIndexMarker155"/> versus real-time integration<a id="_idIndexMarker156"/> approaches is around data immediacy – when exactly do you need the GenAI outputs? This boils down to whether you require responsive just-in-time results, like servicing on-demand user queries, versus use cases where insights from model outputs can accumulate over time before getting consumed.</p>
<p class="normal">Let’s illustrate this with an example of RAG, where LLMs evaluate search results<a id="_idIndexMarker157"/> to formulate human-friendly query responses. This is a real-time use case; you need AI-generated answers with minimal latency to deliver a quality user experience in applications like conversational assistants or search engines. The data has to be put into action as it is produced.</p>
<p class="normal">Contrast that with something like automated content generation workflows, for example, extracting metadata from a product catalog. While you still want that content quickly, there’s more flexibility around when model outputs get ingested downstream. You could run generative models in batches, queueing up prompts and processing them asynchronously based on available capacity. The generated texts then flow into your e-commerce databases on their own schedule.</p>
<p class="normal">The real-time interactive integration mode prioritizes low latency and responsive experiences above all else. Users receive AI results virtually instantly through request/response app interfaces. Batch mode disconnects that coupling, sacrificing instantaneous interactivity for higher overall throughput and cost efficiency at scale. Longer-running jobs optimize utilization across pooled models.</p>
<p class="normal">So, the batch versus real-time decision depends on analyzing data freshness requirements. For experiences demanding perceivable instant gratification, like querying information or iterating on creative ideation, you’ll want a request-scoped interactive architecture. But when targets are more about maximizing generative model output volume with flexible latency tolerances, then batching prompts yields better economics. Getting that integration pattern right is key to harnessing GenAI’s value.</p>
<h1 class="heading-1" id="_idParaDest-48">Different pipeline architectures</h1>
<p class="normal">Beyond just the integration pattern itself, the <a id="_idIndexMarker158"/>decision between real-time and batch processing has major implications for the surrounding data pipelines and infrastructure architecture. Pre-processing and post-processing workflows take on very different characteristics optimized for their respective modes.</p>
<p class="normal">For real-time, low-latency use cases such as query answering or conversational AI, lightweight just-in-time pre-processing pipelines are ideal. These handle prompt cleanup, context augmentation, and other steps with minimal overhead before hitting the generative model with a single inference request. The output then flows through a post-processing stage focused on safety filtering, response ranking, and result formatting. These processes need to be optimized because end-to-end latency is critical.</p>
<p class="normal">Real-time pipelines are typically hosted on dynamically scalable containerized infrastructure or serverless cloud environments. Aggressive caching layers and load balancing distribute the request volume across available inference resources. The entire real-time architecture is optimized for instant responsiveness above all else.</p>
<p class="normal">In contrast, batch processing pipelines undertake much heavier pre-processing work by operating on larger data volumes. This could include tasks like topic clustering, semantic search, translation, and more – all before prompting generative models. Those prompts then flow into asynchronous queuing systems that accumulate requests into batches for higher throughput model execution.</p>
<p class="normal">On the output side, heavyweight post-processing pipelines apply content structuring, summarization, quality filtering, legal compliance scans, and various other enrichment tasks. Staging areas like cloud storage and data warehouses temporarily store outputs for analysis before integration into downstream systems proceeds asynchronously.</p>
<p class="normal">Orchestrated by workflow engines or managed batch services, these pipelines aim to maximize total data throughput within cost constraints over an extended time horizon. Unlike real-time systems, latency is an acceptable trade-off for overall higher scalability and parallelism at lower unit costs.</p>
<p class="normal">Decoupled buffering isolates upstream and downstream systems from direct dependencies on GenAI models. This allows linear scaling across distributed worker pools in cloud or on-prem environments. Auto-scaling policies match provisioned resources to demand constantly.</p>
<p class="normal">While architecturally more complex, batch pipelines efficiently produce immense volumes of AI-generated data on structured schedules. Cost calculations compare scaling infrastructure fees versus fixed infrastructure costs. For many high-scale use cases, batching promptly amortizes despite heavy pipeline overhead.</p>
<p class="normal">When evaluating the<a id="_idIndexMarker159"/> trade-offs, the decision between interactive real-time versus offline batch processing covers many architectural requirements. Real-time optimizes for low latency while batch yields higher overall throughput within more flexible time windows. Thoughtful pipeline design aligning to data demands and generation use cases is critical for overall system performance and cost efficiency.</p>
<h1 class="heading-1" id="_idParaDest-49">Application integration patterns in the integration framework</h1>
<p class="normal">Previously, we dove into the high-level architectural trade-offs between real-time and batch-oriented integration approaches for GenAI models. But how do the nuances of these patterns map to the different components within our integration framework’s stages? The following diagram depicts these steps:</p>
<figure class="mediaobject"><img alt="" height="139" src="img/B22175_04_01.png" width="824"/></figure>
<p class="packt_figref">Figure 4.1: The GenAI application integration framework</p>
<p class="normal">Let’s walk through each step in the next sections.</p>
<h2 class="heading-2" id="_idParaDest-50">Entry point</h2>
<p class="normal">When it comes to the <a id="_idIndexMarker160"/>Entry Point stage, the priorities for real-time versus batch systems differ significantly based on the end user experience. For real-time interactive applications, the entry points where prompts originate need to be highly streamlined with simplicity and ease of use in mind. After all, these inputs will be directly exposed to human users and drive the instantaneous AI responses they receive.</p>
<p class="normal">As such, real-time prompting<a id="_idIndexMarker161"/> surfaces should be optimized for clean, user-friendly design, search bars, focused chat windows, straightforward voice interfaces, and intuitive upload widgets. The experience gets distilled to its essential signals, without extraneous complexity that could detract from the responsiveness. </p>
<p class="normal">These prompts often originate from unpredictable contexts too, so interfaces have to feel natural across device types and usage scenarios.</p>
<figure class="mediaobject"><img alt="" height="462" src="img/B22175_04_02.png" width="825"/></figure>
<p class="packt_figref">Figure 4.2: Image of a chatbot you will build in Chapter 7, Integration Pattern: Real-Time Intent Classification</p>
<p class="normal">In contrast, the entry points for batch processing occur behind the scenes, out of sight of end users. These prompts more commonly stem from data pipelines – whether JSON payloads from APIs, database export files, documents staged in cloud storage, or other structured/unstructured sources. The interfaces optimize for stable, high-throughput intake rather than real-time human interaction.</p>
<p class="normal">Common input formats amenable to batching include JSON/JSONL streams, CSV uploads, Parquet files, and more. Already pre-parsed into consumable shapes, these prompts can be parsed and evaluated by another model, or other mechanisms, and then sent into downstream queuing systems for model execution at scale. Below you can see an example of a JSONL document:</p>
<pre class="programlisting code"><code class="hljs-code">{"name": "Paula", "music_genres": [["blues", "8"], ["rock", "5"]]}
{"name": "John", "music_genres": [["pop", "15"], ["country", "2"]]}
{"name": "Mary", "music_genres": []}
{"name": "Adam", "music_genres": [["pop", "5"]]}
</code></pre>
<h2 class="heading-2" id="_idParaDest-51">Prompt pre-processing</h2>
<p class="normal">The prompt pre-processing<a id="_idIndexMarker162"/> step involves preparing and transforming the input prompt before it is fed into the language model for generation.</p>
<p class="normal">The requirements and constraints for prompt pre-processing can vary substantially depending on whether the system is designed for real-time or batch processing. In real-time applications, such as conversational assistants or search engines, every step in the pre-processing workflow adds precious seconds to the overall response time. This latency can be detrimental to the user experience, especially in scenarios where immediacy and responsiveness are key.</p>
<p class="normal">For instance, in a real-time system that employs a RAG pipeline, the prompt must undergo several time-consuming steps before the language model can generate a response. These steps may include evaluating the prompt for secure AI purposes, generating embedding (vector representations) of the prompt, querying a vector store to retrieve relevant information, and potentially performing additional processing on the retrieved data. Each of these steps contributes to the overall latency, compounding the delay experienced by the user.</p>
<p class="normal">In contrast, batch prompt pre-processing workflows enjoy greater flexibility and can accommodate more computationally intensive operations without significantly impacting the user experience. Since the processing does not happen in real time, there is more leeway to apply deeper enrichment techniques, such as augmenting the prompt by extracting metadata, performing query rewriting to improve the quality of the retrieved information, or applying advanced natural language processing techniques to better understand the intent behind the prompt.</p>
<p class="normal">This flexibility in<a id="_idIndexMarker163"/> batch processing can lead to more comprehensive and accurate responses from the language model, as the pre-processed prompt can be enriched with relevant contextual information and tailored to better align with the model’s strengths. However, it’s important to strike a balance between the depth of pre-processing and the computational resources required, as excessive pre-processing can be translated into higher costs, which may affect the overall ROI of the solution.</p>
<h2 class="heading-2" id="_idParaDest-52">Inference</h2>
<p class="normal">While real-time and batch prompt<a id="_idIndexMarker164"/> pre-processing workflows differ in their approaches and priorities, the core inference stage is where both patterns converge, as they ultimately leverage the same underlying generative model capabilities. However, the optimization strategies employed at this stage can vary significantly between the two integration patterns.</p>
<p class="normal">In real-time GenAI systems, the primary focus is on minimizing latency and maximizing responsiveness for individual requests. These systems typically handle inference requests one at a time, as opposed to batching multiple requests together. When consuming out-of-the-box models such as Google’s Gemini, OpenAI’s ChatGPT, or Anthropic’s Claude, the underlying infrastructure and resource allocation are abstracted away from the user. In such cases, the provider handles the complexities of right-sizing the available resources for inference, ensuring that individual requests are processed efficiently while adhering to the service’s performance and cost objectives.</p>
<p class="normal">However, in scenarios where organizations choose to host and deploy the generative models themselves, such as with Google’s Gemma, Meta’s LLaMA, or other open-source or proprietary models, the responsibility of right-sizing the infrastructure falls on the organization itself. This process, known as a right-sizing exercise, involves carefully balancing the trade-off between potential latency and cost.</p>
<p class="normal">The objective of the right-sizing exercise is to determine the optimal configuration of computational resources, such as the number and type of GPUs, CPU cores, and memory, that can effectively handle the expected load while minimizing latency and controlling costs. This exercise typically involves load testing and benchmarking the model’s performance under various resource configurations and simulated traffic patterns.</p>
<p class="normal">Factors such as the model’s size, complexity, and the nature of the inference tasks (for example, text generation, question answering, summarization) play a crucial role in determining the resource requirements. Larger and more complex models generally require more computational power to achieve acceptable inference latencies, which can increase the overall cost of deployment and operation.</p>
<p class="normal">Organizations must carefully evaluate the trade-off between achieving low latency, which may require over-provisioning resources, and controlling costs, which may involve accepting slightly higher latencies. Finding the right balance is critical, as excessive latency can degrade the user experience, while over-provisioning resources can lead to unnecessary expenses.</p>
<p class="normal">On the other hand, batch integration patterns prioritize cost optimization and throughput for bulk processing of prompts. Instead of handling requests individually, these systems pool multiple prompts into batches, which are then sent to the generative model for inference. By processing prompts in batches, the computational resources can be utilized more efficiently, as the overhead associated with initializing the model and setting up the inference pipeline is amortized across multiple prompts. </p>
<p class="normal">This approach can lead to significant cost savings, especially when dealing with large volumes of prompts, as the computational resources are utilized more effectively, and the overall<a id="_idIndexMarker165"/> throughput is increased.</p>
<p class="normal">However, it’s important to note that the batch processing approach introduces a trade-off between throughput and latency. While it optimizes for cost and overall throughput, individual requests may experience higher latency as they need to wait for a sufficient number of prompts to accumulate before being processed in a batch. As was mentioned before, batch integration patterns are better suited for scenarios where real-time responsiveness is less critical, such as batch text generation, document summarization, or other offline processing tasks.</p>
<h2 class="heading-2" id="_idParaDest-53">Result post-processing</h2>
<p class="normal">In <a id="_idIndexMarker166"/>real-time applications, the post-processing stage plays a crucial role in ensuring a smooth and engaging user experience. As the generated responses are intended for immediate delivery, the post-processing workflow prioritizes techniques that enable quick response filtering, ranking, and rendering. One common practice, particularly in conversational AI applications like chatbots, is to format the generated output using markup languages such as Markdown. This approach allows for the seamless integration of rich text formatting, including headers, lists, code blocks, and other structural elements, enhancing the readability and visual appeal of the responses.</p>
<p class="normal">Real-time post-processing may incorporate techniques tailored to the specific use case, such as sentiment analysis by applying color schemas as a background depending on the sentiment. For example, in a customer service chatbot, responses can be filtered and ranked based on their relevance to the customer’s query, ensuring that the most appropriate and helpful responses are prioritized. </p>
<p class="normal">In contrast to real-time systems, batch processing workflows in GenAI systems afford more flexibility and computational resources for post-processing. As the generated outputs are not intended for immediate delivery, batch post-processing can apply more comprehensive and computationally intensive enhancements across aggregated outputs before persisting them to data stores or downstream systems.</p>
<p class="normal">One common batch post-processing technique is summarization, where the generated outputs are condensed into concise and coherent summaries, facilitating easier consumption and analysis. Structure extraction is another valuable post-processing step, where the system identifies and extracts relevant information, such as key entities, relations, or event descriptions, from the generated text. This structured data can then be stored in databases or used to populate knowledge graphs, enabling more efficient querying and analysis.</p>
<p class="normal">Batch post-processing can also incorporate deeper quality filtering mechanisms, leveraging techniques such as language models fine-tuned for quality assessment, natural language inference, or fact-checking. These advanced filtering methods can help identify and flag low-quality or factually incorrect outputs, ensuring that only high-quality and reliable information has persisted for downstream consumption.</p>
<p class="normal">Batch <a id="_idIndexMarker167"/>post-processing workflows can involve more complex transformations and enhancements, such as text stylization, sentiment transfer, or content generation for specific domains or formats (for example, generating marketing copy, product descriptions, or technical documentation).</p>
<h2 class="heading-2" id="_idParaDest-54">Result presentation</h2>
<p class="normal">Result<a id="_idIndexMarker168"/> presentation is arguably the starkest difference between paradigms. Real-time UI/API integration demands instantaneous updates – often server-rendered or via data-binding frameworks. In batch mode, you’re more likely to bulk export results through pipelines into warehouses or operational data stores for asynchronous consumption in reporting, analytics, document systems, and more.</p>
<p class="normal">Naturally, the logging and monitoring requirements align closely with each mode’s system characteristics. Real-time needs tight instrumentation around per-request metrics like latency, errors, and resource usage. Batch emphasizes throughput volume, pipeline performance, and data lineage observability.</p>
<p class="normal">From a data engineering perspective, real-time follows more of a lambda architecture with optimized speed and paths. Batch leans toward conventional modern data pipelines leveraging cloud storage, spark clusters, managed workflow orchestrators, and MPP data warehouse targets.</p>
<p class="normal">Real-time integration meshes seamlessly with stateful applications and provides very responsive UIs. Batch procession unlocks higher scalability for large, asynchronous AI-powered operations like document generation, report automation, and conversational data annotation at a tremendous scale.</p>
<p class="normal">Both patterns introduce their own unique supportability considerations too. Real-time depends on highly redundant, self-healing service meshes. Batch relies more on robust recovery orchestration, idempotent restarts, and automatic retries.</p>
<p class="normal">As you can see, while leveraging the same core GenAI capabilities, the two paradigms differ significantly in upstream/downstream architecture priorities and delivery characteristics. The right choice comes down to evaluating latency sensitivity, scale targets, cost parameters, and the use case fit. Many enterprises will likely embrace a hybrid mesh incorporating aspects of both patterns.</p>
<h1 class="heading-1" id="_idParaDest-55">Use case example – search enhanced by GenAI</h1>
<p class="normal">To illustrate a real-time and a <a id="_idIndexMarker169"/>batch use case, we are going to work on an example of a company that uses GenAI to enhance its website search experience. In this case, the document ingestion will be a batch process, and the search itself will be real-time.</p>
<p class="normal">Imagine a company that aims to enhance its website’s search experience by leveraging <strong class="keyWord">GenAI</strong> technologies. In this scenario, the company’s objective is to provide more comprehensive and relevant search results to its users, going beyond simple keyword matching and delivering contextually appropriate and natural language responses.</p>
<p class="normal">The document ingestion process, which involves indexing and processing the company’s content corpus (for example, product descriptions, knowledgebase articles, product manuals), would be a batch operation. This step would involve techniques such as text extraction, entity recognition, topic modeling, and semantic embedding generation for the entire corpus of documents. The embeddings, which capture the semantic meaning and context of the documents, would then be stored in a vector database or other appropriate data store.</p>
<p class="normal">During the real-time search experience, when a user submits a query on the company’s website, the query will undergo prompt pre-processing, which could include query rewriting, intent detection, and embedding generation. The generated query embedding would then be used to retrieve the most relevant documents from the vector database, based on semantic similarity. These retrieved documents would serve as the knowledge source for the GenAI model.</p>
<p class="normal">The GenAI model would then generate a natural language response based on the retrieved documents and the user’s query. This response could take the form of a concise summary, a detailed answer, or even a conversational dialogue, depending on the requirements and the tone the company decides to set.</p>
<p class="normal">The real-time post-processing stage would then kick in, formatting the generated response for optimal presentation on the website. This could involve techniques such as response ranking, result structuring (for example, breaking down the response into sections or bullet points), and rendering with appropriate markup or visual elements.</p>
<p class="normal">By combining the batch processing of document ingestion with real-time query processing and generation, the company can deliver a seamless and enriched search experience to its users. The batch processing ensures that the company’s content corpus is thoroughly indexed and semantically understood, while the real-time components leverage this knowledge to provide relevant and natural language responses tailored to each user’s query.</p>
<h1 class="heading-1" id="_idParaDest-56">Batch integration – document ingestion</h1>
<p class="normal">The <a id="_idIndexMarker170"/>batch-processing portion of the document ingestion pipeline plays a crucial role in preparing the company’s content corpus for effective search and retrieval. This stage involves several steps to extract meaningful information and convert it into a format suitable for efficient querying and generation:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Data Extraction and Pre-processing</strong>: The first step is to extract textual data from various sources, such as databases, content management systems, or file repositories. This data may come in various formats (for example, HTML, PDF, Word documents), requiring pre-processing techniques like text extraction, deduplication, and normalization to clean and standardize the input data.</li>
<li class="numberedList"><strong class="keyWord">Metadata Extraction</strong>: Once the text data is preprocessed, advanced natural language processing techniques, such <a id="_idIndexMarker171"/>as <strong class="keyWord">named entity recognition</strong> (<strong class="keyWord">NER</strong>) and entity linking, can be applied. These tasks can be executed either from predictive AI models or from GenAI models. This step identifies and extracts relevant entities (for example, people, organizations, products, locations) from the text, to be leveraged by linking them to external knowledge bases or ontologies, enriching the data with additional contextual information.</li>
<li class="numberedList"><strong class="keyWord">Embedding Generation</strong>: The heart of the batch processing stage is the generation of semantic embeddings for each document. These embeddings are dense vector representations that capture the contextual meaning and relationships within the text. Popular techniques like Transformer language models (for example, BERT and RoBERTa) or specialized embedding models (like, for example, the Google Vertex AI Embeddings for Text model and OpenAI text embeddings) are used to generate these embeddings.</li>
<li class="numberedList"><strong class="keyWord">Vector Database Indexing</strong>: The generated embeddings, along with the extracted entities, topics, and metadata, are stored in a specialized vector database or other suitable data store optimized for similarity search and retrieval. This indexed corpus serves as the knowledge base for the real-time search and <a id="_idIndexMarker172"/>generation process.</li>
</ol>
<figure class="mediaobject"><img alt="" height="280" src="img/B22175_04_03.png" width="816"/></figure>
<p class="packt_figref">Figure 4.3: Document ingestion</p>
<p class="normal">By performing these batch-processing steps, the company’s content corpus is transformed into a highly structured and semantically rich representation, enabling efficient retrieval, and providing the necessary context for the GenAI model to produce relevant and accurate responses during the real-time search experience.</p>
<h2 class="heading-2" id="_idParaDest-57">Real-time integration – search</h2>
<p class="normal">The real-time<a id="_idIndexMarker173"/> portion of the process handles the user’s search queries and generates contextually relevant responses leveraging the knowledge base created during the batch processing stage.</p>
<p class="normal">At a high level, these are the components of the experience shown in <em class="italic">Figure 4.4</em>:</p>
<ol>
<li class="numberedList" value="1"><strong class="keyWord">Query processing</strong>: This is illustrated in <em class="italic">Figure 4.4</em>, in <em class="italic">step 1</em>. When a user submits a search query on the company’s website, the query undergoes pre-processing steps similar to those applied during the batch-processing stage. This may include text normalization, entity recognition, and embedding generation using the same models employed for the document corpus. In this step, you can also evaluate the query for safety.</li>
<li class="numberedList"><strong class="keyWord">Semantic retrieval</strong>: As depicted in <em class="italic">steps 2</em> and <em class="italic">3</em>, the generated query embedding is used to perform a similarity search against the vector database containing the indexed document embeddings. This step retrieves the most relevant documents from the corpus based on their semantic similarity to the user’s query, ensuring that the retrieved information is contextually appropriate. In this step, you can also re-rank results depending on the use case and the metadata available.</li>
<li class="numberedList"><strong class="keyWord">Prompt enrichment and generation</strong>: In <em class="italic">step 4</em>, the retrieved documents, along with the original query and any additional context (for example, user profile or browsing history) are used to construct a rich prompt for the GenAI model. Techniques like prompt engineering, context augmentation, and RAG may be employed to create an informative and concise prompt that captures the essence of the user’s information need.</li>
<li class="numberedList"><strong class="keyWord">Response generation</strong>: In <em class="italic">steps 5</em> and <em class="italic">6</em>, the GenAI model takes the enriched prompt as input and generates a natural language response. This response could be a concise summary, a detailed answer, or even a conversational dialogue, depending on the requirements and the model’s capabilities.</li>
<li class="numberedList"><strong class="keyWord">Post-processing and rendering</strong>: In <em class="italic">step 6</em>, the generated response then goes through a post-processing stage, which may involve formatting, structure extraction, result ranking, and rendering for optimal presentation on the website. This could include techniques like extracting key points or summaries, highlighting relevant entities, and integrating visual elements of multimedia content to <a id="_idIndexMarker174"/>enhance the user experience.</li>
</ol>
<figure class="mediaobject"><img alt="" height="384" src="img/B22175_04_04.png" width="816"/></figure>
<p class="packt_figref">Figure 4.4: Real-time search architecture example</p>
<h1 class="heading-1" id="_idParaDest-58">Summary</h1>
<p class="normal">This chapter covered the two primary patterns for designing systems around LLMs – batch and real-time. The decision depends on your organization’s use case requirements. We learned that batch mode involves sending queries in bulk for higher throughput at the expense of higher latency. It is better suited to long-running workloads and the consumption of a large corpus of data. </p>
<p class="normal">Results are not immediately exposed to users, allowing for additional review pipelines before or after model inference.</p>
<p class="normal">We also learned that real-time mode offers back-and-forth querying at a faster rate, providing quicker feedback to and from the end user. It has lower throughput but is better for low-latency requirements, but the opportunities to review results are reduced to prevent latency increases.</p>
<p class="normal">In this chapter, we addressed the implications of batch versus real-time processing on different components of the integration pipeline. For entry points, real-time optimizes for streamlined user prompting, while batch handles data pipeline inputs. </p>
<p class="normal">In pre-processing, real-time employs lighter techniques to minimize latency, whereas batch allows for heavier enrichment. Inference in real-time focuses on low latency per request, while batch processes requests in groups for improved throughput. </p>
<p class="normal">Post-processing in real-time involves quicker formatting and filtering, but batch processing allows for more complex transformations. In terms of presentation, real-time offers instantaneous UI updates, while batch exports results asynchronously. </p>
<p class="normal">Additionally, the chapter provided an example use case of using GenAI to enhance a website search, with document ingestion occurring in batch mode and search/response generation in real-time mode, transforming the end user experience, and obtaining more relevant and personalized answers.</p>
<p class="normal">In the next chapter, we will dive deep into a use case that leverages GenAI to extract data from 10-K documents.</p>
<h1 class="heading-1">Join our community on Discord</h1>
<p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
<p class="normal"><a href="Chapter_04.xhtml">https://packt.link/genpat</a></p>
<p class="normal"><img alt="" height="177" src="img/QR_Code134841911667913109.png" width="177"/></p>
</div>
</div></body></html>