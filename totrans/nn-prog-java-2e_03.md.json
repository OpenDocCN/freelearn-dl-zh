["```py\nint numberOfInputs=3;\nint numberOfOutputs=2;\n\nLinear outputAcFnc = new Linear(1.0);\nNeuralNet perceptron = new NeuralNet(numberOfInputs,numberOfOutputs,\n                outputAcFnc);\n```", "```py\nint numberOfInputs=3;\nint numberOfOutputs=1;\nint[] numberOfHiddenNeurons={5};\n\nLinear outputAcFnc = new Linear(1.0);\nSigmoid hiddenAcFnc = new Sigmoid(1.0);\nNeuralNet neuralnet = new NeuralNet(numberOfInputs, numberOfOutputs, numberOfHiddenNeurons, hiddenAcFnc, outputAcFnc);\n```", "```py\npublic class Backpropagation extends DeltaRule {\n    private double MomentumRate=0.7;\n    public ArrayList<ArrayList<Double>> deltaNeuron;\n    public ArrayList<ArrayList<ArrayList<Double>>> lastDeltaWeights;\n  â€¦\n}\n```", "```py\npublic Backpropagation(NeuralNet _neuralNet, NeuralDataSet _trainDataSet, DeltaRule.LearningMode _learningMode){\n    super(_neuralNet,_trainDataSet,_learningMode);\n    initializeDeltaNeuron();\n    initializeLastDeltaWeights();\n}\n```", "```py\n@Override\npublic void train() throws NeuralException{\n    neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.TRAINING);\n    epoch=0; // initialization of epoch\n    int k=0; // first training record\n    currentRecord=0; // this attribute keeps track of which record\n                     // is currently under processing in the training\n    forward();  // initial forward step to determine the error\n    forward(k); // forward for backpropagation of first record error\n    while(epoch<MaxEpochs && overallGeneralError>MinOverallError){\n        backward(); // backward step\n        switch(learningMode){\n            case BATCH:\n                if(k==trainingDataSet.numberOfRecords-1)\n                    applyNewWeights(); // batch update\n                break;\n            case ONLINE:\n                applyNewWeights(); //online update\n        }\n        currentRecord=++k; // moving on to the next record\n        if(k>=trainingDataSet.numberOfRecords){ //if it was the last\n            k=0;  \n            currentRecord=0; // reset to the first\n            epoch++;         // and increase the epoch\n        }\n        forward(k); // forward the next record\n    }\n    neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.RUN);\n}\n```", "```py\npublic void backward(){\n  int numberOfLayers=neuralNet.getNumberOfHiddenLayers();\n  for(int l=numberOfLayers;l>=0;l--){\n    int numberOfNeuronsInLayer=deltaNeuron.get(l).size();\n    for(int j=0;j<numberOfNeuronsInLayer;j++){\n      for(int i=0;i<newWeights.get(l).get(j).size();i++){\n\n // get the current weight of the neuron\n        double currNewWeight = this.newWeights.get(l).get(j).get(i);\n       //if it is the first epoch, get directly from the neuron\n        if(currNewWeight==0.0 && epoch==0.0)\n          if(l==numberOfLayers)\n            currNewWeight=neuralNet.getOutputLayer().getWeight(i, j);\n          else\n            currNewWeight=neuralNet.getHiddenLayer(l).\n                               getWeight(i, j);\n       // calculate the delta weight\n        double deltaWeight=calcDeltaWeight(l, i, j);\n       // store the new calculated weight for subsequent update\n        newWeights.get(l).get(j).set(i,currNewWeight+deltaWeight);\n      }\n    }\n  }\n}\n```", "```py\npublic Double calcDeltaWeight(int layer,int input,int neuron) {\n  Double deltaWeight=1.0;\n  NeuralLayer currLayer;\n  Neuron currNeuron;\n  double _deltaNeuron;\n  if(layer==neuralNet.getNumberOfHiddenLayers()){ //output layer\n    currLayer=neuralNet.getOutputLayer();\n    currNeuron=currLayer.getNeuron(neuron);\n    _deltaNeuron=error.get(currentRecord).get(neuron)\n                   *currNeuron.derivative(currLayer.getInputs());\n  }\n  else{ //hidden layer\n    currLayer=neuralNet.getHiddenLayer(layer);\n    currNeuron=currLayer.getNeuron(neuron);\n    double sumDeltaNextLayer=0;\n    NeuralLayer nextLayer=currLayer.getNextLayer();\n    for(int k=0;k<nextLayer.getNumberOfNeuronsInLayer();k++){\n      sumDeltaNextLayer+=nextLayer.getWeight(neuron, k)\n                           *deltaNeuron.get(layer+1).get(k);\n    }\n    _deltaNeuron=sumDeltaNextLayer*\n                      currNeuron.derivative(currLayer.getInputs());\n\n  }\n\n  deltaNeuron.get(layer).set(neuron, _deltaNeuron);\n  deltaWeight*=_deltaNeuron;\n  if(input<currNeuron.getNumberOfInputs()){\n            deltaWeight*=currNeuron.getInput(input);\n  }\n\n  return deltaWeight;\n}\n```", "```py\nHiddenLayer hl = this.neuralNet.getHiddenLayer(l);\n  Double lastDeltaWeight=lastDeltaWeights.get(l).get(j).get(i);\n  // determine the momentum\n  double momentum=MomentumRate*lastDeltaWeight;\n  //the momentum is then added to the new weight\n  double newWeight=this.newWeights.get(l).get(j).get(i)\n                              -momentum;\n  this.newWeights.get(l).get(j).set(i,newWeight);\n  Neuron n=hl.getNeuron(j);\n  // save the current delta weight for the next step\n  double deltaWeight=(newWeight-n.getWeight(i));\n  lastDeltaWeights.get(l).get(j).set(i,(double)deltaWeight);\n  // finally the weight is updated\n  hl.getNeuron(j).updateWeight(i, newWeight);\n```", "```py\nOutputLayer ol = this.neuralNet.getOutputLayer();\n  Neuron n=ol.getNeuron(j);\n  ol.getNeuron(j).updateWeight(i, newWeight);\n```", "```py\nint numberOfInputs=2;\nint numberOfOutputs=1;\n\nint[] numberOfHiddenNeurons={2};\n\nLinear outputAcFnc = new Linear(1.0);\nSigmoid hdAcFnc = new Sigmoid(1.0);\nIActivationFunction[] hiddenAcFnc={hdAcFnc };\n\nNeuralNet mlp = new NeuralNet(numberOfInputs,numberOfOutputs\n                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);\n\nBackpropagation backprop = new Backpropagation(mlp,neuralDataSet\n                ,LearningAlgorithm.LearningMode.ONLINE);\n```", "```py\n    public class LevenbergMarquardt extends Backpropagation {\n\n        private Matrix jacobian = null;\n        private double damping=0.1;\n\n        private ArrayList<ArrayList<ArrayList<Double>>> errorBackpropagation;\n        private Matrix errorLMA;\n\n        public ArrayList<ArrayList<ArrayList<Double>>> lastWeights;\n    }\n    ```", "```py\n@Override\npublic void train() throws NeuralException{\n  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.TRAINING);\n  forward();\n  double currentOverallError=overallGeneralError;\n  buildErrorVector(); // copy the error values to the error matrix\n  while(epoch<MaxEpochs && overallGeneralError>MinOverallError\n           && damping<=10000000000.0){ // to prevent the damping from growing up to infinity\n    backward(); // to determine the error backpropgation\n    calculateJacobian(); // copies the derivatives to the jacobian matrix\n      applyNewWeights(); //update the weights\n      forward(); //forward all records to evaluate new overall error\n      if(overallGeneralError<currentOverallError){\n        if the new error is less than current\n        damping/=10.0; // the damping factor reduces\n        currentOverallError=overallGeneralError;\n      }\n      else{ // otherwise, the damping factor grows\n        damping*=10.0;\n        restoreLastWeights(); // the last weights are recovered\n        forward();\n      }\n      buildErrorVector(); reevaluate the error matrix\n  }\n  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.RUN);\n\n}\n```", "```py\ndouble input;\nif(p==numberOfInputs)\n  input=1.0;\nelse\n  input = n.getInput(p);\ndouble deltaBackprop = errorBackpropagation.get(m).get(l).get(k);\njacobian.setValue(i, j++, deltaBackprop*input);\n```", "```py\nMatrix jacob=jacobian.subMatrix(rowi, rowe, 0, numberOfWeights-1);\nMatrix errorVec = errorLMA.subMatrix(rowi, rowe, 0, 0);\nMatrix pseudoHessian=jacob.transpose().multiply(jacob);\nMatrix miIdent = new IdentityMatrix(numberOfWeights)\n.multiply(damping);\nMatrix inverseHessianMi = pseudoHessian.add(miIdent).inverse();\nMatrix deltaWeight = inverseHessianMi.multiply(jacob.transpose())\n   .multiply(errorVec);\n```", "```py\nNeuron n=nl.getNeuron(k);\ndouble currWeight=n.getWeight(j);\ndouble newWeight=currWeight+deltaWeight.getValue(i++,0);\nnewWeights.get(l).get(k).set(j,newWeight);\nlastWeights.get(l).get(k).set(j,currWeight);\nn.updateWeight(j, newWeight);\n```", "```py\npublic class ELM extends DeltaRule {\n\n  private Matrix H;\n  private Matrix T;\n\n  public ELM(NeuralNet _neuralNet,NeuralDataSet _trainDataSet){\n    super(_neuralNet,_trainDataSet);\n    learningMode=LearningMode.BATCH;\n    initializeMatrices();\n  }\n}\n```", "```py\n@Override\npublic void train() throws NeuralException{\n  if(neuralNet.getNumberOfHiddenLayers()!=1)\n    throw new NeuralException(\"The ELM learning algorithm can be performed only on Single Hidden Layer Neural Network\");\n  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.TRAINING);\n  int k=0;\n  int N=trainingDataSet.numberOfRecords;\n  currentRecord=0;\n  forward();\n  double currentOverallError=overallGeneralError;\n  while(k<N){\n    forward(k);\n    buildMatrices();\n    currentRecord=++k;\n  }\n  applyNewWeights();\n  forward();\n  currentOverallError=overallGeneralError;\n  neuralNet.setNeuralNetMode(NeuralNet.NeuralNetMode.RUN);\n}\n```", "```py\n@Override\npublic void applyNewWeights(){\n  Matrix Ht = H.transpose();\n  Matrix HtH = Ht.multiply(H);\n  Matrix invH = HtH.inverse();\n  Matrix invHt = invH.multiply(Ht);\n  Matrix beta = invHt.multiply(T);\n\n  OutputLayer ol = this.neuralNet.getOutputLayer();\n  HiddenLayer hl = (HiddenLayer)ol.getPreviousLayer();\n  int h = hl.getNumberOfNeuronsInLayer();\n  int n = ol.getNumberOfNeuronsInLayer();\n  for(int i=0;i<=h;i++){\n    for(int j=0;j<n;j++){\n      if(i<h || outputBiasActive)\n        ol.getNeuron(j).updateWeight(i, beta.getValue(i, j));\n      }\n    }\n  }\n```", "```py\npublic class XORTest {\n  public static void main(String[] args){\n    RandomNumberGenerator.seed=0;\n\n    int numberOfInputs=2;\n    int numberOfOutputs=1;\n\n    int[] numberOfHiddenNeurons={2};\n\n    Linear outputAcFnc = new Linear(1.0);\n    Sigmoid hdAcFnc = new Sigmoid(1.0);\n    IActivationFunction[] hiddenAcFnc={hdAcFnc};\n\n    NeuralNet perceptron = new NeuralNet(numberOfInputs,\n         numberOfOutputs,outputAcFnc);\n\n    NeuralNet mlp = new NeuralNet(numberOfInputs,numberOfOutputs\n                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);\n  }\n}\n```", "```py\nDouble[][] _neuralDataSet = {\n            {0.0 , 0.0 , 1.0 }\n        ,   {0.0 , 1.0 , 0.0 }\n        ,   {1.0 , 0.0 , 0.0 }\n        ,   {1.0 , 1.0 , 1.0 }\n        };\n\nint[] inputColumns = {0,1};\nint[] outputColumns = {2};\n\nNeuralDataSet neuralDataSet = new NeuralDataSet(_neuralDataSet,inputColumns,outputColumns);\n\nDeltaRule deltaRule=new DeltaRule(perceptron,neuralDataSet\n                ,LearningAlgorithm.LearningMode.ONLINE);\n\ndeltaRule.printTraining=true;\ndeltaRule.setLearningRate(0.1);\ndeltaRule.setMaxEpochs(4000);\ndeltaRule.setMinOverallError(0.1);\n\nBackpropagation backprop = new Backpropagation(mlp,neuralDataSet\n                ,LearningAlgorithm.LearningMode.ONLINE);\n        backprop.printTraining=true;\n        backprop.setLearningRate(0.3);\n        backprop.setMaxEpochs(4000);\n        backprop.setMinOverallError(0.01);\n        backprop.setMomentumRate(0.6);\n```", "```py\ndeltaRule.train();\n```", "```py\nbackprop.train();\n```", "```py\nDouble[][] _neuralDataSet = {\n            {1.0,   0.73,   1.0,    -1.0}\n        ,   {1.0,   0.81,   1.0,    -1.0}\n        ,   {1.0,   0.86,   1.0,    -1.0}\n        ,   {0.0,   0.65,   1.0,    -1.0}\n        ,   {0.0,   0.45,   1.0,    -1.0}\n        ,   {1.0,   0.70,   -1.0,    1.0}\n        ,   {0.0,   0.51,   -1.0,    1.0}\n        ,   {1.0,   0.89,   -1.0,    1.0}\n        ,   {1.0,   0.79,   -1.0,    1.0}\n        ,   {0.0,   0.54,   -1.0,    1.0}\n\n        };\n\nint[] inputColumns = {0,1};\nint[] outputColumns = {2,3};\n\nNeuralDataSet neuralDataSet = new NeuralDataSet(_neuralDataSet,inputColumns,outputColumns);\n```", "```py\nint numberOfInputs = 2;\nint numberOfOutputs = 2;\nint[] numberOfHiddenNeurons={5};\n\nLinear outputAcFnc = new Linear(1.0);\nSigmoid hdAcFnc = new Sigmoid(1.0);\nIActivationFunction[] hiddenAcFnc={hdAcFnc  };\n\nNeuralNet nnlm = new NeuralNet(numberOfInputs,numberOfOutputs\n                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);\n\nNeuralNet nnelm = new NeuralNet(numberOfInputs,numberOfOutputs\n                ,numberOfHiddenNeurons,hiddenAcFnc,outputAcFnc);\n```", "```py\nLevenbergMarquardt lma = new LevenbergMarquardt(nnlm,\nneuralDataSet,\nLearningAlgorithm.LearningMode.BATCH);\nlma.setDamping(0.001);\nlma.setMaxEpochs(100);\nlma.setMinOverallError(0.0001);\n\nELM elm = new ELM(nnelm,neuralDataSet);\nelm.setMinOverallError(0.0001);\nelm.printTraining=true;\n```"]