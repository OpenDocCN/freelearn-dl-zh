<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-122"><a id="_idTextAnchor211"/>6</h1>
<h1 id="_idParaDest-123"><a id="_idTextAnchor212"/>Understanding Domain Adaptation for Large Language Models</h1>
<p>In the previous chapter, we examined how <strong class="bold">Parameter-Efficient Fine-Tuning</strong> (<strong class="bold">PEFT</strong>) enhances <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) for specific tasks such as question-answering. In <a id="_idIndexMarker482"/>this chapter, we will be introduced to domain adaptation, a distinct fine-tuning approach. Unlike task-specific tuning, domain <a id="_idIndexMarker483"/>adaptation equips models to interpret language that’s unique to specific industries or domains, addressing the gap in LLMs’ understanding of specialized language.</p>
<p>To illustrate this, we’ll introduce <em class="italic">Proxima Investment Group</em>, a hypothetical digital-only investment firm aiming to adapt an LLM to its specific financial language using internal data. We’ll demonstrate how modifying the LLM to process the specific terminology and nuances typical in Proxima’s environment enhances the model’s relevance and effectiveness in the financial domain.</p>
<p>We’ll also explore the practical steps Proxima might take, such as selecting relevant internal <a id="_idIndexMarker484"/>datasets for training, applying PEFT methods such as <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>) to adapt the model efficiently, and using masking techniques to refine the model’s comprehension. Then, we’ll explore how Proxima can evaluate the success of this domain adaptation, assessing the model’s performance in tasks such as analyzing financial trends, responding to client inquiries, and generating reports that align with Proxima’s internal standards and market position.</p>
<p>By the end of this chapter, we will clearly understand the theoretical underpinnings of domain adaptation and its real-world application, particularly in a complex sector such as finance, where the model’s depth of domain understanding can significantly impact business outcomes.</p>
<p>Let’s begin by demystifying the concept, exploring its technical underpinnings, and discussing its importance in accomplishing domain-specific business objective<a id="_idTextAnchor213"/>s.</p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor214"/>Demystifying domain adaptation – understanding its history and importance</h1>
<p>In the context <a id="_idIndexMarker485"/>of generative LLMs, domain adaptation specifically tailors <a id="_idIndexMarker486"/>models such as <strong class="bold">BLOOM</strong>, which have been pre-trained on extensive, generalized datasets (such as news articles and Wikipedia entries) for enhanced understanding of texts from targeted sectors, including biomedical, legal, and financial fields. This type of refinement can be pivotal as LLMs, despite their vast pre-training, may not inherently capture the intricate details and specialized terminology inherent to these domains. This adaptation involves a deliberate process of realigning the model’s learned patterns to the linguistic characteristics, terminologies, and contextual nuances prevalent in the target domain.</p>
<p>Domain adaptation <a id="_idIndexMarker487"/>operates within the ambit of <strong class="bold">transfer learning</strong>. In this broader paradigm, a model’s learnings from one task are repurposed to improve its efficacy on a related yet distinct task. This approach capitalizes on the model’s pre-learned features to improve its efficiency and accuracy on the subsequent task, markedly reducing its reliance on large volumes of domain-specific data and computational resources. Specifically, we begin with a model that’s been trained on broad datasets and use it as a starting point to adapt to specialized domains thereby augmenting their accuracy, relevance, and applicability to more targeted use cases.</p>
<p>In practice, several methodologies can be employed to tailor the model to specific domains, including the following:</p>
<ul>
<li><strong class="bold">Continued pre-training</strong>: The model undergoes additional pre-training on domain-specific <a id="_idIndexMarker488"/>corpora, allowing its parameters to be adapted incrementally to the target domain’s linguistic features, as highlighted in research by Gururangan et al. 2020.</li>
<li><strong class="bold">Intermediate task training</strong>: Here, the model is trained on intermediate tasks, utilizing <a id="_idIndexMarker489"/>domain-specific data before being fine-tuned for downstream applications. This step facilitates a more robust adaptation to the domain (Pruksachatkun et al., 2020).</li>
<li><strong class="bold">Data augmentation</strong>: Techniques <a id="_idIndexMarker490"/>such as <strong class="bold">back translation</strong> (Xie et al., 2019) and <strong class="bold">token replacement</strong> (Anaby-Tavor et al., 2020) are leveraged to generate synthetic domain-specific training examples from limited actual data:<ul><li><strong class="bold">Back translation</strong> entails <a id="_idIndexMarker491"/>translating an existing text from one language (for example, English) into another (for example, French) and then translating it back to the original language. This process generates paraphrased versions of the original text while preserving its semantics.</li><li><strong class="bold">Token replacement</strong> involves altering individual words within a sentence to generate <a id="_idIndexMarker492"/>new sentences. This alteration usually aims to preserve the semantic meaning of the original sentence while introducing variations.</li></ul></li>
<li><strong class="bold">Multi-task learning</strong>: This <a id="_idIndexMarker493"/>framework concurrently optimizes the model for both generic and domain-specific tasks during the adaptation phase, as demonstrated by Clark et al. 2019.</li>
</ul>
<p>As domain adaptation <a id="_idIndexMarker494"/>techniques evolve, they increasingly enhance model performance in specialized fields, even with reduced amounts of domain-specific data. As discussed in <a href="B21773_04.xhtml#_idTextAnchor123"><em class="italic">Chapter 4</em></a>, more recent developments have focused on the computational efficiency of these techniques. Adaptation methods such as LoRA facilitate significant model adjustments with minimal parameter changes without requiring comprehensive retraining. It is important to note that a model's performance will always vary based on various factors like the quality of the dataset, available computational resources, and other implementation details.</p>
<p>Now that we have some insight into domain adaptation techniques and their focus on computational efficiency, we can apply these concepts practically. Our practice project will leverage BLOOM, a state-of-the-art, open source LLM, to demonstrate domain adaptation for the finance sector. Leveraging PEFT, we aim to fine-tune BLOOM with minimal computational resources, illustrating the practical application of these advanced adaptation methods in enhancing model performance within the finance domain.</p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor215"/>Practice project: Transfer learning for the finance domain</h1>
<p>This project aims to fine-tune BLOOM on a curated corpus of specific documents <a id="_idIndexMarker495"/>to imbue it with the ability to interpret and articulate concepts specific to Proxima and its products.</p>
<p>Our methodology is inspired by strategies for domain adaptation across various fields, including biomedicine, finance, and law. A noteworthy study conducted by Cheng et al. in 2023 called <em class="italic">Adapting Large Language Models via Reading Comprehension</em> presents a novel approach for enhancing LLMs’ proficiency in domain-specific tasks. This approach repurposed extensive pre-training corpora into formats conducive to reading comprehension tasks, significantly improving the models’ functionality in specialized domains. In our case, we will apply a similar but simplified approach to continued pre-training by fine-tuning the pre-trained BLOOM model using a bespoke dataset specific to Proxima, effectively continuing the model’s training. This process adjusts the model parameters incrementally to ensure that it understands the language unique to Proxima’s products <a id="_idTextAnchor216"/>and offerings better.</p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor217"/>Training methodologies for financial domain adaptation</h2>
<p>Four our continued training strategy, we’ll employ <strong class="bold">causal language modeling</strong> (<strong class="bold">CLM</strong>). This approach is part of a broader set of training methodologies that optimize model performance for various objectives. Before moving to implementation, let's try to disambiguate our chosen approach from other popular strategies to better understand the CLM methodology:</p>
<ul>
<li><strong class="bold">Masked Language Modeling</strong>(<strong class="bold">MLM</strong>): A cornerstone of Transformer-based models <a id="_idIndexMarker496"/>such as BERT, MLM randomly masks parts of the input text and challenges the model to predict the masked tokens. By considering the entire context around the mask (both before and after), MLM enables a model to develop a bidirectional understanding of language, enriching its grasp of context and semantics.</li>
<li><strong class="bold">Next-Sentence Prediction</strong>(<strong class="bold">NSP</strong>): This methodology further broadens a model’s <a id="_idIndexMarker497"/>narrative understanding by training it to discern whether two sentences logically follow each other. NSP is instrumental in teaching models about text structure and coherence, enabling them to construct and comprehend logical sequences within larger bodies of text.</li>
<li><strong class="bold">CLM</strong>: Our chosen <a id="_idIndexMarker498"/>path for BLOOM’s adaptation diverges here, embracing CLM for its focused, sequential prediction capabilities. Unlike MLM, which looks both ways (before and after the masked token), CLM adopts a unidirectional approach, predicting each subsequent token based solely on the preceding context. This method is intrinsically aligned with natural language generation, making it especially suitable for crafting coherent, contextually rich narratives in the target domain.</li>
</ul>
<p>In selecting CLM for BLOOM’s adaptation, we’ll extend the model’s generative capabilities to produce text sequences that are not only logically structured but also deeply embedded with the nuance of the target domain. CLM’s unidirectional nature ensures that <a id="_idIndexMarker499"/>each token that’s generated is informed by a cohesive understanding of the preceding text, enabling the model to generate detailed, accurate, and domain-specific texts. </p>
<p>Once fine-tuning is complete, we can evaluate the efficacy of the domain-adapted BLOOM model based on its proficiency in generating contextually relevant and domain-specific narratives. We’ll compare the adapted model’s performance against the original model with a special focus on the model’s fluency, accuracy, and overall comprehension of the target domain.</p>
<p>As we’ve done previously, we’ll leverage Google Colab for our initial prototyping phase. As <em class="italic">Chapters 4</em> and <em class="italic">5</em> described, Google Colab offers a preconfigured environment that simplifies the process of testing our methodologies before we consider promoting them to production environments. All the code in this chapter is available in the <code>Chapter 6</code> folder of this book’s GitHub repository (<a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</a>).</p>
<p>We’ll begin with the initial setup, which involves loading a smaller variation of <strong class="bold">BLOOM-1b1</strong> using the Transformers library. We’ll also import the methods that we’ll need to apply PEFT. For this example, we’ll rely on a few libraries that can be installed as follows:</p>
<pre class="console">
pip install sentence-transformers transformers peft datasets</pre>
<p>Once <a id="_idIndexMarker500"/>installed, we can begin importing:</p>
<pre class="source-code">
from <a id="_idTextAnchor218"/>transformers import (
    AutoTokenizer, AutoModelForCausalLM)
from peft import AdaLoraConfig, get_peft_model</pre>
<p>The next step is to load the tokenizer and model:</p>
<pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")
model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b1")</pre>
<p>As discussed previously, we’re incorporating PEFT for efficient adaptation:</p>
<pre class="source-code">
adapter_config = AdaLoraConfig(target_r=16)
model.add_adapter(adapter_config)</pre>
<p>The PEFT technique, specifically through <code>AdaLoraConfig</code>, allows us to introduce a compact, efficient layer so that we can adapt the model to new contexts – here, the finance domain – with a significantly reduced number of trainable parameters:</p>
<pre class="source-code">
model = get_peft_model(model, adapter_config)
model.print_trainable_parameters()</pre>
<p>We must integrate the adapter to finalize the PEFT model setup, effectively creating a model variant that’s optimized for our domain-specific training while focusing on efficiency. We can quantify this by examining the number of trainable parameters our model will use:</p>
<pre class="source-code">
trainable params: 1,769,760 || all params: 1,067,084,088 || trainable%: 0.1658500974667331</pre>
<p>The preceding code provides us with the following information:</p>
<ul>
<li><strong class="bold">Trainable </strong><strong class="bold">parameters</strong>: 1,769,760</li>
<li><strong class="bold">Total parameters in the </strong><strong class="bold">model</strong>: 1,067,084,088</li>
<li><strong class="bold">Percentage of trainable </strong><strong class="bold">parameters</strong>: 0.166%</li>
</ul>
<p>This means that out of over 1 billion parameters in the BLOOM-1b1 model, only about 1.77 million parameters are being fine-tuned for the finance domain adaptation. This small percentage (0.166%) of trainable parameters highlights the efficiency of PEFT, allowing significant <a id="_idIndexMarker501"/>model adaptability with minimal adjustments. This is crucial for practical applications as it reduces both computational costs and the time required for training.</p>
<p>Next, we’ll move on to preparing the data. We’ll assume we have assembled texts encompassing the breadth of knowledge about specialized Proxima products and offerings such <a id="_idIndexMarker502"/>as the <strong class="bold">Proxima Passkey</strong>. CLM training requires distinct testing and training phases to evaluate the model’s ability to accurately predict the next token in a sequence. This ensures it generalizes well beyond the training data to unseen text. During training, the loss calculation measures the difference between the model’s predicted token probabilities and the actual tokens. It guides the model to adjust its parameters to minimize this loss, improving its predictive accuracy over iterations. As such, we must define training and testing texts as our dataset. An example dataset is included in this book’s GitHub repository (linked earlier in the chapter).</p>
<pre class="source-code">
dataset = load_dataset("text",
    data_files={"train": "./train.txt",
        "test": "./test.txt"}
    )</pre>
<p>Next, we must apply preprocessing and tokenization. Texts are cleaned, standardized, and then converted into a numerical format (<code>512</code> tokens so that it aligns with the model’s architecture:</p>
<pre class="source-code">
def preprocess_function(examples):
    inputs = tokenizer(examples["text"], truncation=True,
        padding="max_length", max_length=512)
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs</pre>
<p>The <code>TrainingArguments</code> class configures the training process, setting parameters such as the batch size, number of epochs, and the directory for saving model checkpoints. This configuration is crucial for efficient learning and model evaluation. Meanwhile, the <code>Trainer</code> class orchestrates the model’s training process. Again, continued training gradually adapts the model’s parameters to generate and understand text related to the Proxima Passkey:</p>
<pre class="source-code">
from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir="./model_output",
    per_device_train_batch_size=2,
    num_train_epochs=5,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    prediction_loss_only=True,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)
trainer.train()
model.save_pretrained("./proxima_da_model")</pre>
<p>Generally, our configuration specifies the training parameters and initializes the <code>Trainer</code> class while focusing on domain adaptation. The <code>TrainingArguments</code> class is tailored to manage the training process efficiently, including logging and model-saving strategies. Remember that the batch size we choose for training the model balances the GPU’s memory capacity and how quickly the model learns from the dataset. A larger <a id="_idIndexMarker506"/>batch size allows more data to be processed at once, speeding up training but requiring more memory, which can be a limitation if the GPU has restricted capacity. Conversely, a smaller batch size means the model updates its weights more frequently with fewer samples, which can benefit learning but results in slower overall progress through the dataset.</p>
<p>With training complete, we can use the adapted model to generate text based on prompts related to the Proxima Passkey. The model considers the prompt, generates a sequence of tokens representing the continuation, and then decodes this sequence back into human-readable text:</p>
<pre class="source-code">
def predict(model, prompt="The Proxima Passkey is"):
    inputs = tokenizer(prompt, return_tensors="pt")
    output = model.generate(**inputs, max_length=50)
    return tokenizer.decode(output[0], skip_special_tokens=True)</pre>
<p>Notice the <code>model.generate()</code> function, which takes tokenized input and produces a sequence of tokens as output. These tokens are then decoded into text.</p>
<p>In this example, we adapted the BLOOM language model so that it specializes in the finance domain. This involved loading the pre-trained model, applying a PEFT adapter for efficient <a id="_idIndexMarker507"/>domain adaptation, and preparing a financial dataset for model training through standardization and tokenization. After fine-tuning BLOOM with this domain-specific data, we used the model to generate text relevant to the finance sector. The final step is to evaluate this adapted model’s performance compared to the original pre-trained version, focusing on its effectiveness in accurately handling fina<a id="_idTextAnchor219"/>ncial language and concepts.</p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor220"/>Evaluation and outcome analysis – the ROUGE metric</h2>
<p>Quantitative and qualitative evaluations are essential to assess the adapted BLOOM model against <a id="_idIndexMarker508"/>the original, especially in the context of Proxima’s language. Quantitatively, the model’s output is compared against a reference dataset that mirrors Proxima’s product language using the <strong class="bold">ROUGE</strong> metric. This comparison helps measure the overlap in key terms and styles. Additionally, it’s beneficial to develop specific metrics for evaluating the model’s proficiency in terms of financial terminology and concepts relevant to Proxima:</p>
<pre class="source-code">
from rouge import Rouge
# Example reference text (what we expect the model to generate after training on a complete dataset)
reference = "Proxima's Passkey enables seamless integration of diverse financial portfolios, offering unparalleled access to global investment opportunities and streamlined asset management."
# Example predicted model output
predicted = "The Proxima Passkey provides a unified platform for managing various investment portfolios, granting access to worldwide investment options and efficient asset control."
# Initialize the Rouge metric
rouge = Rouge()
# Compute the Rouge scores
scores = rouge.get_scores(predicted, reference)
print(scores)</pre>
<p>The ROUGE score <a id="_idIndexMarker509"/>would be calculated by comparing the two texts in this example. The score measures the overlap between the predicted output and the reference text <a id="_idIndexMarker510"/>in terms of <strong class="bold">n-grams</strong> (sequences of words). For instance, <strong class="bold">ROUGE-N</strong> (where <em class="italic">N</em> can be 1, 2, or L) calculates <a id="_idIndexMarker511"/>the overlap of n-grams between the predicted and reference texts:</p>
<ul>
<li><strong class="bold">ROUGE-1</strong> evaluates the overlap of unigrams (individual words) between the predicted and reference texts</li>
<li><strong class="bold">ROUGE-2</strong> assesses the overlap of bigrams (two-word phrases) between the texts</li>
<li><strong class="bold">ROUGE-L</strong> focuses on the longest common subsequence, which is useful for evaluating sentence-level structure similarity</li>
</ul>
<p>The ROUGE scores range <a id="_idIndexMarker512"/>from 0 to 1 and quantify the similarity between the predicted text and a reference text, providing insights into how well a model’s output matches the expected content. Scores closer to 1 indicate higher similarity or overlap, while scores near 0 suggest little to no commonality. These scores are divided into three key components – precision, recall, and the F1 score:</p>
<ul>
<li><strong class="bold">Precision</strong> measures the proportion of words in the predicted text that are also found in the <a id="_idIndexMarker513"/>reference text. A high precision score indicates that most of the words generated by the model are relevant and appear in the reference, signifying accuracy in the model’s output.</li>
<li><strong class="bold">Recall</strong> assesses the proportion of words from the reference text that are captured in <a id="_idIndexMarker514"/>the model’s prediction. High recall implies that the model effectively includes most of the relevant content from the reference in its output, indicating comprehensiveness.</li>
<li>The <strong class="bold">F1 score</strong> is the harmonic mean of precision and recall, balancing the two. It is especially <a id="_idIndexMarker515"/>useful for understanding the model’s overall accuracy in generating text that is both relevant (precision) and comprehensive (recall). The F1 score is crucial when equal importance is given to precision and recall in evaluating the model’s performance.</li>
<li>Here’s the output:</li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Metric</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Recall (r)</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Precision (p)</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">F1 </strong><strong class="bold">Score (f)</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>ROUGE-1</p>
</td>
<td class="No-Table-Style">
<p>0.35</p>
</td>
<td class="No-Table-Style">
<p>0.333</p>
</td>
<td class="No-Table-Style">
<p>0.341</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>ROUGE-2</p>
</td>
<td class="No-Table-Style">
<p>0.053</p>
</td>
<td class="No-Table-Style">
<p>0.048</p>
</td>
<td class="No-Table-Style">
<p>0.05</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>ROUGE-L</p>
</td>
<td class="No-Table-Style">
<p>0.35</p>
</td>
<td class="No-Table-Style">
<p>0.333</p>
</td>
<td class="No-Table-Style">
<p>0.341</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1: ROUGE metric outcomes</p>
<p>These scores indicate a moderate level of unigram overlap (ROUGE-1) between the texts but a significantly lower bigram overlap (ROUGE-2). The similarity between the ROUGE-1 and ROUGE-L scores suggests the model captures individual key terms to some extent but may struggle with longer phrase structures, pointing to areas for model improvement.</p>
<p>Overall, while the model demonstrates a basic grasp of key individual terms (as shown by ROUGE-1 and ROUGE-L), its ability to replicate more complex structures or phrases from the reference text (as indicated by ROUGE-2) is quite limited. This suggests that while the model has some understanding of the domain-specific language, further fine-tuning is required for it to effectively replicate the more nuanced and structured aspects of the reference texts. Keep in mind that, as we have seen in other chapters, semantic similarity is also a good measure of domain-specific language understanding and does not rely on lexical overlap the way ROUGE does.</p>
<p>Qualitatively, domain experts should review the model’s outputs to judge their relevance and <a id="_idIndexMarker516"/>accuracy in the context of Proxima’s products and institutional language. These experts can provide insights into the nuances of the model’s performance, which might not be captured by quantitative metrics alone. Comparing their feedback on the outputs from both the original and adapted models will highlight how well the adaptation has aligned BLOOM with Proxima’s specific communication needs. This dual approach ensures a comprehensive evaluation, blending statistical analysis with real-world applicability and relevance.<a id="_idTextAnchor221"/></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor222"/>Summary</h1>
<p>In this chapter, we explored the domain adaptation process for the BLOOM LLM, which is specifically tailored to enhance its proficiency in the financial sector, particularly in understanding and generating content related to Proxima’s product offerings. We began by introducing the concept of domain adaptation within the broader scope of transfer learning, emphasizing its significance in fine-tuning general-purpose models to grasp the intricacies of specialized fields.</p>
<p>The adaptation process involved integrating PEFT techniques into BLOOM and preprocessing a financial dataset for model training. This included standardizing text lengths through truncation and padding and tokenizing the texts for consistency in model input. The adapted model’s performance was then quantitatively assessed against a reference dataset using the ROUGE metric, providing insights into its ability to capture key financial terminologies and phrases. Qualitative evaluation by domain experts was also suggested as a complementary method to gauge the model’s practical effectiveness in real-world scenarios.</p>
<p>Overall, this chapter detailed a common approach to refining an LLM for a specific domain, illustrating both the methodology and the importance of a nuanced evaluation to ascertain the success of such adaptations. In the next chapter, we will explore how to adapt an LLM without fine-tuning using prompt engineering. We will discover how to contextualize and guide model outputs to produce similar results comparable to fine-tuned models.<a id="_idTextAnchor223"/></p>
<h1 id="_idParaDest-129"><a id="_idTextAnchor224"/>References</h1>
<p>This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the subject matter:</p>
<ul>
<li>Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., &amp; Smith, N. A. (2020). <em class="italic">Don’t stop pretraining: Adapt language models to domains and tasks</em>. In arXiv [cs.CL]. <a href="http://arxiv.org/abs/2004.10964/">http://arxiv.org/abs/2004.10964/</a>.</li>
<li>Pruksachatkun, Y., Phang, J., Liu, H., Htut, P. M., Zhang, X., Pang, R. Y., Vania, C., Kann, K., &amp; Bowman, S. R. (2020a). <em class="italic">Intermediate-task transfer learning with pretrained language models: When and why does it work?</em> Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.</li>
<li>Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., &amp; Le, Q. V. (n.d.). <em class="italic">Unsupervised Data Augmentation for Consistency Training</em>. Arxiv.org. Retrieved March 16, 2024, from <a href="http://arxiv.org/abs/1904.12848">http://arxiv.org/abs/1904.12848</a>.</li>
<li>Anaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A., Kour, G., Shlomov, S., Tepper, N., &amp; Zwerdling, N. (2020). <em class="italic">Do not have enough data? Deep learning to the rescue!</em> Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, 34(05), 7383–7390. <a href="https://doi.org/10.1609/aaai.v34i05.6233">https://doi.org/10.1609/aaai.v34i05.6233</a>.</li>
<li>Clark, K., Luong, M.-T., Khandelwal, U., Manning, C. D., &amp; Le, Q. V. (2019). <em class="italic">BAM! Born-again multi-task networks for natural language understanding</em>. In arXiv [cs.CL]. <a href="http://arxiv.org/abs/1907.04829">http://arxiv.org/abs/1907.04829</a>.</li>
</ul>
</div>
</body></html>