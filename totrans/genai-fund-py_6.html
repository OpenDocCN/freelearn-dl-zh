<html><head></head><body>
<div id="_idContainer040">
<h1 class="chapter-number" id="_idParaDest-122"><a id="_idTextAnchor211"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-123"><a id="_idTextAnchor212"/><span class="koboSpan" id="kobo.2.1">Understanding Domain Adaptation for Large Language Models</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we examined how </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Parameter-Efficient Fine-Tuning</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">PEFT</span></strong><span class="koboSpan" id="kobo.7.1">) enhances </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">large language models</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">LLMs</span></strong><span class="koboSpan" id="kobo.11.1">) for specific tasks such as question-answering. </span><span class="koboSpan" id="kobo.11.2">In </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.12.1">this chapter, we will be introduced to domain adaptation, a distinct fine-tuning approach. </span><span class="koboSpan" id="kobo.12.2">Unlike task-specific tuning, domain </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.13.1">adaptation equips models to interpret language that’s unique to specific industries or domains, addressing the gap in LLMs’ understanding of </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">specialized language.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">To illustrate this, we’ll introduce </span><em class="italic"><span class="koboSpan" id="kobo.16.1">Proxima Investment Group</span></em><span class="koboSpan" id="kobo.17.1">, a hypothetical digital-only investment firm aiming to adapt an LLM to its specific financial language using internal data. </span><span class="koboSpan" id="kobo.17.2">We’ll demonstrate how modifying the LLM to process the specific terminology and nuances typical in Proxima’s environment enhances the model’s relevance and effectiveness in the </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">financial domain.</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">We’ll also explore the practical steps Proxima might take, such as selecting relevant internal </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.20.1">datasets for training, applying PEFT methods such as </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">Low-Rank Adaptation</span></strong><span class="koboSpan" id="kobo.22.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.23.1">LoRA</span></strong><span class="koboSpan" id="kobo.24.1">) to adapt the model efficiently, and using masking techniques to refine the model’s comprehension. </span><span class="koboSpan" id="kobo.24.2">Then, we’ll explore how Proxima can evaluate the success of this domain adaptation, assessing the model’s performance in tasks such as analyzing financial trends, responding to client inquiries, and generating reports that align with Proxima’s internal standards and </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">market position.</span></span></p>
<p><span class="koboSpan" id="kobo.26.1">By the end of this chapter, we will clearly understand the theoretical underpinnings of domain adaptation and its real-world application, particularly in a complex sector such as finance, where the model’s depth of domain understanding can significantly impact </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">business outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">Let’s begin by demystifying the concept, exploring its technical underpinnings, and discussing its importance in accomplishing domain-specific </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">business objective</span><a id="_idTextAnchor213"/><span class="koboSpan" id="kobo.30.1">s.</span></span></p>
<h1 id="_idParaDest-124"><a id="_idTextAnchor214"/><span class="koboSpan" id="kobo.31.1">Demystifying domain adaptation – understanding its history and importance</span></h1>
<p><span class="koboSpan" id="kobo.32.1">In the context </span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.33.1">of generative LLMs, domain adaptation specifically tailors </span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.34.1">models such as </span><strong class="bold"><span class="koboSpan" id="kobo.35.1">BLOOM</span></strong><span class="koboSpan" id="kobo.36.1">, which have been pre-trained on extensive, generalized datasets (such as news articles and Wikipedia entries) for enhanced understanding of texts from targeted sectors, including biomedical, legal, and financial fields. </span><span class="koboSpan" id="kobo.36.2">This type of refinement can be pivotal as LLMs, despite their vast pre-training, may not inherently capture the intricate details and specialized terminology inherent to these domains. </span><span class="koboSpan" id="kobo.36.3">This adaptation involves a deliberate process of realigning the model’s learned patterns to the linguistic characteristics, terminologies, and contextual nuances prevalent in the </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">target domain.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">Domain adaptation </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.39.1">operates within the ambit of </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">transfer learning</span></strong><span class="koboSpan" id="kobo.41.1">. </span><span class="koboSpan" id="kobo.41.2">In this broader paradigm, a model’s learnings from one task are repurposed to improve its efficacy on a related yet distinct task. </span><span class="koboSpan" id="kobo.41.3">This approach capitalizes on the model’s pre-learned features to improve its efficiency and accuracy on the subsequent task, markedly reducing its reliance on large volumes of domain-specific data and computational resources. </span><span class="koboSpan" id="kobo.41.4">Specifically, we begin with a model that’s been trained on broad datasets and use it as a starting point to adapt to specialized domains thereby augmenting their accuracy, relevance, and applicability to more targeted </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">use cases.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">In practice, several methodologies can be employed to tailor the model to specific domains, including </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.45.1">Continued pre-training</span></strong><span class="koboSpan" id="kobo.46.1">: The model undergoes additional pre-training on domain-specific </span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.47.1">corpora, allowing its parameters to be adapted incrementally to the target domain’s linguistic features, as highlighted in research by Gururangan et </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">al. </span><span class="koboSpan" id="kobo.48.2">2020.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Intermediate task training</span></strong><span class="koboSpan" id="kobo.50.1">: Here, the model is trained on intermediate tasks, utilizing </span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.51.1">domain-specific data before being fine-tuned for downstream applications. </span><span class="koboSpan" id="kobo.51.2">This step facilitates a more robust adaptation to the domain (Pruksachatkun et </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">al., 2020).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.53.1">Data augmentation</span></strong><span class="koboSpan" id="kobo.54.1">: Techniques </span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.55.1">such as </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">back translation</span></strong><span class="koboSpan" id="kobo.57.1"> (Xie et al., 2019) and </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">token replacement</span></strong><span class="koboSpan" id="kobo.59.1"> (Anaby-Tavor et al., 2020) are leveraged to generate synthetic domain-specific training examples from limited </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">actual data:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.61.1">Back translation</span></strong><span class="koboSpan" id="kobo.62.1"> entails </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.63.1">translating an existing text from one language (for example, English) into another (for example, French) and then translating it back to the original language. </span><span class="koboSpan" id="kobo.63.2">This process generates paraphrased versions of the original text while preserving </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">its semantics.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.65.1">Token replacement</span></strong><span class="koboSpan" id="kobo.66.1"> involves altering individual words within a sentence to generate </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.67.1">new sentences. </span><span class="koboSpan" id="kobo.67.2">This alteration usually aims to preserve the semantic meaning of the original sentence while </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">introducing variations.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.69.1">Multi-task learning</span></strong><span class="koboSpan" id="kobo.70.1">: This </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.71.1">framework concurrently optimizes the model for both generic and domain-specific tasks during the adaptation phase, as demonstrated by Clark et </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">al. </span><span class="koboSpan" id="kobo.72.2">2019.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.73.1">As domain adaptation </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.74.1">techniques evolve, they increasingly enhance model performance in specialized fields, even with reduced amounts of domain-specific data. </span><span class="koboSpan" id="kobo.74.2">As discussed in </span><a href="B21773_04.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.75.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.76.1">, more recent developments have focused on the computational efficiency of these techniques. </span><span class="koboSpan" id="kobo.76.2">Adaptation methods such as LoRA facilitate significant model adjustments with minimal parameter changes without requiring comprehensive retraining. </span><span class="koboSpan" id="kobo.76.3">It is important to note that a model's performance will always vary based on various factors like the quality of the dataset, available computational resources, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">implementation details.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">Now that we have some insight into domain adaptation techniques and their focus on computational efficiency, we can apply these concepts practically. </span><span class="koboSpan" id="kobo.78.2">Our practice project will leverage BLOOM, a state-of-the-art, open source LLM, to demonstrate domain adaptation for the finance sector. </span><span class="koboSpan" id="kobo.78.3">Leveraging PEFT, we aim to fine-tune BLOOM with minimal computational resources, illustrating the practical application of these advanced adaptation methods in enhancing model performance within the </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">finance domain.</span></span></p>
<h1 id="_idParaDest-125"><a id="_idTextAnchor215"/><span class="koboSpan" id="kobo.80.1">Practice project: Transfer learning for the finance domain</span></h1>
<p><span class="koboSpan" id="kobo.81.1">This project aims to fine-tune BLOOM on a curated corpus of specific documents </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.82.1">to imbue it with the ability to interpret and articulate concepts specific to Proxima and </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">its products.</span></span></p>
<p><span class="koboSpan" id="kobo.84.1">Our methodology is inspired by strategies for domain adaptation across various fields, including biomedicine, finance, and law. </span><span class="koboSpan" id="kobo.84.2">A noteworthy study conducted by Cheng et al. </span><span class="koboSpan" id="kobo.84.3">in 2023 called </span><em class="italic"><span class="koboSpan" id="kobo.85.1">Adapting Large Language Models via Reading Comprehension</span></em><span class="koboSpan" id="kobo.86.1"> presents a novel approach for enhancing LLMs’ proficiency in domain-specific tasks. </span><span class="koboSpan" id="kobo.86.2">This approach repurposed extensive pre-training corpora into formats conducive to reading comprehension tasks, significantly improving the models’ functionality in specialized domains. </span><span class="koboSpan" id="kobo.86.3">In our case, we will apply a similar but simplified approach to continued pre-training by fine-tuning the pre-trained BLOOM model using a bespoke dataset specific to Proxima, effectively continuing the model’s training. </span><span class="koboSpan" id="kobo.86.4">This process adjusts the model parameters incrementally to ensure that it understands the language unique to Proxima’s products </span><a id="_idTextAnchor216"/><span class="koboSpan" id="kobo.87.1">and </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">offerings better.</span></span></p>
<h2 id="_idParaDest-126"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.89.1">Training methodologies for financial domain adaptation</span></h2>
<p><span class="koboSpan" id="kobo.90.1">Four our continued training strategy, we’ll employ </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">causal language modeling</span></strong><span class="koboSpan" id="kobo.92.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.93.1">CLM</span></strong><span class="koboSpan" id="kobo.94.1">). </span><span class="koboSpan" id="kobo.94.2">This approach is part of a broader set of training methodologies that optimize model performance for various objectives. </span><span class="koboSpan" id="kobo.94.3">Before moving to implementation, let's try to disambiguate our chosen approach from other popular strategies to better understand the </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">CLM methodology:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Masked Language Modeling</span></strong><span class="koboSpan" id="kobo.97.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.98.1">MLM</span></strong><span class="koboSpan" id="kobo.99.1">): A cornerstone of Transformer-based models </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.100.1">such as BERT, MLM randomly masks parts of the input text and challenges the model to predict the masked tokens. </span><span class="koboSpan" id="kobo.100.2">By considering the entire context around the mask (both before and after), MLM enables a model to develop a bidirectional understanding of language, enriching its grasp of context </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">and semantics.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.102.1">Next-Sentence Prediction</span></strong><span class="koboSpan" id="kobo.103.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.104.1">NSP</span></strong><span class="koboSpan" id="kobo.105.1">): This methodology further broadens a model’s </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.106.1">narrative understanding by training it to discern whether two sentences logically follow each other. </span><span class="koboSpan" id="kobo.106.2">NSP is instrumental in teaching models about text structure and coherence, enabling them to construct and comprehend logical sequences within larger bodies </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">of text.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">CLM</span></strong><span class="koboSpan" id="kobo.109.1">: Our chosen </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.110.1">path for BLOOM’s adaptation diverges here, embracing CLM for its focused, sequential prediction capabilities. </span><span class="koboSpan" id="kobo.110.2">Unlike MLM, which looks both ways (before and after the masked token), CLM adopts a unidirectional approach, predicting each subsequent token based solely on the preceding context. </span><span class="koboSpan" id="kobo.110.3">This method is intrinsically aligned with natural language generation, making it especially suitable for crafting coherent, contextually rich narratives in the </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">target domain.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.112.1">In selecting CLM for BLOOM’s adaptation, we’ll extend the model’s generative capabilities to produce text sequences that are not only logically structured but also deeply embedded with the nuance of the target domain. </span><span class="koboSpan" id="kobo.112.2">CLM’s unidirectional nature ensures that </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.113.1">each token that’s generated is informed by a cohesive understanding of the preceding text, enabling the model to generate detailed, accurate, and domain-specific texts. </span></p>
<p><span class="koboSpan" id="kobo.114.1">Once fine-tuning is complete, we can evaluate the efficacy of the domain-adapted BLOOM model based on its proficiency in generating contextually relevant and domain-specific narratives. </span><span class="koboSpan" id="kobo.114.2">We’ll compare the adapted model’s performance against the original model with a special focus on the model’s fluency, accuracy, and overall comprehension of the </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">target domain.</span></span></p>
<p><span class="koboSpan" id="kobo.116.1">As we’ve done previously, we’ll leverage Google Colab for our initial prototyping phase. </span><span class="koboSpan" id="kobo.116.2">As </span><em class="italic"><span class="koboSpan" id="kobo.117.1">Chapters 4</span></em><span class="koboSpan" id="kobo.118.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.119.1">5</span></em><span class="koboSpan" id="kobo.120.1"> described, Google Colab offers a preconfigured environment that simplifies the process of testing our methodologies before we consider promoting them to production environments. </span><span class="koboSpan" id="kobo.120.2">All the code in this chapter is available in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.121.1">Chapter 6</span></strong></span><span class="koboSpan" id="kobo.122.1"> folder of this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">repository (</span></span><a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python"><span class="No-Break"><span class="koboSpan" id="kobo.124.1">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.125.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">We’ll begin with the initial setup, which involves loading a smaller variation of </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">BLOOM-1b1</span></strong><span class="koboSpan" id="kobo.128.1"> using the Transformers library. </span><span class="koboSpan" id="kobo.128.2">We’ll also import the methods that we’ll need to apply PEFT. </span><span class="koboSpan" id="kobo.128.3">For this example, we’ll rely on a few libraries that can be installed </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">as follows:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.130.1">
pip install sentence-transformers transformers peft datasets</span></pre>
<p><span class="koboSpan" id="kobo.131.1">Once </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.132.1">installed, we can </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">begin importing:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.134.1">
from </span><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.135.1">transformers import (
    AutoTokenizer, AutoModelForCausalLM)
from peft import AdaLoraConfig, get_peft_model</span></pre>
<p><span class="koboSpan" id="kobo.136.1">The next step is to load the tokenizer </span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">and model:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.138.1">
tokenizer = AutoTokenizer.from_pretrained("bigscience/bloom-1b1")
model = AutoModelForCausalLM.from_pretrained(
    "bigscience/bloom-1b1")</span></pre>
<p><span class="koboSpan" id="kobo.139.1">As discussed previously, we’re incorporating PEFT for </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">efficient adaptation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.141.1">
adapter_config = AdaLoraConfig(target_r=16)
model.add_adapter(adapter_config)</span></pre>
<p><span class="koboSpan" id="kobo.142.1">The PEFT technique, specifically through </span><strong class="source-inline"><span class="koboSpan" id="kobo.143.1">AdaLoraConfig</span></strong><span class="koboSpan" id="kobo.144.1">, allows us to introduce a compact, efficient layer so that we can adapt the model to new contexts – here, the finance domain – with a significantly reduced number of </span><span class="No-Break"><span class="koboSpan" id="kobo.145.1">trainable parameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.146.1">
model = get_peft_model(model, adapter_config)
model.print_trainable_parameters()</span></pre>
<p><span class="koboSpan" id="kobo.147.1">We must integrate the adapter to finalize the PEFT model setup, effectively creating a model variant that’s optimized for our domain-specific training while focusing on efficiency. </span><span class="koboSpan" id="kobo.147.2">We can quantify this by examining the number of trainable parameters our model </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">will use:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.149.1">
trainable params: 1,769,760 || all params: 1,067,084,088 || trainable%: 0.1658500974667331</span></pre>
<p><span class="koboSpan" id="kobo.150.1">The preceding code provides us with the </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">following information:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.152.1">Trainable </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.153.1">parameters</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.154.1">: 1,769,760</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.155.1">Total parameters in the </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.156.1">model</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">: 1,067,084,088</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.158.1">Percentage of trainable </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.159.1">parameters</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">: 0.166%</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.161.1">This means that out of over 1 billion parameters in the BLOOM-1b1 model, only about 1.77 million parameters are being fine-tuned for the finance domain adaptation. </span><span class="koboSpan" id="kobo.161.2">This small percentage (0.166%) of trainable parameters highlights the efficiency of PEFT, allowing significant </span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.162.1">model adaptability with minimal adjustments. </span><span class="koboSpan" id="kobo.162.2">This is crucial for practical applications as it reduces both computational costs and the time required </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">for training.</span></span></p>
<p><span class="koboSpan" id="kobo.164.1">Next, we’ll move on to preparing the data. </span><span class="koboSpan" id="kobo.164.2">We’ll assume we have assembled texts encompassing the breadth of knowledge about specialized Proxima products and offerings such </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.165.1">as the </span><strong class="bold"><span class="koboSpan" id="kobo.166.1">Proxima Passkey</span></strong><span class="koboSpan" id="kobo.167.1">. </span><span class="koboSpan" id="kobo.167.2">CLM training requires distinct testing and training phases to evaluate the model’s ability to accurately predict the next token in a sequence. </span><span class="koboSpan" id="kobo.167.3">This ensures it generalizes well beyond the training data to unseen text. </span><span class="koboSpan" id="kobo.167.4">During training, the loss calculation measures the difference between the model’s predicted token probabilities and the actual tokens. </span><span class="koboSpan" id="kobo.167.5">It guides the model to adjust its parameters to minimize this loss, improving its predictive accuracy over iterations. </span><span class="koboSpan" id="kobo.167.6">As such, we must define training and testing texts as our dataset. </span><span class="koboSpan" id="kobo.167.7">An example dataset is included in this book’s GitHub repository (linked earlier in </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">the chapter).</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.169.1">
dataset = load_dataset("text",
    data_files={"train": "./train.txt",
        "test": "./test.txt"}
    )</span></pre>
<p><span class="koboSpan" id="kobo.170.1">Next, we must apply preprocessing and tokenization. </span><span class="koboSpan" id="kobo.170.2">Texts are cleaned, standardized, and then converted into a numerical format (</span><strong class="bold"><span class="koboSpan" id="kobo.171.1">tokens</span></strong><span class="koboSpan" id="kobo.172.1">) that the model can process. </span><span class="koboSpan" id="kobo.172.2">We must also truncate or pad texts to fit the model’s input size constraints and prepare labels for CLM training, where the model learns to predict each subsequent token. </span><span class="koboSpan" id="kobo.172.3">Truncation and padding are preprocessing steps that are used to standardize the length of input texts for machine learning models, particularly those with fixed input size constraints like </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.173.1">many language models. </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">Truncation</span></strong><span class="koboSpan" id="kobo.175.1"> removes parts of the text to shorten inputs that exceed the model’s maximum length, ensuring they fit within the </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.176.1">specified size limit. </span><strong class="bold"><span class="koboSpan" id="kobo.177.1">Padding</span></strong><span class="koboSpan" id="kobo.178.1"> adds filler values (often zeros) to shorter inputs to extend them to the required length, allowing for consistent input dimensions across the dataset. </span><span class="koboSpan" id="kobo.178.2">Consistent input dimensions are necessary to ensure uniformity </span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.179.1">in matrix operations and computations across the entire dataset since LLMs, like other models that rely on deep learning, process inputs through layers of functions that require fixed-size vectors or matrices. </span><span class="koboSpan" id="kobo.179.2">In this case, we’ll set the sequence length to a maximum of </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">512</span></strong><span class="koboSpan" id="kobo.181.1"> tokens so that it aligns with the </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">model’s architecture:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.183.1">
def preprocess_function(examples):
    inputs = tokenizer(examples["text"], truncation=True,
        padding="max_length", max_length=512)
    inputs["labels"] = inputs["input_ids"].copy()
    return inputs</span></pre>
<p><span class="koboSpan" id="kobo.184.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.185.1">TrainingArguments</span></strong><span class="koboSpan" id="kobo.186.1"> class configures the training process, setting parameters such as the batch size, number of epochs, and the directory for saving model checkpoints. </span><span class="koboSpan" id="kobo.186.2">This configuration is crucial for efficient learning and model evaluation. </span><span class="koboSpan" id="kobo.186.3">Meanwhile, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">Trainer</span></strong><span class="koboSpan" id="kobo.188.1"> class orchestrates the model’s training process. </span><span class="koboSpan" id="kobo.188.2">Again, continued training gradually adapts the model’s parameters to generate and understand text related to the </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">Proxima Passkey:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.190.1">
from transformers import Trainer, TrainingArguments
training_args = TrainingArguments(
    output_dir="./model_output",
    per_device_train_batch_size=2,
    num_train_epochs=5,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
    prediction_loss_only=True,
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
)
trainer.train()
model.save_pretrained("./proxima_da_model")</span></pre>
<p><span class="koboSpan" id="kobo.191.1">Generally, our configuration specifies the training parameters and initializes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">Trainer</span></strong><span class="koboSpan" id="kobo.193.1"> class while focusing on domain adaptation. </span><span class="koboSpan" id="kobo.193.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">TrainingArguments</span></strong><span class="koboSpan" id="kobo.195.1"> class is tailored to manage the training process efficiently, including logging and model-saving strategies. </span><span class="koboSpan" id="kobo.195.2">Remember that the batch size we choose for training the model balances the GPU’s memory capacity and how quickly the model learns from the dataset. </span><span class="koboSpan" id="kobo.195.3">A larger </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.196.1">batch size allows more data to be processed at once, speeding up training but requiring more memory, which can be a limitation if the GPU has restricted capacity. </span><span class="koboSpan" id="kobo.196.2">Conversely, a smaller batch size means the model updates its weights more frequently with fewer samples, which can benefit learning but results in slower overall progress through </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">the dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.198.1">With training complete, we can use the adapted model to generate text based on prompts related to the Proxima Passkey. </span><span class="koboSpan" id="kobo.198.2">The model considers the prompt, generates a sequence of tokens representing the continuation, and then decodes this sequence back into </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">human-readable text:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.200.1">
def predict(model, prompt="The Proxima Passkey is"):
    inputs = tokenizer(prompt, return_tensors="pt")
    output = model.generate(**inputs, max_length=50)
    return tokenizer.decode(output[0], skip_special_tokens=True)</span></pre>
<p><span class="koboSpan" id="kobo.201.1">Notice the </span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">model.generate()</span></strong><span class="koboSpan" id="kobo.203.1"> function, which takes tokenized input and produces a sequence of tokens as output. </span><span class="koboSpan" id="kobo.203.2">These tokens are then decoded </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">into text.</span></span></p>
<p><span class="koboSpan" id="kobo.205.1">In this example, we adapted the BLOOM language model so that it specializes in the finance domain. </span><span class="koboSpan" id="kobo.205.2">This involved loading the pre-trained model, applying a PEFT adapter for efficient </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.206.1">domain adaptation, and preparing a financial dataset for model training through standardization and tokenization. </span><span class="koboSpan" id="kobo.206.2">After fine-tuning BLOOM with this domain-specific data, we used the model to generate text relevant to the finance sector. </span><span class="koboSpan" id="kobo.206.3">The final step is to evaluate this adapted model’s performance compared to the original pre-trained version, focusing on its effectiveness in accurately handling fina</span><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.207.1">ncial language </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">and concepts.</span></span></p>
<h2 id="_idParaDest-127"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.209.1">Evaluation and outcome analysis – the ROUGE metric</span></h2>
<p><span class="koboSpan" id="kobo.210.1">Quantitative and qualitative evaluations are essential to assess the adapted BLOOM model against </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.211.1">the original, especially in the context of Proxima’s language. </span><span class="koboSpan" id="kobo.211.2">Quantitatively, the model’s output is compared against a reference dataset that mirrors Proxima’s product language using the </span><strong class="bold"><span class="koboSpan" id="kobo.212.1">ROUGE</span></strong><span class="koboSpan" id="kobo.213.1"> metric. </span><span class="koboSpan" id="kobo.213.2">This comparison helps measure the overlap in key terms and styles. </span><span class="koboSpan" id="kobo.213.3">Additionally, it’s beneficial to develop specific metrics for evaluating the model’s proficiency in terms of financial terminology and concepts relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">to Proxima:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.215.1">
from rouge import Rouge
# Example reference text (what we expect the model to generate after training on a complete dataset)
reference = "Proxima's Passkey enables seamless integration of diverse financial portfolios, offering unparalleled access to global investment opportunities and streamlined asset management."
</span><span class="koboSpan" id="kobo.215.2"># Example predicted model output
predicted = "The Proxima Passkey provides a unified platform for managing various investment portfolios, granting access to worldwide investment options and efficient asset control."
</span><span class="koboSpan" id="kobo.215.3"># Initialize the Rouge metric
rouge = Rouge()
# Compute the Rouge scores
scores = rouge.get_scores(predicted, reference)
print(scores)</span></pre>
<p><span class="koboSpan" id="kobo.216.1">The ROUGE score </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.217.1">would be calculated by comparing the two texts in this example. </span><span class="koboSpan" id="kobo.217.2">The score measures the overlap between the predicted output and the reference text </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.218.1">in terms of </span><strong class="bold"><span class="koboSpan" id="kobo.219.1">n-grams</span></strong><span class="koboSpan" id="kobo.220.1"> (sequences of words). </span><span class="koboSpan" id="kobo.220.2">For instance, </span><strong class="bold"><span class="koboSpan" id="kobo.221.1">ROUGE-N</span></strong><span class="koboSpan" id="kobo.222.1"> (where </span><em class="italic"><span class="koboSpan" id="kobo.223.1">N</span></em><span class="koboSpan" id="kobo.224.1"> can be 1, 2, or L) calculates </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.225.1">the overlap of n-grams between the predicted and </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">reference texts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.227.1">ROUGE-1</span></strong><span class="koboSpan" id="kobo.228.1"> evaluates the overlap of unigrams (individual words) between the predicted and </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">reference texts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.230.1">ROUGE-2</span></strong><span class="koboSpan" id="kobo.231.1"> assesses the overlap of bigrams (two-word phrases) between </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">the texts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.233.1">ROUGE-L</span></strong><span class="koboSpan" id="kobo.234.1"> focuses on the longest common subsequence, which is useful for evaluating sentence-level </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">structure similarity</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.236.1">The ROUGE scores range </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.237.1">from 0 to 1 and quantify the similarity between the predicted text and a reference text, providing insights into how well a model’s output matches the expected content. </span><span class="koboSpan" id="kobo.237.2">Scores closer to 1 indicate higher similarity or overlap, while scores near 0 suggest little to no commonality. </span><span class="koboSpan" id="kobo.237.3">These scores are divided into three key components – precision, recall, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">F1 score:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.239.1">Precision</span></strong><span class="koboSpan" id="kobo.240.1"> measures the proportion of words in the predicted text that are also found in the </span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.241.1">reference text. </span><span class="koboSpan" id="kobo.241.2">A high precision score indicates that most of the words generated by the model are relevant and appear in the reference, signifying accuracy in the </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">model’s output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.243.1">Recall</span></strong><span class="koboSpan" id="kobo.244.1"> assesses the proportion of words from the reference text that are captured in </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.245.1">the model’s prediction. </span><span class="koboSpan" id="kobo.245.2">High recall implies that the model effectively includes most of the relevant content from the reference in its output, </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">indicating comprehensiveness.</span></span></li>
<li><span class="koboSpan" id="kobo.247.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.248.1">F1 score</span></strong><span class="koboSpan" id="kobo.249.1"> is the harmonic mean of precision and recall, balancing the two. </span><span class="koboSpan" id="kobo.249.2">It is especially </span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.250.1">useful for understanding the model’s overall accuracy in generating text that is both relevant (precision) and comprehensive (recall). </span><span class="koboSpan" id="kobo.250.2">The F1 score is crucial when equal importance is given to precision and recall in evaluating the </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">model’s performance.</span></span></li>
<li><span class="koboSpan" id="kobo.252.1">Here’s </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">the output:</span></span></li>
</ul>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.254.1">Metric</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.255.1">Recall (r)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.256.1">Precision (p)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.257.1">F1 </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.258.1">Score (f)</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.259.1">ROUGE-1</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.260.1">0.35</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.261.1">0.333</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.262.1">0.341</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.263.1">ROUGE-2</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.264.1">0.053</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.265.1">0.048</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.266.1">0.05</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.267.1">ROUGE-L</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.268.1">0.35</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.269.1">0.333</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.270.1">0.341</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.271.1">Table 6.1: ROUGE metric outcomes</span></p>
<p><span class="koboSpan" id="kobo.272.1">These scores indicate a moderate level of unigram overlap (ROUGE-1) between the texts but a significantly lower bigram overlap (ROUGE-2). </span><span class="koboSpan" id="kobo.272.2">The similarity between the ROUGE-1 and ROUGE-L scores suggests the model captures individual key terms to some extent but may struggle with longer phrase structures, pointing to areas for </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">model improvement.</span></span></p>
<p><span class="koboSpan" id="kobo.274.1">Overall, while the model demonstrates a basic grasp of key individual terms (as shown by ROUGE-1 and ROUGE-L), its ability to replicate more complex structures or phrases from the reference text (as indicated by ROUGE-2) is quite limited. </span><span class="koboSpan" id="kobo.274.2">This suggests that while the model has some understanding of the domain-specific language, further fine-tuning is required for it to effectively replicate the more nuanced and structured aspects of the reference texts. </span><span class="koboSpan" id="kobo.274.3">Keep in mind that, as we have seen in other chapters, semantic similarity is also a good measure of domain-specific language understanding and does not rely on lexical overlap the way </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">ROUGE does.</span></span></p>
<p><span class="koboSpan" id="kobo.276.1">Qualitatively, domain experts should review the model’s outputs to judge their relevance and </span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.277.1">accuracy in the context of Proxima’s products and institutional language. </span><span class="koboSpan" id="kobo.277.2">These experts can provide insights into the nuances of the model’s performance, which might not be captured by quantitative metrics alone. </span><span class="koboSpan" id="kobo.277.3">Comparing their feedback on the outputs from both the original and adapted models will highlight how well the adaptation has aligned BLOOM with Proxima’s specific communication needs. </span><span class="koboSpan" id="kobo.277.4">This dual approach ensures a comprehensive evaluation, blending statistical analysis with real-world applicability </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">and relevance.</span></span><a id="_idTextAnchor221"/></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.279.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.280.1">In this chapter, we explored the domain adaptation process for the BLOOM LLM, which is specifically tailored to enhance its proficiency in the financial sector, particularly in understanding and generating content related to Proxima’s product offerings. </span><span class="koboSpan" id="kobo.280.2">We began by introducing the concept of domain adaptation within the broader scope of transfer learning, emphasizing its significance in fine-tuning general-purpose models to grasp the intricacies of </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">specialized fields.</span></span></p>
<p><span class="koboSpan" id="kobo.282.1">The adaptation process involved integrating PEFT techniques into BLOOM and preprocessing a financial dataset for model training. </span><span class="koboSpan" id="kobo.282.2">This included standardizing text lengths through truncation and padding and tokenizing the texts for consistency in model input. </span><span class="koboSpan" id="kobo.282.3">The adapted model’s performance was then quantitatively assessed against a reference dataset using the ROUGE metric, providing insights into its ability to capture key financial terminologies and phrases. </span><span class="koboSpan" id="kobo.282.4">Qualitative evaluation by domain experts was also suggested as a complementary method to gauge the model’s practical effectiveness in </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">real-world scenarios.</span></span></p>
<p><span class="koboSpan" id="kobo.284.1">Overall, this chapter detailed a common approach to refining an LLM for a specific domain, illustrating both the methodology and the importance of a nuanced evaluation to ascertain the success of such adaptations. </span><span class="koboSpan" id="kobo.284.2">In the next chapter, we will explore how to adapt an LLM without fine-tuning using prompt engineering. </span><span class="koboSpan" id="kobo.284.3">We will discover how to contextualize and guide model outputs to produce similar results comparable to </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">fine-tuned models.</span></span><a id="_idTextAnchor223"/></p>
<h1 id="_idParaDest-129"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.286.1">References</span></h1>
<p><span class="koboSpan" id="kobo.287.1">This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">subject matter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.289.1">Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., &amp; Smith, N. </span><span class="koboSpan" id="kobo.289.2">A. </span><span class="koboSpan" id="kobo.289.3">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.290.1">Don’t stop pretraining: Adapt language models to domains and tasks</span></em><span class="koboSpan" id="kobo.291.1">. </span><span class="koboSpan" id="kobo.291.2">In arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">cs.CL]. </span></span><a href="http://arxiv.org/abs/2004.10964/"><span class="No-Break"><span class="koboSpan" id="kobo.293.1">http://arxiv.org/abs/2004.10964/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.294.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.295.1">Pruksachatkun, Y., Phang, J., Liu, H., Htut, P. </span><span class="koboSpan" id="kobo.295.2">M., Zhang, X., Pang, R. </span><span class="koboSpan" id="kobo.295.3">Y., Vania, C., Kann, K., &amp; Bowman, S. </span><span class="koboSpan" id="kobo.295.4">R. </span><span class="koboSpan" id="kobo.295.5">(2020a). </span><em class="italic"><span class="koboSpan" id="kobo.296.1">Intermediate-task transfer learning with pretrained language models: When and why does it work?</span></em><span class="koboSpan" id="kobo.297.1"> Proceedings of the 58th Annual Meeting of the Association for </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">Computational Linguistics.</span></span></li>
<li><span class="koboSpan" id="kobo.299.1">Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., &amp; Le, Q. </span><span class="koboSpan" id="kobo.299.2">V. </span><span class="koboSpan" id="kobo.299.3">(n.d.). </span><em class="italic"><span class="koboSpan" id="kobo.300.1">Unsupervised Data Augmentation for Consistency Training</span></em><span class="koboSpan" id="kobo.301.1">. </span><span class="koboSpan" id="kobo.301.2">Arxiv.org. </span><span class="koboSpan" id="kobo.301.3">Retrieved March 16, 2024, </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">from </span></span><a href="http://arxiv.org/abs/1904.12848"><span class="No-Break"><span class="koboSpan" id="kobo.303.1">http://arxiv.org/abs/1904.12848</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.304.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.305.1">Anaby-Tavor, A., Carmeli, B., Goldbraich, E., Kantor, A., Kour, G., Shlomov, S., Tepper, N., &amp; Zwerdling, N. </span><span class="koboSpan" id="kobo.305.2">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.306.1">Do not have enough data? </span><span class="koboSpan" id="kobo.306.2">Deep learning to the rescue!</span></em><span class="koboSpan" id="kobo.307.1"> Proceedings of the ... </span><span class="koboSpan" id="kobo.307.2">AAAI Conference on Artificial Intelligence. </span><span class="koboSpan" id="kobo.307.3">AAAI Conference on Artificial Intelligence, 34(05), </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">7383–7390. </span></span><a href="https://doi.org/10.1609/aaai.v34i05.6233"><span class="No-Break"><span class="koboSpan" id="kobo.309.1">https://doi.org/10.1609/aaai.v34i05.6233</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.310.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.311.1">Clark, K., Luong, M.-T., Khandelwal, U., Manning, C. </span><span class="koboSpan" id="kobo.311.2">D., &amp; Le, Q. </span><span class="koboSpan" id="kobo.311.3">V. </span><span class="koboSpan" id="kobo.311.4">(2019). </span><em class="italic"><span class="koboSpan" id="kobo.312.1">BAM! </span><span class="koboSpan" id="kobo.312.2">Born-again multi-task networks for natural language understanding</span></em><span class="koboSpan" id="kobo.313.1">. </span><span class="koboSpan" id="kobo.313.2">In arXiv [</span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">cs.CL]. </span></span><a href="http://arxiv.org/abs/1907.04829"><span class="No-Break"><span class="koboSpan" id="kobo.315.1">http://arxiv.org/abs/1907.04829</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.316.1">.</span></span></li>
</ul>
</div>
</body></html>