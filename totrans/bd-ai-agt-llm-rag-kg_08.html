<html><head></head><body>
<div id="_idContainer247">
<h1 class="chapter-number" id="_idParaDest-138"><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-139"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.2.1">Reinforcement Learning and AI Agents</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In </span><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapters 5–7</span></em><span class="koboSpan" id="kobo.5.1">, we discussed how to provide our model with access to external memory. </span><span class="koboSpan" id="kobo.5.2">This memory was stored in a database of a different type (vector or graph), and through a search, we could look up the information needed to answer a question. </span><span class="koboSpan" id="kobo.5.3">The model would then receive all the information that was needed in context and then answer, providing definite and discrete </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">real-world information.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">However, as we saw later in </span><a href="B21257_07.xhtml#_idTextAnchor113"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.9.1">, LLMs have limited knowledge and understanding of the real world (both when it comes to commonsense reasoning and when it comes to </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">spatial relations).</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Humans learn to move in space and interact with the environment through exploration. </span><span class="koboSpan" id="kobo.11.2">In a process that is trial and error, we humans learn that we cannot touch fire or how to find our way home. </span><span class="koboSpan" id="kobo.11.3">Likewise, we learn how to relate to other human beings through interactions with them. </span><span class="koboSpan" id="kobo.11.4">Our interactions with the real world allow us to learn but also to modify our surroundings. </span><span class="koboSpan" id="kobo.11.5">The environment provides us with information through perception, information that we process and learn from, and ultimately use to modify the environment. </span><span class="koboSpan" id="kobo.11.6">This is a cyclical process in which we sense changes in the environment </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">and respond.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">We do not learn all these skills just by reading from a book; it wouldn’t be possible. </span><span class="koboSpan" id="kobo.13.2">Interaction with the environment, therefore, is critical to learn certain skills and knowledge. </span><span class="koboSpan" id="kobo.13.3">Without this, we would find it difficult to do certain tasks. </span><span class="koboSpan" id="kobo.13.4">So, we need a system that allows artificial intelligence to interact and learn from the environment through exploration. </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">reinforcement learning</span></strong><span class="koboSpan" id="kobo.15.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.16.1">RL</span></strong><span class="koboSpan" id="kobo.17.1">) is </span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.18.1">a paradigm that focuses on describing how an intelligent agent can take actions in a dynamic environment. </span><span class="koboSpan" id="kobo.18.2">RL governs the behavior of an agent, what actions to take in a given environment (and the state of that environment), and how to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">from it.</span></span></p>
<p><span class="koboSpan" id="kobo.20.1">In this chapter, therefore, we will discuss RL. </span><span class="koboSpan" id="kobo.20.2">We will start with some theory on the topic. </span><span class="koboSpan" id="kobo.20.3">We will start with a simple case in which an agent needs to understand how to balance exploration with exploitation in order to find the winning strategy to solve a problem. </span><span class="koboSpan" id="kobo.20.4">Once the basics are defined, we will describe how we can use a neural network as an agent. </span><span class="koboSpan" id="kobo.20.5">We will look at some of the most popular algorithms used nowadays for interacting and learning from the environment. </span><span class="koboSpan" id="kobo.20.6">In addition, we will show how an agent can be used to be able to explore an environment (such as training an agent to solve a video game). </span><span class="koboSpan" id="kobo.20.7">In the last </span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.21.1">section, we will discuss the intersection of </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">large language models</span></strong><span class="koboSpan" id="kobo.23.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.24.1">LLMs</span></strong><span class="koboSpan" id="kobo.25.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">and RL.</span></span></p>
<p><span class="koboSpan" id="kobo.27.1">In this chapter, we'll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.29.1">Introduction to </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">reinforcement learning</span></span></li>
<li><span class="koboSpan" id="kobo.31.1">Deep </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">reinforcement learning</span></span></li>
<li><span class="koboSpan" id="kobo.33.1">LLM interactions with </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">RL models</span></span></li>
</ul>
<h1 id="_idParaDest-140"><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.35.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.36.1">Most of this code can be run on a CPU, but it is preferable to run it on a GPU. </span><span class="koboSpan" id="kobo.36.2">This is especially true when we are discussing how to train an agent to learn how to play a video game. </span><span class="koboSpan" id="kobo.36.3">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, OpenAI Gym). </span><span class="koboSpan" id="kobo.36.4">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8"><span class="No-Break"><span class="koboSpan" id="kobo.38.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.39.1">.</span></span></p>
<h1 id="_idParaDest-141"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.40.1">Introduction to reinforcement learning</span></h1>
<p><span class="koboSpan" id="kobo.41.1">In previous chapters, we </span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.42.1">discussed a model that learns from a large amount of text. </span><span class="koboSpan" id="kobo.42.2">Humans—and increasingly, AI agents—learn best through trial and error. </span><span class="koboSpan" id="kobo.42.3">Imagine a child learning to stack blocks or riding a bike. </span><span class="koboSpan" id="kobo.42.4">There’s no explicit teacher guiding each move; instead, the child learns by acting, observing the results, and adjusting. </span><span class="koboSpan" id="kobo.42.5">This interaction with the environment—where actions lead to outcomes and those outcomes shape future behavior—is central to how we learn. </span><span class="koboSpan" id="kobo.42.6">Unlike passive learning from books or text, this kind of learning is goal-directed and grounded in experience. </span><span class="koboSpan" id="kobo.42.7">To enable machines to learn in a similar way, we need a new approach. </span><span class="koboSpan" id="kobo.42.8">This learning paradigm is </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">called RL.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">More formally, an infant learns from their interaction with the environment, from the consequential relationship of an action and its effect. </span><span class="koboSpan" id="kobo.44.2">A child’s learning is not simply exploratory but aimed at a specific goal; they learn what actions must be taken to achieve a goal. </span><span class="koboSpan" id="kobo.44.3">Throughout our lives, our learning is often related to our interaction with the environment and how it responds in response to our behavior. </span><span class="koboSpan" id="kobo.44.4">These concepts are seen as the basis of both learning theory and intelligence </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">in general.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">RL is defined </span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.47.1">as a branch of machine learning, where a system must make decisions to maximize cumulative rewards in a given situation. </span><span class="koboSpan" id="kobo.47.2">Unlike in supervised learning (wherein a model learns from labeled examples) or unsupervised learning (wherein a model learns by detecting patterns in the data), in RL the model learns from experience. </span><span class="koboSpan" id="kobo.47.3">In fact, the system is not told what actions it must perform but must explore the environment and find out what actions allow it to have a reward. </span><span class="koboSpan" id="kobo.47.4">In more complex situations, these rewards may not be immediate but come only later (e.g., sacrificing a piece in chess but achieving victory later). </span><span class="koboSpan" id="kobo.47.5">So, on a general level, we can say that the basics of RL are trial and error and the possibility of </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">delayed reward.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">From this, we derive two important concepts that will form the basis of our discussion </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">of RL:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.51.1">Exploration versus exploitation</span></strong><span class="koboSpan" id="kobo.52.1">: The model must exploit previously acquired </span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.53.1">knowledge to achieve its goal. </span><span class="koboSpan" id="kobo.53.2">At the same time, it must explore the environment in order to make better choices in the future. </span><span class="koboSpan" id="kobo.53.3">A balance must be struck between these two aspects, because solving a problem may not be the most obvious path. </span><span class="koboSpan" id="kobo.53.4">A model therefore must test different types of actions (explore) before it can exploit the best action (exploitation). </span><span class="koboSpan" id="kobo.53.5">Even today, choosing the best balance is an open challenge for RL theory. </span><span class="koboSpan" id="kobo.53.6">A helpful way to understand this is by imagining someone trying different restaurants in a new city. </span><span class="koboSpan" id="kobo.53.7">At first, they might try a variety of places (exploration) to see what’s available. </span><span class="koboSpan" id="kobo.53.8">After discovering a few favorites, they might start going to the same ones more often (exploitation). </span><span class="koboSpan" id="kobo.53.9">But if they always stick to the familiar spots, they might miss out on finding an even better restaurant. </span><span class="koboSpan" id="kobo.53.10">The challenge is knowing when to try something new and when to stick with what works—and this is still an open question </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">in RL.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.55.1">Achieving a global goal in an uncertain environment</span></strong><span class="koboSpan" id="kobo.56.1">: RL focuses on achieving a</span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.57.1"> goal without requiring the problem to be reframed into subproblems. </span><span class="koboSpan" id="kobo.57.2">Instead, it addresses a classic supervised machine learning challenge, which involves breaking a complex problem into general subproblems and devising an effective schedule. </span><span class="koboSpan" id="kobo.57.3">In the case of RL, on the other hand, one directly defines a general problem that an agent must solve. </span><span class="koboSpan" id="kobo.57.4">This does not mean that there has to be only one agent but there can be multiple agents with a clear goal interacting with each other. </span><span class="koboSpan" id="kobo.57.5">A relatable example would be learning to commute efficiently in a new city. </span><span class="koboSpan" id="kobo.57.6">At first, you don’t break the task into subproblems like “learn the bus schedule,” “estimate walking time,” or “optimize weather exposure.” </span><span class="koboSpan" id="kobo.57.7">Instead, you treat the goal as a whole: get to work on time every day. </span><span class="koboSpan" id="kobo.57.8">Through trial and error—taking different routes, trying trains versus buses, adjusting for traffic—you learn which options work best. </span><span class="koboSpan" id="kobo.57.9">Over time, you build a strategy without ever explicitly labeling every part of the problem. </span><span class="koboSpan" id="kobo.57.10">If you live with roommates or friends who are doing the same, you might exchange tips or compete for the fastest route, just like multiple agents interacting </span><span class="No-Break"><span class="koboSpan" id="kobo.58.1">in RL.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.59.1">There are several</span><a id="_idIndexMarker913"/><span class="koboSpan" id="kobo.60.1"> elements that are present in</span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.61.1"> an RL system: an </span><strong class="bold"><span class="koboSpan" id="kobo.62.1">agent</span></strong><span class="koboSpan" id="kobo.63.1">, the </span><strong class="bold"><span class="koboSpan" id="kobo.64.1">environment</span></strong><span class="koboSpan" id="kobo.65.1">, a </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">state</span></strong><span class="koboSpan" id="kobo.67.1">, a </span><strong class="bold"><span class="koboSpan" id="kobo.68.1">policy</span></strong><span class="koboSpan" id="kobo.69.1">, a </span><strong class="bold"><span class="koboSpan" id="kobo.70.1">reward signal</span></strong><span class="koboSpan" id="kobo.71.1">, and a </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">value function</span></strong><span class="koboSpan" id="kobo.73.1">. </span><span class="koboSpan" id="kobo.73.2">The agent is clearly the learner or decision-maker (the model that interacts with the environment, makes decisions, and takes actions). </span><span class="koboSpan" id="kobo.73.3">The environment, on the other hand, is everything that the environment interacts with. </span><span class="koboSpan" id="kobo.73.4">A state represents a particular condition or configuration of the environment at a certain time (for example, the state of the pieces on a chessboard before a move). </span><span class="koboSpan" id="kobo.73.5">Given a state, the agent must make choices and choose an action to take. </span><span class="koboSpan" id="kobo.73.6">Not all space is always observable; our agent can only have access to a partial description of the state. </span><span class="koboSpan" id="kobo.73.7">For example, a robotic agent navigating a maze can only get information through the camera and thus observe only what is in front of it. </span><span class="koboSpan" id="kobo.73.8">The information obtained from the camera is an observation, so the model will use only a subset of </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">the state.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer213">
<span class="koboSpan" id="kobo.75.1"><img alt="Figure 8.1 – Representation of elements in the RL system" src="image/B21257_08_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.76.1">Figure 8.1 – Representation of elements in the RL system</span></p>
<p><span class="koboSpan" id="kobo.77.1">In the </span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.78.1">preceding figure, we can see how the environment (in this case, the game screen) is represented in vector form (this is the state). </span><span class="koboSpan" id="kobo.78.2">Also, the three possible actions are represented in this case with a scalar. </span><span class="koboSpan" id="kobo.78.3">This allows us to be able to train </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">an algorithm.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.80.1">Actions</span></strong><span class="koboSpan" id="kobo.81.1"> are</span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.82.1"> the possible</span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.83.1"> decisions or moves that an agent can conduct in an environment (the pieces on the chessboard can only move in certain directions: a bishop only diagonally, the rook vertically or horizontally, and so on). </span><span class="koboSpan" id="kobo.83.2">The action set can be discrete (movements in the maze) but also a continuous action space (in this case, it will be real-value vectors). </span><span class="koboSpan" id="kobo.83.3">These actions are part of a strategy to achieve a certain goal, according to the state of the environment </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">and policy.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer214">
<span class="koboSpan" id="kobo.85.1"><img alt="Figure 8.2 – Interaction of the agent with the environment selecting an action" src="image/B21257_08_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.86.1">Figure 8.2 – Interaction of the agent with the environment selecting an action</span></p>
<p><span class="koboSpan" id="kobo.87.1">In the preceding figure, we can see that time 0 (</span><em class="italic"><span class="koboSpan" id="kobo.88.1">t0</span></em><span class="koboSpan" id="kobo.89.1">) corresponds to a state </span><em class="italic"><span class="koboSpan" id="kobo.90.1">t0</span></em><span class="koboSpan" id="kobo.91.1">; if our agent acts with a move, this changes the environment. </span><span class="koboSpan" id="kobo.91.2">At time </span><em class="italic"><span class="koboSpan" id="kobo.92.1">t1</span></em><span class="koboSpan" id="kobo.93.1">, the environment will be different and therefore we will have a different </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">state, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.95.1">t1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">policy</span></strong><span class="koboSpan" id="kobo.99.1"> defines </span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.100.1">how an agent behaves at a certain time. </span><span class="koboSpan" id="kobo.100.2">Given, then, the state of the environment and the possible actions, the policy maps the action to the state of the system. </span><span class="koboSpan" id="kobo.100.3">The policy can be a set of rules, a lookup table, a function, or something else. </span><span class="koboSpan" id="kobo.100.4">The policy can also be stochastic by specifying a probability for each action. </span><span class="koboSpan" id="kobo.100.5">In a sense, policy is the heart of RL because it determines the agent’s behavior. </span><span class="koboSpan" id="kobo.100.6">In psychology, this can be defined as a set of stimulus-response rules. </span><span class="koboSpan" id="kobo.100.7">For example, a policy might be to eat an opponent’s piece whenever the opportunity arises. </span><span class="koboSpan" id="kobo.100.8">More often, a policy is parameterized: the output of the policy is a computable function that depends on a set of parameters. </span><span class="koboSpan" id="kobo.100.9">One of the most widely used systems is a neural network whose parameters are optimized with an </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">optimization algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.102.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.103.1">reward</span></strong><span class="koboSpan" id="kobo.104.1"> is a </span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.105.1">positive or negative signal received from the environment. </span><span class="koboSpan" id="kobo.105.2">It is another critical factor because it provides a goal to the agent at each time step. </span><span class="koboSpan" id="kobo.105.3">This reward is used to define both the local and global objectives of an agent. </span><span class="koboSpan" id="kobo.105.4">In other words, at each time step, the agent receives a signal from the environment (usually a single number), and in the long run, the agent’s goal is to optimize this reward. </span><span class="koboSpan" id="kobo.105.5">The reward then allows us to determine whether the model is behaving correctly or not and allows us to understand the difference between positive and </span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.106.1">negative events, to understand our interaction with the environment and the appropriate response to the state of the system. </span><span class="koboSpan" id="kobo.106.2">For example, losing a piece can be considered a local negative reward, and winning the game a global reward. </span><span class="koboSpan" id="kobo.106.3">The reward is often used to change the policy and calibrate it in response to </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">the environment.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer215">
<span class="koboSpan" id="kobo.108.1"><img alt="Figure 8.3 – Example of positive and negative reward" src="image/B21257_08_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.109.1">Figure 8.3 – Example of positive and negative reward</span></p>
<p><span class="koboSpan" id="kobo.110.1">The reward, though, gives</span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.111.1"> us information about what is right in the immediate moment while a </span><strong class="bold"><span class="koboSpan" id="kobo.112.1">value function</span></strong><span class="koboSpan" id="kobo.113.1"> defines what is the best approach in the long run. </span><span class="koboSpan" id="kobo.113.2">In more technical terms, the value of a state is the total amount of reward that an agent can expect to get in the future, starting from that state (for example, how many points in a game the agent can collect starting from that position). </span><span class="koboSpan" id="kobo.113.3">In simple words, the value function helps us understand what happens if we consider that state and subsequent states, and what is likely to happen in the future. </span><span class="koboSpan" id="kobo.113.4">There is, however, a dependence between rewards and value; without the former, we cannot calculate the latter, despite that our real goal is value. </span><span class="koboSpan" id="kobo.113.5">For example, sacrificing a piece has a low reward but may ultimately be the key to winning the game. </span><span class="koboSpan" id="kobo.113.6">Clearly, establishing a reward is much easier, while it is difficult to establish a value function because we have to take into account not only the current state, but all previous observations conducted by </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">the agent.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">A classic example of RL is an agent who has to navigate a maze. </span><span class="koboSpan" id="kobo.115.2">A state </span><em class="italic"><span class="koboSpan" id="kobo.116.1">S</span></em><span class="koboSpan" id="kobo.117.1"> defines the agent’s position in the maze; this agent has a possible set of actions </span><em class="italic"><span class="koboSpan" id="kobo.118.1">A</span></em><span class="koboSpan" id="kobo.119.1"> (move east, west, north, or south). </span><span class="koboSpan" id="kobo.119.2">The policy </span><em class="italic"><span class="koboSpan" id="kobo.120.1">π</span></em><span class="koboSpan" id="kobo.121.1"> indicates what action the agent must take in a certain state. </span><span class="koboSpan" id="kobo.121.2">A reward </span><em class="italic"><span class="koboSpan" id="kobo.122.1">R</span></em><span class="koboSpan" id="kobo.123.1"> can be a penalty when the agent chooses an action that is not allowed (slamming on a wall, for example), and the value is getting out of the maze. </span><span class="koboSpan" id="kobo.123.2">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.124.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.125.1">.4</span></em><span class="koboSpan" id="kobo.126.1">, we have a depiction of the interactions between an agent and </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">its environment:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer216">
<span class="koboSpan" id="kobo.128.1"><img alt="Figure 8.4 – Overview model of reinforcement learning system (https://arxiv.org/pdf/2408.07712)" src="image/B21257_08_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.129.1">Figure 8.4 – Overview model of reinforcement learning system (</span><a href="https://arxiv.org/pdf/2408.07712"><span class="koboSpan" id="kobo.130.1">https://arxiv.org/pdf/2408.07712</span></a><span class="koboSpan" id="kobo.131.1">)</span></p>
<p><span class="koboSpan" id="kobo.132.1">At a </span><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.133.1">given time step (</span><em class="italic"><span class="koboSpan" id="kobo.134.1">t</span></em><span class="koboSpan" id="kobo.135.1">), an agent observes the state of the environment (</span><em class="italic"><span class="koboSpan" id="kobo.136.1">S</span></em><span class="subscript"><span class="koboSpan" id="kobo.137.1">t</span></span><span class="koboSpan" id="kobo.138.1">), chooses an action (</span><em class="italic"><span class="koboSpan" id="kobo.139.1">A</span></em><span class="subscript"><span class="koboSpan" id="kobo.140.1">t</span></span><span class="koboSpan" id="kobo.141.1">) according to policy </span><em class="italic"><span class="koboSpan" id="kobo.142.1">π</span></em><span class="koboSpan" id="kobo.143.1">, and receives a reward (</span><em class="italic"><span class="koboSpan" id="kobo.144.1">R</span></em><span class="subscript"><span class="koboSpan" id="kobo.145.1">t</span></span><span class="koboSpan" id="kobo.146.1">). </span><span class="koboSpan" id="kobo.146.2">At this point, the cycle repeats at the new state (</span><em class="italic"><span class="koboSpan" id="kobo.147.1">S</span></em><span class="subscript"><span class="koboSpan" id="kobo.148.1">t+1</span></span><span class="koboSpan" id="kobo.149.1">). </span><span class="koboSpan" id="kobo.149.2">The policy can be static or updated at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">each cycle.</span></span></p>
<p><span class="koboSpan" id="kobo.151.1">In the next section, we will begin to discuss an initial example of RL, starting with the classic </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">multi-armed bandit.</span></span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.153.1">The multi-armed bandit problem</span></h2>
<p><span class="koboSpan" id="kobo.154.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.155.1">k-armed bandit problem</span></strong><span class="koboSpan" id="kobo.156.1"> is</span><a id="_idIndexMarker923"/><span class="koboSpan" id="kobo.157.1"> perhaps</span><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.158.1"> the most classic example to introduce RL. </span><span class="koboSpan" id="kobo.158.2">RL is needed for all those problems in </span><a id="_idIndexMarker925"/><span class="koboSpan" id="kobo.159.1">which a </span><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.160.1">model must learn from its actions rather than be instructed by a positive example. </span><span class="koboSpan" id="kobo.160.2">In the </span><em class="italic"><span class="koboSpan" id="kobo.161.1">k</span></em><span class="koboSpan" id="kobo.162.1">-armed bandit problem, we have a slot machine with </span><em class="italic"><span class="koboSpan" id="kobo.163.1">n</span></em><span class="koboSpan" id="kobo.164.1"> independent arms (bandits), and each of these bandits has its own rigged probability distribution of success. </span><span class="koboSpan" id="kobo.164.2">Each time we pull an arm, we have a stochastic probability of either receiving a reward or failing. </span><span class="koboSpan" id="kobo.164.3">At each action, we have to choose which lever to pull, and the rewards are what we gain. </span><span class="koboSpan" id="kobo.164.4">The goal is to maximize our expected total reward over a certain period of time (e.g., 1,000 actions or time steps). </span><span class="koboSpan" id="kobo.164.5">In other words, we have to figure out which levers give us the best payoff, and we will maximize our actions on them (i.e., we will pull them </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">more often).</span></span></p>
<p><span class="koboSpan" id="kobo.166.1">The </span><a id="_idIndexMarker927"/><span class="koboSpan" id="kobo.167.1">problem</span><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.168.1"> may appear simple, but it is far from trivial. </span><span class="koboSpan" id="kobo.168.2">Our agent does not have access to the true bandit probability distribution and must learn the most favorable bandits through trial and error. </span><span class="koboSpan" id="kobo.168.3">Moreover, as simple as this problem is, it has similarities to several real-world case scenarios: choosing the best treatment for a patient, A/B testing, social media influence, and so on. </span><span class="koboSpan" id="kobo.168.4">At each time step </span><em class="italic"><span class="koboSpan" id="kobo.169.1">t</span></em><span class="koboSpan" id="kobo.170.1">, we can select an </span><em class="italic"><span class="koboSpan" id="kobo.171.1">A</span></em><span class="subscript"><span class="koboSpan" id="kobo.172.1">t</span></span><span class="koboSpan" id="kobo.173.1"> action and get the corresponding reward (</span><em class="italic"><span class="koboSpan" id="kobo.174.1">R</span></em><span class="subscript"><span class="koboSpan" id="kobo.175.1">t</span></span><span class="koboSpan" id="kobo.176.1">). </span><span class="koboSpan" id="kobo.176.2">The value of an arbitrary action </span><em class="italic"><span class="koboSpan" id="kobo.177.1">a</span></em><span class="koboSpan" id="kobo.178.1">, defined as </span><em class="italic"><span class="koboSpan" id="kobo.179.1">q</span></em><em class="italic"><span class="koboSpan" id="kobo.180.1">⇤</span></em><em class="italic"><span class="koboSpan" id="kobo.181.1">(a)</span></em><span class="koboSpan" id="kobo.182.1">, is the expected reward if we selected this action at time </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">step </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.184.1">t</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>*</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>]</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.186.1">If we knew the value of each action, we would have practically solved the problem already (we would always select the one with the highest value). </span><span class="koboSpan" id="kobo.186.2">However, we do not know the value of an action, but we can calculate an estimated value, defined as </span><em class="italic"><span class="koboSpan" id="kobo.187.1">Q</span></em><span class="subscript"><span class="koboSpan" id="kobo.188.1">t(a)</span></span><span class="koboSpan" id="kobo.189.1">, which we wish to be close to </span><em class="italic"><span class="koboSpan" id="kobo.190.1">q*(a)</span></em><span class="koboSpan" id="kobo.191.1">. </span><span class="koboSpan" id="kobo.191.2">At each time step, we have estimated values </span><em class="italic"><span class="koboSpan" id="kobo.192.1">Q</span></em><span class="subscript"><span class="koboSpan" id="kobo.193.1">t(a)</span></span><span class="koboSpan" id="kobo.194.1"> that are greater than the others; selecting these actions (pulling the arms) is called greedy actions and exploiting the current knowledge. </span><span class="koboSpan" id="kobo.194.2">Conversely, selecting an action with a lower estimated value is referred to as exploration (because it allows us to explore what happens with other actions and thus improve our estimation of </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">these actions).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer217">
<span class="koboSpan" id="kobo.196.1"><img alt="Figure 8.5 – Multi-arm bandit" src="image/B21257_08_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.197.1">Figure 8.5 – Multi-arm bandit</span></p>
<p><span class="koboSpan" id="kobo.198.1">Exploring </span><a id="_idIndexMarker929"/><span class="koboSpan" id="kobo.199.1">may bring a decrease in gain in later steps but guarantees a greater gain in the long run. </span><span class="koboSpan" id="kobo.199.2">This is because</span><a id="_idIndexMarker930"/><span class="koboSpan" id="kobo.200.1"> our estimation may not be correct. </span><span class="koboSpan" id="kobo.200.2">Exploring allows us to correct the estimated value for an action. </span><span class="koboSpan" id="kobo.200.3">Especially in the early steps, it is more important to explore so that the system can understand which actions are best. </span><span class="koboSpan" id="kobo.200.4">In the final steps, the model should exploit the best actions. </span><span class="koboSpan" id="kobo.200.5">For this, we need a system that allows us to balance exploration </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">toward exploitation.</span></span></p>
<p><span class="koboSpan" id="kobo.202.1">To get an initial estimate of value, we can take an average of the rewards that </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">are received:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>Q</mi><mi>t</mi></msub><mfenced close=")" open="("><mi>a</mi></mfenced><mo>=</mo><mfrac><mrow><mi>s</mi><mi>u</mi><mi>m</mi><mi>o</mi><mi>f</mi><mi>r</mi><mi>e</mi><mi>w</mi><mi>a</mi><mi>r</mi><mi>d</mi><mi>s</mi><mi>w</mi><mi>h</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>t</mi></mrow><mrow><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mi>t</mi><mi>o</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>·</mo><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub></mrow></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msubsup><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.204.1">In this simple equation, </span><em class="italic"><span class="koboSpan" id="kobo.205.1">1</span></em><span class="koboSpan" id="kobo.206.1"> represents a variable that indicates whether the action was used at the time step (1 if used, 0 if </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">not used).</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Space"><math display="block"><mrow><mrow><msub><mn>1</mn><mrow><mo>{</mo><msub><mi>A</mi><mi>i</mi></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></msub><mo>=</mo><mfenced close="" open="{"><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline" rowspacing="1.0000ex"><mtr><mtd><mrow><mn>1</mn><mi>i</mi><mi>f</mi><mi>a</mi><mi>c</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>i</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>a</mi><mi>t</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi>i</mi></mrow></mtd></mtr><mtr><mtd><mrow><mn>0</mn><mi>o</mi><mi>t</mi><mi>h</mi><mi>e</mi><mi>r</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>e</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.208.1">If the action has never been used, the denominator would be zero; to avoid the result being infinite, we use a default value (e.g., 0). </span><span class="koboSpan" id="kobo.208.2">If the number of steps goes to infinity, the estimated value should converge to the true value. </span><span class="koboSpan" id="kobo.208.3">Once these estimated values are obtained, we can choose the action. </span><span class="koboSpan" id="kobo.208.4">The easiest way to select an action is to choose the highest value (greedy action). </span><span class="koboSpan" id="kobo.208.5">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.209.1">arg max</span></strong><span class="koboSpan" id="kobo.210.1"> function does </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">exactly that:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi>a</mi></munder><msub><mi>Q</mi><mi>t</mi></msub><mfenced close=")" open="("><mi>a</mi></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.212.1">As we </span><a id="_idIndexMarker931"/><span class="koboSpan" id="kobo.213.1">said before, we don’t always want to choose greedy actions, but we want the model to explore other actions as well. </span><span class="koboSpan" id="kobo.213.2">For</span><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.214.1"> this, we can introduce a probability </span><em class="italic"><span class="koboSpan" id="kobo.215.1">ε</span></em><span class="koboSpan" id="kobo.216.1">, so that the agent will select from the other actions with equal probability. </span><span class="koboSpan" id="kobo.216.2">In simple words, the model almost always selects the greedy action, but with a probability </span><em class="italic"><span class="koboSpan" id="kobo.217.1">ε</span></em><span class="koboSpan" id="kobo.218.1">, it selects one of the other actions (regardless of its value). </span><span class="koboSpan" id="kobo.218.2">By increasing the number of steps, the other actions will also be tested (at infinity, they will be tested an infinite number of times) ensuring the convergence of </span><em class="italic"><span class="koboSpan" id="kobo.219.1">Q</span></em><span class="koboSpan" id="kobo.220.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.221.1">q*</span></em><span class="koboSpan" id="kobo.222.1">. </span><span class="koboSpan" id="kobo.222.2">Similarly, the probability of selecting the best action converges to </span><em class="italic"><span class="koboSpan" id="kobo.223.1">1 - ε</span></em><span class="koboSpan" id="kobo.224.1">. </span><span class="koboSpan" id="kobo.224.2">This method is called the </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">ε-greedy</span></strong><span class="koboSpan" id="kobo.226.1"> method, and</span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.227.1"> it allows for some balancing between exploitation </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">and exploration.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">To give a simple example, we can imagine a 10-armed bandit (</span><em class="italic"><span class="koboSpan" id="kobo.230.1">k</span></em><span class="koboSpan" id="kobo.231.1">=10) where we have action values </span><em class="italic"><span class="koboSpan" id="kobo.232.1">q*</span></em><span class="koboSpan" id="kobo.233.1">, where we have a normal distribution to represent the true value of each action. </span><span class="koboSpan" id="kobo.233.2">Here, we have plotted </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">1,000 examples:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer218">
<span class="koboSpan" id="kobo.235.1"><img alt="Figure 8.6 – Action value distribution" src="image/B21257_08_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.236.1">Figure 8.6 – Action value distribution</span></p>
<p><span class="koboSpan" id="kobo.237.1">In the following example, we compare different </span><em class="italic"><span class="koboSpan" id="kobo.238.1">ε</span></em><span class="koboSpan" id="kobo.239.1">-greedy methods. </span><span class="koboSpan" id="kobo.239.2">The rewards increase with agent experience and then go to plateaus. </span><span class="koboSpan" id="kobo.239.3">The pure greedy method is suboptimal </span><a id="_idIndexMarker934"/><span class="koboSpan" id="kobo.240.1">in comparison </span><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.241.1">to methods that also allow exploration. </span><span class="koboSpan" id="kobo.241.2">Similarly, choosing an </span><em class="italic"><span class="koboSpan" id="kobo.242.1">ε</span></em><span class="koboSpan" id="kobo.243.1"> constant that is too high (</span><em class="italic"><span class="koboSpan" id="kobo.244.1">ε</span></em><span class="koboSpan" id="kobo.245.1">=0.5) leads to worse results than a pure </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">greedy method.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer219">
<span class="koboSpan" id="kobo.247.1"><img alt="Figure 8.7 – Average rewards for time step for different greedy methods" src="image/B21257_08_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.248.1">Figure 8.7 – Average rewards for time step for different greedy methods</span></p>
<p><span class="koboSpan" id="kobo.249.1">To investigate this phenomenon, we can look at the optimal choice by the agent (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.250.1">Figure 8</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.251.1">.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer220">
<span class="koboSpan" id="kobo.253.1"><img alt="Figure 8.8 – Percentage optimal choice for time step for different greedy methods" src="image/B21257_08_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.254.1">Figure 8.8 – Percentage optimal choice for time step for different greedy methods</span></p>
<p><span class="koboSpan" id="kobo.255.1">The</span><a id="_idIndexMarker936"/><span class="koboSpan" id="kobo.256.1"> greedy method chooses the</span><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.257.1"> optimal choice only one-third of the time, while a method that includes some exploration selects the optimal choice 80% of the time (ε=0.1)%. </span><span class="koboSpan" id="kobo.257.2">This result shows that an agent that can also explore the environment achieves better results (can recognize the optimal action), while an agent that is greedy in the long run will choose actions that are suboptimal. </span><span class="koboSpan" id="kobo.257.3">In addition, </span><em class="italic"><span class="koboSpan" id="kobo.258.1">ε</span></em><span class="koboSpan" id="kobo.259.1">-greedy methods find the optimal action faster than </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">greedy methods.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">In this case, we explored simple methods, where </span><em class="italic"><span class="koboSpan" id="kobo.262.1">ε</span></em><span class="koboSpan" id="kobo.263.1"> is constant. </span><span class="koboSpan" id="kobo.263.2">In some variants, </span><em class="italic"><span class="koboSpan" id="kobo.264.1">ε</span></em><span class="koboSpan" id="kobo.265.1"> decreases with the number of steps, allowing the agent to shift focus from exploration to exploitation once the environment has been sufficiently explored. </span><span class="koboSpan" id="kobo.265.2">ε-greedy methods work best in almost all cases, especially when there is greater uncertainty (e.g., greater variance) or when the system </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">is nonstationary.</span></span></p>
<p><span class="koboSpan" id="kobo.267.1">The system we have seen so far is not efficient when we have a large number of samples. </span><span class="koboSpan" id="kobo.267.2">So, instead of taking the average of observed rewards, we can use an incremental method (the one most widely used today). </span><span class="koboSpan" id="kobo.267.3">For an action selected </span><em class="italic"><span class="koboSpan" id="kobo.268.1">i</span></em><span class="koboSpan" id="kobo.269.1"> times, the reward will be </span><em class="italic"><span class="koboSpan" id="kobo.270.1">R</span></em><span class="subscript"><span class="koboSpan" id="kobo.271.1">i</span></span><span class="koboSpan" id="kobo.272.1">, and we can calculate the estimated value </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.273.1">Q</span></em></span><span class="No-Break"><span class="subscript"><span class="koboSpan" id="kobo.274.1">n</span></span></span><span class="No-Break"><span class="koboSpan" id="kobo.275.1"> as:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>Q</mi><mi>n</mi></msub><mo>=</mo><mfrac><mrow><msub><mi>R</mi><mn>1</mn></msub><mo>+</mo><msub><mi>R</mi><mn>2</mn></msub><mo>+</mo><mo>⋯</mo><mo>+</mo><msub><mi>R</mi><mi>n</mi></msub></mrow><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.276.1">At this point, we do not have to recollect the average each time but can keep a record of what was calculated and simply conduct an update incrementally in </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">this way:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.278.1">This can be seen simply as a kind of stepwise adjustment of the </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">expected value:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>←</mo><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>n</mi><mi>e</mi><mi>w</mi></mrow></msub><mo>+</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mo>_</mo><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo>[</mo><mi>T</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>e</mi><mi>t</mi><mo>−</mo><msub><mrow><mi>E</mi><mi>s</mi><mi>t</mi><mi>i</mi><mi>m</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>d</mi></mrow><mrow><mi>o</mi><mi>l</mi><mi>d</mi></mrow></msub><mo>]</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.280.1">In fact, we can see [</span><em class="italic"><span class="koboSpan" id="kobo.281.1">Target - estimated</span></em><span class="subscript"><span class="koboSpan" id="kobo.282.1">old</span></span><span class="koboSpan" id="kobo.283.1">] as a kind of error in the estimation that we are trying to correct step by step and bring closer to the real target. </span><span class="koboSpan" id="kobo.283.2">The agent is trying to move the value estimation to the </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">real value.</span></span></p>
<p><span class="koboSpan" id="kobo.285.1">We </span><a id="_idIndexMarker938"/><span class="koboSpan" id="kobo.286.1">can test this incremental</span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.287.1"> implementation, and we can see how, after an initial exploratory phase, the agent begins to exploit </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">optimal choice:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer221">
<span class="koboSpan" id="kobo.289.1"><img alt="Figure 8.9 – Incremental implementation" src="image/B21257_08_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.290.1">Figure 8.9 – Incremental implementation</span></p>
<p><em class="italic"><span class="koboSpan" id="kobo.291.1">1/n</span></em><span class="koboSpan" id="kobo.292.1"> can be replaced by a fixed step size </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">parameter </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.294.1">α</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">.</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>Q</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></span></p>
<p><span class="koboSpan" id="kobo.296.1">Using </span><em class="italic"><span class="koboSpan" id="kobo.297.1">α</span></em><span class="koboSpan" id="kobo.298.1"> not only simplifies the calculation but also reduces the bias inherent in this approach. </span><span class="koboSpan" id="kobo.298.2">In fact, the choice of the initial value estimation for an action, </span><em class="italic"><span class="koboSpan" id="kobo.299.1">Q</span></em><span class="subscript"><span class="koboSpan" id="kobo.300.1">1(a),</span></span><span class="koboSpan" id="kobo.301.1">, can significantly influence early decisions and convergence behavior  </span><em class="italic"><span class="koboSpan" id="kobo.302.1">α</span></em><span class="koboSpan" id="kobo.303.1"> also allows you to handle non-stationary problems better (where the reward probability changes over time). </span><span class="koboSpan" id="kobo.303.2">The initial expected values are in general set to 0, but choose values greater than 0. </span><span class="koboSpan" id="kobo.303.3">This alternative is called the optimistic greedy strategy; these optimistic values stimulate </span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.304.1">the agent to explore the environment more (even when we use a pure greedy approach with </span><em class="italic"><span class="koboSpan" id="kobo.305.1">ε</span></em><span class="koboSpan" id="kobo.306.1">=0). </span><span class="koboSpan" id="kobo.306.2">The disadvantage is </span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.307.1">that we have to test different values for the initial </span><em class="italic"><span class="koboSpan" id="kobo.308.1">Q</span></em><span class="koboSpan" id="kobo.309.1">, and in practice, almost all practitioners set it to 0 </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">for convenience.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">By testing the optimistic greedy method, we can see that it behaves similarly to the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.312.1">ε</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">-greedy method:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer222">
<span class="koboSpan" id="kobo.314.1"><img alt="Figure 8.10 – Optimistic greedy versus the ε-greedy method" src="image/B21257_08_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.315.1">Figure 8.10 – Optimistic greedy versus the ε-greedy method</span></p>
<p><span class="koboSpan" id="kobo.316.1">In non-stationary problems, </span><em class="italic"><span class="koboSpan" id="kobo.317.1">α</span></em><span class="koboSpan" id="kobo.318.1"> can be set to give greater weight to recent rewards than to </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">prior ones.</span></span></p>
<p><span class="koboSpan" id="kobo.320.1">One final note: so far we have chosen greedy actions for their higher estimated value. </span><span class="koboSpan" id="kobo.320.2">In contrast, we </span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.321.1">have chosen non-greedy actions randomly. </span><span class="koboSpan" id="kobo.321.2">Instead of choosing them randomly, we can select them based on their potential optimality and uncertainty. </span><span class="koboSpan" id="kobo.321.3">This </span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.322.1">method is called </span><strong class="bold"><span class="koboSpan" id="kobo.323.1">Upper Confidence Bound</span></strong><span class="koboSpan" id="kobo.324.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.325.1">UCB</span></strong><span class="koboSpan" id="kobo.326.1">), where an action </span><em class="italic"><span class="koboSpan" id="kobo.327.1">A</span></em><span class="koboSpan" id="kobo.328.1"> is selected </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">based on:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi><mi>r</mi><mi>g</mi><munder><mi>max</mi><mi>a</mi></munder><mrow><mrow><mo>[</mo><msub><mi>Q</mi><mi>t</mi></msub><mfenced close=")" open="("><mi>a</mi></mfenced><mo>+</mo><mi>c</mi><msqrt><mfrac><mrow><mi>ln</mi><mi>t</mi></mrow><mrow><msub><mi>N</mi><mi>t</mi></msub><mo>(</mo><mi>a</mi><mo>)</mo></mrow></mfrac></msqrt><mo>]</mo></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.330.1">where </span><em class="italic"><span class="koboSpan" id="kobo.331.1">ln</span></em><span class="koboSpan" id="kobo.332.1">(</span><em class="italic"><span class="koboSpan" id="kobo.333.1">t</span></em><span class="koboSpan" id="kobo.334.1">) represents the natural logarithm of </span><em class="italic"><span class="koboSpan" id="kobo.335.1">t</span></em><span class="koboSpan" id="kobo.336.1">, </span><em class="italic"><span class="koboSpan" id="kobo.337.1">c&gt; 0</span></em><span class="koboSpan" id="kobo.338.1"> controls exploration, and </span><em class="italic"><span class="koboSpan" id="kobo.339.1">N</span></em><span class="subscript"><span class="koboSpan" id="kobo.340.1">t</span></span><span class="koboSpan" id="kobo.341.1"> is the number of </span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.342.1">times an action </span><em class="italic"><span class="koboSpan" id="kobo.343.1">A</span></em><span class="koboSpan" id="kobo.344.1"> has been tested. </span><span class="koboSpan" id="kobo.344.2">This approach means that all actions will be tested but actions that have a lower estimate value (and have been tested frequently) will be chosen again in a decreasing manner. </span><span class="koboSpan" id="kobo.344.3">Think of it as choosing between different restaurants. </span><span class="koboSpan" id="kobo.344.4">UCB helps you balance between going to the one you already like (high estimated value) and trying out others that might be better but haven’t visited much yet (high uncertainty). </span><span class="koboSpan" id="kobo.344.5">Over time, it naturally reduces exploration of poorly performing options while continuing to test under-explored but potentially good ones. </span><span class="koboSpan" id="kobo.344.6">UCB works very well, though it is difficult to apply in approaches other than </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">multi-armed bandits.</span></span></p>
<p><span class="koboSpan" id="kobo.346.1">As you can see, UCB generally gives </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">better results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer223">
<span class="koboSpan" id="kobo.348.1"><img alt="Figure 8.11 – UCB improvements on greedy methods" src="image/B21257_08_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.349.1">Figure 8.11 – UCB improvements on greedy methods</span></p>
<p><span class="koboSpan" id="kobo.350.1">Multi-armed bandit</span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.351.1"> is a </span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.352.1">classic example of RL, but it allows one to begin to understand how </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">RL works.</span></span></p>
<p><span class="koboSpan" id="kobo.354.1">Multi-armed bandit has been used for several applications, but it is a simplistic system that cannot be applied to different real-world situations. </span><span class="koboSpan" id="kobo.354.2">For example, in a chess game, the goal is not to eat the chess pieces but to win the game. </span><span class="koboSpan" id="kobo.354.3">Therefore, in the next subsection, we will begin to look at methods that take into account a purpose farther back in time than </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">immediate gain.</span></span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.356.1">Markov decision processes</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.357.1">Markov decision processes</span></strong><span class="koboSpan" id="kobo.358.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.359.1">MDPs</span></strong><span class="koboSpan" id="kobo.360.1">) are problems where actions impact not only immediate </span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.361.1">rewards </span><a id="_idIndexMarker948"/><span class="koboSpan" id="kobo.362.1">but also future outcomes. </span><span class="koboSpan" id="kobo.362.2">In MDPs then, delayed reward has much more weight than what we saw in the multi-armed bandit problem, but also in deciding the appropriate action for </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">different situations.</span></span></p>
<p><span class="koboSpan" id="kobo.364.1">Imagine you are navigating a maze. </span><span class="koboSpan" id="kobo.364.2">Each intersection or hallway you enter is a state, and every turn you make is an action that changes your state. </span><span class="koboSpan" id="kobo.364.3">Some paths lead you closer to the exit (the final reward), while others might take you in circles or into dead ends. </span><span class="koboSpan" id="kobo.364.4">The reward for each move may not be immediate—you only get the big reward when you reach the end. </span><span class="koboSpan" id="kobo.364.5">So, every action you take needs to consider how it impacts your chances of reaching the goal </span><span class="No-Break"><span class="koboSpan" id="kobo.365.1">later on.</span></span></p>
<p><span class="koboSpan" id="kobo.366.1">In MDPs, this idea is formalized: the agent must decide the best action in each state, not just for instant rewards, but for maximizing long-term success, which makes them more complex than simpler problems such as the multi-armed bandit, where only immediate rewards </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">are considered.</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">Previously we were just estimating </span><em class="italic"><span class="koboSpan" id="kobo.369.1">q*(a)</span></em><span class="koboSpan" id="kobo.370.1">. </span><span class="koboSpan" id="kobo.370.2">Now, we want to estimate the value of action </span><em class="italic"><span class="koboSpan" id="kobo.371.1">a</span></em><span class="koboSpan" id="kobo.372.1"> in the presence of state </span><em class="italic"><span class="koboSpan" id="kobo.373.1">s</span></em><span class="koboSpan" id="kobo.374.1">, </span><em class="italic"><span class="koboSpan" id="kobo.375.1">q*(s,a)</span></em><span class="koboSpan" id="kobo.376.1">. </span><span class="koboSpan" id="kobo.376.2">At each time step, the agent receives a representation of the environment </span><em class="italic"><span class="koboSpan" id="kobo.377.1">S</span></em><span class="subscript"><span class="koboSpan" id="kobo.378.1">t</span></span><span class="koboSpan" id="kobo.379.1"> and performs an action </span><em class="italic"><span class="koboSpan" id="kobo.380.1">A</span></em><span class="subscript"><span class="koboSpan" id="kobo.381.1">t</span></span><span class="koboSpan" id="kobo.382.1">, receives a reward </span><em class="italic"><span class="koboSpan" id="kobo.383.1">R</span></em><span class="koboSpan" id="kobo.384.1">, and moves to a new state </span><em class="italic"><span class="koboSpan" id="kobo.385.1">S</span></em><span class="subscript"><span class="koboSpan" id="kobo.386.1">t+1</span></span><span class="koboSpan" id="kobo.387.1">. </span><span class="koboSpan" id="kobo.387.2">It can be seen that the agent’s action can change the state of </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">the environment.</span></span></p>
<p><span class="koboSpan" id="kobo.389.1">In a finite MDP, the set of states, actions, and rewards contains a finite number of elements. </span><span class="koboSpan" id="kobo.389.2">The variables </span><em class="italic"><span class="koboSpan" id="kobo.390.1">R</span></em><span class="koboSpan" id="kobo.391.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.392.1">S</span></em><span class="koboSpan" id="kobo.393.1"> are probability distributions that depend on both the previous state and the action. </span><span class="koboSpan" id="kobo.393.2">We can describe the dynamics of this system using the state-transition probability function </span><em class="italic"><span class="koboSpan" id="kobo.394.1">p(s’, r | </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.395.1">s, a)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi>Pr</mi><mfenced close="|" open="{"><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>=</mo><mi>r</mi></mrow></msub></mrow></mfenced><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>}</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.397.1">In other words, state and reward depend on the previous state and the previous action. </span><span class="koboSpan" id="kobo.397.2">At each time step, the new state and reward will derive from the previous cycle. </span><span class="koboSpan" id="kobo.397.3">This will repeat a finite series of events. </span><span class="koboSpan" id="kobo.397.4">So, each state will summarize all the previous information (for example, in tic-tac-toe, the new system configuration gives us information about the previous moves) and is said to be a Markov state and possess the Markov property. </span><span class="koboSpan" id="kobo.397.5">The advantage of a Markov state is that each state possesses all the information we need to predict the future. </span><span class="koboSpan" id="kobo.397.6">The preceding function describes to us how one state evolves into another as we perform actions. </span><span class="koboSpan" id="kobo.397.7">An RL problem that respects this property is called an MDP. </span><span class="koboSpan" id="kobo.397.8">So, from this function, we can derive anything we care about the environment. </span><span class="koboSpan" id="kobo.397.9">We can then derive </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">state-transition probabilities:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi>Pr</mi><mfenced close="|" open="{"><mrow><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow></mrow></mfenced><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>}</mo><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></munder><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.399.1">We can also derive the expected reward for </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">state-action pairs:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><mml:math display="block"><mml:mi>r</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi mathvariant="double-struck">E</mml:mi><mml:mfenced close="]" open="[" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>∈</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo stretchy="false">∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>'</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mrow></mml:math></span></p>
<p><span class="koboSpan" id="kobo.401.1">And we can derive the expected rewards for </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">state-action-next-state triples:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>r</mi><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi><mo>,</mo><mi>s</mi><mi mathvariant="normal">ʹ</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><msub><mi>R</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>a</mi><mo>,</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mrow><mi>s</mi><mo>′</mo></mrow></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>r</mi><mo>∈</mo><mi>R</mi></mrow></munder><mrow><mi>r</mi><mfrac><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced></mrow></mfrac></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.403.1">This</span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.404.1"> shows us the flexibility </span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.405.1">of this framework. </span><span class="koboSpan" id="kobo.405.2">A brief note is that </span><em class="italic"><span class="koboSpan" id="kobo.406.1">t</span></em><span class="koboSpan" id="kobo.407.1"> does not need to be time steps but a sequence of states (a series of decisions, movements of a robot, and so on) making MDP a flexible system. </span><span class="koboSpan" id="kobo.407.2">After all, in MDPs, every problem can be reduced to three signals: actions, states, and rewards. </span><span class="koboSpan" id="kobo.407.3">This allows us a better abstraction to represent </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">goal-oriented learning.</span></span></p>
<p><span class="koboSpan" id="kobo.409.1">The goal for an agent is clearly to maximize cumulative reward over a long period of time (rather than immediate gain). </span><span class="koboSpan" id="kobo.409.2">This system has been shown to be very flexible because much of the problem can be formalized in this way (one just has to find a way to define what the rewards are so that the agent learns how to maximize them). </span><span class="koboSpan" id="kobo.409.3">One note is that agents try to maximize the reward in any way possible; if the goal is not well defined, it can lead to unintended results (e.g., in chess, the goal is to win the game; if the reward is to eat a piece and not to win the game, the agent will try to maximize the pieces eaten even when it might lead to losing </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">the game).</span></span></p>
<p><span class="koboSpan" id="kobo.411.1">This can be better expressed formally, where </span><em class="italic"><span class="koboSpan" id="kobo.412.1">G</span></em><span class="koboSpan" id="kobo.413.1"> is the cumulative sum of rewards received from step </span><em class="italic"><span class="koboSpan" id="kobo.414.1">t</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.415.1">and later:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>+</mo><msub><mi>R</mi><mi>T</mi></msub></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.416.1">Our goal is thus to maximize </span><em class="italic"><span class="koboSpan" id="kobo.417.1">G</span></em><span class="koboSpan" id="kobo.418.1">. </span><span class="koboSpan" id="kobo.418.2">This is easier to define for a sequence where we clearly must have an end (e.g., a game where only a defined number of moves can be made). </span><span class="koboSpan" id="kobo.418.3">A defined sequence of steps is called an episode, and the last state is called a terminal state. </span><span class="koboSpan" id="kobo.418.4">Note that each episode is independent of the other (losing one game does not affect the outcome of the next). </span><span class="koboSpan" id="kobo.418.5">This is, of course, not always possible; there are also definite continuous tasks where there is no net end (a robot moving in an environment), for which the previous equation does not work. </span><span class="koboSpan" id="kobo.418.6">In this case, we can use a so-called discount rate </span><em class="italic"><span class="koboSpan" id="kobo.419.1">γ</span></em><span class="koboSpan" id="kobo.420.1">. </span><span class="koboSpan" id="kobo.420.2">This </span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.421.1">parameter allows us to decide the agent’s behavior: when </span><em class="italic"><span class="koboSpan" id="kobo.422.1">γ</span></em><span class="koboSpan" id="kobo.423.1"> is near 0, the agent will try to maximize immediate rewards, while approaching 1, the agent considers future rewards with </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">greater weight:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>=</mo><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>2</mn></mrow></msub><mo>+</mo><msup><mi>γ</mi><mn>2</mn></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>3</mn></mrow></msub><mo>+</mo><mo>…</mo><mo>=</mo><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.425.1">As we</span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.426.1"> saw in the </span><em class="italic"><span class="koboSpan" id="kobo.427.1">multi-armed bandit problem</span></em><span class="koboSpan" id="kobo.428.1"> section, we can estimate value functions (how good it is for an agent to be in a given state). </span><span class="koboSpan" id="kobo.428.2">The value of a state </span><em class="italic"><span class="koboSpan" id="kobo.429.1">S</span></em><span class="koboSpan" id="kobo.430.1"> considering a policy </span><em class="italic"><span class="koboSpan" id="kobo.431.1">π</span></em><span class="koboSpan" id="kobo.432.1">, is the expected return starting from </span><em class="italic"><span class="koboSpan" id="kobo.433.1">S</span></em><span class="koboSpan" id="kobo.434.1"> and following the policy </span><em class="italic"><span class="koboSpan" id="kobo.435.1">π</span></em><span class="koboSpan" id="kobo.436.1"> from </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">that time:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.438.1">This is called the state-value function for the policy π, and </span><em class="italic"><span class="koboSpan" id="kobo.439.1">G</span></em><span class="subscript"><span class="koboSpan" id="kobo.440.1">t</span></span><span class="koboSpan" id="kobo.441.1"> is the expected return. </span><span class="koboSpan" id="kobo.441.2">Similarly, we can define the value of taking action </span><em class="italic"><span class="koboSpan" id="kobo.442.1">A</span></em><span class="koboSpan" id="kobo.443.1"> in state </span><em class="italic"><span class="koboSpan" id="kobo.444.1">S</span></em><span class="koboSpan" id="kobo.445.1"> under </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">policy </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.447.1">π</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><mrow><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mi mathvariant="normal">∞</mi></munderover><mrow><msup><mi>γ</mi><mi>k</mi></msup><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.449.1">This is called the action-value function for policy </span><em class="italic"><span class="koboSpan" id="kobo.450.1">π</span></em><span class="koboSpan" id="kobo.451.1">. </span><span class="koboSpan" id="kobo.451.2">We can estimate these functions by experience (interaction with the environment), and at infinity, they should approach the true value. </span><span class="koboSpan" id="kobo.451.3">Methods like this where we conduct averaging of many random samples of actual returns are called Monte </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">Carlo methods.</span></span></p>
<p><span class="koboSpan" id="kobo.453.1">For efficiency, we can rewrite it in a recursive form (</span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">using discounting):</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mi>G</mi><mi>t</mi></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><msub><mi mathvariant="double-struck">E</mi><mi>π</mi></msub><mfenced close="]" open="["><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><mi>G</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>a</mi></munder><mrow><mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mfenced close=")" open="("><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mfenced close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>′</mo></mrow></mfenced></mrow></mfenced></mrow></mrow><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.455.1">This simplified form is called the Bellman equation. </span><span class="koboSpan" id="kobo.455.2">This can be represented as thinking forward to the next state from the previous state. </span><span class="koboSpan" id="kobo.455.3">From a state with a policy, we choose an action and get a reward (or not) with a given probability. </span><span class="koboSpan" id="kobo.455.4">The Bellman equation conducts the average of these probabilities, giving a weight to the possibility of their occurrence. </span><span class="koboSpan" id="kobo.455.5">This equation is the basis of many great </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">RL algorithms.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer224">
<span class="koboSpan" id="kobo.457.1"><img alt="Figure 8.12 – Bellman backup diagram" src="image/B21257_08_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.458.1">Figure 8.12 – Bellman backup diagram</span></p>
<p><span class="koboSpan" id="kobo.459.1">Now</span><a id="_idIndexMarker953"/><span class="koboSpan" id="kobo.460.1"> that we have both state-value functions </span><em class="italic"><span class="koboSpan" id="kobo.461.1">vπ(s)</span></em><span class="koboSpan" id="kobo.462.1"> and action-value functions </span><em class="italic"><span class="koboSpan" id="kobo.463.1">qπ(s, a)</span></em><span class="koboSpan" id="kobo.464.1">, we can evaluate policies and choose the best ones. </span><span class="koboSpan" id="kobo.464.2">Action-value functions allow us to choose the best action relative to the state. </span><span class="koboSpan" id="kobo.464.3">Consider, for example, a case of a Texas Hold’em poker game. </span><span class="koboSpan" id="kobo.464.4">A </span><a id="_idIndexMarker954"/><span class="koboSpan" id="kobo.465.1">player has $100 and must choose the strategy starting from the state </span><em class="italic"><span class="koboSpan" id="kobo.466.1">π</span></em><span class="koboSpan" id="kobo.467.1">. </span><span class="koboSpan" id="kobo.467.2">The strategy </span><em class="italic"><span class="koboSpan" id="kobo.468.1">π</span></em><span class="subscript"><span class="koboSpan" id="kobo.469.1">1</span></span><span class="koboSpan" id="kobo.470.1"> has a state value function that returns 10, while </span><em class="italic"><span class="koboSpan" id="kobo.471.1">π</span></em><span class="subscript"><span class="koboSpan" id="kobo.472.1">2</span></span><span class="koboSpan" id="kobo.473.1"> has a return of -2. </span><span class="koboSpan" id="kobo.473.2">This means that the first strategy brings an expected gain of 10, while </span><em class="italic"><span class="koboSpan" id="kobo.474.1">π</span></em><span class="subscript"><span class="koboSpan" id="kobo.475.1">2</span></span><span class="koboSpan" id="kobo.476.1"> brings an expected loss of 2. </span><span class="koboSpan" id="kobo.476.2">Given a state </span><em class="italic"><span class="koboSpan" id="kobo.477.1">s</span></em><span class="koboSpan" id="kobo.478.1">, the player wants to figure out which action to choose. </span><span class="koboSpan" id="kobo.478.2">For example, choosing whether to bet 10 or 5, </span><em class="italic"><span class="koboSpan" id="kobo.479.1">q</span></em><span class="subscript"><span class="koboSpan" id="kobo.480.1">π</span></span><span class="koboSpan" id="kobo.481.1"> (s, a) tells us what the expected cumulative reward is from this action. </span><span class="koboSpan" id="kobo.481.2">So, the preceding equations allow us to figure out which action or strategy to choose to maximize </span><span class="No-Break"><span class="koboSpan" id="kobo.482.1">the reward.</span></span></p>
<p><span class="koboSpan" id="kobo.483.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.484.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.485.1">.12</span></em><span class="koboSpan" id="kobo.486.1">, it can be understood that solving an RL task means finding an optimal policy that succeeds in collecting many rewards over the long run. </span><span class="koboSpan" id="kobo.486.2">For MDPs, it is possible to define an optimal policy because we can evaluate whether one policy is better than another if it has a higher expected return for all states </span><em class="italic"><span class="koboSpan" id="kobo.487.1">vπ(s)</span></em><span class="koboSpan" id="kobo.488.1">. </span><em class="italic"><span class="koboSpan" id="kobo.489.1">π*</span></em><span class="koboSpan" id="kobo.490.1"> denotes the optimal policy and is the one that has the maximum value function over all </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">possible policies:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>π</mi></munder><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.492.1">The optimal policies share the same optimal action-value function </span><em class="italic"><span class="koboSpan" id="kobo.493.1">q*</span></em><span class="koboSpan" id="kobo.494.1">, which is defined as the maximum action-value function over all </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">possible policies:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><munder><mi>max</mi><mi>π</mi></munder><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mi>f</mi><mi>o</mi><mi>r</mi><mi>a</mi><mi>l</mi><mi>l</mi><mi>s</mi><mo>∈</mo><mi>S</mi></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.496.1">The relationship between these two functions can be summarized </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">as follows:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.498.1">The equation expresses the cumulative return given a state-action pair. </span><span class="koboSpan" id="kobo.498.2">Optimal value functions are an ideal state in RL, though, and it is difficult to find optimal policies, especially when </span><a id="_idIndexMarker955"/><span class="koboSpan" id="kobo.499.1">tasks are complex or computationally expensive. </span><span class="koboSpan" id="kobo.499.2">RL therefore tries to approximate them, for example, by using </span><strong class="bold"><span class="koboSpan" id="kobo.500.1">dynamic programming</span></strong><span class="koboSpan" id="kobo.501.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.502.1">DP</span></strong><span class="koboSpan" id="kobo.503.1">). </span><span class="koboSpan" id="kobo.503.2">The</span><a id="_idIndexMarker956"/><span class="koboSpan" id="kobo.504.1"> purpose of DP is to use value functions to search for good policies (even if not exact solutions). </span><span class="koboSpan" id="kobo.504.2">At this point, we can derive Bellman optimality equations for the optimal state-value function </span><em class="italic"><span class="koboSpan" id="kobo.505.1">v*(s)</span></em><span class="koboSpan" id="kobo.506.1"> and the optimal action-value function </span><em class="italic"><span class="koboSpan" id="kobo.507.1">q</span></em><span class="subscript"><span class="koboSpan" id="kobo.508.1">∗</span></span><em class="italic"><span class="koboSpan" id="kobo.509.1"> (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.510.1">s, a)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>a</mi></munder><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mi mathvariant="normal">
</mi><mo>=</mo><munder><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mi>a</mi></munder><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mo>+</mo><mi>s</mi><mi mathvariant="normal">ʹ</mi><mo>]</mo></mrow></mrow></mrow></math></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Operator"><math display="block"><mrow><mrow><mrow><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><munder><mi>max</mi><mi>a</mi></munder><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mi mathvariant="normal">
</mi><mo>=</mo><munder><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mi>a</mi></munder><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi mathvariant="normal">*</mi></msub><mo>+</mo><mi>s</mi><mi mathvariant="normal">ʹ</mi><mo>]</mo></mrow></mrow></mrow></math></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><munder><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mi mathvariant="normal">ʹ</mi></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>a</mi><mi mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mi mathvariant="normal">
</mi><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mfenced close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mo>′</mo></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>a</mi><mi mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></math></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Operator"><math display="block"><mrow><mrow><msub><mi>q</mi><mi mathvariant="normal">*</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><munder><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mi mathvariant="normal">ʹ</mi></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>,</mo><mi>a</mi><mi mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mi mathvariant="normal">
</mi><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mfenced close="]" open="["><mrow><mi>r</mi><mo>+</mo><mi>γ</mi><munder><mrow><mi mathvariant="normal">m</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">x</mi></mrow><mrow><mi>a</mi><mo>′</mo></mrow></munder><msup><mi>q</mi><mi mathvariant="normal">*</mi></msup><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>a</mi><mi mathvariant="normal">ʹ</mi><mo>)</mo></mrow></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.512.1">For finite </span><a id="_idIndexMarker957"/><span class="koboSpan" id="kobo.513.1">MDP, Bellman optimality equations have only one solution, and they can be solved if we know the dynamics of the system. </span><span class="koboSpan" id="kobo.513.2">Once we get </span><em class="italic"><span class="koboSpan" id="kobo.514.1">v*</span></em><span class="koboSpan" id="kobo.515.1">, it is easy to identify the optimal policy </span><em class="italic"><span class="koboSpan" id="kobo.516.1">q*</span></em><span class="koboSpan" id="kobo.517.1">; having an optimal </span><em class="italic"><span class="koboSpan" id="kobo.518.1">q*</span></em><span class="koboSpan" id="kobo.519.1">, we can identify the optimal actions. </span><span class="koboSpan" id="kobo.519.2">The beauty of </span><em class="italic"><span class="koboSpan" id="kobo.520.1">v*</span></em><span class="koboSpan" id="kobo.521.1"> is that it allows us to choose the best actions at the moment while still taking into account the long-term goal. </span><span class="koboSpan" id="kobo.521.2">Solving these equations for a problem is solving the problem through RL. </span><span class="koboSpan" id="kobo.521.3">On the other hand, for many problems, solving them means calculating all possibilities and thus would be too computationally expensive. </span><span class="koboSpan" id="kobo.521.4">In other cases, we do not know the dynamics of the environment with certainty or the states do not have Markov properties. </span><span class="koboSpan" id="kobo.521.5">However, these equations are the basis of RL, and many methods are approximations of these equations, often using experience from previous states. </span><span class="koboSpan" id="kobo.521.6">So, these algorithms do not identify the best policy but an approximation. </span><span class="koboSpan" id="kobo.521.7">For example, many algorithms learn optimal actions for the most frequent states but may choose suboptimal actions for infrequent or rare states. </span><span class="koboSpan" id="kobo.521.8">The trick is that these choices should not impact the future amount of reward. </span><span class="koboSpan" id="kobo.521.9">For example, an agent might still win a game even if it does not make the best move in </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">rare situations.</span></span></p>
<p><span class="koboSpan" id="kobo.523.1">DP refers to a collection of algorithms that are used to compute the best policy given a perfect model of the environment as an MDP. </span><span class="koboSpan" id="kobo.523.2">Now, these algorithms require a lot of computation and the assumption of the perfect model does not always hold. </span><span class="koboSpan" id="kobo.523.3">So, these algorithms are not practically used anymore; at the same time, one can define today’s algorithms as inspired by DP algorithms, with the purpose of reducing computation and working even when the assumption of a perfect model of the environment does not hold. </span><span class="koboSpan" id="kobo.523.4">DP algorithms, in short, are obtained from transforming Bellman equations into update </span><a id="_idIndexMarker958"/><span class="koboSpan" id="kobo.524.1">rules to improve the </span><a id="_idIndexMarker959"/><span class="koboSpan" id="kobo.525.1">approximation of desired value functions. </span><span class="koboSpan" id="kobo.525.2">This allows value functions to be used to organize the search for good policies. </span><span class="koboSpan" id="kobo.525.3">To evaluate a policy, we can use the state-value function and evaluate the expected return when following policy </span><em class="italic"><span class="koboSpan" id="kobo.526.1">π</span></em><span class="koboSpan" id="kobo.527.1"> from </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">each state:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><msub><mi>v</mi><mi>π</mi></msub><mfenced close=")" open="("><mi>s</mi></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mi>a</mi></munder><mrow><mi>π</mi><mo>(</mo><mi>a</mi><mo>|</mo><mi>s</mi><mo>)</mo></mrow></mrow><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>)</mo><mo>]</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.529.1">Calculating the value function for a policy aims to identify better policies. </span><span class="koboSpan" id="kobo.529.2">For a state </span><em class="italic"><span class="koboSpan" id="kobo.530.1">s</span></em><span class="koboSpan" id="kobo.531.1">, we want to know whether we should keep that policy, improve it, or choose another. </span><span class="koboSpan" id="kobo.531.2">Remember that the choice of a policy decides what actions an agent will take. </span><span class="koboSpan" id="kobo.531.3">To answer the question “is it better to change policy?”, we can consider what happens if we choose an action in a state </span><em class="italic"><span class="koboSpan" id="kobo.532.1">s</span></em><span class="koboSpan" id="kobo.533.1"> following </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">policy </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.535.1">π</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.536.1">:</span></span></p>
<p class="Normal" lang="en-US" xml:lang="en-US"><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mrow><msub><mi>q</mi><mi>π</mi></msub><mfenced close=")" open="("><mrow><mi>s</mi><mo>,</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mi mathvariant="double-struck">E</mi><mfenced close="]" open="["><mrow><mrow><mrow><msub><mrow><msub><mi>R</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mi>S</mi></mrow><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>)</mo></mrow></mrow><mo>|</mo><msub><mi>S</mi><mi>t</mi></msub><mo>=</mo><mi>s</mi><mo>,</mo><msub><mi>A</mi><mi>t</mi></msub><mo>=</mo><mi>a</mi></mrow></mfenced><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi></mrow></munder><mrow><mi>p</mi><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>,</mo><mi>r</mi><mo>|</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo>)</mo></mrow></mrow><mo>[</mo><mi>r</mi><mo>+</mo><mi>γ</mi><msub><mi>v</mi><mi>π</mi></msub><mo>(</mo><mrow><mi>s</mi><mo>′</mo></mrow><mo>)</mo><mo>]</mo></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.537.1">A better policy </span><em class="italic"><span class="koboSpan" id="kobo.538.1">π’</span></em><span class="koboSpan" id="kobo.539.1"> should provide us with a better value of </span><em class="italic"><span class="koboSpan" id="kobo.540.1">v</span></em><span class="subscript"><span class="koboSpan" id="kobo.541.1">π</span></span><em class="italic"><span class="koboSpan" id="kobo.542.1">(s)</span></em><span class="koboSpan" id="kobo.543.1">. </span><span class="koboSpan" id="kobo.543.2">If </span><em class="italic"><span class="koboSpan" id="kobo.544.1">π’</span></em><span class="koboSpan" id="kobo.545.1"> is less than or equal to </span><em class="italic"><span class="koboSpan" id="kobo.546.1">v</span></em><span class="subscript"><span class="koboSpan" id="kobo.547.1">π</span></span><em class="italic"><span class="koboSpan" id="kobo.548.1">(s)</span></em><span class="koboSpan" id="kobo.549.1">, we can continue the same policy. </span><span class="koboSpan" id="kobo.549.2">In other words, choosing actions according to a policy</span><em class="italic"><span class="koboSpan" id="kobo.550.1"> π</span></em><span class="koboSpan" id="kobo.551.1">’ that has better </span><em class="italic"><span class="koboSpan" id="kobo.552.1">v</span></em><span class="subscript"><span class="koboSpan" id="kobo.553.1">π</span></span><em class="italic"><span class="koboSpan" id="kobo.554.1">(s)</span></em><span class="koboSpan" id="kobo.555.1"> is more beneficial than another </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">policy </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.557.1">π</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.558.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.559.1">In this section, we have seen classic RL algorithms, but none of them use a neural network or other machine learning model. </span><span class="koboSpan" id="kobo.559.2">These algorithms work well for simple cases, while for more complex situations we want a more sophisticated and adaptable system. </span><span class="koboSpan" id="kobo.559.3">In the next section, we will see how we can integrate neural networks into </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">RL algorithms.</span></span></p>
<h1 id="_idParaDest-144"><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.561.1">Deep reinforcement learning</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.562.1">Deep reinforcement learning</span></strong><span class="koboSpan" id="kobo.563.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.564.1">deep RL</span></strong><span class="koboSpan" id="kobo.565.1">) is a</span><a id="_idIndexMarker960"/><span class="koboSpan" id="kobo.566.1"> subfield of RL that combines RL with deep learning. </span><span class="koboSpan" id="kobo.566.2">In other words, the idea behind it is to exploit the learning capabilities of a neural network to solve RL problems. </span><span class="koboSpan" id="kobo.566.3">In traditional RL, policies and value functions are represented by simple functions. </span><span class="koboSpan" id="kobo.566.4">These methods work well with low-dimensional state and action spaces (i.e., when the environment and agent can be easily modeled). </span><span class="koboSpan" id="kobo.566.5">When the environment becomes more complex or larger, traditional methods fail to generalize. </span><span class="koboSpan" id="kobo.566.6">In deep RL, instead, policies and value functions are represented by neural networks. </span><span class="koboSpan" id="kobo.566.7">A neural network can theoretically </span><a id="_idIndexMarker961"/><span class="koboSpan" id="kobo.567.1">represent any complex function (Universal Approximation Theorem), and this allows deep RL methods to solve problems with high-dimensional state spaces (such as those presenting images, videos, or continuous tasks). </span><span class="koboSpan" id="kobo.567.2">Modeling complex functions thus allows the agent to learn a more generalized and flexible policy that is needed in complex situations where defining a function is impossible with traditional methods. </span><span class="koboSpan" id="kobo.567.3">This learning capability has enabled deep RL methods to solve video games, move robots, </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">and more.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer225">
<span class="koboSpan" id="kobo.569.1"><img alt="Figure 8.13 – Overview of deep RL (https://arxiv.org/abs/1708.05866)" src="image/B21257_08_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.570.1">Figure 8.13 – Overview of deep RL (</span><a href="https://arxiv.org/abs/1708.05866"><span class="koboSpan" id="kobo.571.1">https://arxiv.org/abs/1708.05866</span></a><span class="koboSpan" id="kobo.572.1">)</span></p>
<p><span class="koboSpan" id="kobo.573.1">In the upcoming subsections, we will discuss how to classify these algorithms and what the </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">differences are.</span></span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.575.1">Model-free versus model-based approaches</span></h2>
<p><span class="koboSpan" id="kobo.576.1">There are so many methods of deep RL today that it is difficult to make a taxonomy of these models. </span><span class="koboSpan" id="kobo.576.2">Nevertheless, deep RL methods can be broadly divided into two main groups: model-free and model-based. </span><span class="koboSpan" id="kobo.576.3">This division is represented by the answer to this question: does the agent have access to (or learn) a model of </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">the environment?</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.578.1">Model-free methods</span></strong><span class="koboSpan" id="kobo.579.1">: These </span><a id="_idIndexMarker962"/><span class="koboSpan" id="kobo.580.1">methods determine the optimal policy or value function without building a model of the environment. </span><span class="koboSpan" id="kobo.580.2">These models learn directly from observed states, actions, and rewards. </span><span class="koboSpan" id="kobo.580.3">The agent learns directly from trial and error, receives feedback from the environment, and uses this feedback to improve its policy or value estimation. </span><span class="koboSpan" id="kobo.580.4">These approaches are usually easier to implement and conduct parameter tuning (they only require observing state-action-reward sequences or transitions). </span><span class="koboSpan" id="kobo.580.5">They are also more easily scalable and less </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">computationally complex.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.582.1">Model-based methods</span></strong><span class="koboSpan" id="kobo.583.1">: These </span><a id="_idIndexMarker963"/><span class="koboSpan" id="kobo.584.1">methods rely on an internal model of the environment to predict future states and rewards given any state-action pair. </span><span class="koboSpan" id="kobo.584.2">This model can be learned or predefined before training. </span><span class="koboSpan" id="kobo.584.3">Having the model allows the agent to have similar outcomes and plan actions for future scenarios (e.g., what the future actions of an opponent in a game will be and anticipating them). </span><span class="koboSpan" id="kobo.584.4">Model-based approaches have the advantage that they can reduce interaction with the real environment and are better at planning complex tasks. </span><span class="koboSpan" id="kobo.584.5">Potentially improved performance comes at the cost of increased complexity (building an accurate model of the environment can be challenging, especially for high-dimensional environments) and increased </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">computational cost.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer226">
<span class="koboSpan" id="kobo.586.1"><img alt="Figure 8.14 – Model-free versus model-based approaches" src="image/B21257_08_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.587.1">Figure 8.14 – Model-free versus model-based approaches</span></p>
<p><span class="koboSpan" id="kobo.588.1">The primary advantage of a model-based RL method lies in its ability to plan and think ahead. </span><span class="koboSpan" id="kobo.588.2">By utilizing a model (in general, a neural network) to simulate the dynamics of the environment, it </span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.589.1">can predict future scenarios, making it particularly useful in complex environments or situations where decisions need to consider long-term outcomes. </span><span class="koboSpan" id="kobo.589.2">For instance, when rewards are sparse or delayed—such as in chess, where the reward is achieved only by winning the game—the model can simulate various paths to optimize the agent’s strategy for reaching </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">the reward.</span></span></p>
<p><span class="koboSpan" id="kobo.591.1">Planning also proves advantageous in dynamic environments. </span><span class="koboSpan" id="kobo.591.2">The model can update its internal representation quickly, allowing the agent to adapt its policy without relearning from scratch. </span><span class="koboSpan" id="kobo.591.3">This minimizes the need for extensive retraining, as seen in applications such as autonomous driving, where the agent can adjust its strategy without requiring large new datasets. </span><span class="koboSpan" id="kobo.591.4">The insights gained from such planning can then be distilled into a learned policy, enhancing the agent’s performance </span><span class="No-Break"><span class="koboSpan" id="kobo.592.1">over time.</span></span></p>
<p><span class="koboSpan" id="kobo.593.1">Additionally, simulating interactions with the environment reduces the need for extensive real-world exploration, which is critical in scenarios where interactions are costly, dangerous, or time-intensive, such as in robotics or autonomous vehicles. </span><span class="koboSpan" id="kobo.593.2">By leveraging its internal model, the agent can prioritize actions and refine its exploration process to update or improve its understanding of the environment </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">more efficiently.</span></span></p>
<p><span class="koboSpan" id="kobo.595.1">This then helps the agent optimize for long-term goals because it can simulate the long-term consequences of its actions, monitor its progress toward a more distant horizon, and align its actions with a </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">distant goal.</span></span></p>
<p><span class="koboSpan" id="kobo.597.1">Model building can be a complicated task. </span><span class="koboSpan" id="kobo.597.2">However, a ground-truth model of the environment is not always available to the agent. </span><span class="koboSpan" id="kobo.597.3">In this case, the agent is forced to learn only from experience to create its own model. </span><span class="koboSpan" id="kobo.597.4">This can then lead to bias in the agent’s model. </span><span class="koboSpan" id="kobo.597.5">An agent might therefore perform optimally with respect to a learned model but perform terribly (or suboptimally) in the </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">real environment.</span></span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.599.1">On-policy versus off-policy methods</span></h2>
<p><span class="koboSpan" id="kobo.600.1">Another important </span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.601.1">classification in RL is how models learn from experience and whether they learn from the current policy or a different one (they are classified according to the relationship between the policy and the </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">policy update):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.603.1">On-policy methods</span></strong><span class="koboSpan" id="kobo.604.1">: These methods learn from actions learned from the agent’s current policy (so the agent both collects data and learns from the same policy). </span><span class="koboSpan" id="kobo.604.2">On-policy methods evaluate and improve the policy used to make decisions; this is based on actions taken and rewards received while following the current policy (the agent conducts the policy update by directly evaluating and improving the policy). </span><span class="koboSpan" id="kobo.604.3">The agent therefore does not use data from other policies. </span><span class="koboSpan" id="kobo.604.4">The</span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.605.1"> advantages are that the agent tends to be more stable and less prone to variance (the optimized policy is, in fact, the one used to interact with the environment). </span><span class="koboSpan" id="kobo.605.2">On-policy methods are inefficient because they discard data that is obtained from previous policies (sample inefficiency), limiting their use for complex environments since they would require large amounts of data. </span><span class="koboSpan" id="kobo.605.3">In addition, these methods are not very exploratory and therefore less beneficial where more exploration is required (they are favored for environments that are stable). </span><span class="koboSpan" id="kobo.605.4">An example is a chatbot that learns to give better answers to user questions: the chatbot uses a specific policy to give answers and optimizes this policy by leveraging feedback received from users. </span><span class="koboSpan" id="kobo.605.5">On-policy methods ensure that the learned policy is linked to actions taken by the chatbot and by real interactions with users (this </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">ensures stability).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.607.1">Off-policy methods</span></strong><span class="koboSpan" id="kobo.608.1">: Off-policy methods </span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.609.1">learn the value of the optimal policy independently of the agent’s actions (agents learn from experiences that are generated by a different policy from the one used for learning). </span><span class="koboSpan" id="kobo.609.2">So, these methods can learn from past data or data that is generated by other policies. </span><span class="koboSpan" id="kobo.609.3">Off-policy methods separate the behavior policy (used to collect data) from the target policy (the policy being learned). </span><span class="koboSpan" id="kobo.609.4">In other words, the behavior policy is used to explore the environment while the target policy is used to improve the agent’s performance (to ensure more exploratory behavior while learning an optimal target policy). </span><span class="koboSpan" id="kobo.609.5">Off-policy methods have higher sample efficiency because they can reuse data and allow for better exploration, which can lead to faster convergence to an optimal policy. </span><span class="koboSpan" id="kobo.609.6">On the other hand, they are less stable (because they do not learn from actions that have been taken by the current policy, the discrepancy between behavior policy and target policy can lead to higher variance in updates) and can be much more complex. </span><span class="koboSpan" id="kobo.609.7">An example is a music recommendation system that suggests new titles to users and has to explore different genres and new releases. </span><span class="koboSpan" id="kobo.609.8">The behavior policy encourages exploration and thus generates data on user preferences, while the target policy seeks to</span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.610.1"> optimize recommendation performance for users. </span><span class="koboSpan" id="kobo.610.2">Separating the two policies thus allows experimenting with different recommendation strategies without compromising the quality of the final recommendations. </span><span class="koboSpan" id="kobo.610.3">The advantage of these methods is that they allow extensive exploration, which is very useful for complex and </span><span class="No-Break"><span class="koboSpan" id="kobo.611.1">dynamic environments.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer227">
<span class="koboSpan" id="kobo.612.1"><img alt="Figure 8.15 – On-policy and off-policy methods" src="image/B21257_08_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.613.1">Figure 8.15 – On-policy and off-policy methods</span></p>
<p><span class="koboSpan" id="kobo.614.1">In the next subsection, we will begin to go into detail about how deep </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">RL works.</span></span></p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.616.1">Exploring deep RL in detail</span></h2>
<p><span class="koboSpan" id="kobo.617.1">We’ll</span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.618.1"> begin with a definition to better understand deep RL. </span><span class="koboSpan" id="kobo.618.2">A state </span><em class="italic"><span class="koboSpan" id="kobo.619.1">s</span></em><span class="koboSpan" id="kobo.620.1"> in a system is usually a vector, matrix, or other tensor. </span><span class="koboSpan" id="kobo.620.2">At each time step </span><em class="italic"><span class="koboSpan" id="kobo.621.1">t</span></em><span class="koboSpan" id="kobo.622.1">, we can describe the environment in the form of a tensor (e.g., the position of the pieces on a chessboard can be represented by a matrix). </span><span class="koboSpan" id="kobo.622.2">Similarly, the actions </span><em class="italic"><span class="koboSpan" id="kobo.623.1">a</span></em><span class="koboSpan" id="kobo.624.1"> an agent can choose can be represented in a tensor (for example, each action can be associated with a one-hot vector, a matrix, and so on). </span><span class="koboSpan" id="kobo.624.2">All of these are data structures that are already commonly seen in machine learning and that we can use as input to a deep </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">learning model.</span></span></p>
<p><span class="koboSpan" id="kobo.626.1">So far, we have discussed policies generically, but what functions do we use to model them? </span><span class="koboSpan" id="kobo.626.2">Very often, we use neural networks. </span><span class="koboSpan" id="kobo.626.3">So, in this section, we will actively look at how a neural network can be used in RL algorithms. </span><span class="koboSpan" id="kobo.626.4">What we’ll see now is based on what we saw in this chapter, but we’ll use a neural network to decide what action to take (instead of just a function). </span><span class="koboSpan" id="kobo.626.5">As we saw in </span><a href="B21257_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.627.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.628.1">, a neural network is constituted of a series of neurons organized in a series of layers. </span><span class="koboSpan" id="kobo.628.2">Neural networks take a tensor as input and produce a tensor as output. </span><span class="koboSpan" id="kobo.628.3">In this case, the output of the neural network is the choice of an action. </span><span class="koboSpan" id="kobo.628.4">Optimizing the policy, in this case, means optimizing the parameters of the neural network. </span><span class="koboSpan" id="kobo.628.5">An RL algorithm based on experience can change the parameters of the policy function so that it produces </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">better results.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer228">
<span class="koboSpan" id="kobo.630.1"><img alt="Figure 8.16 – Neural network as an RL policy" src="image/B21257_08_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.631.1">Figure 8.16 – Neural network as an RL policy</span></p>
<p><span class="koboSpan" id="kobo.632.1">Neural networks are well-known deep learning models, and we know how to optimize them. </span><span class="koboSpan" id="kobo.632.2">Using gradient-based methods allows us to understand how a change in parameters impacts the outcome of a function. </span><span class="koboSpan" id="kobo.632.3">In this case, we want to know how we should update </span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.633.1">the parameters of our policy </span><em class="italic"><span class="koboSpan" id="kobo.634.1">P</span></em><span class="koboSpan" id="kobo.635.1"> (the neural network model) so that we collect more rewards in the future. </span><span class="koboSpan" id="kobo.635.2">So, having a function that tells us what the expected rewards are for a policy, we can use the gradient to change the parameters of the policy and thus maximize </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">the return.</span></span></p>
<p><span class="koboSpan" id="kobo.637.1">Using a neural network as a policy in RL has </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">several advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.639.1">Neural networks are highly expressive function approximators, so they can map complex nonlinear relationships between inputs (states) and outputs (actions). </span><span class="koboSpan" id="kobo.639.2">This is very useful for complex environments, such as playing video games or controlling robots in 3D environments. </span><span class="koboSpan" id="kobo.639.3">In addition, neural networks scale well for environments that have large and complex state and </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">action spaces.</span></span></li>
<li><span class="koboSpan" id="kobo.641.1">Neural networks possess the ability to generalize to situations they have not encountered before. </span><span class="koboSpan" id="kobo.641.2">This capability makes them particularly useful in handling unexpected state changes, thus promoting adaptability in agents. </span><span class="koboSpan" id="kobo.641.3">All this allows neural networks to be flexible and adaptable to a different range of tasks </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">and environments.</span></span></li>
<li><span class="koboSpan" id="kobo.643.1">Neural networks can handle actions that are continuous rather than discrete, thus enabling their use in real-world problems (where actions are often not limited to a </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">discrete set).</span></span></li>
<li><span class="koboSpan" id="kobo.645.1">Neural networks are versatile. </span><span class="koboSpan" id="kobo.645.2">They can be used with different types of data and do not require feature engineering.  </span><span class="koboSpan" id="kobo.645.3">This is important when the features can be complex or the state representation is complex (sensors, images, video, and </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">so on).</span></span></li>
<li><span class="koboSpan" id="kobo.647.1">They can produce a probability distribution and thus can be used with a stochastic policy. </span><span class="koboSpan" id="kobo.647.2">This is important when we want to add randomness and </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">encourage exploration.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer229">
<span class="koboSpan" id="kobo.649.1"><img alt="Figure 8.17 – Examples of screenshots where a neural network is used to learn how to play Atari games (https://arxiv.org/abs/1312.5602)" src="image/B21257_08_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.650.1">Figure 8.17 – Examples of screenshots where a neural network is used to learn how to play Atari games (</span><a href="https://arxiv.org/abs/1312.5602"><span class="koboSpan" id="kobo.651.1">https://arxiv.org/abs/1312.5602</span></a><span class="koboSpan" id="kobo.652.1">)</span></p>
<p><span class="koboSpan" id="kobo.653.1">We </span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.654.1">will now present five different algorithms in order to understand the differences between the different types of approaches (off- and on-policy, model-free, and </span><span class="No-Break"><span class="koboSpan" id="kobo.655.1">model-based approaches).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer230">
<span class="koboSpan" id="kobo.656.1"><img alt="Figure 8.18 – Summary table of RL approaches" src="image/B21257_08_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.657.1">Figure 8.18 – Summary table of RL approaches</span></p>
<h3><span class="koboSpan" id="kobo.658.1">Q-learning and Deep Q-Network (DQN)</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.659.1">Q-learning</span></strong><span class="koboSpan" id="kobo.660.1"> is a </span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.661.1">lookup-table-based </span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.662.1">approach </span><a id="_idIndexMarker974"/><span class="koboSpan" id="kobo.663.1">underlying </span><strong class="bold"><span class="koboSpan" id="kobo.664.1">Deep Q-Network</span></strong><span class="koboSpan" id="kobo.665.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.666.1">DQN</span></strong><span class="koboSpan" id="kobo.667.1">), an algorithm used by DeepMind to train an agent capable </span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.668.1">of solving video games. </span><span class="koboSpan" id="kobo.668.2">In the Q-learning algorithm, we</span><a id="_idIndexMarker976"/><span class="koboSpan" id="kobo.669.1"> have a </span><strong class="bold"><span class="koboSpan" id="kobo.670.1">Q-table of State-Action values</span></strong><span class="koboSpan" id="kobo.671.1">, where we have a row for each state and a column for each action, and each cell contains an estimated Q-value for the corresponding state-action pair. </span><span class="koboSpan" id="kobo.671.2">The Q-values are initially set to zero. </span><span class="koboSpan" id="kobo.671.3">When the agent receives feedback from interacting with the environment, we iteratively conduct the update of the values (until they converge to the optimal values). </span><span class="koboSpan" id="kobo.671.4">Note that this update is conducted using the Bellman equation (the Q-value in the table represents the expected future rewards if the agent takes that action from that state and follows the best </span><span class="No-Break"><span class="koboSpan" id="kobo.672.1">strategy afterward).</span></span></p>
<p><span class="koboSpan" id="kobo.673.1">Q-learning </span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.674.1">finds the optimal policy by learning the optimal Q-value for each state-action pair. </span><span class="koboSpan" id="kobo.674.2">Initially, the agent chooses actions at random, but by interacting with the environment and receiving feedback (reward), it learns which actions are best. </span><span class="koboSpan" id="kobo.674.3">During each iteration, it conducts the table update using the Bellman equation. </span><span class="koboSpan" id="kobo.674.4">The agent generally</span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.675.1"> chooses the action that has the highest Q-value (greedy strategy), but we can control the degree of exploration (</span><em class="italic"><span class="koboSpan" id="kobo.676.1">ε</span></em><span class="koboSpan" id="kobo.677.1">-greedy policy). </span><span class="koboSpan" id="kobo.677.2">Over time, these estimates become more and more accurate and the model converges to the </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">optimal Q-values.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer231">
<span class="koboSpan" id="kobo.679.1"><img alt="Figure 8.19 – Q learning example" src="image/B21257_08_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.680.1">Figure 8.19 – Q learning example</span></p>
<p><span class="koboSpan" id="kobo.681.1">In complex environments, using</span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.682.1"> a table to store values becomes impractical due to the potentially massive size and computational intractability. </span><span class="koboSpan" id="kobo.682.2">Instead, we can use a Q-function, which maps state-action pairs to Q-values. </span><span class="koboSpan" id="kobo.682.3">Given that neural networks can effectively model complex functions, they can be employed to approximate the Q-function efficiently. </span><span class="koboSpan" id="kobo.682.4">By providing as input the state </span><em class="italic"><span class="koboSpan" id="kobo.683.1">S</span></em><span class="koboSpan" id="kobo.684.1">, the neural network provides as output the Q-value for the state-action pair (in other words, the Q-values for all the actions you can take from that state). </span><span class="koboSpan" id="kobo.684.2">The principle is very similar to the Q-learning algorithm. </span><span class="koboSpan" id="kobo.684.3">We start with random estimates for the Q-values, explore the environment with an </span><em class="italic"><span class="koboSpan" id="kobo.685.1">ε</span></em><span class="koboSpan" id="kobo.686.1">-greedy policy, and conduct the update of </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">the estimates.</span></span></p>
<p><span class="koboSpan" id="kobo.688.1">The DQN architecture</span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.689.1"> consists</span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.690.1"> of three main components: two neural networks (the Q-network and the target network) and an experience replay component. </span><span class="koboSpan" id="kobo.690.2">The Q-network (a classical neural network) is the agent that is trained to produce the optimal state-action value. </span><span class="koboSpan" id="kobo.690.3">Experience replay, on the other hand, is used to generate data to train the </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">neural network.</span></span></p>
<p><span class="koboSpan" id="kobo.692.1">The</span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.693.1"> Q-network is trained on multiple time steps and on many episodes, with the aim of minimizing the difference between predicted Q-values and the target Q-values. </span><span class="koboSpan" id="kobo.693.2">During the agent’s interaction with the environment, each experience (a tuple of state, action, reward, and next state) is stored in this experience replay buffer. </span><span class="koboSpan" id="kobo.693.3">During training, random batches of experiences (a mix of old and new experiences) are selected from the buffer to update the Q-network. </span><span class="koboSpan" id="kobo.693.4">This allows breaking the correlation between consecutive experiences (helps stabilize the training) and reusing the past experience multiple times (increases data efficiency). </span><span class="koboSpan" id="kobo.693.5">The target network is a copy of the Q-network used to generate the target Q-values for training. </span><span class="koboSpan" id="kobo.693.6">Periodically, the target network weights are updated (e.g., every few thousand steps) by copying the Q-network weights; this stabilizes the training. </span><span class="koboSpan" id="kobo.693.7">During training, the Q-network predicts the Q-value for actions given a state (predicted Q-value) and the target network predicts the target Q-value for all actions given the state. </span><span class="koboSpan" id="kobo.693.8">The predicted Q-value, target Q-value, and the observed reward are used to calculate the loss and update the weight of </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">the Q-network.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer232">
<span class="koboSpan" id="kobo.695.1"><img alt="Figure 8.20 – DQN training algorithm" src="image/B21257_08_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.696.1">Figure 8.20 – DQN training algorithm</span></p>
<p><span class="koboSpan" id="kobo.697.1">DQN has a </span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.698.1">number of innovations </span><a id="_idIndexMarker984"/><span class="No-Break"><span class="koboSpan" id="kobo.699.1">and advantages:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.700.1">Experience replay makes training more stable and efficient. </span><span class="koboSpan" id="kobo.700.2">Neural networks usually take a batch of data as input rather than a single state, so during training, the gradient will have less variance and the weights converge more quickly. </span><span class="koboSpan" id="kobo.700.3">Experience replay also allows us to reduce noise during training because we can conduct a kind of “shuffling” of experiences and thus </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">better generalization.</span></span></li>
<li><span class="koboSpan" id="kobo.702.1">The introduction of a target network mitigates the issue of non-stationary targets, which can cause instability in training. </span><span class="koboSpan" id="kobo.702.2">The target network is untrained, so the target Q-values are stable and have </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">few fluctuations.</span></span></li>
<li><span class="koboSpan" id="kobo.704.1">DQN is effective with high-dimensional spaces such as images and is able to extract features by itself and learn effective policies. </span><span class="koboSpan" id="kobo.704.2">These capabilities enabled DQN to master Atari games by taking raw pixels </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">as input.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.706.1">There are, of course, </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">also</span></span><span class="No-Break"><a id="_idIndexMarker985"/></span><span class="No-Break"><span class="koboSpan" id="kobo.708.1"> drawbacks:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.709.1">Although it is more efficient than Q-learning, DQN still requires a large number of samples to learn effectively; this limits its use for tasks where there is little data (</span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">sample inefficiency)</span></span></li>
<li><span class="koboSpan" id="kobo.711.1">It is not stable when the action spaces are continuous, while it works well for discrete </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">action spaces</span></span></li>
<li><span class="koboSpan" id="kobo.713.1">It is sensitive</span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.714.1"> to the choice of hyperparameters (such as learning rate, replay buffer size, and update frequency of the </span><span class="No-Break"><span class="koboSpan" id="kobo.715.1">target network)</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.716.1">The REINFORCE algorithm</span></h3>
<p><span class="koboSpan" id="kobo.717.1">DQN focuses </span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.718.1">on learning </span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.719.1">the value of an action in different states. </span><strong class="bold"><span class="koboSpan" id="kobo.720.1">REINFORCE</span></strong><span class="koboSpan" id="kobo.721.1"> is instead a policy-based method. </span><span class="koboSpan" id="kobo.721.2">These methods learn policy directly, mapping states to actions without learning a value function. </span><span class="koboSpan" id="kobo.721.3">The core idea is to optimize policy by maximizing the expected cumulative reward the agent receives over time. </span><span class="koboSpan" id="kobo.721.4">REINFORCE is a foundational algorithm for learning how to train agents to handle complex, continuous </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">action spaces.</span></span></p>
<p><span class="koboSpan" id="kobo.723.1">The policy is represented by a neural network that takes the current state as input and produces a probability distribution over all possible actions (the probability that an agent will perform a certain action). </span><span class="koboSpan" id="kobo.723.2">This is called stochastic policy because we do not have an action as output directly, but probabilities. </span><span class="koboSpan" id="kobo.723.3">Policy gradient methods try to improve the policy directly (by changing parameters during training) so that the policy produces better results. </span><span class="koboSpan" id="kobo.723.4">So, again, we start with a random policy (the neural network weights are initialized randomly) and let the agent act in the environment according to its policy, which causes a trajectory (a series of states and actions) to be produced. </span><span class="koboSpan" id="kobo.723.5">If this trajectory collects high rewards, we conduct an update of the weights so that this trajectory is more likely to be produced in the future. </span><span class="koboSpan" id="kobo.723.6">If, on the contrary, the agent performs poorly, the update of the weights will be directed to make that trajectory </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">less likely.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<span class="koboSpan" id="kobo.725.1"><img alt="Figure 8.21 – Example of trajectory" src="image/B21257_08_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.726.1">Figure 8.21 – Example of trajectory</span></p>
<p><span class="koboSpan" id="kobo.727.1">So, the </span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.728.1">first step in this process is to</span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.729.1"> initialize a neural network (policy </span><em class="italic"><span class="koboSpan" id="kobo.730.1">P</span></em><span class="koboSpan" id="kobo.731.1">) with its parameters </span><em class="italic"><span class="koboSpan" id="kobo.732.1">θ</span></em><span class="koboSpan" id="kobo.733.1">. </span><span class="koboSpan" id="kobo.733.2">Since these weights are initially random, the policy for a state as input will lead to random actions. </span><span class="koboSpan" id="kobo.733.3">We then generate a trajectory </span><em class="italic"><span class="koboSpan" id="kobo.734.1">τ</span></em><span class="koboSpan" id="kobo.735.1">, letting the agent interact with the environment. </span><span class="koboSpan" id="kobo.735.2">Starting from state </span><em class="italic"><span class="koboSpan" id="kobo.736.1">s</span></em><span class="subscript"><span class="koboSpan" id="kobo.737.1">0</span></span><span class="koboSpan" id="kobo.738.1">, we let the agent move according to policy </span><em class="italic"><span class="koboSpan" id="kobo.739.1">P</span></em><span class="koboSpan" id="kobo.740.1"> with the parameters </span><em class="italic"><span class="koboSpan" id="kobo.741.1">θ</span></em><span class="koboSpan" id="kobo.742.1">. </span><span class="koboSpan" id="kobo.742.2">In practice, state </span><em class="italic"><span class="koboSpan" id="kobo.743.1">S</span></em><span class="koboSpan" id="kobo.744.1"> is given as input to the neural network and generates a distribution of actions. </span><span class="koboSpan" id="kobo.744.2">We select an action </span><em class="italic"><span class="koboSpan" id="kobo.745.1">a</span></em><span class="subscript"><span class="koboSpan" id="kobo.746.1">0</span></span><span class="koboSpan" id="kobo.747.1"> sampling from this distribution. </span><span class="koboSpan" id="kobo.747.2">This process is repeated for as long as possible (e.g., till the end of the game), the set of states and actions being </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">our trajectory.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer234">
<span class="koboSpan" id="kobo.749.1"><img alt="Figure 8.22 – Getting a distribution from a neural network" src="image/B21257_08_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.750.1">Figure 8.22 – Getting a distribution from a neural network</span></p>
<p><span class="koboSpan" id="kobo.751.1">During</span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.752.1"> the trajectory, we </span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.753.1">collect rewards (called reward-to-go or also return </span><em class="italic"><span class="koboSpan" id="kobo.754.1">G</span></em><span class="subscript"><span class="koboSpan" id="kobo.755.1">t</span></span><span class="koboSpan" id="kobo.756.1">). </span><span class="koboSpan" id="kobo.756.2">The return is the total cumulative reward received from time step </span><em class="italic"><span class="koboSpan" id="kobo.757.1">t</span></em><span class="koboSpan" id="kobo.758.1"> to the end of the episode, discounted by a factor </span><em class="italic"><span class="koboSpan" id="kobo.759.1">γ</span></em><span class="koboSpan" id="kobo.760.1">. </span><span class="koboSpan" id="kobo.760.2">The discount factor determines how important future rewards are in relation to immediate rewards. </span><span class="koboSpan" id="kobo.760.3">In this case, we have a function that gives us the expected return for a given policy and we want to maximize it. </span><span class="koboSpan" id="kobo.760.4">So, we calculate the gradient, and via gradient ascent, we modify the parameters of our </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">neural network.</span></span></p>
<p><span class="koboSpan" id="kobo.762.1">REINFORCE is conceptually simple and easy to implement, suitable for continuous action spaces (since it directly learns a policy), and enables end-to-end learning (the algorithm learns directly from raw data). </span><span class="koboSpan" id="kobo.762.2">Some of the challenges with this algorithm, however, are the high variance in policy updates (it relies on full episodes being returned and updates can be noisy and unstable), it requires a lot of data (a large number of episodes are needed because it discards data after each update and does not reuse experiences), it is not suitable for environments where data is expensive, and it does not work well when there are </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">delayed rewards.</span></span></p>
<p><span class="koboSpan" id="kobo.764.1">Note that the REINFORCE algorithm is an on-policy algorithm since the policy receives updates only based on experiences collected with the same policy. </span><span class="koboSpan" id="kobo.764.2">At each iteration, the agent uses the updated policy and collects experience with it for the update. </span><span class="koboSpan" id="kobo.764.3">In the case of off-policy methods, experiences collected with other policies are also used. </span><span class="koboSpan" id="kobo.764.4">This is, for example, what we saw with DQN, where we were using experiences in the batch that were collected with a </span><span class="No-Break"><span class="koboSpan" id="kobo.765.1">different policy.</span></span></p>
<h3><span class="koboSpan" id="kobo.766.1">Proximal Policy Optimization (PPO)</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.767.1">Proximal Policy Optimization</span></strong><span class="koboSpan" id="kobo.768.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.769.1">PPO</span></strong><span class="koboSpan" id="kobo.770.1">) is </span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.771.1">one of the most widely cited and used algorithms</span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.772.1"> in RL. </span><span class="koboSpan" id="kobo.772.2">Introduced by OpenAI in 2017, PPO was designed to balance the simplicity of policy gradient methods, such as REINFORCE, with the stability of more complex algorithms, such as </span><strong class="bold"><span class="koboSpan" id="kobo.773.1">Trust Region Policy Optimization</span></strong><span class="koboSpan" id="kobo.774.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.775.1">TRPO</span></strong><span class="koboSpan" id="kobo.776.1">). </span><span class="koboSpan" id="kobo.776.2">In</span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.777.1"> essence, PPO is a practical and efficient algorithm that performs well on benchmarks while being relatively easy to implement </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">and tune.</span></span></p>
<p><span class="koboSpan" id="kobo.779.1">PPO shares similarities with REINFORCE but includes important improvements that make training much more stable. </span><span class="koboSpan" id="kobo.779.2">One of the challenges in policy-based methods is the choice of hyperparameters (especially the learning rate) and the risk of unstable weight updates. </span><span class="koboSpan" id="kobo.779.3">The key innovation of PPO is to ensure that policy updates are not too large, as this could destabilize training. </span><span class="koboSpan" id="kobo.779.4">PPO achieves this by using a constraint on the objective function that limits how much the policy can change in a single update, thereby avoiding drastic changes in the </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">network weights.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">A significant problem with traditional policy gradient methods is their inability to recover from poor updates. </span><span class="koboSpan" id="kobo.781.2">If a policy performs poorly, the agent may generate sparse or low-quality training data in the next iteration, creating a self-reinforcing loop that can be difficult to escape. </span><span class="koboSpan" id="kobo.781.3">PPO addresses this by stabilizing </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">policy updates.</span></span></p>
<p><span class="koboSpan" id="kobo.783.1">The policy in PPO is represented by a neural network, </span><em class="italic"><span class="koboSpan" id="kobo.784.1">πθ(a</span></em><span class="koboSpan" id="kobo.785.1">|</span><em class="italic"><span class="koboSpan" id="kobo.786.1">s)</span></em><span class="koboSpan" id="kobo.787.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.788.1">θ</span></em><span class="koboSpan" id="kobo.789.1"> represents the network’s weights. </span><span class="koboSpan" id="kobo.789.2">The network takes the current state </span><em class="italic"><span class="koboSpan" id="kobo.790.1">s</span></em><span class="koboSpan" id="kobo.791.1"> as input and outputs a probability distribution over possible actions </span><em class="italic"><span class="koboSpan" id="kobo.792.1">a</span></em><span class="koboSpan" id="kobo.793.1">. </span><span class="koboSpan" id="kobo.793.2">Initially, the weights are randomly initialized. </span><span class="koboSpan" id="kobo.793.3">As the agent interacts with the environment, it generates batches of experiences (state, action, reward) under the current policy. </span><span class="koboSpan" id="kobo.793.4">The agent also calculates the advantage estimate, which measures how much better (or worse) a chosen action was compared to the expected value of </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">the state.</span></span></p>
<p><span class="koboSpan" id="kobo.795.1">The main difference from simpler policy gradient methods lies in PPO’s use of a </span><strong class="bold"><span class="koboSpan" id="kobo.796.1">clipped objective function</span></strong><span class="koboSpan" id="kobo.797.1">. </span><span class="koboSpan" id="kobo.797.2">This</span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.798.1"> function ensures that policy updates are stable and prevents large, destabilizing changes. </span><span class="koboSpan" id="kobo.798.2">If the probability ratio between the new and old policies </span><em class="italic"><span class="koboSpan" id="kobo.799.1">rt(θ)</span></em><span class="koboSpan" id="kobo.800.1"> falls outside the range [1−</span><em class="italic"><span class="koboSpan" id="kobo.801.1">ϵ</span></em><span class="koboSpan" id="kobo.802.1">,1+</span><em class="italic"><span class="koboSpan" id="kobo.803.1">ϵ</span></em><span class="koboSpan" id="kobo.804.1">], where </span><em class="italic"><span class="koboSpan" id="kobo.805.1">ϵ</span></em><span class="koboSpan" id="kobo.806.1"> is a small hyperparameter (e.g., 0.2), the update is clipped. </span><span class="koboSpan" id="kobo.806.2">This clipping mechanism ensures that policy updates remain within a safe range, preventing the policy from diverging too much in a </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">single update.</span></span></p>
<p><span class="koboSpan" id="kobo.808.1">A common variant of PPO </span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.809.1">uses an </span><strong class="bold"><span class="koboSpan" id="kobo.810.1">actor-critic architecture</span></strong><span class="koboSpan" id="kobo.811.1">, where the actor learns the policy, and the critic learns the value function. </span><span class="koboSpan" id="kobo.811.2">The critic provides feedback on the quality of the actions, helping to reduce the variance of the updates and improve learning efficiency (we discuss this more in </span><span class="No-Break"><span class="koboSpan" id="kobo.812.1">detail later).</span></span></p>
<p><span class="koboSpan" id="kobo.813.1">Overall, PPO is </span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.814.1">both a stable and robust algorithm, less prone to instability than simpler policy gradient methods and easier to use than more complex algorithms such as TRPO. </span><span class="koboSpan" id="kobo.814.2">It does not require solving complex optimization problems or calculating second-order gradients, making </span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.815.1">it a practical choice for many applications. </span><span class="koboSpan" id="kobo.815.2">However, PPO still requires careful tuning of hyperparameters, such as the clipping parameter </span><em class="italic"><span class="koboSpan" id="kobo.816.1">ϵ</span></em><span class="koboSpan" id="kobo.817.1">, learning rate, and batch size. </span><span class="koboSpan" id="kobo.817.2">Additionally, it can suffer from high variance in environments with long episodes or </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">delayed rewards.</span></span></p>
<h3><span class="koboSpan" id="kobo.819.1">The actor-critic algorithm</span></h3>
<p><span class="koboSpan" id="kobo.820.1">The </span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.821.1">actor-critic algorithm </span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.822.1">is another popular approach in RL, which combines the strengths of two different methods: value-based methods (such as Q-learning) and policy-based methods. </span><span class="koboSpan" id="kobo.822.2">The actor-critic model consists of </span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">two components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.824.1">Actor</span></strong><span class="koboSpan" id="kobo.825.1">: The actor is responsible for deciding what action should be taken in the current state of the environment. </span><span class="koboSpan" id="kobo.825.2">The policy is generally a neural network that produces a probability distribution over actions. </span><span class="koboSpan" id="kobo.825.3">The actor tries to maximize the expected return by optimizing </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">the policy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.827.1">Critic</span></strong><span class="koboSpan" id="kobo.828.1">: The critic evaluates the actions taken by the actor by estimating the value function. </span><span class="koboSpan" id="kobo.828.2">This function indicates how good an action is in terms of expected future rewards. </span><span class="koboSpan" id="kobo.828.3">The value function can be the state value function </span><em class="italic"><span class="koboSpan" id="kobo.829.1">V(s)</span></em><span class="koboSpan" id="kobo.830.1"> or the action-value </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">function </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.832.1">Q(s,a)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.834.1">The insight behind this approach is that the actor is the decision-maker who learns how to improve decisions over time. </span><span class="koboSpan" id="kobo.834.2">The critic, on the other hand, is a kind of advisor who evaluates the goodness of actions and gives feedback </span><span class="No-Break"><span class="koboSpan" id="kobo.835.1">on strategy.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer235">
<span class="koboSpan" id="kobo.836.1"><img alt="Figure 8.23 – Actor-critic approach" src="image/B21257_08_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.837.1">Figure 8.23 – Actor-critic approach</span></p>
<p><span class="koboSpan" id="kobo.838.1">The</span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.839.1"> process</span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.840.1"> can be defined by </span><span class="No-Break"><span class="koboSpan" id="kobo.841.1">four steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.842.1">The agent interacts with the environment, and based on its policy, selects an action based on the current state. </span><span class="koboSpan" id="kobo.842.2">It then receives feedback from the environment in the form of a reward and a </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">new state.</span></span></li>
<li><span class="koboSpan" id="kobo.844.1">In the second step, the critic uses the reward and the new state to calculate a </span><strong class="bold"><span class="koboSpan" id="kobo.845.1">temporal difference</span></strong><span class="koboSpan" id="kobo.846.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.847.1">TD</span></strong><span class="koboSpan" id="kobo.848.1">) error. </span><span class="koboSpan" id="kobo.848.2">The </span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.849.1">TD error measures how far the critic’s current estimate of the value function is from the observed outcome. </span><span class="koboSpan" id="kobo.849.2">The TD error is then the difference between the reward at time step </span><em class="italic"><span class="koboSpan" id="kobo.850.1">t</span></em><span class="koboSpan" id="kobo.851.1"> (plus a discount factor </span><em class="italic"><span class="koboSpan" id="kobo.852.1">γ</span></em><span class="koboSpan" id="kobo.853.1"> for the critic’s estimates of the value of the next state </span><em class="italic"><span class="koboSpan" id="kobo.854.1">V(st+1)</span></em><span class="koboSpan" id="kobo.855.1"> to serve to balance the impact of immediate and future rewards) and the critic’s estimates of the value of the current </span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">state </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.857.1">V(st)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.859.1">The critic updates its value function parameters to minimize the TD error. </span><span class="koboSpan" id="kobo.859.2">This is done with </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">gradient descent.</span></span></li>
<li><span class="koboSpan" id="kobo.861.1">The actor </span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.862.1">is updated as well. </span><span class="koboSpan" id="kobo.862.2">The actor uses the TD error as a feedback signal. </span><span class="koboSpan" id="kobo.862.3">If the error is positive, it means that the action was better than expected and the actor should take it more often (increase the probability of taking this action in the future). </span><span class="koboSpan" id="kobo.862.4">If the error is negative, the actor should decrease the probability. </span><span class="koboSpan" id="kobo.862.5">The actor maximizes the policy using gradient ascent; we want to maximize the </span><span class="No-Break"><span class="koboSpan" id="kobo.863.1">expected return.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.864.1">Actor-critic methods </span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.865.1">work well with continuous action spaces, where value-based methods have problems. </span><span class="koboSpan" id="kobo.865.2">It is a stable and efficient method and reduces the variance of policy gradient updates. </span><span class="koboSpan" id="kobo.865.3">On the other hand, it is sensitive to hyperparameters, you have to train two networks, and it is more complex than Q-learning </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">or REINFORCE.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.867.1">Advantage Actor-Critic</span></strong><span class="koboSpan" id="kobo.868.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.869.1">A2C</span></strong><span class="koboSpan" id="kobo.870.1">) is a</span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.871.1"> popular variant where multiple agents interact with multiple instances of the environment in parallel. </span><span class="koboSpan" id="kobo.871.2">This allows for </span><span class="No-Break"><span class="koboSpan" id="kobo.872.1">faster training.</span></span></p>
<h3><span class="koboSpan" id="kobo.873.1">AlphaZero</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.874.1">AlphaZero</span></strong><span class="koboSpan" id="kobo.875.1"> is a</span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.876.1"> groundbreaking </span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.877.1">model-based RL algorithm developed by DeepMind in 2017, capable of mastering chess, shogi (Japanese chess), and Go. </span><span class="koboSpan" id="kobo.877.2">It has achieved superhuman performance, defeating human champions in these games. </span><span class="koboSpan" id="kobo.877.3">The success of AlphaZero lies in its innovative combination of deep learning and </span><strong class="bold"><span class="koboSpan" id="kobo.878.1">Monte Carlo Tree Search</span></strong><span class="koboSpan" id="kobo.879.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.880.1">MCTS</span></strong><span class="koboSpan" id="kobo.881.1">), which</span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.882.1"> allows it to learn and plan effectively without human expertise or </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">handcrafted rules.</span></span></p>
<p><span class="koboSpan" id="kobo.884.1">AlphaZero learns entirely through </span><strong class="bold"><span class="koboSpan" id="kobo.885.1">self-play</span></strong><span class="koboSpan" id="kobo.886.1">, starting with no prior knowledge other than the basic rules of the game. </span><span class="koboSpan" id="kobo.886.2">It plays millions of games against itself, gradually understanding what constitutes good or bad moves through trial and error. </span><span class="koboSpan" id="kobo.886.3">This self-play approach allows AlphaZero to discover optimal strategies, often surpassing even those developed by expert human players. </span><span class="koboSpan" id="kobo.886.4">Additionally, it enables the model to generate a vast amount of training data, far more than could be obtained by simply analyzing human games. </span><span class="koboSpan" id="kobo.886.5">The algorithm uses a deep neural network to represent both the policy (which actions to take) and the value function (the expected outcome of the game from a </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">given state).</span></span></p>
<p><span class="koboSpan" id="kobo.888.1">Traditional chess</span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.889.1"> engines used to rely on game-tree search techniques. </span><span class="koboSpan" id="kobo.889.2">At each move, they would construct a game tree that represented all possible legal moves from the current position and performed a </span><strong class="bold"><span class="koboSpan" id="kobo.890.1">depth-first search (DFS)</span></strong><span class="koboSpan" id="kobo.891.1"> to </span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.892.1">a certain depth. </span><span class="koboSpan" id="kobo.892.2">This brute-force search examined all legal moves, assigning values to the final nodes based on heuristic evaluations formulated by the chess community. </span><span class="koboSpan" id="kobo.892.3">These heuristics, such as king safety, pawn structure, and control of the center, mimic factors used by human chess players to judge the quality of </span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">a move.</span></span></p>
<p><span class="koboSpan" id="kobo.894.1">After evaluating the final nodes, traditional engines would backtrack and analyze the positions, pruning fewer promising branches to simplify the search. </span><span class="koboSpan" id="kobo.894.2">Despite these optimizations, this method had limitations, often leading to suboptimal moves and being computationally expensive. </span><span class="koboSpan" id="kobo.894.3">This is where MCTS </span><span class="No-Break"><span class="koboSpan" id="kobo.895.1">comes in.</span></span></p>
<p><span class="koboSpan" id="kobo.896.1">MCTS is </span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.897.1">an algorithm designed for decision-making in environments where planning several moves ahead is essential, especially in games with large state spaces where an exhaustive search is infeasible. </span><span class="koboSpan" id="kobo.897.2">MCTS builds a search tree by simulating games multiple times, gradually improving its understanding of the best actions </span><span class="No-Break"><span class="koboSpan" id="kobo.898.1">through experience.</span></span></p>
<p><span class="koboSpan" id="kobo.899.1">MCTS operates through four main steps, repeated to refine the </span><span class="No-Break"><span class="koboSpan" id="kobo.900.1">search tree:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.901.1">Selection</span></strong><span class="koboSpan" id="kobo.902.1">: Starting </span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.903.1">from the root node (the current state), the algorithm selects child nodes using a strategy that balances exploration (trying less-explored moves) and exploitation (choosing moves that have shown promise). </span><span class="koboSpan" id="kobo.903.2">This</span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.904.1"> is often done using the </span><strong class="bold"><span class="koboSpan" id="kobo.905.1">Upper Confidence Bound for Trees</span></strong><span class="koboSpan" id="kobo.906.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.907.1">UCT</span></strong><span class="koboSpan" id="kobo.908.1">) formula, which considers both the average reward and the number of visits to </span><span class="No-Break"><span class="koboSpan" id="kobo.909.1">each node.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.910.1">Expansion</span></strong><span class="koboSpan" id="kobo.911.1">: If the </span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.912.1">selected node is not a terminal state (the end of the game), the algorithm adds one or more child nodes, representing possible actions from this state. </span><span class="koboSpan" id="kobo.912.2">This expansion allows the search to cover new potential moves </span><span class="No-Break"><span class="koboSpan" id="kobo.913.1">and outcomes.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.914.1">Simulation (rollout</span></strong><span class="koboSpan" id="kobo.915.1">): From a newly added node, MCTS performs a simulation, or “rollout,” by playing </span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.916.1">the game to a terminal state using a simple or random policy. </span><span class="koboSpan" id="kobo.916.2">The outcome of this simulation (win, loss, or draw) provides a reward, serving as an estimate of the value of the </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">actions taken.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.918.1">Backpropagation</span></strong><span class="koboSpan" id="kobo.919.1">: The</span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.920.1"> reward from the simulation is then backpropagated up the tree, updating the values associated with each node along the path to the root. </span><span class="koboSpan" id="kobo.920.2">This includes updating the average reward and the number of visits for each node. </span><span class="koboSpan" id="kobo.920.3">Over time, these updates help the algorithm determine which moves are </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">most promising.</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer236">
<span class="koboSpan" id="kobo.922.1"><img alt="Figure 8.24 – Monte Carlo Tree Search (https://en.wikipedia.org/wiki/Monte_Carlo_tree_search)" src="image/B21257_08_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.923.1">Figure 8.24 – Monte Carlo Tree Search (</span><a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search"><span class="koboSpan" id="kobo.924.1">https://en.wikipedia.org/wiki/Monte_Carlo_tree_search</span></a><span class="koboSpan" id="kobo.925.1">)</span></p>
<p><span class="koboSpan" id="kobo.926.1">AlphaZero</span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.927.1"> then uses a neural network (a convolutional neural network that takes as input the arrangement of pieces on the board) and </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.928.1">produces two outputs: policy head (a probability distribution over all possible moves, guiding the agent on which moves to consider) and value head (the likelihood of winning from the current board position, helping the agent to evaluate the strength of various states). </span><span class="koboSpan" id="kobo.928.2">AlphaZero uses MCTS to simulate potential moves and their outcomes (the agent plans several moves ahead in the game). </span><span class="koboSpan" id="kobo.928.3">Through MCTS, the model explores the moves that seem most promising and gradually improves its understanding of the game. </span><span class="koboSpan" id="kobo.928.4">The tree search uses the policy and value outputs from the neural network to prioritize which branches of the tree to explore. </span><span class="koboSpan" id="kobo.928.5">AlphaZero learns to play by playing against itself (self-play). </span><span class="koboSpan" id="kobo.928.6">In each game, the agent uses MCTS to decide moves and saves </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.929.1">states (positions on the board), chosen moves, and results. </span><span class="koboSpan" id="kobo.929.2">This data is used to improve the policy and value estimates (neural network </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">weight updates).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer237">
<span class="koboSpan" id="kobo.931.1"><img alt="Figure 8.25 – AlphaZero pipeline (https://www.mdpi.com/2079-9292/10/13/1533)" src="image/B21257_08_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.932.1">Figure 8.25 – AlphaZero pipeline (</span><a href="https://www.mdpi.com/2079-9292/10/13/1533"><span class="koboSpan" id="kobo.933.1">https://www.mdpi.com/2079-9292/10/13/1533</span></a><span class="koboSpan" id="kobo.934.1">)</span></p>
<p><span class="koboSpan" id="kobo.935.1">AlphaZero</span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.936.1"> therefore presents three </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">main innovations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.938.1">Generalizing across games</span></strong><span class="koboSpan" id="kobo.939.1">: The same algorithm is used for three different games (chess, shogi, and Go), without the need for </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">game-specific adjustments.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.941.1">It requires no human knowledge</span></strong><span class="koboSpan" id="kobo.942.1">: Unlike traditional chess engines that use an extensive database of human games and strategies, AlphaZero learns the game on its own. </span><span class="koboSpan" id="kobo.942.2">The model prioritizes strategies that offer long-term rewards within the game, rather than focusing solely on immediate benefits from individual moves. </span><span class="koboSpan" id="kobo.942.3">This approach enables the model to discover innovative strategies previously unexplored by humans or traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.943.1">chess engines.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.944.1">Efficient search and learning</span></strong><span class="koboSpan" id="kobo.945.1">: Using MCTS and deep learning allows more efficient use of computational resources. </span><span class="koboSpan" id="kobo.945.2">Instead of conducting an extensive search of all possible moves, AlphaZero focuses only on the most </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">promising moves.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.947.1">Of course, AlphaZero is not without flaws either. </span><span class="koboSpan" id="kobo.947.2">The algorithm has a huge computational cost since it has to play millions of games against itself. </span><span class="koboSpan" id="kobo.947.3">Also, the algorithm works well for games (or settings where there is perfect information) but it is more difficult </span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.948.1">to adapt it to environments where the information is incomplete. </span><span class="koboSpan" id="kobo.948.2">Finally, there is discussion about actually understanding the game or learning abstract concepts, since the model fails some chess puzzles that are easy </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">for humans.</span></span></p>
<p><span class="koboSpan" id="kobo.950.1">In the next subsection, we will discuss the challenges with RL and new, exciting lines </span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">of research.</span></span></p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.952.1">Challenges and future direction for deep RL</span></h2>
<p><span class="koboSpan" id="kobo.953.1">Although</span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.954.1"> RL has made significant progress, several challenges and </span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.955.1">several active lines of </span><span class="No-Break"><span class="koboSpan" id="kobo.956.1">research remain:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.957.1">Generalization in unseen environments</span></strong><span class="koboSpan" id="kobo.958.1">: Generalization in environments that the agent has not seen remains a complex task. </span><span class="koboSpan" id="kobo.958.2">Agents are usually trained in a simulated environment or in specific settings, where they are able to excel after training. </span><span class="koboSpan" id="kobo.958.3">However, transferring learned skills to new environments, dynamic environments, or changing conditions is difficult. </span><span class="koboSpan" id="kobo.958.4">This limits the use of deep RL algorithms in the real world because real environments are rarely static or perfectly predictable. </span><span class="koboSpan" id="kobo.958.5">True generalization requires that a model not only learns solutions that are task-specific but also adapts to a range of situations (even if they did not occur </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">during training).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.960.1">Reward function design</span></strong><span class="koboSpan" id="kobo.961.1">: Reward function controls agent behavior, learning, and performance. </span><span class="koboSpan" id="kobo.961.2">Designing a reward function is difficult, especially in complex, scattered environments. </span><span class="koboSpan" id="kobo.961.3">In sparse reward settings, where there is limited feedback and it is often delayed, defining the reward and function is complex but critical. </span><span class="koboSpan" id="kobo.961.4">Even so, there is often a risk of creating bias, leading the policy to overfitting or unexpected behaviors, or making </span><span class="No-Break"><span class="koboSpan" id="kobo.962.1">it suboptimal.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.963.1">Compound error in model-based planning</span></strong><span class="koboSpan" id="kobo.964.1">: Model-based RL is at risk of compounding errors. </span><span class="koboSpan" id="kobo.964.2">The longer the horizon of predictions, the more errors accumulate in model predictions, leading to significant deviations from the optimal trajectory. </span><span class="koboSpan" id="kobo.964.3">This is especially the case for complex or high-dimensional space environments, thus limiting their use in </span><span class="No-Break"><span class="koboSpan" id="kobo.965.1">real environments.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.966.1">Multi-task learning</span></strong><span class="koboSpan" id="kobo.967.1">: Creating an agent that can be used for multiple tasks remains difficult, with the risk that the agent learns only the easier ones and ignores the more complex (or otherwise very poorly performing) ones. </span><span class="koboSpan" id="kobo.967.2">Also, a multi-task model often exhibits performance that is far inferior to an agent optimized for a single task. </span><span class="koboSpan" id="kobo.967.3">The design of agents that can therefore be used for multi-task RL is difficult and still an active line </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">of research.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.969.1">Multi-modal RL</span></strong><span class="koboSpan" id="kobo.970.1">: With the advancement of computer vision and NLP, there are deep learning models that can either handle one mode individually or multiple modes together. </span><span class="koboSpan" id="kobo.970.2">This is why there is increasing discussion of using multimodal RL, where </span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.971.1">an agent can move </span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.972.1">through a multimodal environment and integrate information from the various modalities. </span><span class="koboSpan" id="kobo.972.2">For example, a robot can acquire information from the environment in an image and receive commands or instructions in natural language. </span><span class="koboSpan" id="kobo.972.3">An agent in video games receives visual information but also information from dialogues with characters or from other players. </span><span class="koboSpan" id="kobo.972.4">Multimodal learning remains complicated because an agent must simultaneously learn how to process multimodal information and optimize policy to interact in a complex environment. </span><span class="koboSpan" id="kobo.972.5">Similarly, it remains difficult to design a reward function for </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">these cases.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.974.1">In the next section, we will see how a neural network can be used to learn how to play a </span><span class="No-Break"><span class="koboSpan" id="kobo.975.1">video game.</span></span></p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.976.1">Learning how to play a video game with reinforcement learning</span></h2>
<p><span class="koboSpan" id="kobo.977.1">In this</span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.978.1"> subsection, we will discuss how to train an agent to play a video game. </span><span class="koboSpan" id="kobo.978.2">In this case, the agent will be parameterized by a neural network. </span><span class="koboSpan" id="kobo.978.3">Following this policy, it will choose among the actions</span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.979.1"> allowed by the video game, receive feedback from the environment, and use this feedback for parameter updating. </span><span class="koboSpan" id="kobo.979.2">In general, video games provide complex and dynamic environments that simulate real-world scenarios, thus making them an excellent testbed for RL algorithms. </span><span class="koboSpan" id="kobo.979.3">Video games provide a high-dimensional state space (pixel-based states, detailed universes) and a rich action space (discrete or continuous), are inspired by the real world, and can</span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.980.1"> provide both immediate and delayed rewards (e.g., some actions may result in the direct death of the protagonist while a long-term strategy is needed to solve puzzles or win the game). </span><span class="koboSpan" id="kobo.980.2">In addition, many games require the user to explore the environment before they can master it. </span><span class="koboSpan" id="kobo.980.3">Enemies are often dynamic, and the model must learn how to defeat opposing agents or understand complex behaviors to overcome them. </span><span class="koboSpan" id="kobo.980.4">The game also provides clear rewards (which are often frequent or can otherwise be accelerated) that can then be easily defined for a reward function </span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.981.1">and thus make a safe playground (e.g., for algorithms for robotics). </span><span class="koboSpan" id="kobo.981.2">In addition, there are clear benchmarks and one can quickly compare the quality of a </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">new algorithm.</span></span></p>
<p><span class="koboSpan" id="kobo.983.1">We chose the actor-critic approach for this training because it has a number </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">of features:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.985.1">Actor-critic can handle complex and continuous action spaces (control a character in a 3D environment) and thus can be used for a wide variety </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">of games.</span></span></li>
<li><span class="koboSpan" id="kobo.987.1">The actor in the system learns the policy directly, making it efficient for scenarios where finding the policy is crucial. </span><span class="koboSpan" id="kobo.987.2">This is necessary in video games where quick decision-making and strategic planning </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">are required.</span></span></li>
<li><span class="koboSpan" id="kobo.989.1">The critic provides feedback and speeds up learning in comparison to purely policy-based methods. </span><span class="koboSpan" id="kobo.989.2">Using a value function (critic) to evaluate actions reduces the variance of policy updates, so it is more stable and efficient in environments where rewards </span><span class="No-Break"><span class="koboSpan" id="kobo.990.1">are scattered.</span></span></li>
<li><span class="koboSpan" id="kobo.991.1">Actor-critic allows for efficient management of the balance between exploration and exploitation, where the actor explores the environment and the critic guides it by providing feedback. </span><span class="koboSpan" id="kobo.991.2">For more complex environments, actor-critic may not be sufficient, though it is a good initial choice and </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">often sufficient.</span></span></li>
<li><span class="koboSpan" id="kobo.993.1">Actor-critic can also handle long-term planning. </span><span class="koboSpan" id="kobo.993.2">Often, in video games, there can be long-term rewards; the critic’s value function helps the agent understand the long-term impact of </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">its actions.</span></span></li>
<li><span class="koboSpan" id="kobo.995.1">Some variants are efficient in parallelizing and using data. </span><span class="koboSpan" id="kobo.995.2">A2C is a good choice for parallelizing environments and thus collecting more data, thus speeding up training </span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">and convergence.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.997.1">We chose Super Mario as our game because it provides a rich and complex environment. </span><span class="koboSpan" id="kobo.997.2">The </span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.998.1">environment resembles the real world, and the representation of pixel-based observations as input is similar to that of real-world computer vision tasks, making Super Mario a good testbed for RL agents who need to learn to extract meaningful features</span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.999.1"> from visual data. </span><span class="koboSpan" id="kobo.999.2">This environment is also partially observable, so it requires the agent to explore and learn about the environment. </span><span class="koboSpan" id="kobo.999.3">Different levels may require different strategies, so the model must be able to balance exploration </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">and exploitation.</span></span></p>
<p><span class="koboSpan" id="kobo.1001.1">In the game, there are different kinds of challenges, such as navigating obstacles, facing different kinds of enemies, and learning to jump optimally and often dynamically. </span><span class="koboSpan" id="kobo.1001.2">These different challenges represent different skills that an agent should develop: testing the agent’s ability to make precise and timely actions (jumping over obstacles or gaps), assessing threats and deciding when to avoid or engage (avoiding or engaging enemies), and spatial awareness and strategic planning (navigating complex levels). </span><span class="koboSpan" id="kobo.1001.3">The levels are progressive, so with a difficulty that progresses as the agent learns. </span><span class="koboSpan" id="kobo.1001.4">In addition, there are both immediate rewards (collecting coins) and delayed rewards (e.g., completing a level), thus allowing for the evaluation of </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">long-term strategies.</span></span></p>
<p><span class="koboSpan" id="kobo.1003.1">Finally, Super Mario has been widely adopted in the RL research community as a benchmark. </span><span class="koboSpan" id="kobo.1003.2">Major libraries support it, or it is found directly integrated, thus allowing a quick way to test algorithms or conduct comparisons. </span><span class="koboSpan" id="kobo.1003.3">There are also already well-researched strategies; the game is well documented and is a good example for both beginners and experts in RL. </span><span class="koboSpan" id="kobo.1003.4">There are also implementations that are parallelizable, thus allowing effective and </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">fast training.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer238">
<span class="koboSpan" id="kobo.1005.1"><img alt="Figure 8.26 – Super Mario screenshots from the training" src="image/B21257_08_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1006.1">Figure 8.26 – Super Mario screenshots from the training</span></p>
<p><span class="koboSpan" id="kobo.1007.1">All the</span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.1008.1"> code can be found within the</span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.1009.1"> repository, at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1010.1">link: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8/RL_SuperMario"><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr8/RL_SuperMario</span></span></a></p>
<div>
<div class="IMG---Figure" id="_idContainer239">
<span class="koboSpan" id="kobo.1012.1"><img alt="Figure 8.27 – Screenshot of the repository" src="image/B21257_08_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1013.1">Figure 8.27 – Screenshot of the repository</span></p>
<h3><span class="koboSpan" id="kobo.1014.1">Description of the scripts</span></h3>
<p><span class="koboSpan" id="kobo.1015.1">To </span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.1016.1">perform the training, we will use some popular RL libraries (OpenAI’s Gym and PyTorch). </span><span class="koboSpan" id="kobo.1016.2">In the repository, there are different scripts that are used to train </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">the agent:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1018.1">env</span></strong><span class="koboSpan" id="kobo.1019.1">: This script defines the environment where our agent acts (Super Mario) and allows us to record a video of our agent playing, preprocess images for the model, define the reward function, set the world, set a virtual joystick, </span><span class="No-Break"><span class="koboSpan" id="kobo.1020.1">and more.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">model</span></strong><span class="koboSpan" id="kobo.1022.1">: This script defines a PyTorch neural network model for an actor-critic architecture. </span><span class="koboSpan" id="kobo.1022.2">The model is designed to process image-like inputs, extract features, and then use those features to output both action probabilities (actor) and state value </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">estimates (critic).</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1024.1">optimizer</span></strong><span class="koboSpan" id="kobo.1025.1">: This code defines a custom optimizer class called </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">GlobalAdam</span></strong><span class="koboSpan" id="kobo.1027.1">, which extends the functionality of PyTorch’s built-in </span><span class="No-Break"><span class="koboSpan" id="kobo.1028.1">Adam optimizer.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1029.1">train</span></strong><span class="koboSpan" id="kobo.1030.1">: This </span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.1031.1">script sets up and runs a distributed RL system using the </span><strong class="bold"><span class="koboSpan" id="kobo.1032.1">Asynchronous Advantage Actor-Critic</span></strong><span class="koboSpan" id="kobo.1033.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1034.1">A3C</span></strong><span class="koboSpan" id="kobo.1035.1">) method to train an agent to play Super </span><span class="No-Break"><span class="koboSpan" id="kobo.1036.1">Mario Bros.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1037.1">test</span></strong><span class="koboSpan" id="kobo.1038.1">: Model testing is in a separate script. </span><span class="koboSpan" id="kobo.1038.2">This script allows you to load the trained model to play the game while rendering </span><span class="No-Break"><span class="koboSpan" id="kobo.1039.1">the gameplay.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1040.1">process</span></strong><span class="koboSpan" id="kobo.1041.1">: This script acts as the linking piece that integrates all the preceding components into a cohesive RL system for training and testing an agent to play Super </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">Mario Bros.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer240">
<span class="koboSpan" id="kobo.1043.1"><img alt="Figure 8.28 – Global view of the scripts" src="image/B21257_08_28.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1044.1">Figure 8.28 – Global view of the scripts</span></p>
<h3><span class="koboSpan" id="kobo.1045.1">Setting up the environment</span></h3>
<p><span class="koboSpan" id="kobo.1046.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1047.1">env</span></strong><span class="koboSpan" id="kobo.1048.1"> script </span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.1049.1">allows us to have our setup of the environment, especially for RL algorithms such as Deep Q-Learning or actor-critic. </span><span class="koboSpan" id="kobo.1049.2">Inside the script, we import the libraries we need, after which there are some functions that are used to create the world and define how the agent can interact </span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">with it:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1051.1">Monitor</span></strong><span class="koboSpan" id="kobo.1052.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1053.1">Monitor</span></strong><span class="koboSpan" id="kobo.1054.1"> class allows the user to save a visual record of the agent’s gameplay, which is useful for debugging, analyzing agent performance, and sharing results. </span><span class="koboSpan" id="kobo.1054.2">This function permits us to save a video of the game </span><span class="No-Break"><span class="koboSpan" id="kobo.1055.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1056.1">.ffmpeg</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1058.1">process_frame</span></strong><span class="koboSpan" id="kobo.1059.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">process_frame</span></strong><span class="koboSpan" id="kobo.1061.1"> function is used to preprocess frames from the game to make them more suitable for training an RL agent. </span><span class="koboSpan" id="kobo.1061.2">This function checks whether the frame is in the right format, converts it to grayscale and reduces the size, and normalizes it (simplifies the input). </span><span class="koboSpan" id="kobo.1061.3">This allows the agent to focus on the important details of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1062.1">visual information.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1063.1">CustomReward</span></strong><span class="koboSpan" id="kobo.1064.1">: This is a modification of the reward to encourage useful behaviors, track the current score, add rewards, check whether the agent finishes</span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.1065.1"> the level, and penalize it if the episode isn’t finished. </span><span class="koboSpan" id="kobo.1065.2">In this way, it tries to incentivize completing the level and making progress by </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">penalizing failures.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1067.1">CustomSkipFrame</span></strong><span class="koboSpan" id="kobo.1068.1">: This serves to speed up training by allowing skip frames, thus reducing computational computation (fewer </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">environment updates).</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">create_train_env</span></strong><span class="koboSpan" id="kobo.1071.1">: This function sets up a fully customized and optimized Super Mario environment, making it ready for training an RL agent with efficient preprocessing, reward shaping, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">frame skipping.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.1073.1">Defining the model</span></h3>
<p><span class="koboSpan" id="kobo.1074.1">In the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1075.1">model</span></strong><span class="koboSpan" id="kobo.1076.1"> script, we</span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.1077.1"> define the architecture for our algorithm. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1078.1">ActorCritic</span></strong><span class="koboSpan" id="kobo.1079.1"> is the class that governs the architecture, and as a neural network, it is based on PyTorch (in fact, we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1080.1">nn.Module</span></strong><span class="koboSpan" id="kobo.1081.1">, a classic neural network in PyTorch). </span><span class="koboSpan" id="kobo.1081.2">The class has two components representing </span><strong class="source-inline"><span class="koboSpan" id="kobo.1082.1">Actor</span></strong><span class="koboSpan" id="kobo.1083.1"> (responsible for choosing actions) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1084.1">Critic</span></strong><span class="koboSpan" id="kobo.1085.1">, which provides feedback. </span><span class="koboSpan" id="kobo.1085.2">You can see that we have a shared </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">feature extractor:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1087.1">
self.conv1 = nn.Conv2d(num_inputs, 32, 3, stride=2, padding=1)
self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)
self.lstm = nn.LSTMCell(32 * 6 * 6, 512)</span></pre> <p><span class="koboSpan" id="kobo.1088.1">Here, we have a convolutional network to extract spatial features from the game; this output is then reshaped into a 2D tensor, which is passed for an LSTM. </span><span class="koboSpan" id="kobo.1088.2">The LSTM has an update of the hidden state </span><strong class="source-inline"><span class="koboSpan" id="kobo.1089.1">hx</span></strong><span class="koboSpan" id="kobo.1090.1"> and the cell state </span><strong class="source-inline"><span class="koboSpan" id="kobo.1091.1">cx</span></strong><span class="koboSpan" id="kobo.1092.1"> (we described the LSTM in detail in </span><a href="B21257_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1093.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.1094.1">), thus managing </span><span class="No-Break"><span class="koboSpan" id="kobo.1095.1">episode memory.</span></span></p>
<p><span class="koboSpan" id="kobo.1096.1">After that, we initialize the </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">two components:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1098.1">
self.critic_linear = nn.Linear(512, 1)
self.actor_linear = nn.Linear(512, num_actions)</span></pre> <p><span class="koboSpan" id="kobo.1099.1">Using a single </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.1100.1">feature extractor allows us to save computation resources. </span><span class="koboSpan" id="kobo.1100.2">The two components produce two different outputs: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1101.1">actor_linear</span></strong><span class="koboSpan" id="kobo.1102.1"> produces the output for the actor, which is a vector of size </span><strong class="source-inline"><span class="koboSpan" id="kobo.1103.1">num_actions</span></strong><span class="koboSpan" id="kobo.1104.1">. </span><span class="koboSpan" id="kobo.1104.2">This represents the probability of taking each action. </span><span class="koboSpan" id="kobo.1104.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1105.1">critic_linear</span></strong><span class="koboSpan" id="kobo.1106.1"> component produces the output for the critic, which is a single scalar value. </span><span class="koboSpan" id="kobo.1106.2">This value represents the estimated value of the current state (the expected return from this state). </span><span class="koboSpan" id="kobo.1106.3">This separation allows us to make sure that the two layers have separate goals and different </span><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">learning signals.</span></span></p>
<p><span class="koboSpan" id="kobo.1108.1">Next, we will define different loss functions in order to allow for different learning. </span><span class="koboSpan" id="kobo.1108.2">As we can see, the two components produce </span><span class="No-Break"><span class="koboSpan" id="kobo.1109.1">different outputs:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1110.1">
def forward(self, x, hx, cx):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        hx, cx = self.lstm(x.view(x.size(0), -1), (hx, cx))
        return self.actor_linear(hx), self.critic_linear(hx), hx, cx</span></pre> <p><span class="koboSpan" id="kobo.1111.1">Since we want our process to be optimized for distributed learning, we use a custom version of Adam. </span><span class="koboSpan" id="kobo.1111.2">Adam is a classical optimizer that is used for updating the parameters of a neural network. </span><span class="koboSpan" id="kobo.1111.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1112.1">GlobalAdam</span></strong><span class="koboSpan" id="kobo.1113.1"> class is designed for distributed RL, where multiple processes or agents share the same optimizer. </span><span class="koboSpan" id="kobo.1113.2">The key idea is to make certain parts of the optimizer’s state shared across processes, allowing agents to coordinate their updates to the model parameters efficiently. </span><span class="koboSpan" id="kobo.1113.3">This is especially useful with actor-critic and especially the variant where there are many agents acting in the same environment. </span><span class="koboSpan" id="kobo.1113.4">The idea is that we play the game several times asynchronously and then conduct global </span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.1114.1">updates, reducing computation. </span><span class="koboSpan" id="kobo.1114.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1115.1">GlobalAdam</span></strong><span class="koboSpan" id="kobo.1116.1"> script is simply an adaptation of Adam to RL problems, allowing averaging and learning from </span><span class="No-Break"><span class="koboSpan" id="kobo.1117.1">different processes:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1118.1">
import torch
class GlobalAdam(torch.optim.Adam):
    def __init__(self, params, lr):
        super(GlobalAdam, self).__init__(params, lr=lr)
        for group in self.param_groups:
            for p in group['params']:
                state = self.state[p]
                state['step'] = 0
                state['exp_avg'] = torch.zeros_like(p.data)
                state['exp_avg_sq'] = torch.zeros_like(p.data)
                state['exp_avg'].share_memory_()
                state['exp_avg_sq'].share_memory_()</span></pre> <h3><span class="koboSpan" id="kobo.1119.1">Training the model</span></h3>
<p><span class="koboSpan" id="kobo.1120.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1121.1">train</span></strong><span class="koboSpan" id="kobo.1122.1"> script </span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.1123.1">then allows us to train the model asynchronously with different processes. </span><span class="koboSpan" id="kobo.1123.2">The script allows us to provide several parameters (default parameters are already entered). </span><span class="koboSpan" id="kobo.1123.3">For example, we can decide the level of the game (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1124.1">--world</span></strong><span class="koboSpan" id="kobo.1125.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1126.1">--stage</span></strong><span class="koboSpan" id="kobo.1127.1">), the type of action (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1128.1">--action_type</span></strong><span class="koboSpan" id="kobo.1129.1">), the learning rate for the optimizer (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1130.1">--lr</span></strong><span class="koboSpan" id="kobo.1131.1">), hyperparameters specific to the algorithm and RL (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1132.1">--gamma</span></strong><span class="koboSpan" id="kobo.1133.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1134.1">--tau</span></strong><span class="koboSpan" id="kobo.1135.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1136.1">--beta</span></strong><span class="koboSpan" id="kobo.1137.1">), or related to the process and its parallelization (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1138.1">--num_processes</span></strong><span class="koboSpan" id="kobo.1139.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1140.1">--num_local_steps</span></strong><span class="koboSpan" id="kobo.1141.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1142.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1143.1">--num_global_steps</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1144.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.1145.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1146.1">train</span></strong><span class="koboSpan" id="kobo.1147.1"> function allows us to initialize the training environment, initialize the policy, and use the GPU. </span><span class="koboSpan" id="kobo.1147.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1148.1">global_model.share_memory()</span></strong><span class="koboSpan" id="kobo.1149.1"> method allows the global model’s parameters to be accessible to all processes, enabling parallel updates. </span><span class="koboSpan" id="kobo.1149.2">You can see we use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1150.1">GlobalAdam</span></strong><span class="koboSpan" id="kobo.1151.1"> to update the global model’s parameters. </span><span class="koboSpan" id="kobo.1151.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1152.1">torch.multiprocessing</span></strong><span class="koboSpan" id="kobo.1153.1"> wrapper (is a wrapper to the multiprocessing module) allows us to create multiple processes that operate asynchronously. </span><span class="koboSpan" id="kobo.1153.2">This script then defines the</span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.1154.1"> training of our model using multiple parallel processes. </span><span class="koboSpan" id="kobo.1154.2">At the same time, the script allows easy configuration </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">and customization:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1156.1">
def train(opt):
    torch.manual_seed(123)
    if os.path.isdir(opt.log_path):
        shutil.rmtree(opt.log_path)
    os.makedirs(opt.log_path)
    if not os.path.isdir(opt.saved_path):
        os.makedirs(opt.saved_path)
    mp = _mp.get_context("spawn")
    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)
    global_model = ActorCritic(num_states, num_actions)
    if opt.use_gpu:
        global_model.cuda()
    global_model.share_memory()
    if opt.load_from_previous_stage:
        if opt.stage == 1:
            previous_world = opt.world - 1
            previous_stage = 4
        else:
            previous_world = opt.world
            previous_stage = opt.stage - 1
        file_ = "{}/A3CSuperMarioBros{}_{}".format(opt.saved_path, previous_world, previous_stage)
        if os.path.isfile(file_):
            global_model.load_state_dict(torch.load(file_))
    optimizer = GlobalAdam(global_model.parameters(), lr=opt.lr)
    processes = []
    for index in range(opt.num_processes):
        if index == 0:
            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer, True))
        else:
            process = mp.Process(target=local_train, args=(index, opt, global_model, optimizer))
        process.start()
        processes.append(process)
    process = mp.Process(target=local_test, args=(opt.num_processes, opt, global_model))
    process.start()
    processes.append(process)
    for process in processes:
        process.join()</span></pre> <h3><span class="koboSpan" id="kobo.1157.1">Testing the system</span></h3>
<p><span class="koboSpan" id="kobo.1158.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1159.1">test</span></strong><span class="koboSpan" id="kobo.1160.1"> script allows</span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.1161.1"> customization such as deciding on some parameters, such as level of play, actions, and so on. </span><span class="koboSpan" id="kobo.1161.2">Once we have trained our model, we can then load it, play the game, and register the agent playing. </span><span class="koboSpan" id="kobo.1161.3">The model then plays with its policy, without optimization in this script, and thus allows us to observe the </span><span class="No-Break"><span class="koboSpan" id="kobo.1162.1">agent’s performance.</span></span></p>
<h3><span class="koboSpan" id="kobo.1163.1">Connecting all the components</span></h3>
<p><span class="koboSpan" id="kobo.1164.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1165.1">process</span></strong><span class="koboSpan" id="kobo.1166.1"> script </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.1167.1">connects all the scripts we have seen so far into one system. </span><span class="koboSpan" id="kobo.1167.2">This script uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1168.1">create_train_env</span></strong><span class="koboSpan" id="kobo.1169.1"> function from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1170.1">env</span></strong><span class="koboSpan" id="kobo.1171.1"> module to set up the Super Mario Bros game environment. </span><span class="koboSpan" id="kobo.1171.2">This is the environment where our agent interacts and learns. </span><span class="koboSpan" id="kobo.1171.3">The script also initializes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1172.1">ActorCritic</span></strong><span class="koboSpan" id="kobo.1173.1"> model (both actor and critic) and uses this model to make decisions and evaluate game states. </span><span class="koboSpan" id="kobo.1173.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1174.1">local_train</span></strong><span class="koboSpan" id="kobo.1175.1"> function is responsible for training and requires the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1176.1">GlobalAdam</span></strong><span class="koboSpan" id="kobo.1177.1"> optimizer. </span><span class="koboSpan" id="kobo.1177.2">This script is also used to evaluate trained model performance, so it uses elements we defined in the test script. </span><span class="koboSpan" id="kobo.1177.3">This script, then, is the central piece that allows us to have a fully functional RL system. </span><span class="koboSpan" id="kobo.1177.4">It orchestrates the environment, model, and training algorithm, making everything work together to train an agent to play Super </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">Mario Bros.</span></span></p>
<p><span class="koboSpan" id="kobo.1179.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1180.1">local_train</span></strong><span class="koboSpan" id="kobo.1181.1"> function enables the agent to train in parallel with other processes while updating a shared global model. </span><span class="koboSpan" id="kobo.1181.2">This function establishes a seed for reproducibility, so we can reproduce the results. </span><span class="koboSpan" id="kobo.1181.3">After that, we initialize the environment (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1182.1">create_train_env</span></strong><span class="koboSpan" id="kobo.1183.1">) and model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1184.1">ActorCritic</span></strong><span class="koboSpan" id="kobo.1185.1">); if there is a GPU, we move the model to the GPU and </span><span class="No-Break"><span class="koboSpan" id="kobo.1186.1">initialize TensorBoard:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1187.1">
def local_train(index, opt, global_model, optimizer, save=False):
    torch.manual_seed(123 + index)
    if save:
        start_time = timeit.default_timer()
    writer = SummaryWriter(opt.log_path)
    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)
    local_model = ActorCritic(num_states, num_actions)
    if opt.use_gpu:
        local_model.cuda()</span></pre> <p><span class="koboSpan" id="kobo.1188.1">At this point, we begin the training loop where each iteration represents an episode of gameplay. </span><span class="koboSpan" id="kobo.1188.2">The</span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.1189.1"> local parameters are synchronized with the global parameters, and at the end of each episode, the hidden and cell states of the LSTM </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">are reset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1191.1">
local_model.train()
    state = torch.from_numpy(env.reset())
    if opt.use_gpu:
        state = state.cuda()
    done = True
    curr_step = 0
    curr_episode = 0
    while True:
        if save:
            if curr_episode % opt.save_interval == 0 and curr_episode &gt; 0:
                torch.save(global_model.state_dict(),
                           "{}/a3c_super_mario_bros_{}_{}".format(opt.saved_path, opt.world, opt.stage))
            print("Process {}. </span><span class="koboSpan" id="kobo.1191.2">Episode {}".format(index, curr_episode))
        curr_episode += 1
        local_model.load_state_dict(global_model.state_dict())
        if done:
            h_0 = torch.zeros((1, 512), dtype=torch.float)
            c_0 = torch.zeros((1, 512), dtype=torch.float)
        else:
            h_0 = h_0.detach()
            c_0 = c_0.detach()
        if opt.use_gpu:
            h_0 = h_0.cuda()
            c_0 = c_0.cuda()</span></pre> <p><span class="koboSpan" id="kobo.1192.1">At this point, we</span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.1193.1"> begin to collect experiences for a number of steps (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1194.1">opt.num_local_steps</span></strong><span class="koboSpan" id="kobo.1195.1">). </span><span class="koboSpan" id="kobo.1195.2">Then, for a state, the model (the local model) produces a set of probabilities, and from these probabilities, we sample an action. </span><span class="koboSpan" id="kobo.1195.3">Having chosen an action, we interact with the environment, so we get a reward and a new state. </span><span class="koboSpan" id="kobo.1195.4">For each of these steps, we record the following: whether the episode has ended, the log probability of the action, the value estimate, the reward, and the entropy of the policy. </span><span class="koboSpan" id="kobo.1195.5">If the episode ends, the state is reset, and the hidden states are detached to prevent </span><span class="No-Break"><span class="koboSpan" id="kobo.1196.1">gradient backpropagation:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1197.1">
for _ in range(opt.num_local_steps):
            curr_step += 1
            logits, value, h_0, c_0 = local_model(state, h_0, c_0)
            policy = F.softmax(logits, dim=1)
            log_policy = F.log_softmax(logits, dim=1)
            entropy = -(policy * log_policy).sum(1, keepdim=True)
            m = Categorical(policy)
            action = m.sample().item()
            state, reward, done, _ = env.step(action)
            state = torch.from_numpy(state)
            if opt.use_gpu:
                state = state.cuda()
            if curr_step &gt; opt.num_global_steps:
                done = True
            if done:
                curr_step = 0
                state = torch.from_numpy(env.reset())
                if opt.use_gpu:
                    state = state.cuda()
            values.append(value)
            log_policies.append(log_policy[0, action])
            rewards.append(reward)
            entropies.append(entropy)
            if done:
                break
        R = torch.zeros((1, 1), dtype=torch.float)
        if opt.use_gpu:
            R = R.cuda()
        if not done:
            _, R, _, _ = local_model(state, h_0, c_0)</span></pre> <p><span class="koboSpan" id="kobo.1198.1">Now, it is time to calculate the loss and conduct backpropagation. </span><span class="koboSpan" id="kobo.1198.2">Here, we use </span><strong class="bold"><span class="koboSpan" id="kobo.1199.1">generalized advantage estimation</span></strong><span class="koboSpan" id="kobo.1200.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1201.1">GAE</span></strong><span class="koboSpan" id="kobo.1202.1">), to balance bias and variance and make the </span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.1203.1">training therefore more efficient. </span><span class="koboSpan" id="kobo.1203.2">Simply, the advantage function </span><em class="italic"><span class="koboSpan" id="kobo.1204.1">A(s,a)</span></em><span class="koboSpan" id="kobo.1205.1"> measures the goodness of an action </span><em class="italic"><span class="koboSpan" id="kobo.1206.1">a</span></em><span class="koboSpan" id="kobo.1207.1"> relative to the average action in a given state </span><em class="italic"><span class="koboSpan" id="kobo.1208.1">s</span></em><span class="koboSpan" id="kobo.1209.1">. </span><span class="koboSpan" id="kobo.1209.2">In the next script, GAE is used to compute the advantage values that drive the actor’s policy updates. </span><span class="koboSpan" id="kobo.1209.3">We </span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.1210.1">use GAE to update the policy in the actor loss in order to maximize the expected return but keep the variance low. </span><span class="koboSpan" id="kobo.1210.2">In other words, we want to keep the training more stable. </span><span class="koboSpan" id="kobo.1210.3">By adding GAE, the training process becomes more efficient and less susceptible to noise from high variance returns or inaccuracies from biased </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">value estimates:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1212.1">
        gae = torch.zeros((1, 1), dtype=torch.float)
        if opt.use_gpu:
            gae = gae.cuda()
        actor_loss = 0
        critic_loss = 0
        entropy_loss = 0
        next_value = R
        for value, log_policy, reward, entropy in list(zip(values, log_policies, rewards, entropies))[::-1]:
            gae = gae * opt.gamma * opt.tau
            gae = gae + reward + opt.gamma * next_value.detach() - value.detach()
            next_value = value
            actor_loss = actor_loss + log_policy * gae
            R = R * opt.gamma + reward
            critic_loss = critic_loss + (R - value) ** 2 / 2
            entropy_loss = entropy_loss + entropy
        total_loss = -actor_loss + critic_loss - opt.beta * entropy_loss
        writer.add_scalar("Train_{}/Loss".format(index), total_loss, curr_episode)
        optimizer.zero_grad()
        total_loss.backward()
        for local_param, global_param in zip(local_model.parameters(), global_model.parameters()):
            if global_param.grad is not None:
                break
            global_param._grad = local_param.grad
        optimizer.step()
        if curr_episode == int(opt.num_global_steps / opt.num_local_steps):
            print("Training process {} terminated".format(index))
            if save:
                end_time = timeit.default_timer()
                print('The code runs for %.2f s ' % (end_time - start_time))
            return</span></pre> <p><span class="koboSpan" id="kobo.1213.1">Note that</span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.1214.1"> we have three separate losses. </span><span class="koboSpan" id="kobo.1214.2">The first is the actor loss, which encourages actions that lead to higher rewards. </span><span class="koboSpan" id="kobo.1214.3">The critic loss penalizes errors in the value estimation, and the entropy loss encourages exploration by penalizing overly confident action distributions (in other penalizing strategies that are too greedy). </span><span class="koboSpan" id="kobo.1214.4">Once we have computed the total loss, we perform the backpropagation as in any neural network. </span><span class="koboSpan" id="kobo.1214.5">Right now, we have performed local training, so we</span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.1215.1"> use the gradients of the local model to conduct the global model update as well. </span><span class="koboSpan" id="kobo.1215.2">Every certain time interval, we save the model and send the loss logs to TensorBoard. </span><span class="koboSpan" id="kobo.1215.3">The process ends when we have reached the total number of </span><span class="No-Break"><span class="koboSpan" id="kobo.1216.1">global steps.</span></span></p>
<p><span class="koboSpan" id="kobo.1217.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1218.1">local_test</span></strong><span class="koboSpan" id="kobo.1219.1"> function allows us to conduct the evaluation of our trained model. </span><span class="koboSpan" id="kobo.1219.2">It runs as a separate process to test how well the agent performs using the </span><span class="No-Break"><span class="koboSpan" id="kobo.1220.1">learned policy:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1221.1">
def local_test(index, opt, global_model):
    torch.manual_seed(123 + index)
    env, num_states, num_actions = create_train_env(opt.world, opt.stage, opt.action_type)
    local_model = ActorCritic(num_states, num_actions)
    local_model.eval()
    state = torch.from_numpy(env.reset())
    done = True
    curr_step = 0
    actions = deque(maxlen=opt.max_actions)
    while True:
        curr_step += 1
        if done:
            local_model.load_state_dict(global_model.state_dict())
        with torch.no_grad():
            if done:
                h_0 = torch.zeros((1, 512), dtype=torch.float)
                c_0 = torch.zeros((1, 512), dtype=torch.float)
            else:
                h_0 = h_0.detach()
                c_0 = c_0.detach()
        logits, value, h_0, c_0 = local_model(state, h_0, c_0)
        policy = F.softmax(logits, dim=1)
        action = torch.argmax(policy).item()
        state, reward, done, _ = env.step(action)
        env.render()
        actions.append(action)
        if curr_step &gt; opt.num_global_steps or actions.count(actions[0]) == actions.maxlen:
            done = True
        if done:
            curr_step = 0
            actions.clear()
            state = env.reset()
        state = torch.from_numpy(state)</span></pre> <p><span class="koboSpan" id="kobo.1222.1">Again, we </span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.1223.1">conduct setup and initialization and load the local </span><strong class="source-inline"><span class="koboSpan" id="kobo.1224.1">ActorCritic</span></strong><span class="koboSpan" id="kobo.1225.1"> model in evaluation mode (in inference mode, practically, the model does not get updates during this process). </span><span class="koboSpan" id="kobo.1225.2">At this point, we start the loop, where we load the last weights from the global model. </span><span class="koboSpan" id="kobo.1225.3">For a state, we compute the probabilities for each action and choose the action with the highest probability. </span><span class="koboSpan" id="kobo.1225.4">Note how, during training, we conducted sampling of the action; in evaluation mode, instead, we chose the action with a greedy policy. </span><span class="koboSpan" id="kobo.1225.5">We interact with the environment and render the game, conduct action tracking, and check whether the agent gets stuck or repeats the same action indefinitely. </span><span class="koboSpan" id="kobo.1225.6">If the agent exceeds the maximum number of steps or gets stuck, the episode ends and we reset the state. </span><span class="koboSpan" id="kobo.1225.7">This function</span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.1226.1"> evaluates the performance of the trained agent, rendering the gameplay so that users can observe how well the agent has learned to play Super Mario Bros. </span><span class="koboSpan" id="kobo.1226.2">It ensures the policy is effective and provides </span><span class="No-Break"><span class="koboSpan" id="kobo.1227.1">visual feedback.</span></span></p>
<p><span class="koboSpan" id="kobo.1228.1">Running the scripts, we can see that the training runs </span><span class="No-Break"><span class="koboSpan" id="kobo.1229.1">in parallel:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer241">
<span class="koboSpan" id="kobo.1230.1"><img alt="Figure 8.29 – Screenshot of the script run" src="image/B21257_08_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1231.1">Figure 8.29 – Screenshot of the script run</span></p>
<p><span class="koboSpan" id="kobo.1232.1">You can check out the video </span><span class="No-Break"><span class="koboSpan" id="kobo.1233.1">here: </span></span><a href="https://www.youtube.com/watch?v=YWx-hnvqjr8"><span class="No-Break"><span class="koboSpan" id="kobo.1234.1">https://www.youtube.com/watch?v=YWx-hnvqjr8</span></span></a></p>
<p><span class="koboSpan" id="kobo.1235.1">To summarize, we used several scripts to implement a variant of the action-critic algorithm (the A3C method). </span><span class="koboSpan" id="kobo.1235.2">This method involves training multiple agents in parallel to explore the environment, collect experiences, and update a shared global model asynchronously. </span><span class="koboSpan" id="kobo.1235.3">In other words, we use a variant that allows us to speed up the training and learn a model that is more robust because it retrieves different experiences from different agents. </span><span class="koboSpan" id="kobo.1235.4">For cleaner organization, we divided the process into several scripts that are then linked into a single script (process script). </span><span class="koboSpan" id="kobo.1235.5">We defined our neural network with a common extractor for the two components, so we can save some computations. </span><span class="koboSpan" id="kobo.1235.6">In addition, we used an LSTM to be able to handle the temporal dependencies there are between one state and another. </span><span class="koboSpan" id="kobo.1235.7">We had to modify our optimizer because we needed shared memory to be able to handle several processes to update a global model. </span><span class="koboSpan" id="kobo.1235.8">Asynchronous training indeed has higher complexities, where each agent needs to access and update the global model. </span><span class="koboSpan" id="kobo.1235.9">After that, we defined how to train our model by collecting some experience. </span><span class="koboSpan" id="kobo.1235.10">Having collected the experience, we conducted an update of the model </span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.1236.1">weights, calculating the loss and performing backpropagation. </span><span class="koboSpan" id="kobo.1236.2">Periodically, we synchronized the global and local model, conducting an update of the global model. </span><span class="koboSpan" id="kobo.1236.3">After that, we defined how to evaluate our agent using the parameters of the global model. </span><span class="koboSpan" id="kobo.1236.4">The agent uses the learned policy to play </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">the game.</span></span></p>
<p><span class="koboSpan" id="kobo.1238.1">These scripts allow efficient parallel training because of the A3C method. </span><span class="koboSpan" id="kobo.1238.2">In fact, we can use several agents in parallel that explore the environment, gather experience, and then lead to a global model update. </span><span class="koboSpan" id="kobo.1238.3">Using a parallel system causes agents to explore different parts of the environment, leading to more diverse experiences and thus a more generalized policy. </span><span class="koboSpan" id="kobo.1238.4">In general, this is favorable because different strategies may be needed in video games. </span><span class="koboSpan" id="kobo.1238.5">In the same vein, we added entropy loss to encourage exploration and prevent the agent from being stuck in a suboptimal strategy. </span><span class="koboSpan" id="kobo.1238.6">The script is designed for efficient use of resources, to reduce computation, and to have fast training (we did not add an experience replay buffer to save memory and thus consume less RAM). </span><span class="koboSpan" id="kobo.1238.7">The use of a global model ensures that knowledge learned by one agent is immediately available to all agents; this usually promotes </span><span class="No-Break"><span class="koboSpan" id="kobo.1239.1">rapid convergence.</span></span></p>
<p><span class="koboSpan" id="kobo.1240.1">The choice of an on-policy learning method such as A3C can result in high variance in policy updates. </span><span class="koboSpan" id="kobo.1240.2">This variance can then be amplified by the asynchronous nature, which may make it difficult to get consistent results across runs. </span><span class="koboSpan" id="kobo.1240.3">In fact, the asynchronous approach introduces non-determinism, meaning that the results can vary significantly between runs. </span><span class="koboSpan" id="kobo.1240.4">This makes the process less predictable and complicates the choice of hyperparameters (which is why we have provided default parameters, although it is possible to experiment with them). </span><span class="koboSpan" id="kobo.1240.5">While we have tried to optimize the resources consumed by this script, the whole process remains resource intensive (like RL </span><span class="No-Break"><span class="koboSpan" id="kobo.1241.1">in general).</span></span></p>
<p><span class="koboSpan" id="kobo.1242.1">A3C primarily relies on CPU-based parallelism; however, incorporating GPU-friendly methods could significantly enhance training efficiency. </span><span class="koboSpan" id="kobo.1242.2">Algorithms such as PPO can leverage GPUs to optimize the training process. </span><span class="koboSpan" id="kobo.1242.3">Effective use of GPUs enables more efficient batch processing, allowing for the accumulation of experiences and bulk updates to the model. </span><span class="koboSpan" id="kobo.1242.4">For readers interested in exploring GPU-based optimization, here are a few </span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">potential ideas:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1244.1">Test different hyperparameters and vary their values to better understand their impact. </span><span class="koboSpan" id="kobo.1244.2">In the script, you can easily set and change hyperparameters. </span><span class="koboSpan" id="kobo.1244.3">We invite you to test lambda (</span><em class="italic"><span class="koboSpan" id="kobo.1245.1">λ</span></em><span class="koboSpan" id="kobo.1246.1">) to find a better balance between bias </span><span class="No-Break"><span class="koboSpan" id="kobo.1247.1">and variance.</span></span></li>
<li><span class="koboSpan" id="kobo.1248.1">Try PPO. </span><span class="koboSpan" id="kobo.1248.2">PPO is a </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.1249.1">popular alternative to A3C that exploits multiple epochs of mini-batch updates. </span><span class="koboSpan" id="kobo.1249.2">As we have seen, it is an algorithm that promotes stability and works well in many cases. </span><span class="koboSpan" id="kobo.1249.3">It also does not require many hyperparameters and the default ones usually </span><span class="No-Break"><span class="koboSpan" id="kobo.1250.1">work well.</span></span></li>
<li><span class="koboSpan" id="kobo.1251.1">Adopt synchronous A2C as it is a simpler, synchronous version of A3C. </span><span class="koboSpan" id="kobo.1251.2">This approach collects experiences in parallel and uses batches for updating. </span><span class="koboSpan" id="kobo.1251.3">It is usually slower but easier </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">to debug.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1253.1">The model shown in this project can be applied to several other video games, showing how an RL algorithm can solve </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">real tasks.</span></span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.1255.1">LLM interactions with RL models</span></h1>
<p><span class="koboSpan" id="kobo.1256.1">RL algorithms </span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.1257.1">have been instrumental for agents that can navigate complex environments, optimize strategies, and make decisions, with successes in areas such as robotics and video games. </span><span class="koboSpan" id="kobo.1257.2">LLMs, on the other hand, have had a strong impact</span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.1258.1"> on </span><strong class="bold"><span class="koboSpan" id="kobo.1259.1">natural language processing</span></strong><span class="koboSpan" id="kobo.1260.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1261.1">NLP</span></strong><span class="koboSpan" id="kobo.1262.1">), enabling machines to understand human language and instructions. </span><span class="koboSpan" id="kobo.1262.2">Although potential synergies can be imagined, so far these two technologies have evolved in parallel. </span><span class="koboSpan" id="kobo.1262.3">In recent years, though, with the heightened interest in LLMs, the two fields have increasingly intersected. </span><span class="koboSpan" id="kobo.1262.4">In this section, we will discuss the interaction between RL </span><span class="No-Break"><span class="koboSpan" id="kobo.1263.1">and LLMs.</span></span></p>
<p><span class="koboSpan" id="kobo.1264.1">We can have three cases </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">of interaction:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1266.1">RL enhancing an LLM</span></strong><span class="koboSpan" id="kobo.1267.1">: Using RL to enhance the performance of an LLM in one or more </span><span class="No-Break"><span class="koboSpan" id="kobo.1268.1">NLP tasks</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1269.1">LLMs enhancing RL</span></strong><span class="koboSpan" id="kobo.1270.1">: Using LLMs to train an RL algorithm that performs a task that is not </span><span class="No-Break"><span class="koboSpan" id="kobo.1271.1">necessarily NLP</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1272.1">RL and LLMs</span></strong><span class="koboSpan" id="kobo.1273.1">: Combining RL models and LLMs to plan a skill set, without either system being used to train or conduct fine-tuning of </span><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">the other</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1275.1">Let’s discuss these </span><span class="No-Break"><span class="koboSpan" id="kobo.1276.1">in detail.</span></span></p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.1277.1">RL-enhanced LLMs</span></h2>
<p><span class="koboSpan" id="kobo.1278.1">We have already </span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.1279.1">discussed alignment and prompt engineering, in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1280.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1281.1">. </span><span class="koboSpan" id="kobo.1281.2">RL is, then, used for fine-tuning, prompt engineering, and </span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.1282.1">the alignment of LLMs. </span><span class="koboSpan" id="kobo.1282.2">As mentioned in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1283.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1284.1">, LLMs are trained to predict the next word in a sequence, leading to a mismatch between the LLM’s training objective and human values. </span><span class="koboSpan" id="kobo.1284.2">This can lead LLMs to produce text with bias or other unsafe content, and likewise to be suboptimal at following instructions. </span><span class="koboSpan" id="kobo.1284.3">Alignment serves to realign the model to human values or to make an LLM more effective for safer deployment. </span><span class="koboSpan" id="kobo.1284.4">One of the most widely used techniques is </span><strong class="bold"><span class="koboSpan" id="kobo.1285.1">reinforcement learning from human feedback</span></strong><span class="koboSpan" id="kobo.1286.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1287.1">RLHF</span></strong><span class="koboSpan" id="kobo.1288.1">), where the reward is inferred from </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.1289.1">human preferences and then used to train the LLM. </span><span class="koboSpan" id="kobo.1289.2">This process follows three steps: collect human feedback data, train a reward model on this data, and conduct fine-tuning of the LLM with RL. </span><span class="koboSpan" id="kobo.1289.3">Generally, the most popular choice of RL algorithm is PPO or derivative methods. </span><span class="koboSpan" id="kobo.1289.4">In fact, we do not want our aligned model to be significantly different from the original model, which </span><span class="No-Break"><span class="koboSpan" id="kobo.1290.1">PPO guarantees.</span></span></p>
<p><span class="koboSpan" id="kobo.1291.1">Interaction with LLMs is through the prompt, and the prompt should condense all the instructions for the task we want LLM to accomplish. </span><span class="koboSpan" id="kobo.1291.2">Some work has focused on using RL to design prompts. </span><span class="koboSpan" id="kobo.1291.3">Prompt optimization can be represented as an RL problem with the goal of incorporating human knowledge and thus drawing interpretable and adaptable prompts. </span><span class="koboSpan" id="kobo.1291.4">The agent is used to construct prompts that are query-dependent and optimized. </span><span class="koboSpan" id="kobo.1291.5">One can also train a policy network to generate desired prompts, with the advantage that the prompts are generally transferable across LLMs. </span><span class="koboSpan" id="kobo.1291.6">An intriguing aspect of this approach is that some of these optimized prompts are grammatically “gibberish,” indicating that high-quality prompts for a task need not follow human </span><span class="No-Break"><span class="koboSpan" id="kobo.1292.1">language patterns.</span></span></p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.1293.1">LLM-enhanced RL</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.1294.1">LLM-enhanced RL</span></strong><span class="koboSpan" id="kobo.1295.1"> refers</span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.1296.1"> to methods that</span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.1297.1"> use multi-modal information processing, generation, reasoning, or other high-level cognitive capabilities of pre-trained LLMs in assisting an RL agent. </span><span class="koboSpan" id="kobo.1297.2">In other words, the difference from traditional RL is the use of an LLM and the exploitation of its knowledge and capabilities in some way. </span><span class="koboSpan" id="kobo.1297.3">The addition of an LLM in some form has a twofold advantage: first, an LLM possesses reasoning and planning skills that allow for improved learning, and second, it has a greater ability to generalize. </span><span class="koboSpan" id="kobo.1297.4">In addition, an LLM has extensive knowledge gained during the pre-training step and that can be transferred across domains and tasks, thus allowing better adaptation to environments that have not been seen. </span><span class="koboSpan" id="kobo.1297.5">Models that are pre-trained generally cannot expand their knowledge or acquire new capabilities (continual learning is an open challenge of deep learning), so using a model trained with huge amounts of knowledge can help with this aspect (LLMs are generalists and have huge amounts of information for different domains </span><span class="No-Break"><span class="koboSpan" id="kobo.1298.1">in memory).</span></span></p>
<p><span class="koboSpan" id="kobo.1299.1">An LLM can then be inserted into the classic framework of an RL system (an agent interacting with and receiving feedback from an environment) at more than one point. </span><span class="koboSpan" id="kobo.1299.2">An LLM can then be integrated to extract information, reprocess state, redesign rewards, make decisions, select actions, interpret policies, analyze world similarity, </span><span class="No-Break"><span class="koboSpan" id="kobo.1300.1">and more.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer242">
<span class="koboSpan" id="kobo.1301.1"><img alt="Figure 8.30 – Framework of LLM-enhanced RL in classical agent-environment interactions (https://arxiv.org/pdf/2404.00282)" src="image/B21257_08_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1302.1">Figure 8.30 – Framework of LLM-enhanced RL in classical agent-environment interactions (</span><a href="https://arxiv.org/pdf/2404.00282"><span class="koboSpan" id="kobo.1303.1">https://arxiv.org/pdf/2404.00282</span></a><span class="koboSpan" id="kobo.1304.1">)</span></p>
<p><span class="koboSpan" id="kobo.1305.1">Thus, an LLM can be used inside the system as an information processor, reward designer, decision-maker, </span><span class="No-Break"><span class="koboSpan" id="kobo.1306.1">and generator.</span></span></p>
<h3><span class="koboSpan" id="kobo.1307.1">Information processor</span></h3>
<p><span class="koboSpan" id="kobo.1308.1">When a </span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.1309.1">task requires textual information or visual features, it can be complex for an agent to understand the information and optimize the policy at the same time. </span><span class="koboSpan" id="kobo.1309.2">As we saw earlier, a convolutional neural network can be used to process images for a model to interact with a video game or board game. </span><span class="koboSpan" id="kobo.1309.3">In the case of a chatbot, we can then use a model that understands language. </span><span class="koboSpan" id="kobo.1309.4">Alternatively, instead of using a model directly on the language, we can use an LLM to extract features that allow the agent to learn more quickly. </span><span class="koboSpan" id="kobo.1309.5">LLMs can be good feature extractors, thereby reducing the dimensionality and complexity of the information. </span><span class="koboSpan" id="kobo.1309.6">Or LLMs can translate natural language into a specific formal language understandable to an agent. </span><span class="koboSpan" id="kobo.1309.7">For example, in the case of a robot, the natural language of different users will be different and not homogeneous, making it difficult for the agent to learn. </span><span class="koboSpan" id="kobo.1309.8">An LLM can transform instructions into a standard, formal language that allows easier learning for </span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1">the agent.</span></span></p>
<p><span class="koboSpan" id="kobo.1311.1">A wide pre-trained model learns a representation of the data that can then be used for subsequent applications. </span><span class="koboSpan" id="kobo.1311.2">A model, then, can be used to extract a data representation that we can use to train an agent. </span><span class="koboSpan" id="kobo.1311.3">An LLM can be used frozen (that is, without the need for further training) to extract a compressed representation of the history of the environment. </span><span class="koboSpan" id="kobo.1311.4">Some studies use an LLM to summarize past visual observations that are provided to the agent, so we can provide a memory to the agent. </span><span class="koboSpan" id="kobo.1311.5">Using a frozen model is clearly the simplest alternative, but when agents are deployed in the real world, performance can degrade rapidly due to real-world variations versus the training environment. </span><span class="koboSpan" id="kobo.1311.6">Therefore, we can conduct fine-tuning of both the agent and the LLM. </span><span class="koboSpan" id="kobo.1311.7">The use of a feature extractor (an LLM or other large model) makes it easier for the agent to learn since these features are more invariant to changes in the environment (changes in brightness, color, etc...), but on the other hand, they have an additional </span><span class="No-Break"><span class="koboSpan" id="kobo.1312.1">computational cost.</span></span></p>
<p><span class="koboSpan" id="kobo.1313.1">The capabilities of LLMs can be used to make the task clearer. </span><span class="koboSpan" id="kobo.1313.2">For example, instructions in natural language can be adapted by an LLM into a set of instructions that are clearer to the agent (for example, when playing a video game, a textual description of the task could be transformed into a set of instructions on how to move the character). </span><span class="koboSpan" id="kobo.1313.3">An LLM can also be used to translate an agent’s surroundings into usable information. </span><span class="koboSpan" id="kobo.1313.4">These approaches are particularly promising but are currently limited </span><span class="No-Break"><span class="koboSpan" id="kobo.1314.1">in scope.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<span class="koboSpan" id="kobo.1315.1"><img alt="Figure 8.31 – LLM as an information processor (https://arxiv.org/pdf/2404.00282)" src="image/B21257_08_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1316.1">Figure 8.31 – LLM as an information processor (</span><a href="https://arxiv.org/pdf/2404.00282"><span class="koboSpan" id="kobo.1317.1">https://arxiv.org/pdf/2404.00282</span></a><span class="koboSpan" id="kobo.1318.1">)</span></p>
<h3><span class="koboSpan" id="kobo.1319.1">Reward designer</span></h3>
<p><span class="koboSpan" id="kobo.1320.1">When </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.1321.1">knowledge of the problem is available, or when the reward can be defined by a clear and deterministic function (such as a game score or a win/loss condition), designing the reward function is straightforward. </span><span class="koboSpan" id="kobo.1321.2">For example, in Atari games (or other games), it is easy to draw a reward function (e.g., victory represents a positive signal and defeat a negative signal). </span><span class="koboSpan" id="kobo.1321.3">There are many applications where this is not possible because the tasks are long and complex, the rewards are scattered, and so on. </span><span class="koboSpan" id="kobo.1321.4">In such cases, the knowledge inherent in an LLM (the knowledge gained during pre-training, coding abilities, and reasoning skills) could be used to generate the reward. </span><span class="koboSpan" id="kobo.1321.5">It can be used indirectly (an implicit reward model) or directly (an explicit reward model). </span><span class="koboSpan" id="kobo.1321.6">For example, a user can define expected behavior in a prompt, and an LLM can evaluate the agent’s behavior during training, providing a </span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.1322.1">reward and a penalty. </span><span class="koboSpan" id="kobo.1322.2">So, you can use direct feedback from the LLM, or an LLM can generate the code for a reward function. </span><span class="koboSpan" id="kobo.1322.3">In the second approach, the function can be modified by the LLM during training (for example, after the agent has acquired some skills, making it harder to get </span><span class="No-Break"><span class="koboSpan" id="kobo.1323.1">a reward).</span></span></p>
<p><span class="koboSpan" id="kobo.1324.1">An LLM can be an implicit reward model that provides a reward (or auxiliary reward) based on the task description. </span><span class="koboSpan" id="kobo.1324.2">One technique for this is direct prompting, in which instructions are given to the LLM to evaluate the agent’s behavior or decide on a reward. </span><span class="koboSpan" id="kobo.1324.3">These approaches can mimic human feedback to evaluate an agent’s behavior in real time. </span><span class="koboSpan" id="kobo.1324.4">Alternatively, an alignment score can be used, for example, between the outcome of an action and the goal (in other words, evaluating the similarity between the expected outcome and reality). </span><span class="koboSpan" id="kobo.1324.5">In some approaches, one uses the contrastive alignment between language instructions and the image observations of the agent, thus exploiting models that are multimodal. </span><span class="koboSpan" id="kobo.1324.6">Obviously, the process of aligning human intentions and LLM-reward generation is not easy. </span><span class="koboSpan" id="kobo.1324.7">There can be ambiguities, and the system does not always work with low-quality instruction, but it seems a </span><span class="No-Break"><span class="koboSpan" id="kobo.1325.1">promising avenue.</span></span></p>
<p><span class="koboSpan" id="kobo.1326.1">An explicit reward model exploits the ability of an LLM to generate code, thus generating a function (making the decision-making and reward-generation process by the LLM more transparent). </span><span class="koboSpan" id="kobo.1326.2">This allows functions for subgoals to be generated automatically (e.g., having a robot learn low-level tasks using high-level instructions that are translated into a reward function by the LLM). </span><span class="koboSpan" id="kobo.1326.3">The main limitation of this approach is the common-sense reasoning limitation of LLMs. </span><span class="koboSpan" id="kobo.1326.4">LLMs are not capable of real reasoning or true generalization, so they are limited by what they have seen during pre-training. </span><span class="koboSpan" id="kobo.1326.5">Highly specialized tasks are not seen by LLMs during pre-training, thus limiting the applicability of these approaches to a selected set of tasks. </span><span class="koboSpan" id="kobo.1326.6">Adding context and additional information could mitigate </span><span class="No-Break"><span class="koboSpan" id="kobo.1327.1">this problem.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer244">
<span class="koboSpan" id="kobo.1328.1"><img alt="Figure 8.32 – LLM as a reward designer (https://arxiv.org/pdf/2404.00282)" src="image/B21257_08_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1329.1">Figure 8.32 – LLM as a reward designer (</span><a href="https://arxiv.org/pdf/2404.00282"><span class="koboSpan" id="kobo.1330.1">https://arxiv.org/pdf/2404.00282</span></a><span class="koboSpan" id="kobo.1331.1">)</span></p>
<h3><span class="koboSpan" id="kobo.1332.1">Decision-maker</span></h3>
<p><span class="koboSpan" id="kobo.1333.1">Since RL has</span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.1334.1"> problems in many cases with sample and exploration inefficiency, LLMs can be used in decision-making and thus help in choosing actions. </span><span class="koboSpan" id="kobo.1334.2">LLMs can be used to reduce the set of actions in a certain state (for example, when many actions are possible). </span><span class="koboSpan" id="kobo.1334.3">Reducing the set of actions reduces the exploration space, thus increasing exploration efficiency. </span><span class="koboSpan" id="kobo.1334.4">For example, an LLM can be used to train robots on what actions to take in a world, reducing </span><span class="No-Break"><span class="koboSpan" id="kobo.1335.1">exploration time.</span></span></p>
<p><span class="koboSpan" id="kobo.1336.1">The transformer (or derivative models) has shown great potential in RL. </span><span class="koboSpan" id="kobo.1336.2">The idea behind it is to treat these problems as sequence modeling problems (instead of trial and error). </span><span class="koboSpan" id="kobo.1336.3">LLMs can then be seen as a decision-making model, which has to decide on a sequence of problems (as we mentioned in </span><a href="B21257_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1337.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.1338.1">, the transformer is trained on a sequence of problems, so making a decision on a sequence of states is congenial to its training). </span><span class="koboSpan" id="kobo.1338.2">An LLM can be fine-tuned to leverage the internal representation of the model. </span><span class="koboSpan" id="kobo.1338.3">In fact, in this way, we leverage the representation learned from an LLM (being trained with a huge quantity of text, an LLM has a vast amount of knowledge already acquired that can be applied to a task) to decide an action. </span><span class="koboSpan" id="kobo.1338.4">Using prior knowledge reduces the need for data collection </span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.1339.1">and exploration (hence, sample efficiency) and makes the system more efficient toward long-term rewards or sparse reward environments. </span><span class="koboSpan" id="kobo.1339.2">Several studies have shown not only the transferability of knowledge learned from an LLM to other models but also the improved performance of the whole system on different benchmarks. </span><span class="koboSpan" id="kobo.1339.3">In addition, vision-language models can be used to be able to adapt the system to multimodal environments. </span><span class="koboSpan" id="kobo.1339.4">Using an LLM as a decision-maker is still computationally expensive (even if only used in inference and without the need for fine-tuning). </span><span class="koboSpan" id="kobo.1339.5">As a result, current studies are focusing on trying to reduce the computational cost of </span><span class="No-Break"><span class="koboSpan" id="kobo.1340.1">these approaches.</span></span></p>
<p><span class="koboSpan" id="kobo.1341.1">Alternatively, an LLM can guide the agent in choosing actions by generating reasonable action candidates or expert actions. </span><span class="koboSpan" id="kobo.1341.2">For example, in environments such as text-based games, the action space is very large, and only a fraction of the actions is currently available, so an agent can learn with extensive trial-and-error; however, this exploration is very inefficient. </span><span class="koboSpan" id="kobo.1341.3">An LLM can reduce this action space by generating an action set by understanding the task. </span><span class="koboSpan" id="kobo.1341.4">This makes it possible to reduce exploration and make it more efficient, collect more rewards, and speed up training. </span><span class="koboSpan" id="kobo.1341.5">Typically, in these approaches, we have an LLM that generates a set of actions and another neural network that generates the Q-values of these candidates. </span><span class="koboSpan" id="kobo.1341.6">The same approach has been extended to robots that have to follow human instructions, where an LLM generates possible actions. </span><span class="koboSpan" id="kobo.1341.7">This approach is limited owing to the inheritance of the biases and limitations of an LLM (since an LLM decides the action space and generates it according to its knowledge </span><span class="No-Break"><span class="koboSpan" id="kobo.1342.1">and biases).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer245">
<span class="koboSpan" id="kobo.1343.1"><img alt="Figure 8.33 – LLM as a decision-maker (https://arxiv.org/pdf/2404.00282)" src="image/B21257_08_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1344.1">Figure 8.33 – LLM as a decision-maker (</span><a href="https://arxiv.org/pdf/2404.00282"><span class="koboSpan" id="kobo.1345.1">https://arxiv.org/pdf/2404.00282</span></a><span class="koboSpan" id="kobo.1346.1">)</span></p>
<h3><span class="koboSpan" id="kobo.1347.1">Generator</span></h3>
<p><span class="koboSpan" id="kobo.1348.1">Model-based RL</span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.1349.1"> relies on world models to learn the dynamics of the environment and simulate trajectories. </span><span class="koboSpan" id="kobo.1349.2">The capabilities of an LLM can be to generate accurate trajectories or to explain </span><span class="No-Break"><span class="koboSpan" id="kobo.1350.1">policy choices.</span></span></p>
<p><span class="koboSpan" id="kobo.1351.1">An LLM has an inherent generative capacity that allows it to be used as a generator. </span><span class="koboSpan" id="kobo.1351.2">An LLM can then be used as a world model simulator, where the system generates accurate trajectories that the agent uses to learn and plan. </span><span class="koboSpan" id="kobo.1351.3">This has been used with video games, where an LLM can generate the trajectories and thus reduce the time it takes an agent to learn the game (better sample efficiency). </span><span class="koboSpan" id="kobo.1351.4">The LLM’s generative capabilities can then be used to predict the future. </span><span class="koboSpan" id="kobo.1351.5">Although promising, there is still difficulty in aligning the abstract knowledge of an LLM with the reality of an environment, limiting the impact of its </span><span class="No-Break"><span class="koboSpan" id="kobo.1352.1">generative capability.</span></span></p>
<p><span class="koboSpan" id="kobo.1353.1">Another interesting approach is where an LLM is used to explain the policy of an RL system. </span><strong class="bold"><span class="koboSpan" id="kobo.1354.1">Explainable RL</span></strong><span class="koboSpan" id="kobo.1355.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1356.1">XRL</span></strong><span class="koboSpan" id="kobo.1357.1">) is a </span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.1358.1">subfield at the intersection of explainable machine learning and RL that is growing. </span><span class="koboSpan" id="kobo.1358.2">XRL seeks to explain an agent’s behavior clearly to a human being. </span><span class="koboSpan" id="kobo.1358.3">An LLM could then be used to explain in natural language why an agent makes a certain decision or responds in a certain way to a change in environment. </span><span class="koboSpan" id="kobo.1358.4">As a policy interpreter, an LLM given a state and an action should explain an agent’s behavior. </span><span class="koboSpan" id="kobo.1358.5">These explanations should then be understandable to a human, thus allowing an agent’s safety to be checked. </span><span class="koboSpan" id="kobo.1358.6">Of course, the quality of the explanations depends on the LLM’s ability to understand the representation of the features of the environment and the implicit</span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.1359.1"> logic of the policy. </span><span class="koboSpan" id="kobo.1359.2">It is difficult to use domain knowledge or examples to improve understanding of a complex policy (especially for </span><span class="No-Break"><span class="koboSpan" id="kobo.1360.1">complex environments).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer246">
<span class="koboSpan" id="kobo.1361.1"><img alt="Figure 8.34 – LLM as a generator (https://arxiv.org/pdf/2404.00282)" src="image/B21257_08_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1362.1">Figure 8.34 – LLM as a generator (</span><a href="https://arxiv.org/pdf/2404.00282"><span class="koboSpan" id="kobo.1363.1">https://arxiv.org/pdf/2404.00282</span></a><span class="koboSpan" id="kobo.1364.1">)</span></p>
<p><span class="koboSpan" id="kobo.1365.1">LLM-enhanced RL can </span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.1366.1">be useful in a variety </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">of applications:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1368.1">Robotics</span></strong><span class="koboSpan" id="kobo.1369.1">: Using LLMs can improve the interaction between humans and robots, help robots better understand human needs or human logic, and improve their decision-making and </span><span class="No-Break"><span class="koboSpan" id="kobo.1370.1">planning capabilities.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1371.1">Autonomous driving</span></strong><span class="koboSpan" id="kobo.1372.1">: RL is used in autonomous driving to make decisions in changing environments that are complex and where input from different sensors (visual, lidar, radar) must be analyzed along with contextual information (traffic laws, human behavior, unexpected problems). </span><span class="koboSpan" id="kobo.1372.2">LLMs can improve the ability to</span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.1373.1"> process and integrate this multimodal information, better understand instructions, and improve the goal and rewards (e.g., design reward functions that take into account not only safety but also passenger comfort and </span><span class="No-Break"><span class="koboSpan" id="kobo.1374.1">engine efficiency).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1375.1">Healthcare recommendations</span></strong><span class="koboSpan" id="kobo.1376.1">: RL is used in healthcare to learn recommendations and suggestions. </span><span class="koboSpan" id="kobo.1376.2">LLMs can be used for their vast knowledge and ability to analyze huge amounts of patient data and medical data, accelerating the agent’s learning process, or providing information for </span><span class="No-Break"><span class="koboSpan" id="kobo.1377.1">better learning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1378.1">Energy management</span></strong><span class="koboSpan" id="kobo.1379.1">: RL is used to improve the use, transportation, conversion, and storage of energy. </span><span class="koboSpan" id="kobo.1379.2">In addition, it is expected to play an important role in future technologies such as nuclear fusion. </span><span class="koboSpan" id="kobo.1379.3">LLMs can be used to improve sample efficiency, multitask optimization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1380.1">much more.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1381.1">Despite all these opportunities, there are also a number of limitations to the use of LLMs in RL. </span><span class="koboSpan" id="kobo.1381.2">The first challenge is that the LLM-enhanced RL paradigm is highly dependent on the capabilities of the LLM. </span><span class="koboSpan" id="kobo.1381.3">LLMs suffer from bias and can hallucinate; an agent then inherits these problems from the LLM. </span><span class="koboSpan" id="kobo.1381.4">In addition, LLMs can also misinterpret the task and data, especially when they are complex or noisy. </span><span class="koboSpan" id="kobo.1381.5">In addition, if the task or environment is not represented in their pre-training, LLMs have problems adapting to new environments and tasks. </span><span class="koboSpan" id="kobo.1381.6">To limit these effects, the use of synthetic data, fine-tuning the model, or use of continual learning has been proposed. </span><span class="koboSpan" id="kobo.1381.7">Continual learning could allow a model to adapt to new tasks and new environments, without forgetting what the model has learned previously. </span><span class="koboSpan" id="kobo.1381.8">To date, though, continual learning and catastrophic forgetting are open problems in </span><span class="No-Break"><span class="koboSpan" id="kobo.1382.1">deep learning.</span></span></p>
<p><span class="koboSpan" id="kobo.1383.1">In addition, the addition of an LLM brings a higher computational cost (both in training and in inference), and an increase in latency time. </span><span class="koboSpan" id="kobo.1383.2">Several techniques can be used to reduce this computational cost, such as quantization, pruning, or using small models. </span><span class="koboSpan" id="kobo.1383.3">Some approaches use </span><em class="italic"><span class="koboSpan" id="kobo.1384.1">mixture of experts</span></em><span class="koboSpan" id="kobo.1385.1">, allowing conditional computation, transformer variants (state space models), caching strategies, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1386.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.1387.1">Finally, one should not forget that the use of LLMs also opens up ethical, legal, and safety issues. </span><span class="koboSpan" id="kobo.1387.2">The same problems we saw in </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1388.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1389.1"> are also applicable to these systems. </span><span class="koboSpan" id="kobo.1389.2">For example, data privacy and intellectual property remain open topics for applications in sensitive fields such as healthcare </span><span class="No-Break"><span class="koboSpan" id="kobo.1390.1">or finance.</span></span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.1391.1">Key takeaways</span></h1>
<p><span class="koboSpan" id="kobo.1392.1">Because this chapter was dense in terms of theory, we decided to add a small recap section. </span><span class="koboSpan" id="kobo.1392.2">This chapter introduced RL as a core approach to enabling intelligent agents to learn from interaction with dynamic environments through trial and error, similar to how humans learn by acting, observing outcomes, and adjusting behavior. </span><span class="koboSpan" id="kobo.1392.3">RL differs from supervised learning by focusing on learning from rewards rather than labeled data, and it is especially suited to tasks with delayed feedback and evolving </span><span class="No-Break"><span class="koboSpan" id="kobo.1393.1">decision sequences.</span></span></p>
<p><span class="koboSpan" id="kobo.1394.1">RL is a machine learning paradigm where an agent learns to make decisions by interacting with an environment to maximize cumulative rewards. </span><span class="koboSpan" id="kobo.1394.2">It learns through trial and error, balancing exploration (trying new actions) and exploitation (using </span><span class="No-Break"><span class="koboSpan" id="kobo.1395.1">known strategies).</span></span></p>
<p><span class="koboSpan" id="kobo.1396.1">In summary, we have these classes </span><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">of methods:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1398.1">Model-free versus </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1399.1">model-based RL</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1400.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1401.1">Model-free methods</span></strong><span class="koboSpan" id="kobo.1402.1"> (e.g., DQN, REINFORCE) learn directly from interaction without modeling the environment. </span><span class="koboSpan" id="kobo.1402.2">They are simpler and </span><span class="No-Break"><span class="koboSpan" id="kobo.1403.1">more scalable.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1404.1">Model-based methods</span></strong><span class="koboSpan" id="kobo.1405.1"> use an internal model to simulate outcomes and plan ahead. </span><span class="koboSpan" id="kobo.1405.2">They are more sample-efficient and suitable for environments where planning is crucial but are harder to design </span><span class="No-Break"><span class="koboSpan" id="kobo.1406.1">and compute.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1407.1">On-policy versus </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1408.1">off-policy methods</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1409.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1410.1">On-policy methods</span></strong><span class="koboSpan" id="kobo.1411.1"> learn from the data generated by the current policy (e.g., REINFORCE, PPO), making them more stable but </span><span class="No-Break"><span class="koboSpan" id="kobo.1412.1">sample inefficient.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1413.1">Off-policy methods</span></strong><span class="koboSpan" id="kobo.1414.1"> (e.g., DQN) can learn from past or alternative policies, improving sample efficiency and </span><span class="No-Break"><span class="koboSpan" id="kobo.1415.1">exploration flexibility.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1416.1">Main </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1417.1">algorithms discussed</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1419.1">Q-Learning and DQN</span></strong><span class="koboSpan" id="kobo.1420.1">: Learn value functions using lookup tables or </span><span class="No-Break"><span class="koboSpan" id="kobo.1421.1">neural networks.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1422.1">REINFORCE</span></strong><span class="koboSpan" id="kobo.1423.1">: A basic policy gradient method using </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">stochastic policies.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1425.1">PPO</span></strong><span class="koboSpan" id="kobo.1426.1">: Balances stability and performance by clipping </span><span class="No-Break"><span class="koboSpan" id="kobo.1427.1">policy updates.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1428.1">Actor-Critic</span></strong><span class="koboSpan" id="kobo.1429.1">: Combines value estimation and policy learning for more </span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">robust updates.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1431.1">AlphaZero</span></strong><span class="koboSpan" id="kobo.1432.1">: Combines deep learning with Monte Carlo Tree Search for self-play-based strategy optimization in </span><span class="No-Break"><span class="koboSpan" id="kobo.1433.1">complex games.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1434.1">Practical </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1435.1">use cases</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1436.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1437.1">Gaming</span></strong><span class="koboSpan" id="kobo.1438.1">: RL agents such as AlphaZero and DQN have mastered games such as Go, Chess, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1439.1">Atari titles.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1440.1">Robotics</span></strong><span class="koboSpan" id="kobo.1441.1">: RL allows robots to learn complex movement and interaction policies through simulation and </span><span class="No-Break"><span class="koboSpan" id="kobo.1442.1">real-world feedback.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1443.1">Autonomous vehicles</span></strong><span class="koboSpan" id="kobo.1444.1">: RL enables the learning of driving strategies in dynamic and </span><span class="No-Break"><span class="koboSpan" id="kobo.1445.1">uncertain environments.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1446.1">Optimization and control</span></strong><span class="koboSpan" id="kobo.1447.1">: Applied in finance, healthcare, logistics, and industrial automation for </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">sequential decision-making.</span></span></li></ul></li>
</ul>
<h1 id="_idParaDest-154"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.1449.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1450.1">In the previous chapters, the main question was how to find information and how to deliver it effectively to an LLM. </span><span class="koboSpan" id="kobo.1450.2">In such cases, the model is a passive agent that receives information and responds. </span><span class="koboSpan" id="kobo.1450.3">With this chapter, we are trying to move away from this paradigm, toward an idea where an agent explores an environment, learns through this exploration, performs actions, and learns from the feedback that the environment provides to it. </span><span class="koboSpan" id="kobo.1450.4">In this view, the model is an active component that interacts with the environment and can modify it. </span><span class="koboSpan" id="kobo.1450.5">This view is also much closer to how we humans learn. </span><span class="koboSpan" id="kobo.1450.6">In our exploration of the external world, we receive feedback that guides us in our learning. </span><span class="koboSpan" id="kobo.1450.7">Although much of the world has been noted in texts, the real world cannot be reduced to a textual description. </span><span class="koboSpan" id="kobo.1450.8">Therefore, an agent cannot learn certain knowledge and skills without interacting with the world. </span><span class="koboSpan" id="kobo.1450.9">RL is a field of artificial intelligence that focuses on an agent’s interactions with the environment and how it can learn </span><span class="No-Break"><span class="koboSpan" id="kobo.1451.1">from it.</span></span></p>
<p><span class="koboSpan" id="kobo.1452.1">In this chapter, therefore, we introduced the fundamentals of RL. </span><span class="koboSpan" id="kobo.1452.2">In the first section, we discussed the basic components of an RL system (agent, environment, reward, and action). </span><span class="koboSpan" id="kobo.1452.3">We then discussed the main question of RL, how to balance exploration and exploitation. </span><span class="koboSpan" id="kobo.1452.4">Indeed, an agent has a goal (accomplish a task) but learns how to accomplish this task through exploration. </span><span class="koboSpan" id="kobo.1452.5">For example, we saw in the multi-armed bandit example how a greedy model performs worse than a model that explores the possibilities. </span><span class="koboSpan" id="kobo.1452.6">This principle remains fundamental when we define an agent to solve complex problems such as solving a video game. </span><span class="koboSpan" id="kobo.1452.7">To solve complex tasks, we introduced the use of neural networks (deep RL). </span><span class="koboSpan" id="kobo.1452.8">We saw that there are different types of algorithms with different advantages and disadvantages, and we saw how we can set one of them to win in a classic video game. </span><span class="koboSpan" id="kobo.1452.9">Once we trained our model, we discussed how LLM and RL fields are increasingly intersecting. </span><span class="koboSpan" id="kobo.1452.10">In this way, we saw how the strengths of the two fields can </span><span class="No-Break"><span class="koboSpan" id="kobo.1453.1">be synergistic.</span></span></p>
<p><span class="koboSpan" id="kobo.1454.1">From this chapter on, the focus will be more applicative. </span><span class="koboSpan" id="kobo.1454.2">We will see how an agent can generally accomplish a task. </span><span class="koboSpan" id="kobo.1454.3">In the upcoming chapters, the agent will mainly be an LLM who will use tools to perform actions and accomplish tasks. </span><span class="koboSpan" id="kobo.1454.4">The choice, then, for the agent will not be which action to take but which tool to choose in order to accomplish a task. </span><span class="koboSpan" id="kobo.1454.5">Despite the fact that an LLM agent interacts with the environment, one main difference is that there will be no training. </span><span class="koboSpan" id="kobo.1454.6">Training an LLM is a complex task, so in these systems, we try to train them as little as possible. </span><span class="koboSpan" id="kobo.1454.7">If, in the previous chapters (</span><em class="italic"><span class="koboSpan" id="kobo.1455.1">5–7</span></em><span class="koboSpan" id="kobo.1456.1">), we tried to leverage the comprehension skills of an LLM, in the next chapters, we will try to leverage the skills of LLMs to interact with the environment or with other agents – skills that are possible anyway because an LLM can understand a task </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">and instructions.</span></span></p>
<h1 id="_idParaDest-155"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.1458.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.1459.1">Ghasemi, </span><em class="italic"><span class="koboSpan" id="kobo.1460.1">An Introduction to Reinforcement Learning: Fundamental Concepts and Practical Applications</span></em><span class="koboSpan" id="kobo.1461.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1462.1">2024, </span></span><a href="https://arxiv.org/abs/2408.07712"><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">https://arxiv.org/abs/2408.07712</span></span></a></li>
<li><span class="koboSpan" id="kobo.1464.1">Mnih, </span><em class="italic"><span class="koboSpan" id="kobo.1465.1">Playing Atari with Deep Reinforcement Learning</span></em><span class="koboSpan" id="kobo.1466.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1467.1">2013, </span></span><a href="https://arxiv.org/abs/1312.5602"><span class="No-Break"><span class="koboSpan" id="kobo.1468.1">https://arxiv.org/abs/1312.5602</span></span></a></li>
<li><span class="koboSpan" id="kobo.1469.1">Hugging Face, </span><em class="italic"><span class="koboSpan" id="kobo.1470.1">Proximal Policy Optimization (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1471.1">PPO)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1">, </span></span><a href="https://huggingface.co/blog/deep-rl-ppo"><span class="No-Break"><span class="koboSpan" id="kobo.1473.1">https://huggingface.co/blog/deep-rl-ppo</span></span></a></li>
<li><span class="koboSpan" id="kobo.1474.1">Wang, </span><em class="italic"><span class="koboSpan" id="kobo.1475.1">Learning Reinforcement Learning by </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1476.1">LearningREINFORCE</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1477.1">, </span></span><a href="https://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1478.1">https://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf</span></span></a></li>
<li><span class="koboSpan" id="kobo.1479.1">Kaufmann, </span><em class="italic"><span class="koboSpan" id="kobo.1480.1">A Survey of Reinforcement Learning from Human Feedback</span></em><span class="koboSpan" id="kobo.1481.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">2024, </span></span><a href="https://arxiv.org/pdf/2312.14925"><span class="No-Break"><span class="koboSpan" id="kobo.1483.1">https://arxiv.org/pdf/2312.14925</span></span></a></li>
<li><span class="koboSpan" id="kobo.1484.1">Bongratz, </span><em class="italic"><span class="koboSpan" id="kobo.1485.1">How to Choose a Reinforcement-Learning Algorithm</span></em><span class="koboSpan" id="kobo.1486.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1487.1">2024, </span></span><a href="https://arxiv.org/abs/2407.20917v1"><span class="No-Break"><span class="koboSpan" id="kobo.1488.1">https://arxiv.org/abs/2407.20917v1</span></span></a></li>
<li><span class="koboSpan" id="kobo.1489.1">Schulman, </span><em class="italic"><span class="koboSpan" id="kobo.1490.1">Proximal Policy Optimization Algorithms</span></em><span class="koboSpan" id="kobo.1491.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1492.1">2017, </span></span><a href="https://arxiv.org/abs/1707.06347"><span class="No-Break"><span class="koboSpan" id="kobo.1493.1">https://arxiv.org/abs/1707.06347</span></span></a></li>
<li><span class="koboSpan" id="kobo.1494.1">OpenAI, </span><em class="italic"><span class="koboSpan" id="kobo.1495.1">Proximal Policy </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1496.1">Optimization</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1497.1">, </span></span><a href="https://openai.com/index/openai-baselines-ppo/"><span class="No-Break"><span class="koboSpan" id="kobo.1498.1">https://openai.com/index/openai-baselines-ppo/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1499.1">OpenAI Spinning UP, </span><em class="italic"><span class="koboSpan" id="kobo.1500.1">Proximal Policy </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1501.1">Optimization</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1502.1">, </span></span><a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html"><span class="No-Break"><span class="koboSpan" id="kobo.1503.1">https://spinningup.openai.com/en/latest/algorithms/ppo.html</span></span></a></li>
<li><span class="koboSpan" id="kobo.1504.1">Bick, </span><em class="italic"><span class="koboSpan" id="kobo.1505.1">Towards Delivering a Coherent Self-Contained Explanation of Proximal Policy Optimization</span></em><span class="koboSpan" id="kobo.1506.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1507.1">2021, </span></span><a href="https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1508.1">https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf</span></span></a></li>
<li><span class="koboSpan" id="kobo.1509.1">Silver, </span><em class="italic"><span class="koboSpan" id="kobo.1510.1">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</span></em><span class="koboSpan" id="kobo.1511.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1512.1">2017, </span></span><a href="https://arxiv.org/abs/1712.01815"><span class="No-Break"><span class="koboSpan" id="kobo.1513.1">https://arxiv.org/abs/1712.01815</span></span></a></li>
<li><span class="koboSpan" id="kobo.1514.1">McGrath, </span><em class="italic"><span class="koboSpan" id="kobo.1515.1">Acquisition of Chess Knowledge in AlphaZero</span></em><span class="koboSpan" id="kobo.1516.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1517.1">2021, </span></span><a href="https://arxiv.org/abs/2111.09259"><span class="No-Break"><span class="koboSpan" id="kobo.1518.1">https://arxiv.org/abs/2111.09259</span></span></a></li>
<li><span class="koboSpan" id="kobo.1519.1">DeepMind, </span><em class="italic"><span class="koboSpan" id="kobo.1520.1">AlphaZero: Shedding </span></em><em class="italic"><span class="koboSpan" id="kobo.1521.1">New Light on Chess, Shogi, and</span></em><em class="italic"><span class="koboSpan" id="kobo.1522.1"> Go</span></em><span class="koboSpan" id="kobo.1523.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1524.1">2018, </span></span><a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/"><span class="No-Break"><span class="koboSpan" id="kobo.1525.1">https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/</span></span></a></li>
<li><span class="koboSpan" id="kobo.1526.1">Gao, </span><em class="italic"><span class="koboSpan" id="kobo.1527.1">Efficiently Mastering the Game of NoGo with Deep Reinforcement Learning Supported by Domain Knowledge</span></em><span class="koboSpan" id="kobo.1528.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1529.1">2021, </span></span><a href="https://www.mdpi.com/2079-9292/10/13/1533"><span class="No-Break"><span class="koboSpan" id="kobo.1530.1">https://www.mdpi.com/2079-9292/10/13/1533</span></span></a></li>
<li><span class="koboSpan" id="kobo.1531.1">Francois-Lavet, </span><em class="italic"><span class="koboSpan" id="kobo.1532.1">An Introduction to Deep Reinforcement Learning</span></em><span class="koboSpan" id="kobo.1533.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1534.1">2018, </span></span><a href="https://arxiv.org/abs/1811.12560"><span class="No-Break"><span class="koboSpan" id="kobo.1535.1">https://arxiv.org/abs/1811.12560</span></span></a></li>
<li><span class="koboSpan" id="kobo.1536.1">Tang, </span><em class="italic"><span class="koboSpan" id="kobo.1537.1">Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes</span></em><span class="koboSpan" id="kobo.1538.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1539.1">2024, </span></span><a href="https://arxiv.org/abs/2408.03539"><span class="No-Break"><span class="koboSpan" id="kobo.1540.1">https://arxiv.org/abs/2408.03539</span></span></a></li>
<li><span class="koboSpan" id="kobo.1541.1">Mohan, </span><em class="italic"><span class="koboSpan" id="kobo.1542.1">Structure in Deep Reinforcement Learning: A Survey and Open Problems</span></em><span class="koboSpan" id="kobo.1543.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1544.1">2023, </span></span><a href="https://arxiv.org/abs/2306.16021"><span class="No-Break"><span class="koboSpan" id="kobo.1545.1">https://arxiv.org/abs/2306.16021</span></span></a></li>
<li><span class="koboSpan" id="kobo.1546.1">Cao, </span><em class="italic"><span class="koboSpan" id="kobo.1547.1">Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods</span></em><span class="koboSpan" id="kobo.1548.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.1549.1">2024, </span></span><a href="https://arxiv.org/abs/2404.00282"><span class="No-Break"><span class="koboSpan" id="kobo.1550.1">https://arxiv.org/abs/2404.00282</span></span></a></li>
</ul>
</div>


<div class="Content" id="_idContainer248">
<h1 id="_idParaDest-156" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.1.1">Part 3: 
Creating Sophisticated AI to Solve Complex Scenarios</span></h1>
<p><span class="koboSpan" id="kobo.2.1">This final part focuses on assembling the components introduced in the previous chapters to build fully-fledged, production-ready AI systems. </span><span class="koboSpan" id="kobo.2.2">It begins with the design and orchestration of single- and multi-agent systems, where LLMs collaborate with tools, APIs, and other models to tackle complex, multi-step tasks. </span><span class="koboSpan" id="kobo.2.3">The section then guides you through the practical aspects of building and deploying AI agent applications using modern tools such as Streamlit, asynchronous programming, and containerization technologies such as Docker. </span><span class="koboSpan" id="kobo.2.4">Finally, the book closes with a forward-looking discussion on the future of AI agents, their impact across industries such as healthcare and law, and the ethical and technical challenges that lie ahead. </span><span class="koboSpan" id="kobo.2.5">This part empowers you to move from experimentation to real-world deployment, preparing them to contribute to the next wave of </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">intelligent systems.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">following chapters:</span></span></p>
<ul>
<li><a href="B21257_09.xhtml#_idTextAnchor156"><em class="italic"><span class="koboSpan" id="kobo.6.1">Chapter 9</span></em></a><em class="italic"><span class="koboSpan" id="kobo.7.1">, Creating Single- and Multi-Agent Systems</span></em></li>
<li><a href="B21257_10.xhtml#_idTextAnchor179"><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 10</span></em></a><em class="italic"><span class="koboSpan" id="kobo.9.1">, Building an AI Agent Application</span></em></li>
<li><a href="B21257_11.xhtml#_idTextAnchor215"><em class="italic"><span class="koboSpan" id="kobo.10.1">Chapter 11</span></em></a><em class="italic"><span class="koboSpan" id="kobo.11.1">, The Future Ahead</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer249">
</div>
</div>
<div>
<div id="_idContainer250">
</div>
</div>
<div>
<div id="_idContainer251">
</div>
</div>
<div>
<div id="_idContainer252">
</div>
</div>
<div>
<div id="_idContainer253">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer254">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer255">
</div>
</div>
<div>
<div id="_idContainer256">
</div>
</div>
<div>
<div id="_idContainer257">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer258">
</div>
</div>
</body></html>