<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer033">
			<h1 id="_idParaDest-189" class="chapter-number"><a id="_idTextAnchor247"/>15</h1>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor248"/>Cross-Validation</h1>
			<p><strong class="bold">Cross-validation</strong> is a<a id="_idIndexMarker749"/> statistical technique used to assess how well a machine learning model generalizes to unseen data. It involves partitioning a dataset into multiple subsets or “folds,” training the model on some of these subsets while testing it on the remaining ones. This process is repeated to ensure a reliable performance estimate. This helps detect overfitting and provides a more robust evaluation than a single train-test split. In the context of LLMs, cross-validation must be adapted to address the complexities of pre-training, fine-tuning, few-shot learning, and domain generalization, making it an essential tool for evaluating model performance across varied tasks and <span class="No-Break">data distributions.</span></p>
			<p>In this chapter, you will explore cross-validation strategies specifically designed for LLMs. We’ll delve into methods for creating appropriate data splits for pre-training and fine-tuning, as well as strategies for few-shot and zero-shot evaluation. You’ll learn how to assess domain and task generalization in LLMs and handle the unique challenges of cross-validation in the context <span class="No-Break">of LLMs.</span></p>
			<p>By the end of this chapter, you’ll be equipped with robust cross-validation techniques to reliably assess your LLM’s performance and generalization capabilities across various domains <span class="No-Break">and tasks.</span></p>
			<p>In this chapter, we’ll be covering the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Pre-training and fine-tuning <span class="No-Break">data splits</span></li>
				<li>Few-shot and zero-shot <span class="No-Break">evaluation strategies</span></li>
				<li>Domain and <span class="No-Break">task generalization</span></li>
				<li>Continual <span class="No-Break">learning evaluation</span></li>
				<li>Cross-validation challenges and <span class="No-Break">best practices</span></li>
			</ul>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor249"/>Pre-training and fine-tuning data splits</h1>
			<p>In LLMs, data<a id="_idIndexMarker750"/> splits refer to the division of datasets<a id="_idIndexMarker751"/> into training, validation, and test sets to ensure the model learns generalizable patterns rather than memorizing data. This is essential for evaluating performance fairly, tuning model parameters, and preventing data leakage. Proper splitting is especially important in LLMs due to their scale, the diversity of tasks, and the need to assess domain and <span class="No-Break">task generalization.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor250"/>Stratified sampling for pre-training data</h2>
			<p><strong class="bold">Stratified sampling</strong> is a <a id="_idIndexMarker752"/>sampling method that first <a id="_idIndexMarker753"/>divides the population into smaller subgroups (<strong class="bold">strata</strong>) based<a id="_idIndexMarker754"/> on shared characteristics and then randomly samples from within each stratum to ensure proportional representation of all groups in the final sample. This is particularly useful when dealing with <span class="No-Break">imbalanced datasets.</span></p>
			<p>When creating data splits for pre-training, it’s important to ensure that each split represents the diversity of the entire dataset. Here’s an example of how you might implement <strong class="bold">stratified sampling</strong> for <span class="No-Break">pre-training data:</span></p>
			<pre class="source-code">
import pandas as pd
from sklearn.model_selection import StratifiedShuffleSplit
def stratified_pretraining_split(
    data, text_column, label_column, test_size=0.1, random_state=42
):
    sss = StratifiedShuffleSplit(
        n_splits=1, test_size=test_size, random_state=random_state)
    for train_index, test_index in sss.split(
        data[text_column], data[label_column]
    ):
        train_data = data.iloc[train_index]
        test_data = data.iloc[test_index]
    return train_data, test_data
# Example usage
data = pd.read_csv('your_pretraining_data.csv')
train_data, test_data = stratified_pretraining_split(
    data, 'text', 'domain')
print(f"Training set size: {len(train_data)}")
print(f"Test set size: {len(test_data)}")</pre>			<p>This code uses <strong class="source-inline">StratifiedShuffleSplit</strong> to create a stratified split of the pre-training data, ensuring <a id="_idIndexMarker755"/>that the distribution of <a id="_idIndexMarker756"/>domains (or any other relevant categorical variable) is similar in both the training and <span class="No-Break">test sets.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor251"/>Time-based splitting for fine-tuning data</h2>
			<p>For fine-tuning<a id="_idIndexMarker757"/> tasks that involve time-sensitive data, it’s<a id="_idIndexMarker758"/> often beneficial to use <span class="No-Break"><strong class="bold">time-based splitting</strong></span><span class="No-Break">.</span></p>
			<p>Time-based splitting is a data partitioning strategy where the dataset is divided according to chronological order, ensuring that earlier data is used for training and later data for validation or testing. This approach is especially important for fine-tuning tasks involving time-sensitive data—such as financial forecasting, user behavior modeling, or event prediction—where future information should not influence past training. By preserving the natural temporal sequence, time-based splitting helps evaluate how well a model can generalize to future, unseen scenarios, closely mimicking <span class="No-Break">real-world deployment.</span></p>
			<p>This approach helps evaluate how well the model generalizes to <span class="No-Break">future data:</span></p>
			<pre class="source-code">
import pandas as pd
def time_based_finetuning_split(data, timestamp_column, split_date):
    data[timestamp_column] = pd.to_datetime(data[timestamp_column])
    train_data = data[data[timestamp_column] &lt; split_date]
    test_data = data[data[timestamp_column] &gt;= split_date]
    return train_data, test_data
# Example usage
data = pd.read_csv('your_finetuning_data.csv')
split_date = '2023-01-01'
train_data, test_data = time_based_finetuning_split(
    data, 'timestamp', split_date)
print(f"Training set size: {len(train_data)}")
print(f"Test set size: {len(test_data)}")</pre>			<p>This function<a id="_idIndexMarker759"/> splits<a id="_idIndexMarker760"/> the data based on a specified date, which is particularly useful for tasks where the model needs to generalize to future events<a id="_idTextAnchor252"/> <span class="No-Break">or trends.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor253"/>Oversampling and weighting techniques for data balancing</h2>
			<p>When working <a id="_idIndexMarker761"/>with <a id="_idIndexMarker762"/>datasets that have an uneven distribution of categories, such as imbalanced domains or label<a id="_idIndexMarker763"/> frequencies, <strong class="bold">oversampling</strong> and <strong class="bold">weighting techniques</strong> can <a id="_idIndexMarker764"/>help ensure that the model learns effectively from all classes. Oversampling involves replicating examples from underrepresented categories to increase their presence in the training data, preventing the model from ignoring them. This can be done using methods such as random oversampling or synthetic data generation (e.g., SMOTE for structured data). On the other hand, weighting techniques adjust the loss function by assigning higher importance to underrepresented categories, so the model learns from them without necessarily increasing the dataset size. Both approaches help mitigate bias, improving the model’s ability to generalize across all categories, rather than favoring the most <span class="No-Break">frequent ones.</span></p>
			<p>Here’s a short code example demonstrating oversampling and class weighting techniques<a id="_idIndexMarker765"/> using PyTorch and sklearn, applied to a text <a id="_idIndexMarker766"/><span class="No-Break">classification task:</span></p>
			<pre class="source-code">
from sklearn.utils.class_weight import compute_class_weight
from torch.utils.data import DataLoader, WeightedRandomSampler
import torch
import numpy as np
# Example class distribution (e.g., from dataset labels)
labels = [0, 0, 0, 1, 1, 2]  # Class 2 is underrepresented
# --- 1. Class Weighting ---
# Compute weights inversely proportional to class frequencies
class_weights = compute_class_weight(
    'balanced', classes=np.unique(labels), y=labels)
class_weights = torch.tensor(class_weights, dtype=torch.float)
# Pass weights to loss function
loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)
# --- 2. Oversampling with Weighted Sampler ---
# Create sample weights: inverse of class frequency for each label
label_counts = np.bincount(labels)
sample_weights = [1.0 / label_counts[label] for label in labels]
# Create sampler for DataLoader
sampler = WeightedRandomSampler(
    weights=sample_weights, num_samples=len(labels), replacement=True
)
# Use the sampler in your DataLoader
# Assuming `train_dataset` is a PyTorch Dataset object
train_loader = DataLoader(train_dataset, sampler=sampler,
    batch_size=4)</pre>			<p>The code demonstrates two common techniques for addressing class imbalance in classification tasks: class weighting and oversampling. First, it uses <strong class="source-inline">compute_class_weight</strong> of <strong class="source-inline">sklearn</strong>  to calculate weights inversely proportional to class frequencies, assigning<a id="_idIndexMarker767"/> higher importance to underrepresented<a id="_idIndexMarker768"/> classes (e.g., class 2, which appears less often). These weights are passed to PyTorch’s <strong class="source-inline">CrossEntropyLoss</strong>, so that during training, misclassifying rare classes penalizes the model more than misclassifying common ones. Second, it performs oversampling by computing a per-sample weight based on the inverse frequency of each sample’s class, which ensures that samples from minority classes have a higher probability of being selected during training. These sample weights are used to initialize PyTorch’s <strong class="source-inline">WeightedRandomSampler</strong>, which enables the <strong class="source-inline">DataLoader</strong> to sample training data in a balanced way across classes without having to physically duplicate data. Together, these techniques help the model learn to treat all classes fairly, improving its generalization on <span class="No-Break">imbalanced datasets.</span></p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor254"/>Few-shot and zero-shot evaluation strategies</h1>
			<p>Few-shot and zero-shot evaluation strategies enable LLMs to generalize across tasks without requiring extensive retraining. Zero-shot learning is useful for tasks where no labeled examples are available, while few-shot learning enhances performance by providing limited guidance. These methods are key to making LLMs adaptable and scalable for <span class="No-Break">real-<a id="_idTextAnchor255"/>world applications.</span></p>
			<p>Here is a comparison between <a id="_idIndexMarker769"/>the <span class="No-Break">two</span><span class="No-Break"><a id="_idIndexMarker770"/></span><span class="No-Break"> strategies:</span></p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Aspect</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Zero-shot</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Few-shot</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>No examples; model must infer task from <span class="No-Break">prompt alone</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Provides a small number of labeled examples in <span class="No-Break">the prompt</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Strengths</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>No labeled data needed, <span class="No-Break">highly flexible</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Higher accuracy, better <span class="No-Break">task comprehension</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Weaknesses</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Lower accuracy, risk <span class="No-Break">of ambiguity</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Requires careful example selection, still less effective <span class="No-Break">than fine-tuning</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Use Cases</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Open-ended Q&amp;A, commonsense reasoning, <span class="No-Break">general-knowledge tasks</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Text classification, translation, summarization, <span class="No-Break">code generation</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 15.1 – Few-shot vs. zero-shot</p>
			<p>Let’s see how to implement each of <span class="No-Break">these strategies.</span></p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor256"/>Few-shot evaluation</h2>
			<p>In <strong class="bold">few-shot evaluation</strong>, we <a id="_idIndexMarker771"/>provide <a id="_idIndexMarker772"/>the model with a small number of examples before asking it to perform a task. Here’s an example of how you might implement <span class="No-Break">few-shot evaluation:</span></p>
			<pre class="source-code">
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch
def few_shot_evaluate(
    model, tokenizer, task_description, examples, test_instance
):
    prompt = f"{task_description}\n\nExamples:\n"
    for example in examples:
        prompt += (
            f"Input: {example['input']}\n"
            f"Output: {example['output']}\n\n"
        )
    prompt += f"Input: {test_instance}\nOutput:"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    with torch.no_grad():
        output = model.generate(input_ids, max_length=100,
            num_return_sequences=1)
    generated_text = tokenizer.decode(output[0],
        skip_special_tokens=True)
    return generated_text.split("Output:")[-1].strip()
# Example usage
model = GPT2LMHeadModel.from_pretrained('gpt2-large')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')
task_description = "Classify the sentiment of the following movie reviews as positive or negative."
examples = [
    {"input": "This movie was fantastic!", "output": "Positive"},
    {"input": "I hated every minute of it.", "output": "Negative"}
]
test_instance = "The acting was superb, but the plot was confusing."
result = few_shot_evaluate(
    model, tokenizer, task_description, examples, test_instance
)
print(f"Few-shot evaluation result: {result}")</pre>			<p>This code demonstrates how to perform few-shot evaluation on a sentiment analysis task using a pre-trained <span class="No-Break">GPT-2 model.</span></p>
			<p>The <strong class="source-inline">few_shot_evaluate</strong> function takes a GPT-2 model, tokenizer, task description, examples, and a test instance as input. It constructs a <strong class="bold">few-shot prompt</strong> by formatting the task description and adding <a id="_idIndexMarker773"/>multiple input-output example pairs to help the model understand the task. The function then appends the test instance, leaving the output blank so the model can complete it. The prompt is tokenized using <strong class="source-inline">tokenizer.encode</strong>, converting it into numerical tokens suitable for the model. The function then uses <strong class="source-inline">model.generate</strong> inside a <strong class="source-inline">torch.no_grad()</strong> block to generate text without computing gradients, making inference more efficient. The model generates a response with a maximum length of <strong class="source-inline">100</strong> tokens, ensuring it stays concise. The generated text is then decoded using <strong class="source-inline">tokenizer.decode</strong>, with <strong class="source-inline">skip_special_tokens=True</strong> to remove unwanted tokens. Finally, the function extracts the part of the response after the last occurrence of <strong class="source-inline">"Output:"</strong> to isolate the model’s <a id="_idIndexMarker774"/>generated <a id="_idIndexMarker775"/>answer, trimming any extra whitespace. This approach effectively <a id="_idIndexMarker776"/>enables <strong class="bold">few-shot learning</strong>, where the model leverages provided examples to make a more <span class="No-Break">informed prediction.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor257"/>Zero-shot evaluation</h2>
			<p><strong class="bold">Zero-shot evaluation</strong> tests <a id="_idIndexMarker777"/>a model’s ability to perform <a id="_idIndexMarker778"/>a task without any specific examples. Here’s how you might implement <span class="No-Break">zero-shot evaluation:</span></p>
			<pre class="source-code">
def zero_shot_evaluate(
    model, tokenizer, task_description, test_instance
):
    prompt = f"{task_description}\n\nInput: {test_instance}\nOutput:"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    with torch.no_grad():
        output = model.generate(
            input_ids, max_length=100, num_return_sequences=1
        )
    generated_text = tokenizer.decode(output[0],
        skip_special_tokens=True)
    return generated_text.split("Output:")[-1].strip()
# Example usage
task_description = "Classify the following text into one of these categories: Science, Politics, Sports, Entertainment."
test_instance = "NASA's Mars rover has discovered traces of ancient microbial life."
result = zero_shot_evaluate(
    model, tokenizer, task_description, test_instance
)
print(f"Zero-shot evaluation result: {result}")</pre>			<p>This function<a id="_idIndexMarker779"/> demonstrates zero-shot evaluation <a id="_idIndexMarker780"/>on a text <span class="No-Break">classification task.</span></p>
			<p>The <strong class="source-inline">zero_shot_evaluate</strong> function performs <strong class="bold">zero-shot inference</strong> using a pre-trained language model by<a id="_idIndexMarker781"/> constructing a prompt that describes the task and presents an input without any labeled examples. It first formats the prompt by combining <strong class="source-inline">task_description</strong> with <strong class="source-inline">test_instance</strong>, ensuring that the model understands the task and what it needs to classify. The phrase <strong class="source-inline">"Output:"</strong> is appended to signal where the model should generate its response. The prompt is then tokenized using tokenizer.encode, converting it into numerical input tensors that the model can process. The function uses <strong class="source-inline">torch.no_grad()</strong> to disable gradient computation, making inference more efficient. The <strong class="source-inline">model.generate</strong> function takes the tokenized prompt and generates an output sequence with a maximum length of <strong class="source-inline">100</strong> tokens while returning only one sequence. The generated output is then decoded back into text using <strong class="source-inline">tokenizer.decode</strong>, ensuring that any special tokens are removed. Finally, the function extracts and returns the portion of the generated text that appears after <strong class="source-inline">"Output:"</strong>, which represents the model’s predicted classification. In the example usage, the function is applied to a classification task where the model is asked to categorize a given text snippet<strong class="source-inline">—"NASA's Mars rover has discovered traces of ancient microbial life."</strong>—into one of the predefined categories: <strong class="source-inline">Science</strong>, <strong class="source-inline">Politics</strong>, <strong class="source-inline">Sports</strong>, or <strong class="source-inline">Entertainment</strong>. The model, without seeing any labeled examples, infers the correct category based on its prior knowledge. The<a id="_idIndexMarker782"/> output is then printed, demonstrating <a id="_idIndexMarker783"/>the model’s zero-shot <span class="No-Break">classification ability.</span></p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor258"/>Domain and task generalization</h1>
			<p>Assessing how well<a id="_idIndexMarker784"/> an LLM generalizes across<a id="_idIndexMarker785"/> different domains and tasks is crucial for understanding its true capabilities. Let’s explore some techniques for <span class="No-Break">this purpose.</span></p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor259"/>Evaluating domain adaptation</h2>
			<p>To <a id="_idIndexMarker786"/>evaluate <strong class="bold">domain adaptation</strong>, we can test the model on data from a different domain than it was trained on. Here’s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
def evaluate_domain_adaptation(
    model, tokenizer, source_domain_data, target_domain_data
):
    def predict(text):
        inputs = tokenizer(
            text, return_tensors='pt', truncation=True, padding=True
        )
        outputs = model(inputs)
        return torch.argmax(outputs.logits, dim=1).item()
    # Evaluate on source domain
    source_predictions = [
        predict(text) for text in source_domain_data['text']
    ]
    source_accuracy = accuracy_score(
        source_domain_data['label'], source_predictions
)
    # Evaluate on target domain
    target_predictions = [
        predict(text) for text in target_domain_data['text']
    ]
    target_accuracy = accuracy_score(
        target_domain_data['label'], target_predictions
)
    return {
        'source_accuracy': source_accuracy,
        'target_accuracy': target_accuracy,
        'adaptation_drop': source_accuracy - target_accuracy
    }</pre>			<p>The following is how we evaluate domain adaptation and print out <span class="No-Break">the result:</span></p>
			<pre class="source-code">
source_domain_data = load_source_domain_data()  # Replace with actual data loading
target_domain_data = load_target_domain_data()  # Replace with actual data loading
results = evaluate_domain_adaptation(
    model, tokenizer, source_domain_data, target_domain_data
)
print(f"Source domain accuracy: {results['source_accuracy']:.2f}")
print(f"Target domain accuracy: {results['target_accuracy']:.2f}")
print(f"Adaptation drop: {results['adaptation_drop']:.2f}")</pre>			<p>Putting it together, we<a id="_idIndexMarker787"/> can use the preceding code to evaluate the model’s performance on both the source domain (what it was trained on) and the target domain, calculating the drop in performance as a measure of <span class="No-Break">domain adaptation.</span></p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor260"/>Evaluating task generalization</h2>
			<p>To assess <strong class="bold">task generalization</strong>, we can <a id="_idIndexMarker788"/>evaluate the model on a variety of tasks it wasn’t specifically fine-tuned for. Here’s an example using the GLUE benchmark (which we discussed in <a href="B31249_14.xhtml#_idTextAnchor230"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">):</span></p>
			<pre class="source-code">
def evaluate_task_generalization(
    model_name, tasks=['mnli', 'qqp', 'qnli', 'sst2']
):
    results = {}
    for task in tasks:
        model = \
            AutoModelForSequenceClassification.from_pretrained(
            model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        dataset = load_dataset('glue', task)
        def tokenize_function(examples):
            return tokenizer(
                examples['sentence'], truncation=True, padding=True)
        tokenized_datasets = dataset.map(tokenize_function,
            batched=True)
        training_args = TrainingArguments(
            output_dir=f"./results_{task}",
            evaluation_strategy="epoch",
            num_train_epochs=1,
        )
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_datasets['train'],
            eval_dataset=tokenized_datasets['validation'],
        )
        eval_results = trainer.evaluate()
        results[task] = eval_results['eval_accuracy']
    return results</pre>			<p>The following is how to run the evaluation based on the previously <span class="No-Break">defined function:</span></p>
			<pre class="source-code">
model_name = "bert-base-uncased"  # Replace with your model
generalization_results = evaluate_task_generalization(model_name)
for task, accuracy in generalization_results.items():
    print(f"{task} accuracy: {accuracy:.2f}")</pre>			<p>Putting together the <a id="_idIndexMarker789"/>preceding code, we can evaluate the model on multiple GLUE tasks to assess its ability to generalize across different <span class="No-Break">NLP tasks.</span></p>
			<h1 id="_idParaDest-201"><a id="_idTextAnchor261"/>Continual learning evaluation</h1>
			<p><strong class="bold">Continual learning</strong> is the<a id="_idIndexMarker790"/> ability of a<a id="_idIndexMarker791"/> model to learn new tasks without forgetting previously learned ones. Here’s an example of how you might evaluate continual learning <span class="No-Break">in LLMs:</span></p>
			<ol>
				<li>Set up our continual learning framework by initializing the model, the tokenizer, and the main <span class="No-Break">function structure:</span><pre class="source-code">
def evaluate_continual_learning(
    model_name, tasks=['sst2', 'qnli', 'qqp'], num_epochs=3
):
    model = \
        AutoModelForSequenceClassification.from_
        pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    results = {}</pre></li>				<li>Define the preprocessing function that handles different input formats for various <span class="No-Break">GLUE tasks:</span><pre class="source-code">
def preprocess_function(examples, task):
    # Different tasks have different input formats
    if task == 'qqp':
        texts = (examples['question1'], examples['question2'])
    elif task == 'qnli':
        texts = (examples['question'], examples['sentence'])
    else:  # sst2
        texts = (examples['sentence'], None)
    tokenized = tokenizer(*texts, padding=True, truncation=True)
    tokenized['labels'] = examples['label']
    return tokenized</pre></li>				<li>Preprocess<a id="_idIndexMarker792"/> and prepare <a id="_idIndexMarker793"/>the dataset for <span class="No-Break">each task:</span><pre class="source-code">
for task in tasks:
    dataset = load_dataset('glue', task)
    tokenized_dataset = dataset.map(
        lambda x: preprocess_function(x, task),
        batched=True,
        remove_columns=dataset['train'].column_names
    )
    model.config.num_labels = 3 if task == 'mnli' else 2</pre></li>				<li>Provide the training setup and execution for <span class="No-Break">each task:</span><pre class="source-code">
trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir=f"./results_{task}",
        num_train_epochs=num_epochs,
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        evaluation_strategy="epoch"
    ),
    train_dataset=tokenized_dataset['train'],
    eval_dataset=tokenized_dataset['validation']
)
trainer.train()</pre></li>				<li>Conduct<a id="_idIndexMarker794"/> an <a id="_idIndexMarker795"/>evaluation across all previously <span class="No-Break">seen tasks:</span><pre class="source-code">
task_results = {}
for eval_task in tasks[:tasks.index(task)+1]:
    eval_dataset = load_dataset('glue', eval_task)['validation']
    eval_tokenized = eval_dataset.map(
        lambda x: preprocess_function(x, eval_task),
        batched=True,
        remove_columns=eval_dataset.column_names
    )
    eval_results = trainer.evaluate(eval_dataset=eval_tokenized)
    task_results[eval_task] = eval_results['eval_accuracy']
results[task] = task_results</pre></li>				<li>Run the evaluation and display <span class="No-Break">the results:</span><pre class="source-code">
model_name = "bert-base-uncased"  # Replace with your model
cl_results = evaluate_continual_learning(model_name)
for task, task_results in cl_results.items():
    print(f"\nAfter training on {task}:")
    for eval_task, accuracy in task_results.items():
        print(f"  {eval_task} accuracy: {accuracy:.2f}")</pre></li>			</ol>
			<p>Putting the preceding code blocks together, we show how to fine-tune the model on a sequence <a id="_idIndexMarker796"/>of tasks<a id="_idIndexMarker797"/> and evaluate its performance on all previously seen tasks after each fine-tuning step, allowing us to assess how well it retains knowledge of <span class="No-Break">earlier tasks.</span></p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor262"/>Cross-validation challenges and best practices</h1>
			<p>LLMs present <a id="_idIndexMarker798"/>unique challenges for cross-validation due to their scale and the nature of their training data. Here are some <span class="No-Break">key challenges:</span></p>
			<ul>
				<li><strong class="bold">Data contamination</strong>: Avoiding test set overlap with pre-training data is difficult given the vast and diverse web data LLMs are trained on, making it hard to ensure a truly unseen <span class="No-Break">validation set</span></li>
				<li><strong class="bold">Computational cost</strong>: Traditional methods such as k-fold cross-validation are often infeasible due to the immense computational resources required for models of <span class="No-Break">this scale</span></li>
				<li><strong class="bold">Domain shift</strong>: LLMs may show inconsistent performance when exposed to data from underrepresented or entirely new domains, complicating the evaluation <span class="No-Break">of generalizability</span></li>
				<li><strong class="bold">Prompt sensitivity</strong>: The performance of LLMs can vary significantly based on subtle differences in prompt wording, adding another layer of variability to the <span class="No-Break">validation process</span></li>
			</ul>
			<p>Based on these challenges, here are some best practices for <span class="No-Break">LLM cross-validation:</span></p>
			<ul>
				<li><strong class="bold">Mitigate data contamination</strong>: Use rigorous data deduplication methods to identify and remove overlaps between the pre-training corpus and validation datasets. Tools such as MinHash or Bloom filters can efficiently detect near-duplicates<a id="_idIndexMarker799"/> in <span class="No-Break">large datasets.</span></li>
			</ul>
			<p class="callout-heading">MinHash</p>
			<p class="callout"><strong class="bold">MinHash</strong> is a probabilistic<a id="_idIndexMarker800"/> technique for quickly estimating how similar two sets are by converting large sets into smaller, representative fingerprints (<strong class="bold">hashes</strong>) where the<a id="_idIndexMarker801"/> probability of hash collision is proportional to the similarity between the original sets, making it particularly useful for detecting near-duplicate content in <span class="No-Break">large datasets.</span></p>
			<p class="callout"><strong class="bold">MinHashLSH</strong> is based <a id="_idIndexMarker802"/>on MinHash and <strong class="bold">locality-sensitive hashing</strong> (<strong class="bold">LSH</strong>), which <a id="_idIndexMarker803"/>groups similar items into the same “buckets” to enable fast lookup <span class="No-Break">and comparison.</span></p>
			<p class="list-inset">The following code example demonstrates data deduplication using MinHash and MinHashLSH for detecting near-duplicates <span class="No-Break">in datasets:</span></p>
			<pre class="source-code">
from datasketch import MinHash, MinHashLSH
import numpy as np
def deduplicate_data(texts, threshold=0.8):
    # Initialize LSH index for fast similarity search
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    unique_texts = []
    for idx, text in enumerate(texts):
        minhash = MinHash(num_perm=128)
        for ngram in get_ngrams(text):
            minhash.update(ngram.encode('utf8'))
        if not lsh.query(minhash):  # Check if similar text exists
            lsh.insert(str(idx), minhash)
            unique_texts.append(text)
    return unique_texts</pre>			<ul>
				<li><strong class="bold">Reduce computational cost</strong>: Use stratified sampling or a single-split validation (e.g., train-validation-test) approach to minimize computational overhead. Alternatively, employ <a id="_idIndexMarker804"/>smaller model checkpoints or distilled versions of the LLM during experimentation before <span class="No-Break">scaling up.</span><p class="list-inset">The following code example shows stratified sampling for <span class="No-Break">efficient validation:</span></p><pre class="source-code">
from sklearn.model_selection import StratifiedKFold
from collections import defaultdict
def create_efficient_splits(data, labels, n_splits=5):
    # Group data by domain
    domain_data = defaultdict(list)
    for text, domain in zip(data, labels):
        domain_data[domain].append(text)
    # Create stratified splits
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)
    splits = []
    for train_idx, val_idx in skf.split(data, labels):
        splits.append((train_idx, val_idx))
    return splits</pre></li>				<li><strong class="bold">Handle domain shift</strong>: Construct validation datasets with explicit representation from diverse domains. Fine-tune models with representative domain-specific data to reduce performance gaps in <span class="No-Break">underrepresented areas.</span><p class="list-inset">This code<a id="_idIndexMarker805"/> example demonstrates handling domain shift through <span class="No-Break">domain-specific validation:</span></p><pre class="source-code">
def evaluate_domain_performance(model, tokenizer, eval_data):
    domain_scores = defaultdict(list)
    for text, domain in eval_data:
        inputs = tokenizer(text, return_tensors='pt')
        with torch.no_grad():
            outputs = model(inputs)
            score = outputs.logits.mean().item()
            domain_scores[domain].append(score)
    # Calculate domain-specific metrics
    return {domain: np.mean(scores)
        for domain, scores in domain_scores.items()}</pre></li>				<li><strong class="bold">Address prompt sensitivity</strong>: Perform prompt engineering systematically. Use techniques such as prompt paraphrasing, instruction tuning, or ensemble evaluation across multiple prompts to ensure robustness and minimize the variability introduced by <span class="No-Break">prompt changes.</span><p class="list-inset">The following<a id="_idIndexMarker806"/> code example shows systematic prompt engineering with <span class="No-Break">multiple variants:</span></p><pre class="source-code">
def evaluate_with_prompt_ensemble(
    model, tokenizer, text, base_prompt
):
    prompt_variants = [
        f"{base_prompt}: {text}",
        f"Please {base_prompt.lower()}: {text}",
        f"I want you to {base_prompt.lower()}: {text}"
    ]
    responses = []
    for prompt in prompt_variants:
        inputs = tokenizer(prompt, return_tensors='pt')
        with torch.no_grad():
            output = model.generate(inputs, max_length=100)
            responses.append(tokenizer.decode(output[0]))
    # Aggregate responses (e.g., by voting or averaging)
    return aggregate_responses(responses)</pre></li>			</ul>
			<p>The following code example shows how to combine all these approaches into a single <span class="No-Break">evaluation </span><span class="No-Break"><a id="_idIndexMarker807"/></span><span class="No-Break">pipeline:</span></p>
			<pre class="source-code">
def robust_evaluation_pipeline(model, data, domains):
    # First deduplicate the data
    clean_data = deduplicate_data(data)
    # Create efficient splits
    splits = create_efficient_splits(clean_data, domains)
    # Evaluate across domains with prompt ensembles
    results = defaultdict(dict)
    for domain in domains:
        domain_data = [d for d, dom in zip(clean_data, domains)
            if dom == domain]
        scores = evaluate_with_prompt_ensemble(model, tokenizer,
            domain_data, "analyze")
        results[domain] <a id="_idTextAnchor263"/>= scores
    return results</pre>			<h1 id="_idParaDest-203"><a id="_idTextAnchor264"/>Summary</h1>
			<p>Cross-validation for LLMs requires careful consideration of their unique characteristics and capabilities. By implementing these advanced techniques and best practices, you can obtain a more robust and comprehensive assessment of your LLM’s performance across various domains <span class="No-Break">and tasks.</span></p>
			<p>As we move forward, the next chapter will delve into the crucial topic of interpretability in LLMs. We’ll explore techniques for understanding and explaining the outputs and behaviors <span class="No-Break">of LLMs.</span></p>
		</div>
	</div></div></body></html>