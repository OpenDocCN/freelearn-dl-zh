<html><head></head><body>
<div id="_idContainer154">
<h1 class="chapter-number" id="_idParaDest-78"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.2.1">Extending Your Agent with RAG to Prevent Hallucinations</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In earlier chapters, we saw what an LLM is, and in the previous chapter, we saw how it can control different tools to succeed at completing a task. </span><span class="koboSpan" id="kobo.3.2">However, some of the limitations of LLMs prevent their deployment in sensitive fields such as medicine. </span><span class="koboSpan" id="kobo.3.3">For example, LLMs crystallize their knowledge at the time of training, and rapidly developing fields such as medical sciences cause this knowledge to be outdated in a short time. </span><span class="koboSpan" id="kobo.3.4">Another problem that has emerged with the use of LLMs is that they can often hallucinate (produce answers that contain factual or conceptual errors). </span><span class="koboSpan" id="kobo.3.5">To overcome these limitations, a new paradigm has emerged: </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">retrieval-augmented generation</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">RAG</span></strong><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">RAG, as </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.8.1">we will see in this chapter, allows for the LLM to refer to memory that is external to the model; thus, it allows knowledge to be found and kept updated. </span><span class="koboSpan" id="kobo.8.2">Similarly, providing contextual guidance to the model’s response allows for the reduction of hallucinations. </span><span class="koboSpan" id="kobo.8.3">Therefore, RAG is widely used today and is considered a </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">promising system.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">In this chapter, we will discuss how this system has evolved, starting with how a transformer can be used to find information. </span><span class="koboSpan" id="kobo.10.2">We will discuss in detail the various components of the system (embedding, vector database, </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">and generation).</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">In this chapter, we’ll be covering the </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.14.1">Exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">naïve RAG</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Retrieval, optimization, </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">and augmentation</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">the output</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Comparison between RAG </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">and fine-tuning</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Using RAG to build a movie </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">recommendation agent</span></span></li>
</ul>
<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.24.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.25.1">Most of this code can be run on a CPU, but it is preferable to be run on a GPU. </span><span class="koboSpan" id="kobo.25.2">The code is written in PyTorch and uses standard libraries for the most part (PyTorch, Hugging Face Transformers, LangChain, SentencePiece, Datasets, and scikit-learn). </span><span class="koboSpan" id="kobo.25.3">The code can be found on </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr5"><span class="No-Break"><span class="koboSpan" id="kobo.27.1">https://github.com/PacktPublishing/Modern-AI-Agents/tree/main/chr5</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.28.1">.</span></span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/><span class="koboSpan" id="kobo.29.1">Exploring naïve RAG</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.30.1">Information retrieval</span></strong><span class="koboSpan" id="kobo.31.1"> is the </span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.32.1">name of the scientific field that deals with finding information in media (often textual but also multimodal). </span><span class="koboSpan" id="kobo.32.2">For example, the user may be interested in</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.33.1"> finding whole documents or chunks in documents; this task is key to question answering, where a model has to find the steps needed to answer a user’s questions. </span><span class="koboSpan" id="kobo.33.2">At the heart of the system is a search engine. </span><span class="koboSpan" id="kobo.33.3">In the case of RAG, the search engine is a transformer (or at least a language model), and in this chapter, we </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.34.1">will focus on that. </span><span class="koboSpan" id="kobo.34.2">We will discuss a system in which we have a </span><strong class="bold"><span class="koboSpan" id="kobo.35.1">collection</span></strong><span class="koboSpan" id="kobo.36.1"> of documents (textual, but could also be web pages, images, videos, or even code or short text passages) that have corresponding indexes in the database. </span><span class="koboSpan" id="kobo.36.2">These documents can be associated with metadata (attributes describing author, size, topic, and keywords). </span><span class="koboSpan" id="kobo.36.3">By convention, a </span><strong class="bold"><span class="koboSpan" id="kobo.37.1">term</span></strong><span class="koboSpan" id="kobo.38.1"> is </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.39.1">defined as a word present in the text but also a passage that can answer the search. </span><span class="koboSpan" id="kobo.39.2">A user produces a </span><strong class="bold"><span class="koboSpan" id="kobo.40.1">query</span></strong><span class="koboSpan" id="kobo.41.1"> that </span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.42.1">can be expressed as terms. </span><span class="koboSpan" id="kobo.42.2">The purpose of the retrieval system is to best match the query with the relevant documents in the collection. </span><span class="koboSpan" id="kobo.42.3">These are then returned in order </span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">of relevance.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<span class="koboSpan" id="kobo.44.1"><img alt="Figure 5.1 – Workflow diagram showing how a query is processed by a search" src="image/B21257_05_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.45.1">Figure 5.1 – Workflow diagram showing how a query is processed by a search</span></p>
<p><span class="koboSpan" id="kobo.46.1">Let’s break down </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.47.1">what we can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.48.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.49.1">.1</span></em><span class="koboSpan" id="kobo.50.1">. </span><span class="koboSpan" id="kobo.50.2">A collection of documents (</span><strong class="bold"><span class="koboSpan" id="kobo.51.1">A</span></strong><span class="koboSpan" id="kobo.52.1">) is indexed (</span><strong class="bold"><span class="koboSpan" id="kobo.53.1">B</span></strong><span class="koboSpan" id="kobo.54.1">) and is entered in an orderly manner into a database (</span><strong class="bold"><span class="koboSpan" id="kobo.55.1">C</span></strong><span class="koboSpan" id="kobo.56.1">). </span><span class="koboSpan" id="kobo.56.2">Each document is assigned metadata and indexes. </span><span class="koboSpan" id="kobo.56.3">A user query (</span><strong class="bold"><span class="koboSpan" id="kobo.57.1">D</span></strong><span class="koboSpan" id="kobo.58.1">) is processed (</span><strong class="bold"><span class="koboSpan" id="kobo.59.1">E</span></strong><span class="koboSpan" id="kobo.60.1">) to obtain a vector representation (</span><strong class="bold"><span class="koboSpan" id="kobo.61.1">F</span></strong><span class="koboSpan" id="kobo.62.1">). </span><span class="koboSpan" id="kobo.62.2">The resulting vector is used during the search to find the documents that are most relevant (</span><strong class="bold"><span class="koboSpan" id="kobo.63.1">G</span></strong><span class="koboSpan" id="kobo.64.1">). </span><span class="koboSpan" id="kobo.64.2">The system returns the documents in order of </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">relevance (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.66.1">H</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">)</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">As we can observe, the system uses a search in a vector space. </span><span class="koboSpan" id="kobo.68.2">In the simplest form, this can be bag-of-words or the TF-IDF we saw in the first chapter. </span><span class="koboSpan" id="kobo.68.3">For example, we can take a set of documents and calculate the TF-IDF. </span><span class="koboSpan" id="kobo.68.4">Once we’ve done that, we can calculate a score (usually cosine similarity) between each of the documents and conduct rank based on the score. </span><span class="koboSpan" id="kobo.68.5">For a document d and a query q in vector form, we use the </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">following formula:</span></span></p>
<p><span class="_-----MathTools-_Math_Function_v-normal"><math display="block"><mrow><mrow><mi>cos</mi><mfenced close=")" open="("><mrow><mi mathvariant="bold-italic">q</mi><mo>,</mo><mi mathvariant="bold-italic">d</mi></mrow></mfenced><mo>=</mo><mfrac><mrow><mi mathvariant="bold-italic">q</mi><mo>∙</mo><mi mathvariant="bold-italic">d</mi></mrow><mrow><mfenced close="|" open="|"><mi mathvariant="bold-italic">q</mi></mfenced><mfenced close="|" open="|"><mi mathvariant="bold-italic">d</mi></mfenced></mrow></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.70.1">We can see an example of this </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">process here:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<span class="koboSpan" id="kobo.72.1"><img alt="Figure 5.2 – Example of retrieving the most relevant documents with TF-IDF" src="image/B21257_05_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.73.1">Figure 5.2 – Example of retrieving the most relevant documents with TF-IDF</span></p>
<p><span class="koboSpan" id="kobo.74.1">This type of research also requires data storage facilities that are suitable. </span><span class="koboSpan" id="kobo.74.2">For example, for TF-IDF (or derivative algorithms), an inverted index is used as the data structure. </span><span class="koboSpan" id="kobo.74.3">The inverted index is a data structure designed specifically to make it efficient to search for terms in a set of documents. </span><span class="koboSpan" id="kobo.74.4">It is a structure composed of a dictionary and postings. </span><span class="koboSpan" id="kobo.74.5">The dictionary indicates the frequency of terms and the posting in which document they are found. </span><span class="koboSpan" id="kobo.74.6">In this way, given a set of terms in the query, we can efficiently find the documents that contain them and calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">the similarity.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<span class="koboSpan" id="kobo.76.1"><img alt="Figure 5.3 – Example of an inverted index" src="image/B21257_05_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.77.1">Figure 5.3 – Example of an inverted index</span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.78.1">BM25</span></strong><span class="koboSpan" id="kobo.79.1"> is a</span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.80.1"> variant of </span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.81.1">TF-IDF where two parameters are added: </span><em class="italic"><span class="koboSpan" id="kobo.82.1">b</span></em><span class="koboSpan" id="kobo.83.1">, which controls the</span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.84.1"> importance of document length normalization, and </span><em class="italic"><span class="koboSpan" id="kobo.85.1">k</span></em><span class="koboSpan" id="kobo.86.1">, which controls the relationship</span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.87.1"> between </span><strong class="bold"><span class="koboSpan" id="kobo.88.1">term frequency</span></strong><span class="koboSpan" id="kobo.89.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.90.1">TF</span></strong><span class="koboSpan" id="kobo.91.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">inverse document </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.93.1">frequency</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.94.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.95.1">IDF</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">).</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>B</mi><mi>M</mi><mn>25</mn><mi>s</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mo>=</mo><mrow><munder><mo>∑</mo><mrow><mi>t</mi><mi mathvariant="normal">ϵ</mi><mi>q</mi></mrow></munder><mover><mover><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo>(</mo><mfrac><mi>N</mi><msub><mrow><mi>d</mi><mi>f</mi></mrow><mi>t</mi></msub></mfrac><mo>)</mo></mrow><mo stretchy="true">⏞</mo></mover><mrow><mi>I</mi><mi>D</mi><mi>F</mi></mrow></mover></mrow><mover><mover><mfrac><msub><mrow><mi>t</mi><mi>f</mi></mrow><mrow><mi>t</mi><mo>,</mo><mi>d</mi></mrow></msub><mrow><mi>k</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>b</mi><mo>+</mo><mi>b</mi><mo>(</mo><mstyle scriptlevel="+1"><mfrac><mrow><mo>|</mo><mi>d</mi><mo>|</mo></mrow><mrow><mo>|</mo><msub><mi>d</mi><mrow><mi>a</mi><mi>v</mi><mi>g</mi></mrow></msub><mo>|</mo></mrow></mfrac></mstyle><mo>)</mo><mo>)</mo><mo>+</mo><msub><mrow><mi>t</mi><mi>f</mi></mrow><mrow><mi>t</mi><mo>,</mo><mi>d</mi></mrow></msub></mrow></mfrac><mo stretchy="true">⏞</mo></mover><mrow><mi>T</mi><mi>F</mi></mrow></mover></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.97.1">The preceding equation is a variation of TF-IDF for a document </span><em class="italic"><span class="koboSpan" id="kobo.98.1">d</span></em><span class="koboSpan" id="kobo.99.1"> and a query </span><em class="italic"><span class="koboSpan" id="kobo.100.1">q</span></em><span class="koboSpan" id="kobo.101.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.102.1">d avg</span></em><span class="koboSpan" id="kobo.103.1"> represents the average length of </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">a document).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<span class="koboSpan" id="kobo.105.1"><img alt="Figure 5.4 – Effect of k and b parameters on the BM25 score" src="image/B21257_05_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.106.1">Figure 5.4 – Effect of k and b parameters on the BM25 score</span></p>
<p><span class="koboSpan" id="kobo.107.1">We can see some </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">interesting points:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.109.1">By selecting </span><em class="italic"><span class="koboSpan" id="kobo.110.1">k</span></em><span class="koboSpan" id="kobo.111.1"> equal to zero, no TF is used in the score. </span><span class="koboSpan" id="kobo.111.2">The TF component becomes irrelevant; the score does not consider how often a term appears in a document, only whether it appears at all. </span><span class="koboSpan" id="kobo.111.3">Higher </span><em class="italic"><span class="koboSpan" id="kobo.112.1">k</span></em><span class="koboSpan" id="kobo.113.1"> values give greater weight to TF. </span><em class="italic"><span class="koboSpan" id="kobo.114.1">k</span></em><span class="koboSpan" id="kobo.115.1"> is used to adjust TF saturation – in other words, how much a single query term impacts the score of a single document. </span><em class="italic"><span class="koboSpan" id="kobo.116.1">b=1</span></em><span class="koboSpan" id="kobo.117.1"> means normalize for document length, while 0 means </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">eliminate normalization.</span></span></li>
<li><span class="koboSpan" id="kobo.119.1">The system is sensitive to both TF and document length without adding too many parameters. </span><span class="koboSpan" id="kobo.119.2">The usually recommended values are </span><em class="italic"><span class="koboSpan" id="kobo.120.1">b=0.75</span></em><span class="koboSpan" id="kobo.121.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.122.1">k</span></em><span class="koboSpan" id="kobo.123.1"> between 1.2 and 2. </span><span class="koboSpan" id="kobo.123.2">BM25 is much more flexible than TF-IDF and can be adapted to </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">different scenarios.</span></span></li>
<li><span class="koboSpan" id="kobo.125.1">It is not much more complex than TF-IDF and is therefore scalable to large datasets, and more robust to </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">sparse matrices.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.127.1">It is not always easy to</span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.128.1"> find the optimal parameters for a precise dataset. </span><span class="koboSpan" id="kobo.128.2">The model is sensitive to the choice of hyperparameters. </span><span class="koboSpan" id="kobo.128.3">BM25 has a limited understanding of semantics since it is based on term frequency, not capturing the meaning of a document. </span><span class="koboSpan" id="kobo.128.4">Also, many terms are polysemous (with multiple meanings) and BM25 does not capture the context of a term. </span><span class="koboSpan" id="kobo.128.5">Another serious problem is a vocabulary mismatch problem – that is, when there is no complete overlap between terms in the query </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">and documents.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">The solution to these problems is to use dense vectors that include contextual information. </span><span class="koboSpan" id="kobo.130.2">This is done by using a transformer and extracting the representation for a document. </span><span class="koboSpan" id="kobo.130.3">More formally, given a sequence of tokens, we use the representation </span><em class="italic"><span class="koboSpan" id="kobo.131.1">z</span></em><span class="koboSpan" id="kobo.132.1">, obtained from the final layer. </span><span class="koboSpan" id="kobo.132.2">This allows us to obtain a high-dimensional representation that we can use to disambiguate the meaning of a word. </span><span class="koboSpan" id="kobo.132.3">This is called </span><a id="_idIndexMarker490"/><span class="No-Break"><span class="koboSpan" id="kobo.133.1">the </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.134.1">z-score</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<span class="koboSpan" id="kobo.136.1"><img alt="Figure 5.5 – Contextual embeddings for the word “bank”" src="image/B21257_05_05.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.137.1">Figure 5.5 – Contextual embeddings for the word “bank”</span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.138.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.139.1">.5</span></em><span class="koboSpan" id="kobo.140.1"> shows a </span><strong class="bold"><span class="koboSpan" id="kobo.141.1">t-distributed stochastic neighbor embedding</span></strong><span class="koboSpan" id="kobo.142.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.143.1">t-SNE</span></strong><span class="koboSpan" id="kobo.144.1">)</span><strong class="bold"><span class="koboSpan" id="kobo.145.1"> visualization</span></strong><span class="koboSpan" id="kobo.146.1"> of the </span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.147.1">contextual embedding for the word “bank” in different contexts (both money and river-related meanings). </span><span class="koboSpan" id="kobo.147.2">The t-SNE is conducted on the BERT embedding of the word for </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">each sentence.</span></span></p>
<p><span class="koboSpan" id="kobo.149.1">There are several ways </span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.150.1">to retrieve this representation from the model. </span><span class="koboSpan" id="kobo.150.2">For convenience, the last layer is used, but it is generally proposed to conduct an average pool of the representation of multiple layers (each block learns a different text representation due to self-attention). </span><span class="koboSpan" id="kobo.150.3">As we saw in </span><a href="B21257_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.151.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.152.1">, these vectors have geometric properties and can be used for operations (clustering, similarity computation, and </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.154.1">Generally, some transformation is conducted to optimize the use of embedding. </span><span class="koboSpan" id="kobo.154.2">For example, a normalization of vectors (z-score or other methods) is conducted. </span><span class="koboSpan" id="kobo.154.3">In fact, the vectors of many words are similar due to anisotropy. </span><span class="koboSpan" id="kobo.154.4">In fact, taking random words, the cosine similarity is higher than it should be. </span><span class="koboSpan" id="kobo.154.5">This is due to rogue dimensions, a small number of dimensions (1–5) that dominate contextual embedding because they have high magnitude and disproportionately high variance. </span><span class="koboSpan" id="kobo.154.6">This causes similarity to be calculated on reduced embedding space. </span><span class="koboSpan" id="kobo.154.7">These rogue dimensions are highly correlated with absolute position and punctuation and are therefore uninformative. </span><span class="koboSpan" id="kobo.154.8">Transformations such as z-score can reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">the problem.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer138">
<span class="koboSpan" id="kobo.156.1"><img alt="Figure 5.6 – Relative contribution of each dimension to cosine similarity (https://aclanthology.org/2021.emnlp-main.372.pdf)" src="image/B21257_05_06.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.157.1">Figure 5.6 – Relative contribution of each dimension to cosine similarity (</span><a href="https://aclanthology.org/2021.emnlp-main.372.pdf"><span class="koboSpan" id="kobo.158.1">https://aclanthology.org/2021.emnlp-main.372.pdf</span></a><span class="koboSpan" id="kobo.159.1">)</span></p>
<p><span class="koboSpan" id="kobo.160.1">In addition, retrieving the </span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.161.1">embedding for each word in the embedding is unnecessarily laborious. </span><span class="koboSpan" id="kobo.161.2">For bidirectional encoders, we can use two main strategies: use a single encoder or a bi-encoder. </span><span class="koboSpan" id="kobo.161.3">In the first case, we provide the model with both query and document, thus allowing bidirectional self-attention to attend all tokens. </span><span class="koboSpan" id="kobo.161.4">The representation will be representative of both the query and the document. </span><span class="koboSpan" id="kobo.161.5">The format used is [CLS]-query-[SEP]-document. </span><span class="koboSpan" id="kobo.161.6">The representation for the [CLS] token is then fed to a linear layer to produce the similarity score (this layer is fine-tuned). </span><span class="koboSpan" id="kobo.161.7">Normally, this process is done not for the whole document but for a series of chunks (non-overlapping fragments of the document), because documents are usually longer than the context length (for BERT, this is 512 tokens, so the sum of query and document must be no more than </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">512 tokens).</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">This system is expensive because it requires that we have to pass a query along with the entire corpus of documents. </span><span class="koboSpan" id="kobo.163.2">To reduce the cost, a more efficient architecture known as a bi-encoder was implemented. </span><span class="koboSpan" id="kobo.163.3">One encoder is used to extract the representation for the query, [CLS]q, and another to extract the representation for each document (or chunk), [CLS]d. </span><span class="koboSpan" id="kobo.163.4">Basically, taking a corpus, we compute the embedding for each document in the corpus and store this representation in a database. </span><span class="koboSpan" id="kobo.163.5">After that, we compute the cosine similarity between the representation for the query and all the vectors in the database. </span><span class="koboSpan" id="kobo.163.6">This system is much faster but less accurate because part of the interactions there are between the terms in the query and in </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">the documents.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<span class="koboSpan" id="kobo.165.1"><img alt="Figure 5.7 – Two different approaches for contextual embedding" src="image/B21257_05_07.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.166.1">Figure 5.7 – Two different approaches for contextual embedding</span></p>
<p><span class="koboSpan" id="kobo.167.1">Let’s examine </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.168.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.169.1">.7</span></em><span class="koboSpan" id="kobo.170.1"> in more</span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.171.1"> detail. </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">A</span></strong><span class="koboSpan" id="kobo.173.1"> is a unique model for generating the contextual vector. </span><span class="koboSpan" id="kobo.173.2">A linear layer is fine-tuned to generate a score of similarity between query and vector. </span><span class="koboSpan" id="kobo.173.3">The linear layer has as input the </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">[CLS]</span></strong><span class="koboSpan" id="kobo.175.1"> representation. </span><strong class="bold"><span class="koboSpan" id="kobo.176.1">B</span></strong><span class="koboSpan" id="kobo.177.1"> shows two models, which are used to generate two different and separate representations. </span><span class="koboSpan" id="kobo.177.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">[CLS]</span></strong><span class="koboSpan" id="kobo.179.1"> representation is generated for the query and all the vectors. </span><span class="koboSpan" id="kobo.179.2">We calculate cosine similarity using </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">both representations.</span></span></p>
<p><span class="koboSpan" id="kobo.181.1">Then, we can conduct embedding of the whole corpus and index the documents in a database and then, when a query comes, calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">the similarity.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer140">
<span class="koboSpan" id="kobo.183.1"><img alt="Figure 5.8 – Cosine similarities between a set of documents and two queries where the meaning of the word “bank” is different" src="image/B21257_05_08.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.184.1">Figure 5.8 – Cosine similarities between a set of documents and two queries where the meaning of the word “bank” is different</span></p>
<p><span class="koboSpan" id="kobo.185.1">As we mentioned earlier, generative </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.186.1">models can produce hallucinations. </span><span class="koboSpan" id="kobo.186.2">Given a query, LLMs can generate output that contains erroneous information. </span><span class="koboSpan" id="kobo.186.3">This stems from the fact that LLMs are good at explaining concepts but have problems retaining specific information. </span><span class="koboSpan" id="kobo.186.4">During training, knowledge of a concept is reinforced by the repetition of similar pieces of information. </span><span class="koboSpan" id="kobo.186.5">This works well for concepts but less so for specific pieces of information such as dates, numerical values, and rare pieces of information. </span><span class="koboSpan" id="kobo.186.6">In addition, datasets contain both correct and incorrect information, often conflicting. </span><span class="koboSpan" id="kobo.186.7">When a model generates a response, it samples from a distribution and must choose from the information it has learned, thus leading </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">to hallucinations.</span></span></p>
<p><span class="koboSpan" id="kobo.188.1">In addition, incorrect architecture, overfitting, or misalignment during training can also promote hallucinations. </span><span class="koboSpan" id="kobo.188.2">Fine-tuning the model or over-optimization for some tasks can be an additional cause. </span><span class="koboSpan" id="kobo.188.3">For example, optimizing the model to write long text outputs promotes the model to become verbose and generate hallucinations. </span><span class="koboSpan" id="kobo.188.4">Similarly, raising the temperature increases the stochasticity of sampling, leading to sample tokens that are less likely and thus hallucinate more. </span><span class="koboSpan" id="kobo.188.5">Incorrect prompting can also promote </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">this behavior.</span></span></p>
<p><span class="koboSpan" id="kobo.190.1">Hallucinations are most evident when using a model in a specific domain (healthcare, finance, and so on). </span><span class="koboSpan" id="kobo.190.2">The model lacks the context to best understand the query. </span><span class="koboSpan" id="kobo.190.3">This is because the model has been trained on a huge number of tokens, and they have not been restricted to specialized topics. </span><span class="koboSpan" id="kobo.190.4">The loss is calculated on the set of texts and thus more on general knowledge than on particular information. </span><span class="koboSpan" id="kobo.190.5">Therefore, the model favors a generalist function but performs less well when applied to a particular domain. </span><span class="koboSpan" id="kobo.190.6">This is a common factor, irrespective of the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">model parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.192.1">Several possible solutions have been tested to reduce or prevent hallucinations. </span><span class="koboSpan" id="kobo.192.2">One approach is to provide context as part of the LLM prompt (when it is possible to add all this context to the prompt). </span><span class="koboSpan" id="kobo.192.3">However, this means that the user has to find the relevant context again. </span><span class="koboSpan" id="kobo.192.4">When you have many different documents, this becomes a complex and laborious task. </span><span class="koboSpan" id="kobo.192.5">Alternatively, fine-tuning, in which the model is trained further on specific documents, has been proposed. </span><span class="koboSpan" id="kobo.192.6">This has a computational cost, though, and should be conducted repeatedly if new </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">documents arrive.</span></span></p>
<p><span class="koboSpan" id="kobo.194.1">In 2020, Meta proposed an </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.195.1">alternative approach: RAG for LLMs. </span><span class="koboSpan" id="kobo.195.2">This approach assumes augmenting the generation of an LLM by finding the context in an external source (such as a database). </span><span class="koboSpan" id="kobo.195.3">This database can be domain-specific and continuously updated. </span><span class="koboSpan" id="kobo.195.4">In other words, we find the documents needed to answer the query and take advantage of the fact that an LLM has powerful abilities for </span><span class="No-Break"><span class="koboSpan" id="kobo.196.1">in-context learning.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer141">
<span class="koboSpan" id="kobo.197.1"><img alt="Figure 5.9 – Diagram showing the process of RAG" src="image/B21257_05_09.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.198.1">Figure 5.9 – Diagram showing the process of RAG</span></p>
<p><span class="koboSpan" id="kobo.199.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.200.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.201.1">.9</span></em><span class="koboSpan" id="kobo.202.1">, the ranked documents are incorporated in the prompt (query, retrieved documents, and additional information), which is presented to the LLM. </span><span class="koboSpan" id="kobo.202.2">The LLM uses the additional context to respond to the query of </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">the user.</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">We define the knowledge from the LLM as parametric memory and that obtained from the RAG as external or nonparametric. </span><span class="koboSpan" id="kobo.204.2">More formally, RAG is a system that, in its most basic form, consists of </span><span class="No-Break"><span class="koboSpan" id="kobo.205.1">three parts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.206.1">Indexing</span></strong><span class="koboSpan" id="kobo.207.1">: Indexing </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.208.1">deals with the entire process from raw data to storage in a vector database. </span><span class="koboSpan" id="kobo.208.2">It begins with ingesting data in various formats (PDF, HTML, Markdown, or XML) that must be converted to text. </span><span class="koboSpan" id="kobo.208.3">The text is processed according to the embedding model chosen (it is divided into chunks that must be smaller in size than the context length of the model). </span><span class="koboSpan" id="kobo.208.4">The chunks are then embedded (transformed into a vector representation), assigned an identifier, and stored in a </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">vector database.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.210.1">Retrieval</span></strong><span class="koboSpan" id="kobo.211.1">: When </span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.212.1">a query arrives, the most relevant chunks must be found. </span><span class="koboSpan" id="kobo.212.2">The same encoder used for document embedding is used to obtain a vector for the query. </span><span class="koboSpan" id="kobo.212.3">The similarity score between the query vector and the vectors stored in the database is then calculated. </span><span class="koboSpan" id="kobo.212.4">Top </span><em class="italic"><span class="koboSpan" id="kobo.213.1">K</span></em><span class="koboSpan" id="kobo.214.1"> chunks are selected based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">similarity score.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.216.1">Generation</span></strong><span class="koboSpan" id="kobo.217.1">: The chunks found together with the query are incorporated into a consistent prompt for LLM</span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.218.1"> used for generation. </span><span class="koboSpan" id="kobo.218.2">Different LLMs may require different elements in the prompt to work best; similarly, we can have prompts that are tailored for specific tasks. </span><span class="koboSpan" id="kobo.218.3">In addition, we can also add elements from the previous </span><span class="No-Break"><span class="koboSpan" id="kobo.219.1">conversation (history).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.220.1">For an autoregressive model, we can modify the equation seen in </span><a href="B21257_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.221.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.222.1">, where we defined that an LLM computes the probability of a sequence of tokens given the </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">previous tokens:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>|</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.224.1">For a question-answering task, given a question (or query) q, we can rewrite the equation </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>p</mi><mo>(</mo><mi>q</mi><mo>;</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.226.1">In RAG, we have additional elements: the prompt Pr, the context retrieved R, and the question q, which are </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">all concatenated:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>p</mi><mfenced close=")" open="("><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>,</mo><msub><mi>x</mi><mn>2</mn></msub><mo>,</mo><mo>…</mo><mo>,</mo><msub><mi>x</mi><mi>n</mi></msub></mrow></mfenced><mo>=</mo><mrow><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mrow><mi>p</mi><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>|</mo><mi>P</mi><mi>r</mi><mo>;</mo><mi>R</mi><mo>;</mo><mi>q</mi><mo>;</mo><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo>)</mo></mrow></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.228.1">This process is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.229.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.230.1">.10</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer142">
<span class="koboSpan" id="kobo.232.1"><img alt="Figure 5.10 – Representative instance of the RAG process and its steps (https://arxiv.org/pdf/2312.10997)" src="image/B21257_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.233.1">Figure 5.10 – Representative instance of the RAG process and its steps (</span><a href="https://arxiv.org/pdf/2312.10997"><span class="koboSpan" id="kobo.234.1">https://arxiv.org/pdf/2312.10997</span></a><span class="koboSpan" id="kobo.235.1">)</span></p>
<p><span class="koboSpan" id="kobo.236.1">This is the general architecture, but there are also more complex variations (which we will discuss in detail </span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.237.1">in the next chapter). </span><span class="koboSpan" id="kobo.237.2">For completeness, an alternative to this architecture is </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">span extraction</span></strong><span class="koboSpan" id="kobo.239.1">. </span><span class="koboSpan" id="kobo.239.2">In this case, instead of finding the most appropriate chunks, we have a language model (usually also derived from BERT) that is used to find passages in the text that answer a</span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.240.1"> query (</span><strong class="bold"><span class="koboSpan" id="kobo.241.1">span labeling</span></strong><span class="koboSpan" id="kobo.242.1">). </span><span class="koboSpan" id="kobo.242.2">For example, if our corpus is Wikipedia and our query is “</span><em class="italic"><span class="koboSpan" id="kobo.243.1">Who is the president of France?</span></em><span class="koboSpan" id="kobo.244.1">”, the extractor will label the passage on the page that answers the question (in RAG, we retrieve the text chunks that are relevant instead). </span><span class="koboSpan" id="kobo.244.2">RAG (or </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">span extractor</span></strong><span class="koboSpan" id="kobo.246.1">) has shown </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.247.1">interesting abilities in reducing hallucinations and improving the abilities of LLMs in open-domain question answering (also called </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">open-book QA).</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">In the next section, we will go on to discuss these steps in more detail and what choices we need to make in order to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">the system.</span></span></p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/><span class="koboSpan" id="kobo.251.1">Retrieval, optimization, and augmentation</span></h1>
<p><span class="koboSpan" id="kobo.252.1">In the previous section, we discussed the high-level RAG paradigm. </span><span class="koboSpan" id="kobo.252.2">In this section, we are going to look at the components in detail and analyze the possible choices a practitioner can make when they want to implement a </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">RAG system.</span></span></p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/><span class="koboSpan" id="kobo.254.1">Chunking strategies</span></h2>
<p><span class="koboSpan" id="kobo.255.1">We have stated that text is divided</span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.256.1"> into chunks before being embedded in the database. </span><span class="koboSpan" id="kobo.256.2">Dividing into chunks has a very important impact on what information is included in the vector and then found during the search. </span><span class="koboSpan" id="kobo.256.3">Chunks </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.257.1">that are too small lose the context of the data, while chunks that are too large are non-specific (and present irrelevant information that also impacts response generation). </span><span class="koboSpan" id="kobo.257.2">This then impacts the retrieval of query-specific information. </span><span class="koboSpan" id="kobo.257.3">The larger the chunking size, the larger the amount of tokens that will be introduced into the prompt and thus an increase in the inference cost (but the computational cost of the database also increases with the number of chunks per document). </span><span class="koboSpan" id="kobo.257.4">Excessive context can also lead to hallucinations and detract from LLM performance. </span><span class="koboSpan" id="kobo.257.5">In addition, the chunk size must not exceed the context length of the embedder, or we will lose information (this is known as truncation). </span><span class="koboSpan" id="kobo.257.6">In other words, chunk size is an important factor that affects both the quality of retrieval </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">and generation.</span></span></p>
<p><span class="koboSpan" id="kobo.259.1">The simplest strategies are those based on a fixed length of chunking. </span><span class="koboSpan" id="kobo.259.2">Character chunking divides the document into chunks based on a predetermined number of characters or tokens (common choices are 100 or 256 tokens or 500 characters). </span><span class="koboSpan" id="kobo.259.3">The size should be chosen according to the type of document. </span><span class="koboSpan" id="kobo.259.4">This is the cheapest and easiest system to implement. </span><span class="koboSpan" id="kobo.259.5">One variation is a random chunk size where the size of the chunks is variable. </span><span class="koboSpan" id="kobo.259.6">This variant can be used when the collection is non-homogenous and potentially captures more semantic context. </span><span class="koboSpan" id="kobo.259.7">Separation into chunks can be with or without overlap. </span><span class="koboSpan" id="kobo.259.8">Chunking without overlap (</span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.260.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.261.1">.11</span></em><span class="koboSpan" id="kobo.262.1">) works well if there are clear boundaries between chunks (such as if the context changes drastically between adjacent chunks). </span><span class="koboSpan" id="kobo.262.2">This is rarely the case, though, and the lack of overlap destroys context. </span><span class="koboSpan" id="kobo.262.3">One can then use a sliding window that maintains an overlap between chunks. </span><span class="koboSpan" id="kobo.262.4">This system maintains contextual information at the chunk boundaries, allowing better semantic content and increasing the chance that relevant information will be found if it spans across multiple chunks. </span><span class="koboSpan" id="kobo.262.5">This strategy is more expensive, though, because we need to divide it into more chunks, so we will have a database with many more entries. </span><span class="koboSpan" id="kobo.262.6">Also, some of the information is redundant, so the overlap should be no more than a small percentage of the entire</span><a id="_idIndexMarker505"/> <span class="No-Break"><span class="koboSpan" id="kobo.263.1">chunk size.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer143">
<span class="koboSpan" id="kobo.264.1"><img alt="Figure 5.11 – Effect of different chunking strategies used on an extract from Hamlet" src="image/B21257_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.265.1">Figure 5.11 – Effect of different chunking strategies used on an extract from Hamlet</span></p>
<p><span class="koboSpan" id="kobo.266.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.267.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.268.1">.11</span></em><span class="koboSpan" id="kobo.269.1">, we can see </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.271.1">A)</span></strong><span class="koboSpan" id="kobo.272.1">: Simple chunking based on the number of tokens and </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">without overlap</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.274.1">B)</span></strong><span class="koboSpan" id="kobo.275.1">: Simple chunking based on the number of tokens and </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">with overlap</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.277.1">C)</span></strong><span class="koboSpan" id="kobo.278.1">: Simple chunking based on the number of tokens and the presence of the new line in the </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">text (character-based)</span></span></li>
</ul>
<p><strong class="bold"><span class="koboSpan" id="kobo.280.1">Context-aware chunking</span></strong><span class="koboSpan" id="kobo.281.1"> is a </span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.282.1">strategy in which we divide text into chunks using </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.283.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.284.1">regular expression</span></strong><span class="koboSpan" id="kobo.285.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.286.1">regex</span></strong><span class="koboSpan" id="kobo.287.1">). </span><span class="koboSpan" id="kobo.287.2">For example, we </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.288.1">can divide based on periods, commas, or paragraph breaks. </span><span class="koboSpan" id="kobo.288.2">Variants of this strategy are based on the type of text we are splitting (for example, HTML tags, Markdown information, XML, domain-specific signs, and so on). </span><span class="koboSpan" id="kobo.288.3">This system is not without its drawbacks; it can sometimes be difficult to determine boundaries (for example, for compound sentences, dirty text, and so on). </span><span class="koboSpan" id="kobo.288.4">You can therefore have chunks that are of varying sizes. </span><span class="koboSpan" id="kobo.288.5">A more sophisticated variant is </span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.289.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.290.1">recursive chunking</span></strong><span class="koboSpan" id="kobo.291.1">, in which </span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.292.1">the chunk is split similarly to context-aware chunks. </span><span class="koboSpan" id="kobo.292.2">After that, the chunks are joined up to a predetermined number of tokens (for example, the maximum context length of the embedder). </span><span class="koboSpan" id="kobo.292.3">This approach tries to keep all information that is contextually related in the same chunk and maintain semantic consistency (for example, if possible, all chunks belonging to a paragraph are merged). </span><span class="koboSpan" id="kobo.292.4">Alternatively, the text is iteratively split until the chunks reach the desired size. </span><strong class="bold"><span class="koboSpan" id="kobo.293.1">Hierarchical clustering</span></strong><span class="koboSpan" id="kobo.294.1"> is a</span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.295.1"> similar method that seeks to respect the structure of the text. </span><span class="koboSpan" id="kobo.295.2">By examining relationships in the text, it tries to divide it into segments that respect its hierarchy (sections, subsections, paragraphs, and sentences). </span><span class="koboSpan" id="kobo.295.3">This system is useful for documents that have a complex and known structure (business reports, scientific articles, and websites). </span><span class="koboSpan" id="kobo.295.4">This method also makes it possible to inspect the structure obtained and to understand the relationship between the various chunks. </span><span class="koboSpan" id="kobo.295.5">This system works poorly when dealing </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.296.1">with documents that are </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">poorly formatted.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer144">
<span class="koboSpan" id="kobo.298.1"><img alt="Figure 5.12 – Demonstration of hierarchical chunking" src="image/B21257_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.299.1">Figure 5.12 – Demonstration of hierarchical chunking</span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.300.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.301.1">.12</span></em><span class="koboSpan" id="kobo.302.1"> shows the same document in Markdown (</span><strong class="bold"><span class="koboSpan" id="kobo.303.1">A</span></strong><span class="koboSpan" id="kobo.304.1">) or LaTex (</span><strong class="bold"><span class="koboSpan" id="kobo.305.1">B</span></strong><span class="koboSpan" id="kobo.306.1">). </span><span class="koboSpan" id="kobo.306.2">Using a specific chunker, we can split respecting the language structure. </span><span class="koboSpan" id="kobo.306.3">LangChain uses hierarchical clustering to </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">achieve that.</span></span></p>
<p><span class="koboSpan" id="kobo.308.1">Another family of methods</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.309.1"> is </span><strong class="bold"><span class="koboSpan" id="kobo.310.1">semantic chunking</span></strong><span class="koboSpan" id="kobo.311.1">. </span><span class="koboSpan" id="kobo.311.2">The purpose of these techniques is to take into account </span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.312.1">the context and meaning of words. </span><span class="koboSpan" id="kobo.312.2">These methods try to group chunks that would otherwise be distant in the text (presence of digression or other elements). </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">K-means chunking</span></strong><span class="koboSpan" id="kobo.314.1"> is an</span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.315.1"> approach in</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.316.1"> which we conduct an embedding of the various sentences, then use </span><em class="italic"><span class="koboSpan" id="kobo.317.1">k</span></em><span class="koboSpan" id="kobo.318.1">-means clustering to group sentences that are similar into various clusters. </span><span class="koboSpan" id="kobo.318.2">This approach requires setting the optimal number of clusters (hyperparameters) to choose and can lead to loss of sentence order (with potential risk to chronological order or contextual relationships). </span><span class="koboSpan" id="kobo.318.3">Instead of </span><a id="_idIndexMarker517"/><span class="koboSpan" id="kobo.319.1">considering division on sentences, </span><strong class="bold"><span class="koboSpan" id="kobo.320.1">propositions-based chunking</span></strong><span class="koboSpan" id="kobo.321.1"> divides </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.322.1">on contextual understanding. </span><span class="koboSpan" id="kobo.322.2">So-called “propositions” are identified as atomic expressions that contain factoids (sentences that are self-contained and describe a piece of knowledge, such as “</span><em class="italic"><span class="koboSpan" id="kobo.323.1">The capital of France is Paris</span></em><span class="koboSpan" id="kobo.324.1">”). </span><span class="koboSpan" id="kobo.324.2">These propositions are then evaluated by an LLM that groups them according to semantic coherence. </span><span class="koboSpan" id="kobo.324.3">This approach can give optimal results but is computationally expensive and depends on the choice of LLM used. </span><strong class="bold"><span class="koboSpan" id="kobo.325.1">Statistical merging</span></strong><span class="koboSpan" id="kobo.326.1">, on the</span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.327.1"> other hand, evaluates similarities and differences in the embedding of sentences to decide whether to merge (or split) them. </span><span class="koboSpan" id="kobo.327.2">For example, after embedding, the difference in statistical properties (standard deviation, percentile, or interquartile difference) is evaluated, and if it exceeds a predefined threshold, the sentences are separated. </span><span class="koboSpan" id="kobo.327.3">This system creates chunks of different sizes and has a higher computational cost but can give better results when contextual boundaries between sentences </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">are unclear.</span></span></p>
<p><span class="koboSpan" id="kobo.329.1">Finally, a multimodal chunk may be needed. </span><span class="koboSpan" id="kobo.329.2">For example, a PDF file may contain both text and images. </span><span class="koboSpan" id="kobo.329.3">In this case, it will be necessary for our chunking pipeline to be able to extract both images </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">and text.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">There is no universal best</span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.332.1"> chunker – the best chunker is the one most suited to our specific case. </span><span class="koboSpan" id="kobo.332.2">We can establish guidelines though, </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.334.1">Align chunking with document structure</span></strong><span class="koboSpan" id="kobo.335.1">: Text structure heavily influences the chunk size and chunk strategy. </span><span class="koboSpan" id="kobo.335.2">In cases where we have documents of the same type (HTML, LaTex, Markdown, and so on), a specific chunk might be the best choice. </span><span class="koboSpan" id="kobo.335.3">If they are a heterogeneous collection, we could create a pipeline that conducts chunking according to the </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">file types.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.337.1">Optimize for performance and resources</span></strong><span class="koboSpan" id="kobo.338.1">: If we have space and computational cost limitations, a simple fixed-size chunker might be an optimal choice. </span><span class="koboSpan" id="kobo.338.2">Semantic chunking is slightly less performant but better respects information integrity and improves the relevance and accuracy of found chunks. </span><span class="koboSpan" id="kobo.338.3">It requires knowledge of the text, though, and might not be the optimal choice for a system that is used by general users. </span><span class="koboSpan" id="kobo.338.4">Contextual chunking may have better performance but has a high </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">computational cost.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.340.1">Respect model context limitations</span></strong><span class="koboSpan" id="kobo.341.1">: Chunk size should respect the dimension of the context length. </span><span class="koboSpan" id="kobo.341.2">We must take into account both the size of the embedding model and the LLM that we will then use to </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">generate it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.343.1">Match chunking strategy to user query patterns</span></strong><span class="koboSpan" id="kobo.344.1">: Consider the type of question we expect our potential users to ask the system (the RAG). </span><span class="koboSpan" id="kobo.344.2">For example, if the user is going to ask questions that require the model to find multiple facts, it is better to have a strategy with small chunks but containing a direct answer. </span><span class="koboSpan" id="kobo.344.3">Or if the system is more discursive, it would be better to have chunks that give</span><a id="_idIndexMarker521"/> <span class="No-Break"><span class="koboSpan" id="kobo.345.1">more context.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.346.1">In conclusion, a developer has to inspect the text and the output delivered when testing different chunking strategies for the RAG system. </span><span class="koboSpan" id="kobo.346.2">In any case, each strategy should then be evaluated (in later sections, we will discuss how to </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">evaluate them).</span></span></p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/><span class="koboSpan" id="kobo.348.1">Embedding strategies</span></h2>
<p><span class="koboSpan" id="kobo.349.1">As we saw earlier, an embedding</span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.350.1"> is a dense vector representation of a text (representation lying in a multidimensional space). </span><span class="koboSpan" id="kobo.350.2">We exploit these vectors to find the appropriate context for our query. </span><span class="koboSpan" id="kobo.350.3">We can have encoders that produce scattered vectors (such as TF-IDF or BM25) or encoders that generate dense encoders. </span><span class="koboSpan" id="kobo.350.4">As mentioned earlier, dense encoders are transformers that produce vectors. </span><span class="koboSpan" id="kobo.350.5">The advantage is that these models are trainable and thus can be adapted to the similarity task between queries and chunks. </span><span class="koboSpan" id="kobo.350.6">BERT-based backbone is one of the most widely used; the approach is to create two parallel BERT encoders (two streams: one for the query and the other for the chunk) called the </span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.351.1">bi-encoder approach. </span><span class="koboSpan" id="kobo.351.2">In the first RAG approaches, these weights are identical (frozen), and we only have one layer that is trained to generate the embedding vector. </span><span class="koboSpan" id="kobo.351.3">Having the same weights allows us to be able to pass the query first and then the documents and then calculate similarity. </span><span class="koboSpan" id="kobo.351.4">Later models, on the other hand, conduct fine-tuning of the weights to improve the model’s ability to generate better </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">embedding vectors.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<span class="koboSpan" id="kobo.353.1"><img alt="Figure 5.13 – Bi-encoder for generating embedding vectors" src="image/B21257_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.354.1">Figure 5.13 – Bi-encoder for generating embedding vectors</span></p>
<p><span class="koboSpan" id="kobo.355.1">The </span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.356.1">bi-encoder shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.357.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.358.1">.13</span></em><span class="koboSpan" id="kobo.359.1"> generates a query vector and a document vector. </span><span class="koboSpan" id="kobo.359.2">On these two vectors, we can calculate </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">the similarity.</span></span></p>
<p><span class="koboSpan" id="kobo.361.1">Alternatively, a model can be trained from scratch for this task. </span><span class="koboSpan" id="kobo.361.2">Typically, it is better to take an LLM that has been trained unsupervised and then adapt it for embedding and retrieval. </span><span class="koboSpan" id="kobo.361.3">Normally, this model is adapted using contrastive learning. </span><span class="koboSpan" id="kobo.361.4">As we saw in</span><em class="italic"> </em><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.362.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.363.1">, contrastive learning is a technique used to learn semantic representations in the form of embedding. </span><span class="koboSpan" id="kobo.363.2">In </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.364.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.365.1">, we used CLIP, which was trained using images and captions. </span><span class="koboSpan" id="kobo.365.2">In this case, we want to train a model that generates embeddings that allow us to find the documents that are most akin to our query. </span><span class="koboSpan" id="kobo.365.3">One of the most used datasets is the Multi-Genre Natural Language Inference (MultiNLI) corpus, which contains 433,000 sentence pairs </span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.366.1">annotated with textual entailment information. </span><span class="koboSpan" id="kobo.366.2">Given a hypothesis, a second sentence represents an entailment, a contradiction, or </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">neither (neutral).</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">In contrastive learning, we need positive and negative examples. </span><span class="koboSpan" id="kobo.368.2">Having taken a sentence, we want the embedding of our sentence to be as close to a positive example as possible and as dissimilar from a negative example. </span><span class="koboSpan" id="kobo.368.3">In this case, we can derive positive and negative examples for one of our sentences from MultiNLI. </span><span class="koboSpan" id="kobo.368.4">In fact, an entailment sentence represents a positive example while a contradiction is a </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">negative example.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<span class="koboSpan" id="kobo.370.1"><img alt="Figure 5.14 – Example of sentences that are in entailment or contradiction and can be used for training an encoder" src="image/B21257_05_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.371.1">Figure 5.14 – Example of sentences that are in entailment or contradiction and can be used for training an encoder</span></p>
<p><span class="koboSpan" id="kobo.372.1">Once we have the dataset, these </span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.373.1">models are trained with a loss function that is suitable for </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">the task:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.375.1">Cosine similarity loss</span></strong><span class="koboSpan" id="kobo.376.1">: This is </span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.377.1">one of the loss functions that is most commonly used because it measures the semantic closeness of two sentences. </span><span class="koboSpan" id="kobo.377.2">The model is optimized to give a similarity as close as possible to </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">1</span></strong><span class="koboSpan" id="kobo.379.1"> for sentences that are similar (original sentence and positive example) and </span><strong class="source-inline"><span class="koboSpan" id="kobo.380.1">0</span></strong><span class="koboSpan" id="kobo.381.1"> for sentences that are dissimilar (original sentence and negative example). </span><span class="koboSpan" id="kobo.381.2">As a loss, we calculate the similarity between the two sentences, and then it is compared with the </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">predicted label.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.383.1">Multiple negatives ranking loss</span></strong><span class="koboSpan" id="kobo.384.1">: This is </span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.385.1">another popular alternative (also</span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.386.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.387.1">InfoNCE</span></strong><span class="koboSpan" id="kobo.388.1">). </span><span class="koboSpan" id="kobo.388.2">Only positive examples are used for this type of loss. </span><span class="koboSpan" id="kobo.388.3">In this case, we have the original sentence and the corresponding positive example (the entailment sentence). </span><span class="koboSpan" id="kobo.388.4">For negative examples, we take our original sentence and a sentence that is in entailment for another sentence. </span><span class="koboSpan" id="kobo.388.5">After that, we calculate embedding and similarity. </span><span class="koboSpan" id="kobo.388.6">The idea is to maximize the similarity between a sentence and one that is related (its positive example) while minimizing the similarity with examples that are unrelated (our negative examples). </span><span class="koboSpan" id="kobo.388.7">In this way, this task becomes a classification task, and we can use cross-entropy. </span><span class="koboSpan" id="kobo.388.8">However, the negative examples are </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.389.1">completely unrelated, and thus the task can be too easy for the model (instead, it is better to add negative sentences that are related but not the </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">right answer).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.391.1">The choice of embedder is</span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.392.1"> a critical decision that will strongly impact the performance of our system. </span><span class="koboSpan" id="kobo.392.2">A poor embedder will lead to poor retrieval and context not relevant to the query, which, paradoxically, could increase the risk of hallucinations. </span><span class="koboSpan" id="kobo.392.3">The encoder choice impacts </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.394.1">Cost</span></strong><span class="koboSpan" id="kobo.395.1">: An embedder is a transformer. </span><span class="koboSpan" id="kobo.395.2">The bigger it is, the higher the computational cost. </span><span class="koboSpan" id="kobo.395.3">A closed-source encoder, on the other hand, has a cost relative to the API, so the more it is used, the greater the cost. </span><span class="koboSpan" id="kobo.395.4">In addition, there are computational costs associated with embedding documents and also with </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">each query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.397.1">Storage cost</span></strong><span class="koboSpan" id="kobo.398.1">: The larger the size of the embedded vectors, the higher the storage cost of </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">our vectors.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.400.1">Latency</span></strong><span class="koboSpan" id="kobo.401.1">: Larger models have </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">higher latency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.403.1">Performance</span></strong><span class="koboSpan" id="kobo.404.1">: The cost of some choices is justified if our major concern is performance. </span><span class="koboSpan" id="kobo.404.2">Often, larger models have </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">better performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.406.1">Domain requirements</span></strong><span class="koboSpan" id="kobo.407.1">: There are now specialized encoders for some domains (finance, medicine, science, programming, and so on) and some are multilingual (most support only English but others support up to a hundred languages). </span><span class="koboSpan" id="kobo.407.2">Some domains have different text granularity and require models that are specialized for </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">long text.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.409.1">Deciding which encoder model to use is not easy and depends on various factors. </span><span class="koboSpan" id="kobo.409.2">A good way to start is</span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.410.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.411.1">MTEB leaderboard</span></strong><span class="koboSpan" id="kobo.412.1"> on Hugging Face (</span><a href="https://huggingface.co/spaces/mteb/leaderboard"><span class="koboSpan" id="kobo.413.1">https://huggingface.co/spaces/mteb/leaderboard</span></a><span class="koboSpan" id="kobo.414.1">), which is an up-to-date list of encoding models and their performance on different benchmarks and tasks. </span><span class="koboSpan" id="kobo.414.2">Often, though, these results are self-reported and are obtained on standard benchmarks (some of this benchmark data may be leaked in the training data and thus overestimate the model’s capabilities). </span><span class="koboSpan" id="kobo.414.3">Thus, we should not choose just one model but test several on one’s dataset. </span><span class="koboSpan" id="kobo.414.4">The</span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.415.1"> leaderboard, however, provides some important information that allows us to guide </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">our choice:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.417.1">Retrieval average</span></strong><span class="koboSpan" id="kobo.418.1">: Calculates </span><strong class="bold"><span class="koboSpan" id="kobo.419.1">normalized discounted cumulative gain</span></strong><span class="koboSpan" id="kobo.420.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.421.1">NDCG</span></strong><span class="koboSpan" id="kobo.422.1">) across </span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.423.1">several datasets (an evaluation metric used for ranking </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">retrieval systems)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.425.1">Model size</span></strong><span class="koboSpan" id="kobo.426.1">: This gives us an insight into the computational cost and resources we need to </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">use it</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.428.1">Max tokens</span></strong><span class="koboSpan" id="kobo.429.1">: The number of tokens that can be used in the </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">context length</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.431.1">Embedding dimensions</span></strong><span class="koboSpan" id="kobo.432.1">: Considers the size of the vectors </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">after embedding</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer147">
<span class="koboSpan" id="kobo.434.1"><img alt="Figure 5.15 – MTEB leaderboard dedicated to embedding models and their performance" src="image/B21257_05_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.435.1">Figure 5.15 – MTEB leaderboard dedicated to embedding models and their performance</span></p>
<p><span class="koboSpan" id="kobo.436.1">It should also be noted that the leaderboard measures on generic domains. </span><span class="koboSpan" id="kobo.436.2">This means it measures general performance, which could result in poor performance in our domain or task </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">of interest.</span></span></p>
<p><span class="koboSpan" id="kobo.438.1">Once we have selected an encoder, we can reduce this cost without affecting the performance. </span><span class="koboSpan" id="kobo.438.2">Regarding cost and scalability, for each dimension of the vector embedding, we need 4 bytes of memory if they are in a float format. </span><span class="koboSpan" id="kobo.438.3">This can lead to exorbitant costs in storage. </span><span class="koboSpan" id="kobo.438.4">In </span><a href="B21257_03.xhtml#_idTextAnchor042"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.439.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.440.1">, we discussed quantization – this can also be applied to embedding </span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.441.1">models. </span><strong class="bold"><span class="koboSpan" id="kobo.442.1">Binary quantization</span></strong><span class="koboSpan" id="kobo.443.1"> (which reduces models to 1 bit per dimension) can lead to a reduction in memory and storage of up to 32 times. </span><span class="koboSpan" id="kobo.443.2">The simplest binary quantization is to use a </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">zero threshold:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>f</mi><mfenced close=")" open="("><mi>x</mi></mfenced><mo>=</mo><mfenced close="" open="{"><mtable columnalign="center" columnwidth="auto" rowalign="baseline baseline" rowspacing="1.0000ex"><mtr><mtd><mrow><mn>0</mn><mi>i</mi><mi>f</mi><mi>x</mi><mo>≤</mo><mn>0</mn></mrow></mtd></mtr><mtr><mtd><mrow><mn>1</mn><mi>i</mi><mi>f</mi><mi>x</mi><mo>&gt;</mo><mn>0</mn></mrow></mtd></mtr></mtable></mfenced></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.445.1">We can then use the </span><strong class="bold"><span class="koboSpan" id="kobo.446.1">Hamming distance</span></strong><span class="koboSpan" id="kobo.447.1"> to find the value for the weights more efficiently (Hamming distance</span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.448.1"> is another evaluation metric most suitable for these binary vectors. </span><span class="koboSpan" id="kobo.448.2">You can read more about it using the link in the </span><em class="italic"><span class="koboSpan" id="kobo.449.1">Further reading</span></em><span class="koboSpan" id="kobo.450.1"> section at the end of the chapter). </span><span class="koboSpan" id="kobo.450.2">Binary encoding allows unprecedented speed and memory reduction but generally means a loss of performance. </span><span class="koboSpan" id="kobo.450.3">More sophisticated versions of this approach can maintain up to 96% similarity (approaches on re-scoring found chunks). </span><span class="koboSpan" id="kobo.450.4">This quantization is considered extreme; often, a good compromise is to go from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.451.1">float32</span></strong><span class="koboSpan" id="kobo.452.1"> format to </span><strong class="source-inline"><span class="koboSpan" id="kobo.453.1">int8</span></strong><span class="koboSpan" id="kobo.454.1"> (a format in which we represent values using 256 distinct levels). </span><span class="koboSpan" id="kobo.454.2">As we described</span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.455.1"> earlier, this is done by recalibrating the vectors during </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">the transformation.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<span class="koboSpan" id="kobo.457.1"><img alt="Figure 5.16 – Graph showing memory deduction after quantization" src="image/B21257_05_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.458.1">Figure 5.16 – Graph showing memory deduction after quantization</span></p>
<p><span class="koboSpan" id="kobo.459.1">An alternative technique is Matryoshka Representation Learning. </span><span class="koboSpan" id="kobo.459.2">Deep learning models tend to spread the information </span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.460.1">over the entire vector; this technique attempts to compress the information over several representations with fewer dimensions instead. </span><span class="koboSpan" id="kobo.460.2">In other words, it progressively reduces the divisions of vector embeddings without losing too much performance. </span><span class="koboSpan" id="kobo.460.3">In a Matryoshka embedding, smaller embeddings are obtained that can be used as larger embeddings. </span><span class="koboSpan" id="kobo.460.4">This is because the system tries to force storage of the most important information in early dimensions and less important information in later dimensions (in this way, we can truncate the vector while maintaining performance in downstream tasks). </span><span class="koboSpan" id="kobo.460.5">To train an encoder, we produce embeddings for a batch of text and then compute the loss. </span><span class="koboSpan" id="kobo.460.6">For Matryoshka embedding models, the loss also takes into account the quality of the embedding at different dimensionalities. </span><span class="koboSpan" id="kobo.460.7">These values are summed in the final loss. </span><span class="koboSpan" id="kobo.460.8">Thus, the model tries to optimize the model weights in a way that the most important information (for the embedding vectors) is</span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.461.1"> located in the </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">first dimensions.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer149">
<span class="koboSpan" id="kobo.463.1"><img alt="Figure 5.17 – Benchmark of Matryoshka versus original embedding quality over number dimensions" src="image/B21257_05_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.464.1">Figure 5.17 – Benchmark of Matryoshka versus original embedding quality over number dimensions</span></p>
<p><span class="koboSpan" id="kobo.465.1">Once we have our vectors, we need to store them. </span><span class="koboSpan" id="kobo.465.2">In the next section, we will discuss where to </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">store them.</span></span></p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/><span class="koboSpan" id="kobo.467.1">Embedding databases</span></h2>
<p><span class="koboSpan" id="kobo.468.1">A vector database</span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.469.1"> is a specialized database for the storage of high-dimensional vectors. </span><span class="koboSpan" id="kobo.469.2">This database is therefore optimized for handling unstructured and semi-structured data such as vectors. </span><span class="koboSpan" id="kobo.469.3">The function of this database is to allow efficient storing, indexing, and searching. </span><span class="koboSpan" id="kobo.469.4">The vector database we choose also has a big impact on RAG performance. </span><span class="koboSpan" id="kobo.469.5">Today, there are </span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.470.1">dozens of possible vector databases, so choosing the best solution can be a daunting task. </span><span class="koboSpan" id="kobo.470.2">Fortunately, there are sites that conduct a comparison of </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">possible systems.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<span class="koboSpan" id="kobo.472.1"><img alt="Figure 5.18 – Vector DB leaderboard, a practical source to address the vector database choice (https://superlinked.com/vector-db-comparison)" src="image/B21257_05_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.473.1">Figure 5.18 – Vector DB leaderboard, a practical source to address the vector database choice (</span><a href="https://superlinked.com/vector-db-comparison"><span class="koboSpan" id="kobo.474.1">https://superlinked.com/vector-db-comparison</span></a><span class="koboSpan" id="kobo.475.1">)</span></p>
<p><span class="koboSpan" id="kobo.476.1">There is probably no best vector database, but there will be one that is suitable for our project. </span><span class="koboSpan" id="kobo.476.2">Some criteria that can guide our choice are </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.478.1">Open source or private source</span></strong><span class="koboSpan" id="kobo.479.1">: Open source databases offer transparency and the ability to customize the system. </span><span class="koboSpan" id="kobo.479.2">They usually have an active community and no associated costs. </span><span class="koboSpan" id="kobo.479.3">Private source databases, on the contrary, can be an expensive solution but often have dedicated support. </span><span class="koboSpan" id="kobo.479.4">Similarly, it is important to check the license; it may not be compatible with </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">your product.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.481.1">Language support</span></strong><span class="koboSpan" id="kobo.482.1">: Vector databases are generally compatible with major programming languages (Python, Java, and C), but for our project, we may need a database compatible with another language (Rust, Go, Scala, and so on). </span><span class="koboSpan" id="kobo.482.2">Also, not all databases are compatible with all libraries. </span><span class="koboSpan" id="kobo.482.3">So, it is good to make sure that the system is compatible with </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">our project.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.484.1">Maturity</span></strong><span class="koboSpan" id="kobo.485.1">: Especially for projects that are production-oriented, it is important that the system is stable, scalable, and reliable. </span><span class="koboSpan" id="kobo.485.2">Likewise, the system must be supported, adopted by industry, and </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">maintained frequently.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.487.1">Performance</span></strong><span class="koboSpan" id="kobo.488.1">: This is influenced by</span><a id="_idIndexMarker542"/> <span class="No-Break"><span class="koboSpan" id="kobo.489.1">two parameters:</span></span><ul><li> <strong class="bold"><span class="koboSpan" id="kobo.490.1">Insertion speed</span></strong><span class="koboSpan" id="kobo.491.1">: the rate at which vectors can be added to a database (which affects latency). </span><span class="koboSpan" id="kobo.491.2">This especially impacts applications that are in real time or have a large user base. </span><span class="koboSpan" id="kobo.491.3">Some databases implement techniques such as batch processing (efficient partitioning of various data packets), parallelization (distribution of tasks across various nodes, especially important for the cloud), or data partitioning (the dataset is divided into segments in order to conduct insertions and deletions at the </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">same time).</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.493.1">Query speed</span></strong><span class="koboSpan" id="kobo.494.1">: this refers to the time it takes to find vectors in response to a query. </span><span class="koboSpan" id="kobo.494.2">This directly affects the </span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">latency time.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.496.1">There are optimization techniques such as index structures (structuring indexes to make the search faster), caching systems (data that is accessed frequently is saved separately), or specific algorithms. </span><span class="koboSpan" id="kobo.496.2">Then there are performance issues related to the specific products, such as the number of concurrent requests that can be made to the dataset. </span><span class="koboSpan" id="kobo.496.3">Regulatory compliance and privacy issues are also key. </span><span class="koboSpan" id="kobo.496.4">The database should be able to allow differential access (access authorization) and protect vectors from access by </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">unauthorized users.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.498.1">Component integration</span></strong><span class="koboSpan" id="kobo.499.1">: Our system can have several components besides the LLM and embedder (we will see this in more detail in the next chapter). </span><span class="koboSpan" id="kobo.499.2">We need to be sure that the database can be integrated with our encoder (and the library we use for the encoder). </span><span class="koboSpan" id="kobo.499.3">Also, not all databases accept other components such as a re-ranker, hybrid search, and </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">so on.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.501.1">Cost</span></strong><span class="koboSpan" id="kobo.502.1">: Cloud solutions can have very high costs, so it is recommended to decide in advance what budget you have. </span><span class="koboSpan" id="kobo.502.2">The cost could also be associated with the maintenance and support needed to keep the system operational. </span><span class="koboSpan" id="kobo.502.3">For example, vectors are valuable data and the cost of backup can grow </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">very quickly.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.504.1">For example, there</span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.505.1"> are vector libraries that are static (the index data is immutable); this makes it difficult to add new data. </span><span class="koboSpan" id="kobo.505.2">Libraries such as </span><strong class="bold"><span class="koboSpan" id="kobo.506.1">FAISS</span></strong><span class="koboSpan" id="kobo.507.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.508.1">Facebook AI Similarity Search</span></strong><span class="koboSpan" id="kobo.509.1">) are not </span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.510.1">designed for </span><strong class="bold"><span class="koboSpan" id="kobo.511.1">create, read, update, and delete</span></strong><span class="koboSpan" id="kobo.512.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.513.1">CRUD</span></strong><span class="koboSpan" id="kobo.514.1">) operations, so they are not a good choice for dynamic systems</span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.515.1"> where there are multiple users accessing and conducting operations. </span><span class="koboSpan" id="kobo.515.2">In contrast, if our database is immutable and we only grant access, FAISS can be a good solution. </span><span class="koboSpan" id="kobo.515.3">There are SQL databases that allow support for vectors (an extension of the classic database). </span><span class="koboSpan" id="kobo.515.4">These databases allow for efficient indexing of associated metadata. </span><span class="koboSpan" id="kobo.515.5">However, these databases are not scalable and often have limitations for vector size (maximum number of dimensions), and performance is lower. </span><span class="koboSpan" id="kobo.515.6">SQL databases are a good choice for internal projects that need to be connected to existing enterprise databases (which will probably already be in SQL) but are not a good choice when scalability and performance </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">are important.</span></span></p>
<p><span class="koboSpan" id="kobo.517.1">Vector-dedicated databases are usually the best solution, especially for performance. </span><span class="koboSpan" id="kobo.517.2">In fact, they usually have implemented dedicated and efficient algorithms for searching and indexing vectors. </span><span class="koboSpan" id="kobo.517.3">Several of these</span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.518.1"> algorithms are variations of the </span><strong class="bold"><span class="koboSpan" id="kobo.519.1">approximate nearest neighbors</span></strong><span class="koboSpan" id="kobo.520.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.521.1">ANN</span></strong><span class="koboSpan" id="kobo.522.1">) algorithm. </span><span class="koboSpan" id="kobo.522.2">ANN usually allows for a good trade-off between efficiency, storage, and accuracy. </span><span class="koboSpan" id="kobo.522.3">Approximate search speeds up the search while trying to maintain accuracy (HNSW (Hierarchical Navigable Small World) sacrifices some accuracy but is much faster than an accurate algorithm such as Flat indexing). </span><span class="koboSpan" id="kobo.522.4">These databases are also compatible with major languages and libraries (LlamaIndex, LangChain, and </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.524.1">Once we have our vector database populated with our vectors, we need to evaluate how good our system is. </span><span class="koboSpan" id="kobo.524.2">Now, we’ll see how we can </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">do that.</span></span></p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.526.1">Evaluating the output</span></h1>
<p><span class="koboSpan" id="kobo.527.1">In information retrieval, we are interested in measuring whether a found document is either relevant or irrelevant. </span><span class="koboSpan" id="kobo.527.2">Therefore, the most commonly used valuation metrics are precision and recall. </span><span class="koboSpan" id="kobo.527.3">Precision</span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.528.1"> is the fraction of retrieved documents that are relevant, while recall is the fraction of relevant documents that are successfully retrieved. </span><span class="koboSpan" id="kobo.528.2">Consider a query in which </span><em class="italic"><span class="koboSpan" id="kobo.529.1">R</span></em><span class="koboSpan" id="kobo.530.1"> represents all relevant documents and </span><em class="italic"><span class="koboSpan" id="kobo.531.1">NR</span></em><span class="koboSpan" id="kobo.532.1"> represents the irrelevant ones in a corpus of documents </span><em class="italic"><span class="koboSpan" id="kobo.533.1">D</span></em><span class="koboSpan" id="kobo.534.1">. </span><em class="italic"><span class="koboSpan" id="kobo.535.1">Rq</span></em><span class="koboSpan" id="kobo.536.1"> represents the relevant documents found and </span><em class="italic"><span class="koboSpan" id="kobo.537.1">Dq</span></em><span class="koboSpan" id="kobo.538.1"> is the documents returned by the system. </span><span class="koboSpan" id="kobo.538.2">We can define the two metrics </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>=</mo><mfrac><mrow><mi>R</mi><mi>q</mi></mrow><mrow><mi>D</mi><mi>q</mi></mrow></mfrac><mi>r</mi><mi>e</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>l</mi><mo>=</mo><mfrac><mrow><mi>R</mi><mi>q</mi></mrow><mi>R</mi></mfrac></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.540.1">The problem with these two metrics is that they do not return goodness of ranking, only whether we are finding all relevant documents or the percentage of relevant documents in the total. </span><span class="koboSpan" id="kobo.540.2">Usually, when we use a retriever, we select a number (</span><em class="italic"><span class="koboSpan" id="kobo.541.1">k</span></em><span class="koboSpan" id="kobo.542.1">) of documents that we use for context (top-k), so we need a metric that takes ranking </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">into account.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<span class="koboSpan" id="kobo.544.1"><img alt="Figure 5.19 – Rank-specific precision and recall calculated assuming we have five relevant documents in a corpus" src="image/B21257_05_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.545.1">Figure 5.19 – Rank-specific precision and recall calculated assuming we have five relevant documents in a corpus</span></p>
<p><span class="koboSpan" id="kobo.546.1">We can use the precision-recall curve</span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.547.1"> for this purpose. </span><span class="koboSpan" id="kobo.547.2">Whenever we find a relevant document in the rank, recall increases. </span><span class="koboSpan" id="kobo.547.3">Precision, on the other hand, increases with documents but decreases with each irrelevant document. </span><span class="koboSpan" id="kobo.547.4">By plotting a graph with a curve, we can see </span><span class="No-Break"><span class="koboSpan" id="kobo.548.1">this behavior:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<span class="koboSpan" id="kobo.549.1"><img alt="Figure 5.20 – Graph showing the precision and recall curve for the data shown in Figure 5.19" src="image/B21257_05_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.550.1">Figure 5.20 – Graph showing the precision and recall curve for the data shown in Figure 5.19</span></p>
<p><span class="koboSpan" id="kobo.551.1">Because the precision goes up and down, we can use an interpolated curve. </span><span class="koboSpan" id="kobo.551.2">This curve is less precise but allows us to better understand the behavior of the system (and to be able to compare different systems by comparing </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">their curves).</span></span></p>
<p><span class="koboSpan" id="kobo.553.1">Another metric that is used </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.554.1">is </span><strong class="bold"><span class="koboSpan" id="kobo.555.1">mean average precision</span></strong><span class="koboSpan" id="kobo.556.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.557.1">MAP</span></strong><span class="koboSpan" id="kobo.558.1">). </span><span class="koboSpan" id="kobo.558.2">We calculate precision values at the points where a relevant item is retrieved (</span><strong class="bold"><span class="koboSpan" id="kobo.559.1">average precision</span></strong><span class="koboSpan" id="kobo.560.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.561.1">AP</span></strong><span class="koboSpan" id="kobo.562.1">) and </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.563.1">then average these AP values. </span><span class="koboSpan" id="kobo.563.2">Suppose we have retrieved the following list of documents: </span><em class="italic"><span class="koboSpan" id="kobo.564.1">[1, 0, 1, 0, 1]</span></em><span class="koboSpan" id="kobo.565.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.566.1">1</span></em><span class="koboSpan" id="kobo.567.1"> means relevant and </span><em class="italic"><span class="koboSpan" id="kobo.568.1">0</span></em><span class="koboSpan" id="kobo.569.1"> means not relevant. </span><span class="koboSpan" id="kobo.569.2">The precision for each relevant item is </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.571.1">The first relevant document is in position 1: Precision (</span><em class="italic"><span class="koboSpan" id="kobo.572.1">N</span></em><span class="koboSpan" id="kobo.573.1"> relevant document retrieved / total of document retrieved) = 1/1 = 1</span></li>
<li><span class="koboSpan" id="kobo.574.1">Second relevant document (position 3): Precision = 2/3 ≈ </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">0.67</span></span></li>
<li><span class="koboSpan" id="kobo.576.1">Third relevant item (position 5): Precision = 3/5 = </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">0.6</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.578.1">The average precision (AP) value for a single </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">query is:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>t</mi><mi>e</mi><mi>m</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>r</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>v</mi><mi>a</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>t</mi><mi>e</mi><mi>m</mi><mi>s</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∑</mo><mrow><mrow><mo>(</mo><mn>1</mn><mo>+</mo><mn>0.67</mn><mo>+</mo><mn>0.6</mn><mo>)</mo></mrow></mrow></mrow><mn>3</mn></mfrac><mo>=</mo><mn>0.76</mn></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.580.1">The MAP is the average of all AP values across all the queries. </span><span class="koboSpan" id="kobo.580.2">Here, we suppose we have 3 queries – </span><em class="italic"><span class="koboSpan" id="kobo.581.1">0.76</span></em><span class="koboSpan" id="kobo.582.1">, </span><em class="italic"><span class="koboSpan" id="kobo.583.1">0.5</span></em><span class="koboSpan" id="kobo.584.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.585.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.586.1">0.67</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>M</mi><mi>A</mi><mi>P</mi><mo>=</mo><mfrac><mrow><mo>∑</mo><mi>a</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>a</mi><mi>g</mi><mi>e</mi><mi>p</mi><mi>r</mi><mi>e</mi><mi>c</mi><mi>i</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>c</mi><mi>h</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>y</mi></mrow><mrow><mi>T</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi><mi>n</mi><mi>u</mi><mi>m</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>o</mi><mi>f</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>e</mi><mi>s</mi></mrow></mfrac><mo>=</mo><mfrac><mrow><mo>∑</mo><mrow><mrow><mo>(</mo><mn>0.76</mn><mo>+</mo><mn>0.5</mn><mo>+</mo><mn>0.67</mn><mo>)</mo></mrow></mrow></mrow><mn>3</mn></mfrac><mo>=</mo><mn>0.64</mn></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.588.1">One metric that is specific to question </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.589.1">answering is </span><strong class="bold"><span class="koboSpan" id="kobo.590.1">mean reciprocal rank</span></strong><span class="koboSpan" id="kobo.591.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.592.1">MRR</span></strong><span class="koboSpan" id="kobo.593.1">). </span><span class="koboSpan" id="kobo.593.2">MRR is designed to assess the quality of a short-ranked list having the correct answer (usually of human labels). </span><span class="koboSpan" id="kobo.593.3">The reciprocal rank is the reciprocal of the rank of the first item relevant to the question. </span><span class="koboSpan" id="kobo.593.4">For a set of queries </span><em class="italic"><span class="koboSpan" id="kobo.594.1">Q</span></em><span class="koboSpan" id="kobo.595.1">, we take the reciprocal ranks and conduct </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">the average:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><math display="block"><mrow><mrow><mi>M</mi><mi>R</mi><mi>R</mi><mo>=</mo><mfrac><mn>1</mn><mi>Q</mi></mfrac><mrow><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>Q</mi></munderover><mfrac><mn>1</mn><msub><mrow><mi>r</mi><mi>a</mi><mi>n</mi><mi>k</mi></mrow><mi>i</mi></msub></mfrac></mrow></mrow></mrow></math></span></p>
<p><span class="koboSpan" id="kobo.597.1">Alternatively, we can evaluate the response after generation as if it were a </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">classification task.</span></span></p>
<p><span class="koboSpan" id="kobo.599.1">Recently, another way to evaluate RAG pipelines is to use an LLM as a judge of the pipeline. </span><span class="koboSpan" id="kobo.599.2">Typically, we must have a dataset that contains ground truth so that LLM evaluates whether the RAG pipeline has found both the necessary steps and the generation is correct. </span><span class="koboSpan" id="kobo.599.3">For example, there are different metrics that leverage an LLM as </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">a judge:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.601.1">Faithfulness</span></strong><span class="koboSpan" id="kobo.602.1">: This </span><a id="_idIndexMarker552"/><span class="koboSpan" id="kobo.603.1">metric (also called groundedness) measures the factual consistency of the generated answer (range between 0 and 1). </span><span class="koboSpan" id="kobo.603.2">An answer is faithful if the claims that are produced in the answer can be inferred from the context. </span><span class="koboSpan" id="kobo.603.3">Faithfulness is the ratio of the number of claims in the generated answer that can be inferred from the context to the total number of claims in the generated answer. </span><span class="koboSpan" id="kobo.603.4">To find the claims, we need an LLM that evaluates the claims in both the response and </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">the context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.605.1">Context recall</span></strong><span class="koboSpan" id="kobo.606.1">: This metric </span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.607.1">refers to how much context is found relative to the ground truth (range of 0 to 1). </span><span class="koboSpan" id="kobo.607.2">Ideally, the system should find all the sentences in the </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">ground truth.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.609.1">Context precision</span></strong><span class="koboSpan" id="kobo.610.1">: This</span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.611.1"> metric measures ground-truth relevant items in the context, which are ranked higher (relevant chunks should find themselves higher </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">after retrieval).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.613.1">Context relevancy</span></strong><span class="koboSpan" id="kobo.614.1">: This</span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.615.1"> metric measures the relevance of context to the query. </span><span class="koboSpan" id="kobo.615.2">Ideally, our system should only find information that is relevant to </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">the query.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.617.1">Context entities recall</span></strong><span class="koboSpan" id="kobo.618.1">: This</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.619.1"> metric provides a measure of context recall by specifically analyzing the entities that are found. </span><span class="koboSpan" id="kobo.619.2">In other words, it measures the fraction of entities in the ground truth that are found in the context. </span><span class="koboSpan" id="kobo.619.3">This metric is useful when we are interested in the system specifically finding entities (for example, the context needs to find medical entities such as diseases, drugs, or </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">other parameters).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.621.1">Answer correctness</span></strong><span class="koboSpan" id="kobo.622.1">: This</span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.623.1"> metric focuses on critically evaluating whether the answer is correct. </span><span class="koboSpan" id="kobo.623.2">To have a high value, the system must generate answers that are semantically similar to ground truth but also </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">factually correct.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.625.1">Summarization score</span></strong><span class="koboSpan" id="kobo.626.1">: This </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.627.1">metric assesses how well a summary captures the important information that is present in the context. </span><span class="koboSpan" id="kobo.627.2">The answer is a kind of summary of the context, and a good summary must contain the </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">important information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.629.1">Answer relevance</span></strong><span class="koboSpan" id="kobo.630.1">: This </span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.631.1">metric calculates how relevant the generated response is in response to a prompt. </span><span class="koboSpan" id="kobo.631.2">A low score means that the response is incomplete or contains </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">redundant information.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.633.1">Fluency</span></strong><span class="koboSpan" id="kobo.634.1">: This</span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.635.1"> metric assesses the quality of individual </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">sentences generated.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.637.1">Coherence</span></strong><span class="koboSpan" id="kobo.638.1">: This </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.639.1">metric assesses whether the entire response is a cohesive corpus (avoids the response being a group of </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">unconnected sentences).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.641.1">These metrics require that there be an evaluator who is either human or an LLM. </span><span class="koboSpan" id="kobo.641.2">They are not simply statistical values but require that the response (and/or the found context) be </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">evaluated critically.</span></span></p>
<p><span class="koboSpan" id="kobo.643.1">RAG is also often discussed as an alternative to fine-tuning; thus, it is important to compare them, which we’ll </span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">do next.</span></span></p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.645.1">Comparison between RAG and fine-tuning</span></h1>
<p><span class="koboSpan" id="kobo.646.1">RAG and fine-tuning are often compared and considered techniques in opposition. </span><span class="koboSpan" id="kobo.646.2">Both fine-tuning and RAG have a similar purpose, which is to provide the model with knowledge it did not acquire during training. </span><span class="koboSpan" id="kobo.646.3">In general, we can say that there are two types of fine-tuning: one directed at adapting a model to a specific domain (such as medicine, finance, or other) and one directed at improving the LLM’s ability to perform a particular task or class of tasks (math problem solving, question answering, and </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">so on).</span></span></p>
<p><span class="koboSpan" id="kobo.648.1">There are </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.649.1">several differences</span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.650.1"> between fine-tuning </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">and RAG:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.652.1">Knowledge updates</span></strong><span class="koboSpan" id="kobo.653.1">: RAG allows a direct knowledge update (of both structured and unstructured information). </span><span class="koboSpan" id="kobo.653.2">This update can be dynamic for RAG (information can be saved and deleted in real time). </span><span class="koboSpan" id="kobo.653.3">In contrast, fine-tuning requires retraining because the update is static (impractical for </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">frequent changes).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.655.1">Data processing</span></strong><span class="koboSpan" id="kobo.656.1">: Data processing is minimal for RAG, while fine-tuning requires quality datasets (datasets with not enough examples will not be able to help with </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">noticeable improvements).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.658.1">Model customization</span></strong><span class="koboSpan" id="kobo.659.1">: RAG provides additional information to the LLM but does not change its behavior or writing style. </span><span class="koboSpan" id="kobo.659.2">Fine-tuning allows changes in model behavior, writing style, and even </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">new skills.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.661.1">Interpretability</span></strong><span class="koboSpan" id="kobo.662.1">: RAG increases the interpretability of the system and allows tracking of responses and sources used. </span><span class="koboSpan" id="kobo.662.2">Fine-tuning makes the model less interpretable and makes it more difficult to track whether a behavior comes from fine-tuning or the </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">original model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.664.1">Computational resources</span></strong><span class="koboSpan" id="kobo.665.1">: RAG has an additional cost associated with the encoder and databases (finding information, embedding data, storing information, and so on). </span><span class="koboSpan" id="kobo.665.2">This can increase latency cost (you have to add retrieval time to generation time). </span><span class="koboSpan" id="kobo.665.3">Fine-tuning requires preparing and curating quality datasets (acquiring certain datasets can be expensive and labor-intensive). </span><span class="koboSpan" id="kobo.665.4">In addition, fine-tuning has computational costs associated with model retraining, but it provides lower latency. </span><span class="koboSpan" id="kobo.665.5">Moreover, RAG requires less technical expertise </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">than fine-tuning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.667.1">Reducing hallucinations</span></strong><span class="koboSpan" id="kobo.668.1">: RAG is inherently less prone to hallucinations and allows the tracking of which context is used. </span><span class="koboSpan" id="kobo.668.2">Fine-tuning can reduce hallucinations (but, often, fine-tuned LLMs do still </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">exhibit hallucinations).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.670.1">Ethical and privacy issues</span></strong><span class="koboSpan" id="kobo.671.1">: In the case of RAG, we must be careful how the information stored in the database is saved. </span><span class="koboSpan" id="kobo.671.2">The database must be protected against potential intrusions and prevent leakage. </span><span class="koboSpan" id="kobo.671.3">For fine-tuning, it is important to take </span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.672.1">care of the training dataset and prevent it from</span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.673.1"> containing </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">sensitive data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.675.1">RAG is the best system when we need a dynamic system that can adapt to real-time data or we have large amounts of internal data that are not well structured, though. </span><span class="koboSpan" id="kobo.675.2">Likewise, RAG is preferred when it is important to minimize hallucinations, as we need to track the sources of the response when transparency is vital. </span><span class="koboSpan" id="kobo.675.3">Fine-tuning is a priority choice when we need to have the model develop specific skills or want to align the model with a particular style of writing </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">or vocabulary.</span></span></p>
<p><span class="koboSpan" id="kobo.677.1">Some </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.678.1">practical examples show where it is best to choose fine-tuning </span><a id="_idIndexMarker567"/><span class="No-Break"><span class="koboSpan" id="kobo.679.1">or RAG:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.680.1">Summarization</span></strong><span class="koboSpan" id="kobo.681.1"> is important, especially for cases where the domain is highly specialized. </span><span class="koboSpan" id="kobo.681.2">It is more critical that the model best understands the context, so fine-tuning is </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">more appropriate.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.683.1">Question answering</span></strong><span class="koboSpan" id="kobo.684.1"> is an extremely relevant task that is often used in different domains (questions about documentation, products, and so on). </span><span class="koboSpan" id="kobo.684.2">In this case, reducing hallucinations and transparency are critical aspects but customization is much less important. </span><span class="koboSpan" id="kobo.684.3">RAG is therefore a </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">better choice.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.686.1">Code generation</span></strong><span class="koboSpan" id="kobo.687.1"> is a task that requires that the code base be dynamic; at the same time, it is important to reduce hallucinations and errors. </span><span class="koboSpan" id="kobo.687.2">On the other hand, we need the model to be adapted as much as possible to the task. </span><span class="koboSpan" id="kobo.687.3">So, both RAG and fine-tuning would be </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">of benefit.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.689.1">As the last example shows, there are cases where both fine-tuning and RAG would be beneficial. </span><span class="koboSpan" id="kobo.689.2">The two systems are not necessarily in opposition to each other. </span><span class="koboSpan" id="kobo.689.3">We can conduct fine-tuning of both the LLM and the embedder. </span><span class="koboSpan" id="kobo.689.4">So, having a system built with RAG </span><em class="italic"><span class="koboSpan" id="kobo.690.1">plus</span></em><span class="koboSpan" id="kobo.691.1"> fine-tuning the LLM (or even the RAG encoder) would be beneficial. </span><span class="koboSpan" id="kobo.691.2">Targeted fine-tuning to improve capabilities for specific tasks in the domain of our data can lead to better performance. </span><span class="koboSpan" id="kobo.691.3">This allows for a dynamic information system (where we conduct the RAG update) but adapts the style of the LLM to the domain. </span><span class="koboSpan" id="kobo.691.4">The LLM will also be more able to understand and use the context that is provided by RAG. </span><span class="koboSpan" id="kobo.691.5">Another case where fine-tuning the model is beneficial is when the found data is in a specific format (code, tables, XML, or other formats) – we can then use an LLM that is adapted to these specific formats (instead of a naive LLM). </span><span class="koboSpan" id="kobo.691.6">Additionally, we can optimize our LLM for the task of generating answers by exploiting the provided context. </span><span class="koboSpan" id="kobo.691.7">In this case, the LLM can be pushed to make the best use of the </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">found context.</span></span></p>
<p><span class="koboSpan" id="kobo.693.1">The encoder can also be fine-tuned. </span><span class="koboSpan" id="kobo.693.2">Fine-tuning the embedder with a specific dataset increases the contextual understanding of the model (remember that the encoder is a language model that has some contextual understanding of the data). </span><span class="koboSpan" id="kobo.693.3">This allows the LLM to better understand the domain-specific nuances of the data, leading to better contextual retrieval (chunks that are more relevant to the query). </span><span class="koboSpan" id="kobo.693.4">Of course, it is not always possible to obtain datasets for this task. </span><span class="koboSpan" id="kobo.693.5">However, there are approaches in which synthetic data is generated or large LLMs are used to create datasets appropriate for the </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">encoder fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">Fine-tuning an</span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.696.1"> embedder is much cheaper than training it from scratch. </span><span class="koboSpan" id="kobo.696.2">Today, many</span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.697.1"> models are available, and libraries such as Sentence Transformer facilitate the process of fine-tuning. </span><span class="koboSpan" id="kobo.697.2">These models have been pre-trained as embedders, but during fine-tuning, we want to better fit them to our particular data type or domain. </span><span class="koboSpan" id="kobo.697.3">Typically, this fine-tuning is conducted with supervised learning using datasets similar to those used for embedder training (with positive and </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">negative examples).</span></span></p>
<p><span class="koboSpan" id="kobo.699.1">Now that we have seen the main components, in the next section, we will assemble </span><span class="No-Break"><span class="koboSpan" id="kobo.700.1">the system.</span></span></p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.701.1">Using RAG to build a movie recommendation agent</span></h1>
<p><span class="koboSpan" id="kobo.702.1">In the previous sections, we </span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.703.1">discussed what RAG is and how this system can be used to reduce hallucinations or extend model knowledge. </span><span class="koboSpan" id="kobo.703.2">As we mentioned, this system is composed of the </span><a id="_idIndexMarker571"/><span class="No-Break"><span class="koboSpan" id="kobo.704.1">following components:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.705.1">An LLM to generate </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">the answer</span></span></li>
<li><span class="koboSpan" id="kobo.707.1">An encoder/retriever that transforms queries and documents </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">into vectors</span></span></li>
<li><span class="koboSpan" id="kobo.709.1">A vector database where we save </span><span class="No-Break"><span class="koboSpan" id="kobo.710.1">our vectors</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.711.1">We have, in this case, a dataset of movies and their description, and we want to create a system that, by asking a natural language question, will suggest the most suitable movies based on the information we’ve provided. </span><span class="koboSpan" id="kobo.711.2">Our LLM has no specific knowledge of the movies, and its parametric memory does not contain information about the latest releases. </span><span class="koboSpan" id="kobo.711.3">Therefore, RAG is a good system to supplement </span><span class="No-Break"><span class="koboSpan" id="kobo.712.1">its knowledge.</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.713.1">The first step is to obtain a corpus of chunks. </span><span class="koboSpan" id="kobo.713.2">Having taken a corpus of documents, we have to reduce it into chunks. </span><span class="koboSpan" id="kobo.713.3">A good compromise is to use a text splitter that preserves semantic information (without the need to use an LLM). </span><span class="koboSpan" id="kobo.713.4">In this case, we use a chunker with a size of 1,500 characters. </span><span class="koboSpan" id="kobo.713.5">We apply it to the column of our </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">data frame:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.715.1">
text_splitter = NLTKTextSplitter(chunk_size=1500)
def split_overview(overview):
    if pd.isna(overview):
        return []
    return text_splitter.split_text(str(overview))
df['chunks'] = df['text_column'].apply(split_overview)</span></pre></li> <li><span class="koboSpan" id="kobo.716.1">Next, we </span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.717.1">need to transform our chunks into vectors. </span><span class="koboSpan" id="kobo.717.2">In this case, we are using </span><strong class="source-inline"><span class="koboSpan" id="kobo.718.1">all-MiniLM-L6-v2</span></strong><span class="koboSpan" id="kobo.719.1"> as an embedder. </span><strong class="source-inline"><span class="koboSpan" id="kobo.720.1">all-MiniLM-L6-v2</span></strong><span class="koboSpan" id="kobo.721.1"> is a small model that has a good balance between embedding quality and speed. </span><span class="koboSpan" id="kobo.721.2">In fact, the </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.722.1">model has only 22.7 million parameters, which makes it extremely fast and a good initial choice for testing one’s </span><span class="No-Break"><span class="koboSpan" id="kobo.723.1">RAG pipeline.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.724.1">In this case, we do </span><span class="No-Break"><span class="koboSpan" id="kobo.725.1">the following:</span></span></p><ul><li><span class="koboSpan" id="kobo.726.1">Load the model (</span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">the embedder)</span></span></li><li><span class="koboSpan" id="kobo.728.1">Create a function to conduct </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">the embedding</span></span></li><li><span class="koboSpan" id="kobo.730.1">Conduct the embedding of the </span><span class="No-Break"><span class="koboSpan" id="kobo.731.1">various vectors</span></span><pre class="source-code"><span class="koboSpan" id="kobo.732.1">
embedder = SentenceTransformer('all-MiniLM-L6-v2')
def encode_chunk(chunk):
    if not isinstance(chunk, str) or chunk.strip() == "":
        return None
    return embedder.encode(chunk).tolist()
chunked_df['embeddings'] = chunked_df['chunks'].apply(encode_chunk)</span></pre></li></ul></li> <li><span class="koboSpan" id="kobo.733.1">At this point, we need to save the vectors we have created to a database. </span><span class="koboSpan" id="kobo.733.2">A popular choice as a vector database is Chroma. </span><span class="koboSpan" id="kobo.733.3">In this case, we need to do </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">the following:</span></span><ul><li><span class="koboSpan" id="kobo.735.1">Start the </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">Chroma client</span></span></li><li><span class="koboSpan" id="kobo.737.1">Create a </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">new collection</span></span></li><li><span class="koboSpan" id="kobo.739.1">Insert </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">the chunks</span></span><pre class="source-code"><span class="koboSpan" id="kobo.741.1">
chunked_df.dropna(subset=['embeddings'], inplace=True)
client = chromadb.Client()
collection = client.create_collection(name='movies')
for idx, row in chunked_df.iterrows():
    collection.add(
        ids=[str(idx)],
        embeddings=[row['embeddings']],
        metadatas=[{
            'original_title': row['original_title'],
            'chunk': row['chunks']
        }]
    )</span></pre></li></ul></li> </ol>
<p class="callout-heading"><span class="koboSpan" id="kobo.742.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.743.1">Note that we can add metadata (in this case, the title of </span><span class="No-Break"><span class="koboSpan" id="kobo.744.1">the film).</span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.745.1">We have </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.746.1">now implemented </span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.747.1">only the first part of the pipeline we described earlier. </span><span class="koboSpan" id="kobo.747.2">At present, we have a database with the </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">RAG vectors.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<span class="koboSpan" id="kobo.749.1"><img alt="Figure 5.21 – Vector database pipeline" src="image/B21257_05_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.750.1">Figure 5.21 – Vector database pipeline</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.751.1">Now, we</span><a id="_idIndexMarker576"/><span class="koboSpan" id="kobo.752.1"> need to create a pipeline when a query arrives (in inference). </span><span class="koboSpan" id="kobo.752.2">In</span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.753.1"> this case, we want to create a vector for the query and search the top </span><em class="italic"><span class="koboSpan" id="kobo.754.1">k</span></em><span class="koboSpan" id="kobo.755.1"> similar vectors in our vector database. </span><span class="koboSpan" id="kobo.755.2">This will return the text to an LLM and then generate an answer to </span><span class="No-Break"><span class="koboSpan" id="kobo.756.1">our query.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.757.1">We use the same embedder model we used to find the chunks, though, so we need a function that does </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">the following:</span></span><ul><li><span class="koboSpan" id="kobo.759.1">Creates a vector for </span><span class="No-Break"><span class="koboSpan" id="kobo.760.1">the query</span></span></li><li><span class="koboSpan" id="kobo.761.1">Finds the most </span><span class="No-Break"><span class="koboSpan" id="kobo.762.1">similar documents</span></span></li><li><span class="koboSpan" id="kobo.763.1">Returns the </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">associated text</span></span><pre class="source-code"><span class="koboSpan" id="kobo.765.1">
def retrieve_documents(query, collection, top_k=5):
    query_embedding = embedder.encode(query).tolist()
    results = collection.query( query_embeddings=[query_embedding], n_results=top_k )
    chunks = []
    titles = []
    for document in results['metadatas'][0]:
        chunks.append(document['chunk'])
        titles.append(document['original_title'])
    return chunks, titles</span></pre></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.766.1">At this</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.767.1"> point, we need to generate the answers, so we need an LLM. </span><span class="koboSpan" id="kobo.767.2">The</span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.768.1"> idea is to provide the LLM with clear instructions, so we create a simple prompt that explains the task to the model. </span><span class="koboSpan" id="kobo.768.2">We also provide the model with both the context and </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">the question:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.770.1">tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-Instruct-v0.1")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    device_map='auto')
text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    return_full_text=True,
    max_new_tokens=800)
def generate_answer(query, chunks, titles, text_generation_pipeline):
    context = "\n\n".join([f"Title: {title}\nChunk: {chunk}" for title, chunk in zip(titles, chunks)])
    prompt = f"""[INST]
    Instruction: You're an expert in movie suggestions. </span><span class="koboSpan" id="kobo.770.2">Your task is to analyze carefully the context and come up with an exhaustive answer to the following question:
    {query}
    Here is the context to help you:
    {context}
    [/INST]"""
    generated_text = text_generation_pipeline(prompt)[0]['generated_text']
    return generated_text</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.771.1">Now, we can test</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.772.1"> it. </span><span class="koboSpan" id="kobo.772.2">We </span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.773.1">can ask the system a question and see whether it generates </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">a response:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.775.1">
client = chromadb.Client()
collection = client.get_collection(name='movies')
query = "What are some good movies to watch on a rainy day?"
</span><span class="koboSpan" id="kobo.775.2">top_k = 5
chunks, titles = retrieve_documents(query, collection, top_k)
print(f"Retrieved Chunks: {chunks}")
print(f"Retrieved Titles: {titles}")
if chunks and titles:
    answer = generate_answer(query, chunks, titles, text_generation_pipeline)
    print(answer)
else:
    print("No relevant documents found to generate an answer.")</span></pre> <p><span class="koboSpan" id="kobo.776.1">We now have a complete system. </span><span class="koboSpan" id="kobo.776.2">The principle applies to any corpus </span><span class="No-Break"><span class="koboSpan" id="kobo.777.1">of documents.</span></span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.778.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.779.1">RAG is one of the fastest-growing paradigms in the field of LLMs. </span><span class="koboSpan" id="kobo.779.2">Eliminating hallucinations is one of the most important challenges and one of the most problematic constraints for LLMs and agents to be put into production. </span><span class="koboSpan" id="kobo.779.3">RAG is also a flexible system that has several advantages over fine-tuning. </span><span class="koboSpan" id="kobo.779.4">As we have seen, this system can be updated frequently with minimal cost and is compatible with different types of data. </span><span class="koboSpan" id="kobo.779.5">The naïve RAG is the basic system, consisting of three main components: an LLM, an embedder, and a </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">vector database.</span></span></p>
<p><span class="koboSpan" id="kobo.781.1">In the next chapter, we will see how this system is evolving. </span><span class="koboSpan" id="kobo.781.2">There are now many new additional components, which we will also look at. </span><span class="koboSpan" id="kobo.781.3">Despite RAG, sometimes the model still hallucinates as if it ignores the context. </span><span class="koboSpan" id="kobo.781.4">This is why sophisticated components have evolved, which we will look at in detail. </span><span class="koboSpan" id="kobo.781.5">We will also discuss the subtle interplay between parametric memory </span><span class="No-Break"><span class="koboSpan" id="kobo.782.1">and context.</span></span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.783.1">Further reading</span></h1>
<ul>
<li><span class="koboSpan" id="kobo.784.1">Lewis, </span><em class="italic"><span class="koboSpan" id="kobo.785.1">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span></em><span class="koboSpan" id="kobo.786.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">2020, </span></span><a href="https://arxiv.org/abs/2005.11401 "><span class="No-Break"><span class="koboSpan" id="kobo.788.1">https://arxiv.org/abs/2005.11401</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.789.1">ANN-Benchmarks</span></em><span class="koboSpan" id="kobo.790.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.791.1">2024, </span></span><a href="https://ann-benchmarks.com/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.792.1">https://ann-benchmarks.com/index.html</span></span></a></li>
<li><em class="italic"><span class="koboSpan" id="kobo.793.1">Hamming Distance between </span></em><em class="italic"><span class="koboSpan" id="kobo.794.1">Two </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.795.1">Strings</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.796.1">: </span></span><a href="https://www.geeksforgeeks.org/hamming-distance-two-strings/"><span class="No-Break"><span class="koboSpan" id="kobo.797.1">https://www.geeksforgeeks.org/hamming-distance-two-strings/</span></span></a></li>
</ul>
</div>
</body></html>