["```py\n    import re\n    import pandas as pd\n    ```", "```py\n    data_file = \"../data/DataScientist.csv\"\n    df = pd.read_csv(data_file, encoding='utf-8')\n    print(df)\n    ```", "```py\n    def get_list_of_items(df, column_name):\n        values = df[column_name].values\n        values = [item for sublist in values for item in sublist]\n        list_of_items = list(set(values))\n        return list_of_items\n    ```", "```py\n    def get_emails(df):\n        email_regex = '[^\\s:|()\\']+@[a-zA-Z0-9\\.]+\\.[a-zA-Z]+'\n        df['emails'] = df['Job Description'].apply(\n            lambda x: re.findall(email_regex, x))\n        emails = get_list_of_items(df, 'emails')\n        return emails\n    ```", "```py\n    emails = get_emails(df)\n    print(emails)\n    ['hrhelpdesk@phila.gov', 'talent@quartethealth.com', …, 'careers@edo.com', 'Talent.manager@techquarry.com', 'resumes@nextgentechinc.com', …, 'talent@ebay.com', …, 'info@springml.com',…]\n    ```", "```py\n    def get_urls(df):\n        url_regex = '(http[s]?://(www\\.)?[A-Za-z0-9–_\\.\\-]+\\.[A-Za-z]+/?[A-Za-z0-9$\\–_\\-\\/\\.]*)[\\.)\\\"]*'\n        df['urls'] = df['Job Description'].apply(\n            lambda x: [\n                x[item.span()[0]:item.span()[1]] \n                for item in re.finditer(url_regex, x)\n            ]\n        )\n        urls = get_list_of_items(df, 'urls')\n        return urls\n    ```", "```py\n    urls = get_urls(df)\n    print(urls)\n    ```", "```py\n    ['https://youtu.be/c5TgbpE9UBI', 'https://www.linkedin.com/in/emma-riley-72028917a/', 'https://www.dol.gov/ofccp/regs/compliance/posters/ofccpost.htm', 'https://www.naspovaluepoint.org/portfolio/mmis-provider-services-module-2018-2028/hhs-technology-group/).', 'https://www.instagram.com/gatestonebpo', 'http://jobs.sdsu.edu', 'http://www.colgatepalmolive.com.', 'http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf', 'https://www.gofundme.com/2019https', 'https://www.decode-m.com/', 'https://bit.ly/2lCOcYS',…]\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    import pandas as pd\n    import Levenshtein\n    ```", "```py\n    data_file = \"../data/DataScientist.csv\"\n    df = pd.read_csv(data_file, encoding='utf-8')\n    ```", "```py\n    emails = get_emails(df)\n    ```", "```py\n    def find_levenshtein(input_string, df):\n        df['distance_to_' + input_string] = \\\n            df['emails'].apply(lambda x: Levenshtein.distance(\n                input_string, x))\n        return df\n    ```", "```py\n    def get_closest_email_lev(df, email):\n        df = find_levenshtein(email, df)\n        column_name = 'distance_to_' + email\n        minimum_value_email_index = df[column_name].idxmin()\n        email = df.loc[minimum_value_email_index]['emails']\n        return email\n    ```", "```py\n    new_df = pd.DataFrame(emails,columns=['emails'])\n    input_string = \"rohitt.macdonald@prelim.com\"\n    email = get_closest_email_lev(new_df, input_string)\n    print(email)\n    ```", "```py\n    rohit.mcdonald@prolim.com\n    ```", "```py\n    def find_jaro(input_string, df):\n        df['distance_to_' + input_string] = df['emails'].apply(\n            lambda x: Levenshtein.jaro(input_string, x)\n        )\n        return df\n    ```", "```py\n    def get_closest_email_jaro(df, email):\n        df = find_jaro(email, df)\n        column_name = 'distance_to_' + email\n        maximum_value_email_index = df[column_name].idxmax()\n        email = df.loc[maximum_value_email_index]['emails']\n        return email\n    ```", "```py\n    email = get_closest_email_jaro(new_df, input_string)\n    print(email)\n    ```", "```py\n    rohit.mcdonald@prolim.com\n    ```", "```py\n    print(Levenshtein.jaro_winkler(\"rohit.mcdonald@prolim.com\",\n        \"rohit.mcdonald@prolim.org\"))\n    ```", "```py\n    1.0\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    from datasets import load_dataset\n    from nltk import word_tokenize\n    from math import ceil\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from nltk.corpus import stopwords\n    ```", "```py\n    train_dataset = load_dataset(\"SetFit/bbc-news\", split=\"train\")\n    test_dataset = load_dataset(\"SetFit/bbc-news\", split=\"test\")\n    train_df = train_dataset.to_pandas()\n    test_df = test_dataset.to_pandas()\n    print(train_df)\n    print(test_df)\n    ```", "```py\n         text  label     label_text\n    0  wales want rugby league training wales could f... 2  sport\n    1     china aviation seeks rescue deal scandal-hit j...  business\n    ...     ...    ...            ...\n    1223  why few targets are better than many the econo... 1  business\n    1224  boothroyd calls for lords speaker betty boothr... 4  politics\n    [1225 rows x 3 columns]\n         text  label     label_text\n    0  carry on star patsy rowlands dies actress pats... 3  entertainment\n    1    sydney to host north v south game sydney will ... 2  sport\n    ..     ...    ...            ...\n    998  stormy year for property insurers a string of ... 1  business\n    999  what the election should really be about  a ge... 4  politics\n    [1000 rows x 3 columns]\n    ```", "```py\n    vectorizer = TfidfVectorizer(stop_words='english', \n        min_df=2, max_df=0.95)\n    vectorizer.fit(train_df[\"text\"])\n    ```", "```py\n    def sort_data_tfidf_score(coord_matrix):\n        tuples = zip(coord_matrix.col, coord_matrix.data)\n        return sorted(tuples, key=lambda x: (x[1], x[0]), \n            reverse=True)\n    ```", "```py\n    def get_keyword_strings(vectorizer, num_words, sorted_vector):\n        words = []\n        index_dict = vectorizer.get_feature_names_out()\n        for (item_index, score) in sorted_vector[0:num_words]:\n            word = index_dict[item_index]\n            words.append(word)\n        return words\n    ```", "```py\n    def get_keywords_simple(vectorizer, input_text,\n        num_output_words=10):\n        vector = vectorizer.transform([input_text])\n        sorted = sort_data_tfidf_score(vector.tocoo())\n        words = get_keyword_strings(vectorizer, num_output_words, \n            sorted)\n        return words\n    ```", "```py\n    print(test_df.iloc[0][\"text\"])\n    keywords = get_keywords_simple(vectorizer,\n        test_df.iloc[0][\"text\"])\n    print(keywords)\n    ```", "```py\n    carry on star patsy rowlands dies actress patsy rowlands  known to millions for her roles in the carry on films  has died at the age of 71.  rowlands starred in nine of the popular carry on films  alongside fellow regulars sid james  kenneth williams and barbara windsor...\n    ['carry', 'theatre', 'scholarship', 'appeared', 'films', 'mrs', 'agent', 'drama', 'died', 'school']\n    ```", "```py\n    stop_words = list(stopwords.words('english'))\n    stop_words.remove(\"the\")\n    trigram_vectorizer = TfidfVectorizer(\n        stop_words=stop_words, min_df=2,\n        ngram_range=(1,3), max_df=0.95)\n    trigram_vectorizer.fit(train_df[\"summary\"])\n    ```", "```py\n    def get_keyword_strings_all(vectorizer, sorted_vector):\n        words = []\n        index_dict = vectorizer.get_feature_names_out()\n        for (item_index, score) in sorted_vector:\n            word = index_dict[item_index]\n            words.append(word)\n        return words\n    ```", "```py\n    def get_keywords_complex(\n        vectorizer, input_text, spacy_model, num_words=70\n    ):\n        keywords = []\n        doc = spacy_model(input_text)\n        vector = vectorizer.transform([input_text])\n        sorted = sort_coo(vector.tocoo())\n        ngrams = get_keyword_strings_all(vectorizer, sorted)\n        ents = [ent.text.lower() for ent in doc.noun_chunks]\n        for i in range(0, num_words):\n            keyword = ngrams[i]\n            if keyword.lower() in ents and not\n            keyword.isdigit() and keyword not in keywords:\n                keywords.append(keyword)\n        return keywords\n    ```", "```py\n    keywords = get_keywords_complex(trigram_vectorizer,\n        test_df.iloc[0][\"summary\"], small_model)\n    print(keywords)\n    ```", "```py\n    ['the gop', 'the 50 states', 'npr', '11 states', 'state', 'republican governors', 'the dems', 'reelection', 'the helm', 'grabs']\n    ```", "```py\npython -m spacy download en_core_web_sm\npython -m spacy download en_core_web_lg\n```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    article = \"\"\"iPhone 12: Apple makes jump to 5G\n    Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks.\n    The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen.\n    The US firm bucked a wider industry downturn by increasing its handset sales over the past year.\n    But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6.\n    …\n    \"Networks are going to have to offer eye-wateringly attractive deals, and the way they're going to do that is on great tariffs and attractive trade-in deals,\"\n    predicted Ben Wood from the consultancy CCS Insight. Apple typically unveils its new iPhones in September, but opted for a later date this year.\n    It has not said why, but it was widely speculated to be related to disruption caused by the coronavirus pandemic. The firm's shares ended the day 2.7% lower.\n    This has been linked to reports that several Chinese internet platforms opted not to carry the livestream,\n    although it was still widely viewed and commented on via the social media network Sina Weibo.\"\"\"\n    ```", "```py\n    doc = small_model(article)\n    print(len(doc.ents))\n    small_model_ents = doc.ents\n    for ent in doc.ents:\n        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n    ```", "```py\n    44\n    12 7 9 CARDINAL\n    Apple 11 16 ORG\n    5 31 32 CARDINAL\n    …\n    a later date this year 2423 2445 DATE\n    2.7% 2594 2598 PERCENT\n    Chinese 2652 2659 NORP\n    Sina Weibo 2797 2807 PERSON\n    ```", "```py\n    doc = large_model(article)\n    print(len(doc.ents))\n    large_model_ents = doc.ents\n    for ent in doc.ents:\n        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n    ```", "```py\n    46\n    12 7 9 CARDINAL\n    Apple 11 16 ORG\n    5 31 32 CARDINAL\n    …\n    the day 2586 2593 DATE\n    2.7% 2594 2598 PERCENT\n    Chinese 2652 2659 NORP\n    Sina Weibo 2797 2807 PERSON\n    ```", "```py\n    small_model_ents = [str(ent) for ent in small_model_ents]\n    large_model_ents = [str(ent) for ent in large_model_ents]\n    in_small_not_in_large = set(small_model_ents) \\ \n        - set(large_model_ents)\n    in_large_not_in_small = set(large_model_ents) \\ \n        - set(small_model_ents)\n    print(in_small_not_in_large)\n    print(in_large_not_in_small)\n    ```", "```py\n    {'iPhone 11', 'iPhone', 'iPhones'}\n    {'6', 'the day', 'IDC', '11', 'Pro', 'G\\nApple', 'SE'}\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    import pandas as pd\n    from spacy.cli.train import train\n    from spacy.cli.evaluate import evaluate\n    from spacy.tokens import DocBin\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    music_ner_df = pd.read_csv('../data/music_ner.csv')\n    print(music_ner_df)\n    ```", "```py\n    def change_label(input_label):\n        input_label = input_label.replace(\"_deduced\", \"\")\n        return input_label\n    music_ner_df[\"label\"] = music_ner_df[\"label\"].apply(change_label)\n    print(music_ner_df)\n    ```", "```py\n    train_db = DocBin()\n    test_db = DocBin()\n    ```", "```py\n    # Get a unique list of unique ids\n    ids = list(set(music_ner_df[\"id\"].values))\n    print(len(ids))\n    # Split ids into training and test\n    train_ids, test_ids = train_test_split(ids)\n    print(len(train_ids))\n    print(len(test_ids))\n    ```", "```py\n    227\n    170\n    57\n    ```", "```py\n    for id in ids:\n        entity_rows = music_ner_df.loc[music_ner_df['id'] == id]\n        text = entity_rows.head(1)[\"text\"].values[0]\n        doc = small_model(text)\n        ents = []\n        for index, row in entity_rows.iterrows():\n            label = row[\"label\"]\n            start = row[\"start_offset\"]\n            end = row[\"end_offset\"]\n            span = doc.char_span(start, end, label=label, \n                alignment_mode=\"contract\")\n            ents.append(span)\n        doc.ents = ents\n        if id in train_ids:\n            train_db.add(doc)\n        else:\n            test_db.add(doc)\n    train_db.to_disk('../data/music_ner_train.spacy')\n    test_db.to_disk('../data/music_ner_test.spacy')\n    ```", "```py\n    train(\"../data/spacy_config_ner.cfg\", output_path=\"../models/spacy_music_ner\")\n    ```", "```py\n    nlp = spacy.load(\"../models/spacy_music_ner/model-last\")\n    first_test_id = test_ids[0]\n    test_rows = music_ner_df.loc[music_ner_df['id'] \n        == first_test_id]\n    input_text = entity_rows.head(1)[\"text\"].values[0]\n    print(input_text)\n    print(\"Gold entities:\")\n    for index, row in entity_rows.iterrows():\n        label = row[\"label\"]\n        start = row[\"start_offset\"]\n        end = row[\"end_offset\"]\n        span = doc.char_span(start, end, label=label,\n            alignment_mode=\"contract\")\n        print(span)\n    doc = nlp(input_text)\n    print(\"Predicted entities: \")\n    for entity in doc.ents:\n        print(entity)\n    ```", "```py\n    songs with themes of being unable to settle | ex hoziers someone new elle kings exes and ohs\n    Gold entities:\n    hoziers\n    someone new\n    elle kings\n    exes and ohs\n    Predicted entities:\n    hoziers\n    someone new\n    elle kings\n    exes and\n    ```", "```py\n    evaluate('../models/spacy_music_ner/model-last', '../data/music_ner_tes t.spacy')\n    ```", "```py\n    {'token_acc': 1.0,\n     'token_p': 1.0,\n     'token_r': 1.0,\n     'token_f': 1.0,\n     'tag_acc': 0.800658978583196,\n    …\n     'ents_p': 0.4421052631578947,\n     'ents_r': 0.42,\n     'ents_f': 0.4307692307692308,\n     'ents_per_type': {'WoA': {'p': 0.4358974358974359,\n       'r': 0.425,\n       'f': 0.43037974683544306},\n      'Artist_or_WoA': {'p': 0.1,\n       'r': 0.09090909090909091,\n       'f': 0.09523809523809525},\n      'Artist': {'p': 0.5217391304347826,\n       'r': 0.4897959183673469,\n       'f': 0.5052631578947369}},\n     'speed': 3835.591242612551}\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    from datasets import (\n        load_dataset, Dataset, Features, Value,\n        ClassLabel, Sequence, DatasetDict)\n    import pandas as pd\n    from transformers import AutoTokenizer, AutoModel\n    from transformers import DataCollatorForTokenClassification\n    from transformers import (\n        AutoModelForTokenClassification,\n        TrainingArguments, Trainer)\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n    from evaluate import load\n    ```", "```py\n    music_ner_df = pd.read_csv('../data/music_ner.csv')\n    def change_label(input_label):\n        input_label = input_label.replace(\"_deduced\", \"\")\n        return input_label\n    music_ner_df[\"label\"] = music_ner_df[\"label\"].apply(\n        change_label)\n    music_ner_df[\"text\"] = music_ner_df[\"text\"].apply(\n        lambda x: x.replace(\"|\", \",\"))\n    print(music_ner_df)\n    ```", "```py\n    ids = list(set(music_ner_df[\"id\"].values))\n    docs = {}\n    for id in ids:\n        entity_rows = music_ner_df.loc[music_ner_df['id'] == id]\n        text = entity_rows.head(1)[\"text\"].values[0]\n        doc = small_model(text)\n        ents = []\n        for index, row in entity_rows.iterrows():\n            label = row[\"label\"]\n            start = row[\"start_offset\"]\n            end = row[\"end_offset\"]\n            span = doc.char_span(start, end, label=label,\n                alignment_mode=\"contract\")\n            ents.append(span)\n        doc.ents = ents\n        docs[doc.text] = doc\n    ```", "```py\n    data_file = \"../data/music_ner_bio.bio\"\n    tag_mapping = {\"O\": 0, \"B-Artist\": 1, \"I-Artist\": 2, \n        \"B-WoA\": 3, \"I-WoA\": 4}\n    with open(data_file) as f:\n        data = f.read()\n    tokens = []\n    ner_tags = []\n    spans = []\n    sentences = data.split(\"\\n\\n\")\n    for sentence in sentences:\n        words = []\n        tags = []\n        this_sentence_spans = []\n        word_tag_pairs = sentence.split(\"\\n\")\n        for pair in word_tag_pairs:\n            (word, tag) = pair.split(\"\\t\")\n            words.append(word)\n            tags.append(tag_mapping[tag])\n        sentence_text = \" \".join(words)\n        try:\n            doc = docs[sentence_text]\n        except:\n            pass\n        ent_dict = {}\n        for ent in doc.ents:\n            this_sentence_spans.append(f\"{ent.label_}: {ent.text}\")\n        tokens.append(words)\n        ner_tags.append(tags)\n        spans.append(this_sentence_spans)\n    ```", "```py\n    indices = range(0, len(spans))\n    train, test = train_test_split(indices, test_size=0.1)\n    train_tokens = []\n    test_tokens = []\n    train_ner_tags = []\n    test_ner_tags = []\n    train_spans = []\n    test_spans = []\n    for i, (token, ner_tag, span) in enumerate(\n        zip(tokens, ner_tags, spans)\n    ):\n        if i in train:\n            train_tokens.append(token)\n            train_ner_tags.append(ner_tag)\n            train_spans.append(span)\n        else:\n            test_tokens.append(token)\n            test_ner_tags.append(ner_tag)\n            test_spans.append(span)\n    print(len(train_spans))\n    print(len(test_spans))\n    ```", "```py\n    539\n    60\n    ```", "```py\n    training_df = pd.DataFrame({\"tokens\":train_tokens,\n        \"ner_tags\": train_ner_tags, \"spans\": train_spans})\n    test_df = pd.DataFrame({\"tokens\": test_tokens,\n        \"ner_tags\": test_ner_tags, \"spans\": test_spans})\n    training_df[\"text\"] = training_df[\"tokens\"].apply(\n        lambda x: \" \".join(x))\n    test_df[\"text\"] = test_df[\"tokens\"].apply(lambda x: \" \".join(x))\n    training_df.dropna()\n    test_df.dropna()\n    print(test_df)\n    ```", "```py\n                                                   tokens  \\\n    0   [i, love, radioheads, kid, a, something, simil...\n    1   [bluesy, songs, kinda, like, evil, woman, by, ...\n    ...\n    58  [looking, for, like, electronic, music, with, ...\n    59  [looking, for, pop, songs, about, the, end, of...\n                                                 ner_tags  \\\n    0       [0, 0, 1, 3, 4, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0]\n    1                         [0, 0, 0, 0, 3, 4, 0, 1, 2]\n    ...\n    58      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    59                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n                                                    spans  \\\n    0     [Artist: radioheads, Artist_or_WoA: aphex twin]\n    1            [WoA: evil woman, Artist: black sabbath]\n    ...\n    58  [WoA: the piper at the gates of dawn, Artist: ...\n    59  [WoA: the piper at the gates of dawn, Artist: ...\n                                                     text\n    0   i love radioheads kid a something similar , ki...\n    1   bluesy songs kinda like evil woman by black sa...\n    ...\n    58  looking for like electronic music with a depre...\n    59   looking for pop songs about the end of the world\n    ```", "```py\n    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n    features = Features(\n        {'tokens': Sequence(feature=Value(dtype='string',\n                id=None),\n            length=-1, id=None),\n                'ner_tags': Sequence(feature=ClassLabel(\n                    names=['O', 'B-Artist', 'I-Artist',\n                    'B-WoA', 'I-WoA'], id=None),\n                    length=-1, id=None),\n                'spans': Sequence(\n                    feature=Value(dtype='string',id=None),\n                    length=-1, id=None),\n                'text': Value(dtype='string', id=None)\n                        })\n    training_dataset = Dataset.from_pandas(\n        training_df, features=features)\n    test_dataset = Dataset.from_pandas(test_df, features=features)\n    dataset = DatasetDict({\"train\":training_dataset, \n        \"test\":test_dataset})\n    print(dataset[\"train\"].features)\n    label_names = \\\n        dataset[\"train\"].features[\"ner_tags\"].feature.names\n    print(dataset)\n    ```", "```py\n    {'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(names=['O', 'B-Artist', 'I-Artist', 'B-WoA', 'I-WoA'], id=None), length=-1, id=None), 'spans': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'text': Value(dtype='string', id=None)}\n    DatasetDict({\n        train: Dataset({\n            features: ['tokens', 'ner_tags', 'spans', 'text'],\n            num_rows: 539\n        })\n        test: Dataset({\n            features: ['tokens', 'ner_tags', 'spans', 'text'],\n            num_rows: 60\n        })\n    })\n    ```", "```py\n    def tokenize_adjust_labels(all_samples_per_split):\n        tokenized_samples = tokenizer.batch_encode_plus(\n        all_samples_per_split[\"text\"])\n        total_adjusted_labels = []\n        for k in range(0, len(tokenized_samples[\"input_ids\"])):\n            prev_wid = -1\n            word_ids_list = tokenized_samples.word_ids(\n                batch_index=k)\n            existing_label_ids = all_samples_per_split[\n                \"ner_tags\"][k]\n            i = -1\n            adjusted_label_ids = []\n            for wid in word_ids_list:\n                if (wid is None):\n                    adjusted_label_ids.append(-100)\n                elif (wid != prev_wid):\n                    i = i + 1\n                    adjusted_label_ids.append(existing_label_ids[i])\n                    prev_wid = wid\n                else:\n                    label_name =label_names[existing_label_ids[i]]\n                    adjusted_label_ids.append(existing_label_ids[i])\n            total_adjusted_labels.append(adjusted_label_ids)\n        tokenized_samples[\"labels\"] = total_adjusted_labels\n        return tokenized_samples\n    ```", "```py\n    tokenized_dataset = dataset.map(tokenize_adjust_labels, \n        batched=True)\n    ```", "```py\n    data_collator = DataCollatorForTokenClassification(tokenizer)\n    ```", "```py\n    metric = load(\"seqeval\")\n    def compute_metrics(data):\n        predictions, labels = data\n        predictions = np.argmax(predictions, axis=2)\n        data = zip(predictions, labels)\n        data = [\n            [(p, l) for (p, l) in zip(prediction, label) \n                if l != -100]\n            for prediction, label in data\n        ]\n        true_predictions = [\n            [label_names[p] for (p, l) in data_point]\n            for data_point in data\n        ]\n        true_labels = [\n            [label_names[l] for (p, l) in data_point]\n            for data_point in data\n        ]\n        results = metric.compute(predictions=true_predictions, \n            references=true_labels)\n        flat_results = {\n            \"overall_precision\": results[\"overall_precision\"],\n            \"overall_recall\": results[\"overall_recall\"],\n            \"overall_f1\": results[\"overall_f1\"],\n            \"overall_accuracy\": results[\"overall_accuracy\"],\n        }\n        for k in results.keys():\n          if (k not in flat_results.keys()):\n            flat_results[k + \"_f1\"] = results[k][\"f1\"]\n        return flat_results\n    ```", "```py\n    model = AutoModelForTokenClassification.from_pretrained(\n        'bert-base-uncased', num_labels=len(label_names))\n    training_args = TrainingArguments(\n        output_dir=\"./fine_tune_bert_output\",\n        evaluation_strategy=\"steps\",\n        learning_rate=2e-5,\n        per_device_train_batch_size=16,\n        per_device_eval_batch_size=16,\n        num_train_epochs=7,\n        weight_decay=0.01,\n        logging_steps = 1000,\n        run_name = \"ep_10_tokenized_11\",\n        save_strategy='no'\n    )\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset[\"train\"],\n        eval_dataset=tokenized_dataset[\"test\"],\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics\n    )\n    trainer.train()\n    ```", "```py\n    TrainOutput(global_step=238, training_loss=0.25769581514246326, metrics={'train_runtime': 25.8951, 'train_samples_per_second': 145.703, 'train_steps_per_second': 9.191, 'total_flos': 49438483110900.0, 'train_loss': 0.25769581514246326, 'epoch': 7.0})\n    ```", "```py\n    trainer.evaluate()\n    ```", "```py\n    {'eval_loss': 0.28670933842658997,\n     'eval_overall_precision': 0.6470588235294118,\n     'eval_overall_recall': 0.7096774193548387,\n     'eval_overall_f1': 0.6769230769230768,\n     'eval_overall_accuracy': 0.9153605015673981,\n     'eval_Artist_f1': 0.761904761904762,\n     'eval_WoA_f1': 0.5217391304347826,\n     'eval_runtime': 0.3239,\n     'eval_samples_per_second': 185.262,\n     'eval_steps_per_second': 12.351,\n     'epoch': 7.0}\n    ```", "```py\n    trainer.save_model(\"../models/bert_fine_tuned\")\n    ```", "```py\n    model = AutoModelForTokenClassification.from_pretrained(\"../models/bert_fine_tuned\")\n    tokenizer = AutoTokenizer.from_pretrained(\n        \"../models/bert_fine_tuned\")\n    ```", "```py\n    text = \"music similar to morphine robocobra quartet | featuring elements like saxophone prominent bass\"\n    from transformers import pipeline\n    pipe = pipeline(task=\"token-classification\",\n        model=model.to(\"cpu\"), tokenizer=tokenizer,\n        aggregation_strategy=\"simple\")\n    pipe(text)\n    # tag_mapping = {\"O\": 0, \"B-Artist\": 1, \"I-Artist\": 2, \"B-WoA\": 3, \"I-WoA\": 4}\n    ```", "```py\n    [{'entity_group': 'LABEL_0',\n      'score': 0.9991929,\n      'word': 'music similar to',\n      'start': 0,\n      'end': 16},\n     {'entity_group': 'LABEL_1',\n      'score': 0.8970744,\n      'word': 'morphine robocobra',\n      'start': 17,\n      'end': 35},\n     {'entity_group': 'LABEL_2',\n      'score': 0.5060059,\n      'word': 'quartet',\n      'start': 36,\n      'end': 43},\n     {'entity_group': 'LABEL_0',\n      'score': 0.9988042,\n      'word': '| featuring elements like saxophone prominent bass',\n      'start': 44,\n      'end': 94}]\n    ```"]