<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-108"><a id="_idTextAnchor180"/><a id="_idTextAnchor181"/>5</h1>
<h1 id="_idParaDest-109"><a id="_idTextAnchor182"/>Fine-Tuning Generative Models for Specific Tasks</h1>
<p>In our narrative with StyleSprint, we described using a pre-trained generative AI model for creating engaging product descriptions. While this model showed adeptness in generating diverse content, StyleSprint’s evolving needs require a shift in focus. The new challenge is not just about producing content but also about engaging in specific, task-oriented interactions such as automatically answering customer’s specific questions about the products described.</p>
<p>In this chapter, we introduce the concept of fine-tuning, a vital step in adapting a pre-trained model to perform specific downstream tasks. For StyleSprint, this means transforming the model from a versatile content generator to a specialized tool capable of providing accurate and detailed responses to customer questions.</p>
<p>We will explore and define a range of scalable fine-tuning techniques, comparing them with other approaches such as in-context learning. We will demonstrate advanced fine-tuning methods, including parameter-efficient fine-tuning and prompt tuning, to demonstrate how they can fine-tune a model’s abilities for specific tasks such as Q&amp;A.</p>
<p>By the end of this chapter, we will have trained a language model to answer questions and do so in a way that aligns with StyleSprint’s brand guidelines. However, before we explore the mechanics of fine-tuning and its importance in our application, we will revisit the history of fine-tuning in the context of LLMs.<a id="_idTextAnchor183"/></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor184"/>Foundation and relevance – an introduction to fine-tuning</h1>
<p>Fine-tuning is the process<a id="_idIndexMarker420"/> of leveraging a model pre-trained on a large dataset and continuing the training process on a smaller, task-specific dataset to improve its performance on that task. It may also involve additional training that adapts a model to the nuances of a new domain. The latter<a id="_idIndexMarker421"/> is known as domain adaptation, which we will cover in <a href="B21773_06.xhtml#_idTextAnchor211"><em class="italic">Chapter 6</em></a>. The former is typically referred to as task-specific fine-tuning, and it can<a id="_idIndexMarker422"/> be performed to accomplish several tasks, including Q&amp;A, summarization, classification, and many others. For this chapter, we will focus on task-specific fine-tuning to improve a general-purpose model’s performance when answering questions.</p>
<p>For StyleSprint, fine-tuning a model to handle a specific task such as answering customer inquiries about products introduces unique challenges. Unlike generating product descriptions, which primarily involves language generation using an out-of-the-box pre-trained model, answering customer questions requires the model to have an extensive understanding of product-specific data and should have a brand-aware voice. Specifically, the model must accurately interpret and respond to questions about product features, sizes, availability, user reviews, and many other details. It should also produce answers consistent with StyleSprint’s distinct brand tone. This task requires both generalized natural language proficiency (from pre-training) and robust knowledge of product metadata and customer feedback, accomplished through fine-tuning.</p>
<p>Models such as GPT initially learn to predict text through an unsupervised learning process that involves being trained on wide-ranging and vast datasets. This pre-training phase exposes the model to a diverse array of texts, enabling it to gain a broad understanding of language, including syntax, grammar, and context, without any specific task-oriented guidance. However, fine-tuning applies task-oriented, supervised learning to refine the model’s capabilities to accomplish the specified task – specifically, semi-supervised learning, which, as described by Radford et al. (2018), involves adapting the model to a specific supervised task by exposing it to a dataset comprising input sequences (<code>x1</code>, ..., <code>xm</code>) and corresponding labels (<code>y</code>).</p>
<p>Throughout the chapter, we will detail the fine-tuning<a id="_idIndexMarker423"/> process, including how to selectively train the model on a curated dataset of product-related information and customer interactions, enabling it to respond with the informed, brand-aligned precision that customers expect. However, fine-tuning an LLM, which could have billions of parameters, would typically require an enormous number of resources and time. This <a id="_idIndexMarker424"/>is where advanced techniques such as <strong class="bold">Parameter-Efficient Fine-Tuning</strong> (<strong class="bold">PEFT</strong>) become particularly valuable in making fine-tuning access<a id="_idTextAnchor185"/>i<a id="_idTextAnchor186"/>ble.</p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor187"/>PEFT</h1>
<p>Traditional fine-tuning<a id="_idIndexMarker425"/> methods become increasingly impractical as the model size grows due to the immense computational resources and time required to train and update all model parameters. For most businesses, including larger organizations, a classical approach to fine-tuning is cost-prohibitive and, effectively, a non-starter.</p>
<p>Alternatively, PEFT methods modify only a small subset of a model’s parameters, reducing the computational burden while still achieving state-of-the-art performance. This method is advantageous for adapting large models to specific tasks without extensive retraining.</p>
<p>One such PEFT method is the <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>) methodology, developed by Hu et al. (20<a id="_idTextAnchor188"/>21).</p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor189"/>LoRA</h2>
<p>The LoRA method focuses<a id="_idIndexMarker426"/> on selectively fine-tuning specific components<a id="_idIndexMarker427"/> within the Transformer architecture to enhance efficiency and effectiveness in LLMS. LoRA targets the weight matrices found in the self-attention module of the Transformer, which, as discussed in <a href="B21773_03.xhtml#_idTextAnchor081"><em class="italic">Chapter 3</em></a>, are key to its functionality and include four matrices: wq (query), wk (key), wv (value), and wo (output). Although these matrices can be divided into multiple heads in a multi-head attention setting – where each <em class="italic">head</em> represents one of several parallel attention mechanisms that process inputs independently – LoRA treats them as singular matrices, simplifying the adaptation process.</p>
<p>LoRA’s approach involves adapting only the attention weights for downstream tasks, while the weights<a id="_idIndexMarker428"/> in the other component of the Transformer, the <strong class="bold">feed-forward network</strong> (<strong class="bold">FFN</strong>), are unchanged. This decision to focus exclusively on the attention weights and freeze the FFN is made for simplicity and parameter efficiency. By doing so, LoRA ensures a more manageable and resource-efficient fine-tuning process, avoiding the complexities and demands of retraining the entire network.</p>
<p>This selective fine-tuning strategy enables LoRA to effectively tailor the model for specific tasks while maintaining the overall structure and strengths of the pre-trained model. This makes LoRA a practical solution for adapting LLMs to new tasks with a reduced computational burden without requiring comprehensive parameter updates across the entire model (Liu et al., 2021).</p>
<p>Building upon<a id="_idIndexMarker429"/> the foundation of LoRA, <strong class="bold">Adaptive Low-Rank Adaptation</strong> (<strong class="bold">AdaLoRA</strong>), as introduced in a study by Liu et al. (2022), represents a further advancement in PEFT methods. The key difference between LoRA and AdaLoRA lies in (as the name suggests) its adaptiveness. While LoRA applies a consistent, low-rank approach to fine-tuning across the model, AdaLoRA tailors the updates to the needs of each layer, offering a more flexible<a id="_idIndexMarker430"/> and potentially more effective way to fine-tune<a id="_idIndexMarker431"/> large models for speci<a id="_idTextAnchor190"/>fic tasks.</p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor191"/>AdaLoRA</h2>
<p>AdaLoRA’s key innovation<a id="_idIndexMarker432"/> lies in its adaptive<a id="_idIndexMarker433"/> allocation of the <strong class="bold">parameter budget</strong> among the weight matrices<a id="_idIndexMarker434"/> of the pre-trained model. Many PEFT methods tend to distribute the parameter budget evenly across all pre-trained weight matrices, potentially neglecting the varying importance of different weight parameters. AdaLoRA overcomes this by assigning importance scores to these weight matrices and allocating the parameter budget accordingly. <strong class="bold">Importance scores</strong> in the context of AdaLoRA are metrics<a id="_idIndexMarker435"/> used to determine the significance (or importance) of different weight parameters in a model, guiding the allocation of the parameter budget more effectively during fine-tuning.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">Parameter budget</em> refers to the predefined limit<a id="_idIndexMarker436"/> on the number of additional parameters that can be introduced during the fine-tuning of a pre-trained model. This budget is set to ensure that the model’s complexity does not increase significantly, which can lead to challenges such as overfitting, increased computational costs, and longer training times.</p>
<p>Additionally, AdaLoRA applies <strong class="bold">singular value decomposition</strong> (<strong class="bold">SVD</strong>) to efficiently organize the incremental<a id="_idIndexMarker437"/> updates made during the model’s fine-tuning process. SVD allows for the effective pruning of singular values associated with less critical updates, reducing the overall parameter budget required for fine-tuning. It is important to note that this method also avoids the need for computationally intensive exact computations, making the fine-tuning process more efficient.</p>
<p>AdaLoRA has been empirically tested across various domains, including natural language processing, question-answering, and natural language generation. Extensive experiments have demonstrated its effectiveness in improving model performance, particularly in question-answering tasks. The adaptability and efficiency of AdaLoRA make it an ideal choice for applications requiring precise and efficient model adjustments for complex tasks.</p>
<p>In the case of StyleSprint, AdaLoRA presents an opportunity to fine-tune its language model for answering customer questions without the considerable overhead that would be incurred by traditional fine-tuning, which would require adjusting all of the model parameters. By adopting AdaLoRA, StyleSprint can efficiently adapt its model to handle nuanced customer inquiries by adjusting significantly fewer parameters. Specifically, AdaLoRA’s adaptive allocation of parameter budgets means that StyleSprint can optimize its model for the specific nuances of customer queries without using extensive computational resources.</p>
<p>By the end of this chapter, we will have fine-tuned an LLM using AdaLoRA for our Q&amp;A task. However, we should first decide whether fine-tuning is truly the right approach. Prompt-based LLMs offer a viable<a id="_idIndexMarker438"/> alternative known as in-context learning, where the model can learn from examples given in the prompt, meaning that the prompt would contain the customer’s question paired with a few key historical examples of how other questions were answered. The model can infer from the examples how to answer the question at hand in a way that is consistent with the examples. In the next section, we will explore the benefits and drawbacks of in-context learning to help us determine whether fine-tuning<a id="_idIndexMarker439"/> is the best approach<a id="_idIndexMarker440"/> to enable a model to answer very<a id="_idTextAnchor192"/> specific questions.</p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor193"/>In-context learning</h1>
<p>In-context learning is a technique<a id="_idIndexMarker441"/> where the model generates responses based on a few examples provided in the input prompt. This method leverages the model’s pre-trained knowledge and the specific context or examples included in the prompt to perform tasks without the need for parameter updates or retraining. The general approach, detailed in <em class="italic">Language Models are Few-Shot Learners</em> by Brown et al. (2020), describes how the extensive pre-training of these models enables them to perform tasks and generate responses based on a limited set of examples paired with instructions embedded within prompts. Unlike traditional methods that require fine-tuning for each specific task, in-context learning allows the model to adapt and respond based on the additional context provided at inference.</p>
<p>Central to in-context learning<a id="_idIndexMarker442"/> is the concept of few-shot prompting, which is critical for enabling models to adapt to and perform tasks without additional training data, relying instead on their pre-trained knowledge and the context provided within input prompts. For context, we’ll describe how an LLM typically works, which<a id="_idIndexMarker443"/> is known as the zero-shot approach, and contrast it to in-context learning, which uses the few-shot approach:</p>
<ul>
<li><code>x</code>. The model calculates the likelihood of a potential output sequence, <code>y</code>, expressed as <code>P(y|x)</code>. This computation is performed without prior examples specific to the task, relying entirely on the model’s general pre-training. Meaning, the zero-shot approach has no specific context apart from its general knowledge. For example, if we were to ask <em class="italic">Are winter coats available in children’s sizes?</em>, the model could not provide a specific answer about StyleSprint’s inventory. It could only provide some generic answer.</li>
<li><code>x</code>) to form an extended input sequence. So, our question <em class="italic">Are winter coats available in children’s sizes?</em> might be paired with a few examples such as the following:<ul><li><code>Do you sell anything in </code><code>children’s sizes?</code></li></ul><p class="list-inset"><code>Any items for children are specifically listed on the “StyleSprint for </code><code>Kids” page</code>.</p><ul><li><code>What do you offer </code><code>for kids?</code></li></ul><p class="list-inset"><code> StyleSprint offers a variety of children’s fashions</code><code><a id="_idIndexMarker446"/></code><code> on its “StyleSprint for </code><code>Kids” page</code>.</p></li>
</ul>
<p>The LLM then computes the probability of generating a specific output sequence, <code>y</code>, given this extended input sequence, <code>x</code>. Mathematically, this can be conceptualized as the model estimating the joint probability distribution of <code>y</code> and <code>x</code> (where <code>x</code> includes both the prompt and the few-shot examples, as demonstrated previously). The model uses this joint probability distribution to generate a response consistent with the instructions paired with the examples given in the input sequence.</p>
<p>In both cases, the model’s ability to adapt its output based on the given context, whether with zero examples or a few, demonstrates the flexibility and sophistication of its underlying architecture and training. However, the few-shot approach allows the LLM to learn from the very specific examples provided.</p>
<p>Let’s consider how StyleSprint<a id="_idIndexMarker447"/> could apply in-context learning to answer customer queries. Performance using in-context learning (or the few-shot approach) consistently reflects significant gains over zero-shot behavior (Brown et al., 2020). We can expand our prior example to where a customer asks about the availability of a specific product. Again, the StyleSprint team could systematically append a few examples to each prompt as follows.</p>
<p>Here is the prompt: <code>Respond to the following {question} about </code><code>product availability.</code></p>
<p>These are some examples:</p>
<ul>
<li>Example 1:<ul><li><code>Do you carry black </code><code>leather handbags?</code></li><li><code>Give me a moment while I retrieve information about that </code><code>particular item.</code></li></ul></li>
<li>Example 2:<ul><li><code>Do you have the silk scarves </code><code>in blue?</code></li><li><code>Let me search our inventory for blue </code><code>silk scarves.</code></li></ul></li>
</ul>
<p>StyleSprint can provide examples that effectively help the model understand the nature of the inquiry and generate a response that is informative and aligned with the company’s policies and product offerings. In this example, we see that the responses are intended to be paired with a search component.  This is a common approach<a id="_idIndexMarker448"/> and can be accomplished using a technique called <strong class="bold">Retrieval Augmented Generation</strong> (<strong class="bold">RAG</strong>), which is a component that facilitates retrieval of real-time data to inform the generated response. Combining a few-shot in-context learning approach with RAG could ensure that the system provides a logical and specific answer.</p>
<p>In-context learning using a few-shot approach<a id="_idIndexMarker449"/> allows the model to rapidly adapt to various customer queries using a limited set of examples. When augmented with RAG, StyleSprint could potentially satisfy their use case and reduce the time and resources needed to fine-tune. However, this approach must be weighed against the depth of specialization and consistency of task-specific fine-tuning, which, as described, could also produce highly accurate answers that fit the brand tone.</p>
<p>In the next section, we will formulate metrics that help us draw a direct comparison to guide StyleSprint in making an informed decision that best suits its customer service objectiv<a id="_idTextAnchor194"/>es and operational framework.</p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor195"/>Fine-tuning versus in-context learning</h1>
<p>We learned how in-context learning<a id="_idIndexMarker450"/> could allow StyleSprint’s model to handle<a id="_idIndexMarker451"/> a diverse range of customer queries without requiring extensive retraining. Specifically, a few-shot approach combined with RAG could facilitate quick adaptation to new inquiries, as the model can generate responses based on a few examples. However, the effectiveness of in-context learning heavily relies on the quality and relevance of the examples provided in the prompts. Its success would also rely on the implementation of RAG. Moreover, without fine-tuning, responses may lack consistency or may not adhere as strictly to StyleSprint’s brand tone and customer service policies. Finally, depending entirely on a generative model without fine-tuning may inadvertently introduce bias, as discussed in <a href="B21773_04.xhtml#_idTextAnchor123"><em class="italic">Chapter 4</em></a>.</p>
<p>In practice, we have two very comparable and viable approaches. However, to make an informed decision, we should first perform a more in-depth comparis<a id="_idTextAnchor196"/><a id="_idTextAnchor197"/>on using quantitative methods.</p>
<p>To impartially assess the efficacy of in-context learning compared to fine-tuning, we can measure the quality and consistency of the generated responses. We can accomplish this using established and reliable metrics to compare outcomes from each of the approaches. Like prior evaluations, we will want to apply quantitative and qualitative methods applied across the following key dimensions:</p>
<ul>
<li><strong class="bold">Alignment with human judgment</strong>: We can again apply semantic similarity to provide a quantitative measure of how often the model’s responses are correct or relevant based on a reference answer written by a human.<p class="list-inset">StyleSprint’s brand communication experts can review a subset of the responses to provide a qualitative evaluation of the response accuracy and alignment with brand tone and voice.</p></li>
<li><strong class="bold">Consistency and stability</strong>: It is important to measure the degree to which questions are answered consistently each time despite minor variations in how the question is posed. Again, we can leverage semantic similarity to compare each new output to the prior when the input is held constant.</li>
</ul>
<p>In addition to evaluating the quality of model responses for each approach, we can also directly compare the operational and computational overhead required for each.</p>
<p>For fine-tuning, we will need to understand<a id="_idIndexMarker452"/> the overhead involved<a id="_idIndexMarker453"/> in training the model. While the PEFT method will significantly reduce the training effort, there could be considerably more infrastructure-related costs compared to in-context learning, which requires no additional training. Alternatively, for in-context learning, commoditized models such as OpenAI’s GPT-4 have a per-token cost model. StyleSprint must also consider the cost of tokens required to embed a sufficient number of few-shot examples in the prompt.</p>
<p>In both cases, StyleSprint will incur some operational costs to create best-in-class examples written by humans that can be used as a “gold standard” in either the few-shot approach or for additional model training.</p>
<p>By conducting these comparative tests and analyzing the results, StyleSprint will gain valuable insights into which approach – in-context learning or fine-tuning – best aligns with its operational goals and customer service standards. This data-driven evaluation will inform the decision<a id="_idIndexMarker454"/> on the optimal AI strategy<a id="_idIndexMarker455"/> for enhancing their customer service experience. We will implement these comparisons i<a id="_idTextAnchor198"/>n<a id="_idTextAnchor199"/> the practice project that follows.</p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor200"/>Practice project: Fine-tuning for Q&amp;A using PEFT</h1>
<p>For our practice project, we will experiment<a id="_idIndexMarker456"/> with AdaLoRA to efficiently fine-tune a model for a customer<a id="_idIndexMarker457"/> query and compare it directly to the output of a <strong class="bold">state-of-the-art</strong> (<strong class="bold">SOTA</strong>) model using in-context learning. Like the previous chapter, we can rely on a prototyping environment such as Google Colab to complete the evaluation and comparison of the two approaches. We will demonstrate how to configure model traini<a id="_idTextAnchor201"/>ng to use AdaLoRA as our PEFT method.</p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor202"/>Background regarding question-answering fine-tuning</h2>
<p>Our project utilizes the Hugging Face<a id="_idIndexMarker458"/> training pipeline library, a widely recognized resource in the machine learning community. This library offers a variety of pre-built pipelines, including one for question-answering, which allows us to fine-tune pre-trained models with minimal setup. Hugging Face pipelines abstract much of the complexity involved in model training, making it accessible for developers to implement advanced natural language processing tasks directly and efficiently In particular, this pipeline behaves as an interface to a transformer model with a specific head for question-answering tasks. Recall that when we fine-tune a transformer model, we keep the architecture of the model – including the self-attention mechanism and the transformer layers – but we train the model’s parameters on a specific task, which, in this case, results in a model refined specifically to answer questions. Recall our practice project in <a href="B21773_03.xhtml#_idTextAnchor081"><em class="italic">Chapter 3</em></a> where the resulting model was a translator; we used a translator head to accomplish translation from English to French. For this project, the “head” is aligned to learn patterns in question-answering data.</p>
<p>However, when using<a id="_idIndexMarker459"/> a question-answer training pipeline, it is important to understand that the model does not simply memorize question-answer pairs, it learns the connection between questions and answers. Moreover, to answer appropriately, the model cannot rely entirely on training. It also requires additional context as input to compose a relevant answer. To understand this further, we decompose the model inferencing step as follows:</p>
<ol>
<li>When feeding a question to a model, we must also include context relevant to the topic.</li>
<li>The model then determines the most relevant part of the context that answers the question. It does this by assigning probability scores to each token (word or sub-word) in the context.</li>
<li>The model “thinks” of the context as a potential source for the answer and assigns each token two scores: one score for being the <strong class="bold">start</strong> of the answer, and another for being the <strong class="bold">end</strong> of the answer.</li>
<li>The token with the highest “start” score and “end” score is then chosen to form the answer <strong class="bold">span</strong>. The span is what is presented to the user.</li>
</ol>
<p>To provide a concrete example, if we ask the model, <code>Does StyleSprint have any leather jackets?</code> and provide a context of <code>StyleSprint sells a variety of coats, jackets and outerwear</code>, the model will process this context and identify that the most likely answer is something like <code>Yes, StyleSprint sells a variety of outerwear</code>. However, if the answer to a question is not included in the provided context, the model cannot generate a reliable answer. Additionally, if the context is too unspecific, the model may provide a more generic answer. Like in-context learning, the fine-tuned approach for question-answering requires relevant context. This means that, in practice, the model must be integrated with a search component that can retrieve additional context to pair with each question.</p>
<p>Consider our leather jacket example. When a question is received, the system could perform a search of its knowledge base and retrieve any contextual information relevant to a leather jacket (e.g., a paragraph about outerwear). Again, since the model was trained to answer questions in a way that aligns with the brand tone, it will extract the relevant information from the context provided to formulate an appropriate answer. Not only will integration with search provide the model with the context it needs but it will also allow the model to have up-to-date and real-time information.</p>
<p>Additionally, we might incorporate a confidence threshold, where the model only gives an answer if it assigns a high enough probability to the start and end tokens. If the highest probability is below this threshold, we might say the model does not know, or request more information. Overall, the model efficacy relies heavily on the quality and size of the training data as well as the relevance of the context with regard to the questions posed.</p>
<p>Now that we have a better understanding<a id="_idIndexMarker460"/> of how fine-tuning for question-answering works and what to expect when using the question-answering pipeline from Hugging Face, <a id="_idTextAnchor203"/>we can begin to write our implementation.</p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor204"/>Implementation in Python</h2>
<p>First and foremost, we<a id="_idIndexMarker461"/> install<a id="_idIndexMarker462"/> the required libraries:</p>
<pre class="source-code">
!pip install transformers peft sentence-transformers</pre>
<p>Then, we import the question-answering modules from the transformers library. For<a id="_idIndexMarker463"/> our project, we will use Google’s <strong class="bold">Flan T5 (small)</strong>, which is considered a SOTA alternative to GPT 3.5. As one of our goals continues to be to measure the performance versus efficiency trade-off, we begin with the smallest version of Flan T5, which has 80M parameters. This will enable faster training and more rapid iteration. However, please note that even a small model trained over a small number of epochs will require a high-RAM runtime environment:</p>
<pre class="source-code">
from transformers import (
    AutoModelForQuestionAnswering, AutoTokenizer)
model_name = " google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)</pre>
<p>With the pre-trained model instantiated, we can now configure the model to adapt its training process to use AdaLoRA, which, as we’ve learned, is specifically designed to allocate the parameter budget efficiently during the fine-tuning process:</p>
<pre class="source-code">
from peft import AdaLoraConfig
# Example configuration; adjust parameters as needed
adapter_config = AdaLoraConfig(target_r=16)
model.add_adapter(adapter_config)</pre>
<p>As discussed, fine-tuning<a id="_idIndexMarker464"/> relies heavily on the<a id="_idIndexMarker465"/> quality and size of the training data. In the StyleSprint scenario, the company could aggregate question-answer pairs from its FAQ page, social media, and customer service transcripts. For this exercise, we will construct a simple dataset that looks similar to the following:</p>
<pre class="source-code">
demo_data = [{
"question": "What are the latest streetwear trends available at Stylesprint?",
  "answer": "Stylesprint's latest streetwear collection includes hoodies, and graphic tees, all inspired by the latest hip-hop fashion trends."
...
}]</pre>
<p>However, in order to integrate our dataset with the question-answer pipeline, we should first understand the <code>Trainer</code> class. The <code>Trainer</code> class in the Hugging Face transformers library expects the training and evaluation datasets to be in a specific format, usually as a PyTorch <code>Dataset</code> object, not just as simple lists of dictionaries. Further, each entry in the dataset needs to be tokenized and structured with the necessary fields such as <code>input_ids</code>, <code>attention_mask</code>, and, for question-answering tasks, <code>start_positions</code> and <code>end_positions</code>. Let us explore these in more detail:</p>
<ul>
<li><code>input_ids</code>: This is a sequence of integers that represent the input sentence in the model. Each word or sub-word in the sentence is converted into a unique integer or ID. Recall from<a id="_idIndexMarker466"/> earlier chapters that this process is known as <code>[101, </code><code>354, 2459]</code>.</li>
<li><code>attention_mask</code>: An attention mask is a sequence of binary values where 1s indicate real tokens and 0s indicate padding tokens. In other words, in the places where 1s are present, the model will understand that those places need attention and the places with 0s will be ignored by the model. This is crucial when dealing with sentences of varying lengths and dealing with batches of sentences in training models.</li>
<li><code>start_positions</code> and <code>end_positions</code>: These are for question-answering tasks. They represent the indices of the start and end tokens of the answer in the tokenized form of the context. For example, in the context <em class="italic">Paris is the capital of France</em>, if the question is <em class="italic">What is the capital of France?</em> and the answer given is <em class="italic">Paris</em>, after tokenization, <code>start_position</code> and <code>end_position</code> will correspond to the index of <em class="italic">Paris</em> in the context.</li>
</ul>
<p>With that understanding, we can create<a id="_idIndexMarker467"/> a class that adapts<a id="_idIndexMarker468"/> our dataset to meet the expectations of the trainer, as follows:</p>
<pre class="source-code">
from torch.utils.data import Dataset
class StylesprintDataset(Dataset):
   def __init__(self, tokenizer, data):
       tokenizer.pad_token = tokenizer.eos_token
       self.tokenizer = tokenizer
       self.data = data</pre>
<p>For the complete custom dataset class code, visit this book’s GitHub repository at <a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</a>.</p>
<p>With the training set prepared<a id="_idIndexMarker469"/> and our pipeline configured<a id="_idIndexMarker470"/> to apply the AdaLoRA method, we can finally move to the training step. For this project, we will configure the training to run for just a few epochs, but in the StyleSprint scenario, a much more robust training process would be required:</p>
<pre class="source-code">
from transformers import Trainer, TrainingArguments
# Split the mock dataset into training and evaluation sets (50/50)
train_data = StylesprintDataset(
    tokenizer, demo_data[:len(demo_data)//2])
eval_data = StylesprintDataset(
    tokenizer, demo_data[len(demo_data)//2:])
# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)
# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=eval_data
)
# Start training
trainer.train()</pre>
<p>For our simple experiment, we do not expect a highly performant model; however, we can learn how to interpret the training output, which describes how well the model performed on the evaluation samples. The <code>Trainer</code> class<a id="_idIndexMarker471"/> will output a training summary that includes<a id="_idIndexMarker472"/> the loss metric.</p>
<h3>Training loss</h3>
<p>Training loss is a measure<a id="_idIndexMarker473"/> of how well the model is performing; a lower loss indicates<a id="_idIndexMarker474"/> better performance. In many deep learning models, especially those dealing with complex tasks such as language understanding, it’s common to start with a relatively high loss. The expectation is that this value should decrease as training progresses.</p>
<p>In the early stages of training, a high loss isn’t a cause for alarm as it commonly decreases as the model continues to learn. However, if the loss remains high, this signals that additional training may be needed. If the loss continues to be high after prolonged training, the learning rate and other hyperparameters may require adjustment, as an inappropriate learning rate can impact the model’s learning effectiveness. Moreover, the quality and quantity of your training data should be evaluated as insufficient data can hinder the training. For example, as we only use a few examples for the experiment, we expect a relatively high loss.</p>
<p>The next step is to use our newly fine-tuned model<a id="_idIndexMarker475"/> to infer or predict. We should also secure<a id="_idIndexMarker476"/> our trained model parameters so we can reuse it without retraining:</p>
<pre class="source-code">
import torch
# save parameters
model.save_pretrained("./stylesprint_qa_model")
def ask_question(model, question, context):
   # Tokenize the question and context
   inputs = tokenizer.encode_plus(question, context,
        add_special_tokens=True, return_tensors="pt")
   # Get model predictions
   with torch.no_grad():
       outputs = model(**inputs)
   # Get the start and end positions
   answer_start_scores = outputs.start_logits
   answer_end_scores = outputs.end_logits
   # Find the tokens with the highest `start` and `end` scores
   answer_start = torch.argmax(answer_start_scores)
   answer_end = torch.argmax(answer_end_scores) + 1
   # Convert the tokens to the answer string
   answer = tokenizer.convert_tokens_to_string(
        tokenizer.convert_ids_to_tokens(
            inputs["input_ids"][0][answer_start:answer_end]
            )
        )
   return answer
question = "What is the return policy for online purchases?"
context = """Excerpt from return policy returned from search."""
answer = ask_question(model, question, context)
print(answer)</pre>
<p>As discussed, we introduce context along with a question to the model, so that it can identify which fragment of the context responds most appropriately to the query. Consequently, we may want to consider integrating a vector search system (such as RAG) to automatically identify relevant documents from large datasets based on semantic similarities to a query. These search results may not provide specific answers, but the trained QA model can extract more precise answers from the results.</p>
<p>With this hybrid approach, the vector search system first retrieves documents or text segments that are semantically related to the query. The QA model<a id="_idIndexMarker477"/> then analyzes<a id="_idIndexMarker478"/> this context to identify the precise answer t<a id="_idTextAnchor205"/>hat aligns with StyleSprint’s guidelines and expectations.</p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor206"/>Evaluation of results</h2>
<p>To evaluate our<a id="_idIndexMarker479"/> model outcomes, StyleSprint might apply the qualitative and quantitative approaches we have discussed in the chapter already. For the purpose of our experiment, we can measure the output of the model to a golden standard response using a simple measure for semantic similarity:</p>
<pre class="source-code">
from sentence_transformers import SentenceTransformer, util
import pandas as pd
# Example of a gold standard answer written by a human
gs = "Our policy at Stylesprint is to accept returns on online purchases within 30 days, with the condition that the items are unused and remain in their original condition."
# Example of answer using GPT 3.5 with in-context learning reusing a relevant subset of the training data examples
gpt_35 = "Stylesprint accepts returns within 30 days of purchase, provided the items are unworn and in their original condition."
# Load your dataset
dataset = pd.DataFrame([
   (gs, gpt_35, answer)
])# pd.read_csv("dataset.csv")
dataset.columns = ['gold_standard_response',
    'in_context_response', 'fine_tuned_response']
# Load a pre-trained sentence transformer model
eval_model = SentenceTransformer('all-MiniLM-L6-v2')
# Function to calculate semantic similarity
def calculate_semantic_similarity(model, response, gold_standard):
    response_embedding = model.encode(
        response, convert_to_tensor=True)
    gold_standard_embedding = model.encode(gold_standard,
        convert_to_tensor=True)
    return util.pytorch_cos_sim(response_embedding,
        gold_standard_embedding).item()
# Measure semantic similarity
dataset['in_context_similarity'] = dataset.apply(
    lambda row:calculate_semantic_similarity(
        eval_model, row['in_context_response'],
        row['gold_standard_response']
    ), axis=1)
dataset['fine_tuned_similarity'] = dataset.apply(
    lambda row:calculate_semantic_similarity(
        eval_model, row['fine_tuned_response'],
        row['gold_standard_response']
    ), axis=1)
# Print semantic similarity
print("Semantic similarity for in-context learning:", 
    dataset['in_context_similarity'])
print("Semantic similarity for fine-tuned model:", 
    dataset['fine_tuned_similarity'])</pre>
<p>The results of our evaluation<a id="_idIndexMarker480"/> are as follows:</p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>PEFT Flan T5</p>
</td>
<td class="No-Table-Style">
<p>GPT 3.5T</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p>Fine-tuned</p>
</td>
<td class="No-Table-Style">
<p>In-context</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Semantic Similarity</p>
</td>
<td class="No-Table-Style">
<p>0.543</p>
</td>
<td class="No-Table-Style">
<p>0.91</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor207"/>Table 5.1: Semantic similarity scores for fine-tuned Flan and GPT 3.5 Turbo, respectively</p>
<p>Undoubtedly, the in-context learning arrived at an answer that was much closer to our gold standard reference. However, the fine-tuned model was not far behind. This tells us that with a more robust training dataset and considerably more epochs, the fine-tuned model could be comparable to GPT 3.5. With more iteration and experimentation, StyleSprint<a id="_idIndexMarker481"/> could have a very robust fine-tuned model to answer very specific questions for its customers.</p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor208"/>Summary</h1>
<p>In this chapter, we focused on the strategic decision-making process between fine-tuning and in-context learning for StyleSprint’s AI-driven customer service system. While in-context learning, particularly few-shot learning, offers adaptability and resource efficiency, it may not consistently align with StyleSprint’s brand tone and customer service guidelines. This method relies heavily on the quality and relevance of the examples provided in the prompts, requiring careful crafting to ensure optimal outcomes.</p>
<p>On the other hand, PEFT methods such as AdaLoRA, offer a more focused approach to adapt a pre-trained model to the specific demands of customer service queries. PEFT methods modify only a small subset of a model’s parameters, reducing the computational burden while still achieving high performance. This efficiency is crucial for real-world applications where computational resources and response accuracy are both key considerations.</p>
<p>Ultimately, the choice between in-context learning and fine-tuning is not just a technical decision but also a strategic one, deeply intertwined with the company’s operational goals, resource allocation, and the desired customer experience. The chapter suggests conducting comparative tests to assess the efficacy of both approaches, evaluating outcomes at scale through reliable metrics. This data-driven evaluation will inform StyleSprint’s decision on the optimal AI strategy for enhancing their customer service experience.</p>
<p>In summary, we now have a more complete understanding of the implications of fine-tuning versus in-context learning in LLMs, specifically in the context of customer service. It highlights the need for a company like StyleSprint to make a well-informed strategic decision, balancing the depth of specialization and consistency offered by fine-tuning against the adaptability and efficiency of in-context learning.</p>
<p>In the next chapter, we will explore PEFT for domain adaptation where the outcome of our training is a general-purpose model refined to understand a highly specific domain like finance or law.<a id="_idTextAnchor209"/></p>
<h1 id="_idParaDest-121"><a id="_idTextAnchor210"/>References</h1>
<p>This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the subject matter:</p>
<ul>
<li>Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. (2018). <em class="italic">Improving language understanding by generative </em><em class="italic">pre-training</em>. OpenAI.</li>
<li>Hu, E. J., Shen, Y., Wallis, P., Li, Y., Wang, S., Wang, L., and Chen, W. (2021). <em class="italic">LoRA: Low-Rank Adaptation of Large Language Models</em>. ArXiv. /abs/2106.09685</li>
<li>Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. (2023). <em class="italic">Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</em>. ArXiv. /abs/2303.10512</li>
<li>Brown TB, Mann B, Ryder N, et al. 2020. <em class="italic">Language Models are Few-Shot </em><em class="italic">Learners</em>. ArXiv:2005.14165.</li>
</ul>
</div>
</body></html>