<html><head></head><body>
<div id="_idContainer039">
<h1 class="chapter-number" id="_idParaDest-108"><a id="_idTextAnchor180"/><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-109"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.2.1">Fine-Tuning Generative Models for Specific Tasks</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In our narrative with StyleSprint, we described using a pre-trained generative AI model for creating engaging product descriptions. </span><span class="koboSpan" id="kobo.3.2">While this model showed adeptness in generating diverse content, StyleSprint’s evolving needs require a shift in focus. </span><span class="koboSpan" id="kobo.3.3">The new challenge is not just about producing content but also about engaging in specific, task-oriented interactions such as automatically answering customer’s specific questions about the </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">products described.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In this chapter, we introduce the concept of fine-tuning, a vital step in adapting a pre-trained model to perform specific downstream tasks. </span><span class="koboSpan" id="kobo.5.2">For StyleSprint, this means transforming the model from a versatile content generator to a specialized tool capable of providing accurate and detailed responses to </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">customer questions.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">We will explore and define a range of scalable fine-tuning techniques, comparing them with other approaches such as in-context learning. </span><span class="koboSpan" id="kobo.7.2">We will demonstrate advanced fine-tuning methods, including parameter-efficient fine-tuning and prompt tuning, to demonstrate how they can fine-tune a model’s abilities for specific tasks such </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">as Q&amp;A.</span></span></p>
<p><span class="koboSpan" id="kobo.9.1">By the end of this chapter, we will have trained a language model to answer questions and do so in a way that aligns with StyleSprint’s brand guidelines. </span><span class="koboSpan" id="kobo.9.2">However, before we explore the mechanics of fine-tuning and its importance in our application, we will revisit the history of fine-tuning in the context </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">of LLMs.</span></span><a id="_idTextAnchor183"/></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.11.1">Foundation and relevance – an introduction to fine-tuning</span></h1>
<p><span class="koboSpan" id="kobo.12.1">Fine-tuning is the process</span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.13.1"> of leveraging a model pre-trained on a large dataset and continuing the training process on a smaller, task-specific dataset to improve its performance on that task. </span><span class="koboSpan" id="kobo.13.2">It may also involve additional training that adapts a model to the nuances of a new domain. </span><span class="koboSpan" id="kobo.13.3">The latter</span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.14.1"> is known as domain adaptation, which we will cover in </span><a href="B21773_06.xhtml#_idTextAnchor211"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.15.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.16.1">. </span><span class="koboSpan" id="kobo.16.2">The former is typically referred to as task-specific fine-tuning, and it can</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.17.1"> be performed to accomplish several tasks, including Q&amp;A, summarization, classification, and many others. </span><span class="koboSpan" id="kobo.17.2">For this chapter, we will focus on task-specific fine-tuning to improve a general-purpose model’s performance when </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">answering questions.</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">For StyleSprint, fine-tuning a model to handle a specific task such as answering customer inquiries about products introduces unique challenges. </span><span class="koboSpan" id="kobo.19.2">Unlike generating product descriptions, which primarily involves language generation using an out-of-the-box pre-trained model, answering customer questions requires the model to have an extensive understanding of product-specific data and should have a brand-aware voice. </span><span class="koboSpan" id="kobo.19.3">Specifically, the model must accurately interpret and respond to questions about product features, sizes, availability, user reviews, and many other details. </span><span class="koboSpan" id="kobo.19.4">It should also produce answers consistent with StyleSprint’s distinct brand tone. </span><span class="koboSpan" id="kobo.19.5">This task requires both generalized natural language proficiency (from pre-training) and robust knowledge of product metadata and customer feedback, accomplished </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">through fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">Models such as GPT initially learn to predict text through an unsupervised learning process that involves being trained on wide-ranging and vast datasets. </span><span class="koboSpan" id="kobo.21.2">This pre-training phase exposes the model to a diverse array of texts, enabling it to gain a broad understanding of language, including syntax, grammar, and context, without any specific task-oriented guidance. </span><span class="koboSpan" id="kobo.21.3">However, fine-tuning applies task-oriented, supervised learning to refine the model’s capabilities to accomplish the specified task – specifically, semi-supervised learning, which, as described by Radford et al. </span><span class="koboSpan" id="kobo.21.4">(2018), involves adapting the model to a specific supervised task by exposing it to a dataset comprising input sequences (</span><strong class="source-inline"><span class="koboSpan" id="kobo.22.1">x1</span></strong><span class="koboSpan" id="kobo.23.1">, ..., </span><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">xm</span></strong><span class="koboSpan" id="kobo.25.1">) and corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">labels (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">y</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">Throughout the chapter, we will detail the fine-tuning</span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.30.1"> process, including how to selectively train the model on a curated dataset of product-related information and customer interactions, enabling it to respond with the informed, brand-aligned precision that customers expect. </span><span class="koboSpan" id="kobo.30.2">However, fine-tuning an LLM, which could have billions of parameters, would typically require an enormous number of resources and time. </span><span class="koboSpan" id="kobo.30.3">This </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.31.1">is where advanced techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">Parameter-Efficient Fine-Tuning</span></strong><span class="koboSpan" id="kobo.33.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.34.1">PEFT</span></strong><span class="koboSpan" id="kobo.35.1">) become particularly valuable in making </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">fine-tuning access</span><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.37.1">i</span><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.38.1">ble.</span></span></p>
<h1 id="_idParaDest-111"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.39.1">PEFT</span></h1>
<p><span class="koboSpan" id="kobo.40.1">Traditional fine-tuning</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.41.1"> methods become increasingly impractical as the model size grows due to the immense computational resources and time required to train and update all model parameters. </span><span class="koboSpan" id="kobo.41.2">For most businesses, including larger organizations, a classical approach to fine-tuning is cost-prohibitive and, effectively, </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">a non-starter.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Alternatively, PEFT methods modify only a small subset of a model’s parameters, reducing the computational burden while still achieving state-of-the-art performance. </span><span class="koboSpan" id="kobo.43.2">This method is advantageous for adapting large models to specific tasks without </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">extensive retraining.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">One such PEFT method is the </span><strong class="bold"><span class="koboSpan" id="kobo.46.1">Low-Rank Adaptation</span></strong><span class="koboSpan" id="kobo.47.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.48.1">LoRA</span></strong><span class="koboSpan" id="kobo.49.1">) methodology, developed by Hu et </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">al. </span><span class="koboSpan" id="kobo.50.2">(20</span><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.51.1">21).</span></span></p>
<h2 id="_idParaDest-112"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.52.1">LoRA</span></h2>
<p><span class="koboSpan" id="kobo.53.1">The LoRA method focuses</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.54.1"> on selectively fine-tuning specific components</span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.55.1"> within the Transformer architecture to enhance efficiency and effectiveness in LLMS. </span><span class="koboSpan" id="kobo.55.2">LoRA targets the weight matrices found in the self-attention module of the Transformer, which, as discussed in </span><a href="B21773_03.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.56.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.57.1">, are key to its functionality and include four matrices: w</span><span class="subscript"><span class="koboSpan" id="kobo.58.1">q</span></span><span class="koboSpan" id="kobo.59.1"> (query), w</span><span class="subscript"><span class="koboSpan" id="kobo.60.1">k</span></span><span class="koboSpan" id="kobo.61.1"> (key), w</span><span class="subscript"><span class="koboSpan" id="kobo.62.1">v</span></span><span class="koboSpan" id="kobo.63.1"> (value), and w</span><span class="subscript"><span class="koboSpan" id="kobo.64.1">o</span></span><span class="koboSpan" id="kobo.65.1"> (output). </span><span class="koboSpan" id="kobo.65.2">Although these matrices can be divided into multiple heads in a multi-head attention setting – where each </span><em class="italic"><span class="koboSpan" id="kobo.66.1">head</span></em><span class="koboSpan" id="kobo.67.1"> represents one of several parallel attention mechanisms that process inputs independently – LoRA treats them as singular matrices, simplifying the </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">adaptation process.</span></span></p>
<p><span class="koboSpan" id="kobo.69.1">LoRA’s approach involves adapting only the attention weights for downstream tasks, while the weights</span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.70.1"> in the other component of the Transformer, the </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">feed-forward network</span></strong><span class="koboSpan" id="kobo.72.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.73.1">FFN</span></strong><span class="koboSpan" id="kobo.74.1">), are unchanged. </span><span class="koboSpan" id="kobo.74.2">This decision to focus exclusively on the attention weights and freeze the FFN is made for simplicity and parameter efficiency. </span><span class="koboSpan" id="kobo.74.3">By doing so, LoRA ensures a more manageable and resource-efficient fine-tuning process, avoiding the complexities and demands of retraining the </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">entire network.</span></span></p>
<p><span class="koboSpan" id="kobo.76.1">This selective fine-tuning strategy enables LoRA to effectively tailor the model for specific tasks while maintaining the overall structure and strengths of the pre-trained model. </span><span class="koboSpan" id="kobo.76.2">This makes LoRA a practical solution for adapting LLMs to new tasks with a reduced computational burden without requiring comprehensive parameter updates across the entire model (Liu et </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">al., 2021).</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">Building upon</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.79.1"> the foundation of LoRA, </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">Adaptive Low-Rank Adaptation</span></strong><span class="koboSpan" id="kobo.81.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.82.1">AdaLoRA</span></strong><span class="koboSpan" id="kobo.83.1">), as introduced in a study by Liu et al. </span><span class="koboSpan" id="kobo.83.2">(2022), represents a further advancement in PEFT methods. </span><span class="koboSpan" id="kobo.83.3">The key difference between LoRA and AdaLoRA lies in (as the name suggests) its adaptiveness. </span><span class="koboSpan" id="kobo.83.4">While LoRA applies a consistent, low-rank approach to fine-tuning across the model, AdaLoRA tailors the updates to the needs of each layer, offering a more flexible</span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.84.1"> and potentially more effective way to fine-tune</span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.85.1"> large models for </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">speci</span><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.87.1">fic tasks.</span></span></p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.88.1">AdaLoRA</span></h2>
<p><span class="koboSpan" id="kobo.89.1">AdaLoRA’s key innovation</span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.90.1"> lies in its adaptive</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.91.1"> allocation of the </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">parameter budget</span></strong><span class="koboSpan" id="kobo.93.1"> among the weight matrices</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.94.1"> of the pre-trained model. </span><span class="koboSpan" id="kobo.94.2">Many PEFT methods tend to distribute the parameter budget evenly across all pre-trained weight matrices, potentially neglecting the varying importance of different weight parameters. </span><span class="koboSpan" id="kobo.94.3">AdaLoRA overcomes this by assigning importance scores to these weight matrices and allocating the parameter budget accordingly. </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">Importance scores</span></strong><span class="koboSpan" id="kobo.96.1"> in the context of AdaLoRA are metrics</span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.97.1"> used to determine the significance (or importance) of different weight parameters in a model, guiding the allocation of the parameter budget more effectively </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">during fine-tuning.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.99.1">Note</span></p>
<p class="callout"><em class="italic"><span class="koboSpan" id="kobo.100.1">Parameter budget</span></em><span class="koboSpan" id="kobo.101.1"> refers to the predefined limit</span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.102.1"> on the number of additional parameters that can be introduced during the fine-tuning of a pre-trained model. </span><span class="koboSpan" id="kobo.102.2">This budget is set to ensure that the model’s complexity does not increase significantly, which can lead to challenges such as overfitting, increased computational costs, and longer </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">training times.</span></span></p>
<p><span class="koboSpan" id="kobo.104.1">Additionally, AdaLoRA applies </span><strong class="bold"><span class="koboSpan" id="kobo.105.1">singular value decomposition</span></strong><span class="koboSpan" id="kobo.106.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.107.1">SVD</span></strong><span class="koboSpan" id="kobo.108.1">) to efficiently organize the incremental</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.109.1"> updates made during the model’s fine-tuning process. </span><span class="koboSpan" id="kobo.109.2">SVD allows for the effective pruning of singular values associated with less critical updates, reducing the overall parameter budget required for fine-tuning. </span><span class="koboSpan" id="kobo.109.3">It is important to note that this method also avoids the need for computationally intensive exact computations, making the fine-tuning process </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">more efficient.</span></span></p>
<p><span class="koboSpan" id="kobo.111.1">AdaLoRA has been empirically tested across various domains, including natural language processing, question-answering, and natural language generation. </span><span class="koboSpan" id="kobo.111.2">Extensive experiments have demonstrated its effectiveness in improving model performance, particularly in question-answering tasks. </span><span class="koboSpan" id="kobo.111.3">The adaptability and efficiency of AdaLoRA make it an ideal choice for applications requiring precise and efficient model adjustments for </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">complex tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.113.1">In the case of StyleSprint, AdaLoRA presents an opportunity to fine-tune its language model for answering customer questions without the considerable overhead that would be incurred by traditional fine-tuning, which would require adjusting all of the model parameters. </span><span class="koboSpan" id="kobo.113.2">By adopting AdaLoRA, StyleSprint can efficiently adapt its model to handle nuanced customer inquiries by adjusting significantly fewer parameters. </span><span class="koboSpan" id="kobo.113.3">Specifically, AdaLoRA’s adaptive allocation of parameter budgets means that StyleSprint can optimize its model for the specific nuances of customer queries without using extensive </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">computational resources.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">By the end of this chapter, we will have fine-tuned an LLM using AdaLoRA for our Q&amp;A task. </span><span class="koboSpan" id="kobo.115.2">However, we should first decide whether fine-tuning is truly the right approach. </span><span class="koboSpan" id="kobo.115.3">Prompt-based LLMs offer a viable</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.116.1"> alternative known as in-context learning, where the model can learn from examples given in the prompt, meaning that the prompt would contain the customer’s question paired with a few key historical examples of how other questions were answered. </span><span class="koboSpan" id="kobo.116.2">The model can infer from the examples how to answer the question at hand in a way that is consistent with the examples. </span><span class="koboSpan" id="kobo.116.3">In the next section, we will explore the benefits and drawbacks of in-context learning to help us determine whether fine-tuning</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.117.1"> is the best approach</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.118.1"> to enable a model to answer very</span><a id="_idTextAnchor192"/> <span class="No-Break"><span class="koboSpan" id="kobo.119.1">specific questions.</span></span></p>
<h1 id="_idParaDest-114"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.120.1">In-context learning</span></h1>
<p><span class="koboSpan" id="kobo.121.1">In-context learning is a technique</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.122.1"> where the model generates responses based on a few examples provided in the input prompt. </span><span class="koboSpan" id="kobo.122.2">This method leverages the model’s pre-trained knowledge and the specific context or examples included in the prompt to perform tasks without the need for parameter updates or retraining. </span><span class="koboSpan" id="kobo.122.3">The general approach, detailed in </span><em class="italic"><span class="koboSpan" id="kobo.123.1">Language Models are Few-Shot Learners</span></em><span class="koboSpan" id="kobo.124.1"> by Brown et al. </span><span class="koboSpan" id="kobo.124.2">(2020), describes how the extensive pre-training of these models enables them to perform tasks and generate responses based on a limited set of examples paired with instructions embedded within prompts. </span><span class="koboSpan" id="kobo.124.3">Unlike traditional methods that require fine-tuning for each specific task, in-context learning allows the model to adapt and respond based on the additional context provided </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">at inference.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">Central to in-context learning</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.127.1"> is the concept of few-shot prompting, which is critical for enabling models to adapt to and perform tasks without additional training data, relying instead on their pre-trained knowledge and the context provided within input prompts. </span><span class="koboSpan" id="kobo.127.2">For context, we’ll describe how an LLM typically works, which</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.128.1"> is known as the zero-shot approach, and contrast it to in-context learning, which uses the </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">few-shot approach:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.130.1">Zero-shot prompting</span></strong><span class="koboSpan" id="kobo.131.1">: Models such as GPT respond</span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.132.1"> to instruction based on their vast pre-training and the specific task or instruction described in the input prompt. </span><span class="koboSpan" id="kobo.132.2">These models estimate a conditional probability distribution over possible outputs for a given input sequence, </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">x</span></strong><span class="koboSpan" id="kobo.134.1">. </span><span class="koboSpan" id="kobo.134.2">The model calculates the likelihood of a potential output sequence, </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">y</span></strong><span class="koboSpan" id="kobo.136.1">, expressed as </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">P(y|x)</span></strong><span class="koboSpan" id="kobo.138.1">. </span><span class="koboSpan" id="kobo.138.2">This computation is performed without prior examples specific to the task, relying entirely on the model’s general pre-training. </span><span class="koboSpan" id="kobo.138.3">Meaning, the zero-shot approach has no specific context apart from its general knowledge. </span><span class="koboSpan" id="kobo.138.4">For example, if we were to ask </span><em class="italic"><span class="koboSpan" id="kobo.139.1">Are winter coats available in children’s sizes?</span></em><span class="koboSpan" id="kobo.140.1">, the model could not provide a specific answer about StyleSprint’s inventory. </span><span class="koboSpan" id="kobo.140.2">It could only provide some </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">generic answer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Few-shot prompting</span></strong><span class="koboSpan" id="kobo.143.1">: Using the few-shot approach, we provide</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.144.1"> the model with a prompt paired with a few examples. </span><span class="koboSpan" id="kobo.144.2">These examples are concatenated to the prompt (represented as </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">x</span></strong><span class="koboSpan" id="kobo.146.1">) to form an extended input sequence. </span><span class="koboSpan" id="kobo.146.2">So, our question </span><em class="italic"><span class="koboSpan" id="kobo.147.1">Are winter coats available in children’s sizes?</span></em><span class="koboSpan" id="kobo.148.1"> might be paired with a few examples such as </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Q</span></strong><span class="koboSpan" id="kobo.151.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.152.1">Do you sell anything in </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.153.1">children’s sizes?</span></strong></span></li></ul><p class="list-inset"><strong class="bold"><span class="koboSpan" id="kobo.154.1">     A</span></strong><span class="koboSpan" id="kobo.155.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">Any items for children are specifically listed on the “StyleSprint for </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.157.1">Kids” page</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">.</span></span></p><ul><li><strong class="bold"><span class="koboSpan" id="kobo.159.1">Q</span></strong><span class="koboSpan" id="kobo.160.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.161.1">What do you offer </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.162.1">for kids?</span></strong></span></li></ul><p class="list-inset"><strong class="bold"><span class="koboSpan" id="kobo.163.1">     A</span></strong><span class="koboSpan" id="kobo.164.1">:</span><strong class="source-inline"><span class="koboSpan" id="kobo.165.1"> StyleSprint offers a variety of children’s fashions</span></strong><strong class="source-inline"><a id="_idIndexMarker446"/></strong><strong class="source-inline"><span class="koboSpan" id="kobo.166.1"> on its “StyleSprint for </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">Kids” page</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">.</span></span></p></li>
</ul>
<p><span class="koboSpan" id="kobo.169.1">The LLM then computes the probability of generating a specific output sequence, </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">y</span></strong><span class="koboSpan" id="kobo.171.1">, given this extended input sequence, </span><strong class="source-inline"><span class="koboSpan" id="kobo.172.1">x</span></strong><span class="koboSpan" id="kobo.173.1">. </span><span class="koboSpan" id="kobo.173.2">Mathematically, this can be conceptualized as the model estimating the joint probability distribution of </span><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">y</span></strong><span class="koboSpan" id="kobo.175.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">x</span></strong><span class="koboSpan" id="kobo.177.1"> (where </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">x</span></strong><span class="koboSpan" id="kobo.179.1"> includes both the prompt and the few-shot examples, as demonstrated previously). </span><span class="koboSpan" id="kobo.179.2">The model uses this joint probability distribution to generate a response consistent with the instructions paired with the examples given in the </span><span class="No-Break"><span class="koboSpan" id="kobo.180.1">input sequence.</span></span></p>
<p><span class="koboSpan" id="kobo.181.1">In both cases, the model’s ability to adapt its output based on the given context, whether with zero examples or a few, demonstrates the flexibility and sophistication of its underlying architecture and training. </span><span class="koboSpan" id="kobo.181.2">However, the few-shot approach allows the LLM to learn from the very specific </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">examples provided.</span></span></p>
<p><span class="koboSpan" id="kobo.183.1">Let’s consider how StyleSprint</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.184.1"> could apply in-context learning to answer customer queries. </span><span class="koboSpan" id="kobo.184.2">Performance using in-context learning (or the few-shot approach) consistently reflects significant gains over zero-shot behavior (Brown et al., 2020). </span><span class="koboSpan" id="kobo.184.3">We can expand our prior example to where a customer asks about the availability of a specific product. </span><span class="koboSpan" id="kobo.184.4">Again, the StyleSprint team could systematically append a few examples to each prompt </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">as follows.</span></span></p>
<p><span class="koboSpan" id="kobo.186.1">Here is the prompt: </span><strong class="source-inline"><span class="koboSpan" id="kobo.187.1">Respond to the following {question} about </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.188.1">product availability.</span></strong></span></p>
<p><span class="koboSpan" id="kobo.189.1">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">some examples:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.191.1">Example 1:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.192.1">Customer query</span></strong><span class="koboSpan" id="kobo.193.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">Do you carry black </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">leather handbags?</span></strong></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.196.1">AI response</span></strong><span class="koboSpan" id="kobo.197.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">Give me a moment while I retrieve information about that </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">particular item.</span></strong></span></li></ul></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.200.1">Example 2:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.201.1">Customer query</span></strong><span class="koboSpan" id="kobo.202.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">Do you have the silk scarves </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">in blue?</span></strong></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.205.1">AI response</span></strong><span class="koboSpan" id="kobo.206.1">: </span><strong class="source-inline"><span class="koboSpan" id="kobo.207.1">Let me search our inventory for blue </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">silk scarves.</span></strong></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.209.1">StyleSprint can provide examples that effectively help the model understand the nature of the inquiry and generate a response that is informative and aligned with the company’s policies and product offerings. </span><span class="koboSpan" id="kobo.209.2">In this example, we see that the responses are intended to be paired with a search component.  </span><span class="koboSpan" id="kobo.209.3">This is a common approach</span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.210.1"> and can be accomplished using a technique called </span><strong class="bold"><span class="koboSpan" id="kobo.211.1">Retrieval Augmented Generation</span></strong><span class="koboSpan" id="kobo.212.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.213.1">RAG</span></strong><span class="koboSpan" id="kobo.214.1">), which is a component that facilitates retrieval of real-time data to inform the generated response. </span><span class="koboSpan" id="kobo.214.2">Combining a few-shot in-context learning approach with RAG could ensure that the system provides a logical and </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">specific answer.</span></span></p>
<p><span class="koboSpan" id="kobo.216.1">In-context learning using a few-shot approach</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.217.1"> allows the model to rapidly adapt to various customer queries using a limited set of examples. </span><span class="koboSpan" id="kobo.217.2">When augmented with RAG, StyleSprint could potentially satisfy their use case and reduce the time and resources needed to fine-tune. </span><span class="koboSpan" id="kobo.217.3">However, this approach must be weighed against the depth of specialization and consistency of task-specific fine-tuning, which, as described, could also produce highly accurate answers that fit the </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">brand tone.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">In the next section, we will formulate metrics that help us draw a direct comparison to guide StyleSprint in making an informed decision that best suits its customer service objectiv</span><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.220.1">es and </span><span class="No-Break"><span class="koboSpan" id="kobo.221.1">operational framework.</span></span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.222.1">Fine-tuning versus in-context learning</span></h1>
<p><span class="koboSpan" id="kobo.223.1">We learned how in-context learning</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.224.1"> could allow StyleSprint’s model to handle</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.225.1"> a diverse range of customer queries without requiring extensive retraining. </span><span class="koboSpan" id="kobo.225.2">Specifically, a few-shot approach combined with RAG could facilitate quick adaptation to new inquiries, as the model can generate responses based on a few examples. </span><span class="koboSpan" id="kobo.225.3">However, the effectiveness of in-context learning heavily relies on the quality and relevance of the examples provided in the prompts. </span><span class="koboSpan" id="kobo.225.4">Its success would also rely on the implementation of RAG. </span><span class="koboSpan" id="kobo.225.5">Moreover, without fine-tuning, responses may lack consistency or may not adhere as strictly to StyleSprint’s brand tone and customer service policies. </span><span class="koboSpan" id="kobo.225.6">Finally, depending entirely on a generative model without fine-tuning may inadvertently introduce bias, as discussed in </span><a href="B21773_04.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.226.1">Chapter 4</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.227.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.228.1">In practice, we have two very comparable and viable approaches. </span><span class="koboSpan" id="kobo.228.2">However, to make an informed decision, we should first perform a more in-depth comparis</span><a id="_idTextAnchor196"/><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.229.1">on using </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">quantitative methods.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">To impartially assess the efficacy of in-context learning compared to fine-tuning, we can measure the quality and consistency of the generated responses. </span><span class="koboSpan" id="kobo.231.2">We can accomplish this using established and reliable metrics to compare outcomes from each of the approaches. </span><span class="koboSpan" id="kobo.231.3">Like prior evaluations, we will want to apply quantitative and qualitative methods applied across the following </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">key dimensions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.233.1">Alignment with human judgment</span></strong><span class="koboSpan" id="kobo.234.1">: We can again apply semantic similarity to provide a quantitative measure of how often the model’s responses are correct or relevant based on a reference answer written by </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">a human.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.236.1">StyleSprint’s brand communication experts can review a subset of the responses to provide a qualitative evaluation of the response accuracy and alignment with brand tone </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">and voice.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.238.1">Consistency and stability</span></strong><span class="koboSpan" id="kobo.239.1">: It is important to measure the degree to which questions are answered consistently each time despite minor variations in how the question is posed. </span><span class="koboSpan" id="kobo.239.2">Again, we can leverage semantic similarity to compare each new output to the prior when the input is </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">held constant.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.241.1">In addition to evaluating the quality of model responses for each approach, we can also directly compare the operational and computational overhead required </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">for each.</span></span></p>
<p><span class="koboSpan" id="kobo.243.1">For fine-tuning, we will need to understand</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.244.1"> the overhead involved</span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.245.1"> in training the model. </span><span class="koboSpan" id="kobo.245.2">While the PEFT method will significantly reduce the training effort, there could be considerably more infrastructure-related costs compared to in-context learning, which requires no additional training. </span><span class="koboSpan" id="kobo.245.3">Alternatively, for in-context learning, commoditized models such as OpenAI’s GPT-4 have a per-token cost model. </span><span class="koboSpan" id="kobo.245.4">StyleSprint must also consider the cost of tokens required to embed a sufficient number of few-shot examples in </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">the prompt.</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">In both cases, StyleSprint will incur some operational costs to create best-in-class examples written by humans that can be used as a “gold standard” in either the few-shot approach or for additional </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">model training.</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">By conducting these comparative tests and analyzing the results, StyleSprint will gain valuable insights into which approach – in-context learning or fine-tuning – best aligns with its operational goals and customer service standards. </span><span class="koboSpan" id="kobo.249.2">This data-driven evaluation will inform the decision</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.250.1"> on the optimal AI strategy</span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.251.1"> for enhancing their customer service experience. </span><span class="koboSpan" id="kobo.251.2">We will implement these comparisons i</span><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.252.1">n</span><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.253.1"> the practice project </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">that follows.</span></span></p>
<h1 id="_idParaDest-116"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.255.1">Practice project: Fine-tuning for Q&amp;A using PEFT</span></h1>
<p><span class="koboSpan" id="kobo.256.1">For our practice project, we will experiment</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.257.1"> with AdaLoRA to efficiently fine-tune a model for a customer</span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.258.1"> query and compare it directly to the output of a </span><strong class="bold"><span class="koboSpan" id="kobo.259.1">state-of-the-art</span></strong><span class="koboSpan" id="kobo.260.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.261.1">SOTA</span></strong><span class="koboSpan" id="kobo.262.1">) model using in-context learning. </span><span class="koboSpan" id="kobo.262.2">Like the previous chapter, we can rely on a prototyping environment such as Google Colab to complete the evaluation and comparison of the two approaches. </span><span class="koboSpan" id="kobo.262.3">We will demonstrate how to configure model traini</span><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.263.1">ng to use AdaLoRA as our </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">PEFT method.</span></span></p>
<h2 id="_idParaDest-117"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.265.1">Background regarding question-answering fine-tuning</span></h2>
<p><span class="koboSpan" id="kobo.266.1">Our project utilizes the Hugging Face</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.267.1"> training pipeline library, a widely recognized resource in the machine learning community. </span><span class="koboSpan" id="kobo.267.2">This library offers a variety of pre-built pipelines, including one for question-answering, which allows us to fine-tune pre-trained models with minimal setup. </span><span class="koboSpan" id="kobo.267.3">Hugging Face pipelines abstract much of the complexity involved in model training, making it accessible for developers to implement advanced natural language processing tasks directly and efficiently In particular, this pipeline behaves as an interface to a transformer model with a specific head for question-answering tasks. </span><span class="koboSpan" id="kobo.267.4">Recall that when we fine-tune a transformer model, we keep the architecture of the model – including the self-attention mechanism and the transformer layers – but we train the model’s parameters on a specific task, which, in this case, results in a model refined specifically to answer questions. </span><span class="koboSpan" id="kobo.267.5">Recall our practice project in </span><a href="B21773_03.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.268.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.269.1"> where the resulting model was a translator; we used a translator head to accomplish translation from English to French. </span><span class="koboSpan" id="kobo.269.2">For this project, the “head” is aligned to learn patterns in </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">question-answering data.</span></span></p>
<p><span class="koboSpan" id="kobo.271.1">However, when using</span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.272.1"> a question-answer training pipeline, it is important to understand that the model does not simply memorize question-answer pairs, it learns the connection between questions and answers. </span><span class="koboSpan" id="kobo.272.2">Moreover, to answer appropriately, the model cannot rely entirely on training. </span><span class="koboSpan" id="kobo.272.3">It also requires additional context as input to compose a relevant answer. </span><span class="koboSpan" id="kobo.272.4">To understand this further, we decompose the model inferencing step </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.274.1">When feeding a question to a model, we must also include context relevant to </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">the topic.</span></span></li>
<li><span class="koboSpan" id="kobo.276.1">The model then determines the most relevant part of the context that answers the question. </span><span class="koboSpan" id="kobo.276.2">It does this by assigning probability scores to each token (word or sub-word) in </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">the context.</span></span></li>
<li><span class="koboSpan" id="kobo.278.1">The model “thinks” of the context as a potential source for the answer and assigns each token two scores: one score for being the </span><strong class="bold"><span class="koboSpan" id="kobo.279.1">start</span></strong><span class="koboSpan" id="kobo.280.1"> of the answer, and another for being the </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">end</span></strong><span class="koboSpan" id="kobo.282.1"> of </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">the answer.</span></span></li>
<li><span class="koboSpan" id="kobo.284.1">The token with the highest “start” score and “end” score is then chosen to form the answer </span><strong class="bold"><span class="koboSpan" id="kobo.285.1">span</span></strong><span class="koboSpan" id="kobo.286.1">. </span><span class="koboSpan" id="kobo.286.2">The span is what is presented to </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">the user.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.288.1">To provide a concrete example, if we ask the model, </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">Does StyleSprint have any leather jackets?</span></strong><span class="koboSpan" id="kobo.290.1"> and provide a context of </span><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">StyleSprint sells a variety of coats, jackets and outerwear</span></strong><span class="koboSpan" id="kobo.292.1">, the model will process this context and identify that the most likely answer is something like </span><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">Yes, StyleSprint sells a variety of outerwear</span></strong><span class="koboSpan" id="kobo.294.1">. </span><span class="koboSpan" id="kobo.294.2">However, if the answer to a question is not included in the provided context, the model cannot generate a reliable answer. </span><span class="koboSpan" id="kobo.294.3">Additionally, if the context is too unspecific, the model may provide a more generic answer. </span><span class="koboSpan" id="kobo.294.4">Like in-context learning, the fine-tuned approach for question-answering requires relevant context. </span><span class="koboSpan" id="kobo.294.5">This means that, in practice, the model must be integrated with a search component that can retrieve additional context to pair with </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">each question.</span></span></p>
<p><span class="koboSpan" id="kobo.296.1">Consider our leather jacket example. </span><span class="koboSpan" id="kobo.296.2">When a question is received, the system could perform a search of its knowledge base and retrieve any contextual information relevant to a leather jacket (e.g., a paragraph about outerwear). </span><span class="koboSpan" id="kobo.296.3">Again, since the model was trained to answer questions in a way that aligns with the brand tone, it will extract the relevant information from the context provided to formulate an appropriate answer. </span><span class="koboSpan" id="kobo.296.4">Not only will integration with search provide the model with the context it needs but it will also allow the model to have up-to-date and </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">real-time information.</span></span></p>
<p><span class="koboSpan" id="kobo.298.1">Additionally, we might incorporate a confidence threshold, where the model only gives an answer if it assigns a high enough probability to the start and end tokens. </span><span class="koboSpan" id="kobo.298.2">If the highest probability is below this threshold, we might say the model does not know, or request more information. </span><span class="koboSpan" id="kobo.298.3">Overall, the model efficacy relies heavily on the quality and size of the training data as well as the relevance of the context with regard to the </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">questions posed.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">Now that we have a better understanding</span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.301.1"> of how fine-tuning for question-answering works and what to expect when using the question-answering pipeline from Hugging Face, </span><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.302.1">we can begin to write </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">our implementation.</span></span></p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.304.1">Implementation in Python</span></h2>
<p><span class="koboSpan" id="kobo.305.1">First and foremost, we</span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.306.1"> install</span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.307.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">required libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.309.1">
!pip install transformers peft sentence-transformers</span></pre>
<p><span class="koboSpan" id="kobo.310.1">Then, we import the question-answering modules from the transformers library. </span><span class="koboSpan" id="kobo.310.2">For</span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.311.1"> our project, we will use Google’s </span><strong class="bold"><span class="koboSpan" id="kobo.312.1">Flan T5 (small)</span></strong><span class="koboSpan" id="kobo.313.1">, which is considered a SOTA alternative to GPT 3.5. </span><span class="koboSpan" id="kobo.313.2">As one of our goals continues to be to measure the performance versus efficiency trade-off, we begin with the smallest version of Flan T5, which has 80M parameters. </span><span class="koboSpan" id="kobo.313.3">This will enable faster training and more rapid iteration. </span><span class="koboSpan" id="kobo.313.4">However, please note that even a small model trained over a small number of epochs will require a high-RAM </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">runtime environment:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.315.1">
from transformers import (
    AutoModelForQuestionAnswering, AutoTokenizer)
model_name = " google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForQuestionAnswering.from_pretrained(model_name)</span></pre>
<p><span class="koboSpan" id="kobo.316.1">With the pre-trained model instantiated, we can now configure the model to adapt its training process to use AdaLoRA, which, as we’ve learned, is specifically designed to allocate the parameter budget efficiently during the </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">fine-tuning process:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.318.1">
from peft import AdaLoraConfig
# Example configuration; adjust parameters as needed
adapter_config = AdaLoraConfig(target_r=16)
model.add_adapter(adapter_config)</span></pre>
<p><span class="koboSpan" id="kobo.319.1">As discussed, fine-tuning</span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.320.1"> relies heavily on the</span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.321.1"> quality and size of the training data. </span><span class="koboSpan" id="kobo.321.2">In the StyleSprint scenario, the company could aggregate question-answer pairs from its FAQ page, social media, and customer service transcripts. </span><span class="koboSpan" id="kobo.321.3">For this exercise, we will construct a simple dataset that looks similar to </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.323.1">
demo_data = [{
"question": "What are the latest streetwear trends available at Stylesprint?",
  "answer": "Stylesprint's latest streetwear collection includes hoodies, and graphic tees, all inspired by the latest hip-hop fashion trends."
</span><span class="koboSpan" id="kobo.323.2">...
</span><span class="koboSpan" id="kobo.323.3">}]</span></pre>
<p><span class="koboSpan" id="kobo.324.1">However, in order to integrate our dataset with the question-answer pipeline, we should first understand the </span><strong class="source-inline"><span class="koboSpan" id="kobo.325.1">Trainer</span></strong><span class="koboSpan" id="kobo.326.1"> class. </span><span class="koboSpan" id="kobo.326.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">Trainer</span></strong><span class="koboSpan" id="kobo.328.1"> class in the Hugging Face transformers library expects the training and evaluation datasets to be in a specific format, usually as a PyTorch </span><strong class="source-inline"><span class="koboSpan" id="kobo.329.1">Dataset</span></strong><span class="koboSpan" id="kobo.330.1"> object, not just as simple lists of dictionaries. </span><span class="koboSpan" id="kobo.330.2">Further, each entry in the dataset needs to be tokenized and structured with the necessary fields such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">input_ids</span></strong><span class="koboSpan" id="kobo.332.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.333.1">attention_mask</span></strong><span class="koboSpan" id="kobo.334.1">, and, for question-answering tasks, </span><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">start_positions</span></strong><span class="koboSpan" id="kobo.336.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.337.1">end_positions</span></strong><span class="koboSpan" id="kobo.338.1">. </span><span class="koboSpan" id="kobo.338.2">Let us explore these in </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">more detail:</span></span></p>
<ul>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">input_ids</span></strong><span class="koboSpan" id="kobo.341.1">: This is a sequence of integers that represent the input sentence in the model. </span><span class="koboSpan" id="kobo.341.2">Each word or sub-word in the sentence is converted into a unique integer or ID. </span><span class="koboSpan" id="kobo.341.3">Recall from</span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.342.1"> earlier chapters that this process is known as </span><strong class="bold"><span class="koboSpan" id="kobo.343.1">tokenization</span></strong><span class="koboSpan" id="kobo.344.1">. </span><span class="koboSpan" id="kobo.344.2">The words or tokens are looked up in the vocabulary of the language model and the corresponding integer is then used in the model. </span><span class="koboSpan" id="kobo.344.3">For example, a sentence such as </span><em class="italic"><span class="koboSpan" id="kobo.345.1">I love Paris</span></em><span class="koboSpan" id="kobo.346.1"> might be converted into something like </span><strong class="source-inline"><span class="koboSpan" id="kobo.347.1">[101, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">354, 2459]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">attention_mask</span></strong><span class="koboSpan" id="kobo.351.1">: An attention mask is a sequence of binary values where 1s indicate real tokens and 0s indicate padding tokens. </span><span class="koboSpan" id="kobo.351.2">In other words, in the places where 1s are present, the model will understand that those places need attention and the places with 0s will be ignored by the model. </span><span class="koboSpan" id="kobo.351.3">This is crucial when dealing with sentences of varying lengths and dealing with batches of sentences in </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">training models.</span></span></li>
<li><strong class="source-inline"><span class="koboSpan" id="kobo.353.1">start_positions</span></strong><span class="koboSpan" id="kobo.354.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.355.1">end_positions</span></strong><span class="koboSpan" id="kobo.356.1">: These are for question-answering tasks. </span><span class="koboSpan" id="kobo.356.2">They represent the indices of the start and end tokens of the answer in the tokenized form of the context. </span><span class="koboSpan" id="kobo.356.3">For example, in the context </span><em class="italic"><span class="koboSpan" id="kobo.357.1">Paris is the capital of France</span></em><span class="koboSpan" id="kobo.358.1">, if the question is </span><em class="italic"><span class="koboSpan" id="kobo.359.1">What is the capital of France?</span></em><span class="koboSpan" id="kobo.360.1"> and the answer given is </span><em class="italic"><span class="koboSpan" id="kobo.361.1">Paris</span></em><span class="koboSpan" id="kobo.362.1">, after tokenization, </span><strong class="source-inline"><span class="koboSpan" id="kobo.363.1">start_position</span></strong><span class="koboSpan" id="kobo.364.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.365.1">end_position</span></strong><span class="koboSpan" id="kobo.366.1"> will correspond to the index of </span><em class="italic"><span class="koboSpan" id="kobo.367.1">Paris</span></em><span class="koboSpan" id="kobo.368.1"> in </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">the context.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.370.1">With that understanding, we can create</span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.371.1"> a class that adapts</span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.372.1"> our dataset to meet the expectations of the trainer, </span><span class="No-Break"><span class="koboSpan" id="kobo.373.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.374.1">
from torch.utils.data import Dataset
class StylesprintDataset(Dataset):
   def __init__(self, tokenizer, data):
       tokenizer.pad_token = tokenizer.eos_token
       self.tokenizer = tokenizer
       self.data = data</span></pre>
<p><span class="koboSpan" id="kobo.375.1">For the complete custom dataset class code, visit this book’s GitHub repository </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">at </span></span><a href="https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python"><span class="No-Break"><span class="koboSpan" id="kobo.377.1">https://github.com/PacktPublishing/Generative-AI-Foundations-in-Python</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.378.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.379.1">With the training set prepared</span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.380.1"> and our pipeline configured</span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.381.1"> to apply the AdaLoRA method, we can finally move to the training step. </span><span class="koboSpan" id="kobo.381.2">For this project, we will configure the training to run for just a few epochs, but in the StyleSprint scenario, a much more robust training process would </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">be required:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.383.1">
from transformers import Trainer, TrainingArguments
# Split the mock dataset into training and evaluation sets (50/50)
train_data = StylesprintDataset(
    tokenizer, demo_data[:len(demo_data)//2])
eval_data = StylesprintDataset(
    tokenizer, demo_data[len(demo_data)//2:])
# Training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
)
# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=eval_data
)
# Start training
trainer.train()</span></pre>
<p><span class="koboSpan" id="kobo.384.1">For our simple experiment, we do not expect a highly performant model; however, we can learn how to interpret the training output, which describes how well the model performed on the evaluation samples. </span><span class="koboSpan" id="kobo.384.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.385.1">Trainer</span></strong><span class="koboSpan" id="kobo.386.1"> class</span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.387.1"> will output a training summary that includes</span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.388.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">loss metric.</span></span></p>
<h3><span class="koboSpan" id="kobo.390.1">Training loss</span></h3>
<p><span class="koboSpan" id="kobo.391.1">Training loss is a measure</span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.392.1"> of how well the model is performing; a lower loss indicates</span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.393.1"> better performance. </span><span class="koboSpan" id="kobo.393.2">In many deep learning models, especially those dealing with complex tasks such as language understanding, it’s common to start with a relatively high loss. </span><span class="koboSpan" id="kobo.393.3">The expectation is that this value should decrease as </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">training progresses.</span></span></p>
<p><span class="koboSpan" id="kobo.395.1">In the early stages of training, a high loss isn’t a cause for alarm as it commonly decreases as the model continues to learn. </span><span class="koboSpan" id="kobo.395.2">However, if the loss remains high, this signals that additional training may be needed. </span><span class="koboSpan" id="kobo.395.3">If the loss continues to be high after prolonged training, the learning rate and other hyperparameters may require adjustment, as an inappropriate learning rate can impact the model’s learning effectiveness. </span><span class="koboSpan" id="kobo.395.4">Moreover, the quality and quantity of your training data should be evaluated as insufficient data can hinder the training. </span><span class="koboSpan" id="kobo.395.5">For example, as we only use a few examples for the experiment, we expect a relatively </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">high loss.</span></span></p>
<p><span class="koboSpan" id="kobo.397.1">The next step is to use our newly fine-tuned model</span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.398.1"> to infer or predict. </span><span class="koboSpan" id="kobo.398.2">We should also secure</span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.399.1"> our trained model parameters so we can reuse it </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">without retraining:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.401.1">
import torch
# save parameters
model.save_pretrained("./stylesprint_qa_model")
def ask_question(model, question, context):
   # Tokenize the question and context
   inputs = tokenizer.encode_plus(question, context,
        add_special_tokens=True, return_tensors="pt")
   # Get model predictions
   with torch.no_grad():
       outputs = model(**inputs)
   # Get the start and end positions
   answer_start_scores = outputs.start_logits
   answer_end_scores = outputs.end_logits
   # Find the tokens with the highest `start` and `end` scores
   answer_start = torch.argmax(answer_start_scores)
   answer_end = torch.argmax(answer_end_scores) + 1
   # Convert the tokens to the answer string
   answer = tokenizer.convert_tokens_to_string(
        tokenizer.convert_ids_to_tokens(
            inputs["input_ids"][0][answer_start:answer_end]
            )
        )
   return answer
question = "What is the return policy for online purchases?"
</span><span class="koboSpan" id="kobo.401.2">context = """Excerpt from return policy returned from search."""
</span><span class="koboSpan" id="kobo.401.3">answer = ask_question(model, question, context)
print(answer)</span></pre>
<p><span class="koboSpan" id="kobo.402.1">As discussed, we introduce context along with a question to the model, so that it can identify which fragment of the context responds most appropriately to the query. </span><span class="koboSpan" id="kobo.402.2">Consequently, we may want to consider integrating a vector search system (such as RAG) to automatically identify relevant documents from large datasets based on semantic similarities to a query. </span><span class="koboSpan" id="kobo.402.3">These search results may not provide specific answers, but the trained QA model can extract more precise answers from </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">the results.</span></span></p>
<p><span class="koboSpan" id="kobo.404.1">With this hybrid approach, the vector search system first retrieves documents or text segments that are semantically related to the query. </span><span class="koboSpan" id="kobo.404.2">The QA model</span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.405.1"> then analyzes</span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.406.1"> this context to identify the precise answer t</span><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.407.1">hat aligns with StyleSprint’s guidelines </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">and expectations.</span></span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.409.1">Evaluation of results</span></h2>
<p><span class="koboSpan" id="kobo.410.1">To evaluate our</span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.411.1"> model outcomes, StyleSprint might apply the qualitative and quantitative approaches we have discussed in the chapter already. </span><span class="koboSpan" id="kobo.411.2">For the purpose of our experiment, we can measure the output of the model to a golden standard response using a simple measure for </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">semantic similarity:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.413.1">
from sentence_transformers import SentenceTransformer, util
import pandas as pd
# Example of a gold standard answer written by a human
gs = "Our policy at Stylesprint is to accept returns on online purchases within 30 days, with the condition that the items are unused and remain in their original condition."
</span><span class="koboSpan" id="kobo.413.2"># Example of answer using GPT 3.5 with in-context learning reusing a relevant subset of the training data examples
gpt_35 = "Stylesprint accepts returns within 30 days of purchase, provided the items are unworn and in their original condition."
</span><span class="koboSpan" id="kobo.413.3"># Load your dataset
dataset = pd.DataFrame([
   (gs, gpt_35, answer)
])# pd.read_csv("dataset.csv")
dataset.columns = ['gold_standard_response',
    'in_context_response', 'fine_tuned_response']
# Load a pre-trained sentence transformer model
eval_model = SentenceTransformer('all-MiniLM-L6-v2')
# Function to calculate semantic similarity
def calculate_semantic_similarity(model, response, gold_standard):
    response_embedding = model.encode(
        response, convert_to_tensor=True)
    gold_standard_embedding = model.encode(gold_standard,
        convert_to_tensor=True)
    return util.pytorch_cos_sim(response_embedding,
        gold_standard_embedding).item()
# Measure semantic similarity
dataset['in_context_similarity'] = dataset.apply(
    lambda row:calculate_semantic_similarity(
        eval_model, row['in_context_response'],
        row['gold_standard_response']
    ), axis=1)
dataset['fine_tuned_similarity'] = dataset.apply(
    lambda row:calculate_semantic_similarity(
        eval_model, row['fine_tuned_response'],
        row['gold_standard_response']
    ), axis=1)
# Print semantic similarity
print("Semantic similarity for in-context learning:", 
    dataset['in_context_similarity'])
print("Semantic similarity for fine-tuned model:", 
    dataset['fine_tuned_similarity'])</span></pre>
<p><span class="koboSpan" id="kobo.414.1">The results of our evaluation</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.415.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">as follows:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.417.1">PEFT </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">Flan T5</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.419.1">GPT 3.5T</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.420.1">Fine-tuned</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.421.1">In-context</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.422.1">Semantic Similarity</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.423.1">0.543</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.424.1">0.91</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.425.1">Table 5.1: Semantic similarity scores for fine-tuned Flan and GPT 3.5 Turbo, respectively</span></p>
<p><span class="koboSpan" id="kobo.426.1">Undoubtedly, the in-context learning arrived at an answer that was much closer to our gold standard reference. </span><span class="koboSpan" id="kobo.426.2">However, the fine-tuned model was not far behind. </span><span class="koboSpan" id="kobo.426.3">This tells us that with a more robust training dataset and considerably more epochs, the fine-tuned model could be comparable to GPT 3.5. </span><span class="koboSpan" id="kobo.426.4">With more iteration and experimentation, StyleSprint</span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.427.1"> could have a very robust fine-tuned model to answer very specific questions for </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">its customers.</span></span></p>
<h1 id="_idParaDest-120"><a id="_idTextAnchor208"/><span class="koboSpan" id="kobo.429.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.430.1">In this chapter, we focused on the strategic decision-making process between fine-tuning and in-context learning for StyleSprint’s AI-driven customer service system. </span><span class="koboSpan" id="kobo.430.2">While in-context learning, particularly few-shot learning, offers adaptability and resource efficiency, it may not consistently align with StyleSprint’s brand tone and customer service guidelines. </span><span class="koboSpan" id="kobo.430.3">This method relies heavily on the quality and relevance of the examples provided in the prompts, requiring careful crafting to ensure </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">optimal outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.432.1">On the other hand, PEFT methods such as AdaLoRA, offer a more focused approach to adapt a pre-trained model to the specific demands of customer service queries. </span><span class="koboSpan" id="kobo.432.2">PEFT methods modify only a small subset of a model’s parameters, reducing the computational burden while still achieving high performance. </span><span class="koboSpan" id="kobo.432.3">This efficiency is crucial for real-world applications where computational resources and response accuracy are both </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">key considerations.</span></span></p>
<p><span class="koboSpan" id="kobo.434.1">Ultimately, the choice between in-context learning and fine-tuning is not just a technical decision but also a strategic one, deeply intertwined with the company’s operational goals, resource allocation, and the desired customer experience. </span><span class="koboSpan" id="kobo.434.2">The chapter suggests conducting comparative tests to assess the efficacy of both approaches, evaluating outcomes at scale through reliable metrics. </span><span class="koboSpan" id="kobo.434.3">This data-driven evaluation will inform StyleSprint’s decision on the optimal AI strategy for enhancing their customer </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">service experience.</span></span></p>
<p><span class="koboSpan" id="kobo.436.1">In summary, we now have a more complete understanding of the implications of fine-tuning versus in-context learning in LLMs, specifically in the context of customer service. </span><span class="koboSpan" id="kobo.436.2">It highlights the need for a company like StyleSprint to make a well-informed strategic decision, balancing the depth of specialization and consistency offered by fine-tuning against the adaptability and efficiency of </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">in-context learning.</span></span></p>
<p><span class="koboSpan" id="kobo.438.1">In the next chapter, we will explore PEFT for domain adaptation where the outcome of our training is a general-purpose model refined to understand a highly specific domain like finance </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">or law.</span></span><a id="_idTextAnchor209"/></p>
<h1 id="_idParaDest-121"><a id="_idTextAnchor210"/><span class="koboSpan" id="kobo.440.1">References</span></h1>
<p><span class="koboSpan" id="kobo.441.1">This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">subject matter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.443.1">Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. </span><span class="koboSpan" id="kobo.443.2">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.444.1">Improving language understanding by generative </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.445.1">pre-training</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">. </span><span class="koboSpan" id="kobo.446.2">OpenAI.</span></span></li>
<li><span class="koboSpan" id="kobo.447.1">Hu, E. </span><span class="koboSpan" id="kobo.447.2">J., Shen, Y., Wallis, P., Li, Y., Wang, S., Wang, L., and Chen, W. </span><span class="koboSpan" id="kobo.447.3">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.448.1">LoRA: Low-Rank Adaptation of Large Language Models</span></em><span class="koboSpan" id="kobo.449.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">ArXiv. </span><span class="koboSpan" id="kobo.450.2">/abs/2106.09685</span></span></li>
<li><span class="koboSpan" id="kobo.451.1">Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., and Zhao, T. </span><span class="koboSpan" id="kobo.451.2">(2023). </span><em class="italic"><span class="koboSpan" id="kobo.452.1">Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</span></em><span class="koboSpan" id="kobo.453.1">. </span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">ArXiv. </span><span class="koboSpan" id="kobo.454.2">/abs/2303.10512</span></span></li>
<li><span class="koboSpan" id="kobo.455.1">Brown TB, Mann B, Ryder N, et al. </span><span class="koboSpan" id="kobo.455.2">2020. </span><em class="italic"><span class="koboSpan" id="kobo.456.1">Language Models are Few-Shot </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.457.1">Learners</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">. </span><span class="koboSpan" id="kobo.458.2">ArXiv:2005.14165.</span></span></li>
</ul>
</div>
</body></html>