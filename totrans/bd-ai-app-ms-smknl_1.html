<html><head></head><body>
<div><div><div><h1 id="_idParaDest-15" class="chapter-number"><a id="_idTextAnchor014"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Introducing Microsoft Semantic Kernel</h1>
			<p>The <strong class="bold">generative artificial intelligence</strong> (<strong class="bold">GenAI</strong>) space is evolving quickly, with dozens of new products <a id="_idIndexMarker000"/>and services being launched weekly; it is becoming hard for developers to keep up with the ever-changing features <a id="_idIndexMarker001"/>and <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) for each of the services. In this book, you will learn about <strong class="bold">Microsoft Semantic Kernel</strong>, an <a id="_idIndexMarker002"/>API that will make it a lot easier for you to use GenAI as a developer, making your code shorter, simpler, and more maintainable. Microsoft Semantic Kernel will allow you, as a developer, to use a single interface to connect with several different GenAI providers. Microsoft used Semantic Kernel to develop its copilots, such as Microsoft 365 Copilot.</p>
			<p>Billions of people already use GenAI as consumers, and you are probably one of them. We will start this chapter by showing some examples of what you can do with GenAI as a consumer. Then, you will learn how you can start using GenAI as a developer to add AI services to your own applications.</p>
			<p>In this chapter, you will learn the differences between using GenAI as a user and as a developer and how to create and run a simple end-to-end request with Microsoft Semantic Kernel. This will help you see how powerful and simple Semantic Kernel is and will serve as a framework for all further chapters. It will enable you to begin integrating AI into your own apps right away.</p>
			<p>In this chapter, we’ll be covering the following topics:</p>
			<ul>
				<li>Understanding the basic use of a generative AI application like ChatGPT</li>
				<li>Installing Microsoft Semantic Kernel</li>
				<li>Configuring Semantic Kernel to interact with AI services</li>
				<li>Running a simple task using Semantic Kernel</li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Technical requirements</h1>
			<p>To complete this chapter, you will need to have a recent, supported version of your preferred Python or C# development environment:</p>
			<ul>
				<li>For Python, the minimum supported version is Python 3.10, and the recommended version is Python 3.11</li>
				<li>For C#, the minimum supported version is .NET 8</li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">The examples are presented in C# and Python, and you can choose to only read the examples of your preferred language. Occasionally, a feature is available in only one of the languages. In such cases, we provide an alternative in the other language for how to achieve the same objectives.</p>
			<p>In this chapter, we will call OpenAI services. Given the amount that companies spend on training these large language models (LLMs), it’s no surprise that using these services is not free. You will <a id="_idIndexMarker003"/>need an <strong class="bold">OpenAI API</strong> key, obtained either directly through <strong class="bold">OpenAI</strong> or <strong class="bold">Microsoft</strong>, via the <strong class="bold">Azure </strong><strong class="bold">OpenAI</strong> service.</p>
			<p class="callout-heading">Important: Using the OpenAI services is not free</p>
			<p class="callout">The examples that we <a id="_idIndexMarker004"/>will run throughout this book will call the OpenAI API. These calls require a paid subscription, and each call will incur a cost. The costs are usually small per request (for example, GPT-4 costs up to $0.12 per 1,000 tokens), but they can add up. In addition, note that different models have different prices, with GPT-3.5 being 30 times less expensive per token than GPT-4.</p>
			<p class="callout">OpenAI pricing <a id="_idIndexMarker005"/>information can be found here: <a href="https://openai.com/pricing">https://openai.com/pricing</a></p>
			<p class="callout">Azure OpenAI pricing information can be found here: <a href="https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/">https://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/</a></p>
			<p>If you use .NET, the code for this chapter is at <a href="https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1">https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/dotnet/ch1</a>.</p>
			<p>If you use Python, the code for this chapter is at <a href="https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1">https://github.com/PacktPublishing/Building-AI-Applications-with-Microsoft-Semantic-Kernel/tree/main/python/ch1</a>.</p>
			<p>You can install the required packages by going to the GitHub repository and using the following: <code>pip install -</code><code>r requirements.txt</code>.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor017"/>Obtaining an OpenAI API key</h2>
			<ol>
				<li>Go to the <a id="_idIndexMarker006"/>OpenAI Platform website (<a href="https://platform.openai.com">https://platform.openai.com</a>).</li>
				<li>Sign up for a new account or sign in with an existing account. You can use your email or an existing Microsoft, Google, or Apple account.</li>
				<li>On the left sidebar menu, select <strong class="bold">API keys</strong>.</li>
				<li>On the <strong class="bold">Project API keys</strong> screen, click the button labeled <strong class="bold">+ Create new secret key</strong> (optionally, give it a name).</li>
			</ol>
			<p class="callout-heading">Important</p>
			<p class="callout">You have to copy and save the key immediately. It will disappear as soon as you click <strong class="bold">Done</strong>. If you didn’t copy the key or if you lost it, you need to generate a new one. There’s no cost to generate a new key. Remember to delete old keys.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Obtaining an Azure OpenAI API key</h2>
			<p>Currently, you need <a id="_idIndexMarker007"/>to submit an application to obtain access to the Azure OpenAI Service. To apply for access, you need to complete a form at <a href="https://aka.ms/oai/access">https://aka.ms/oai/access</a>.</p>
			<p>The instructions to obtain an Azure OpenAI API key are available at <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource">https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/create-resource</a>.</p>
			<h1 id="_idParaDest-20"><a id="_idTextAnchor019"/>Generative AI and how to use it</h1>
			<p>Generative AI refers to a subset of artificial intelligence programs that are capable of creating content <a id="_idIndexMarker008"/>that is similar to what humans can produce. These systems use training from very large datasets to learn their patterns, styles, and structures. Then, they can <a id="_idIndexMarker009"/>generate entirely new content, such as synthesized images, music, and text.</p>
			<p>Using GenAI as consumer or end-user is very easy, and as a technical person, you probably have already done it. There are many consumer-facing AI products. The most famous is OpenAI’s <strong class="bold">ChatGPT</strong>, but there are many others that have hundreds of millions of users every day, such as Microsoft Copilot, Google Gemini (formerly Bard), and Midjourney. As of October 2023, Meta, the parent company of Facebook, WhatsApp, and Instagram, is making GenAI services available to all its users, increasing the number of GenAI daily users to billions.</p>
			<p>While the concept of GenAI has existed for a while, it gained a lot of users with the release of OpenAI’s ChatGPT in November of 2022. The initial version of ChatGPT was based on <a id="_idIndexMarker010"/>a model called <strong class="bold">generative pre-trained transformer</strong> (<strong class="bold">GPT</strong>) version 3.5. That version was a lot better than earlier versions in the task of mimicking human-like writing. In addition, OpenAI made it easy to use by adding a chatbot-like interface and making it available to the general public. This interface is called ChatGPT. With ChatGPT, users can easily initiate tasks in their own words. At its launch, ChatGPT was the product with the fastest adoption rate in history.</p>
			<p>The GenAI concept was further popularized with the release of Midjourney, an application that allows for the generation of high-quality images from prompts submitted through Discord, a popular chat application, and Microsoft Copilot, a free web application that can generate text by using GPT-4 (the newest version of OpenAI’s GPT) and images by using an OpenAI model called DALL-E 3.</p>
			<p>In the upcoming subsections, we will discuss text and image generation using GenAI applications and explain the differences between generating them using applications such as ChatGPT and with an API as a developer.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Text generation models</h2>
			<p>The initial use <a id="_idIndexMarker011"/>cases for GenAI were to generate text based on a simple instruction called a <strong class="bold">prompt</strong>.</p>
			<p>The technology <a id="_idIndexMarker012"/>used behind most text-based GenAI products is called a <strong class="bold">transformer</strong>, and it was introduced in the paper <em class="italic">Attention is All you Need</em> [1] in 2017. The transformer immensely improved the quality of the text being generated, and in just <a id="_idIndexMarker013"/>a few years, the text looked very similar to human-generated text. The transformer greatly improved the ability of AI to guess masked words in a phrase after being trained on a large number of documents (a <strong class="bold">corpus</strong>). Models <a id="_idIndexMarker014"/>trained on very large corpuses are called <strong class="bold">large language </strong><strong class="bold">models</strong> (<strong class="bold">LLMs</strong>).</p>
			<p>If LLMs are given a phrase such as “<em class="italic">I went to the fast-food restaurant to &lt;X&gt;</em>,” they can generate good options for <em class="italic">X</em>, such as “<em class="italic">eat</em>.” Applying the transformer repeatedly can generate coherent phrases and even stories. The next iteration could be “<em class="italic">I went to the fast-food restaurant to eat &lt;X&gt;</em>,” returning “<em class="italic">a</em>,” and then “<em class="italic">I went to the fast-food restaurant to eat a &lt;X&gt;</em>,” could return “<em class="italic">burger</em>,” forming the full phrase “<em class="italic">I went to the fast-food restaurant to eat </em><em class="italic">a burger</em>.”</p>
			<p>The performance of an LLM model depends on the number of parameters, which is roughly proportional to how many comparisons a model can make at once, the context window, the maximum size of the text that can be handled at once, and the data used to train the model, which is usually kept secret by the companies that create LLMs.</p>
			<p>The GPT is a model created by OpenAI that uses transformers and is good at generating text. There are many versions of GPT:</p>
			<p>GPT-1, launched in February 2018, had 120 million parameters and a <strong class="bold">context window</strong> of 512 tokens.</p>
			<p>GPT-2 was launched in February 2019, with the number of parameters increasing to 1.5 billion and the context window increasing to 1,024 tokens. Up to this point, while they sometimes produced interesting results, these models were mostly used by academics.</p>
			<p>This changed with GPT-3, launched in June 2020, which had several sizes: small, medium, large, and extra-large. Extra-large had 175 billion parameters and a 2,048 token context window. The generated text was, in most cases, hard to distinguish from human-generated text. OpenAI followed it with GPT-3.5, released in November 2022, with still 175 billion parameters and a context window of 4,096 tokens (now expanded to 16,384 tokens), and launched a user interface named ChatGPT.</p>
			<p>ChatGPT is a web and mobile application that uses the GPT models in the background and allows users to submit prompts to the GPT models and get responses online. It was launched together with GPT-3.5, and at the time, it was the consumer product with the fastest adoption rate, reaching 100 million users in less than two months.</p>
			<p>In February 2023, Microsoft released Bing Chat, which also uses OpenAI’s GPT models in the back end, further popularizing the usage of transformer models and AI. Recently, Microsoft renamed it to Microsoft Copilot.</p>
			<p>Just a month later, in March 2023, OpenAI released the GPT-4 model, which was quickly incorporated into the backend of consumer products such as ChatGPT and Bing.</p>
			<p>Not all details <a id="_idIndexMarker015"/>about the GPT-4 model have been released to the public. It’s known that it has a context window of up to 32,768 tokens; however, its number of parameters has not been made public, but it has been estimated at 1.8 trillion.</p>
			<p>The GPT-4 model is notably good at human-like tasks related to text generation. A benchmark test shown in the GPT-4 technical report academic paper [2] shows the performance of GPT-3.5 and GPT-4 in taking exams. GPT-4 could pass many high-school and college level exams. You can read the paper at <a href="https://doi.org/10.48550/arXiv.2303.08774">https://doi.org/10.48550/arXiv.2303.08774</a>.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Understanding the difference between applications and models</h2>
			<p>Most people, including you, have likely used a GenAI application, such as ChatGPT, Microsoft Copilot, Bing Image Creator, Bard (now named Gemini), or Midjourney. These applications <a id="_idIndexMarker016"/>use GenAI models in their backend, but they also add a user interface and configurations that restrict and control the output of the models.</p>
			<p>When you are <a id="_idIndexMarker017"/>developing your own application, you will need to do these things by yourself. You may not yet realize how much is carried out behind the scenes by applications such as Bing and ChatGPT.</p>
			<p>When you submit a prompt to an application, the application may add several additional instructions to the prompt you submitted. The most typical things added are instructions to restrict some types of output, for example: “<em class="italic">your reply should contain no curse words</em>.” For example, when you submit the prompt “<em class="italic">Tell me a joke</em>” to an application like ChatGPT, it may modify your prompt to “<em class="italic">Tell me a joke. Your reply should contain no curse words</em>” and pass that modified prompt to the model.</p>
			<p>The application may also add a summary of the questions that you have already submitted and the answers that were already given. For example, if you ask, “<em class="italic">How warm is Rio de Janeiro, Brazil, in the summer?</em>,” the answer may be, “<em class="italic">Rio de Janeiro is typically between 90 and 100 degrees Fahrenheit (30-40 degrees Celsius) in the summer</em>.” If you then ask the question, “<em class="italic">How long is the flight from New York to there?</em>,” an application such as ChatGPT will not submit “<em class="italic">How long is the flight from New York to there?”</em> directly to the model because the answer would be something like “<em class="italic">I don’t know what you mean </em><em class="italic">by ‘there’</em>.”</p>
			<p>A straightforward way to address this problem is to save everything that the user entered and <a id="_idIndexMarker018"/>all the answers that were provided and re-submit <a id="_idIndexMarker019"/>them with every new prompt. For example, when the user submits “<em class="italic">How long is the flight from New York to there?</em>” after asking about the temperature, the application prepends the earlier questions and answers to the prompt, and what is actually submitted to the model is: “<em class="italic">How warm is Rio de Janeiro, Brazil, in the summer? Rio de Janeiro is typically between 90 and 100 degrees Fahrenheit (30-40 degrees Celsius) in the summer. How long is the flight from New York to there?</em>” Now, the model knows that “<em class="italic">there</em>” means “<em class="italic">Rio de Janeiro</em>,” and the answer will be something like “<em class="italic">Approximately </em><em class="italic">10 hours</em>.”</p>
			<p>The consequence of appending all earlier prompts and responses to each new prompt is that it consumes a lot of space in the context window. Therefore, some techniques have been developed to compress the information that is added to the user prompt. The simplest technique is to keep only the earlier user questions, but not the answers given by the application. In that case, for example, the modified prompt would be something like “<em class="italic">Earlier I said: ‘How warm is Rio de Janeiro, Brazil, in the summer?, now answer only: “How long is the flight from New York to there?”</em>’. Note that the prompt needs to tell the model to respond only to the last question submitted by the user.</p>
			<p>Knowing that applications modify your prompts will be relevant for you if you test your prompts using consumer applications because the output you get from them can be substantially different from what you get when you use the model directly through an API, such as Microsoft Semantic Kernel. There’s usually no way to know how the applications are modifying your prompts, as the providers usually don’t reveal all their techniques.</p>
			<p>Furthermore, a significant part of what you will do as an application developer will be to create prompt <a id="_idIndexMarker020"/>modifications that match your own application. Therefore, when <a id="_idIndexMarker021"/>your user submits their prompt, you will add your own prompt modifications to ensure they get an appropriate result. The techniques <a id="_idIndexMarker022"/>to modify user prompts are called <strong class="bold">prompt engineering</strong>, which we will explore briefly in the next chapter.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Generating text using consumer applications</h2>
			<p>Let’s explore the two most popular text generation applications: ChatGPT and Microsoft Copilot. If you <a id="_idIndexMarker023"/>are not familiar with the power of GenAI, trying them out will give you a sense of what can be done with them and how powerful they are. We will also briefly talk about their configuration parameters and their architecture, which can help you decide the appropriate architecture for your own applications later.</p>
			<h3>OpenAI ChatGPT</h3>
			<p>If you have never used a GenAI app, ChatGPT is likely the best place to start. ChatGPT’s default backend <a id="_idIndexMarker024"/>model is GPT 3.5, a fast and very powerful model. ChatGPT is free to use when OpenAI has available capacity on <a id="_idIndexMarker025"/>their servers. You can also buy a subscription to ChatGPT Plus for $20 per month, and that will give you the ability to use their most powerful model (currently GPT-4) and ensure you will always have capacity.</p>
			<p>To use ChatGPT, go to <a href="https://chat.openai.com">https://chat.openai.com</a>.</p>
			<p>The ChatGPT interface is very simple. It allows you to choose the backend model on top, some suggestions of prompts in the middle, and a text box to enter prompts in the bottom. It also includes a notice that the output produced may include incorrect information:</p>
			<div><div><img src="img/B21826_01_1.jpg" alt="Figure 1.1 – Submitting requests to ChatGPT using the web interface" width="975" height="626"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Submitting requests to ChatGPT using the web interface</p>
			<p>I will submit <a id="_idIndexMarker026"/>the following prompt in the <strong class="bold">Send a </strong><strong class="bold">Message</strong> textbox:</p>
			<p><code>How long is the flight between New York City and Rio </code><code>de Janeiro?</code></p>
			<p>ChatGPT, using the <a id="_idIndexMarker027"/>GPT-3.5 model, provides the following answer:</p>
			<pre class="console">
The flight duration between New York City and Rio de Janeiro can vary depending on several factors, including the specific departure and arrival airports, the flight route, and any layovers or stopovers. On average, a non-stop flight between John F. Kennedy International Airport (JFK) in New York City and Rio de Janeiro-Galeão International Airport (GIG) in Rio de Janeiro takes approximately 9 to 10 hours.
Keep in mind that flight durations can vary, so it's always a good idea to check with airlines for the most up-to-date and accurate flight schedules and durations for your specific travel dates and routes. Additionally, if you have layovers or stopovers, the total travel time will be longer.</pre>			<h3>Microsoft Copilot</h3>
			<p>Another free <a id="_idIndexMarker028"/>alternative is Microsoft Copilot, formerly Bing Chat. It is available from the <a href="http://www.bing.com">www.bing.com</a> page, but it can be accessed directly from <a href="https://www.bing.com/chat">https://www.bing.com/chat</a>.</p>
			<p>The user interface for Microsoft Copilot is like the interface of ChatGPT. It has some suggestions for <a id="_idIndexMarker029"/>prompts in the middle of the screen and a text box, where the user can enter a prompt at the bottom. The Microsoft Copilot UI also shows a couple of options that will be relevant from when we use models programmatically.</p>
			<p>The first is the conversation style. Copilot offers the option of being More Creative, More Balanced, or More Precise. This is related to the temperature parameter that will be passed to the underlying model. We will talk about the temperature parameter in <a href="B21826_03.xhtml#_idTextAnchor071"><em class="italic">Chapter 3</em></a>, but in short, the temperature parameter determines how common the words chosen by the LLM are.</p>
			<p class="callout-heading">Parameters in Microsoft Copilot</p>
			<p class="callout">While Microsoft Copilot does not reveal the exact configuration values, <code>0</code> and <code>0.2</code>), resulting in very safe guesses for the next word. For <code>0.4</code> and <code>0.6</code>), resulting <em class="italic">mostly</em> in safe guesses, but with the occasional guess being rare. <code>0.8</code>. Most guesses will still be safe, but more guesses will be rare words. Since LLMs guess words of a phrase in sequence, previous guesses influence subsequent guesses. When generating a phrase, each rare word makes the whole phrase more unusual.</p>
			<p>Another interesting component in the UI is that the bottom right of the text box shows how many characters have been entered, giving you an idea of how much you will consume of the underlying model’s context window. Note that you cannot know for sure how much you will consume because the Copilot application will modify your prompt.</p>
			<div><div><img src="img/B21826_01_2.jpg" alt="Figure 1.2 – Microsoft Copilot user interface" width="1656" height="1424"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Microsoft Copilot user interface</p>
			<p>On August 11, 2023, Mikhail Parakhin, Bing’s former <a id="_idIndexMarker031"/>CEO, posted <a id="_idIndexMarker032"/>on X/Twitter that Microsoft Copilot outperforms GPT-4 because it uses <strong class="bold">retrieval augmented </strong><strong class="bold">inference</strong> (<a href="https://x.com/MParakhin/status/1689824478602424320?s=20">https://x.com/MParakhin/status/1689824478602424320?s=20</a>):</p>
			<div><div><img src="img/B21826_01_3.jpg" alt="Figure 1.3 – Post by Bing’s former CEO about Microsoft Copilot using RAG" width="814" height="220"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Post by Bing’s former CEO about Microsoft Copilot using RAG</p>
			<p>We will talk more about retrieval augmented inference in <em class="italic">Chapters 6</em> and <em class="italic">7</em>, but for our current purposes, this means that Microsoft Copilot does not directly submit your prompt to the model. Bing has not publicly released the details of their architecture yet, but it is likely that Bing modifies your prompt (it shows the modified prompt in the UI, under <strong class="bold">Searching for</strong>), makes a regular Bing query using the modified prompt, gathers the results of that Bing query, concatenates them, and submits the concatenated results as a large prompt to a GPT model, asking it to combine the results to output a coherent answer.</p>
			<p>Using <a id="_idIndexMarker033"/>retrieval augmentation allows Bing to <a id="_idIndexMarker034"/>add citations and advertisements more easily. In the figure below, note that my prompt <code>How long is the flight between New York City and Rio de Janeiro?</code> was modified by Copilot to <code>Searching for flight duration New York City Rio </code><code>de Janeiro</code>:</p>
			<div><div><img src="img/B21826_01_4.jpg" alt="Figure 1.4 – An example of using Microsoft Copilot" width="1650" height="802"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – An example of using Microsoft Copilot</p>
			<p>As you can see, you can use consumer applications such as ChatGPT and Microsoft Copilot to familiarize yourself with how LLMs work for GenAI and to do some initial testing of your prompts, but be aware that the prompt you submit may be heavily modified by the application, and the response that you get from the underlying model can be very different from the responses that you will get when you actually create your own application.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Generating images</h2>
			<p>Besides generating text, AI can also be used to generate images from text prompts. The details of the process used to generate images from prompts are outside the scope of this book, but we <a id="_idIndexMarker035"/>will provide a quick summary. The main models in the image generation space are Midjourney, which is available through the Midjourney Bot in Discord; the open-source Stable Diffusion, which is also used by OpenAI’s DALL-E 2; and the DALL-E 3, released in October 2023 and available through the Bing Chat (now Microsoft Copilot) and ChatGPT applications.</p>
			<p>At the time of writing, Microsoft Semantic Kernel only supports DALL-E; therefore, this is the example we are going to explore. DALL-E 3 is available for free through the Microsoft Copilot application, with some limitations.</p>
			<p>If you are using the Microsoft Copilot application from the earlier example, make sure to reset your chat history by clicking on the <strong class="bold">New Topic</strong> button to the left of the text box. To generate images, make sure your conversation style is set to <strong class="bold">More Creative</strong>, as image generation only works in Creative mode:</p>
			<div><div><img src="img/B21826_01_5.jpg" alt="Figure 1.5 – Choosing the conversation style" width="496" height="177"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Choosing the conversation style</p>
			<p>I will use the following prompt:</p>
			<p><code>create a photorealistic image of a salt-and-pepper standard schnauzer on a street corner holding a sign "Will do tricks </code><code>for cheese."</code></p>
			<p>Microsoft Copilot will call DALL-E 3 and generate four images based on my specification:</p>
			<div><div><img src="img/B21826_01_6.jpg" alt="Figure 1.6 – Images generated by Microsoft Copilot" width="1010" height="1013"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Images generated by Microsoft Copilot</p>
			<p>One way in <a id="_idIndexMarker036"/>which DALL-E 3 is better than other image-generating models is that it can correctly add text to images. Earlier versions of DALL-E and most other models cannot spell words properly.</p>
			<p>The images are presented in a grid with a total resolution of 1024 x 1024 pixels (512 x 512 per image). If you select one image, that specific image will be upscaled to the 1024 x 1024 pixel resolution. In my case, I will select the image in the bottom left corner. You can see the final result in the next figure:</p>
			<div><div><img src="img/B21826_01_7.jpg" alt="Figure 1.7 – High-resolution image generated by Microsoft Copilot" width="1013" height="1013"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – High-resolution image generated by Microsoft Copilot</p>
			<p>As you can see, GenAI can <a id="_idIndexMarker037"/>also be used to generate images and now you have an idea of how powerful it can be. We will explore generating images with Microsoft Semantic Kernel in <a href="B21826_04.xhtml#_idTextAnchor086"><em class="italic">Chapter 4</em></a>. There is a lot to explore before we get there, and we will start with a quick, comprehensive tour of Microsoft Semantic Kernel.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Microsoft Semantic Kernel</h1>
			<p>Microsoft Semantic Kernel (<a href="https://github.com/microsoft/semantic-kernel">https://github.com/microsoft/semantic-kernel</a>) is a thin, open <a id="_idIndexMarker038"/>source <strong class="bold">software development toolkit</strong> (<strong class="bold">SDK</strong>) that makes it easier for applications developed in C# and Python to interact with <a id="_idIndexMarker039"/>AI services such as the ones made available through OpenAI, Azure OpenAI, and Hugging Face. Semantic Kernel can receive requests from your application and route them to different AI services. Furthermore, if you extend the functionality of Semantic Kernel by adding your own functions, which we will explore in <a href="B21826_03.xhtml#_idTextAnchor071"><em class="italic">Chapter 3</em></a>, Semantic Kernel can automatically discover which functions need to be used, and in which order, to fulfill a request. The request can come directly from the user and be passed through directly from your application, or your application can modify and enrich the user request before sending it to Semantic Kernel.</p>
			<p>It was originally designed to power different versions of Microsoft Copilot, such as Microsoft 365 Copilot and the Bing Copilot, and then be released to the developer community as an open source package. Developers can use Semantic Kernel to create plugins that execute complex actions using AI services and combine these plugins using just a few lines of code.</p>
			<p>In addition, Semantic Kernel <a id="_idIndexMarker040"/>can automatically orchestrate <a id="_idIndexMarker041"/>different plugins by using a <strong class="bold">planner</strong>. With the planner, a user can ask your application to achieve a complex goal. For example, if you have a function that identifies which animal is in a picture and another function that tells knock-knock jokes, your user can say, “<em class="italic">Tell me a knock-knock joke about the animal in the picture in this URL</em>,” and the planner will automatically understand that it needs to call the identification function first and the “tell joke” function after it. Semantic Kernel will automatically search and combine your plugins to achieve that goal and create a plan. Then, Semantic Kernel will execute that plan and provide a response to the user:</p>
			<div><div><img src="img/B21826_01_8.jpg" alt="Figure 1.8 – Structure of Microsoft Semantic Kernel" width="1650" height="493"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Structure of Microsoft Semantic Kernel</p>
			<p>In the upcoming sections, we will do a quick end-to-end tour of Semantic Kernel, going through most of the steps in <em class="italic">Figure 1</em><em class="italic">.8</em>. We will send a request, create a plan, call the API, and call a native function and a semantic function. These, combined, will generate an answer to the user. First, we will do this manually, step-by-step, and then we will do everything in <a id="_idIndexMarker042"/>one go using the planner. You will see how powerful Semantic Kernel can be with just a little code.</p>
			<p>Before we start experimenting with Microsoft Semantic Kernel, we need to install it.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Installing the Microsoft Semantic Kernel package</h2>
			<p>To use <a id="_idIndexMarker043"/>Microsoft Semantic Kernel, you must install it in your environment. Please note that Microsoft Semantic Kernel is still in active development, and there may be differences between versions.</p>
			<h3>Installing Microsoft Semantic Kernel in Python</h3>
			<p>To install <a id="_idIndexMarker044"/>Microsoft Semantic <a id="_idIndexMarker045"/>Kernel in Python, start in a new directory and follow these steps:</p>
			<ol>
				<li>Create a new virtual environment with <code>venv</code>:<pre class="source-code">
<strong class="bold">python -m venv .venv</strong></pre></li>				<li>Activate the new environment you just created. This will ensure that Microsoft Semantic Kernel will be installed only for this directory:<ul><li>In PowerShell, use the following:<pre class="source-code">
<code>pip</code>:<pre class="source-code">
<strong class="bold">pip install semantic-kernel</strong></pre></li>			</ol>
			<h3>Installing Microsoft Semantic Kernel in C#</h3>
			<p>To <a id="_idIndexMarker046"/>install Microsoft Semantic <a id="_idIndexMarker047"/>Kernel in C#, follow these steps:</p>
			<ol>
				<li>Create a new project targeting .NET 8:<pre class="source-code">
<strong class="bold">dotnet new console -o ch1 -f net8.0</strong></pre></li>				<li>Change into the application directory:<pre class="source-code">
<code>Microsoft.SemanticKernel</code> NuGet package:<pre class="source-code">
<strong class="bold">dotnet add package Microsoft.SemanticKernel --prerelease</strong></pre><p class="list-inset">The kernel object itself is very lightweight. It is simply a repository of all the services and plugins that are connected to your application. Most applications start by instantiating an empty kernel and then adding services and functions to it.</p></li>			</ol>
			<p>Run the program with the following simple instructions just to make sure that the installation succeeded:</p>
			<ul>
				<li>Instantiating the kernel in Python:<pre class="source-code">
<strong class="bold">import semantic_kernel as sk</strong>
<strong class="bold">kernel = sk.Kernel()</strong></pre></li>				<li>Instantiating the kernel in C#:<pre class="source-code">
<strong class="bold">using Microsoft.SemanticKernel;</strong>
<strong class="bold">Kernel kernel = Kernel.CreateBuilder().Build()</strong></pre></li>			</ul>
			<p>We have now installed Semantic Kernel in your environment, and now we’re ready to connect it to AI services and start using them.</p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Using Semantic Kernel to connect to AI services</h1>
			<p>To complete this section, you must have an API key. The process to obtain an API key was <a id="_idIndexMarker048"/>described at the beginning of this chapter.</p>
			<p>In the <a id="_idIndexMarker049"/>upcoming subsections, we are only going to connect to the OpenAI text models GPT-3.5 and GPT-4. If you have access to the OpenAI models through Azure, you will need to make minor modifications to your code.</p>
			<p>Although it would be simpler to connect to a single model, we are already going to show a simple but powerful Microsoft Semantic Kernel feature: we’re going to connect to two different models and run a simple prompt using the simpler but less expensive model, GPT-3.5, and a more complex prompt on the more advanced but also more expensive model, GPT-4.</p>
			<p>This process of sending simpler requests to simpler models and more complex requests to more complex models is something that you will frequently do when creating your own applications. This <a id="_idIndexMarker050"/>approach is called <strong class="bold">LLM cascade</strong>, and it was popularized in the FrugalGPT [3] paper. It can result in substantial cost savings.</p>
			<p class="callout-heading">Important: Order matters</p>
			<p class="callout">The order in which you load your services matters. Both for Python (<em class="italic">step 3</em> in the<em class="italic"> Connecting to OpenAI Services using Python</em> section) and for C# (<em class="italic">step 4</em> in the <em class="italic">Connecting to OpenAI Services using C#</em> section), we are going to first load the GPT-3.5 model into the kernel, followed by the GPT-4 model. This will make GPT-3.5 the default model. Later, we will specify which model will be used for which command; if we don’t, GPT-3.5 will be used. If you load GPT-4 first, you will incur more costs.</p>
			<p>We assume that you are using the OpenAI service instead of the Azure OpenAI service. You will need your OpenAI key and the organization ID, which can be found under <strong class="bold">Settings</strong> in the left menu of <a href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a>. All examples work with Azure OpenAI; you just need to use the Azure connection information instead of the OpenAI connection information.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Connecting to OpenAI Services using Python</h2>
			<p>This section assumes that you are using the OpenAI service. Before connecting to the OpenAI <a id="_idIndexMarker051"/>service, create a .env file in the ch1 directory containing your OpenAI key and your OpenAI <a id="_idIndexMarker052"/>organization ID. The organization ID can be found under Settings in the left menu of <a href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a>.</p>
			<p>Your .env file should look like this, with the appropriate values replacing x in the following example:</p>
			<pre class="source-code">
OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
OPENAI_ORG_ID="org-xxxxxxxxxxxxxxxxxxxxxxxx"</pre>			<p>To connect to the OpenAI service using Python, perform the following steps:</p>
			<ol>
				<li>Load an empty kernel:<pre class="source-code">
import semantic_kernel as sk
kernel = sk.Kernel()</pre></li>				<li>Load your API key and Organization ID into variables using the <code>openai_settings_from_dot_env</code> method from the <code>semantic_kernel_utils.settings</code> package:<pre class="source-code">
from semantic_kernel.utils.settings import openai_settings_from_dot_env
api_key, org_id = openai_settings_from_dot_env()</pre></li>				<li>Use the <code>OpenAIChatCompletion</code> method to create connections to chat services:<pre class="source-code">
from semantic_kernel.connectors.ai.open_ai import OpenAIChatCompletion
gpt35 = OpenAIChatCompletion("gpt-3.5-turbo", api_key, org_id, service_id = "gpt35")
gpt4 = OpenAIChatCompletion("gpt-4", api_key, org_id, service_id = "gpt4")
kernel.add_service(gpt35)
kernel.add_service(gpt4)</pre><p class="list-inset">If <a id="_idIndexMarker053"/>you are <a id="_idIndexMarker054"/>using OpenAI through Azure, instead of using <code>OpenAIChatCompletion</code>, you need to use <code>AzureOpenAIChatCompletion</code>, as shown here:</p><pre class="source-code">kernel.add_service(
    AzureChatCompletion(
        service_id=service_id,
        deployment_name=deployment_name,
        endpoint=endpoint,
        api_key=api_key,
    ),
)</pre></li>			</ol>
			<p>Your Semantic Kernel is now ready to make calls, which we will do in the <em class="italic">Running a simple </em><em class="italic">prompt</em> section.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Connecting to OpenAI services using C#</h2>
			<p>Before <a id="_idIndexMarker055"/>connecting to the OpenAI service, create a <code>config.json</code> file in the <code>ch1/config</code> directory, containing your OpenAI key and your OpenAI organization ID.</p>
			<p>To <a id="_idIndexMarker056"/>avoid keeping a key in your code, we will load your keys from a configuration file. Your <code>config/settings.json</code> file should look like the following example, with the appropriate values in the <code>apiKey</code> and <code>orgId</code> fields (<code>orgId</code> is optional. If you don’t have <code>orgId</code>, delete the field. An empty string will not work):</p>
			<pre class="source-code">
{
    "apiKey": "... your API key here ...",
    "orgId": "... your Organization ID here ..."
}</pre>			<p>To <a id="_idIndexMarker057"/>connect to the OpenAI service in C#, perform the following steps:</p>
			<ol>
				<li>Since <a id="_idIndexMarker058"/>we’re going to reuse the API Key and Organization ID a lot, create a class to load them in <code>Settings.cs</code>:<pre class="source-code">
using System.Text.Json;
public static class Settings {
  public static (string apiKey, string? orgId)
        LoadFromFile(string configFile = "config/settings.json")
    {
        if (!File.Exists(configFile))
        {
            Console.WriteLine("Configuration not found: " + configFile);
            throw new Exception("Configuration not found");
        }
        try
        {
            var config = JsonSerializer.Deserialize&lt;Dictionary&lt;string, string&gt;&gt;(File.ReadAllText(configFile));
            // check whether config is null
            if (config == null)
            {
                Console.WriteLine("Configuration is null");
                throw new Exception("Configuration is null");
            }
            string apiKey = config["apiKey"];
            string? orgId;
            // check whether orgId is in the file
            if (!config.ContainsKey("orgId"))
            {
                orgId = null;
            }
            else
            {
                orgId = config["orgId"];
            }
            return (apiKey, orgId);
        }
        catch (Exception e)
        {
            Console.WriteLine("Something went wrong: " + e.Message);
            return ("", "");
        }
    }
}</pre><p class="list-inset">The preceding code is boilerplate code to read a JSON file and load its attributes into C# variables. We’re looking for two attributes: <code>apiKey</code> and <code>orgID</code>.</p></li>				<li>Load the <a id="_idIndexMarker059"/>settings from <code>config/settings.json</code>. We’re going to create a class that <a id="_idIndexMarker060"/>will make this easier, as we are going to be doing this a lot. The class is very simple. It first checks whether the configuration file exists, and if it does, the class uses the JSON deserializer to load its contents into the <code>apiKey</code> and <code>orgId</code> variables:<pre class="source-code">
using Microsoft.SemanticKernel;
var (apiKey, orgId) = Settings.LoadFromFile();</pre></li>				<li>Next, use the <code>OpenAIChatCompletion</code> method to create connections to chat services. Note that we’re using <code>serviceID</code> to have a shortcut name for the model.<p class="list-inset">After you load the components, build the kernel with the <code>Build</code> method:</p><pre class="source-code">
Kernel kernel = Kernel.CreateBuilder()
        .AddOpenAIChatCompletion("gpt-3.5-turbo", apiKey, orgId, serviceId: "gpt3")
        .AddOpenAIChatCompletion("gpt-4", apiKey, orgId, serviceId: "gpt4")
                        .Build();</pre><p class="list-inset">If <a id="_idIndexMarker061"/>you are <a id="_idIndexMarker062"/>using OpenAI through Azure, instead of using <code>AddOpenAIChatCompletion</code>, you need to use <code>AddAzureOpenAIChatCompletion</code>, as shown here:</p><pre class="source-code">Kernel kernel = Kernel.CreateBuilder()
                      .AddAzureOpenAIChatCompletion(modelId, endpoint, apiKey)
                      .Build();</pre></li>			</ol>
			<p>Your Semantic Kernel is now ready to make calls, which we will do in the next section.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Running a simple prompt</h1>
			<p>This section assumes you completed the prior sections and builds upon the same code. By now, you should <a id="_idIndexMarker063"/>have instantiated Semantic Kernel and loaded both the GPT-3.5 and the GPT-4 services into it in that order. When you submit a prompt, it will default to the first service, and will run the prompt on GPT-3.5.</p>
			<p>When we <a id="_idIndexMarker064"/>send the prompt to the service, we will also send a parameter called <code>0.0</code> to <code>1.0</code>, and it controls how random the responses are. We’re going to explain the temperature parameter in more detail in later chapters. A temperature parameter of <code>0.8</code> generates a more creative response, while a temperature parameter of <code>0.2</code> generates a more precise response.</p>
			<p>To send the prompt to the service, we will use a method called <code>create_semantic_function</code>. For now, don’t worry about what a semantic function is. We’re going to explain it in the <em class="italic">Using generative AI to solve simple </em><em class="italic">problems</em> section.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Running a simple prompt in Python</h2>
			<p>To run a <a id="_idIndexMarker065"/>prompt in Python, follow these steps:</p>
			<ol>
				<li>Load the prompt in a string variable:<pre class="source-code">
prompt = "Finish the following knock-knock joke. Knock, knock. Who's there? Dishes. Dishes who?"</pre></li>				<li>Create a <a id="_idIndexMarker066"/>function by using the <code>add_function</code> method of the kernel. The <code>function_name</code> and <code>plugin_name</code> parameters are required but are not used, so you can give your function and plugin whatever name you want:<pre class="source-code">
prompt_function = kernel.add_function(function_name="ex01", plugin_name="sample", prompt=prompt)</pre></li>				<li>Call the function. Note that all invocation methods are asynchronous, so you need to use <code>await</code> to wait for their return:<pre class="source-code">
response = await kernel.invoke(prompt_function, request=prompt)</pre></li>				<li>Print the response:<pre class="source-code">
print(response)</pre></li>			</ol>
			<p>The response is nondeterministic. Here’s a possible response:</p>
			<pre class="console">
Dishes the police, open up!</pre>			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Running a simple prompt in C#</h2>
			<p>To run <a id="_idIndexMarker067"/>a prompt in C#, follow these steps:</p>
			<ol>
				<li>Load the <a id="_idIndexMarker068"/>prompt in a string variable:<pre class="source-code">
string prompt = "Finish the following knock-knock joke. Knock, knock. Who's there? Dishes. Dishes who?";</pre></li>				<li>Call the prompt by using the <code>Kernel.InvokePromptAsync</code> function:<pre class="source-code">
var joke = await kernel.InvokePromptAsync(prompt);</pre></li>				<li>Print the response:<pre class="source-code">
Console.Write(joke)</pre></li>			</ol>
			<p>The response <a id="_idIndexMarker069"/>is nondeterministic. Here’s a possible response:</p>
			<pre class="console">
Dishes a very bad joke, but I couldn't resist!</pre>			<p>We have now <a id="_idIndexMarker070"/>connected to an AI service, submitted a prompt to it, and obtained a response. We’re now ready to start creating our own functions.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Using generative AI to solve simple problems</h1>
			<p>Microsoft Semantic Kernel distinguishes between two types of functions that can be loaded into it: <strong class="bold">semantic functions</strong> and <strong class="bold">native functions</strong>.</p>
			<p>Semantic <a id="_idIndexMarker071"/>functions are functions that connect to AI services, usually LLMs, to perform a task. The service is not part of your codebase. Native functions are <a id="_idIndexMarker072"/>regular functions written in the language of your application.</p>
			<p>The reason <a id="_idIndexMarker073"/>to differentiate a native function from any other regular function in your code is that the native function will have additional attributes that will tell the kernel what it does. When you load a native function into the kernel, you can use it in chains that combine native and semantic functions. In addition, Semantic Kernel planner can use the function when creating a plan to achieve a user goal.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Creating semantic functions</h2>
			<p>We have <a id="_idIndexMarker074"/>already created a semantic function (<code>knock</code>) in the previous section. Now, we’re going to add a parameter to it. The default parameter for all semantic functions is called <code>{{$input}}</code>.</p>
			<h3>Modified semantic function in Python</h3>
			<p>We’re going to make minor modifications to our previous code to allow the semantic function to <a id="_idIndexMarker075"/>receive a parameter. Again, the <a id="_idIndexMarker076"/>following code assumes that you have already instantiated a kernel and connected to at least one service:</p>
			<pre class="source-code">
    from semantic_kernel.functions.kernel_arguments import KernelArguments
    args = KernelArguments(input="Boo")
    response = await kernel.invoke(prompt_function, request=prompt, arguments=args)
    print(response)</pre>			<p>The only differences from the code before are that now we have a variable, <code>{{$input}}</code>, and we’re calling the function using a parameter, the string <code>"Boo"</code>. To add the variable, we need to import the <code>KernelArguments</code> class from the <code>semantic_kernel_functions.kernel_arguments</code> package and create an instance of the object with the value we want.</p>
			<p>The answer is nondeterministic. Here’s a possible answer:</p>
			<pre class="console">
Don't cry, it's just a joke!</pre>			<h3>Modified semantic function in C#</h3>
			<p>To <a id="_idIndexMarker077"/>create a function in C#, we are going to use <a id="_idIndexMarker078"/>the CreateFunctionFromPrompt kernel method, and to add a parameter, we will use the KernelArguments object:</p>
			<pre class="source-code">
string prompt = "Finish the following knock-knock joke. Knock, knock. Who's there? {{$input}}, {{$input}} who?";
KernelFunction jokeFunction = kernel.CreateFunctionFromPrompt(prompt);
var arguments = new KernelArguments() { ["input"] = "Boo" };
var joke = await kernel.InvokeAsync(jokeFunction, arguments);
Console.WriteLine(joke);</pre>			<p>Here, too, the <a id="_idIndexMarker079"/>only differences from the code before <a id="_idIndexMarker080"/>are that now we have a variable, <code>{{$input}}</code>, and we’re calling the function using a parameter, the string <code>"Boo"</code>.</p>
			<p>The answer is nondeterministic. Here’s a possible answer:</p>
			<pre class="console">
Don't cry, it's just a joke!</pre>			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Creating native functions</h2>
			<p>Native functions are created in the same language your application is using. For example, if you are writing code in Python, a native function can be written in Python.</p>
			<p>Although you <a id="_idIndexMarker081"/>can call a native function directly without loading it into the kernel, loading makes it available to the planner, which we will see in the last section of this chapter.</p>
			<p>We’re going to explore native functions in greater detail in <a href="B21826_03.xhtml#_idTextAnchor071"><em class="italic">Chapter 3</em></a>, but for now, let’s create and load a simple native function in the Kernel.</p>
			<p>The native function we’re going to create chooses a theme for a joke. For now, the themes are <code>Boo</code>, <code>Dishes</code>, <code>Art</code>, <code>Needle</code>, <code>Tank</code>, and <code>Police</code>, and the function simply returns one of these themes at random.</p>
			<h3>Creating a native function in Python</h3>
			<p>In Python, the native <a id="_idIndexMarker082"/>functions need to be inside <a id="_idIndexMarker083"/>a class. The class used to be called a <strong class="bold">skill</strong>, and in some <a id="_idIndexMarker084"/>places, this name is still used. The name has recently <a id="_idIndexMarker085"/>changed to <strong class="bold">plugin</strong>. A plugin (formerly called skill) is just a collection of functions. You cannot mix native and semantic functions in the same skill.</p>
			<p>We’re going to name our class <code>ShowManager</code>.</p>
			<p>To create a native function, you will use the <code>@kernel_function</code> decorator. The decorator must contain fields for <code>description</code> and <code>name</code>. To add a decorator, you must import <code>kernel_function</code> from the <code>semantic_kernel.functions.kernel_function_decorator</code> package.</p>
			<p>The function <a id="_idIndexMarker086"/>body comes immediately after the <a id="_idIndexMarker087"/>decorator. In our case, we are simply going to have the list of themes and use the <code>random.choice</code> function to return one random element from the list:</p>
			<pre class="source-code">
import random
class ShowManager():
    @kernel_function(
    description="Randomly choose among a theme for a joke",
    name="random_theme"
  )
  def random_theme(self) -&gt; str:
      themes = ["Boo", "Dishes", "Art",
              "Needle", "Tank", "Police"]
      theme = random.choice(themes)
      return theme</pre>			<p>Then, to load the plugin and all its functions in the kernel, we use the <code>add_plugin</code> method of the kernel. When you are adding a plugin, you need to give it a name:</p>
			<pre class="source-code">
theme_choice = kernel.add_plugin(ShowManager(), "ShowManager")</pre>			<p>To call the native function from a plugin, simply put the name of the function within brackets, as shown here:</p>
			<pre class="source-code">
    response = await kernel.invoke(theme_choice["random_theme"])
    print(response)</pre>			<p>The function is not deterministic, but a possible result might be:</p>
			<pre class="console">
Tank</pre>			<h3>Creating a native function in C#</h3>
			<p>In C#, native functions need to be inside a class. The class used to be called a skill, and this name <a id="_idIndexMarker088"/>is still used in some places; for example, in the <a id="_idIndexMarker089"/>SDK, we will need to import <code>Microsoft.SemanticKernel.SkillDefinition</code>. Skills have recently been renamed to plugins. A plugin is just a collection of functions. You cannot mix native and semantic functions in the same skill.</p>
			<p>We’re going to name our class <code>ShowManager</code>.</p>
			<p>To create a native function, you will use the <code>[KernelFunction]</code> decorator. The decorator must contain <code>Description</code>. The function body comes immediately after the decorator. In our case, we are simply going to have a list of themes and use the <code>Random().Next</code> method to return one random element from the list. We will call our class <code>ShowManager</code> and our function <code>RandomTheme</code>:</p>
			<pre class="source-code">
using System.ComponentModel;
using Microsoft.SemanticKernel;
namespace Plugins;
public class ShowManager
{
    [KernelFunction, Description("Take the square root of a number")]
    public string RandomTheme()
    {
        var list = new List&lt;string&gt; { "boo", "dishes", "art", "needle", "tank", "police"};
        return list[new Random().Next(0, list.Count)];
    }
}</pre>			<p>Then, to load the plugin and all its functions into the kernel, we use the <code>ImportPluginFromObject</code> method:</p>
			<pre class="source-code">
string prompt = "Finish the following knock-knock joke. Knock, knock. Who's there? {{$input}}, {{$input}} who?";
KernelFunction jokeFunction = kernel.CreateFunctionFromPrompt(prompt);
var showManagerPlugin = kernel.ImportPluginFromObject(new Plugins.ShowManager());
var joke = await kernel.InvokeAsync(jokeFunction, arguments);
Console.WriteLine(joke);</pre>			<p>To call <a id="_idIndexMarker090"/>the native function from a plugin, simply put the <a id="_idIndexMarker091"/>function name within brackets. You can pass parameters by using the <code>KernelArguments</code> class, as shown here:</p>
			<pre class="source-code">
var result = await kernel.InvokeAsync(showManagerPlugin["RandomTheme"]);
Console.WriteLine("I will tell a joke about " + result);
var arguments = new KernelArguments() { ["input"] = result };</pre>			<p>The function is not deterministic, but a possible result might be the following:</p>
			<pre class="console">
I will tell a joke about art</pre>			<p>Now that you can run simple prompts from your code, let’s learn to separate the prompt configuration from the code that calls it by using plugins.</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Plugins</h1>
			<p>One of the greatest strengths of Microsoft Semantic Kernel is that you can create semantic plugins that <a id="_idIndexMarker092"/>are language agnostic. Semantic plugins are collections of semantic functions that can be imported into the kernel. Creating semantic plugins allows you to separate your code from the AI function, which makes your application easier to maintain. It also allows other people to work on the prompts, making it easier to implement prompt engineering, which will be explored in <a href="B21826_02.xhtml#_idTextAnchor045"><em class="italic">Chapter 2</em></a>.</p>
			<p>Each function is defined by a directory containing two text files: <code>config.json</code>, which contains the configuration for the semantic function, and <code>skprompt.txt</code>, which contains its prompt.</p>
			<p>The configuration of the semantic function includes the preferred engine to use, the temperature parameter, and a description of what the semantic function does and its inputs.</p>
			<p>The text file contains the prompt that will be sent to the AI service to generate the response.</p>
			<p>In this section, we are going to define a plugin that contains two semantic functions. The first semantic function is a familiar function: the knock-knock joke generator. The second function is a function that receives a joke as an input and tries to explain why it’s funny. Since this is a more complicated task, we’re going to use GPT-4 for this.</p>
			<pre class="source-code">
Let's take a look at the directory structure:└───plugins
    └───jokes
        |───knock_knock_joke
        |    ├───config.json
        |    └───skprompt.txt
        ├───explain_joke
             ├───config.json
             └───skprompt.txt</pre>			<p>We will now see how to create the <code>config.json</code> and <code>skprompt.txt</code> files and how to load the plugin into our program.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>The config.json file for the knock-knock joke function</h2>
			<p>The <a id="_idIndexMarker093"/>following configuration <a id="_idIndexMarker094"/>file shows a possible <a id="_idIndexMarker095"/>configuration for the semantic function that generates knock-knock jokes:</p>
			<pre class="source-code">
{
    "schema": 1,
    "type": "completion",
    "description": "Generates a knock-knock joke based on user input",
    "default_services": [
        "gpt35",
        "gpt4"
    ],
    "execution_settings": {
        "default": {
            "temperature": 0.8,
            "number_of_responses": 1,
            "top_p": 1,
            "max_tokens": 4000,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0
        }
    },
    "input_variables": [
        {
            "name": "input",
            "description": "The topic that the joke should be written about",
            "required": true
        }
    ]
}</pre>			<p>The <code>default_services</code> property is an array of the preferred engines to use (in order). Since <a id="_idIndexMarker096"/>knock-knock jokes are simple, we’re going to use GPT-3.5 for it. All the parameters in the preceding <a id="_idIndexMarker097"/>file are <a id="_idIndexMarker098"/>required. In future chapters, we will explain each parameter in detail, but for now, you should just copy them.</p>
			<p>The <code>description</code> field is important because it can be used later by the planner, which will be explained in the last section of this chapter.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>The skprompt.txt file for the knock-knock joke function</h2>
			<p>Since <a id="_idIndexMarker099"/>we want to explain <a id="_idIndexMarker100"/>the joke later, we <a id="_idIndexMarker101"/>need our application to return the whole joke, not only the punchline. This will enable us to save the whole joke and pass it as a parameter to the explain-the-joke function. To do so, we need to modify the prompt. You can see the final prompt here:</p>
			<pre class="source-code">
You are given a joke with the following setup:
Knock, knock!
Who's there?
{{$input}}!
{{$input}} who?
Repeat the whole setup and finish the joke with a funny punchline.</pre>			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>The config.json file for the semantic function that explains jokes</h2>
			<p>You <a id="_idIndexMarker102"/>should now create a file for <a id="_idIndexMarker103"/>the function that explains jokes. Since <a id="_idIndexMarker104"/>this is a more complicated task, we should set <code>default_services</code> to use GPT-4.</p>
			<p>This file is almost exactly the same as the <code>config.json</code> file used for the knock-knock joke function. We have made only three changes:</p>
			<ul>
				<li>The description</li>
				<li>The description of the <code>input</code> variable</li>
				<li>The <code>default_services</code> field</li>
			</ul>
			<p>This can be seen in the following:</p>
			<pre class="source-code">
{
    "schema": 1,
    "type": "completion",
    "description": "Given a joke, explain why it is funny",
    "default_services": [
        "gpt4"
    ],
    "execution_settings": {
        "default": {
            "temperature": 0.8,
            "number_of_responses": 1,
            "top_p": 1,
            "max_tokens": 4000,
            "presence_penalty": 0.0,
            "frequency_penalty": 0.0
        }
    },
    "input_variables": [
        {
            "name": "input",
            "description": "The joke that we want explained",
            "required": true
        }
    ]
}</pre>			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>The skprompt.txt file for the explain joke function</h2>
			<p>The <a id="_idIndexMarker105"/>prompt for the function <a id="_idIndexMarker106"/>that explains jokes is very <a id="_idIndexMarker107"/>simple:</p>
			<pre class="source-code">
You are given the following joke:
{{$input}}
First, tell the joke.
Then, explain the joke.</pre>			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Loading the plugin from a directory into the kernel</h2>
			<p>Now that the semantic functions are defined in text files, you can load them into the kernel <a id="_idIndexMarker108"/>by simply pointing to the directory where they <a id="_idIndexMarker109"/>are. This can also help you to separate the prompt engineering function from the development function. Prompt engineers can work with the text files without ever having to touch the code of your application.</p>
			<h3>Loading the plugin using Python</h3>
			<p>You can load all the functions inside a plugin directory using the <code>add_plugin</code> method from the <a id="_idIndexMarker110"/>kernel object. Just set the first parameter to <code>None</code> and set the <code>parent_directory</code> parameter to the directory where the plugin is:</p>
			<pre class="source-code">
jokes_plugin = kernel.add_plugin(None, parent_directory="../../plugins", plugin_name="jokes")</pre>			<p>You can <a id="_idIndexMarker111"/>call the functions in the same way as you would call a function from a native plugin by putting the function name within brackets:</p>
			<pre class="source-code">
knock_joke = await kernel.invoke(jokes_plugin["knock_knock_joke"], KernelArguments(input=theme))
print(knock_joke)</pre>			<p>The result of the preceding call is nondeterministic. Here’s a sample result:</p>
			<pre class="console">
Knock, knock!
Who's there?
Dishes!
Dishes who?
Dishes the police, open up, we've got some dirty plates to wash!</pre>			<p>We can pass the results of the preceding call to the <code>explain_joke</code> function:</p>
			<pre class="source-code">
explanation = await kernel.invoke(jokes_plugin["explain_joke"], KernelArguments(input=knock_joke))
print(explanation)</pre>			<p>Remember <a id="_idIndexMarker112"/>that this function is configured to use GPT-4. The <a id="_idIndexMarker113"/>results of this function are nondeterministic. Here’s a sample result:</p>
			<pre class="console">
This joke is funny because it plays off the expectation set by the traditional "knock, knock" joke format. Typically, the person responding sets up a pun or a simple joke with their question ("...who?"), but instead, the punchline in this joke is a whimsical and unexpected twist: the police are here not to arrest someone, but to wash dirty plates. This absurdity creates humor. Also, the word 'dishes' is used in a punning manner to sound like 'this is'.</pre>			<h3>Loading the plugin using C#</h3>
			<p>You can <a id="_idIndexMarker114"/>load all the functions inside a plugin directory. First, we <a id="_idIndexMarker115"/>obtain the path to the directory (your path may be different):</p>
			<pre class="source-code">
var pluginsDirectory = Path.Combine(System.IO.Directory.GetCurrentDirectory(),
        "..", "..", "..", "plugins", "jokes");</pre>			<p>Then, we use <code>ImportPluginFromPromptDirectory</code> to load the functions into a variable. The result is a collection of functions. You can access them by referencing them inside brackets:</p>
			<pre class="source-code">
var jokesPlugin = kernel.ImportPluginFromPromptDirectory(pluginsDirectory, "jokes");</pre>			<p>The last step is to call the function. To call it, we use the <code>InvokeAsync</code> method of the kernel object. We will, again, pass a parameter using the <code>KernelArguments</code> class:</p>
			<pre class="source-code">
var result = await kernel.InvokeAsync(jokesPlugin["knock_knock_joke"], new KernelArguments() {["input"] = theme.ToString()});)</pre>			<p>The result of the preceding call is nondeterministic. Here’s a sample result:</p>
			<pre class="console">
Knock, knock!
Who's there?
Dishes!
Dishes who?
Dishes the best joke you've heard in a while!</pre>			<p>To get <a id="_idIndexMarker116"/>an explanation, we can pass the results of the preceding <a id="_idIndexMarker117"/>call to the <code>explain_joke</code> function:</p>
			<pre class="source-code">
var explanation = await kernel.InvokeAsync(jokesPlugin["explain_joke"], new KernelArguments() {["input"] = result});
Console.WriteLine(explanation);</pre>			<p>Here’s a sample result:</p>
			<pre class="console">
Knock, knock!
Who's there?
Dishes!
Dishes who?
Dishes the best joke you've heard in a while!
Now, let's break down the joke:
The joke is a play on words and relies on a pun. The setup follows the classic knock, knock joke format, with the person telling the joke pretending to be at the door. In this case, they say "Dishes" when asked who's there.
Now, the pun comes into play when the second person asks "Dishes who?" Here, the word "Dishes" sounds similar to the phrase "This is." So, it can be interpreted as the person saying "This is the best joke you've heard in a while!"
The punchline subverts the expectation of a traditional knock, knock joke response, leading to a humorous twist. It plays on the double meaning of the word "Dishes" and brings humor through wordplay and cleverness.</pre>			<p>Now that <a id="_idIndexMarker118"/>you have seen how to create and call one function <a id="_idIndexMarker119"/>of a plugin, we are going to learn how to use a planner to call multiple functions from different plugins.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Using a planner to run a multistep task</h1>
			<p>Instead of calling functions yourself, you can let Microsoft Semantic Kernel choose the functions for you. This can make your code a lot simpler and can give your users the ability to <a id="_idIndexMarker120"/>combine your code in ways that you haven’t considered.</p>
			<p>Right now, this will not seem very useful because we only have a few functions and plugins. However, in a large application, such as Microsoft Office, you may have hundreds or <a id="_idIndexMarker121"/>even thousands of plugins, and your users may want to combine them in ways that you can’t yet imagine. For example, you may be creating a copilot that helps a user be more efficient when learning about a subject, so you write a function that downloads the latest news about that subject from the web. You may also have independently created a function that explains a piece of text to the user so that the user can paste content to learn more about it. The user may decide to combine them both with “<em class="italic">download the news and write an article explaining them to me</em>,” something that you never thought about and didn’t add to your code. Semantic Kernel will understand that it can call the two functions you wrote in sequence to complete that task.</p>
			<p>When you let users request their own tasks, they will use natural language, and you can let Semantic Kernel inspect all the functions that are loaded into it and use a planner to decide the best way of handling the user request.</p>
			<p>For now, we are only going to show a quick example of using a planner, but we will explore the topic in more depth in <a href="B21826_05.xhtml#_idTextAnchor106"><em class="italic">Chapter 5</em></a>. Planners are still under active development, and there might be changes over time. Currently, Semantic Kernel is expected to have two planners: the <strong class="bold">Function Calling Stepwise planner</strong>, available for Python and C#, and the <strong class="bold">Handlebars planner</strong>, available only for C# at the time of writing.</p>
			<p>Although the following example is very simple and both planners behave in the same way, we will show how to use the Stepwise planner (Function Calling Stepwise Planner) with Python and the Handlebars planner with C#.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Calling the Function Calling Stepwise planner with Python</h2>
			<p>To use <a id="_idIndexMarker122"/>the Stepwise planner, we first create an object of the <code>FunctionCallingStepwisePlanner</code> class <a id="_idIndexMarker123"/>and make a request to it. In our case, we’re going to ask it to choose a random theme, create a knock-knock joke, and explain it.</p>
			<p>We’re going to modify our earlier program, delete the function calls, and add a call to the planner instead:</p>
			<pre class="source-code">
ask = f"""Choose a random theme for a joke, generate a knock-knock joke about it and explain it"""
options = FunctionCallingStepwisePlannerOptions(
  max_iterations=10,
  max_tokens=4000)
planner = FunctionCallingStepwisePlanner(service_id="gpt4", options=options)
result = await planner.invoke(kernel, ask)
print(result.final_answer)</pre>			<p>There are a couple of details to note. The first one is that I used the class <code>FunctionCallingStepwisePlannerOptions</code> to pass a <code>max_tokens</code> parameter to the planner. Behind the scenes, the planner will create a prompt and send it to the AI service. The default <code>max_tokens</code> for most AI services tends to be small. At the time of writing, it was <code>250</code>, which may cause an error if the prompt generated by the planner is too large. The second detail to note is that I printed <code>result.final_answer</code> instead of <code>result</code>. The <code>result</code> variable contains the whole plan: the definition of the functions, the chat with the OpenAI model explaining how to proceed, etc. It’s interesting to print the <code>result</code> variable to see how the planner works internally, but to see the outcome of the planner execution, all you need to do is print <code>result.final_answer</code>.</p>
			<p>Here <a id="_idIndexMarker124"/>is a sample response, first <a id="_idIndexMarker125"/>telling the joke and then explaining it:</p>
			<pre class="console">
First, the joke:
Knock, knock!
Who's there?
Police!
Police let me in, it's cold out here!
Now, the explanation:
The humor in this joke comes from the play on words. The word "police" is being used in a different context than typically used. Instead of referring to law enforcement, it's used as a pun to sound like "Please". So, when the jokester says "Police let me in, it's cold out here!", it sounds like "Please let me in, it's cold out here!". Knock, knock jokes are a form of humor that relies on word play and puns, and this joke is a standard example of that.</pre>			<p>As you can see, the planner generated the joke and the explanation, as expected, without us needing to tell Semantic Kernel in which order to call the functions.</p>
			<h3>Calling the Handlebars planner in C#</h3>
			<p>At the <a id="_idIndexMarker126"/>time of writing, the Handlebars planner is in <a id="_idIndexMarker127"/>version 1.0.1-preview, and it’s still experimental in C#, although it’s likely that a release version will be made available soon.</p>
			<p>To use the Handlebars planner, you first need to install it, which you can do by using the following command (you should use the latest version available to you):</p>
			<pre class="console">
dotnet add package Microsoft.SemanticKernel.
s.Handlebars --version 1.0.1-preview</pre>			<p>To use the Handlebars planner, you need to use the following <code>pragma</code> warning in your code. The Handlebars planner code is still experimental, and if you don’t add the<code> #pragma</code> directive, your code will fail, with a warning that it contains experimental code. You also need to import the <code>Microsoft.SemanticKernel.Planning.Handlebars</code> package:</p>
			<pre class="source-code">
#pragma warning disable SKEXP0060
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Planning.Handlebars;</pre>			<p>We <a id="_idIndexMarker128"/>proceed as usual, instantiating our kernel and adding <a id="_idIndexMarker129"/>native and semantic functions to it:</p>
			<pre class="source-code">
var (apiKey, orgId) = Settings.LoadFromFile();
Kernel kernel = Kernel.CreateBuilder()
        .AddOpenAIChatCompletion("gpt-3.5-turbo", apiKey, orgId, serviceId: "gpt3")
        .AddOpenAIChatCompletion("gpt-4", apiKey, orgId, serviceId: "gpt4")
                        .Build();
var pluginsDirectory = Path.Combine(System.IO.Directory.GetCurrentDirectory(),
        "..", "..", "..", "plugins", "jokes");</pre>			<p>The big difference happens now – instead of telling which functions to call and how, we simply ask the planner to do what we want:</p>
			<pre class="source-code">
var goalFromUser = "Choose a random theme for a joke, generate a knock-knock joke about it and explain it";
var planner = new HandlebarsPlanner
(new HandlebarsPlannerOptions() { AllowLoops = false });
var plan = await
planner.CreatePlanAsync(kernel, goalFromUser);</pre>			<p>We can execute the plan by calling <code>InvokeAsync</code> from the <code>plan</code> object:</p>
			<pre class="source-code">
var result = await plan.InvokeAsync(kernel);
Console.WriteLine(result);</pre>			<p>The result <a id="_idIndexMarker130"/>is nondeterministic. Here is a sample result, first <a id="_idIndexMarker131"/>telling the joke and then explaining it:</p>
			<pre class="console">
Knock, knock!
Who's there?
Police!
Police who?
Police let me know if you find my sense of humor arresting!
Explanation:
This joke is a play on words and relies on the double meaning of the word "police."
In the setup, the person telling the joke says "Knock, knock!" which is a common way to begin a joke. The other person asks "Who's there?" which is the expected response.
The person telling the joke then says "Police!" as the punchline, which is a word that sounds like "please." So it seems as if they are saying "Please who?" instead of "Police who?"
Finally, the person telling the joke completes the punchline by saying "Police let me know if you find my sense of humor arresting!" This is a play on words because "arresting" can mean two things: first, it can mean being taken into custody by the police, and second, it can mean captivating or funny. So the person is asking if the listener finds their sense of humor funny or engaging and is also using the word "police" to continue the play on words.</pre>			<p>As you <a id="_idIndexMarker132"/>can see, the planner generated the joke and the <a id="_idIndexMarker133"/>explanation, as expected, without us needing to tell Semantic Kernel in which order to call the functions.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Summary</h1>
			<p>In this chapter, you learned about Generative AI and the main components of Microsoft Semantic Kernel. You learned how to create a prompt and submit it to a service and how to embed that prompt into a semantic function. You also learned how to execute multistep requests by using a planner.</p>
			<p>In the next chapter, we are going to learn how to make our prompts better through a topic called <strong class="bold">prompt engineering</strong>. This will help you create prompts that get your users the correct result faster and use fewer tokens, therefore reducing costs.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>References</h1>
			<p class="Bibliography">[1] A. Vaswani et al., “Attention Is All You Need,” Jun. 2017.</p>
			<p class="Bibliography">[2] OpenAI, “GPT-4 Technical Report.” arXiv, Mar. 27, 2023. doi: 10.48550/arXiv.2303.08774.</p>
			<p class="Bibliography">[3] L. Chen, M. Zaharia, and J. Zou, “FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance.” arXiv, May 09, 2023. doi: 10.48550/arXiv.2305.05176.</p>
		</div>
	</div>
</div>
</body></html>