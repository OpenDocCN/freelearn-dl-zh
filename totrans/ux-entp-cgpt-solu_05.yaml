- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scoring Stories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the secrets of successful development efforts is to work on the proper
    work. The agile method helps break down effort into manageable pieces, but some
    concepts, like Estimating Poker (we will explain), focus on the *cost* of doing
    the work, not the *value*. We want to share a method to systematically break down
    stories so they can be prioritized based on the value to the customer. ChatGPT
    solutions require a lot of decision-making. We need to prioritize those decisions.
    Adding or editing knowledge, doing more testing, adding a new integration, changing
    the models, improving fine-tuning, or refining prompts must all be prioritized.
    It might seem obvious, but work on the work that provides the most value as soon
    as possible. Items that hold little value or benefit a small subset of customers
    could wait. We have a way of putting this backlog of work into an order the team
    can understand.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss the concept of *User Needs Scoring* in-depth and tie it to the
    development cost. That will set up [*Chapter 5*](B21964_05_split_000.xhtml#_idTextAnchor108),
    Define the Desired (User) Experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Prioritizing the backlog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating more complex scoring methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world hiccups with scoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritizing the backlog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, some context and history. A **backlog** means the collection of known
    work to be evaluated by the team for development. The closer an item on the backlog
    is to being worked, the more detail is needed to understand and scope it. This
    is why we have different approaches to managing items on this list of work. Sometimes,
    a large team will have many sprint teams, each managing its backlog of work. However,
    since all teams are marching toward the same goals, it is essential to understand
    that everyone is working on the backlog with the most value. We will be better
    off having a repeatable and consistent understanding of what work to do first.
    **Weighted shortest job first** (**WSJF**) is one of these ways.
  prefs: []
  type: TYPE_NORMAL
- en: WSJF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The agile concept of WSJF helps one decide which job/task/project/feature/content
    should be given resources to complete first.
  prefs: []
  type: TYPE_NORMAL
- en: We learn that the most critical tasks don’t always come first because their
    value depends on the development cost. Sometimes, doing something different on
    the backlog can provide better value for the development investment (in the USA,
    we call this getting a *good bang for your buck*—a buck is another term for a
    US dollar).
  prefs: []
  type: TYPE_NORMAL
- en: I had a great Agile teacher who explained this the best and will try to do it
    justice. WSJF prioritizes work based on the **cost of delay** (**CoD**) *to the
    customer*. Customers don’t get any value from features that don’t ship. Shipping
    the most valuable features sooner minimizes the CoD. However, for two features
    of equal value, deliver the one with the least cost first. Why? Because the customer
    gets the value from the product sooner rather than waiting for the more costly
    feature.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine *Figure 4**.1*, from the **Scaled Agile** **Framework** (**SAFe**).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Explaining the CoD
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [WSJF](https://scaledagileframework.com/wsjf/) ([https://scaledagileframework.com/wsjf/](https://scaledagileframework.com/wsjf/))'
  prefs: []
  type: TYPE_NORMAL
- en: In the SAFe, this example is based on item C having a small CoD. However, because
    of its long time to complete it, its development blocks getting value from delivering
    B and A, represented in the *Low WSJF First* graphic with their `1.0` and `0.1`
    WSJF values. Consider that CoD is an unrealized value that can’t be recovered.
    Instead, take the *High WSJF First* example, where A is done, B, and then C. Now,
    because A contains a high CoD and is first, there isn’t as much remaining CoD.
    Customers get value sooner. Less value is wasted, like in the *Low WSJF* example,
    where we delivered C first.
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows for comparing features in the backlog needed by the generative
    AI product. Let’s try an example with two features: password reset support and
    ordering a replacement part. Suppose they are of equal value to the customer (and
    we will go deep into how to calculate their value later). Which one should be
    done first?'
  prefs: []
  type: TYPE_NORMAL
- en: Password reset takes 45 days to develop, while the replacement part feature
    takes 90 days. The team can only work on one feature due to staffing. If the password
    reset feature ships, customers get 45 days of value before the replacement part
    feature would have shipped. They get value from this feature starting on day 46.After
    135 days and the completion of both features, the customer enjoys 90 days of value
    from the reset feature. Had they completed the replacement part feature first,
    they would have only received 45 days of value from both features combined. By
    changing the order of development, the customer effectively doubled the value.
  prefs: []
  type: TYPE_NORMAL
- en: I find it challenging to wrap my head around negative words in the CoD. The
    CoD happens because the work is holding off delivery to the customer and can’t
    provide value to the customers until it ships. Doing this effectively assumes
    a cost of development estimate. We can all agree that estimating is hard. All
    development teams know this. The relative cost *is reasonable* to compare with
    consistent costing across features. We will make estimation errors. For Agile
    Scrum teams, use the retrospective at the end of each sprint to continually improve
    estimating costs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The CoD is focused on the cost to a consumer. The size of the CoD is a function
    of the delay in the customer getting value. Delaying a more valuable job to deliver
    a less valuable job incurs a more significant CoD.
  prefs: []
  type: TYPE_NORMAL
- en: '*CoD is the money lost by delaying or not* *doing a job for a specific time.
    It’s a measure of the economic value of a job over time.* – SAFe website'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.2* from the Scaled Agile website shows that the CoD model has some
    user-centric elements.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – CoD
  prefs: []
  type: TYPE_NORMAL
- en: After talking with the Scaled Agile team, we adopted a different approach that
    can offer similar value. Implementing a repeatable, trainable process that provides
    consistent results is the most important thing to consider. A **rubric**, a set
    of criteria, and descriptions create a consistent method for judgment. This makes
    it easier for people outside of the team to understand and easier to appreciate
    than the traditional CoD.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a wealth of details on WSJF, visit the Scaled Agile website:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Weighted Shortest Job First](https://scaledagileframework.com/wsjf/)
    ([https://scaledagileframework.com/wsjf/](https://scaledagileframework.com/wsjf/))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reinertsen’s book is genius for improving development processes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Book: [The Principles of Product Development Flow](https://amzn.to/3A3u34C)
    by Donald Reinertsen ([https://amzn.to/3A3u34C](https://amzn.to/3A3u34C))'
  prefs: []
  type: TYPE_NORMAL
- en: The book is precious for those who want to understand development processes
    beyond WSJF. In enterprise development, the SAFe is a well-thought-out, robust,
    and mature model to support Agile across large organizations. I strongly encourage
    exploration and deep dives into what is available.
  prefs: []
  type: TYPE_NORMAL
- en: With the basics of WSJF, go away knowing there is value in delivering equivalency
    in a shorter period. However, not all features are equal. We propose a new method
    to calculate a user-centric version of the CoD. It is called the **User Needs**
    **Score** (**UNS**).
  prefs: []
  type: TYPE_NORMAL
- en: User Needs Scoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The UNS is a user-centric approach to the CoD, accounting for the scope, frequency,
    and severity of a problem or feature.
  prefs: []
  type: TYPE_NORMAL
- en: UNS is based on the fine work uncovered in Bruce Tognazzini’s book *Tog on Design*
    (1992), which Phil Haine shared with me while working as a consultant. I then
    took this work and adapted it into the numerator for the WSJF calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Tog’s book referenced a study by Robin Jeffries et al. that evaluated different
    techniques for finding usability problems, including heuristic evaluation and
    guidelines, which we will cover in [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190),
    *Guidelines and Heuristics* usability testing ([*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031),
    *Conducting Effective User Research*), and cognitive walkthrough (a solid technique).
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [User Interface Evaluation in the Real World: Comparison of Four Techniques](https://dl.acm.org/doi/pdf/10.1145/108844.108862)
    by Robin Jeffries et. al. ([https://dl.acm.org/doi/pdf/10.1145/108844.108862](https://dl.acm.org/doi/pdf/10.1145/108844.108862))'
  prefs: []
  type: TYPE_NORMAL
- en: In Jeffries’s study, the participants were asked to analyze issues to “take
    into account the impact of the problem, the frequency with which it would be encountered,
    and the relative number of users that would be affected.” They used a metric ranging
    from 1 (trivial) to 9 (critical). We will build on this concept with a more repeatable
    way to score.
  prefs: []
  type: TYPE_NORMAL
- en: The official CoD from SAFe is user-business value + time criticality + risk
    reduction and/or opportunity engagement, as shown in *Figure 4**.2*. The elements
    of the CoD make sense; do work benefiting customers (user-business value), have
    value in delivering sooner rather than later (time criticality), and help change
    the playing field by reducing risk.
  prefs: []
  type: TYPE_NORMAL
- en: We need a consistent and repeatable metric that non-experts can easily explain
    and repeat. The approach is to follow three similar metrics and provide rubrics
    that can be applied consistently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the User Needs Scoring model. This score will be for the feature/bug
    fix/enhancement and will be the numerator in our WSJF calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scope**: How many users does it impact? (3 – All, 2 – Some, 1 – A few or
    a limited role)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequency**: How often is it used? (3 – Always, 2 – Sometimes, 1 – Infrequently)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Severity**: How bad is the problem? (4 – Severe, 3 – Critical, 2 – Important,
    1 – Minor Importance)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then model the CoD as *scope* times *frequency* times *severity*. This means
    the values range from 1 (1*1*1) to 36 (3*3*4). We will explain each value and
    how to score with many examples so anyone can apply this model.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring the user value and dividing it by the cost brings both sides of the
    discussion into play. Scores can be compared between teams, and in SAFe, they
    can be used to get stories that other teams need to complete into a fair and manageable
    order (or at least to have the discussion). Whatever is *most important* is scored
    highest in WSJF and should be done first. We do this ordering by scoring each
    item. This is more repeatable than just *moving things around* in an Agile tracking
    tool until it looks right.
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>W</mi><mi>S</mi><mi>F</mi><mi>J</mi><mo>=</mo><mfrac><mrow><mi>S</mi><mi>c</mi><mi>o</mi><mi>p</mi><mi>e</mi><mfenced
    open="(" close=")"><mrow><mn>1</mn><mi>t</mi><mi>o</mi><mn>3</mn></mrow></mfenced><mi
    mathvariant="normal">*</mi><mi>F</mi><mi>r</mi><mi>e</mi><mi>q</mi><mi>u</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>y</mi><mfenced
    open="(" close=")"><mrow><mn>1</mn><mi>t</mi><mi>o</mi><mn>3</mn></mrow></mfenced><mi
    mathvariant="normal">*</mi><mi>S</mi><mi>e</mi><mi>v</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>t</mi><mi>y</mi><mo>(</mo><mn>1</mn><mi>t</mi><mi>o</mi><mn>4</mn><mo>)</mo></mrow><mrow><mi>J</mi><mi>o</mi><mi>b</mi><mi>D</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac></mrow></mrow></math>](img/2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>W</mi><mi>S</mi><mi>F</mi><mi>J</mi><mo>=</mo><mfrac><mrow><mi>U</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>e</mi><mi>e</mi><mi>d</mi><mi>s</mi><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi></mrow><mrow><mi>J</mi><mi>o</mi><mi>b</mi><mi>D</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac></mrow></mrow></math>](img/3.png)'
  prefs: []
  type: TYPE_IMG
- en: We can adopt an approach that Agile teams use to evaluate these UNS estimates,
    which might vary between team members. In Agile, there is a concept called **Estimating
    Poker**. The idea is to independently calculate the cost of a feature and then
    discuss why the values were selected when they are different. This helps uncover
    unknown elements that a member might not have considered. We can apply the same
    approach to estimating user needs and continue to do this for development costs.
  prefs: []
  type: TYPE_NORMAL
- en: In Estimating Poker, team members can collaboratively review and discuss the
    UNS, and we expose gaps in our understanding and expectations. Two to three people
    must compare estimates to judge the UNS. This will teach the team how to do this
    scoring. One person can score alone but will miss the value of other people’s
    perspectives. A simple scoring method on a one to three scale makes it more likely
    to agree. However, the poker method is there to help communicate and clarify differences
    and come to a consensus when disagreements occur. The score will also help decide
    whether a bug should be fixed before adding a new feature. Going through many
    examples will make this method more straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating Poker (also called Planning Poker) appears two-thirds of the way
    into this article. Please read it to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Estimating Poker within the story article](https://scaledagileframework.com/story)
    ([https://scaledagileframework.com/story](https://scaledagileframework.com/story)/)'
  prefs: []
  type: TYPE_NORMAL
- en: Scoring enterprise solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to learn how to create a scoring method is to try doing the scoring
    method. We provide the rules, show examples from typical enterprise applications,
    and coach through how to score some tricky stories. Then, practice with your own
    stories.
  prefs: []
  type: TYPE_NORMAL
- en: How to score items
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, the *wrong way* to prioritize a backlog.
  prefs: []
  type: TYPE_NORMAL
- en: Look at a list and see which ones the product owner thinks should be done first;
    consider the ones where the developers can do it quickly. Another common mistake
    comes from the strongest-willed team member who speaks the loudest to get their
    work to rise to the top. This is what we want to avoid. An unbiased approach to
    scoring stories allows the value of the story to drive the need. The story’s value
    should speak for itself. Stories might match use cases from the previous chapter
    or be software bugs. Sometimes, use cases are broken down into multiple stories
    to complete a story in a sprint. For our discussion, the story is the piece of
    the use case we plan on delivering in a development period.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *right way* is to use an *unbiased* method to put the items in order using
    a three-step rating system. Anyone knowledgeable about the stories can generate
    a score. Hopefully, product owners, designers, engineers, or other related team
    members can do this and then discuss and resolve things that all parties haven’t
    considered. This is done by answering the following questions. We will back up
    these criteria and levels of performance with many examples and descriptions for
    each score:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scope**: How many users does it impact? Answer with a score from 3 to 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Frequency**: How often is it used? Answer with a score from 3 to 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Severity**: How bad is the problem? Answer with a score from 4 to 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate each item’s score by multiplying them (e.g., 3*2*2 = 12).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Order stories by the score. As new stories come in, score them. They will naturally
    fall into the correct order of user value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Development estimates the cost of these stories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the score by that estimated cost. Generally, work down from the most
    significant user needs scoring items.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If something costs too much, it is a candidate to break down into smaller stories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to score consistently
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By learning some simple rules, independent people with a shared understanding
    of the scores should be able to score the same item similarly. However, differences
    will exist. Let someone else score the same story, and if they come up with a
    different score, discuss why. Reaching a joint agreement occurs nine times out
    of ten. If there is confusion about the scope of the problem, someone is likely
    underestimating it. Scope rarely shrinks over time, so go conservative. Use the
    larger value. We will drill down into the rubrics for each score in the UNS. I
    have seen teams split the difference (they can’t decide on a 2 or 3, so they score
    2.5). This is an option, but this hedging approach is typically unnecessary. One
    needs to ensure the story tracking tool supports this. I developed integrations
    with Jira and Oracle’s internal bug system to use drop menus to make it easier
    to select whole numbers. However, we appreciate not wasting time, so do it when
    needed. Don’t automatically split the difference with a disagreement. Discuss
    the reasons behind the scores. We can explain the rubric behind each score.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When scoring the feature, isolating the dimension being scored is critical.
    Ignore the other two factors. Score each factor independently of the other two.
  prefs: []
  type: TYPE_NORMAL
- en: Scope – how many users does it impact?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*3 – All, 2 – Some, 1 – A few or a Limited* *User Role*'
  prefs: []
  type: TYPE_NORMAL
- en: When considering who this change is made for, think of that group in the context
    of the total number of customers for the product. It cannot be based only on the
    people using the specific feature. *Everyone* signs into a secure portal to access
    enterprise ChatGPT or other services– so rating problems with sign-in get a score
    of 3\. At the same time, only a few people customize their home page. One could
    rate that a 1\. Don’t change the scope and say, “Well, *within* the people who
    customize, *all* of them will use this feature.” It doesn’t work that way. Scores
    need to be measured against other stories and across different teams. The goal
    is to determine where the most good would come from by applying resources, so
    cheating doesn’t help the product. Focus only on the number of users impacted.
    Ignore Severity and how bad the problem is at this step.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, being unable to sign in is different than a visual distraction such
    as the words “sign in” wrapping oddly on a mobile phone. We will handle that in
    the last user metric.
  prefs: []
  type: TYPE_NORMAL
- en: The enterprise solution might be in a chat interface; parts might be conversational
    results in hybrid UIs or data rendered from a ChatGPT backend. No matter the form,
    each can impact a few or many users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are Scope examples that score as 3s:'
  prefs: []
  type: TYPE_NORMAL
- en: Common questions and answers (because “everyone” does this)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sign in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues on the landing page that prevent sign-in or registration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up an account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very slow to respond in almost all cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing the primary use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are Scope examples that can score as 2s:'
  prefs: []
  type: TYPE_NORMAL
- en: Frequent questions and answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Registration for the service, portal, or website
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing the user’s profile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow to respond in some conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to export chat logs (the score depends on the use case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing secondary use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some Scope examples of 1s:'
  prefs: []
  type: TYPE_NORMAL
- en: Uncommon questions and answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding an avatar to their profile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to export chat logs (the score depends on the use case)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow to respond in a particular condition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A missing help link in a feature used by a small audience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing supporting use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to be flexible on *everyone*, like in *Figure 4**.3*. Think of it as
    almost everyone, or about 80% or more, 2 representing the next 15%, and 1 would
    be 5% or less of users.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – All users are 80% or greater, while Some and A Few are in the tail
  prefs: []
  type: TYPE_NORMAL
- en: Discuss and debate, but work to score consistently. Sometimes, the term everyone
    means very close to everyone, such as customers who sign into online banking.
    However, we know some older adults might avoid online banking, and a few of those
    who sign in are high-value customers. If all things are equal, there is more value
    in doing something for all users rather than for a few. Once the number of impacted
    users is understood, consider how often the issue, task, or action will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency – how often is it used
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*3 – All of the Time, 2 – Sometimes, 1 –* *Infrequently*'
  prefs: []
  type: TYPE_NORMAL
- en: If every time users come here, the problem exists, it is easy to make this a
    3\. If it only happens in a specific mode or state, then it is a 2, and those
    annoying errors that pop up rarely would be a 1\. Judge whether issues occur sometimes
    or infrequently because multiplying by one doesn’t do anything. Consider that
    intermittent errors are challenging to reproduce. If errors are noted in a log
    analysis or bugs that keep coming up, set this as a 2\. There is flexibility,
    but all the time means all of the time; use a consistent approach to judging between
    2s and 1s. If it is less than 5% to 10% of the time, it is a 1\. Don’t consider
    the importance of what is being scored. This will be judged in the third score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few Frequency examples of 3s:'
  prefs: []
  type: TYPE_NORMAL
- en: The answer to the question is always wrong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every time a dialog box is opened, it is empty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A typo or grammar error that appears all of the time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scroll bar always appears even when not needed or wanted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tone or style of the conversation needs to be corrected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chat doesn’t recall the context set in this session
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some Frequency examples of 2s:'
  prefs: []
  type: TYPE_NORMAL
- en: The answer to a question sometimes needs to be corrected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your saved name is too large, then it truncates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A time-out error appears after using the product for 10 minutes, and this is
    too early (if the time-out is usually 4 hours)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dialog box appears off-screen some of the time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In many conditions, the style or tone of the conversation needs to be corrected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The chat doesn’t recall context sometimes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some Frequency examples of 1s:'
  prefs: []
  type: TYPE_NORMAL
- en: In a specific use case, the style or tone of the conversation is wrong
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An error appears rarely, and no understanding of why. It only happened once
    in that session, and everything is working
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When saving a chat session, it errors out on rare occasions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every once in a while, when going back in the chat window, it forgets what step
    we are on and shows the wrong information that was already affirmed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When looking at logs, it might not be obvious how often something is happening.
    It would be best to look at a larger cross-section of logs or test instances to
    learn about frequency. But it is sometimes more of a judgment call for a two or
    a one because there is so little data. Remember, this is just frequency; don’t
    get freaked out because some are bad issues; we should catch that next.
  prefs: []
  type: TYPE_NORMAL
- en: Severity – how bad is the problem?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*4 – Severe, 3 – Critical, 2 – Important, 1 –* *Not Important*'
  prefs: []
  type: TYPE_NORMAL
- en: This is the easiest to score, or is it? Severity is sometimes hotly contested.
    Our values are inverted compared to most organizations’ common Bug Severity (Sev).
    Because all organizations have Severity, it would seem easy to do. But not always.
    Our score needs to use larger numbers to express more value. We can break down
    how people think about Sev and the scores we assign to each severity.
  prefs: []
  type: TYPE_NORMAL
- en: An actual **Severity 1** issue only comes up occasionally. Severity 1 means
    **service is down – unavailable**, and no workaround. We don’t often see Sev 1s
    in development because the code is not in production, and thus, the production
    system isn’t being evaluated. But from a usability perspective, if the task can’t
    be completed, that is a usability Sev 1 and thus is worth the most, so we assign
    it the largest value, a 4\. Likely, these are just bugs. An organization should
    have a list of definitions to help prioritize bugs; start with that. If not, create
    one. I had one printed out by my desk to remind me of classes of bugs and their
    values. At my previous company, we had a category just below Severity 1 called
    a **Severity 2** **Showstopper**. Because they were showstoppers, a significant
    issue that impedes progress, we would also classify these as 4\. From there, we
    go to Sev 2, worth 3 points. It represents critical issues that cause significant
    user roadblocks. Sev 3 is worth 2 points. These are problems that cause some issues
    but likely won’t cause the customer to scream and yell. Finally, Sev 4 is assigned
    1 point. These are minor issues. Most organizations have the 1 to 4 severity scale,
    so we have the 4 to 1 values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Severity is well understood in most organizations; extending the definition
    to include LLM issues and inverting the values is a good start. If the organization
    scoring model extends beyond 4, consider mapping it down. Applying a five—or six-point
    scale for Severity will dramatically increase its weight in the UNS. It’s a choice,
    but we will discuss the downside of weighting in the UNS calculation later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Severity is well understood for bugs but might not be used for criticality or
    priority of new features. For our purposes, we use one measure here. Examples
    consider the issues, stories, features, or interactions in UIs or ChatGPT-enabled
    solutions. They are just examples to further the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of Severity for 4 – Severe (the equivalent of Severity 1):'
  prefs: []
  type: TYPE_NORMAL
- en: '**American with Disabilities Act (ADA) and Web Content Accessibility Guidelines
    (WCAG) or a country’s equivalent**: Major accessibility issues, including missing
    labels, non-standard abbreviations, using only color to distinguish **UI** elements
    and chats, and recommendations that are not navigable or unreadable by a screen
    reader. Chat or recommendations are not accessible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [Web Content Accessibility Guidelines](https://en.wikipedia.org/wiki/Web_Content_Accessibility_Guidelines)
    ([https://en.wikipedia.org/wiki/Web_Content_Accessibility_Guidelines](https://en.wikipedia.org/wiki/Web_Content_Accessibility_Guidelines))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**National Language Support (NLS)**: Missing message files are causing pages
    not to render, garbled error messages, can’t translate the string, or the wrong
    language to appear in a chat or recommendation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Article: [Internationalization and Localization](https://en.wikipedia.org/wiki/Internationalization_and_localization)
    ([https://en.wikipedia.org/wiki/Internationalization_and_localization](https://en.wikipedia.org/wiki/Internationalization_and_localization))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Help doesn’t appear or leads the user down the wrong path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance issues (beyond the level of service)**: The system or product
    needs to be more responsive. Its response time is significantly worse than any
    set goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The wrong model is engaged (i.e., routed to time reporting when the prompt should
    have sent the user to expense reporting).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Company-specific language is misunderstood, initiating the wrong flow or task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user is stuck in a loop, and LLM won’t let them out.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM misunderstands channel capabilities, which causes task failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of Severity for 3 – Critical (typically a P2 bug):'
  prefs: []
  type: TYPE_NORMAL
- en: Help is wrong but doesn’t cause further errors by the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A significant percentage of users need assistance to complete the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context lost during workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typos (that impacts or changes meaning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability issues (such as a shuttle component with thousands of elements)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user isn’t prevented from making a grave mistake (and there is no undo)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple entities are not understood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Company-specific language is not understood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users have to repeat themselves in a different way to be understood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The LLM misunderstands channel capabilities, which causes task issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of Severity for 2 – Important (P3’s in the bug system):'
  prefs: []
  type: TYPE_NORMAL
- en: Help is unclear, wordy, or too complex but factually correct
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large number of users would need assistance to complete the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concepts presented to the customer are complex to understand or too technical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The layout needs to be more explicit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect page header or section label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect breadcrumbs (bump up if the user loses context)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An incorrect time format causes a user error (such as scheduling a meeting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammatical errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of a collection of entities is understood, but one is missed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The solution does not effectively use advantages for a specific channel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significant inconsistencies (such as button order) that cause user error or
    a missing unit that would confuse the user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user isn’t prevented from making a grave mistake (but undo is available)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples of Severity for 1 – Minor (P4s for a bug):'
  prefs: []
  type: TYPE_NORMAL
- en: Minor inconsistencies (button order, using Delete for Remove, OK instead of
    Continue, or missing units after numbers, but it would still be understood)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorrect usage of blank table cells versus N/A or unavailable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of correct pagination or segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overly wordy or too terse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A different layout, table, format, or UI element can improve an LLM response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all of these are as cut and dry as we might have led on. Don’t fret; decide
    what is essential to customers. We don’t expect each reader to agree with all
    of the preceding examples. Make decisions on what goes in which bucket, share
    examples with the team to reduce confusion, create a shared understanding for
    your rubric, and fix the issues based on their score.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of scoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Practice to learn the method. To score, we use the UNS values and multiply them
    together. Then, include development to cost efforts to prioritize and solve these
    issues.
  prefs: []
  type: TYPE_NORMAL
- en: Dev teams usually start with T-shirt sizing. They estimate the cost of development
    with **Extra Small** (**XS**), **Small** (**S**), **Medium** (**M**), **Large**
    (**L**), **Extra Large** (**XL**), and so on. They must do the math at some point,
    converting this to story points, a standard unit of measure for Agile development
    or development days. The Fibonacci sequence is commonly used for this estimation
    when using numbers. The numbers (1, 2, 3, 5, 8, 13, 21, 34) grow with each gap
    to convey the complexity of measuring a large project. The larger the estimate,
    the more significant the gap because more extensive efforts have more variability
    when estimating. They are commonly used for costing development. Once there are
    numbers, divide the UNS by the development cost. This is only done once this story
    is close enough to development that understanding is clear enough to cost it beyond
    T-shirt sizing. Development can use a proxy like T-shirt sizing (1-XS, 2-S, 5-M,
    8-L, 13-XL, etc.) that the team agrees on. However, do an actual costing exercise
    to understand better what is being built. *Table 4.1* contains some GUI design
    examples. These examples include conversational issues. Once these stories are
    understood, development can cost them more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Samples from a ChatGPT** **Web Experience** | **Scope** | **Frequency**
    | **Severity** | **UNS** | **Dev Cost** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **1\. The Browser Back does not** **go “Back”** | 3 | 3 | 4 | 36 | XL |'
  prefs: []
  type: TYPE_TB
- en: '| **2\. Text: “Run” should be called “Search” in** **the toolbar** | 3 | 3
    | 2 | 18 | XS |'
  prefs: []
  type: TYPE_TB
- en: '| **3\. Can’t submit a prompt by pressing return in** **a field** | 3 | 3 |
    2 | 18 | L |'
  prefs: []
  type: TYPE_TB
- en: '| **4\. The vertical scroll bar is missing from the** **Conversational UI**
    | 2 | 3 | 3 | 18 | M |'
  prefs: []
  type: TYPE_TB
- en: '| **5\. “Enterprise Support Recommendations” is a** **confusing term** | 2
    | 3 | 2 | 12 | XS |'
  prefs: []
  type: TYPE_TB
- en: '| **6\. Columnated answers need to be** **sorted correctly** | 2 | 3 | 2 |
    12 | M |'
  prefs: []
  type: TYPE_TB
- en: '| **7\. The default UI language does not match** **the website** | 2 | 3 |
    2 | 12 | M |'
  prefs: []
  type: TYPE_TB
- en: '| **8\. Text and formatted numbers should** **align correctly** | 3 | 3 | 1
    | 9 | XS |'
  prefs: []
  type: TYPE_TB
- en: '| **9\. Images don’t** **have labels** | 2 | 2 | 2 | 8 | L |'
  prefs: []
  type: TYPE_TB
- en: '| **10\. A task region cannot be dragged or moved onto** **the screen** | 1
    | 3 | 2 | 6 | M |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Scoring issues on User Needs Scoring metrics
  prefs: []
  type: TYPE_NORMAL
- en: It is challenging to get the team to decide on a costing method; this focus
    is on a value method. To explore why one uses Fibonacci numbers rather than time
    in hours or days, learn about story points. After starting with this simple explanation,
    do some searching. If you are on board with story points, skip the article.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Why do we use Story Points for Estimating?](https://www.scrum.org/resources/blog/why-do-we-use-story-points-estimating)
    ([https://www.scrum.org/resources/blog/why-do-we-use-story-points-estimating](https://www.scrum.org/resources/blog/why-do-we-use-story-points-estimating))'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a rationale behind the UNS values for each item in *Table 4.1*. Because
    you are unfamiliar with the application, context is provided with each item. We
    will relate these examples to LLM issues typical in generative AI applications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The browser’s back button does not go “back”:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: When building a web application, a process likely goes from
    one web page to another; it is natural for a user to want to occasionally return
    to the previous step. So, they click the browser’s back button. Without coding
    for this, the application fails to navigate correctly.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Because the user is *lost* and can’t return to the main UI,
    this problem is worth fixing immediately. The tech stack had an issue supporting
    back, which is why development costs are high. It means rearchitecting how the
    browser session is created, thus making it a challenge. If the cost is less than
    twice that of item number two (because its score is half as big), it would still
    be more valuable to fix item 1 before item 2\. When LLM chat is integrated into
    existing apps, all kinds of issues appear: does the conversation pick up where
    it left off when opening a new tab or window on the same site? How does the back
    button in the browser impact the conversation thread? Does the conversation get
    confused with multiple open windows when the customer interacts in multiple windows?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: “Run” should be called “Search” in the toolbar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: We typically see a search field when designing a UI to search
    a database. Terms other than “Search” can be confusing.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Unless dynamic (text that changes based on the user profile
    or other factors), text on a UI will always be visible to all users; it always
    scores a three for the number of users impacted. Everyone searches in this app.
    If the Run label is generated based on a user profile, it might appear only for
    a segment of the audience and only in some conditions. In this example, it is
    always on screen and is used all the time, so the Frequency is also a three. When
    scoring severity, if it causes the wrong interaction, it scores a three; if the
    text is just confusing, score it a two.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternative UI considerations**: If this were a conversational search from
    a chat prompt, we would train our model to understand various names for our search
    function (e.g., **Find me all phone models that have…**, **What are the phone
    models with…**, **List the phone models…**). The default models should pick up
    this nuance very quickly, but with enterprise-specific tasks, train the model
    to understand the tasks and the various ways one can ask for them. This will require
    prompt engineering and fine-tuning, and we will start to cover that in [*Chapter
    7*](B21964_07.xhtml#_idTextAnchor150), *Prompt Engineering*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Can’t submit a prompt by pressing return in a field:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: There is a similar issue in the ChatGPT playground. They decided
    that the *return* key on the keyboard should be used to create a carriage return
    in the text entry field. To submit the text, press *command* + *return* (on a
    Mac), as shown in *Figure 4.4*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21964_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Example of using an unnatural keyboard entry to submit
  prefs: []
  type: TYPE_NORMAL
- en: '**Reasoning**: We understand how folks naturally press return on the keyboard
    on mobile or desktop to submit a form. They must stop and deal with the onscreen
    button press when this doesn’t happen. In *Figure 4.4* from ChatGPT, they have
    to press two keyboard buttons together. That is annoying and not natural. Hence,
    it is a two on the Severity scale. Users will likely *never* get used to this
    interaction because it is unlike the 99% of interactions they already do. An OpenAI
    interaction designer likely lost this battle, but they did get to include the
    *command* + *return* command in the button label. A hack at best, but at least
    it is a visible reminder. This is an example of a visual affordance covered in
    [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190), *Guidelines* *and Heuristics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The vertical scroll bar is missing from the conversational UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: If there is scrolling text and the scroll bar is missing, the
    user will have trouble getting to their results.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: In this case, the scroll bar was missing. It wasn’t just hidden
    because of an operating system setting, but it didn’t happen to all users (score
    of 2). It only occurs when there is a lot of text and not all answers are lengthy.
    However, if the user wants to read the whole answer, the scroll bar not appearing
    or not being supported is a big issue (scores a 3). This case has some workarounds,
    but we can still see high scores for this critical issue (Severity score of 3).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Enterprise Support Recommendations* is a term that needs to be clarified:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: If terms are unfamiliar to the user base, they might miss important
    information about their task.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Customer feedback suggests that this term needs to be clarified.
    We understand that the results could be written more user-centric and less technical.
    Some people will understand this term, so it only rates a two. It will always
    be used; everyone will see it, so it is a three on the *Frequency* scale. But
    there is help, and the term includes keywords such as *support* and *recommendation*,
    so the additional word “enterprise” doesn’t add any value; it is not critical.
    As expected, it is not hard to edit a word, so the cost is XS. Conversational
    style, tone, and language will likely cause issues. Review how knowledge and help
    is written and work to address how an LLM responds using prompt engineering.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Columnated results need to be sorted correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: The logical order for a list should be the default. If multiple
    logical orders exist, user control should be allowed, and customer history could
    be used to decide the default.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: A challenge with data in documents and tables is preserving
    table-like features. With this, some information is maintained and clear. For
    example, showing the country’s top 25 college football teams alphabetically isn’t
    helpful. They should be sorted by their current college rank. The table should
    include the calendar date for the last ranking (so old data is apparent). However,
    only some users ask for this kind of data, and if the table is sortable, then
    there is an easy workaround to ask for the data to be sorted. In this example,
    we don’t expect sorting for the top 25 teams in football. In [*Chapter 5*](B21964_05_split_000.xhtml#_idTextAnchor108),
    *Defining the Desired Experience*, we will go into detail about the use of tables
    in small chat windows. The real estate for a table is sometimes tight at best.
    A table acceptable on a desktop web experience won’t work on a mobile chat experience.
    In this case, not all users were impacted by the table (a score of 2), but it
    was always an issue because it was affixed with a poor sort choice (a score of
    3). It wasn’t a big issue because of the kind of data (score of 2). Critical data,
    such as deals likely to close this month, demand a good sort order (deal size,
    likelihood of closing) and the ability to change the order using a GUI or conversationally.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sort the deals by closest diving distance to me
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Show the orders by quantity
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Display deals for me that require the most attention that will give me the most
    commission
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The default UI language does not match the website:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: [*Chapter 9*](B21964_09_split_000.xhtml#_idTextAnchor190),
    *Guidelines and Heuristics* will discuss matching a customer’s language and understanding.
    If there are different terms to mean the same thing (common in an enterprise),
    the user might only understand terms within the proper context. This applies to
    spoken and written languages.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Respect the customers’ wishes in an international UI. If they
    have set their UI to Spanish, they should be given their conversational chat or
    AI results in Spanish. If they have to ask for it in Spanish, *“Español, por favor,”*
    this is a fail. If not all the information is in Spanish, the failure is worse.
    If the LLM supports multiple languages, how will it react to a combination of
    two languages? Issues around the choice of words, internationalization, or cultural
    issues should be evaluated for repair or rework. We will talk about some of this
    in the next chapter. In this example, most customers work in English, so each
    primary language scores 2 for customers impacted. They would always have this
    issue (a 3). Depending on the language and user, the severity is typically a 2\.
    If the user can only work in their native language, and the site doesn’t support
    it, score this a 4.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text and formatted numbers should align correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: Currency, table data, and **name:value** pairs must be aligned
    in specific ways.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: As long as labels and alignment don’t impact understanding,
    these score smaller values even if they always appear. Decide how vital style
    and a clean experience are to the company. However, as shown, these are low (XS)
    costs, so with some work, they will be prioritized accordingly and fixed. Formatting,
    in general, can be an issue for an LLM. In chat experiences, the customer might
    be able to direct the LLM to change the format (e.g., **show as a table**) or
    present the findings differently. Improvements to the instructions given to the
    LLM might be warranted.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Images don’t have labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: When a UI includes images, for example, in search results,
    they must be labeled with words. It is possible to use an LLM to label these more
    descriptively than a human would. For example, we might show shoes on a clothing
    site. “Shoes” is not an uncommon label that a screen reader can read for someone
    visually impaired. An image analysis LLM might provide the more accurate “Nike
    tennis shoes with a leather upper, and red, green, and yellow panels on a clear
    sole with Velcro straps.”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Depending on the context, images without labels can be more
    important than just an accessibility concern. In this case, the photos are more
    ornamental and are supported by text in the conversation. But watch out for this:
    make sure the experience is accessible and that labels, if generated by the AI,
    represent a clear understanding of their purpose. Significant images must be described
    for the visually impaired. GPT 4o can understand and present details for an image.
    So the user can hear the description and make purchasing decisions. These descriptions
    are not necessarily shown on screen. A screen reader will pick up the description
    from the HTML metadata and speak it. This can be done at a lower cost by sending
    the request for descriptions to the LLM by batch processing and storing the descriptions
    in the database. Batch processing in ChatGPT saves money. These robust labels
    can also be fed into the search to enable robust searching without additional
    LLM costs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A task region cannot be dragged or moved onto the screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Background**: Enterprise users have dashboards. Dashboards will get smarter
    because of LLMs. A little preview: [*Chapter 12*](B21964_12.xhtml#_idTextAnchor259),
    *Conclusion* will discuss the value of Wisdom, which every dashboard should have.
    The user tries to add a region that wasn’t displayed by default. If they go to
    customize the page and can’t add or move some regions around, they will think
    this is a bug. *Figure 4.5* shows a Chat region being added to the dashboard by
    dragging. If it didn’t work, that would be this problem.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B21964_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Dragging a region onto a dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Reasoning**: This relates to a UI feature that allows the regions and, thus,
    the ChatGPT experience to be customized. However, only some people do this (score
    a 1); they leave it where it is on the page. Even though the issue is always there,
    most people will not notice it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even within a ChatGPT chat window, we can present tables, include images in
    results (such as product images, schematics, or diagrams), use incorrect terms,
    or even have scrolling issues. ChatGPT is not a stand-alone solution in the enterprise;
    it is part of the more extensive solution and thus can have not only problems
    novel to LLMs but can include GUI issues.
  prefs: []
  type: TYPE_NORMAL
- en: Putting a backlog into order
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s now complete our thought. We have replaced our development cost with story
    points. We can then calculate and prioritize our backlog. What will rise to the
    top in *Table 4.2* with the score filled in?
  prefs: []
  type: TYPE_NORMAL
- en: '| **Samples from an Integrated ChatGPT** **Web Experience** | **UNSs** | **Dev
    Cost** | **WSJF** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **2\. Text: “Run” should be called “Search” in** **the toolbar** | 18 | 3
    | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| **5\. “Enterprise Support Recommendations” is a** **confusing term** | 12
    | 4 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| **8\. Text and formatted numbers should** **align correctly** | 9 | 6 | 1.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| **4\. The vertical scroll bar is missing from the** **conversational UI**
    | 18 | 14 | 1.23 |'
  prefs: []
  type: TYPE_TB
- en: '| **6\. Columnated answers need to be** **sorted correctly** | 12 | 10 | 1.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| **7\. The default UI language does not match** **the website** | 12 | 12
    | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **3\. Can’t submit a prompt by pressing return in** **a field** | 18 | 20
    | 0.9 |'
  prefs: []
  type: TYPE_TB
- en: '| **1\. The browser’s back button does not** **go “back”** | 36 | 43 | 0.84
    |'
  prefs: []
  type: TYPE_TB
- en: '| **9\. Images don’t** **have labels** | 8 | 16 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **10\. Customization of a task region cannot be dragged onto the screen**
    **when empty** | 6 | 24 | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.2 – The stories ordered by WSJF (larger WSJF should be developed first)
  prefs: []
  type: TYPE_NORMAL
- en: 'The row numbers are shown from the previous table to show how the order changed.
    Some observations from the scoring results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first three items are all low-cost, also called *low-hanging fruit*. They
    are easy to pick off and accomplish, but they could be more exciting, couldn’t
    they?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eighth item in the table (1\. The browser’s back button does not go “back”)
    has the most significant cost, but we notice it has the highest UNS. Review this
    again and evaluate whether a less complex solution can bring most of the value
    at less cost. If true, that new solution might jump to the third spot on the list.
    Sometimes, simple solutions can solve a piece of a complex problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple items have the same UNS, but once WSJF is calculated, no two items
    have the same score. This is really because we only scored ten items. In reality,
    there will be ties. It just means doing any tied item is acceptable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patching case study revisited
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the patching example from [*Chapter 3*](B21964_03.xhtml#_idTextAnchor058),
    *Identifying Optimal Use Cases for ChatGPT* we covered multiple LLM opportunities.
    However, we should revisit these interactions now that we have user scoring. To
    keep it simple, we will take the top three opportunities from *Table 3.4*, reintroduce
    the steps, and add the WSJF scores, as shown in *Table 4.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Prioritized Steps Identified for** **ChatGPT Support** | **UNS****(****A*B*C)**
    | **Dev Cost** | **WSJF** |'
  prefs: []
  type: TYPE_TB
- en: '| **1) Step 11: Generate tests from existing cases. Identify gaps** **in testing.**
    | 12 | 8 | 1.5 |'
  prefs: []
  type: TYPE_TB
- en: '| **2) Step 12: Monitor production instances** **for abnormalities.** | 8 |
    16 | 0.50 |'
  prefs: []
  type: TYPE_TB
- en: '| **3) Step 6: Predict the results of the patch plan, and the implications
    for missing or** **conflicting patches.** | 8 | 32 | 0.25 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.3 – ChatGPT use cases prioritized by WSJF
  prefs: []
  type: TYPE_NORMAL
- en: So, we now have the top patch issue to address. Because the UNSs were so close
    (as discussed in [*Chapter 3*](B21964_03.xhtml#_idTextAnchor058), *Identifying
    Optimal Use Cases for ChatGPT*), it came down to which item was most straightforward
    to implement. The cost of doing an LLM solution to *Step 11* is estimated to be
    less because the model requires fewer moving parts to generate test cases. We
    need to know meta-information about the software, versions, platform, OS patching
    level, the list of existing patches, and we can use to tie in with existing bug
    information to create an LLM advisor. Developing models for *Step 12* to monitor
    for abnormalities requires additional data sources and, thus, more API work to
    gather and provide the data. *Step 6* for predicting patch plan success is the
    most expensive to build and maintain because of the complexities of predicting
    how *trillions* of possible patch combinations might influence an installation.
    Each of these steps represents its own LLM model, possibly more than one model,
    discuss in [*Chapter 8*](B21964_08.xhtml#_idTextAnchor172), *Fine-Tuning*. There
    is a lot of overlap, but the inputs and outputs will vary. Knowing what patch
    collections are stable based on customer data from production instances is better
    understood than knowing what *should* work. The problem gets even more complex
    when we try to include an understanding of what could make it better or worse,
    which is why the estimate for *Step 6* is much larger. Your use cases will be
    easier to understand than our attempt to explain the patching processes for those
    unfamiliar with the area. There are also examples around the quality of the responses
    from an LLM as part of a conversation or coming from a recommendation; let me
    touch on that as well.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the LLM will attempt to hallucinate and will try to respond. [*Chapter
    7*](B21964_07.xhtml#_idTextAnchor150), *Prompt Engineering*, will discuss how
    to control the output by creating guardrails and instructions for the LLM. When
    LLM generates wrong output this becomes an issue to score. The logs show which
    customers are impacted by the problem. Monitoring the output will help identify
    the frequency of the problem. Judge the severity of the issues against the rubric.
  prefs: []
  type: TYPE_NORMAL
- en: There is no one answer for scoring these; it will depend. Hallucinations can
    be harmful or not so bad. They happen constantly or rarely and could impact all
    customers or only a select audience. Likewise, this might be a lack of or conflicting
    knowledge, a missing API, a source not considered by the LLM, poor training, unclear
    prompts, or just out of the application’s scope. With log analysis, reviewed in
    [*Chapter 2*](B21964_02_split_000.xhtml#_idTextAnchor031), *Conducting Effective
    User Research* , plus scoring and prioritizing effort, the team has what it needs
    to tackle important work first. Extend software tracking tools to instill this
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Extending tracking tools with scoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most Agile, product, or bug tracking tools can expose additional fields or columns.
    For example, a development priority drop menu (1 to 4 is typical) might already
    exist. UNS fields are similar but couched more in user-centric attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Expose all four fields (the three individual and the final score that should
    be sortable) in the company’s tracking tool. This allows for visibility and discussions
    to validate assumptions. As mentioned, there might be slight disagreements, and
    exposing scores brings it into the open. Expect to be able to use this to drive
    the sort order for stories and bugs so that the sprint teams, release management,
    and customers can see how and why this order exists. Transparency is best here.
    Score and then sort by scores in the tracking tool. Jira is very popular and supports
    custom fields and even calculations. WSJF is supported in Rally, and most tools
    can handle these scores via customization.
  prefs: []
  type: TYPE_NORMAL
- en: Try the User Needs Scoring method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is a spreadsheet with examples, coded with the drop menus and calculations.
    It might help to get started. Bring a few examples, work as a team to understand
    the method, and fill in stories and scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Score Stories Samples Worksheet](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-ScoringStoriesSamples.xlsx)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-ScoringStoriesSamples.xlsx](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-ScoringStoriesSamples.xlsx))'
  prefs: []
  type: TYPE_NORMAL
- en: Click the download button, highlighted in *Figure 4**.6*, to download the file
    to the desktop. There is no viewer for these files on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – How to download a file from GitHub
  prefs: []
  type: TYPE_NORMAL
- en: A simple paper and pen worksheet can be shared in a workshop to introduce this
    concept to the team and coach people through this process.
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Scoring Worksheet](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-Scoring_Worksheet_for_Design%20WSJF.pdf)
    ([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-Scoring_Worksheet_for_Design%20WSJF.pdf](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-Scoring_Worksheet_for_Design%20WSJF.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, sometimes too much is not a good thing. Resist the need to enhance
    scoring with more measures and capabilities. Start small. Let me explain why.
  prefs: []
  type: TYPE_NORMAL
- en: Creating more complex scoring methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are reminded of the **KISS (Keep it simple, silly) principle**. An organization
    can *go all the way* and create a score so complex that no one would understand
    the difference between something with a score of 2,032 and 2,840\. I know a system
    with 17 factors adding and subtracting based on who escalated the issue (a VP
    escalating the issue is worth more than if I do it), how old the problem is when
    it was filed, and its severity, among many other factors. Everyone wants to get
    their factor into a score. Resist that approach.
  prefs: []
  type: TYPE_NORMAL
- en: I find that (and some simple research could confirm) a mortal would have no
    chance of getting a good feel for working with the output of a score based on
    17 factors. Indeed, the proposed simple score method will not differentiate 20
    stories, all with a score of 12\. However, we suspect there will be more spread
    when accounting for the cost and work allocated across a few Agile teams. It is
    OK for items to wait their turn in an Agile backlog. Consider if including rules
    for ranking tied items would help. For example, a 12 for an issue for all customers
    should be prioritized higher than a 12 coming from a worse bug for fewer customers.
    However, the more complexity, the more difficult it is to understand and judge
    differences. Determine if it is worth the additional complexity, confusion, and
    overhead. Minimally, we advocate not ranking stories and scoring arbitrarily.
    Use a method that is repeatable and even can be consistently applied from team
    to team. So, aggregating rankings on a backlog is an apples-to-apples comparison.
    Once stories are divided by the costs, the values will spread out. Also, our goal
    is not spread. If two stories are judged the same, doing either story is just
    as valuable.
  prefs: []
  type: TYPE_NORMAL
- en: Working top-down from WSJF is not a religion. We don’t necessarily follow the
    WSJF as a dogma. It is a guide. Besides being a guide, different teams sometimes
    have specific expertise (not all organizations can be perfect about an Agile team
    doing it all). So, this can influence what teams pick from the backlog. Selecting
    the third item and leaving the first two for another team might be the most efficient
    and intelligent approach.
  prefs: []
  type: TYPE_NORMAL
- en: Working with multiple backlogs in Agile
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Usually, we have more than one backlog in a large enterprise development environment.
    So, this discussion is for those familiar with the complexities of multiple sprint
    team planning. The scrum team for the sprint has a backlog, and so does the project,
    maybe the program, and even the release. But effectively, these are virtual, and
    tagging an item for a sprint doesn’t mean ignoring the context of all other backlog
    items. A single large backlog is sometimes helpful to get a few key metrics out
    of it if done right. In Agile at scale, they look at different roll-ups of stories.
    At the program level, features that decompose into stories can be viewed. If the
    feature has a score of 36, it won’t mean that all stories in that feature will
    have the same value. As they are broken down, some stories are more important
    than others.
  prefs: []
  type: TYPE_NORMAL
- en: The program or product-level backlog might span 30 or more teams, as it did
    for my old organization. If a few teams represent 50% of the backlog (thinking
    now about story points or work effort, not in terms of the number of items), then
    maybe reconsider how teams are allocated. It is hard to compare if teams use different
    models to cost stories. However, we also want to balance team autonomy. In Scaled
    Agile, where many Agile teams are developing the same application, there is a
    need for a well-documented approach to story points. One more attempt to sell
    story points for those unconvinced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [What is a Story Point](https://agilefaq.wordpress.com/2007/11/13/what-is-a-story-point/)
    ([https://agilefaq.wordpress.com/2007/11/13/what-is-a-story-point/](https://agilefaq.wordpress.com/2007/11/13/what-is-a-story-point/))'
  prefs: []
  type: TYPE_NORMAL
- en: Don’t be trapped by estimating in hours. There is value in abstract story points.
    However, being accurate with hours is a headache, and because different people
    work at different paces, there are complications. Mike Cohn writes about this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Don’t Equate Story Points to Hours](https://www.mountaingoatsoftware.com/blog/dont-equate-story-points-to-hours)
    ([https://www.mountaingoatsoftware.com/blog/dont-equate-story-points-to-hours](https://www.mountaingoatsoftware.com/blog/dont-equate-story-points-to-hours))'
  prefs: []
  type: TYPE_NORMAL
- en: If there is one backlog and a consistent story point method, the delivery based
    on **customer points** can be compared. One team has five stories, each with a
    score of 12, having (5x12) 60 customer points in that sprint. Meanwhile, another
    team might do just three stories with scores of 36, 24, and 12, thus delivering
    roughly the same value (62 points) to the customer. A team tackling 15 stories
    with a sum of 30 provides 1/2 the value. A reallocation of teams to projects that
    have more customer points is warranted. This is not a challenge for a single sprint
    team working one list top down, but when doing Agile at scale, one has to consider
    when to reallocate sprint resources to be the most effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Scaled Agile Framework](https://scaledagileframework.com) ([https://scaledagileframework.com](https://scaledagileframework.com)/)'
  prefs: []
  type: TYPE_NORMAL
- en: I can’t stress enough the value that SAFe provides. The full version of SAFe,
    as shown in *Figure 4**.7*, includes matured models, concepts, refined processes,
    and extensive documentation, detail, training, and support. Go to the website,
    as this overview is constantly evolving.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21964_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The SAFe map
  prefs: []
  type: TYPE_NORMAL
- en: For example, a typical org might count and report on bugs per software unit.
    A team with 50 editing and typographical bugs might not be best compared to a
    team with three performance bugs, and how quickly they fixed those bugs or if
    it was worth the time and energy. The team with three bugs might have been doing
    something far more valuable and costly (development cost) than the team with 50
    easy bugs to fix. For design, we look at customer points. For development, in
    Agile, we can look at story points or the level of effort. Then, we can look at
    this single virtual backlog based on scores and learn something valuable.
  prefs: []
  type: TYPE_NORMAL
- en: An **ordered backlog** is less helpful than one with scoring. Ranking on an
    ordinal scale (1 is higher than 2) is less potent than scoring (a 15 is 50% more
    than a 10). We learned this in introductory statistics. An **interval scale**
    (where the difference between two numbers is an equal interval) is more powerful
    and carries more information. A user score of 10 is twice a 5\. Ranking something
    5th and 10th does not communicate this importance. The 10th item is, at best,
    five behind item 5\. That is, items 6 to 9 might not have huge differences in
    value. An interval (or ratio) scale reveals this.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A **score** is an interval scale, while a **ranking** is not. Scores are more
    powerful. A ratio scale is similar to an interval scale but has an absolute zero.
    The difference between them is not essential to this discussion. Just realize
    the value in knowing that the difference between 2 scores (6 to 12 and 12 to 18)
    represents the same difference.
  prefs: []
  type: TYPE_NORMAL
- en: This is why they show the points used to rank college football teams. Football
    fans then know how far *behind* their favorite team is from the next spot up.
  prefs: []
  type: TYPE_NORMAL
- en: Returning to *Table 4.2*, we can see that the first item on the original list,
    which scored 36, drops way down in priority because of its significant development
    cost. Look carefully and see the cost of delivering the first five items on the
    list; fixing all of them is better than fixing the single back button item, ranked
    first by UNS. This value is only revealed when accounting for development costs
    and rank by WSJF.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It would seem counterintuitive to not do something so valuable that it scores
    a 36 even with a long development lead time, which would be strategic and thus
    prioritized. However, those smaller items give more value. We optimize customer
    value.
  prefs: []
  type: TYPE_NORMAL
- en: There are scoring issues. Realize this can be challenging. Being aware of possible
    hiccups means they can be addressed along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world hiccups with scoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No scoring method is perfectly repeatable when people’s opinions come into play.
    We do our best to try to make a system that avoids debate. I have used this system
    for many years, and there are gotcha’s. Scott McNealy, one of the founders of
    Sun Microsystems, is often attributed with this variation of a Teddy Roosevelt
    quote, “The best decision is the right decision, the next best decision is the
    wrong decision, and the worst decision is no decision.” So, let’s put a stake
    in the ground, make the decisions to prioritize our backlog, know some limitations,
    and move forward.
  prefs: []
  type: TYPE_NORMAL
- en: I know Agile, and this is not WSJF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I defer to the SAFe website or Donald Reinertsen’s *Principles of Product Development
    Flow* book. User Needs Scoring is a variation on the WSJF solution because it
    can be coached in an hour and repeated easily. The UNS is a proxy for the numerator
    (it is a combination of the values for user + business value, time criticality,
    and risk reduction + a score for opportunity enablement value). The actual WSJF
    uses a Fibonacci-type number, and using that scale has more breadth. Again, this
    is to help *keep it* *simple, silly*.
  prefs: []
  type: TYPE_NORMAL
- en: Is WSJF deployed in your organization? If so, excellent work. Is the CoD metric
    reliable and repeatable? If not, adopt a consistent rubric like the one proposed.
  prefs: []
  type: TYPE_NORMAL
- en: The use of simple numbers one to four
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notice that the SAFe supports the concept of an interval scale for scoring.
    The Fibonacci sequence of numbers (1, 2, 3, 5, 8, 13, 21, 34) conveys an interval
    scale. Interval scales are commonly used for costing development. Our straightforward
    scores are also on an interval scale but don’t have the spread of the Fibonacci
    sequence, as they only go from one to four. We can explore why the three—or four-point
    scales make sense.
  prefs: []
  type: TYPE_NORMAL
- en: Dean Leffingwell, the lead author of SAFe, suggests that because the larger
    numbers are less precise, using “*Fibonacci makes* *consensus easier*.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Article: [Backlogs with WSJF](https://web.archive.org/web/20220919164755/https://techbeacon.com/app-dev-testing/prioritize-your-backlog-use-weighted-shortest-job-first-wsjf-improved-roi?amp)
    ([https://web.archive.org/web/20220919164755/https://techbeacon.com/app-dev-testing/prioritize-your-backlog-use-weighted-shortest-job-first-wsjf-improved-roi?amp](https://web.archive.org/web/20220919164755/https://techbeacon.com/app-dev-testing/prioritize-your-backlog-use-weighted-shortest-job-first-wsjf-improved-roi?amp))'
  prefs: []
  type: TYPE_NORMAL
- en: A consensus can be reached quickly with a small collection of numbers, but it
    is easy to appreciate that larger numbers are rough estimates and, thus, might
    not be worth debating. Simplicity makes the process easier to digest. If you have
    strong feelings about using Fibonacci and have good experiences with it for story
    estimates, *continue using it for cost*. It is an excellent tool for cost estimating.
    For User Needs Scoring, the spread in our numerator between small and unbounded
    large numbers won’t work. It would overwhelm stories. Scoring from 1 to 10 was
    evaluated, but the time debating a four versus a six doesn’t seem worth it. Having
    a ten-point rubric is very difficult. Stick with three or four values on the requirements
    side (the numerator). Development decides the denominator based on story points
    or the poker estimation we discussed earlier. Another consideration is to weigh
    each factor in the user score.
  prefs: []
  type: TYPE_NORMAL
- en: Weighting factors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is always a good idea to consider whether values should be weighted more.
    We could introduce weighting without changing the rubric or scale (one to three
    or four). Let’s take the Severity scores. We considered doing scores to align
    with a 100-point scale but didn’t find a good balance. 4 points are worth more
    than 3, but do we need it to be 100, 50, 25, or 10? Does multiplying or maybe
    not using a 1 for the last value make a difference? Someone should take that up
    as a research project. What would it look like to weigh each factor? It makes
    the calculation more complex.
  prefs: []
  type: TYPE_NORMAL
- en: We put the stake in the ground with our numbers. By multiplying the values together,
    each score is worth 33%. Weighting can be introduced if one score is worth more.
    I don’t know which one that would be. They all seem equally important. In any
    case, try it as documented, or do better, but don’t do less. The numbers allow
    for easy math.
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>W</mi><mi>S</mi><mi>J</mi><mi>F</mi><mo>=</mo><mn>3</mn><mfrac><mrow><mo>(</mo><mi>M</mi><mi>a</mi><mi>n</mi><mi>y</mi><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi
    mathvariant="normal">*</mi><mn>0.33</mn><mo>)</mo><mi mathvariant="normal">*</mi><mo>(</mo><mi>O</mi><mi>f</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi
    mathvariant="normal">*</mi><mn>0.33</mn><mo>)</mo><mi mathvariant="normal">*</mi><mi>B</mi><mi>a</mi><mi>d</mi><mi>S</mi><mi>c</mi><mi>o</mi><mi>r</mi><mi>e</mi><mi
    mathvariant="normal">*</mi><mn>0.33</mn><mo>)</mo></mrow><mrow><mi>J</mi><mi>o</mi><mi>b</mi><mi>D</mi><mi>u</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi></mrow></mfrac></mrow></mrow></math>](img/4.png)'
  prefs: []
  type: TYPE_IMG
- en: What is essential becomes more apparent after a few cycles. It is common to
    find that scores of 12 or greater are prioritized. Items that score lower sometimes
    are low-hanging fruit—that is, they are cheap to fix, so they don’t take up much
    time or energy but typically won’t drive excitement. They do drive consistency
    and quality. Severity deserves additional insight.
  prefs: []
  type: TYPE_NORMAL
- en: Severity seems complicated to judge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every development organization has some form of Bug severity. Typically, technology
    companies have four levels of bugs, but some companies go to five or six. Here
    is a GitHub document that provides an extensive collection of examples of severity.
    With practice, it becomes easier to judge.
  prefs: []
  type: TYPE_NORMAL
- en: Deep dive
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub: [Defining severity in a ChatGPT use cases](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-Deep_Dive_into_Severity.pdf)([https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-Deep_Dive_into_Severity.pdf](https://github.com/PacktPublishing/UX-for-Enterprise-ChatGPT-Solutions/blob/main/Chapter4-Deep_Dive_into_Severity.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: What we have seen is that Sev 2s get all the effort. Sev 1s are worked immediately,
    while Sev 4s, which generally revolve around fit and finish, never get worked.
    A collection of Sev 4s on the same feature adds up. Would anyone continue to use
    generative AI with poor grammar, typos, and a confusing layout? Each issue might
    seem insignificant, but having multiple problems in one element will quickly erode
    trust. Generally, we see very few of these errors in LLM outputs; it will be more
    about hallucinations, understanding questions that lack enough context, and lack
    of the correct enterprise knowledge, which will be a problem. These should be
    scored in the 1 to 3 range. Practice as a team so the collective expertise improves
    as the team judges’ severity. Don’t be scared by those big problems; Agile can
    help break them down into manageable parts.
  prefs: []
  type: TYPE_NORMAL
- en: The cost is so high that we can’t ever get the work done
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is a solution in Agile for this. If the cost is high, consider how to
    break down the story into smaller parts. We don’t want jobs blocked, so we work
    with agile methods to get the right-sized stories. Delivering a piece of a story
    can still provide value. Sometimes, backend work is required that won’t return
    user value directly. Judge these stories by what user value they will enable down
    the road. Judge individual parts independently of the larger story or epic. It
    is sometimes a creative endeavor to figure out how a large story can be broken
    up. We can’t give a magic solution. Brainstorm, consider where user value is derived
    from, and look for partial solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping issues into bugs to protect the quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The grouping of similar bugs makes sense. We don’t need a dozen bugs for the
    same fundamental change.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We are switching gears here. This is about the Severity of the issue as documented
    by the enterprise bug system. Recall that scores invert these values. Grouping
    issues allows the team to manage less work in the bug or tracking system. Save
    time tracking 100s of bugs when tracking 30 would make more sense.
  prefs: []
  type: TYPE_NORMAL
- en: 'One suggestion is to look at the collection of issues and consider whether
    they are all related to the same problem. That is, can one fix and resolve three
    different bugs? Another issue is scope. If a collection of bugs is associated
    with the same material or within a single ChatGPT instance (when doing RAG), use
    rules to aggregate bugs:'
  prefs: []
  type: TYPE_NORMAL
- en: A collection of 3 or more Severity 2 bugs for a specific problem, interaction,
    results, or element is equivalent to a Severity 1 bug (the “three strikes” rule)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A collection of training synonyms or fine-tuning examples for one concept can
    be grouped into a single Severity 2 bug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Five or more Severity 3 issues should be tracked as one Severity 2 item
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A missing intent or task in conversational AI can be a Severity 2 “bug” (a bug
    because the original design/specification defined it) or a Severity 3 ER (if it
    was not realistic to have anticipated it).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, consider adding a gate to protect the quality of the release. We cannot
    know the scale of your problem, so adjust as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: No Severity 1s for any instance or model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No more than 2 Severity 2s open issues per instance or model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bug and feature tracking system is required. Work WSJF into that software
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: How to work WSJF into the organization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These are simple calculations. Any spreadsheet can keep track of the results
    and be used to sort and order a list. However, building this into tracking tools
    such as Jira, Rally, or any homegrown tool is better. Include custom fields and
    drop-down menus for the scores, and then use a calculation field to divide the
    UNS by the development cost. Typically done in story points (recommended), but
    some organizations use days; any consistent method is acceptable compared to being
    random. Sort by this each time the backlog is reviewed to select work. This approach
    ensures the biggest bang for the buck.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything up to this point is to understand use cases and where to apply resources
    to evaluate and decide what to do with a ChatGPT solution. This chapter is an
    agile method that applies to ChatGPT interactions, recommender experiences, backend
    solutions, or any software development project. We needed a method to bridge the
    gap between defining use cases and deciding what use cases to tackle. Creating
    a repeatable method applies to any development project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do a few things with the learnings in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Estimate the value of existing stories, use cases, bugs, or features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with development to explain the story and get their cost estimate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work out issues in scoring stories and prioritize the backlog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate WSJF into the product life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a solution is up and running, we can apply more sophisticated approaches
    to evaluate LLM performance, but the same scoring concepts can be used there.
    We are almost done with our pre-development journey – one more stop. The next
    chapter will focus on designing the right experience. After that, once we go technical
    into ChatGPT technology, we can reevaluate and verify what was built and how it
    should improve, again drawing on the research and scorings discussion.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '| ![](img/04.jpg) | The links, book recommendations, and GitHub files in this
    chapter are posted on the reference page.Web Page: [Chapter 4 References](https://uxdforai.com/references#C4)
    ([https://uxdforai.com/references#C4](https://uxdforai.com/references#C4)) |'
  prefs: []
  type: TYPE_TB
