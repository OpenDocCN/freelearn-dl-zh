- en: <st c="0">13</st>
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="0">13</st>
- en: <st c="3">Using Prompt Engineering to Improve RAG Efforts</st>
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="3">使用提示工程来提升RAG努力</st>
- en: <st c="51">Pop quiz, what do you use to generate content from a</st> **<st c="105">large
    language</st>** **<st c="120">model</st>** <st c="125">(</st>**<st c="127">LLM</st>**<st
    c="130">)?</st>
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51">快速问答，您使用什么从大型语言</st> **<st c="105">模型</st>** **<st c="120">（</st>**<st
    c="127">LLM</st>**<st c="130">）** 中生成内容？</st>
- en: <st c="133">A prompt!</st>
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="133">一个提示！</st>
- en: <st c="143">Clearly, the prompt is a key element for any generative AI application,
    and therefore any</st> **<st c="234">retrieval-augmented generation</st>** <st
    c="264">(</st>**<st c="266">RAG</st>**<st c="269">) application.</st> <st c="285">RAG
    systems blend the capabilities of information retrieval and generative language
    models to enhance the quality and relevance of generated text.</st> <st c="432">Prompt
    engineering, in this context, involves the strategic formulation and refinement
    of input prompts to improve the retrieval of pertinent information, which subsequently
    enhances the generation process.</st> <st c="639">Prompts are yet another area
    within the generative AI world that entire books can be written about.</st> <st
    c="739">There are numerous strategies that focus on different areas of prompts
    that can be employed to improve the results of your LLM usage.</st> <st c="873">However,
    we are going to focus specifically on the strategies that are more specific to</st>
    <st c="961">RAG applications.</st>
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="143">显然，提示是任何生成式AI应用的关键元素，因此任何</st> **<st c="234">检索增强生成</st>** <st c="264">(</st>**<st
    c="266">RAG</st>**<st c="269">) 应用。</st> <st c="285">RAG系统结合了信息检索和生成语言模型的能力，以提升生成文本的质量和相关性。</st>
    <st c="432">在此背景下，提示工程涉及战略性地制定和优化输入提示，以改善相关信息的检索，从而提高生成过程。</st> <st c="639">提示是生成式AI世界中另一个可以写满整本书的领域。</st>
    <st c="739">有许多策略专注于提示的不同领域，可以用来改善您LLM使用的成果。</st> <st c="873">然而，我们将专注于更具体于</st>
    <st c="961">RAG应用</st> 的策略。
- en: <st c="978">In this chapter, we are going to focus our efforts on the</st> <st
    c="1037">following topics:</st>
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="978">在本章中，我们将集中精力探讨以下主题：</st>
- en: <st c="1054">Key prompt engineering concepts</st> <st c="1087">and parameters</st>
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1054">关键提示工程概念</st> <st c="1087">和参数</st>
- en: <st c="1101">The fundamentals of prompt design and prompt engineering specifically
    for</st> <st c="1176">RAG applications</st>
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1101">针对RAG应用的具体提示设计和提示工程的基本原理</st>
- en: <st c="1192">Adapting prompts for different LLMs beyond just</st> <st c="1241">OpenAI
    models</st>
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1192">适应不同LLM的提示，而不仅仅是</st> <st c="1241">OpenAI模型</st>
- en: <st c="1254">Code lab 13.1 – Creating custom</st> <st c="1287">prompt templates</st>
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1254">代码实验室13.1 – 创建自定义</st> <st c="1287">提示模板</st>
- en: <st c="1303">Code lab 13.2 –</st> <st c="1320">Prompting options</st>
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="1303">代码实验室13.2 –</st> <st c="1320">提示选项</st>
- en: <st c="1337">By the end of this chapter, you will have a solid foundation in
    prompt engineering for RAG and be equipped with practical techniques to optimize
    prompts for retrieving relevant information, generating high-quality text, and
    adapting to your specific use case.</st> <st c="1598">We will kick our discussion
    off by covering some of the key concepts within the prompting world, starting
    with</st> <st c="1709">prompt parameters.</st>
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1037">到本章结束时，您将具备RAG提示工程的坚实基础，并掌握优化提示以检索相关信息、生成高质量文本以及适应特定用例的实用技术。</st>
    <st c="1598">我们将从介绍提示世界中的关键概念开始，首先是</st> <st c="1709">提示参数</st>。
- en: <st c="1727">Technical requirements</st>
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1727">技术要求</st>
- en: <st c="1750">The code for this chapter is placed in the following GitHub</st>
    <st c="1811">repository:</st> [<st c="1823">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_13</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_13
    )
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1750">本章的代码放置在以下GitHub</st> <st c="1811">仓库中：</st> [<st c="1823">https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_13</st>](https://github.com/PacktPublishing/Unlocking-Data-with-Generative-AI-and-RAG/tree/main/Chapter_13
    )
- en: <st c="1920">Prompt parameters</st>
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="1920">提示参数</st>
- en: '<st c="1938">There are numerous parameters that are common among most LLMs,
    but we are going to discuss a small subset that is most likely to have an impact
    on your RAG efforts: temperature, top-p,</st> <st c="2124">and seed.</st>'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="1938">在大多数LLM中存在许多共同参数，但我们将讨论一个可能对您的RAG努力产生影响的较小子集：温度、top-p，以及种子。</st>
    <st c="2124">。</st>
- en: <st c="2133">Temperature</st>
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="2133">温度</st>
- en: <st c="2145">If you think of your</st> <st c="2166">output as a string of</st>
    **<st c="2189">tokens</st>**<st c="2195">, an LLM, in a</st> <st c="2209">basic
    sense, is predicting the</st> *<st c="2241">next word</st>* <st c="2250">(or token)
    based on the data you’ve provided and the previous tokens it has already generated.</st>
    <st c="2346">The next word that the LLM predicts is a product of a probability
    distribution representing all potential words and</st> <st c="2462">their probabilities.</st>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2145">如果你将你的</st> <st c="2166">输出视为一系列</st> **<st c="2189">标记</st>**<st
    c="2195">，那么在基本意义上，一个LLM（大型语言模型）是根据你提供的数据和它已经生成的先前标记来预测下一个单词（或标记）的。</st> <st c="2241">LLM预测的下一个单词是表示所有潜在单词及其概率的概率分布的结果。</st>
    <st c="2462">LLM预测的下一个单词是表示所有潜在单词及其概率的概率分布的结果。</st>
- en: <st c="2482">In many cases, the probability of certain words is going to be
    much higher than most others, but there is still a probabilistic chance that the
    LLM selects one of the less likely words.</st> <st c="2669">Temperature is the
    setting that dictates how likely it is for the model to choose a word further
    down the probability distribution.</st> <st c="2801">In other words, this allows
    you to use temperature to set the degree of randomness of the model’s output.</st>
    <st c="2907">You can pass temperature into your LLM definition as a parameter.</st>
    <st c="2973">It is optional.</st> <st c="2989">If you do not use it, the default
    is</st> `<st c="3026">1</st>`<st c="3027">. You can set the temperature value
    between</st> `<st c="3071">0</st>` <st c="3072">and</st> `<st c="3077">2</st>`<st
    c="3078">. Higher values will make the output more random, meaning it will strongly
    consider words further down the probability distribution, while lower values will
    do</st> <st c="3238">the opposite.</st>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="2482">在许多情况下，某些单词的概率可能远高于其他大多数单词，但LLM仍然有一定概率选择其中不太可能出现的单词。</st> <st c="2669">温度是决定模型选择概率分布中较后单词的可能性大小的设置。</st>
    <st c="2801">换句话说，这允许你使用温度来设置模型输出的随机程度。</st> <st c="2907">你可以将温度作为一个参数传递给你的LLM定义。</st>
    <st c="2973">这是可选的。</st> <st c="2989">如果你不使用它，默认值是</st> `<st c="3026">1</st>`<st
    c="3027">。你可以设置温度值在</st> `<st c="3071">0</st>` <st c="3072">和</st> `<st c="3077">2</st>`<st
    c="3078">之间。</st> 较高的值会使输出更加随机，这意味着它将强烈考虑概率分布中较后的单词，而较低的值则会做</st> <st c="3238">相反的事情。</st>
- en: <st c="3251">Simple temperature example</st>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3251">简单的温度示例</st>
- en: <st c="3278">Let’s review a very simple example of a</st> *<st c="3319">next-word</st>*
    <st c="3328">probability distribution to illustrate how temperature works.</st>
    <st c="3391">Let’s say you have the sentence</st> `<st c="3423">The dog ran</st>`
    <st c="3434">and you are waiting for the model to predict the</st> *<st c="3484">next
    word</st>*<st c="3493">. Let’s say that based on this model’s training and all
    of the other data it is considering as a part of this prediction, a very simple
    example of the conditional probability distribution for this is</st> <st c="3693">as
    follows:</st>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3278">让我们回顾一个简单的</st> *<st c="3319">下一个单词</st>* <st c="3328">概率分布的例子，以说明温度是如何工作的。</st>
    <st c="3391">假设你有一个句子</st> `<st c="3423">The dog ran</st>` <st c="3434">，并且你正在等待模型预测下一个</st>
    *<st c="3484">单词</st>*<st c="3493">。假设基于这个模型的训练和它考虑的所有其他数据，这个预测的简单条件概率分布如下：</st>
    <st c="3693">如下：</st>
- en: '`<st c="3704">P("next word" | "The dog ran") = {"down": 0.4, "to" : 0.3, "with":
    0.2, "</st>``<st c="3778">away": 0.1}</st>`'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '`<st c="3704">P("next word" | "The dog ran") = {"down": 0.4, "to" : 0.3, "with":
    0.2, "</st>``<st c="3778">away": 0.1}</st>`'
- en: <st c="3790">The total probabilities add up to</st> `<st c="3825">1</st>`<st
    c="3826">. The most likely word is</st> `<st c="3852">down</st>` <st c="3856">and
    the second most likely word is</st> `<st c="3892">to</st>`<st c="3894">. However,
    that does not mean that</st> `<st c="3929">away</st>` <st c="3933">will never
    appear in the inference.</st> <st c="3970">The model will apply a probabilistic
    model to this selection and sometimes, randomly, the less likely word will be
    selected.</st> <st c="4095">In some scenarios, this is an advantage for your RAG
    application, but in others, this may be a disadvantage.</st> <st c="4204">If you
    set the temperature to</st> `<st c="4234">0</st>`<st c="4235">, it will only use
    the most likely word.</st> <st c="4276">If you set it to</st> `<st c="4293">2</st>`<st
    c="4294">, it is much more likely to look at all of the options and randomly pick
    less likely words most of the time.</st> <st c="4403">In other words, you can
    increase the random nature of the model by increasing</st> <st c="4481">the temperature.</st>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="3790">总概率加起来是</st> `<st c="3825">1</st>`<st c="3826">。最可能的词是</st> `<st
    c="3852">down</st>` <st c="3856">，其次是</st> `<st c="3892">to</st>`<st c="3894">。然而，这并不意味着</st>
    `<st c="3929">away</st>` <st c="3933">永远不会出现在推理中。</st> <st c="3970">模型将对这个选择应用概率模型，有时，随机地，不太可能的词会被选中。</st>
    <st c="4095">在某些场景中，这可能是您RAG应用的优势，但在其他情况下，这可能是劣势。</st> <st c="4204">如果您将温度设置为</st>
    `<st c="4234">0</st>`<st c="4235">，它将只使用最可能的词。</st> <st c="4276">如果您将其设置为</st>
    `<st c="4293">2</st>`<st c="4294">，则更有可能查看所有选项，并且大多数情况下会随机选择不太可能的词。</st> <st c="4403">换句话说，您可以通过增加温度来增加模型的随机性。</st>
- en: <st c="4497">We have been using temperature since the beginning, setting it
    to zero.</st> <st c="4570">Here is the line that we</st> <st c="4595">have added:</st>
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4497">我们从一开始就在使用温度，将其设置为零。</st> <st c="4570">以下是添加的行：</st> <st c="4595">（此处省略了代码行）</st>
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: <st c="4664">The intention here was to make the results of our code labs more
    predictable so that when you run them, you get something similar.</st> <st c="4796">Your
    results will likely differ somewhat, but at least with a</st> `<st c="4858">0</st>`
    <st c="4859">temperature, they have a better chance of</st> <st c="4902">being
    similar.</st>
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4664">这里的目的是使我们的代码实验室的结果更可预测，以便当您运行它们时，您可以得到类似的结果。</st> <st c="4796">您的结果可能会有所不同，但至少在</st>
    `<st c="4858">0</st>` <st c="4859">温度下，它们有更大的可能性</st> <st c="4902">相似。</st>
- en: <st c="4916">You may not always want to use a</st> `<st c="4950">0</st>` <st
    c="4951">temperature.</st> <st c="4965">Consider scenarios, such as when you want
    a more</st> *<st c="5014">creative</st>* <st c="5022">output from the LLM, wherein
    you may want to use the temperature to your advantage in your</st> <st c="5114">RAG
    application.</st>
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="4916">您可能并不总是想使用</st> `<st c="4950">0</st>` <st c="4951">温度。</st> <st
    c="4965">考虑以下场景，例如当您希望从LLM获得更</st> *<st c="5014">有创意</st>* <st c="5022">的输出时，您可能想利用温度在您的</st>
    <st c="5114">RAG应用</st>中。</st>
- en: <st c="5130">Temperature</st> <st c="5142">and top-p are somewhat related in
    that they both manage randomness in your LLM’s output.</st> <st c="5232">However,
    there are differences.</st> <st c="5264">Let’s discuss top-p and talk about what
    these</st> <st c="5310">differences are.</st>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5130">温度</st> <st c="5142">和top-p在某种程度上是相关的，因为它们都管理LLM输出的随机性。</st> <st
    c="5232">然而，它们之间有差异。</st> <st c="5264">让我们讨论top-p，并谈谈这些</st> <st c="5310">差异是什么。</st>
- en: <st c="5326">Top-p</st>
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="5326">Top-p</st>
- en: <st c="5332">Similar to temperature, top-p can</st> <st c="5367">also help you
    introduce randomness into your model’s output.</st> <st c="5428">However, where
    temperature deals with the general emphasis on how random you would like your
    input to be, top-p can help you target a specific part of the probability distribution
    with that randomness.</st> <st c="5630">In the simple example provided, we discussed
    the</st> <st c="5679">probability distribution:</st>
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5332">与温度类似，top-p也可以帮助您将随机性引入模型输出。</st> <st c="5428">然而，温度处理的是对输入随机性的总体强调，而top-p可以帮助您通过那种随机性针对概率分布的特定部分。</st>
    <st c="5630">在提供的简单示例中，我们讨论了</st> <st c="5679">概率分布：</st>
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: <st c="5789">Remember that we noted that the total probability represented here
    adds up to</st> `<st c="5868">1.0</st>`<st c="5871">. With top-p, you can target
    what portion of that probability you would like included.</st> <st c="5958">So
    for example, if you set the top-p to</st> `<st c="5998">0.7</st>`<st c="6001">,
    it will only consider the first two words in this probability distribution, which
    add up to the first</st> `<st c="6105">0.7</st>` <st c="6108">(out of 1.0) of
    the probability distribution.</st> <st c="6155">You do not have that kind of targeted
    control with temperature.</st> <st c="6219">Top-p is also optional.</st> <st c="6243">If
    you do not use it, the default is</st> `<st c="6280">1</st>`<st c="6281">, meaning
    it considers</st> <st c="6304">all options.</st>
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="5789">记住，我们提到这里表示的总概率加起来是</st> `<st c="5868">1.0</st>`<st c="5871">。使用top-p，你可以指定你希望包含的概率部分。</st>
    <st c="5958">例如，如果你将top-p设置为</st> `<st c="5998">0.7</st>`<st c="6001">，它将只考虑这个概率分布中的前两个词，这些词的总和是概率分布中的第一个</st>
    `<st c="6105">0.7</st>` <st c="6108">(1.0中的)。</st> <st c="6155">使用温度时，你没有这种针对性的控制。</st>
    <st c="6219">Top-p也是可选的。</st> <st c="6243">如果你不使用它，默认值是</st> `<st c="6280">1</st>`<st
    c="6281">，这意味着它考虑了</st> <st c="6304">所有选项。</st>
- en: <st c="6316">You may be tempted to use both temperature and top-p, but that
    can get very complicated and unpredictable.</st> <st c="6424">Therefore, it is
    commonly recommended to use either, but not both at the</st> <st c="6497">same
    time.</st>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6316">你可能想同时使用温度和top-p，但这可能会变得非常复杂且不可预测。</st> <st c="6424">因此，通常建议使用其中之一，而不是同时使用。</st>
    <st c="6497">同时使用。</st>
- en: '| **<st c="6507">LLM parameter</st>** | **<st c="6521">Outcome</st>** |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **<st c="6507">LLM参数</st>** | **<st c="6521">结果</st>** |'
- en: '| <st c="6529">Temperature</st> | <st c="6541">General randomness</st> |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| <st c="6529">温度</st> | <st c="6541">一般随机性</st> |'
- en: '| <st c="6560">Top-p</st> | <st c="6566">Focused randomness</st> |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| <st c="6560">Top-p</st> | <st c="6566">针对性随机性</st> |'
- en: '| <st c="6585">Temperature +</st> <st c="6600">top-p</st> | <st c="6605">Unpredictable
    complexity</st> |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| <st c="6585">温度 +</st> <st c="6600">top-p</st> | <st c="6605">不可预测的复杂性</st>
    |'
- en: <st c="6630">Table 13.1 –Showing the type of outcomes from each LLM parameter</st>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6630">表13.1 – 展示每个LLM参数的结果类型</st>
- en: <st c="6695">Next, we’ll learn how to</st> <st c="6721">use top-p with your
    model.</st> <st c="6748">It is slightly different than the other parameters we
    have used in that you have to pass it as part of the</st> `<st c="6855">model_kwargs</st>`
    <st c="6867">variables, which looks</st> <st c="6891">like this:</st>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6695">接下来，我们将学习如何</st> <st c="6721">使用top-p与你的模型结合。</st> <st c="6748">这与我们之前使用的其他参数略有不同，因为你必须将其作为</st>
    `<st c="6855">model_kwargs</st>` <st c="6867">变量的一部分传递，这看起来像这样：</st>
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: <st c="6973">The</st> `<st c="6978">model_kwargs</st>` <st c="6990">variables
    are a convenient way to pass parameters that are not built directly into LangChain
    but that exist in the LLM’s underlying API.</st> <st c="7128">The top-p is a parameter
    for this ChatGPT model, but it may be called something different or not exist
    for other models.</st> <st c="7249">Be sure to check the documentation for each
    API you use and to use the proper reference to access that model’s parameters.</st>
    <st c="7372">Now that we have learned more about the parameters that help define
    randomness in our outputs, let’s learn about seed setting, which is</st> <st c="7508">meant
    to help us control the</st> <st c="7537">uncontrollable randomness.</st>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="6973">`<st c="6978">model_kwargs</st>` <st c="6990">变量是传递那些没有直接集成到LangChain但存在于LLM底层API中的参数的便捷方式。</st>
    <st c="7128">top-p是这个ChatGPT模型的参数，但其他模型可能叫法不同或不存在。</st> <st c="7249">务必检查你使用的每个API的文档，并使用正确的引用来访问该模型的参数。</st>
    <st c="7372">现在我们已经了解了帮助定义我们输出中随机性的参数，让我们学习种子设置，这是</st> <st c="7508">旨在帮助我们控制</st>
    <st c="7537">不可控的随机性。</st>
- en: <st c="7563">Seed</st>
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="7563">种子</st>
- en: <st c="7568">LLM responses are non-deterministic</st> <st c="7605">by default,
    meaning inference results can differ from request to request.</st> <st c="7679">However,
    as data scientists, we often have the need for a more deterministic, reproducible
    outcome.</st> <st c="7779">These details seem to be at odds with each other, but
    that does not have to be the case.</st> <st c="7868">OpenAI and others have recently
    made efforts to offer some control toward deterministic outputs by giving you
    access to this seed parameter and the</st> `<st c="8016">system_fingerprint</st>`
    <st c="8034">response field.</st>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="7568">LLM 响应默认是非确定性的</st> <st c="7605">，这意味着推理结果可能因请求而异。</st> <st c="7679">然而，作为数据科学家，我们通常需要更确定性和可重复的结果。</st>
    <st c="7779">这些细节似乎相互矛盾，但情况并不一定如此。</st> <st c="7868">OpenAI 和其他人最近已经努力提供一些控制，以便通过提供对种子参数和</st>
    `<st c="8016">system_fingerprint</st>` <st c="8034">响应字段的访问来实现。</st>
- en: <st c="8050">The seed is a common setting in many software applications that
    involve generating random numbers or random data sequences.</st> <st c="8175">By
    using a seed, you can still generate random sequences, but you can produce the
    same random sequence every time.</st> <st c="8290">This gives you the control
    to receive (mostly) deterministic outputs across API calls.</st> <st c="8377">You
    can set the seed parameter to any integer of your choice and use the same value
    across requests you’d like deterministic outputs for.</st> <st c="8515">Furthermore,
    if you use a seed, even with other random settings such as temperature or top-p,
    you can still (mostly) rely on receiving the same</st> <st c="8659">exact response.</st>
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8050">种子是许多涉及生成随机数或随机数据序列的软件应用中的常见设置。</st> <st c="8175">通过使用种子，你仍然可以生成随机序列，但你可以每次都生成相同的随机序列。</st>
    <st c="8290">这让你能够控制通过 API 调用接收（主要是）确定性的输出。</st> <st c="8377">你可以将种子参数设置为任何你选择的整数，并在你希望获得确定性输出的请求中使用相同的值。</st>
    <st c="8515">此外，如果你使用种子，即使与其他随机设置（如温度或 top-p）一起使用，你仍然可以（主要是）依赖接收相同的</st> <st c="8659">确切响应。</st>
- en: <st c="8674">It should be noted that your results can still be different, even
    with the use of a seed, because you are working with an API connected to a service
    where changes are constantly being made.</st> <st c="8865">Those changes can cause
    different results over time.</st> <st c="8918">Models such as ChatGPT provide
    a</st> `<st c="8951">system_fingerprint</st>` <st c="8969">field in their outputs,
    which you can compare to each other as an indication of system changes that may
    cause differences in the response.</st> <st c="9109">If the</st> `<st c="9116">system_fingerprint</st>`
    <st c="9134">value changes from the last time you called that LLM API, while you
    were using the same seed, you may still see different outputs due to changes OpenAI
    made to</st> <st c="9295">their systems.</st>
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="8674">需要注意的是，即使使用了种子，你的结果仍然可能不同，因为你正在使用一个连接到服务的 API，该服务正在不断进行更改。</st>
    <st c="8865">这些更改可能导致随着时间的推移结果不同。</st> <st c="8918">例如 ChatGPT 这样的模型在其输出中提供了一个</st>
    `<st c="8951">system_fingerprint</st>` <st c="8969">字段，你可以将其相互比较，作为系统变化可能引起响应差异的指示。</st>
    <st c="9109">如果你上一次调用那个 LLM API 时</st> `<st c="9116">system_fingerprint</st>`
    <st c="9134">值发生了变化，而你当时使用了相同的种子，那么你仍然可能会看到不同的输出，这是由于 OpenAI 对其系统所做的更改造成的。</st>
- en: <st c="9309">The seed parameter is also optional and does not exist in the LangChain
    set of LLM parameters.</st> <st c="9405">So, once again, like the</st> `<st c="9430">top-p</st>`
    <st c="9435">parameter, we must pass it through the</st> `<st c="9475">model_kwargs</st>`
    <st c="9487">parameter</st> <st c="9498">like this:</st>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9309">种子参数也是可选的，并且不在 LangChain 的 LLM 参数集中。</st> <st c="9405">因此，再次强调，就像</st>
    `<st c="9430">top-p</st>` <st c="9435">参数一样，我们必须通过</st> `<st c="9475">model_kwargs</st>`
    <st c="9487">参数传递它：</st>
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: <st c="9628">Here, we add the seed parameter alongside the</st> `<st c="9675">top-p</st>`
    <st c="9680">parameter in a dictionary of parameters that we will pass to the</st>
    `<st c="9746">model_kwargs</st>` <st c="9758">parameter.</st>
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9628">在这里，我们将种子参数与</st> `<st c="9675">top-p</st>` <st c="9680">参数一起添加到我们将传递给</st>
    `<st c="9746">model_kwargs</st>` <st c="9758">参数的参数字典中。</st>
- en: <st c="9769">There are many other parameters for the different models you could
    use that we encourage you to explore, but</st> <st c="9879">these parameters are
    likely to have the most impact on your</st> <st c="9939">RAG application.</st>
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9769">你可以探索许多其他模型参数，我们鼓励你这样做，但这些参数可能对你的</st> <st c="9879">RAG 应用影响最大。</st>
- en: <st c="9955">The next prompt-oriented key concept we will touch on is the</st>
    **<st c="10017">shot</st>** <st c="10021">concept, focusing on the amount of background
    information you provide to</st> <st c="10095">the LLM.</st>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="9955">我们将要讨论的下一个以提示为导向的关键概念是</st> **<st c="10017">镜头</st>** <st c="10021">概念，重点关注你提供给</st>
    <st c="10095">LLM</st> 的背景信息量。
- en: <st c="10103">Take your shot</st>
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="10103">尝试你的镜头</st>
- en: '**<st c="10118">No-shot</st>**<st c="10126">,</st> **<st c="10128">single-shot</st>**<st
    c="10139">,</st> **<st c="10141">few-shot</st>**<st c="10149">, and</st> **<st
    c="10155">multi-shot</st>** <st c="10165">are common terms you will hear when
    talking about your</st> <st c="10221">prompting strategy.</st> <st c="10241">They
    all stem from the same concept, where a shot is</st> <st c="10293">one example
    you give to your LLM to help it determine how to respond to</st> <st c="10365">your
    query.</st> <st c="10378">If that is not clear, then I could give you an</st>
    <st c="10425">example of what I am talking about.</st> <st c="10461">Oh wait,
    that is exactly</st> <st c="10485">the idea behind the shot concept!</st> <st
    c="10520">You can give no examples (no-shot), one example (single-shot), or more</st>
    <st c="10591">than one example (few-shot or multi-shot).</st> <st c="10634">Each</st>
    <st c="10638">shot is an example; each example is a shot.</st> <st c="10683">Here
    is an</st> <st c="10694">example of what you would say to an LLM (we could call
    this single-shot, since I am only providing</st> <st c="10793">one example):</st>'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**<st c="10118">无镜头</st>**<st c="10126">、**<st c="10128">单镜头</st>**<st c="10139">、**<st
    c="10141">少镜头</st>**<st c="10149">和**<st c="10155">多镜头</st>** <st c="10165">是在讨论你的提示策略时经常听到的术语。</st>
    <st c="10241">它们都源自同一个概念，即一个镜头是你给LLM的一个例子，以帮助它确定如何回应你的查询。</st> <st c="10378">如果这还不清楚，我可以给你一个</st>
    <st c="10425">例子来说明我所说的内容。</st> <st c="10461">哦，等等，这正是</st> <st c="10485">镜头概念背后的想法！</st>
    <st c="10520">你可以提供没有例子（无镜头）、一个例子（单镜头）或多个</st> <st c="10591">例子（少镜头或多镜头）。</st>
    <st c="10634">每个镜头都是一个例子；每个例子都是一个镜头。</st> <st c="10683">以下是你对LLM可能说的话的例子（我们可以称之为单镜头，因为我只提供了一个</st>
    <st c="10793">例子）：</st>'
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: <st c="11030">The assumption here is that by providing that example, you are
    helping guide the LLM in how</st> <st c="11123">you respond.</st>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11030">这里的假设是，通过提供那个例子，你正在帮助引导LLM如何</st> <st c="11123">回应。</st>
- en: <st c="11135">In a RAG application, you will often provide examples in your
    context.</st> <st c="11207">That is not always the case, as sometimes context
    is just additional (but important) data.</st> <st c="11298">However, if you are
    providing actual examples of questions and answers in the context with the intention
    of directing the LLM to answer the new user query in a similar manner, then you
    are using a shot approach.</st> <st c="11510">You will find that some RAG applications
    follow the multi-shot pattern much more closely, but it really depends on the
    goal of your application and the data you</st> <st c="11671">have available.</st>
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11135">在RAG应用中，你通常会提供上下文中的例子。</st> <st c="11207">这并不总是如此，因为有时上下文只是额外的（但重要的）数据。</st>
    <st c="11298">然而，如果你在上下文中提供实际的问题和答案的例子，目的是指导LLM以类似的方式回答新的用户查询，那么你就是在使用镜头方法。</st>
    <st c="11510">你会发现一些RAG应用更严格地遵循多镜头模式，但这实际上取决于你应用的目标和可用的数据。</st>
- en: <st c="11686">Examples and shots are not the only concepts that are important
    to understand in prompts, as you will also want to understand the difference in
    the terms referring to your approach to prompts.</st> <st c="11880">We will talk
    about these</st> <st c="11905">approaches next.</st>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11686">在提示中，例子和镜头并不是唯一需要理解的概念，因为你还需要了解指代你如何处理提示的术语之间的差异。</st> <st c="11880">我们将在下一节中讨论这些</st>
    <st c="11905">方法。</st>
- en: <st c="11921">Prompting, prompt design, and prompt engineering revisited</st>
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="11921">提示、提示设计和提示工程回顾</st>
- en: <st c="11980">In the vocabulary section of</st> [*<st c="12010">Chapter 1</st>*](B22475_01.xhtml#_idTextAnchor015)<st
    c="12019">, we discussed these three concepts and how they interplay.</st> <st
    c="12079">As a refresher, we provided</st> <st c="12107">these bullets:</st>
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="11980">在词汇部分</st> [*<st c="12010">第1章</st>*](B22475_01.xhtml#_idTextAnchor015)<st
    c="12019">中，我们讨论了这三个概念及其相互作用。</st> <st c="12079">为了复习，我们提供了以下要点：</st>
- en: '**<st c="12121">Prompting</st>** <st c="12131">is the act of</st> <st c="12146">sending
    a query or</st> *<st c="12165">prompt</st>* <st c="12171">to</st> <st c="12175">an
    LLM.</st>'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="12121">提示**</st> <st c="12131">是指</st> <st c="12146">发送一个查询或</st>
    *<st c="12165">提示</st> <st c="12171">到</st> <st c="12175">一个LLM。</st>'
- en: '**<st c="12182">Prompt design</st>** <st c="12196">refers to the strategy you
    take to</st> *<st c="12232">design</st>* <st c="12238">the prompt you will send
    to the LLM.</st> <st c="12276">Many different prompt design</st> <st c="12304">strategies
    work in</st> <st c="12324">different scenarios.</st>'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="12182">提示设计</st>** <st c="12196">指的是你采取的策略来</st> *<st c="12232">设计</st>
    <st c="12238">你将发送给LLM的提示。</st> <st c="12276">许多不同的提示设计</st> <st c="12304">策略在不同的场景下都有效。</st>'
- en: '**<st c="12344">Prompt engineering</st>** <st c="12363">focuses</st> <st c="12371">more
    on the technical aspects surrounding the prompt that you use to improve the outputs
    from the LLM.</st> <st c="12475">For example, you may break up a complex query
    into two or three different LLM interactions,</st> *<st c="12567">engineering</st>*
    <st c="12578">it better to achieve</st> <st c="12600">superior results.</st>'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="12344">提示工程</st>** <st c="12363">更关注</st> <st c="12371">围绕你使用的提示的技术方面，以改进LLM的输出。</st>
    <st c="12475">例如，你可能将一个复杂的查询分解成两个或三个不同的LLM交互，</st> *<st c="12567">工程化</st> <st
    c="12578">它以实现</st> <st c="12600">更优的结果。</st>'
- en: '<st c="12617">We had promised to revisit these topics in</st> [*<st c="12661">Chapter
    13</st>*](B22475_13.xhtml#_idTextAnchor256)<st c="12671">, and so we are here
    to deliver on that promise!</st> <st c="12720">We will not only revisit these
    topics but also show you how this is actually performed in code.</st> <st c="12816">Prompting
    is a relatively straightforward concept, so we will focus on the other two topics:
    design</st> <st c="12916">and engineering.</st>'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12617">我们曾承诺在</st> [*<st c="12661">第13章</st> *](B22475_13.xhtml#_idTextAnchor256)<st
    c="12671">中重新审视这些主题，所以我们现在来履行这个承诺！</st> <st c="12720">我们不仅将重新审视这些主题，还会向你展示如何在代码中实际执行这些操作。</st>
    <st c="12816">提示是一个相对直接的概念，所以我们将会关注其他两个主题：设计和工程。</st>
- en: <st c="12932">Prompt design versus engineering approaches</st>
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="12932">提示设计对比工程方法</st>
- en: <st c="12976">When we discussed the different</st> *<st c="13009">shot</st>*
    <st c="13013">approaches in the</st> *<st c="13032">Take your shot</st>* <st c="13046">section,
    that fell under prompt design.</st> <st c="13087">However, we also implemented
    prompt engineering when we filled in the prompt template with the question and
    context data we pulled from other parts of the RAG system.</st> <st c="13254">When
    we fill this prompt with data from other parts of the system, you may remember
    that this is called hydrating, which is a specific prompt engineering approach.</st>
    <st c="13418">Prompt design and prompt engineering have significant overlap and
    so you will often hear the terms used interchangeably.</st> <st c="13539">In our
    case, we are going to talk about them together, particularly how they can be used
    to improve our</st> <st c="13643">RAG application.</st>
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="12976">当我们讨论了在“*<st c="13009">射击</st> *<st c="13013">方法</st> *<st c="13032">中”的不同</st>
    *<st c="13046">射击</st> *<st c="13087">方法，这属于提示设计。</st> <st c="13087">然而，当我们用从RAG系统的其他部分提取的问题和上下文数据填写提示模板时，我们也实施了提示工程。</st>
    <st c="13254">当我们用来自系统其他部分的数据填写这个提示时，你可能记得这被称为“水化”，这是一种特定的提示工程方法。</st> <st c="13418">提示设计和提示工程有显著的交集，因此你经常会听到这两个术语被交替使用。</st>
    <st c="13539">在我们的案例中，我们将一起讨论它们，特别是它们如何被用来改进我们的</st> <st c="13643">RAG应用。</st>
- en: <st c="13659">I have seen these concepts described in many different ways over
    the past few years, and so it would seem that our field still hasn’t formed a
    complete definition of each or drawn the line between them.</st> <st c="13863">For
    the purpose of understanding these concepts for this book, the way I would describe
    the difference between prompt design and prompt engineering is that prompt engineering
    is a broader concept that encompasses not only the design of the prompt but also
    the optimization and fine-tuning of the entire interaction between the user and
    the</st> <st c="14203">language model.</st>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="13659">在过去几年中，我看到了这些概念以许多不同的方式被描述，因此似乎我们的领域还没有形成对每个概念的完整定义，也没有在它们之间划清界限。</st>
    <st c="13863">为了理解这本书中的这些概念，我会描述提示设计和提示工程之间的区别是：提示工程是一个更广泛的概念，它不仅包括提示的设计，还包括用户与语言模型之间整个交互的优化和微调。</st>
- en: <st c="14218">There are numerous prompt design techniques, which could all be
    used to improve your RAG application, in theory.</st> <st c="14332">It is important
    to keep track of the options you have and understand which scenarios each approach
    is most applicable to.</st> <st c="14454">It will take some experimentation with
    different prompt design approaches to determine which is best for your application.</st>
    <st c="14577">There is no one-size-fits-all solution for prompt design.</st> <st
    c="14635">We will provide a short list of examples, but we highly encourage you
    to learn more about prompt design from other sources and take note of which approaches
    might help your specific</st> <st c="14817">applications out:</st>
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="14218">理论上，有无数种提示设计技术，都可以用来改进你的RAG应用。</st> <st c="14332">跟踪你拥有的选项并了解每种方法最适合哪种场景很重要。</st>
    <st c="14454">需要通过不同提示设计方法的实验来确定哪种最适合你的应用。</st> <st c="14577">提示设计没有一劳永逸的解决方案。</st>
    <st c="14635">我们将提供一些示例列表，但我们强烈建议你从其他来源了解更多关于提示设计的信息，并注意哪些方法可能有助于你的特定应用：</st>
    <st c="14817"></st>
- en: '**<st c="14834">Shot design</st>**<st c="14846">:</st>'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="14834">镜头设计</st>**<st c="14846">：</st>'
- en: <st c="14848">The starting</st> <st c="14860">point for any prompt design</st>
    <st c="14889">thought process</st>
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="14848">任何提示设计</st> <st c="14860">思维过程的起点</st> <st c="14889"></st>
- en: <st c="14904">Involves carefully crafting the initial prompt to use examples
    to help guide the AI model toward the</st> <st c="15006">desired output</st>
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="14904">涉及精心设计初始提示，使用示例帮助引导AI模型达到期望的输出</st> <st c="15006"></st>
- en: <st c="15020">Can be applied and/or mixed with other design patterns to enhance
    the quality and relevance of the</st> <st c="15120">generated content</st>
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15020">可以与其他设计模式结合使用，以增强生成内容的质量和相关性</st> <st c="15120"></st>
- en: '**<st c="15137">Chain-of-thought prompting</st>**<st c="15164">:</st>'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="15137">思维链提示</st>**<st c="15164">：</st>'
- en: <st c="15166">Breaks down complex problems</st> <st c="15194">into smaller,
    more manageable steps, prompting the LLM for intermediate reasoning at</st> <st
    c="15280">each step</st>
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15166">将复杂问题分解成更小、更易于管理的步骤，在每个步骤中提示LLM进行中间推理</st> <st c="15194"></st>
- en: <st c="15289">Enhances the quality of LLM-generated answers by providing a clear,
    step-by-step thought process, ensuring better understanding and more</st> <st
    c="15427">accurate responses</st>
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15289">通过提供清晰、逐步的思维过程，提高LLM生成答案的质量，确保更好的理解和更准确的响应</st> <st c="15427"></st>
- en: '**<st c="15445">Personas (</st>****<st c="15456">role prompting)</st>**<st
    c="15472">:</st>'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="15445">角色（</st>****<st c="15456">角色提示）</st>**'
- en: <st c="15474">Involves creating a fictional</st> <st c="15503">character based
    on a representative segment of a user population or group, defined with details
    such as name, occupation, demographics, personal story, pain points,</st> <st
    c="15669">and challenges</st>
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15474">涉及创建一个基于用户群体或群体的代表性部分的虚构</st> <st c="15503">角色，包括姓名、职业、人口统计、个人故事、痛点</st>
    <st c="15669">和挑战</st>
- en: <st c="15683">Ensures that the output is relevant, useful, and consistent with
    the needs and preferences of the target audience, giving the content more personality</st>
    <st c="15835">and style</st>
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15683">确保输出与目标受众的需求和偏好相关、有用且一致，给内容增添更多个性和风格</st> <st c="15835"></st>
- en: <st c="15844">A powerful tool for developing effective language models that
    align with the needs</st> <st c="15928">of users</st>
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15844">是开发符合用户需求的有效语言模型的有力工具</st> <st c="15928"></st>
- en: '**<st c="15936">Chain of</st>** **<st c="15946">density (summarization)</st>**<st
    c="15969">:</st>'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="15936">密度链</st>** **<st c="15946">（摘要）</st>**<st c="15969">：</st>'
- en: <st c="15971">Focuses on ensuring that the</st> <st c="16000">LLM has done a
    proper job summarizing content, checking that no vital information has been left
    out and that the summary is</st> <st c="16124">concise enough</st>
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="15971">确保LLM已经正确地总结了内容，检查是否有重要信息被遗漏，并且摘要是否足够简洁</st> <st c="16000"></st>
- en: <st c="16138">Uses entity density as the LLM iterates through a summary, ensuring
    that the most important entities</st> <st c="16240">are included</st>
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16138">在LLM遍历摘要时使用实体密度，确保包含最重要的实体</st> <st c="16240"></st>
- en: '**<st c="16252">Tree of thoughts (exploration</st>** **<st c="16283">over thoughts)</st>**'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="16252">思维树（探索</st>** **<st c="16283">思维）</st>**'
- en: <st c="16297">Starts with an initial prompt, which</st> <st c="16334">generates
    multiple thought options, and iteratively selects the best options to generate
    the next round</st> <st c="16439">of thoughts</st>
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16297">从一个初始提示开始，它</st> <st c="16334">生成多个思考选项，并迭代选择最佳选项以生成下一轮</st> <st
    c="16439">的思考</st>
- en: <st c="16450">Allows for a more diverse and comprehensive exploration of ideas
    and concepts until the</st> <st c="16539">desired output text</st> <st c="16559">is
    generated</st>
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16450">允许更全面和多样化的探索想法和概念，直到生成</st> <st c="16539">所需的输出文本</st> <st c="16559">。</st>
- en: '**<st c="16571">Graph prompting</st>**'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="16571">图提示</st>**'
- en: <st c="16587">A new prompting framework</st> <st c="16613">specifically designed
    for working with</st> <st c="16653">graph-structured data</st>
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16587">一个专门为处理</st> <st c="16613">图结构数据</st> <st c="16653">而设计的新的提示框架</st>
- en: <st c="16674">Enables the LLM to understand and generate content based on the
    relationships between entities in</st> <st c="16773">a graph</st>
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16674">使LLM能够根据图中的实体关系理解和生成内容</st> <st c="16773">。</st>
- en: '**<st c="16780">Knowledge augmentation</st>**'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="16780">知识增强</st>**'
- en: <st c="16803">Involves augmenting prompts</st> <st c="16831">with extra, relevant
    information to improve the quality and accuracy of the</st> <st c="16908">generated
    content</st>
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16803">涉及通过添加额外的、相关的信息来增强提示，以提高</st> <st c="16831">生成内容的</st> <st c="16908">质量和准确性</st>
- en: <st c="16925">Can be achieved through techniques such as RAG, which incorporates
    external knowledge into</st> <st c="17017">the prompt</st>
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="16925">可以通过将外部知识纳入</st> <st c="17017">提示</st> <st c="17017">的技术，如RAG来实现</st>
- en: '**<st c="17027">Show Me versus Tell</st>** **<st c="17048">Me prompts</st>**'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**<st c="17027">展示给我而不是告诉我</st>** **<st c="17048">提示</st>**'
- en: <st c="17058">Two different approaches to</st> <st c="17086">providing instructions
    to generative AI models:</st> *<st c="17135">Show Me</st>* <st c="17142">involves
    providing examples or demonstrations, while</st> *<st c="17196">Tell Me</st>*
    <st c="17203">involves providing explicit instructions</st> <st c="17245">or documentation</st>
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="17058">两种向生成式AI模型提供指令的不同方法：</st> *<st c="17135">展示给我</st>* <st c="17142">涉及提供示例或演示，而</st>
    *<st c="17196">告诉我</st>* <st c="17203">涉及提供明确的指令</st> <st c="17245">或文档</st>
- en: <st c="17261">Using both approaches offers flexibility and can potentially increase
    the accuracy of the generative AI’s responses based on the specific context and
    complexity of</st> <st c="17426">the task</st>
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="17261">结合这两种方法提供灵活性，并可能根据特定任务的具体上下文和复杂性提高生成式AI响应的准确性</st> <st c="17426">。</st>
- en: <st c="17434">This list is just scratching the surface, as there are numerous
    other approaches that can be employed to improve prompt engineering and generative
    AI performance.</st> <st c="17598">As the field of prompt engineering continues
    to evolve, new and innovative techniques are likely to emerge, further enhancing
    the capabilities of generative</st> <st c="17755">AI models.</st>
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17434">这份列表只是触及了表面，因为还有许多其他方法可以用来提高提示工程和生成式AI的性能。</st> <st c="17598">随着提示工程领域的持续发展，可能会出现新的创新技术，进一步增强生成式AI模型的能力。</st>
    <st c="17755">。</st>
- en: <st c="17765">Let’s talk about the fundamentals of prompt design that can help
    with RAG</st> <st c="17840">applications next.</st>
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17765">接下来，让我们谈谈有助于RAG应用的提示设计的基本原则。</st> <st c="17840">。</st>
- en: <st c="17858">Fundamentals of prompt design</st>
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="17858">提示设计的基本原则</st>
- en: <st c="17888">When designing prompts for RAG applications, it’s essential to
    keep the following fundamentals</st> <st c="17984">in mind:</st>
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="17888">在设计RAG应用的提示时，必须牢记以下基本要点</st> <st c="17984">：</st>
- en: '`<st c="18174">Please analyze the given context and provide an answer to the
    question, taking into account all the relevant information and details</st>` <st
    c="18306">would be less concise and specific than saying</st> `<st c="18354">Based
    on the context provided, answer the following question: [</st>``<st c="18417">specific
    question]</st>`<st c="18436">.</st>'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="18174">请分析给定上下文并回答问题，考虑到所有相关信息和细节</st>` <st c="18306">可能不如说</st> `<st
    c="18354">根据提供的上下文，回答以下问题：[</st>``<st c="18417">具体问题</st>`<st c="18436">。</st>'
- en: '`<st c="18626">Summarize the main points of the context, identify the key entities
    mentioned, and then answer the given question</st>`<st c="18739">, that is multiple
    tasks you are asking for at the same time.</st> <st c="18801">You would likely
    have better results if you broke this into multiple prompts and said something
    similar</st> <st c="18905">to this:</st>'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="18626">总结上下文的主要观点，识别提到的关键实体，然后回答给定的问题</st>`<st c="18739">，即你同时要求的多项任务。</st>
    <st c="18801">如果你将这个问题拆分成多个提示，并说类似以下的内容，你可能会得到更好的结果</st> <st c="18905">这样的：</st>'
- en: '`<st c="18913">Summarize the main points of the following</st>` `<st c="18957">context:
    [context]</st>`'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="18913">总结以下</st>` `<st c="18957">上下文的主要观点：[上下文]</st>`'
- en: '`<st c="18975">Identify the key entities mentioned in the following summary:
    [summary from</st>` `<st c="19052">previous prompt]</st>`'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="18975">识别以下总结中提到的关键实体：[来自</st>` `<st c="19052">先前提示的总结]</st>`'
- en: '`<st c="19068">Using the context and entities identified, answer the following
    question: [</st>``<st c="19144">specific question]</st>`'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="19068">使用上下文和识别的实体回答以下问题：[</st>``<st c="19144">具体问题]</st>`'
- en: '`<st c="19427">Based on the context, what is the sentiment expressed towards
    the topic?</st>`<st c="19499">, you would likely have better results if you said</st>
    `<st c="19550">Based on the context, classify the sentiment expressed towards
    the topic as either positive, negative,</st>` `<st c="19653">or neutral</st>`<st
    c="19663">.</st>'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="19427">根据上下文，对主题表达的情感是什么？</st>`<st c="19499">，如果你这样说，你可能会得到更好的结果</st>
    `<st c="19550">根据上下文，将对主题表达的情感分类为正面、负面</st>` `<st c="19653">或中性</st>`<st c="19663">。</st>'
- en: '`<st c="19967">Answer the following question based on the provided context</st>`<st
    c="20026">, you would want to say something similar to</st> `<st c="20071">Using
    the examples below as a guide, answer the following question based on the provided
    context: Example 1: [question] [context] [answer] Example 2: [question] [context]
    [answer] Current question: [question]</st>` `<st c="20280">Context: [context]</st>`<st
    c="20298">.</st>'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="19967">根据提供的上下文回答以下问题</st>`<st c="20026">，你可能会想这样说</st> `<st c="20071">以下列出的示例作为指导，根据提供的上下文回答以下问题：示例
    1：[问题] [上下文] [答案] 示例 2：[问题] [上下文] [答案] 当前问题：[问题]</st>` `<st c="20280">上下文：[上下文]</st>`<st
    c="20298">。</st>'
- en: '`<st c="20548">Summarize the main points of the article, identify key entities,
    and answer the following question: [question].</st> <st c="20660">Provide examples
    and use the following format for your answer: [format].</st> <st c="20733">Article:
    [lengthy article text]</st>` <st c="20764">prompt will likely be less effective
    than prompting with multiple iterations such as</st> <st c="20850">the following:</st>'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="20548">总结文章的主要观点，识别关键实体，并回答以下问题：[问题]。</st> <st c="20660">提供示例，并使用以下格式回答：[格式]。</st>
    <st c="20733">文章：[长篇文章文本]</st>` <st c="20764">提示可能不如以下多次迭代提示有效：</st> <st c="20850">以下：</st>'
- en: '`<st c="20879">Summarize the main points of the following article: [</st>``<st
    c="20932">article text]</st>`'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="20879">总结以下文章的主要观点：[</st>``<st c="20932">文章文本]</st>`'
- en: '`<st c="20961">Summarize the main points and identify key entities in the following
    article: [</st>``<st c="21040">article text]</st>`'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="20961">总结以下文章的主要观点并识别关键实体：[</st>``<st c="21040">文章文本]</st>`'
- en: '`<st c="21069">Based on the summary and key entities, answer the following
    question: [question] Article: [</st>``<st c="21160">article text]</st>`'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="21069">基于总结和关键实体，回答以下问题：[问题] 文章：[</st>``<st c="21160">文章文本]</st>`'
- en: '`<st c="21319">###</st>` <st c="21322">to distinguish between the instruction
    and context sections.</st> <st c="21384">This helps the AI model better understand
    and follow the given instructions.</st> <st c="21461">For example, you will have
    less success with a prompt such as</st> `<st c="21523">[Context] Please use the
    above context to answer the following question: [question].</st> <st c="21608">Provide
    your answer in a concise manner</st>` <st c="21647">compared to this</st> <st
    c="21664">prompt:</st> `<st c="21673">Instructions: Using the context provided
    below, answer the question in a concise manner.</st> <st c="21762">Context: [context]</st>`
    `<st c="21781">Question: [question]</st>`<st c="21801">.</st>'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="21319">###</st>` <st c="21322">用于区分指令和上下文部分。</st> <st c="21384">这有助于AI模型更好地理解和遵循给定的指令。</st>
    <st c="21461">例如，与以下提示相比</st> `<st c="21523">[上下文]请使用上述上下文回答以下问题：[问题]。</st> <st
    c="21608">以简洁的方式提供您的答案</st>` <st c="21647">，您将获得更少的成功</st> <st c="21664">。</st>
    `<st c="21673">指令：使用以下提供的上下文，以简洁的方式回答问题。</st> <st c="21762">上下文：[上下文]</st>` `<st
    c="21781">问题：[问题]</st>`<st c="21801">。</st>'
- en: <st c="21802">While the fundamentals of prompt design provide a solid foundation
    for creating effective prompts, it’s important to remember that different language
    models may require specific adaptations to achieve optimal results.</st> <st c="22021">Let’s
    discuss that</st> <st c="22040">topic next.</st>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="21802">虽然提示设计的根本原则为创建有效的提示提供了坚实的基础，但重要的是要记住，不同的语言模型可能需要特定的调整以达到最佳效果。</st>
    <st c="22021">让我们接下来讨论这个</st> <st c="22040">话题。</st>
- en: <st c="22051">Adapting prompts for different LLMs</st>
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="22051">为不同的LLM调整提示</st>
- en: <st c="22087">As the</st> <st c="22095">AI landscape evolves, people are no
    longer solely relying on</st> <st c="22155">OpenAI for their language modeling
    needs.</st> <st c="22198">Other players, such as Anthropic with their Claude models,
    have gained popularity due to their ability to handle long context windows.</st>
    <st c="22333">Google is also releasing (and will continue to release) powerful
    models.</st> <st c="22406">Moreover, the open source model community is catching
    up, with models such as Llama proving to be</st> <st c="22504">viable alternatives.</st>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22087">随着</st> <st c="22095">AI领域的不断发展，人们不再仅仅依赖于</st> <st c="22155">OpenAI来满足他们的语言建模需求。</st>
    <st c="22198">其他玩家，如Anthropic及其Claude模型，因其处理长上下文窗口的能力而受到欢迎。</st> <st c="22333">Google也在发布（并将继续发布）强大的模型。</st>
    <st c="22406">此外，开源模型社区正在迎头赶上，Llama等模型已被证明是</st> <st c="22504">可行的替代品。</st>
- en: <st c="22524">However, it’s important to note that prompts do not seamlessly
    transfer from one LLM to another.</st> <st c="22622">Each LLM may have specific
    tricks and techniques that work best for its architecture.</st> <st c="22708">For
    example, Claude-3 prefers XML encoding when prompting, while Llama3 uses a specific
    syntax when labeling different parts of your prompt, such as SYS and INST.</st>
    <st c="22871">Here is an example prompt for Llama models using the SYS and</st>
    <st c="22932">INST tags:</st>
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="22524">然而，需要注意的是，提示（prompts）并不能无缝地从一种大型语言模型（LLM）转移到另一种。</st> <st c="22622">每种LLM可能都有最适合其架构的特定技巧和技术。</st>
    <st c="22708">例如，Claude-3在提示时更喜欢使用XML编码，而Llama3在标记提示的不同部分（如SYS和INST）时使用特定的语法。</st>
    <st c="22871">以下是一个使用SYS和INST标签的Llama模型的示例提示：</st> <st c="22932">。</st>
- en: '`<st c="22942"><SYS> You are an AI assistant created to provide helpful and
    informative responses to user</st>` `<st c="23034">questions.</st> <st c="23045"></SYS></st>`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="22942"><SYS>您是一个AI助手，旨在为用户提供有帮助和信息的回答。</st> <st c="23034">问题。</st>
    <st c="23045"></SYS></st>`'
- en: '`<st c="23051"><INST> Analyze the user''s question below and provide a clear,
    concise answer using your knowledge base.</st> <st c="23156">If the question is
    unclear, ask</st>` `<st c="23188">for clarification.</st>`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<st c="23051"><INST>分析以下用户的问题，并使用您的知识库提供清晰、简洁的答案。</st> <st c="23156">如果问题不清楚，请</st>`
    `<st c="23188">要求澄清。</st>`'
- en: <st c="23206">User’s question:</st> `<st c="23223">"What are the main advantages
    of using renewable energy sources compared to fossil</st>` `<st c="23307">fuels?"</st>
    <st c="23315"></INST></st>`
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: <st c="23206">用户问题：</st> `<st c="23223">"与化石燃料相比，使用可再生能源的主要优势是什么?"</st> <st
    c="23307"></st>` <st c="23315"></INST></st>
- en: <st c="23322">In this example, the</st> `<st c="23344">SYS</st>` <st c="23347">tag
    briefly establishes the AI’s role as an assistant designed to offer helpful responses.</st>
    <st c="23439">The</st> `<st c="23443">INST</st>` <st c="23447">tag provides specific
    instructions for answering the user’s question, which is included within the</st>
    `<st c="23547">INST</st>` <st c="23551">block.</st> **<st c="23559">SYS</st>**
    <st c="23562">is used as a shorthand for</st> **<st c="23590">system</st>** <st
    c="23596">or</st> **<st c="23600">system message</st>**<st c="23614">, while</st>
    **<st c="23622">INST</st>** <st c="23626">is used in the place</st> <st c="23648">of</st>
    **<st c="23651">instructions</st>**<st c="23663">.</st>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23322">在这个例子中，</st> `<st c="23344">SYS</st>` <st c="23347">标签简要确立了AI作为辅助工具的角色，旨在提供有用的响应。</st>
    <st c="23439">`INST` <st c="23443">标签提供了回答用户问题的具体指令，这些指令包含在</st> `<st c="23547">INST</st>`
    <st c="23551">块中。</st> **<st c="23559">SYS</st>** <st c="23562">用作**<st c="23590">系统</st>**
    <st c="23596">或**<st c="23600">系统消息</st>**的简称，而**<st c="23622">INST</st>** <st
    c="23626">用于代替**<st c="23651">指令</st>**<st c="23663">。</st>
- en: <st c="23664">When</st> <st c="23670">designing prompts for RAG applications,
    it’s</st> <st c="23715">crucial to consider the specific requirements and best
    practices associated with the chosen LLM to ensure optimal performance and results.</st>
    <st c="23854">All of the most well-known models have prompting documentation that
    can explain what you need to do if you</st> <st c="23961">use them.</st>
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23664">在设计用于RAG应用的提示时，考虑与所选LLM相关的具体要求和最佳实践至关重要，以确保最佳性能和结果。</st> <st c="23715">所有最著名的模型都有提示文档，可以解释如果你使用它们时需要做什么。</st>
    <st c="23854">所有最著名的模型都有提示文档，可以解释如果你使用它们时需要做什么。</st> <st c="23961">如果你使用它们。</st>
- en: <st c="23970">Now, let’s put all of the concepts we’ve covered in the first
    part of this chapter into practice with a</st> <st c="24075">code lab!</st>
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="23970">现在，让我们将本章第一部分所涵盖的所有概念通过一个</st> <st c="24075">代码实验室</st>付诸实践！
- en: <st c="24084">Code lab 13.1 – Custom prompt template</st>
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="24084">代码实验室 13.1 – 自定义提示模板</st>
- en: <st c="24123">The</st> <st c="24127">Prompt template is a class representing
    the mechanism to manage and use prompts in LangChain.</st> <st c="24222">As with
    most templates, there is text provided, as well as variables that represent inputs
    to the template.</st> <st c="24330">Using the</st> `<st c="24340">PromptTemplate</st>`
    <st c="24354">package to manage your prompts ensures that it works well within
    the LangChain ecosystem.</st> <st c="24445">This code builds off the code we completed
    in</st> [*<st c="24491">Chapter 8</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="24500">’s</st> *<st c="24504">8.3 code lab</st>*<st c="24516">, and can be
    found in the</st> `<st c="24542">CHAPTER13</st>` <st c="24551">directory of the
    GitHub repo</st> <st c="24581">as</st> `<st c="24583">CHAPTER13-1_PROMPT_TEMPLATES.ipynb</st>`<st
    c="24618">.</st>
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24123">提示模板是一个表示在LangChain中管理和使用提示的机制的类。</st> <st c="24222">与大多数模板一样，提供了文本，以及代表模板输入的变量。</st>
    <st c="24330">使用`<st c="24340">PromptTemplate</st>` <st c="24354">包来管理你的提示，确保它在LangChain生态系统中运行良好。</st>
    <st c="24445">此代码基于我们在</st> [*<st c="24491">第8章</st>*](B22475_08.xhtml#_idTextAnchor152)<st
    c="24500">的</st> *<st c="24504">8.3代码实验室</st>*<st c="24516">中完成的代码，可以在GitHub仓库的</st>
    `<st c="24542">CHAPTER13</st>` <st c="24551">目录中找到，作为`<st c="24583">CHAPTER13-1_PROMPT_TEMPLATES.ipynb</st>`<st
    c="24618">。</st>
- en: <st c="24619">As a refresher, this is the template we have used</st> <st c="24670">the
    most:</st>
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24619">作为复习，这是我们使用最多的模板：</st>
- en: '[PRE5]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: <st c="24722">Printing this prompt out looks</st> <st c="24754">like this:</st>
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24722">打印这个提示看起来</st> <st c="24754">像这样：</st>
- en: '[PRE6]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: <st c="24993">This is stored in the</st> `<st c="25016">PromptTemplate</st>`
    <st c="25030">object that you can print out.</st> <st c="25062">If you do, you
    will see something</st> <st c="25096">like this:</st>
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="24993">这存储在你可以打印出来的</st> `<st c="25016">PromptTemplate</st>` <st c="25030">对象中。</st>
    <st c="25062">如果你这样做，你会看到类似这样的内容：</st>
- en: '[PRE7]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: <st c="25675">So, as we can</st> <st c="25690">see here, there is more to the
    full</st> `<st c="25726">PromptTemplate</st>` <st c="25740">object than just the
    text and variables.</st> <st c="25782">First, we can say that this is a specific
    version of the</st> `<st c="25839">PromptTemplate</st>` <st c="25853">object called
    a</st> `<st c="25870">ChatPromptTemplate</st>` <st c="25888">object, which suggests
    that it is designed to be most useful in chat scenarios.</st> <st c="25969">The
    input variables are</st> `<st c="25993">context</st>` <st c="26000">and</st> `<st
    c="26005">question</st>`<st c="26013">, which show up later in the template string
    itself.</st> <st c="26066">In a moment, we will design a custom template, but
    this particular template comes from the LangChain hub.</st> <st c="26172">You
    can see metadata here indicating the owner, repo, and commit hash associated with</st>
    <st c="26258">the hub.</st>
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="25675">所以，正如我们</st> <st c="25690">在这里可以看到的，完整的</st> `<st c="25726">PromptTemplate</st>`
    <st c="25740">对象不仅仅是文本和变量。</st> <st c="25782">首先，我们可以这样说，这是一个名为</st> `<st c="25839">PromptTemplate</st>`
    <st c="25853">对象的具体版本，称为</st> `<st c="25870">ChatPromptTemplate</st>` <st c="25888">对象，这表明它被设计成在聊天场景中最有用。</st>
    <st c="25969">输入变量是</st> `<st c="25993">context</st>` <st c="26000">和</st> `<st
    c="26005">question</st>`<st c="26013">，它们在模板字符串本身中稍后出现。</st> <st c="26066">稍后，我们将设计一个自定义模板，但这个特定的模板来自LangChain
    hub。</st> <st c="26172">你可以在这里看到与hub相关的元数据，包括所有者、仓库和提交哈希。</st>
- en: <st c="26266">Let’s start our code lab by replacing this prompt template with
    our own</st> <st c="26339">customized template.</st>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26266">让我们通过用我们自己的定制模板替换这个提示模板来开始我们的代码实验室。</st> <st c="26339">：</st>
- en: <st c="26359">We are already importing the LangChain package</st> <st c="26407">for
    this:</st>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26359">我们已经导入LangChain包用于以下目的：</st> <st c="26407">：</st>
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: <st c="26466">There is no need to add any more imports for this!</st> <st c="26518">We
    are going to replace</st> <st c="26542">this code:</st>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26466">对于这个，没有必要添加更多的导入！</st> <st c="26518">我们将替换以下代码：</st> <st c="26542">：</st>
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: <st c="26595">Here is the code</st> <st c="26613">we will be replacing</st>
    <st c="26634">it with:</st>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="26595">这里是我们将要替换的代码</st> <st c="26613">我们将用以下内容替换它：</st>
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: <st c="27076">As you can see, we have customized this prompt template to focus
    specifically on the topic that our data (the Google environmental report) is focused
    on.</st> <st c="27231">We use a personas prompt design pattern to establish a
    role that we want the LLM to play, which we hope will make it more in tune with
    our</st> <st c="27370">specific topic(s).</st>
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27076">正如你所见，我们已经定制了这个提示模板，使其专注于我们的数据（谷歌环境报告）所关注的主题。</st> <st c="27231">我们使用角色提示设计模式来建立一个我们希望LLM扮演的角色，我们希望这会使它更符合我们的</st>
    <st c="27370">特定主题。</st>
- en: <st c="27388">Prompt templates take a dictionary as input, where each key represents
    a variable in the prompt template to fill in.</st> <st c="27506">The output from
    a</st> `<st c="27524">PromptTemplate</st>` <st c="27538">object is a</st> `<st
    c="27551">PromptValue</st>` <st c="27562">variable, which can be passed to an
    LLM or ChatModel instance either directly or as a step in an</st> <st c="27660">LCEL
    chain.</st>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27388">提示模板接受一个字典作为输入，其中每个键代表提示模板中的一个变量，用于填充。</st> <st c="27506">`PromptTemplate`
    <st c="27524">对象输出的结果是</st> `<st c="27551">PromptValue</st>` <st c="27562">变量，可以直接传递给LLM或ChatModel实例，或者作为LCEL链中的一个步骤。</st>
- en: <st c="27671">Print out the prompt object using</st> <st c="27706">this code:</st>
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27671">使用以下代码打印出提示对象：</st> <st c="27706">：</st>
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <st c="27730">The output will</st> <st c="27747">be</st> <st c="27750">as follows:</st>
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="27730">输出将</st> <st c="27747">是</st> <st c="27750">以下内容：</st>
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: <st c="28236">We see that it has captured the input variables without us having
    to explicitly</st> <st c="28317">state them:</st>
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28236">我们看到它已经捕获了输入变量，而无需我们明确地</st> <st c="28317">声明它们：</st>
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: <st c="28368">We can print out just the text using</st> <st c="28406">this line:</st>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28368">我们可以使用这条命令来打印出文本：</st> <st c="28406">：</st>
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: <st c="28439">This gives you a more human-readable version of the last output
    we showed you, but just the</st> <st c="28532">prompt itself.</st>
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28439">这会给你一个比我们之前展示的输出更易于阅读的版本，但只包含</st> <st c="28532">提示本身。</st>
- en: <st c="28546">You will notice that just below this, we already have a customized
    prompt template focused on determining our</st> <st c="28657">relevance score:</st>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="28546">你将注意到，就在这个下面，我们已经有了一个针对确定我们的</st> <st c="28657">相关性分数</st>的定制提示模板：</st>
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: <st c="29084">If you run the remainder of the code, you can see the difference
    it makes to the final outcome of the RAG application.</st> <st c="29204">This
    prompt template is considered a String prompt template, meaning it is created
    using a plain string that contains the prompt text along with placeholders for
    dynamic content (e.g.,</st> `<st c="29389">{question}</st>` <st c="29399">and</st>
    `<st c="29404">{retrieved_context}</st>`<st c="29423">).</st> <st c="29427">You
    can also format with the</st> `<st c="29456">ChatPromptTemplate</st>` <st c="29474">object,
    which is used to format a list of messages.</st> <st c="29527">It consists of
    a list of</st> <st c="29552">templates itself.</st>
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29084">如果你运行剩余的代码，你可以看到它对RAG应用最终结果的影响。</st> <st c="29204">这个提示模板被认为是字符串提示模板，这意味着它是使用包含提示文本和动态内容占位符的普通字符串创建的（例如，</st>
    `<st c="29389">{question}</st>` <st c="29399">和</st> `<st c="29404">{retrieved_context}</st>`<st
    c="29423">）。</st> <st c="29427">你还可以使用</st> `<st c="29456">ChatPromptTemplate</st>`
    <st c="29474">对象进行格式化，该对象用于格式化消息列表。</st> <st c="29527">它由一系列</st> <st c="29552">模板本身组成。</st>
- en: <st c="29569">Prompt templates</st> <st c="29587">play a key supporting role
    in maximizing the performance of RAG systems.</st> <st c="29660">We will use prompt
    templates as the primary element in the remaining code labs in this chapter.</st>
    <st c="29756">However, we will now shift our focus to a series of concepts to
    keep in mind when we write our prompts.</st> <st c="29860">Our next code lab focuses
    on all of these concepts, starting with</st> <st c="29926">prompt formatting.</st>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29569">提示模板</st> <st c="29587">在最大化RAG系统性能中扮演着关键支持角色。</st> <st c="29660">在本章剩余的代码实验室中，我们将使用提示模板作为主要元素。</st>
    <st c="29756">然而，我们现在将重点转移到我们在编写提示时需要记住的一系列概念上。</st> <st c="29860">我们的下一个代码实验室将专注于所有这些概念，从</st>
    <st c="29926">提示格式化</st>开始。
- en: <st c="29944">Code lab 13.2 – Prompting options</st>
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="29944">代码实验室 13.2 – 提示选项</st>
- en: <st c="29978">This code can be found in</st> <st c="30004">the</st> `<st c="30009">CHAPTER13-2_PROMPT_OPTIONS.ipynb</st>`
    <st c="30041">file in the</st> `<st c="30054">CHAPTER13</st>` <st c="30063">directory
    of the</st> <st c="30081">GitHub repository.</st>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="29978">此代码可在</st> <st c="30004">的</st> `<st c="30009">CHAPTER13-2_PROMPT_OPTIONS.ipynb</st>`
    <st c="30041">文件中找到，位于</st> `<st c="30054">CHAPTER13</st>` <st c="30063">目录下的</st>
    <st c="30081">GitHub仓库。</st>
- en: <st c="30099">Generally, when you approach the design of your prompt, there
    are a variety of general concepts that you will want to keep in mind.</st> <st
    c="30232">These include iterating, summarizing, transforming, and expanding.</st>
    <st c="30299">Each of these concepts has different use cases and they can all
    often be combined in various ways.</st> <st c="30398">You will find it useful,
    when improving your RAG applications, to have this foundational knowledge of how
    to design your prompts.</st> <st c="30528">We will walk through different prompt
    approaches using a real-world scenario wherein you are helping to write prompts
    for the marketing department at your company.</st> <st c="30692">We will start</st>
    <st c="30706">with iterating.</st>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30099">通常，当你接近设计你的提示时，会有许多一般概念是你希望记住的。</st> <st c="30232">这些包括迭代、总结、转换和扩展。</st>
    <st c="30299">这些概念各有不同的用途，并且它们通常可以以各种方式组合。</st> <st c="30398">当你改进你的RAG应用时，了解如何设计你的提示的基础知识将非常有用。</st>
    <st c="30528">我们将通过一个真实场景来介绍不同的提示方法，在这个场景中，你正在帮助公司市场部门编写提示。</st> <st c="30692">我们将从</st>
    <st c="30706">迭代</st>开始。
- en: <st c="30721">Iterating</st>
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="30721">迭代</st>
- en: <st c="30731">This concept is simply focused on iterating</st> <st c="30775">your
    prompt to get better results.</st> <st c="30811">It is rare that your first prompt
    will be the best and final prompt you could use.</st> <st c="30894">This section
    focuses on some basic techniques and concepts to keep in mind to help you quickly
    iterate your prompt to make it more suitable for your</st> <st c="31043">RAG application.</st>
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="30731">这个概念简单来说就是迭代</st> <st c="30775">你的提示以获得更好的结果。</st> <st c="30811">你的第一个提示很少会是最佳和最终的提示。</st>
    <st c="30894">本节将重点介绍一些基本技巧和概念，以帮助您快速迭代提示，使其更适合您的</st> <st c="31043">RAG应用。</st>
- en: <st c="31059">Iterating the tone</st>
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="31059">迭代语气</st>
- en: <st c="31078">Your boss just called.</st> <st c="31102">They have told</st>
    <st c="31117">you that the marketing people are saying that they want to use the
    output from the RAG application in their marketing materials, but that it has
    to be provided in more of a marketing fact sheet format.</st> <st c="31319">No
    problem; we can prompt design that right</st> <st c="31363">in there!</st>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31078">你的老板刚刚打电话。</st> <st c="31102">他们告诉你营销人员表示他们想要在他们的营销材料中使用RAG应用程序的输出，但必须以更营销事实表的形式提供。</st>
    <st c="31319">没问题；我们可以在那里直接设计提示设计！</st>
- en: <st c="31372">We will add a second prompt after the</st> <st c="31411">first
    prompt:</st>
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31372">在第一个提示之后</st> <st c="31411">添加第二个提示：</st>
- en: '[PRE16]{context}[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE16]{上下文}[PRE17]'
- en: <st c="31834">You then need to change the prompt in the</st> `<st c="31877">rag_chain_from_docs</st>`
    <st c="31896">chain to</st> `<st c="31906">prompt2</st>`<st c="31913">. Look just
    past the</st> `<st c="31934">RunnablePassthrough()</st>` <st c="31955">line:</st>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="31834">然后你需要将</st> `<st c="31877">rag_chain_from_docs</st>` <st c="31896">链中的提示更改为</st>
    `<st c="31906">prompt2</st>`<st c="31913">。查看</st> `<st c="31934">RunnablePassthrough()</st>`
    <st c="31955">行之后的代码：</st>
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: <st c="32080">Then, rerun the</st> <st c="32097">code from</st> `<st c="32107">prompt2</st>`
    <st c="32114">down for</st> <st c="32124">this result:</st>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="32080">然后，重新运行</st> <st c="32097">从</st> `<st c="32107">prompt2</st>`
    <st c="32114">向下到</st> <st c="32124">这个结果：</st>
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: <st c="33169">If you read</st> <st c="33181">through the complete output, you
    will notice that this does seem more marketing-oriented.</st> <st c="33272">This
    may be what the marketing team is looking for.</st> <st c="33324">However, you
    just remembered that your boss also said that this was going to go in a small
    square on the website that can only hold 50 words</st> <st c="33465">at most!</st>
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33169">如果你阅读</st> <st c="33181">完整的输出，你会注意到这确实更偏向于营销导向。</st> <st c="33272">这可能正是营销团队所寻找的。</st>
    <st c="33324">然而，你刚刚记得你的老板也说过这将被放在网站上只能容纳50个单词的小方块里</st> <st c="33465">最多！</st>
- en: <st c="33473">Shorten the length</st>
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="33473">缩短长度</st>
- en: <st c="33492">For</st> `<st c="33497">prompt3</st>`<st c="33504">, we just need
    to add</st> <st c="33526">this little snippet:</st> `<st c="33547">Use at most
    50 words</st>`<st c="33567">. That looks</st> <st c="33580">like this:</st>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="33492">对于</st> `<st c="33497">prompt3</st>`<st c="33504">，我们只需要添加</st>
    <st c="33526">这个小的片段：</st> `<st c="33547">最多使用50个单词</st>`<st c="33567">。看起来是这样的：</st>
- en: '[PRE20]{context}[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE20]{上下文}[PRE21]'
- en: <st c="34022">Update the prompt in the chain to</st> `<st c="34057">prompt3</st>`<st
    c="34064">. Run the remaining code, and you will have</st> <st c="34108">this
    output:</st>
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34022">更新链中的提示为</st> `<st c="34057">prompt3</st>`<st c="34064">。运行剩余的代码，你将得到</st>
    <st c="34108">以下输出：</st>
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: <st c="34451">The marketing team</st> <st c="34470">loves your work!</st> <st
    c="34488">Things are great!</st> <st c="34506">Good job!</st> <st c="34516">Time</st>
    <st c="34521">goes by…</st>
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34451">营销团队</st> <st c="34470">喜欢你的工作！</st> <st c="34488">一切都很顺利！</st>
    <st c="34506">做得好！</st> <st c="34516">时间</st> <st c="34521">过得真快…</st>
- en: <st c="34529">A month later, it is decided that instead of focusing on all your
    client’s environmental efforts, it would be best to focus on the</st> <st c="34661">technology-related
    efforts.</st>
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34529">一个月后，决定与其关注你所有客户的环保努力，不如专注于</st> <st c="34661">与技术相关的努力。</st>
- en: <st c="34688">Changing the focus</st>
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="34688">改变焦点</st>
- en: <st c="34707">We want the LLM to focus on aspects</st> <st c="34744">that are
    more specific to technology.</st> <st c="34782">This is for a pamphlet, so it
    can be longer again.</st> <st c="34833">Let’s design</st> `<st c="34846">prompt4</st>`
    <st c="34853">to direct the LLM to change its focus and we will take out the</st>
    <st c="34917">length limitation:</st>
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="34707">我们希望LLM关注更具体于技术的方面。</st> <st c="34744">这是为了小册子，所以它可以再次变长。</st>
    <st c="34833">让我们设计</st> `<st c="34846">prompt4</st>` <st c="34853">来指导LLM改变其焦点，我们将取消</st>
    <st c="34917">长度限制：</st>
- en: '[PRE23]{context}[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE23]{上下文}[PRE24]'
- en: <st c="35499">Again, remember</st> <st c="35515">to update the prompt in the
    chain to</st> `<st c="35553">prompt4</st>` <st c="35560">and then run the rest
    of the code to get</st> <st c="35602">the output:</st>
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="35499">再次提醒</st> <st c="35515">更新链中的提示为</st> `<st c="35553">prompt4</st>`
    <st c="35560">，然后运行剩余的代码以获取</st> <st c="35602">以下输出：</st>
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: <st c="36771">Again, we had to shorten it here in the book, but if you look
    at this in the code, the results are impressive.</st> <st c="36883">There is clearly
    a higher focus on the technological aspects of their environmental aspects.</st>
    <st c="36976">Your marketing team</st> <st c="36996">is impressed!</st>
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="36771">同样，我们在这里的书中不得不缩短它，但如果你在代码中查看，结果令人印象深刻。</st> <st c="36883">显然，对他们的环境方面的技术方面有更高的关注。</st>
    <st c="36976">你的营销团队</st> <st c="36996">印象深刻！</st>
- en: <st c="37009">This was a fun example, but this is not too far from what happens
    when building these types of systems.</st> <st c="37114">In the real world, you
    will likely iterate significantly more times, but taking an iterative approach
    to your prompt design will help you get to a more optimal RAG application just
    as much as any other part of your</st> <st c="37329">RAG system.</st>
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37009">这是一个有趣的例子，但这并不远离构建这些类型系统时发生的事情。</st> <st c="37114">在现实世界中，你可能会迭代更多次，但采用迭代方法来设计提示将帮助你达到更优的RAG应用，就像你的RAG系统的任何其他部分一样。</st>
- en: <st c="37340">Next, let’s</st> <st c="37353">talk about how to take a lot of
    data and compact it into a much smaller amount, also known</st> <st c="37444">as
    summarization.</st>
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37340">接下来，让我们</st> <st c="37353">谈谈如何将大量数据压缩成更小的数据量，这通常被称为摘要。</st>
- en: <st c="37461">Summarizing</st>
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="37461">摘要</st>
- en: <st c="37473">Summarization</st> <st c="37487">is a popular use of RAG.</st>
    <st c="37513">Taking massive amounts</st> <st c="37536">of data that is internal
    to a company and digesting it into smaller, more concise information can be a
    quick and easy way to boost productivity.</st> <st c="37681">This is particularly
    useful for jobs that rely on information or keep up with rapidly changing information.</st>
    <st c="37789">We’ve already seen how to design a prompt to use a word limit, which
    was in</st> `<st c="37865">prompt3</st>`<st c="37872">. However, in this case,
    we are going to focus the LLM more on summarizing the content, rather than trying
    to be an expert or write a marketing piece.</st> <st c="38023">The code is</st>
    <st c="38035">as follows:</st>
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="37473">摘要</st> <st c="37487">是RAG（检索增强生成）的一个流行用途。</st> <st c="37513">将公司内部的大量数据消化成更小、更简洁的信息，可以是一种快速简单的方法来提高生产力。</st>
    <st c="37681">这对于依赖信息或需要跟上快速变化信息的职位尤其有用。</st> <st c="37789">我们已经看到了如何设计一个提示来使用字数限制，这在</st>
    `<st c="37865">prompt3</st>`<st c="37872">中。</st> 然而，在这种情况下，我们将更多地关注LLM的摘要内容，而不是试图成为一个专家或撰写营销文章。</st>
    <st c="38023">代码如下：</st> <st c="38035">如下：</st>
- en: '[PRE26]{context}[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE26]{上下文}[PRE27]'
- en: <st c="38395">Update to</st> `<st c="38406">prompt5</st>` <st c="38413">in the
    chain, and then run the rest of the code again.</st> <st c="38469">The results
    are</st> <st c="38485">as follows:</st>
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38395">更新链中的</st> `<st c="38406">prompt5</st>` <st c="38413">，然后再次运行剩余的代码。</st>
    <st c="38469">结果如下：</st>
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: <st c="38687">OK, this is great, short, and summarizing.</st> <st c="38731">Nothing
    but</st> <st c="38743">the facts!</st>
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38687">好的，这很好，简短，并且是摘要。</st> <st c="38731">只有事实！</st>
- en: <st c="38753">The next example is</st> <st c="38773">another situation where
    we can focus the LLM, but with the added effort</st> <st c="38846">of summarizing.</st>
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38753">下一个例子是</st> <st c="38773">另一种我们可以关注LLM的情况，但增加了摘要的努力。</st>
- en: <st c="38861">Summarizing with a focus</st>
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="38861">专注于摘要的摘要</st>
- en: <st c="38886">For</st> `<st c="38891">prompt6</st>`<st c="38898">, we are going
    to</st> <st c="38915">maintain most of what we had in the previous prompt.</st>
    <st c="38969">However, we will try to focus the LLM specifically on the eco-friendly
    aspects of</st> <st c="39051">their products:</st>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="38886">对于</st> `<st c="38891">prompt6</st>`<st c="38898">，我们将</st> <st
    c="38915">保留之前提示中的大部分内容。</st> <st c="38969">然而，我们将尝试将LLM特别集中在他们产品的环保方面：</st>
- en: '[PRE29]{context}[PRE30]'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE29]{上下文}[PRE30]'
- en: <st c="39496">Update the prompt in the chain to</st> `<st c="39531">prompt6</st>`<st
    c="39538">, and then run the code to get</st> <st c="39569">this output:</st>
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39496">将链中的提示更新为</st> `<st c="39531">prompt6</st>`<st c="39538">，然后运行代码以获得</st>
    <st c="39569">以下输出：</st>
- en: '[PRE31]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: <st c="39755">This is short, and if you check it against the more verbose descriptions,
    it does seem to focus specifically on the products that were featured in the PDF.</st>
    <st c="39912">This was a pretty good result, but often when you ask for a summary,
    even when you focus the LLM on specific aspects, the LLM can still include information
    you did not want to be included.</st> <st c="40101">To combat this, we</st> <st
    c="40119">turn to the</st> *<st c="40132">extract</st>* <st c="40139">method.</st>
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="39755">这是简短的，如果您将其与更冗长的描述进行比较，它似乎确实专注于PDF中展示的产品。</st> <st c="39912">这是一个相当好的结果，但通常当您请求摘要时，即使您将LLM聚焦于特定方面，LLM仍然可能包含您不希望包含的信息。</st>
    <st c="40101">为了应对这种情况，我们</st> <st c="40119">转向</st> *<st c="40132">extract</st>*
    <st c="40139">方法。</st>
- en: <st c="40147">extract instead of summarize</st>
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="40147">extract instead of summarize</st>
- en: <st c="40176">If you run into the common problem with the</st> <st c="40220">summary
    including too much unwanted information, try using the word</st> *<st c="40289">extract</st>*
    <st c="40296">rather than</st> *<st c="40309">summarize</st>*<st c="40318">. This
    may seem like a small bit of nuance, but it can make a big difference to the LLM.</st>
    *<st c="40407">Extract</st>* <st c="40414">gives the impression that you are pulling
    specific information out, rather than just trying to capture the overall data
    in the entire text.</st> <st c="40555">The LLM does not miss this nuance, and
    this can be a good technique to help you avoid this challenge that summarization
    sometimes poses.</st> <st c="40692">We will design</st> `<st c="40707">prompt7</st>`
    <st c="40714">with this change</st> <st c="40732">in mind:</st>
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="40176">如果您遇到常见的总结包含过多不必要信息的问题，尝试使用单词</st> *<st c="40289">extract</st>*
    <st c="40296">而不是</st> *<st c="40309">summarize</st>*<st c="40318">。这看起来可能只是细微的差别，但它对LLM来说可能有很大的影响。</st>
    *<st c="40407">Extract</st>* <st c="40414">给人一种您正在提取特定信息的印象，而不是仅仅试图捕捉整个文本中的整体数据。</st>
    <st c="40555">LLM不会错过这个细微差别，这可以是一个很好的技巧，帮助您避免总结有时带来的挑战。</st> <st c="40692">我们将考虑到这个变化来设计</st>
    `<st c="40707">prompt7</st>` <st c="40714">：</st>
- en: '[PRE32]{context}[PRE33]'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE32]{context}[PRE33]'
- en: <st c="41183">Update the prompt in the chain to</st> `<st c="41218">prompt7</st>`<st
    c="41225">, and then run the code to get</st> <st c="41256">this output:</st>
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41183">将链中的提示更新为</st> `<st c="41218">prompt7</st>`<st c="41225">，然后运行代码以获取</st>
    <st c="41256">以下输出：</st>
- en: '[PRE34]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: <st c="41494">This is a slightly different response compared to</st> `<st c="41545">prompt6</st>`<st
    c="41552">, but we already had a well-focused result.</st> <st c="41596">When
    your summary responses have unnecessary data, try this technique to help improve</st>
    <st c="41682">your results.</st>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41494">这与</st> `<st c="41545">prompt6</st>`<st c="41552">的响应略有不同，但我们已经得到了一个很好的聚焦结果。</st>
    <st c="41596">当您的总结响应包含不必要的数据时，尝试这个技巧来帮助提高</st> <st c="41682">您的结果。</st>
- en: <st c="41695">Iterating and</st> <st c="41709">summarization are not the only
    concepts to understand to improve your prompting efforts, however.</st> <st c="41808">We
    will next talk about how to utilize your RAG application to infer information
    from your</st> <st c="41899">existing data.</st>
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41695">迭代和总结并不是提高提示努力所需要理解的唯一概念。</st> <st c="41709">我们将接下来讨论如何利用您的RAG应用从您的</st>
    <st c="41899">现有数据中推断信息。</st>
- en: <st c="41913">Inference</st>
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="41913">推理
- en: <st c="41923">At the root of inference, you are</st> <st c="41957">asking the
    model to look at your data and provide some sort of additional analysis.</st>
    <st c="42042">This often involves extracting labels, names, and topics, or even
    determining the sentiment of the text.</st> <st c="42147">These capabilities have
    far-reaching implications for RAG applications in that they enable tasks that
    were considered to solely be in the domain of human readers not too long ago.</st>
    <st c="42327">Let’s start with a simple Boolean-style sentiment analysis, wherein
    we consider whether a text is positive</st> <st c="42434">or negative:</st>
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="41923">在推理的根源，您是</st> <st c="41957">要求模型查看您的数据并提供某种额外的分析。</st> <st c="42042">这通常涉及提取标签、名称和主题，甚至确定文本的情感。</st>
    <st c="42147">这些能力对RAG应用具有深远的影响，因为它们使那些不久前被认为仅属于人类读者领域的工作成为可能。</st> <st c="42327">让我们从一个简单的布尔式情感分析开始，其中我们考虑文本是积极的</st>
    <st c="42434">还是消极的：</st>
- en: '[PRE35]{context}[PRE36]'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE35]{context}[PRE36]'
- en: <st c="43026">In this code, we build off the summary from the previous prompt
    but add an</st> *<st c="43102">analysis</st>* <st c="43110">of the sentiment of
    the data that the LLM is digesting as well.</st> <st c="43175">In this case, it
    determines the sentiment to</st> <st c="43220">be</st> <st c="43222">positive:</st>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43026">在这段代码中，我们基于前一个提示的总结，但增加了一个</st> *<st c="43102">分析</st>* <st c="43110">LLM正在消化的数据的情感</st>
    <st c="43175">在这种情况下，它确定情感为</st> <st c="43220">积极</st> <st c="43222">：</st>
- en: '[PRE37]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: <st c="43456">Another common application in a similar analytic vane as this
    is extracting specific data from</st> <st c="43552">the context.</st>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43456">在类似的分析领域中，另一个常见的应用是从</st> <st c="43552">上下文中</st> 提取特定数据。
- en: <st c="43564">Extracting key data</st>
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="43564">提取关键数据</st>
- en: <st c="43584">As a reference point, you are now tasked to</st> <st c="43628">identify
    any specific products your customer mentions in their documentation in relation
    to their environmental efforts.</st> <st c="43750">In this case, Google (the client)
    has many products, but they only mention a handful of them in this document.</st>
    <st c="43861">How would you quickly pull those products out and identify them?</st>
    <st c="43926">Let’s try this with</st> <st c="43946">our prompt:</st>
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="43584">作为一个参考点，你现在被要求识别客户在其与环境努力相关的文档中提到的任何特定产品。</st> <st c="43628">在这种情况下，谷歌（客户）有许多产品，但在这个文档中只提到了其中的一小部分。</st>
    <st c="43750">你将如何快速提取这些产品并识别它们？</st> <st c="43926">让我们用</st> <st c="43946">我们的提示</st>
    来试试：
- en: '[PRE38]{context}[PRE39]'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE38]{上下文}[PRE39]'
- en: <st c="44663">In this code, we continue to build off previous prompts, but instead
    of asking for a sentiment analysis, we are asking for the list of products related
    to the text we retrieved.</st> <st c="44842">The GPT-4o-mini model we are using
    is successful in following these instructions, listing each</st> <st c="44937">of
    the products specifically named in</st> <st c="44975">the text:</st>
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="44663">在这段代码中，我们继续基于之前的提示进行构建，但不是要求情感分析，而是要求与我们所检索到的文本相关的产品列表。</st> <st
    c="44842">我们使用的GPT-4o-mini模型成功地遵循了这些指示，列出了文本中特别命名的每个</st> <st c="44937">产品：</st>
- en: '[PRE40]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: <st c="45258">Once again, the LLM is able to handle everything we ask of it.</st>
    <st c="45322">However, sometimes we just want to get an overall sense of the topic.</st>
    <st c="45392">We will discuss the concept of inference using the</st> <st c="45443">LLM
    next.</st>
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45258">再次，LLM能够处理我们所要求的一切。</st> <st c="45322">然而，有时我们只想对主题有一个整体的感觉。</st>
    <st c="45392">我们将使用LLM来讨论推理的概念。</st>
- en: <st c="45452">Inferring topics</st>
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="45452">推断主题</st>
- en: <st c="45469">You might think of this as an extreme</st> <st c="45508">case
    of summarization.</st> <st c="45531">In this example, we are taking thousands
    of words and summarizing them into one short set of topics.</st> <st c="45632">Can
    this be done?</st> <st c="45650">Let’s try!</st> <st c="45661">We’ll be starting
    with</st> <st c="45684">this code:</st>
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="45469">你可能认为这是一个极端的</st> <st c="45508">总结案例。</st> <st c="45531">在这个例子中，我们正在将数千个单词总结成一组简短的主题。</st>
    <st c="45632">这能行吗？</st> <st c="45650">让我们试试！</st> <st c="45661">我们将从</st> <st
    c="45684">以下代码</st> 开始：
- en: '[PRE41]{context}[PRE42]'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE41]{上下文}[PRE42]'
- en: <st c="46423">Here, we use a similar approach as in previous prompts, but instead
    of asking for the product list, we are asking for the list of at least eight topics
    related to the text we retrieved.</st> <st c="46610">Once again, the GPT-4o mini
    model we use is successful in following these instructions, listing eight</st>
    <st c="46711">highly relevant topics specifically covered in</st> <st c="46759">the
    text:</st>
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="46423">在这里，我们使用与之前提示类似的方法，但不是要求产品列表，而是要求与我们所检索到的文本相关的至少八个主题列表。</st> <st
    c="46610">再次，我们使用的GPT-4o mini模型成功地遵循了这些指示，列出了八个</st> <st c="46711">与文本特别相关的、高度相关的主题：</st>
- en: '[PRE43]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: <st c="47145">We have covered iterating, summarization, and inference, which
    all show great promise for improving your prompting efforts.</st> <st c="47270">Yet
    another concept we will cover</st> <st c="47304">is transformation.</st>
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47145">我们已经涵盖了迭代、总结和推理，这些都显示出极大的潜力来提高你的提示效果。</st> <st c="47270">我们将要介绍的概念还有</st>
    <st c="47304">转换</st>。
- en: <st c="47322">Transformation</st>
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="47322">转换</st>
- en: <st c="47337">Transformation</st> <st c="47352">is about taking your current
    data and transforming</st> <st c="47403">it into a different state or format.</st>
    <st c="47441">A very common example is language translation, but there are many
    others, including putting data into a certain coding format such as JSON or HTML.</st>
    <st c="47589">You can also apply transformations such as checking for spelling
    or</st> <st c="47657">grammar errors.</st>
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47337">转换</st> <st c="47352">是将您当前的数据转换</st> <st c="47403">成不同的状态或格式。</st>
    <st c="47441">一个非常常见的例子是语言翻译，但还有许多其他情况，包括将数据放入某种编码格式，如JSON或HTML。</st> <st c="47589">您还可以应用转换，如检查拼写或</st>
    <st c="47657">语法错误。</st>
- en: <st c="47672">We will begin with</st> <st c="47692">language translation.</st>
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47672">我们将从</st> <st c="47692">语言翻译</st>开始。</st>
- en: <st c="47713">Language transformation (translation)</st>
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="47713">语言转换（翻译）</st>
- en: <st c="47751">Marketing called</st> <st c="47768">again.</st> <st c="47776">The
    work you have done so far has been stellar but now things are ramping up and we
    are going international!</st> <st c="47885">The first international markets we
    have chosen include speakers of Spanish and French.</st> <st c="47972">A new investor
    in our firm is also a big fan of anything that has to do with pirates, so yes,
    we are going to cover that dialect as well!</st> <st c="48110">Since we are talking
    about</st> *<st c="48137">transformations</st>*<st c="48152">, we call this</st>
    <st c="48167">language transformation, but it is also very common to see the term</st>
    *<st c="48235">translation</st>* <st c="48246">used in this context.</st> <st
    c="48269">Let’s</st> <st c="48275">get started:</st>
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="47751">营销部门又</st> <st c="47768">打电话来了。</st> <st c="47776">你到目前为止所做的工作非常出色，但现在事情正在加速，我们将走向国际市场了！</st>
    <st c="47885">我们选择的第一批国际市场包括西班牙语和法语的使用者。</st> <st c="47972">我们公司的一位新投资者也是任何与海盗有关的东西的大粉丝，所以是的，我们也将涵盖这种方言！</st>
    <st c="48110">既然我们在谈论</st> *<st c="48137">转换</st>*<st c="48152">，我们称之为</st> <st
    c="48167">语言转换，但在这个上下文中，看到术语</st> *<st c="48235">翻译</st>* <st c="48246">也很常见。</st>
    <st c="48269">让我们</st> <st c="48275">开始吧：</st>
- en: '[PRE44]{context}[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE44]{上下文}[PRE45]'
- en: '<st c="48953">In this code, we build</st> <st c="48977">off former prompts,
    but then task the LLM to generate four different versions of the short summary:
    English, Spanish, French, and English pirate.</st> <st c="49123">Clearly, we should
    be speaking in pirate more often, as this is</st> <st c="49186">the</st> <st c="49191">most
    entertaining:</st>'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="48953">在这段代码中，我们基于之前的提示，但随后要求LLM生成四种不同版本的简短摘要：英语、西班牙语、法语和英语海盗语。</st>
    <st c="49123">显然，我们应该更经常地说海盗话，因为这是</st> <st c="49186">最</st> <st c="49191">有趣的：</st>
- en: '[PRE46]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: <st c="50132">Language translation is a popular use of RAG, but there are other
    transformations that could be of use.</st> <st c="50237">Let’s review an example
    of adding a different tone to</st> <st c="50291">a summary.</st>
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50132">语言翻译是RAG的一个流行用途，但还有其他转换可能很有用。</st> <st c="50237">让我们回顾一下向摘要添加不同语气的例子。</st>
- en: <st c="50301">Tone transformation</st>
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="50301">语气转换</st>
- en: <st c="50321">Our efforts so far</st> <st c="50341">have been successful in
    writing summaries and even</st> <st c="50391">marketing copy, but now we need
    to expand to other channels, such as email, and give our summaries a more friendly
    tone to match this format.</st> <st c="50534">For this, we will apply</st> <st
    c="50558">tone transformation:</st>
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="50321">我们到目前为止的努力</st> <st c="50341">在撰写摘要甚至</st> <st c="50391">营销文案方面都取得了成功，但现在我们需要扩展到其他渠道，如电子邮件，并给我们的摘要一个更友好的语气以适应这种格式。</st>
    <st c="50534">为此，我们将应用</st> <st c="50558">语气转换：</st>
- en: '[PRE47]{context}[PRE48]'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE47]{上下文}[PRE48]'
- en: <st c="51132">Here, we continue the original</st> <st c="51164">summarization,
    but then we task the LLM with writing an email using the same information with
    a</st> <st c="51260">casual</st> <st c="51266">tone:</st>
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51132">在这里，我们继续原来的</st> <st c="51164">总结，但随后我们要求LLM使用相同的信息写一封</st> <st
    c="51260">轻松</st> <st c="51266">语气</st>的电子邮件：</st>
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: <st c="51916">As we continue to see with these examples, there are so many ways
    LLMs can be used to improve RAG applications.</st> <st c="52029">Other options
    that are less applicable in our example (but still very valuable in other scenarios)
    include translating into a specific coding format, or from one coding format to
    another.</st> <st c="52217">Spelling and grammar checks are also popular transformations</st>
    <st c="52277">that</st> <st c="52283">can</st> <st c="52287">be applied.</st>
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="51916">正如我们通过这些例子所看到的，LLM可以以多种方式用于改进RAG应用。</st> <st c="52029">在我们的例子中不太适用（但在其他场景中仍然非常有价值）的其他选项包括翻译成特定的编码格式，或者从一个编码格式转换到另一个编码格式。</st>
    <st c="52217">拼写和语法检查也是可以应用的热门转换。</st> <st c="52277">这些。</st> <st c="52283">可以。</st>
    <st c="52287">应用。</st>
- en: '<st c="52298">We have covered iterating, summarization, inference, and transformation.</st>
    <st c="52372">There is one more concept we will cover to conclude this code</st>
    <st c="52434">lab: expansion.</st>'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52298">我们已经涵盖了迭代、总结、推理和转换。</st> <st c="52372">为了结束这个代码实验，我们还将介绍一个概念。</st>
    <st c="52434">扩展。</st>
- en: <st c="52449">Expansion</st>
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <st c="52449">扩展</st>
- en: '<st c="52459">The goal of expansion, in</st> <st c="52485">many ways, can be
    thought of as a reverse of the goals of previous concepts we covered, such as
    summarization.</st> <st c="52597">With summarization, we are taking a lot of data
    and consolidating it into smaller amounts of data while trying to preserve the
    meaning of that data.</st> <st c="52746">Expansion seeks to do the opposite, taking
    a small amount of data and</st> *<st c="52816">expanding</st>* <st c="52825">it
    to a larger set of information.</st> <st c="52861">Let’s walk through a scenario
    where this can be implemented: the expansion of a</st> <st c="52941">short summary.</st>'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52459">从许多方面来说，扩展的目标可以被视为我们之前介绍的概念（如总结）目标的一种反向。</st> <st c="52485">在总结中，我们正在将大量数据合并成更小的数据量，同时试图保留数据的含义。</st>
    <st c="52597">扩展试图做的是相反的，它将一小部分数据扩展到更大的信息集。</st> <st c="52746">让我们通过一个可以实施这种扩展的场景来了解一下：简短总结的扩展。</st>
    <st c="52816">扩展</st> <st c="52825">它</st> <st c="52861">让我们通过一个可以实施这种扩展的场景来了解一下：简短总结的扩展。</st>
    <st c="52941">简短总结的扩展。</st>
- en: <st c="52955">Expand on a short text</st>
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: <st c="52955">扩展简短文本</st>
- en: <st c="52978">Our efforts continue to grow!</st> <st c="53009">Our latest task
    is to take the environmental concepts we have been discussing and start promoting
    their benefits to investors in our client.</st> <st c="53150">In this next prompt,
    we will take the short summary we have been given and assume for a moment that
    this is all the content that we have available.</st> <st c="53298">We will then
    ask the LLM to expand on that short summary with a focus on how it can appeal
    to investors.</st> <st c="53403">Let’s see what it can come</st> <st c="53430">up
    with:</st>
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="52978">我们的努力持续增长！</st> <st c="53009">我们最新的任务是向我们的客户推广我们一直在讨论的环境概念的好处。</st>
    <st c="53150">在接下来的提示中，我们将根据我们得到的简短总结，暂时假设这就是我们拥有的全部内容。</st> <st c="53298">然后我们将要求LLM在此基础上进行扩展，重点关注它如何吸引投资者。</st>
    <st c="53403">让我们看看它能提出什么：</st> <st c="53430">：</st>
- en: '[PRE50]{context}[PRE51]'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE50]{上下文}[PRE51]'
- en: <st c="54079">In a real-world scenario, you</st> <st c="54109">will probably
    mostly not have the original context available when you want the LLM to expand
    on your data.</st> <st c="54217">Therefore, by telling the LLM to limit the source
    for its expansion to just what is provided in the summary, we are simulating this
    scenario more accurately.</st> <st c="54375">Our LLM does not disappoint us, as
    it provides an expanded description of the environmental efforts mentioned in
    the original summary, and then tailors it</st> <st c="54530">for investors:</st>
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="54079">在现实世界的场景中，当你希望LLM扩展你的数据时，你很可能大部分情况下都没有原始的上下文。</st> <st c="54109">因此，通过告诉LLM将其扩展的来源仅限于总结中提供的内容，我们更准确地模拟了这种场景。</st>
    <st c="54217">我们的LLM没有让我们失望，因为它提供了对原始总结中提到的环境努力的扩展描述，并针对投资者进行了调整。</st> <st c="54375">然后，它针对投资者进行了调整：</st>
- en: '[PRE52]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: <st c="55585">That is just one example of</st> <st c="55614">how the expansion
    concept can be utilized.</st> <st c="55657">Consider how and when an expansion
    of your data in your RAG application can be utilized</st> <st c="55745">as well.</st>
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55585">这只是扩展概念可以应用的一个例子。</st> <st c="55614">考虑一下在你的RAG应用中如何以及何时利用数据扩展。</st>
    <st c="55657">以及。</st>
- en: '<st c="55753">This concludes all the key concepts for how to improve your prompt
    design: iteration, summarization, inference, transformation, and expansion.</st>
    <st c="55897">These concepts form the foundation of many of the more in-depth
    and complex concepts that can make your RAG application more effective.</st> <st
    c="56033">Consider this the start of your knowledge in this area and continue
    to track advances and new techniques as they</st> <st c="56146">become known.</st>'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="55753">这总结了如何改进你的提示设计的关键概念：迭代、总结、推断、转换和扩展。</st> <st c="55897">这些概念构成了许多更深入和复杂概念的基础，可以使你的RAG应用更加有效。</st>
    <st c="56033">把这视为你在该领域的知识起点，并继续跟踪该领域的进步和新技术，随着它们的被发现。</st>
- en: <st c="56159">Summary</st>
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: <st c="56159">摘要</st>
- en: <st c="56167">In this chapter, we explored the crucial role of prompt engineering
    in enhancing the performance and effectiveness of RAG systems.</st> <st c="56299">By
    strategically designing and refining input prompts, we can improve the retrieval
    of relevant information and subsequently enhance the quality of generated text.</st>
    <st c="56463">We discussed various prompt design techniques, such as shot design,
    chain-of-thought prompting, personas, and knowledge augmentation, which can be
    applied to optimize</st> <st c="56630">RAG applications.</st>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56167">在本章中，我们探讨了提示工程在增强RAG系统性能和有效性中的关键作用。</st> <st c="56299">通过战略性地设计和精炼输入提示，我们可以提高相关信息的检索，从而提高生成文本的质量。</st>
    <st c="56463">我们讨论了各种提示设计技术，例如射击设计、思维链提示、角色扮演和知识增强，这些技术可以应用于优化</st> <st c="56630">RAG应用。</st>
- en: <st c="56647">Throughout the chapter, we discussed the fundamental concepts
    of prompt design, including the importance of being concise, specific, and well-defined,
    as well as the need to iterate gradually and use clear separators.</st> <st c="56866">We
    also highlighted the fact that different LLMs require different prompts, as well
    as the importance of adapting prompts to the specific model</st> <st c="57010">being
    used.</st>
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="56647">在本章中，我们讨论了提示设计的根本概念，包括简洁、具体和明确的重要性，以及逐步迭代和使用清晰分隔符的必要性。</st> <st
    c="56866">我们还强调了不同LLM需要不同的提示，以及适应特定模型使用的提示的重要性。</st>
- en: <st c="57021">Through a series of code labs, we learned how to create custom
    prompt templates using the</st> `<st c="57112">PromptTemplate</st>` <st c="57126">class
    in LangChain, as well as how to apply various prompting concepts to improve our
    RAG efforts.</st> <st c="57226">These concepts included iterating to refine prompts,
    summarizing to condense information, inferring to extract additional insights,
    transforming data into different formats or tones, and expanding on short summaries
    to generate more comprehensive descriptions.</st> <st c="57487">We also explored
    the use of prompt parameters, such as temperature, top-p, and seed, to control
    the randomness and determinism of</st> <st c="57617">LLM outputs.</st>
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57021">通过一系列代码实验室，我们学习了如何使用LangChain中的`<st c="57112">PromptTemplate</st>`
    <st c="57126">类创建自定义提示模板，以及如何将各种提示概念应用于提高我们的RAG工作。</st> <st c="57226">这些概念包括迭代以精炼提示、总结以浓缩信息、推断以提取额外见解、将数据转换为不同的格式或语气，以及扩展简短摘要以生成更全面的描述。</st>
    <st c="57487">我们还探讨了使用提示参数，如温度、top-p和种子，来控制LLM输出的随机性和确定性。</st>
- en: <st c="57629">By leveraging the techniques and concepts covered in this chapter,
    we can significantly enhance the performance of our RAG applications, making them
    more effective at retrieving relevant information, generating high-quality text,
    and adapting to specific use cases.</st> <st c="57896">As the field of prompt
    engineering continues to evolve, staying up to date with the latest techniques
    and best practices will be essential for maximizing the potential of RAG systems
    in</st> <st c="58082">various domains.</st>
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="57629">通过利用本章介绍的技术和概念，我们可以显著提高RAG应用的表现，使它们在检索相关信息、生成高质量文本和适应特定用例方面更加有效。</st>
    <st c="57896">随着提示工程领域的不断发展，跟上最新的技术和最佳实践对于在各个领域最大化RAG系统的潜力至关重要。</st>
- en: <st c="58098">In our next and final chapter, we will discuss some more advanced
    techniques that you can use to make potential significant improvements to your</st>
    <st c="58244">RAG application!</st>
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: <st c="58098">在我们下一章和最后一章中，我们将讨论一些更高级的技术，你可以使用这些技术对你的RAG应用进行潜在的显著改进！</st>
