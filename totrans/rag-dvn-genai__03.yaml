- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building Index-Based RAG with LlamaIndex, Deep Lake, and OpenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indexes increase precision and speed performances, but they offer more than
    that. Indexes transform retrieval-augmented generative AI by adding a layer of
    transparency. With an index, the source of a response generated by a RAG model
    is fully traceable, offering visibility into the precise location and detailed
    content of the data used. This improvement not only mitigates issues like bias
    and hallucinations but also addresses concerns around copyright and data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore how indexed data allows for greater control over
    generative AI applications. If the output is unsatisfactory, it’s no longer a
    mystery why, since the index allows us to identify and examine the exact data
    source of the issue. This capability makes it possible to refine data inputs,
    tweak system configurations, or switch components, such as vector store software
    and generative models, to achieve better outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin the chapter by laying out the architecture of an index-based RAG
    pipeline that will enhance speed, precision, and traceability. We will show how
    LlamaIndex, Deep Lake, and OpenAI can be seamlessly integrated without having
    to create all the necessary functions ourselves. This provides a solid base to
    start building from. Then, we’ll introduce the main indexing types we’ll use in
    our programs, such as vector, tree, list, and keyword indexes. Then, we will build
    a domain-specific drone technology LLM RAG agent that a user can interact with.
    Drone technology is expanding to all domains, such as fire detection, traffic
    information, and sports events; hence, I’ve decided to use it in our example.
    The goal of this chapter is to prepare an LLM drone technology dataset that we
    will enhance with multimodal data in the next chapter. We will also illustrate
    the key indexing types in code.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll be adept at manipulating index-based RAG
    through vector stores, datasets, and LLMs, and know how to optimize retrieval
    systems and ensure full traceability. You will discover how our integrated toolkit—combining
    LlamaIndex, Deep Lake, and OpenAI—not only simplifies technical complexities but
    also frees your time to develop and hone your analytical skills, enabling you
    to dive deeper into understanding RAG-driven generative AI.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a semantic search engine with a LlamaIndex framework and indexing methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Populating Deep Lake vector stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of LlamaIndex, Deep Lake, and OpenAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Score ranking and cosine similarity metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metadata enhancement for traceability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query setup and generation configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing automated document ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector, tree, list, and keyword indexing types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why use index-based RAG?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Index-based search takes advanced RAG-driven generative AI to another level.
    It increases the speed of retrieval when faced with large volumes of data, taking
    us from raw chunks of data to organized, indexed nodes that we can trace from
    the output back to the source of a document and its location.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s understand the differences between a vector-based similarity search and
    an index-based search by analyzing the architecture of an index-based RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Index-based search is faster than vector-based search in RAG because it directly
    accesses relevant data using indices, while vector-based search sequentially compares
    embeddings across all records. We implemented a vector-based similarity search
    program in *Chapter 2*, *RAG Embedding Vector Stores with Deep Lake and OpenAI*,
    as shown in *Figure 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We collected and prepared data in *Pipeline #1: Data Collection and Preparation*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We embedded the data and stored the prepared data in a vector store in *Pipeline
    #2: Embeddings and vector store*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We then ran retrieval queries and generative AI with *Pipeline #3* to process
    user input, run retrievals based on vector similarity searches, augment the input,
    generate a response, and apply performance metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach is flexible because it gives you many ways to implement each component,
    depending on the needs of your project.
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a process  Description automatically generated](img/B31169_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: RAG-driven generative AI pipelines, as described in Chapter 2,
    with additional functionality'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, implementing index-based searches will take us into the future of
    AI, which will be faster, more precise, and traceable. We will follow the same
    process as in *Chapter 2*, with three pipelines, to make sure that you are ready
    to work in a team in which the tasks are specialized. Since we are using the same
    pipelines as in *Chapter 2*, let’s add the functions from that chapter to them,
    as shown in *Figure 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline Component #1 and D2-Index**: We will collect data and preprocess
    it. However, this time, we will prepare the data source one document at a time
    and store them in separate files. We will then add their name and location to
    the metadata we load into the vector store. The metadata will help us trace a
    response all the way back to the exact file that the retrieval function processed.
    We will have a direct link from a response to the data that it was based on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline Component #2 and D3-Index**: We will load the data into a vector
    store by installing and using the innovative integrated `llama-index-vector-stores-deeplake`
    package, which includes everything we need in an optimized starter scenario: chunking,
    embedding, storage, and even LLM integration. We have everything we need to get
    to work on index-based RAG in a few lines of code! This way, once we have a solid
    program, we can customize and expand the pipelines as we wish, as we did, for
    example, in *Chapter 2*, when we explicitly chose the LLM models and chunking
    sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline Component #3 and D4-Index**: We will load the data in a dataset
    by installing and using the innovative integrated `llama-index-vector-stores-deeplake`
    package, which includes everything we need to get indexed-based retrieval and
    generation started, including automated ranking and scoring. The process is seamless
    and extremely productive. We’ll leverage LlamaIndex with Deep Lake to streamline
    information retrieval and processing. An integrated retriever will efficiently
    fetch relevant data from the Deep Lake repository, while an LLM agent will then
    intelligently synthesize and interact with the retrieved information to generate
    meaningful insights or actions. Indexes are designed for fast retrieval, and we
    will implement several indexing methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline Component #3 and E1-Index**: We will add a time and score metric
    to evaluate the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous chapter, we implemented vector-based similarity search and
    retrieval. We embedded documents to transform data into high-dimensional vectors.
    Then, we performed retrieval by calculating distances between vectors. In this
    chapter, we will go further and create a vector store. However, we will load the
    data into a dataset that will be reorganized using retrieval indexing types. *Table
    3.1* shows the differences between vector-based and index-based search and retrieval
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Vector-based similarity search and retrieval** | **Index-based
    vector, tree, list, and keyword search and retrieval** |'
  prefs: []
  type: TYPE_TB
- en: '| Flexibility | High | Medium (precomputed structure) |'
  prefs: []
  type: TYPE_TB
- en: '| Speed | Slower with large datasets | Fast and optimized for quick retrieval
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Limited by real-time processing | Highly scalable with large
    datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Complexity | Simpler setup | More complex and requires an indexing step |'
  prefs: []
  type: TYPE_TB
- en: '| Update Frequency | Easy to update | Requires re-indexing for updates |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3.1: Vector-based and index-based characteristics'
  prefs: []
  type: TYPE_NORMAL
- en: We will now build a semantic index-based RAG program with Deep Lake, LlamaIndex,
    and OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Building a semantic search engine and generative agent for drone technology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a semantic index-based search engine and generative
    AI agent engine using Deep Lake vector stores, LlamaIndex, and OpenAI. As mentioned
    earlier, drone technology is expanding in domains such as fire detection and traffic
    control. As such, the program’s goal is to provide an index-based RAG agent for
    drone technology questions and answers. The program will demonstrate how drones
    use computer vision techniques to identify vehicles and other objects. We will
    implement the architecture illustrated in *Figure 3.1*, described in the *Architecture*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Open `2-Deep_Lake_LlamaIndex_OpenAI_indexing.ipynb` from the GitHub repository
    of this chapter. The titles of this section are the same as the section titles
    in the notebook, so you can match the explanations with the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first begin by installing the environment. Then, we will build the
    three main pipelines of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline 1**: Collecting and preparing the documents. Using sources like
    GitHub and Wikipedia, collect and clean documents for indexing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline 2**: Creating and populating a Deep Lake vector store. Create and
    populate a Deep Lake vector store with the prepared documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline 3**: Index-based RAG for query processing and generation. Applying
    time and score performances with LLMs and cosine similarity metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When possible, break your project down into separate pipelines so that teams
    can progress independently and in parallel. The pipelines in this chapter are
    an example of how this can be done, but there are many other ways to do this,
    depending on your project. For now, we will begin by installing the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The environment is mostly the same as in the previous chapter. Let’s focus
    on the packages that integrate LlamaIndex, vector store capabilities for Deep
    Lake, and also OpenAI modules. This integration is a major step forward to seamless
    cross-platform implementations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The program requires additional Deep Lake functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The program also requires LlamaIndex functionalities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now check if the packages can be properly imported from `llama-index`,
    including vector stores for Deep Lake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With that, we have installed the environment. We will now collect and prepare
    the documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 1: Collecting and preparing the documents'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will collect and prepare the drone-related documents with
    the metadata necessary to trace the documents back to their source. The goal is
    to trace a response’s content back to the exact chunk of data retrieved to find
    its source. First, we will create a data directory in which we will load the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use a heterogeneous corpus for the drone technology data that
    we will process using `BeautifulSoup`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The corpus contains a list of sites related to drones, computer vision, and
    related technologies. However, the list also contains noisy links such as [https://keras.io/](https://keras.io/)
    and [https://pytorch.org/](https://pytorch.org/), which do *not* contain the specific
    information we are looking for.
  prefs: []
  type: TYPE_NORMAL
- en: In real-life projects, we will not always have the luxury of working on perfect,
    pertinent, structured, and well-formatted data. Our RAG pipelines must be sufficiently
    robust to retrieve relevant data in a noisy environment.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we are working with unstructured data in various formats and variable
    quality as related to drone technology. Of course, in a closed environment, we
    can work with the persons or organizations that produce the documents, but we
    must be ready for any type of document in a fast-moving, digital world.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will fetch and clean the data, as it did in *Chapter 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Each project will require specific names and paths for the original data. In
    this case, we will introduce an additional function to save each piece of text
    with the name of its data source, by creating a keyword based on its URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the goal is achieved, although some documents could not
    be decoded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Depending on the project’s goals, you can choose to investigate and ensure that
    all documents are retrieved, or estimate that you have enough data for user queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check `./data/`, we will find that each article is now in a separate
    file, as shown in the content of the directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: List of prepared documents'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program now loads the documents from `./data/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The LlamaIndex `SimpleDirectoryReader` class is designed for working with unstructured
    data. It recursively scans the directory and identifies and loads all supported
    file types, such as `.txt`, `.pdf`, and `.docx`. It then extracts the content
    from each file and returns a list of document objects with its text and metadata,
    such as the filename and file path. Let’s display the first entry of this list
    of dictionaries of the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that the directory reader has provided fully transparent information
    on the source of its data, including the name of the document, such as `1804.06985.txt`
    in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The content of this document contains noise that seems unrelated to the drone
    technology information we are looking for. But that is exactly the point of this
    program, which aims to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with all the raw, unstructured, loosely drone-related data we can get
    our hands on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulate how real-life projects often begin
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate how well an index-based RAG generative AI program can perform in a
    challenging environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now create and populate a Deep Lake vector store in complete transparency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 2: Creating and populating a Deep Lake vector store'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will create a Deep Lake vector store and populate it with
    the data in our documents. We will implement a standard tensor configuration with:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text (str)`: The text is the content of one of the text files listed in the
    dictionary of documents. It will be seamless, and chunking will be optimized,
    breaking the text into meaningful chunks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata(json)`: In this case, the metadata will contain the filename source
    of each chunk of text for full transparency and control. We will see how to access
    this information in code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding (float32)`: The embedding is seamless, using an OpenAI embedding
    model called directly by the `LlamaIndex-Deep Lake-OpenAI` package.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id (str, auto-populated)`: A unique ID is attributed automatically to each
    chunk. The vector store will also contain an index, which is a number from `0`
    to `n`, but it cannot be used semantically, since it will change each time we
    modify the dataset. However, the unique ID field will remain unchanged until we
    decide to optimize it with index-based search strategies, as we will see in the
    *Pipeline 3: Index-based RAG* section that follows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The program first defines our vector store and dataset paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Replace the vector store and dataset paths with your account name and the name
    of the dataset you wish to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create a vector store, populate it, and create an index over the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Notice that `overwrite` is set to `True` to create the vector store and overwrite
    any existing one. If `overwrite=False`, the dataset will be appended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The index created will be reorganized by the indexing methods, which will rearrange
    and create new indexes when necessary. However, the responses will always provide
    the original source of the data. The output confirms that the dataset has been
    created and the data is uploaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output also shows the structure of the dataset once it is populated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is stored in tensors with their type and shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Dataset structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now load our dataset in memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the dataset online by clicking on the link provided in the
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also decide to add code to display the dataset. We begin by loading
    the data in a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a function to display a record:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can select a record and display each field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The `id` is a unique string code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `metadata` field contains the information we need to trace the content
    back to the original file and file path, as well as everything we need to understand
    this record, from the source to the embedded vector. It also contains the information
    of the node created from the record’s data, which can then be used for the indexing
    engine we will run in *Pipeline 3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`file_path`: Path to the file in the dataset `(/content/data/1804.06985.txt`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file_name`: Name of the file (`` `1804.06985.txt` ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file_type`: Type of file (`` `text/plain` ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`file_size`: Size of the file in bytes (`` `3700` ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`creation_date`: Date the file was created (`` `2024-08-09` ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`last_modified_date`: Date the file was last modified (`` `2024-08-09` ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_node_content`: Detailed content of the node, including the following main
    items:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`id_`: Unique identifier for the node (`` `a89cdb8c-3a85-42ff-9d5f-98f93f414df6
    ` ``).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding`: Embedding related to the text (`null`).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: Repeated metadata about the file.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`excluded_embed_metadata_keys`: Keys excluded from embedding metadata (not
    necessary for embedding).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`excluded_llm_metadata_keys`: Keys excluded from LLM metadata (not necessary
    for an LLM).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`relationships`: Information about relationships to other nodes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: Actual text content of the document. It can be the text itself, an
    abstract, a summary, or any other approach to optimize search functions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`start_char_idx`: Starting character index of the text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`end_char_idx`: Ending character index of the text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_template`: Template for displaying text with metadata.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata_template`: Template for displaying metadata.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata_seperator`: Separator used in metadata display.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_name`: Type of node (e.g., `` `TextNode` ``).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`_node_type`: Type of node (`` `TextNode` ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`document_id`: Identifier for the document (`` `61e7201d-0359-42b4-9a5f-32c4d67f345e`
    ``).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc_id`: Document ID, same as `document_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ref_doc_id`: Reference document ID, same as `document_id`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `text` field contains the field of this chunk of data, not the whole original
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Embedding` field contains the embedded vector of the text content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The structure and format of RAG datasets vary from one domain or project to
    another. However, the following four columns of this dataset provide valuable
    information on the evolution of AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`id`: The `id` is the index we will be using to organize the chunks of text
    of the `text` column in the dataset. The chunks will be transformed into *nodes*
    that can contain the original text, summaries of the original text, and additional
    information, such as the source of the data used for the output that is stored
    in the metadata column. We created this index in **Pipeline 2** of this notebook
    when we created the vector store. However, we can generate indexes in memory on
    an existing database that contains no indexes, as we will see in *Chapter 4*,
    *Multimodal Modular RAG for Drone Technology*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: The metadata was generated automatically in **Pipeline 1** when
    Deep Lake’s `SimpleDirectoryReader` loaded the source documents in a documents
    object, and also when the vector store was created. In *Chapter 2*, *RAG Embedding
    Vector Stores with Deep Lake and OpenAI*, we only had one file source of data.
    In this chapter, we stored the data in one file for each data source (URL).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text`: The text processed by Deep Lake’s vector store creation functionality
    that we ran in **Pipeline 2** automatically chunked the data, without us having
    to configure the size of the chunks, as we did in the *Retrieving a batch of prepared
    documents* section in *Chapter 2*. Once again, the process is seamless. We will
    see how smart chunking is done in the *Optimized chunking* section of *Pipeline
    3: Index-based RAG* in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`embedding`: The embedding for each chunk of data was generated through an
    embedding model that we do not have to configure. We could choose an embedding
    model, as we did in the *Data embedding and storage* section in *Chapter 2*, *RAG
    Embedding Vector Stores with Deep Lake and OpenAI*. We selected an embedding model
    and wrote a function. In this program, Deep Lake selects the embedding model and
    embeds the data, without us having to write a single line of code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that embedding, chunking, indexing, and other data processing functions
    are now encapsulated in platforms and frameworks, such as Activeloop Deep Lake,
    LlamaIndex, OpenAI, LangChain, Hugging Face, Chroma, and many others. Progressively,
    the initial excitement of generative AI models and RAG will fade, and they will
    become industrialized, encapsulated, and commonplace components of AI pipelines.
    AI is evolving, and it might be helpful to facilitate a platform that offers a
    default configuration based on effective practices. Then, once we have implemented
    a basic configuration, we can customize and expand the pipelines as necessary
    for our projects.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to run index-based RAG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pipeline 3: Index-based RAG'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will implement an index-based RAG pipeline using `LlamaIndex`,
    which uses the data we have prepared and processed with Deep Lake. We will retrieve
    relevant information from the heterogeneous (noise-containing) drone-related document
    collection and synthesize the response through OpenAI’s LLM models. We will implement
    four index engines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vector Store Index Engine**: Creates a vector store index from the documents,
    enabling efficient similarity-based searches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tree Index**: Builds a hierarchical tree index from the documents, offering
    an alternative retrieval structure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**List Index**: Constructs a straightforward list index from the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Keyword Table Index**: Creates an index based on keywords extracted from
    the documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement querying with an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query Response and Source**: Queries the index with user input, retrieves
    the relevant documents, and returns a synthesized response along with source information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will measure the responses with a *time-weighted average metric with LLM
    score and cosine similarity* that calculates a time-weighted average, based on
    retrieval and similarity scores. The content and execution times might vary from
    one run to another due to the stochastic algorithms implemented.
  prefs: []
  type: TYPE_NORMAL
- en: User input and query parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The user input will be the reference question for the four index engines we
    will run. We will evaluate each response based on the index engine’s retrievals
    and measure the outputs, using time and score ratios. The input will be submitted
    to the four index and query engines we will build later.
  prefs: []
  type: TYPE_NORMAL
- en: 'The user input is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The four query engines that implement an LLM (in this case, an OpenAI model)
    will seamlessly be called with the same parameters. The three parameters that
    we will set are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'These key parameters are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`k=3`: The query engine will be required to find the top 3 most probable responses
    by setting the top-k (most probable choices) to 3\. In this case, k will serve
    as a ranking function that will force the LLM to select the top documents.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`temp=0.1`: A low temperature such as `0.1` will encourage the LLM to produce
    precise results. If the temperature is increased to `0.9`, for example, the response
    will be more creative. However, in this case, we are exploring drone technology,
    which requires precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mt=1024`: This parameter will limit the number of tokens of the output to
    `1,024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user input and parameters will be applied to the four query engines. Let’s
    now build the cosine similarity metric.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine similarity metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The cosine similarity metric was described in the *Evaluating the Output with
    the Cosine Similarity* section in *Chapter 2*. If necessary, take the time to
    go through that section again. Here, we will create a function for the responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The function uses `sklearn` and also Hugging Face’s `SentenceTransformer`. The
    program first creates the vector store engine.
  prefs: []
  type: TYPE_NORMAL
- en: Vector store index query engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`VectorStoreIndex` is a type of index within LlamaIndex that implements vector
    embeddings to represent and retrieve information from documents. These documents
    with similar meanings will have embeddings that are closer together in the vector
    space, as we explored in the previous chapter. However, this time, the `VectorStoreIndex`
    does not automatically use the existing Deep Lake vector store. It can create
    a new in-memory vector index, re-embed the documents, and create a new index structure.
    We will take this approach further in *Chapter 4*, *Multimodal Modular RAG for
    Drone Technology*, when we implement a dataset that contains no indexes or embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no silver bullet to deciding which indexing method is suitable for
    your project! The best way to make a choice is to test the vector, tree, list,
    and keyword indexes introduced in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create the vector store index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We then display the vector store index we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will receive the following output, which confirms that the engine was created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We now need a query engine to retrieve and synthesize the document(s) retrieved
    with an LLM—in our case, an OpenAI model (installed with `!pip install llama-index-vector-stores-deeplake==0.1.2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We defined the parameters of the query engine in the *User input and query parameters*
    subsection. We can now query the dataset and generate a response.
  prefs: []
  type: TYPE_NORMAL
- en: Query response and source
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s define a function that will manage the query and return information on
    the content of the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`index_query(input_query)` executes a query using a vector query engine and
    processes the results into a structured format. The function takes an input query
    and retrieves relevant information, using the query engine in a pandas DataFrame:
    `Node ID`, `Score`, `File Path`, `Filename`, and `Text`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will now call the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will evaluate the time it takes for the query to retrieve the relevant data
    and generate a response synthesis with the LLM (in this case, an OpenAI model).
    The output of the semantic search first returns a response synthesized by the
    LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output then displays the elapsed time of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The output now displays node information. The score of each node of three `k=3`
    documents was retrieved with their text excerpts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close-up of a number  Description automatically generated](img/B31169_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Node information output'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ID of the node guarantees full transparency and can be traced back to the
    original document, even when the index engines re-index the dataset. We can obtain
    the node source of the first node, for example, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides the node ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can drill down and retrieve the full text of the node containing the document
    that was synthesized by the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will display the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: We can also peek into the nodes and retrieve their chunk size.
  prefs: []
  type: TYPE_NORMAL
- en: Optimized chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can predefine the chunk size, or we can let LlamaIndex select it for us.
    In this case, the code determines the chunk size automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The advantage of an automated chunk size is that it can be variable. For example,
    in this case, the chunk size shown in the size of the output nodes is probably
    in the 4000-to-5500-character range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The chunking function does not linearly cut content but optimizes the chunks
    for semantic search.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will also implement a performance metric based on the accuracy of the queries
    and the time elapsed. This function calculates and prints a performance metric
    for a query, along with its execution time. The metric is based on the weighted
    average relevance scores of the retrieved information, divided by the time it
    took to get the results. Higher scores indicate better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first calculate the sum of the scores and the average score, and then we
    divide the weighted average by the time elapsed to perform the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a ratio based on the average weight divided by the elapsed time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then call the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output provides an estimation of the quality of the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This performance metric is not an absolute value. It’s an indicator that we
    can use to compare this output with the other index engines. It may also vary
    from one run to another, due to the stochastic nature of machine learning algorithms.
    Additionally, the quality of the output depends on the user’s subjective perception.
    In any case, this metric will help compare the query engines’ performances in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We can already see that the average score is satisfactory, even though we loaded
    heterogeneous and sometimes unrelated documents in the dataset. The integrated
    retriever and synthesizer functionality of LlamaIndex, Deep Lake, and OpenAI have
    proven to be highly effective.
  prefs: []
  type: TYPE_NORMAL
- en: Tree index query engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The tree index in LlamaIndex creates a hierarchical structure for managing
    and querying text documents efficiently. However, think of something other than
    a classical hierarchical structure! The tree index engine optimizes the hierarchy,
    content, and order of the nodes, as shown in *Figure 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a tree index  Description automatically generated](img/B31169_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Optimized tree index'
  prefs: []
  type: TYPE_NORMAL
- en: The tree index organizes documents in a tree structure, with broader summaries
    at higher levels and detailed information at lower levels. Each node in the tree
    summarizes the text it covers. The tree index is efficient for large datasets
    and queries large collections of documents rapidly by breaking them down into
    manageable optimized chunks. Thus, the optimization of the tree structure allows
    for rapid retrieval by traversing the relevant nodes without wasting time.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing this part of the pipeline and adjusting parameters such as tree depth
    and summary methods can be a specialized task for a team member. Depending on
    the project and workload, working on the tree structure could be part of **Pipeline
    2** when creating and populating a vector store. Alternatively, the tree structure
    can be created in memory at the beginning of each session. The flexibility of
    the structure and implementation of tree structures and index engines, in general,
    can be a fascinating and valuable specialization in a RAG-driven generative AI
    team.
  prefs: []
  type: TYPE_NORMAL
- en: In this index model, the LLM (an OpenAI model in this case) acts like it is
    answering a multiple-choice question when selecting the best nodes during a query.
    It analyzes the query, compares it with the summaries of the current node’s children,
    and decides which path to follow to find the most relevant information.
  prefs: []
  type: TYPE_NORMAL
- en: The integrated LlamaIndex-Deep Lake-OpenAI process in this chapter is industrializing
    components seamlessly, taking AI to another level. LLM models can now be used
    for embedding, document ranking, and conversational agents. The market offers
    various language models from providers like OpenAI, Cohere, AI21 Labs, and Hugging
    Face. LLMs have evolved from the early days of being perceived as magic to becoming
    industrialized, seamless, multifunctional, and integrated components of broader
    AI pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a tree index in two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The code then checks the class we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that we are in the `TreeIndex` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now make our tree index the query engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters of the LLM are those defined in the *User input and query parameters*
    section. The code now calls the query, measures the time elapsed, and processes
    the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The query time and the response are both satisfactory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Let’s apply a performance metric to the output.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This performance metric will calculate the cosine similarity defined in the
    *Cosine similarity metric* section between the user input and the response of
    our RAG pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that although the quality of the response was satisfactory,
    the execution time was slow, which brings the performance metric down:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Of course, the execution time depends on the server (power) and the data (noise).
    As established earlier, the execution times might vary from one run to another,
    due to the stochastic algorithms used. Also, when the dataset increases in volume,
    the execution times of all the indexing types may change.
  prefs: []
  type: TYPE_NORMAL
- en: The list index query engine may or may not be better in this case. Let’s run
    it to find out.
  prefs: []
  type: TYPE_NORMAL
- en: List index query engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t think of `ListIndex` as simply a list of nodes. The query engine will
    process the user input and each document as a prompt for an LLM. The LLM will
    evaluate the semantic similarity relationship between the documents and the query,
    thus implicitly ranking and selecting the most relevant nodes. LlamaIndex will
    filter the documents based on the rankings obtained, and it can also take the
    task further by synthesizing information from multiple nodes and documents.
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the selection process with an LLM is not rule-based. Nothing
    is predefined, which means that the selection is prompt-based by combining the
    user input with a collection of documents. The LLM evaluates each document in
    the list *independently*, assigning a score based on its perceived relevance to
    the query. This score isn’t relative to other documents; it’s a measure of how
    well the LLM thinks the current document answers the question. Then, the top-k
    documents are retained by the query engine if we wish, as in the function used
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the tree index, the list index can also be created in two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The code verifies the class that we are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output confirms that we are in the `list` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The list index is a `SummaryIndex`, which shows the large amount of document
    summary optimization that is running under the hood! We can now utilize our list
    index as a query engine in the seamless framework provided by LlamaIndex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The LLM parameters remain unchanged so that we can compare the indexing types.
    We can now run our query, wrap the response up, and display the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows a longer execution time but an acceptable response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The execution time is longer because the query goes through a list, not an optimized
    tree. However, we cannot draw conclusions from this because each project or even
    each sub-task of a project has different requirements. Next, let’s apply the performance
    metric.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the cosine similarity, as we did for the tree index, to evaluate
    the similarity score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance metric is lower than the tree index due to the longer execution
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Again, remember that this execution time may vary from one run to another, due
    to the stochastic algorithms implemented.
  prefs: []
  type: TYPE_NORMAL
- en: If we look back at the performance metric of each indexing type, we can see
    that, for the moment, the vector store index was the fastest. Once again, let’s
    not jump to conclusions. Each project might produce surprising results, depending
    on the type and complexity of the data processed. Next, let’s examine the keyword
    index.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword index query engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`KeywordTableIndex` is a type of index in LlamaIndex, designed to extract keywords
    from your documents and organize them in a table-like structure. This structure
    makes it easier to query and retrieve relevant information based on specific keywords
    or topics. Once again, don’t think about this function as a simple list of extracted
    keywords. The extracted keywords are organized into a table-like format where
    each keyword is associated with an ID that points to the related nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The program creates the keyword index in two lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s extract the data and create a pandas DataFrame to see how the index is
    structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shows that each keyword is associated with an ID that contains a
    document or a summary, depending on the way LlamaIndex optimizes the index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B31169_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Keywords linked to document IDs in a DataFrame'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now define the keyword index as the query engine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run the keyword query and see how well and fast it can produce a response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is satisfactory, as well as the execution time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We can now measure the output with a performance metric.
  prefs: []
  type: TYPE_NORMAL
- en: Performance metric
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code runs the same metric as for the tree and list index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance metric is acceptable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Once again, we can draw no conclusions. The results of all the indexing types
    are relatively satisfactory. However, each project comes with its dataset complexity
    and machine power availability. Also, the execution times may vary from one run
    to another, due to the stochastic algorithms employed.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have reviewed some of the main indexing types and retrieval strategies.
    Let’s summarize the chapter and move on to multimodal modular retrieval and generation
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter explored the transformative impact of index-based search on RAG
    and introduced a pivotal advancement: *full traceability*. The documents become
    nodes that contain chunks of data, with the source of a query leading us all the
    way back to the original data. Indexes also increase the speed of retrievals,
    which is critical as the volume of datasets increases. Another pivotal advance
    is the integration of technologies such as LlamaIndex, Deep Lake, and OpenAI,
    which are emerging in another era of AI. The most advanced AI models, such as
    OpenAI GPT-4o, Hugging Face, and Cohere, are becoming seamless *components* in
    a RAG-driven generative AI pipeline, like GPUs in a computer.'
  prefs: []
  type: TYPE_NORMAL
- en: We started by detailing the architecture of an index-based RAG generative AI
    pipeline, illustrating how these sophisticated technologies can be seamlessly
    integrated to boost the creation of advanced indexing and retrieval systems. The
    complexity of AI implementation is changing the way we organize separate pipelines
    and functionality for a team working in parallel on projects that scale and involve
    large amounts of data. We saw how every response generated can be traced back
    to its source, providing clear visibility into the origins and accuracy of the
    information used. We illustrated the advanced RAG technology implemented through
    drone technology.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we introduced the essential tools to build these systems,
    including vector stores, datasets, chunking, embedding, node creation, ranking,
    and indexing methods. We implemented the LlamaIndex framework, Deep Lake vector
    stores, and OpenAI’s models. We also built a Python program that collects data
    and adds critical metadata to pinpoint the origin of every chunk of data in a
    dataset. We highlighted the pivotal role of indexes (vector, tree, list, and keyword
    types) in giving us greater control over generative AI applications, enabling
    precise adjustments and improvements.
  prefs: []
  type: TYPE_NORMAL
- en: We then thoroughly examined indexed-based RAG through detailed walkthroughs
    in Python notebooks, guiding you through setting up vector stores, conducting
    advanced queries, and ensuring the traceability of AI-generated responses. We
    introduced metrics based on the quality of a response and the time elapsed to
    obtain it. Exploring drone technology with LLMs showed us the new skillsets required
    to build solid AI pipelines, and we learned how drone technology involves computer
    vision and, thus, multimodal nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapter, we include multimodal data in our datasets and expand
    multimodular RAG.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Answer the following questions with *Yes* or *No*:'
  prefs: []
  type: TYPE_NORMAL
- en: Do indexes increase precision and speed in retrieval-augmented generative AI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can indexes offer traceability for RAG outputs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is index-based search slower than vector-based search for large datasets?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does LlamaIndex integrate seamlessly with Deep Lake and OpenAI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are tree, list, vector, and keyword indexes the only types of indexes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the keyword index rely on semantic understanding to retrieve data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is LlamaIndex capable of automatically handling chunking and embedding?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are metadata enhancements crucial for ensuring the traceability of RAG-generated
    outputs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can real-time updates easily be applied to an index-based search system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is cosine similarity a metric used in this chapter to evaluate query accuracy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'LlamaIndex: [https://docs.llamaindex.ai/en/stable/](https://docs.llamaindex.ai/en/stable/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Activeloop Deep Lake: [https://docs.activeloop.ai/](https://docs.activeloop.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI: [https://platform.openai.com/docs/overview](https://platform.openai.com/docs/overview)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'High-Level Concepts (RAG), LlamaIndex: [https://docs.llamaindex.ai/en/stable/getting_started/concepts/](https://docs.llamaindex.ai/en/stable/getting_started/concepts/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packt.link/rag](https://www.packt.link/rag)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code50409000288080484.png)'
  prefs: []
  type: TYPE_IMG
