<html><head></head><body>
		<div><h1 id="_idParaDest-16" class="chapter-number"><a id="_idTextAnchor015"/>1</h1>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Cloud Computing Meets Generative AI: Bridging  Infinite Impossibilities</h1>
			<p>During the last few decades, we have <a id="_idIndexMarker000"/>seen unprecedented progress in the world of <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) due to the <a id="_idIndexMarker001"/>rise of computing, especially cloud computing, and the massive influx of data from the digital revolution. In 2022, the subset of AI known as generative AI emerged as a significant turning point. We have surpassed an inflection point in AI and we believe this will boost incredible productivity and growth in society in the coming years. This is the <a id="_idIndexMarker002"/>field of conversational AI powered by <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), a fascinating paradigm where computers learn and generate human-like text, images, audio, and video, engaging with us in increasingly interactive and intelligent ways. The transformative potential of LLMs, epitomized by models, such as OpenAI’s GPT-based ChatGPT, marks a major <a id="_idIndexMarker003"/>shift in how we interact with technology. Generative AI models now have improved accuracy and effectiveness. Use cases that were unattainable for non-technical users in businesses a couple of years ago are now readily implementable. Additionally, the easy availability of open source models, which can be tailored to specific business requirements, coupled with access to high-performance GPUs via cloud computing, has played a crucial role in propelling the advancement of generative AI.</p>
			<p>This chapter aims to provide a comprehensive introduction to conversational and generative AI and delve into its fundamentals and powerful capabilities. ChatGPT, a very powerful conversational AI agent, is built on an LLM; hence, to fully understand how ChatGPT works and to learn how to implement it in your applications or services to harness its power, it’s necessary to understand the evolution of conversational AI systems and the broader context of LLMs.</p>
			<p>We will cover the following main topics in this chapter:</p>
			<ul>
				<li>Evolution of conversation AI</li>
				<li>Introduction to generative AI</li>
				<li>Trending models and business applications</li>
				<li>Deep dive: open source vs closed source models</li>
				<li>Cloud computing for scalability, cost optimization, and automation</li>
				<li>From vision to value: navigating the journey to production</li>
			</ul>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Evolution of conversation AI</h1>
			<p>Understanding the <a id="_idIndexMarker004"/>evolution of conversational AI is crucial for learning generative AI as it provides foundational knowledge and context. This historical perspective reveals how AI technologies have progressed from simple, rule-based systems to complex machine learning and deep learning models that are core to both conversational and generative AI.</p>
			<p>This section explores the evolution of conversational AI, culminating in an in-depth look at LLMs, the technological backbone of contemporary chatbots.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>What is conversational AI?</h2>
			<p>Conversational AI<a id="_idIndexMarker005"/> refers to technologies that enable machines to engage in human-like dialogue, comprehend complex commands, and respond intelligently. This is achieved through machine learning and natural language processing capabilities, enabling the system to learn, understand, and improve over time. The following figure demonstrates one such conversation:</p>
			<div><div><img src="img/B21443_01_1.jpg" alt="Figure 1.1 – Conversations with Alexa"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Conversations with Alexa</p>
			<p>For instance, a customer interacts with a conversational AI to book a flight. They might say, “I’d like a flight to New York next Friday.” The system comprehends the request, asks for any further specific details (such as departure city or preferred time), and delivers the results, all without human intervention.</p>
			<p>Some popular<a id="_idIndexMarker006"/> conversational AI systems include Microsoft’s Cortana, Amazon Alexa, Apple’s Siri, and Google Assistant, which can respond to complex commands and respond intelligently.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Evolution of conversational AI</h2>
			<p>Exploring the<a id="_idIndexMarker007"/> evolution of conversational AI, from rule-based chatbots to AI-powered systems, is vital as it offers historical context, highlights the technological advancements from the 1960s and the historical challenges, and sets the stage for understanding how LLMs have revolutionized natural language interactions. The following figure depicts the <a id="_idIndexMarker008"/>conversational AI timeline:</p>
			<div><div><img src="img/B21443_01_2.jpg" alt="Figure 1.2 – Timeline showing the evolution of chatbots"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Timeline showing the evolution of chatbots</p>
			<h3>Rule-based chatbots</h3>
			<p>Chatbots that were<a id="_idIndexMarker009"/> initially developed during the 1960s operated on a rule-based system. Eliza, the first chatbot software, was created by Joseph Weizenbaum at MIT’s <a id="_idIndexMarker010"/>Artificial Intelligence Laboratory in 1966. It used pattern matching and substitution technology. Users interacted with Eliza through a text-based platform, with the chatbot’s responses being based on scripted templates. Like Eliza, the first-generation chatbots were rule-based. They utilized pattern-matching techniques to align user inputs with predetermined responses. The chatbot’s conversation flows were mapped out by developers who decided how it should respond to anticipated customer inquiries. Responses were formulated based on predefined rules and written in languages such <a id="_idIndexMarker011"/>as <strong class="bold">artificial intelligence markup language</strong> (<strong class="bold">AIML</strong>), Rivescript, Chatscript, and others. These chatbots, typically used as FAQ agents, could answer simple questions or common queries about a specific situation.</p>
			<p>However, rule-based systems had<a id="_idIndexMarker012"/> significant limitations:</p>
			<p>Rule-based systems required manual design, forcing developers to program each response</p>
			<p>They were effective only in the scenarios for which they were specifically trained</p>
			<p>It was difficult for developers to anticipate and program all possible responses</p>
			<p>These chatbots were unable to identify grammatical or syntactic errors in user inputs, often resulting in misunderstandings</p>
			<p>They were unable to learn from interactions or generate new responses, limiting their adaptability and intelligence</p>
			<p>Despite their speed, the inability to understand context or user intents made interactions feel mechanical rather than conversational</p>
			<p>This mechanical interaction often led to user frustration with systems that failed to accurately understand <a id="_idIndexMarker013"/>and meet their needs</p>
			<p>Over time, there has been a significant increase in demand for intelligent, real-time, and personalized interactions in customer support services. As a result, rule-based chatbots have evolved into AI-powered chatbots that offer advanced features such as human-like voice, intent extraction, sentiment analysis, contextual semantic search, grammatical analysis, learning over time, and scalability to allow for seamless integration with more demanding applications and services.</p>
			<h3>LLM-powered chatbots – multimodal, context-aware, and agent-based</h3>
			<p>In contrast to<a id="_idIndexMarker014"/> rule-based systems, AI-based systems utilize<a id="_idIndexMarker015"/> natural language processing to facilitate natural conversations and extract context from user inputs. They can also learn from past interactions aka context. Recently, deep learning has significantly advanced conversational AI, even surpassing human performance in some tasks, attributed to its incredible reasoning engine. This has decreased the reliance on extensive linguistic knowledge and rule-based techniques when building language services. As a result, AI-based systems have seen widespread adoption across various industries, including media, entertainment, telecommunications, finance, healthcare, and retail, to name a few.</p>
			<p>Current conversational AI systems, leveraging LLMs such as GPT-4-Turbo, differ significantly from traditional rule-based systems in their approach and capabilities:</p>
			<p>While rule-based systems rely on predefined rules and responses, limiting them to specific, anticipated interactions, LLMs harness extensive datasets and advanced reasoning abilities to produce responses that are not only natural and varied but also highly context-aware</p>
			<p>They are also multimodal, which means they can understand and respond to multiple forms of communication such as text, voice, image, or video</p>
			<p>These exceptional reasoning abilities enable them to handle tasks with increased efficiency and sophistication, leading to conversations that closely mimic human interaction and understanding</p>
			<p>Let’s take the scenario <a id="_idIndexMarker016"/>of a customer service interaction as an <a id="_idIndexMarker017"/>example to highlight the differences between traditional rule-based systems and modern conversational AI systems that use LLMs, such as GPT-4.</p>
			<p>The following is a rule-based system example:</p>
			<pre class="source-code">
Customer: "I want to return a gift I received without a receipt. Can you help me?"
Rule-Based Chatbot: "Please enter your order number to proceed with a return."</pre>			<p>In this case, the rule-based chatbot is programmed to ask for an order number as a part of its return process script. It can’t handle the nuance of the customer’s situation where they don’t have a receipt. It’s stuck in its predefined rules and can’t adapt to the unexpected scenario.</p>
			<p>The following is an LLM-powered conversational AI example:</p>
			<pre class="source-code">
Customer: "I want to return a gift I received without a receipt. Can you help me?"
LLM-Powered Chatbot: "Certainly! Gifts can often be returned without a receipt by verifying the purchaser's details or using a gift return code. Do you have the purchaser's name or email, or a gift return code?"</pre>			<p>The LLM-powered chatbot, on the other hand, understands the context of not having a receipt and offers alternative methods for returning the item. It does not require the customer to stick to a strict script but instead adapts to the context of the conversation and provides a helpful response. This showcases the advanced reasoning capabilities of LLMs, allowing for more natural, flexible, and human-like conversations.</p>
			<p>LLM-powered <a id="_idIndexMarker018"/>chatbots also possess inherent limitations, including <a id="_idIndexMarker019"/>difficulties in generating accurate up-to-date information, a tendency to hallucinate, and the reproduction of biases present in their training data. We explore these limitations throughout this book, along with strategies to mitigate and eliminate them.</p>
			<h3>Chatbots and agents</h3>
			<p>GenAI-based chatbots <a id="_idIndexMarker020"/>can also execute tasks or actions with the help of<a id="_idIndexMarker021"/> agents. LLM agents are programs that enhance standard LLMs by connecting to external tools, such as APIs and plugins, and assist in planning and executing tasks. They often interact with other software and databases for complex tasks, such as chatbot scheduling meetings and needing access to calendars and emails. When a user requests a meeting, the chatbot, utilizing its LLM, comprehends the request’s specifics, such as time, participants, and purpose. It then autonomously interacts with the employees’ digital calendars and email systems to find a suitable time slot, considering everyone’s availability. Once it identifies an appropriate time, the chatbot schedules the meeting and sends invites via email, managing the entire process without human intervention. This showcases the chatbot’s ability to perform complex, multi-step tasks efficiently, blending language understanding and reasoning with practical action in a business environment. We will learn more about LLM agents in <a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>.</p>
			<p>ChatGPT, launched in<a id="_idIndexMarker022"/> November 2022 by OpenAI, attracted 100 million users within just two months due to its advanced language capabilities and broad applicability across various tasks.</p>
			<p>In the upcoming section, we will delve into the fundamentals of LLMs as the driving force behind modern chatbots and their significance.</p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Introduction to generative AI</h1>
			<p>Generative AI refers to<a id="_idIndexMarker023"/> a field of AI (as stated in the preceding figure) that focuses on creating or generating new content, such as images, text, music, video, code, 3D objects, or synthetic data that is not directly copied or replicated from existing data. It involves training deep learning models to understand patterns and relationships within a given dataset and then using that knowledge to generate novel and unique content. The following is a visualization of what generative AI is:</p>
			<div><div><img src="img/B21443_01_3.jpg" alt="Figure 1.3 – What is generative AI?"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – What is generative AI?</p>
			<p>It is a broad field whose primary function is to generate novel content. Examples of generative AI models<a id="_idIndexMarker024"/> include<a id="_idIndexMarker025"/> image generation models such as <strong class="bold">DALL-E</strong> and <strong class="bold">MidJourney</strong>, text generation<a id="_idIndexMarker026"/> models such <a id="_idIndexMarker027"/>as <strong class="bold">GPT-4</strong>, <strong class="bold">PaLM</strong>, and <strong class="bold">Claude</strong>, code <a id="_idIndexMarker028"/>generation models such as <strong class="bold">Codex</strong>, audio generation <a id="_idIndexMarker029"/>tools such <a id="_idIndexMarker030"/>as <strong class="bold">MusicLM</strong>, and video generation models such<a id="_idIndexMarker031"/> as <strong class="bold">SORA</strong>.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>The rise of generative AI in 2022-23</h2>
			<p>Generative AI has reached an <a id="_idIndexMarker032"/>inflection point in recent times, and this can be attributed to three key factors:</p>
			<ul>
				<li><strong class="bold">Size and variety of datasets</strong>: The surge in available data due to the digital revolution has been crucial for training AI models to generate human-like content.</li>
				<li><strong class="bold">Innovative deep learning models</strong>: Advancements in model architectures <a id="_idIndexMarker033"/>such as <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>) and transformer-based models facilitate the learning of complex patterns, resulting in high-quality AI-generated outputs. The research paper “Attention Is All You Need” (<a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>) introduced transformer architecture, enabling significantly more efficient and powerful models for natural language processing, which became foundational for the development of advanced generative AI models. Progress has also been significantly fueled by the availability of open source state-of-the-art pre-trained models via platforms such as the Hugging Face Community.</li>
				<li><strong class="bold">Powerful computing</strong>: Advancements in hardware such as Nvidia GPUs and access to computing through cloud computing have enabled the training of complex AI models, driving advancements in generative AI.</li>
			</ul>
			<p>There are <a id="_idIndexMarker034"/>various <a id="_idIndexMarker035"/>types of<a id="_idIndexMarker036"/> generative AI models with different underlying architectures. Among them, <strong class="bold">VAEs</strong>, <strong class="bold">diffusion models</strong>, <strong class="bold">GANs</strong>, and <strong class="bold">autoregressive models</strong> are<a id="_idIndexMarker037"/> particularly popular. While we <a id="_idIndexMarker038"/>won’t delve into every model architecture extensively as it is outside the scope of this book. In <a href="B21443_02.xhtml#_idTextAnchor036"><em class="italic">Chapter 2</em></a>, we will focus on a more detailed <a id="_idIndexMarker039"/>discussion of ChatGPT’s LLM architecture, which utilizes an <strong class="bold">autoregressive-based </strong><strong class="bold">transformer architecture</strong>.</p>
			<p>Moving from the topic of <a id="_idIndexMarker040"/>generative AI, we now turn our attention to foundation models. Often used interchangeably with LLMs, these models are the driving force behind the success and possibilities of generative AI. The remarkable strides made in foundation models have been instrumental in propelling the advancements we witness today in generative AI applications. Their development has not only enabled more sophisticated AI capabilities but has also set the stage for a new era of innovation and possibilities in AI.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Foundation models</h2>
			<p>The term foundation models<a id="_idIndexMarker041"/> was coined by Stanford in 2021 in the paper “On the Opportunities and Risks of Foundation Models” (<a href="https://arxiv.org/pdf/2108.07258.pdf">https://arxiv.org/pdf/2108.07258.pdf</a>). Foundation models are a class of large-scale model that are pre-trained on vast amounts of data across various domains and tasks. They serve as a base for further fine-tuning or adaptation to a wide range of downstream tasks, not limited to language but including vision, sound, and other modalities. The term <em class="italic">foundation</em> signifies that these models provide a foundational layer of understanding and capabilities upon which specialized models can be built. They are characterized by their ability to learn and generalize from the training data to a variety of applications, sometimes with little to no additional training data. The model is as follows:.</p>
			<div><div><img src="img/B21443_01_4.jpg" alt="Figure 1.4 – Foundation models"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – Foundation models</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>LLMs</h2>
			<p>LLMs, on the other hand, are<a id="_idIndexMarker042"/> a subset of foundation models that specifically deal with natural language processing tasks. They are trained in large text corpora and are designed to understand, generate, and translate language at a scale and sophistication that closely resembles human language understanding. LLMs are trained on massive amounts of data, such as books, articles, and the internet. For example, ChatGPT’s base model was trained on 45 TB of data.</p>
			<p>LLMs such as GPTs use transformer architecture to process text sequences, training themselves to predict the next word in a given sequence. Through exposure to vast amounts of text, these models adjust their internal weights based on the difference between predicted and actual words, a process known as backpropagation. Over time, by repeatedly refining these weights across multiple layers of attention mechanisms, they capture intricate statistical patterns and dependencies in the language, enabling them to generate contextually relevant text. In <a href="B21443_02.xhtml#_idTextAnchor036"><em class="italic">Chapter 2</em></a>, we will delve deeper into the transformer architecture of LLMs that enables the ChatGPT application.</p>
			<p>LLMs traditionally refer to models that handle large-scale language tasks; the principles and architecture underlying them can be, and are being, extended to other domains such as image generation. This expansion of capabilities reflects the versatility and adaptability of the transformer-based models that power both LLMs and their multimodal counterparts.</p>
			<p>Models such as DALL-E, for instance, are sometimes referred to as <a id="_idIndexMarker043"/>LLMs due to their foundation in transformer architecture, which was originally developed for language tasks. However, DALL-E is more accurately described as a multimodal AI model because it understands both text and images and can generate images from textual descriptions.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>Core attributes of LLMs</h2>
			<p>In the process of creating<a id="_idIndexMarker044"/> LLM-based AI applications, it is <a id="_idIndexMarker045"/>crucial to understand the core attributes of LLMs, such as model parameters, licensing model, privacy, cost, quality, and latency. It is important to note that there isn’t a flawless model, and making tradeoffs might be necessary to align with the specific business requirements of the application. The following content concentrates only on vital considerations when designing LLM applications.</p>
			<h3>Model parameters</h3>
			<ul>
				<li>Model parameters in <a id="_idIndexMarker046"/>LLMs are the internal settings that the model uses to understand and generate text. These parameters can be coefficients, weights, and biases and are part of large mathematical equations that underlie LLM models. They are adjusted through training, where the model learns from vast amounts of data how to predict the next word in a sentence, understand context, and generate coherent and relevant text.<p class="list-inset">For example, in the context of LLMs, model parameters are akin to internal notes that guide predictions based on learned data patterns. For example, if an LLM frequently encounters the phrase “sunny weather” during training, it adjusts its parameters to strengthen the connection between “sunny” and “weather.” These adjustments are like turning knobs to increase the likelihood of predicting “weather” after “sunny” in new sentences. Thus, the model’s parameters encode relationships between words, enabling it to generate contextually relevant text based on its training.</p></li>
				<li>The number of parameters indicates the model’s size and complexity, with larger models generally capable of capturing more complex patterns and nuances in language but requiring more computational resources.</li>
				<li>Understanding the parameters in LLMs is crucial for interpreting model behavior, customizing and adapting the model, and evaluating and comparing different models.</li>
				<li>Smaller models are more fine-tunable because of the lower number of parameters as compared to larger models.</li>
				<li>While designing applications, it’s crucial to understand whether a smaller model can fulfill the needs of a particular use case by means of fine-tuning/in-context learning or whether a larger model is necessary. For example, smaller models such as GPT-3.5 and FLAN-T5 typically come with lower costs as compared to GPT-4 and <a id="_idIndexMarker047"/>often prove highly efficient with fine-tuning or in-context learning, especially in specific tasks such as conversation summarization.</li>
			</ul>
			<h3>Licensing</h3>
			<ul>
				<li>Open source <a id="_idIndexMarker048"/>models can be used as-is or <a id="_idIndexMarker049"/>customized for commercial and non-commercial use. They are usually smaller than proprietary LLM models, less expensive, and more task-specific. For example, Whisper is an open source speech-to-text model developed by Open AI, and Llama from Facebook is an open source model.</li>
				<li>Proprietary models are usually larger models and require licensing. They may be restricted for commercial use and modifications. For example, GPT-4 is a proprietary model developed by Open AI.</li>
				<li>When designing applications, it is important to understand whether it is an open source or a licensed model and whether it is permitted for commercial use. This is crucial to ensure legal compliance, financial planning, ethical considerations, customization<a id="_idIndexMarker050"/> possibilities, and the long-term success of your application.</li>
			</ul>
			<h3>Privacy</h3>
			<ul>
				<li>Ensuring the security of <a id="_idIndexMarker051"/>data used for fine-tuning and prompting LLMs, especially when it involves sensitive customer information, is paramount.</li>
				<li>Guardrails must be established to ensure that customer data is redacted before fine-tuning the models and also when using them in prompts.</li>
				<li>It is also crucial to understand how the data will be stored and utilized by the model. Data controls can be configured in ChatGPT to prevent chats from being saved by the system and thus not allowing them to be used to train the models.</li>
			</ul>
			<h3>Cost</h3>
			<ul>
				<li>When<a id="_idIndexMarker052"/> architecting LLM applications, it is important to understand the cost<a id="_idIndexMarker053"/> of acquiring the model (e.g. licensing costs), infrastructure costs related to data storage, computing, data transfer, fine-tuning, and maintenance costs such as monitoring.</li>
			</ul>
			<h3>Latency</h3>
			<ul>
				<li>This is crucial for ensuring smooth<a id="_idIndexMarker054"/> interaction for users. When deciding on models, you must discern whether the output requires real-time or near-real-time responses.</li>
				<li>Larger model APIs may have slightly slower response times and higher costs as compared to smaller models, but the quality of outputs may be better in certain scenarios. For example, GPT-4 is slightly slower than GPT 3.5 Turbo but may perform better in certain scenarios where complex reasoning is involved.</li>
				<li>Attaining low latency necessitates considering several elements, such as picking the right LLM API or hardware infrastructure for self-hosted open source LLMs or modifying the length of input and output. The application of methods such as cache and load balancing of APIs can drastically reduce response durations, leading<a id="_idIndexMarker055"/> to a<a id="_idIndexMarker056"/> fluid user experience.</li>
			</ul>
			<p>The core attributes mentioned provide an excellent starting point for shortlisting models based on business requirements. However, it’s important to understand that some LLMs may exhibit more biases and a higher tendency to hallucinate. In <a href="B21443_03.xhtml#_idTextAnchor052"><em class="italic">Chapter 3</em></a>, we discuss industry-leading benchmarks that will help you make informed decisions considering these limitations.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Relationship between generative AI, foundation models, and LLMs</h2>
			<p>Generative AI broadly <a id="_idIndexMarker057"/>refers to AI systems that can create new content, such as text, image, audio, or video. Foundation models are a subset of generative AI, characterized by their large scale and versatility across multiple tasks, often trained on extensive and diverse datasets. LLMs, a type of foundation model, specifically focus on understanding and generating human language, exemplified by systems such as GPT-3.5-Turbo and Llama 2.</p>
			<p>Foundation models <a id="_idIndexMarker058"/>can be applied to a variety of AI tasks beyond language, such as image recognition, whereas<a id="_idIndexMarker059"/> LLMs are specifically focused on language-related tasks.</p>
			<p>In practice, the terms can sometimes be used interchangeably when the context is clearly about language tasks, but it’s important to know that the concept of foundation models was originally supposed to be broader and encompass a wider range of AI capabilities.</p>
			<p>However, now, as LLMs <a id="_idIndexMarker060"/>such as GPT-4 Turbo<a id="_idIndexMarker061"/> are extending to multimodal capabilities, this difference between foundation models and LLMs has been narrowing.</p>
			<p>Generative AI encompasses a wide array <a id="_idIndexMarker062"/>of AI models designed to create new, previously unseen content, spanning domains from text and images to music. The following image illustrates the relationship between generative AI, LLMs, and foundation models:</p>
			<div><div><img src="img/B21443_01_5.jpg" alt="Figure 1.5 – What is an LLM?"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – What is an LLM?</p>
			<h3>The LLMs behind ChatGPT</h3>
			<p>As of<a id="_idIndexMarker063"/> early 2024, ChatGPT is a specialized application <a id="_idIndexMarker064"/>of GPT-3.5 and GPT-4 that is fine-tuned for conversational interactions. While GPT-3.5/4 is a general language model capable of a variety of language tasks, ChatGPT has been specifically trained to respond to prompts in a way that mimics human conversation. The process starts with the base foundation model GPT-3.5/4 model that has been pre-trained on a large corpus of text from the internet. Then, to create ChatGPT, OpenAI conducts further training (fine-tuning) on datasets that include many examples of human dialogue. This helps ChatGPT to better understand and generate conversational responses. In essence, GPT-3.5/4 can be thought of as the underlying technology, and ChatGPT as a specific implementation of that technology optimized for conversation.</p>
			<p>Google’s Bard (now known as Gemini) is a similar application to ChatGPT and is built on an LLM called <a id="_idIndexMarker065"/>PaLM-2.</p>
			<p>Open source models such<a id="_idIndexMarker066"/> as Llama 2 from Facebook have become more popular lately. But how do they contrast with closed source or proprietary models? What are their advantages? In the next section, we will explore more about the details of and what defines an LLM as an open source model.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Deep dive – open source vs closed source/proprietary models</h2>
			<p>Open source models such<a id="_idIndexMarker067"/> as <strong class="bold">Llama 2</strong>, <strong class="bold">Mistral</strong>, and <strong class="bold">Falcon</strong> have become increasingly popular in the recent past. As <a id="_idIndexMarker068"/>Gen AI Cloud Architects, the authors have witnessed considerable<a id="_idIndexMarker069"/> debate on choosing between open source and closed source models and identifying the appropriate contexts for their use. This section delves into the fundamental distinctions between these models on “What is revealed?” and “What is not revealed?” along with key deployment differences, drawing on our insights from the field.</p>
			<h3>Closed source LLMs (e.g., GPT-4, PaLM-2, Claude-2)</h3>
			<p>What is revealed is the following:</p>
			<ul>
				<li><strong class="bold">Functionality and capabilities</strong>: Users <a id="_idIndexMarker070"/>know what the model can do, such as generating<a id="_idIndexMarker071"/> text, answering questions, and more.</li>
				<li><strong class="bold">Usage guidelines</strong>: Information on how to interact with the model (e.g., APIs) and its intended use cases are revealed. OpenAI provides API access to GPT models, but the underlying models are not openly distributed.</li>
				<li><strong class="bold">Performance metrics</strong>: OpenAI shares details about GPT’s performance in various tasks and benchmarks.</li>
				<li><strong class="bold">Ethical standards</strong>: OpenAI discusses the ethical considerations and guidelines followed during development.</li>
				<li><strong class="bold">General architecture overview</strong>: While not in detail, there’s usually some high-level information about the model’s architecture.</li>
			</ul>
			<p>What is not revealed is the following:</p>
			<ul>
				<li><strong class="bold">Source code</strong>: The actual codebase of closed-source models is not publicly available</li>
				<li><strong class="bold">Model weights</strong>: Access to the actual model weights for complete replication is restricted</li>
				<li><strong class="bold">Training data details</strong>: Specifics about the training datasets, including their sources and compositions, are generally not disclosed</li>
				<li><strong class="bold">Detailed model architecture</strong>: The intricate details of the model’s architecture and algorithms are proprietary</li>
				<li><strong class="bold">Training process</strong>: Specifics on how the model was trained, including hyperparameters and<a id="_idIndexMarker072"/> training<a id="_idIndexMarker073"/> duration, are not shared</li>
			</ul>
			<p>The above conclusions were drawn up based on the GPT-4 Technical Report (<a href="https://arxiv.org/pdf/2303.08774.pdf">https://arxiv.org/pdf/2303.08774.pdf</a>) released by OpenAI. In the report, OpenAI states that due to the competitive landscape and safety implications of large-scale models such as GPT-4, it doesn’t reveal intricate details about the architecture, including model size, hardware, training computing, dataset construction, training method, or similar.</p>
			<h3>Open source LLMs (e.g., Llama 2, Mistral, Falcon)</h3>
			<p>What is revealed is the following:</p>
			<ul>
				<li><strong class="bold">Source code</strong>: The full <a id="_idIndexMarker074"/>codebase is usually available for public access. Hence, individuals<a id="_idIndexMarker075"/> and businesses can deploy open source models on personal PCs and in on-premises or internal servers.</li>
				<li><strong class="bold">Model weights</strong>: The weights of the model can be downloaded and used by researchers and developers.</li>
				<li><strong class="bold">Training process details</strong>: Detailed information about how the model was trained, including datasets and hyperparameters.</li>
				<li><strong class="bold">Full architecture details</strong>: Comprehensive information on the model’s architecture is provided.</li>
				<li><strong class="bold">Dataset information</strong>: Although with some constraints, more information about the training datasets may be available.</li>
			</ul>
			<p>What is not revealed is the<a id="_idIndexMarker076"/> following:</p>
			<ul>
				<li><strong class="bold">Resource requirements</strong>: Specific details on the computational resources required for training might not be fully disclosed</li>
				<li><strong class="bold">Ethical considerations</strong>: Open source projects may not always have the same level of ethical oversight as some closed source projects</li>
				<li><strong class="bold">Performance optimization secrets</strong>: Some nuances of performance optimization during training might be left out</li>
				<li><strong class="bold">Full training data</strong>: Even in open source models, sharing the entire training data can be impractical due to size and licensing issues</li>
				<li><strong class="bold">Continuous updates</strong>: Unlike some closed source models, open source models may not receive<a id="_idIndexMarker077"/> continuous <a id="_idIndexMarker078"/>updates or support</li>
			</ul>
			<p>The following table details the key deployment differences<a id="_idIndexMarker079"/> between open and closed source models:</p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Closed </strong><strong class="bold" lang="en-US" xml:lang="en-US">source models</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold" lang="en-US" xml:lang="en-US">Open source </strong><strong class="bold" lang="en-US" xml:lang="en-US">LLMs (OSS)</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Access, cost, and deployment endpoint</p>
						</td>
						<td class="No-Table-Style">
							<p>Access is typically restricted to paid licenses, APIs, or subscription models. The cost can be a barrier for smaller organizations or individual developers.</p>
							<p>Costs associated with such deployments are typically associated with the number of tokens in prompts and completions.</p>
							<p>For example, as of early 2024, OpenAI charges $0.01 /1K tokens for prompts and $0.03 /1K tokens for completions for gpt-4-0125-preview.</p>
						</td>
						<td class="No-Table-Style">
							<p>Generally, the source code is freely available. Deploying open source models necessitates the initial setup of compute instances, serving as the foundation for an inference endpoint. This endpoint can operate in real time or process data in batches. The expenses linked to this deployment strategy primarily involve the operational costs of the compute resources.</p>
							<p>However, new pricing models have emerged, such as MaaS (model-as-a-service), which charges just like API-based models based on the tokens used.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Customization and flexibility</p>
						</td>
						<td class="No-Table-Style">
							<p>Due to the unavailability of source code, customization options are often limited to what the provider allows. Users may not be able to modify the model’s core architecture or training datasets.</p>
						</td>
						<td class="No-Table-Style">
							<p>Greater flexibility for customization is offered. Developers can tweak the models, retrain with specific datasets, or even adjust the underlying algorithms.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Support and documentation</p>
						</td>
						<td class="No-Table-Style">
							<p>Usually, they come with professional support and comprehensive documentation, ensuring smoother deployment, and troubleshooting processes.</p>
						</td>
						<td class="No-Table-Style">
							<p>While there is often a community for support, the quality and availability of formal support and documentation can vary.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Integration and compatibility</p>
						</td>
						<td class="No-Table-Style">
							<p>They<a id="_idIndexMarker080"/> might have better integration with other proprietary tools or platforms offered by the same provider but could be less flexible in terms of compatibility with a wide range of technologies.</p>
						</td>
						<td class="No-Table-Style">
							<p>They are typically designed to be more flexible and compatible with a variety of platforms and tools, though integration may require more effort from the user.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Security and updates</p>
						</td>
						<td class="No-Table-Style">
							<p>Security updates and patches are typically managed by the provider, ensuring a consistent level of maintenance.</p>
						</td>
						<td class="No-Table-Style">
							<p>Security relies on the community and maintainers, which can lead to varying degrees of promptness and effectiveness in updates.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Ethics, compliance, and liability</p>
						</td>
						<td class="No-Table-Style">
							<p>Providers are generally responsible for compliance with regulations, offering a certain level of assurance for businesses.</p>
						</td>
						<td class="No-Table-Style">
							<p>Users often need to ensure compliance themselves, which can be a significant consideration for businesses in regulated industries.</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Risks</p>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Potentially <a id="_idIndexMarker081"/>higher costs due to licensing fees</li>
								<li>Limited ability to customize to meet business requirements as compared to open source</li>
								<li>Vendor Lock-In</li>
								<li>Reduced transparency, due to the limited knowledge of the internal workings of the LLMs</li>
							</ul>
						</td>
						<td class="No-Table-Style">
							<ul>
								<li>Potential security vulnerabilities as they are community-driven and might enable malicious use</li>
								<li>Lack of centralized quality control can lead to inconsistencies in updates and improvements</li>
								<li>Reliance on community support may lead to inconsistent troubleshooting and issue resolution, affecting projects that need stable, continuous maintenance</li>
							</ul>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Key deployment differences</p>
			<p>The decision for organizations to adopt open source or closed source models is inherently subjective and hinges on their unique needs and goals. A more pertinent question might be: after conducting internal benchmarking, which model emerges as the most effective for your specific use case? These benchmarks are available on Hugging Face (<a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard">https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard</a>).</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Trending models, tasks, and business applications</h1>
			<p>Generative AI has a wide range of applications across various industries, presenting several use cases that can bring significant benefits to businesses, and the applications are continuing to grow at a fast pace. In this section, we will discuss popular tasks and models and examine the latest emerging business applications that have gained significant traction recently.</p>
			<p>Let’s begin with text generation models.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Text</h2>
			<p>Text generation models <a id="_idIndexMarker082"/>can be used for diverse tasks as outlined here. In the following, we have mentioned the most popular tasks that we have seen architecting solutions with our customers:</p>
			<ul>
				<li><strong class="bold">Summarization</strong>: They can <a id="_idIndexMarker083"/>condense long documents, such as textbook chapters or detailed product descriptions, into concise summaries while retaining the key information.</li>
				<li><strong class="bold">Question answering</strong>: These <a id="_idIndexMarker084"/>models can provide accurate answers to questions, which is particularly useful in automating the creation of FAQ documents from extensive knowledge base content.</li>
				<li><strong class="bold">Classification</strong>: Text <a id="_idIndexMarker085"/>generation models can classify text, assigning labels based on criteria such as grammatical correctness or other predefined categories.</li>
				<li><strong class="bold">Sentiment analysis</strong>: As a <a id="_idIndexMarker086"/>specialized form of classification, these models can analyze and label the sentiment of a text, identifying emotions such as happiness and anger or general positive and negative tones.</li>
				<li><strong class="bold">Entity extraction</strong>: They can <a id="_idIndexMarker087"/>extract specific pieces of information, such as movie names, from larger text bodies, aiding in information retrieval and organization.</li>
				<li><strong class="bold">Translation</strong>: Language <a id="_idIndexMarker088"/>models excel in translation by quickly and accurately converting text from one language to another, leveraging vast datasets to understand and maintain context and nuances. Code generation can be considered a type of translation, where the language model translates human language instructions into programming code.</li>
			</ul>
			<p>These capabilities make text-generation models invaluable tools and have led to the creation of innovative applications. Here we have mentioned a few interesting business applications we have observed across various industries due to the proliferation of text generation models:</p>
			<ul>
				<li><strong class="bold">Enterprise chatbots</strong>: Text <a id="_idIndexMarker089"/>generation models power conversational agents that can engage in natural language conversations <a id="_idIndexMarker090"/>with users, offering customer support, HR support, L&amp;D, and assistance with tasks. The top use case in terms of popularity that we observed was the implementation of an enterprise chatbot grounded on organizational data.</li>
				<li><strong class="bold">Content creation (articles, blog posts, reports, books)</strong>: Text generation models can automatically generate high-quality <a id="_idIndexMarker091"/>written content on various topics, saving time and effort for content creators and enabling seamless Q&amp;A experiences on the same. This has been a major productivity booster in the media, marketing, entertainment, and publication industries.</li>
				<li><strong class="bold">Real estate listings</strong>: Text generation <a id="_idIndexMarker092"/>models enable real estate companies to effortlessly craft attractive house listings by inputting details such as the number of bedrooms, property age, neighborhood information, and other unique selling points, significantly enhancing the appeal of properties to potential buyers.</li>
				<li><strong class="bold">Automatic email drafting</strong>: Text <a id="_idIndexMarker093"/>generation models assist in composing personalized and contextually relevant emails, streamlining communication, and improving productivity in email correspondence, for example, Microsoft’s Copilot application.</li>
				<li><strong class="bold">Personalized advertising</strong>: These models <a id="_idIndexMarker094"/>help tailor marketing messages and content to individual users, enhancing the effectiveness of advertising campaigns by delivering more relevant and engaging content.</li>
				<li><strong class="bold">Proposal creation</strong>: They <a id="_idIndexMarker095"/>significantly streamline the operations of real estate companies by automating the creation of <a id="_idIndexMarker096"/>proposals for <strong class="bold">request for proposal</strong> (<strong class="bold">RFP</strong>) responses. This tool also facilitated efficient searching through RFP submissions and greatly assisted marketing teams in crafting and authoring high-quality content.</li>
				<li><strong class="bold">Ad campaigns</strong>: In the realm of <a id="_idIndexMarker097"/>marketing and advertising campaigns, text generation models offer a powerful advantage by providing precise and efficient summarization of lengthy content. Moreover, these models seamlessly translate text between various languages, effectively dismantling language barriers. This capability enhanced cross-cultural communication, enabling marketers to reach and resonate with a diverse, global audience more effectively.</li>
				<li><strong class="bold">Code co-pilot</strong>: Developer <a id="_idIndexMarker098"/>productivity in <a id="_idIndexMarker099"/>organizations has increased tremendously due to products such as GitHub Copilot.</li>
			</ul>
			<p>The following highlights the leading text generation models as of early 2024 in a rapidly advancing field:</p>
			<ul>
				<li><strong class="bold">GPT-4-Turbo</strong>: Developed <a id="_idIndexMarker100"/>by OpenAI, the most popular model in production today. GPT-4 is a large multimodal model with <a id="_idIndexMarker101"/>deep learning capabilities, enabling it to generate human-like, conversational text. It can accept both text and image inputs to produce human-like text outputs. It accepts 128,000 tokens in its context window, which is close to 300 pages of text.</li>
				<li><strong class="bold">Llama 2</strong>: The Llama 2 <a id="_idIndexMarker102"/>open source <a id="_idIndexMarker103"/>models have been trained on 2 trillion tokens and offer double the context length (~4K tokens) of their predecessor, Llama 1. These models excel in a variety of benchmarks, including reasoning, coding, proficiency, and knowledge tests, and include specialized chat models trained on over one million new human annotations.</li>
				<li><strong class="bold">Mistral</strong>: Developed by <a id="_idIndexMarker104"/>Mistral AI, founded by former Meta and Google AI researchers, Mistral is a leading open source model LLM with 7.3 <a id="_idIndexMarker105"/>billion parameters, capable of generating coherent text and performing various natural language processing tasks. It represents a significant advancement over previous models, outperforming many existing AI models in a variety of benchmarks.</li>
				<li><strong class="bold">PaLM-2</strong>: Developed <a id="_idIndexMarker106"/>by Google, PaLM-2, which <a id="_idIndexMarker107"/>stands for pathways language model, is a next-generation language model part of a family of LLMs trained on a vast amount of data for next-word prediction. It shows improved multilingual, reasoning, and coding capabilities, and is extensively trained on multilingual text, covering over 100 languages.</li>
				<li><strong class="bold">Claude2</strong>: Developed by Anthropic, <a id="_idIndexMarker108"/><a id="_idIndexMarker109"/>Claude2 is an advanced version of its predecessor, Claude. This LLM is designed to be safer and <a id="_idIndexMarker110"/>more capable, with improved performance and longer response capabilities. It can handle a context window of up to 100K tokens, allowing it to work with extensive documents. Claude-2 has been noted for its focus on AI safety and its potential as a competitor in the field of conversational AI.</li>
				<li><strong class="bold">Gemini 1.5</strong>: Google’s latest model was <a id="_idIndexMarker111"/>released in <a id="_idIndexMarker112"/>February 2024 with more efficient architecture and enhanced performance. It comes in three sizes: Ultra, Pro, and Nano, and can accept up to one million tokens in the context window.</li>
			</ul>
			<p>Next, let’s explore image generation models.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Image</h2>
			<p>In the evolving landscape of <a id="_idIndexMarker113"/>computer vision, image generation models are advancing, with key areas such as image synthesis and classification already somewhat mature. Emerging fields include visual question and answer, which interprets images to answer queries, and image segmentation, which breaks down images for detailed analysis. The key areas are detailed in the following:</p>
			<ul>
				<li><strong class="bold">Image synthesis</strong>: Generating new <a id="_idIndexMarker114"/>images or altering existing ones based on specific inputs or requirements</li>
				<li><strong class="bold">Image classification</strong>: Identifying <a id="_idIndexMarker115"/>and categorizing objects within an image into predefined classes, crucial for applications such as facial recognition and automated photo tagging</li>
				<li><strong class="bold">Visual question answering</strong> (<strong class="bold">VQA</strong>): Combining <a id="_idIndexMarker116"/>image processing and natural language understanding to answer questions about a given image</li>
				<li><strong class="bold">Image segmentation</strong>: Dividing an <a id="_idIndexMarker117"/>image into segments or parts for simpler, more meaningful analysis</li>
			</ul>
			<p>These capabilities make image-generation <a id="_idIndexMarker118"/>models invaluable tools and have led to the creation of innovative applications. In the following, we have mentioned a few interesting business applications that are emerging across various industries due to the advancements in recent image generation models:</p>
			<ul>
				<li><strong class="bold">Generating Images from Text Descriptions</strong>: Image <a id="_idIndexMarker119"/>generation models can take text descriptions as input and create corresponding images. This is valuable in applications such as generating illustrations for books, articles, or product listings. For example, a text description of a tropical beach scene can be turned into a realistic image of that scene, aiding in visual storytelling and marketing.</li>
				<li><strong class="bold">Storyboarding</strong>: Entertainment firms are <a id="_idIndexMarker120"/>utilizing image-generation models for crafting storyboards. These visual aids depict narratives, concepts, or scripts, offering a glimpse into how a story might appear when animated or performed.</li>
				<li><strong class="bold">Fashion design</strong>: Image <a id="_idIndexMarker121"/>generation models are helping fashion designers create new clothing designs by generating various apparel designs, patterns, and color combinations. Designers can input parameters or inspiration, and the model can generate visual concepts to inspire new collections.</li>
				<li><strong class="bold">Interior design</strong>: Similarly, for <a id="_idIndexMarker122"/>interior designers, these models can generate room layouts, furniture arrangements, and decor ideas based on input criteria, enabling quick and creative design exploration.</li>
				<li><strong class="bold">Automatic photo editing</strong>: Image <a id="_idIndexMarker123"/>generation models can be used to automate and enhance the photo editing process. They can intelligently adjust color balance, contrast, and lighting, remove unwanted objects or blemishes, and apply artistic filters or styles to photos. This can significantly reduce the time and effort required for manual photo editing tasks.</li>
				<li><strong class="bold">Creating digital artwork</strong>: Digital artists <a id="_idIndexMarker124"/>and illustrators can use image generation models to spark their creativity. These models can generate abstract or realistic art pieces, offer new design ideas, or assist in creating concept art for various projects. Artists can use the generated images as a starting point for their work.</li>
				<li><strong class="bold">Doctor copilot</strong>: This falls under the <a id="_idIndexMarker125"/>multimodal category, where the diverse functionalities of LLMs are applied to a variety of medical imaging tasks, including medical visual question-and-answer scenarios. Essentially, this involves developing applications that can respond to queries from doctors regarding X-rays or CT scans as well as aid in the generation of radiology reports.</li>
				<li><strong class="bold">Facial recognition</strong>: Image generation <a id="_idIndexMarker126"/>models can enhance facial recognition by creating diverse, high-quality training datasets, enabling the <a id="_idIndexMarker127"/>algorithms to learn and identify a wide range of facial features and expressions under various conditions. Additionally, they can assist in reconstructing partial or obscured faces in images, improving the accuracy and reliability of recognition systems.</li>
			</ul>
			<p>The following highlights the leading image generation models as of December 2023 in a rapidly advancing field:</p>
			<ul>
				<li><strong class="bold">DALL-E3</strong>: Developed <a id="_idIndexMarker128"/>by OpenAI, DALL-E 3 is an <a id="_idIndexMarker129"/>advanced AI model capable of generating detailed and imaginative images from textual descriptions.</li>
				<li><strong class="bold">Google’s Imagen</strong>: Imagen by <a id="_idIndexMarker130"/>Google is a text-to-image diffusion AI model known for producing highly photorealistic <a id="_idIndexMarker131"/>images from textual prompts.</li>
				<li><strong class="bold">Stable Diffusion</strong>: Stable Diffusion, an <a id="_idIndexMarker132"/>open source model created by Stability AI, is a text-to-image model designed to <a id="_idIndexMarker133"/>generate high-quality images based on user-provided text descriptions.</li>
				<li><strong class="bold">Midjourney v5.2</strong>: Midjourney v5.2, developed by Midjourney Inc. and launched in June 2023, represents the latest and most <a id="_idIndexMarker134"/>sophisticated iteration of Midjourney’s AI image generation model. This version <a id="_idIndexMarker135"/>focuses on enhancing the performance, consistency, and quality of the generated images. It is known for producing more detailed and sharper results with improved colors, contrast, and compositions compared to its predecessors.</li>
				<li><strong class="bold">Segment Anything Model</strong> (<strong class="bold">SAM</strong>): The Segment Anything Model developed by Facebook’s Meta AI is not primarily an image generation model; instead, it’s an image segmentation model. Image segmentation models are <a id="_idIndexMarker136"/>designed to identify and delineate specific parts or objects within an image, essentially segmenting the <a id="_idIndexMarker137"/>image into different areas based on the objects present. We have mentioned it here as it falls under models within the realm of computer vision.<p class="list-inset">The following figure shows the segmentation of the New York skyline into different objects using SAM:</p></li>
			</ul>
			<div><div><img src="img/B21443_01_7.jpg" alt="Figure 1.7 – Image segmentation example"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Image segmentation example</p>
			<p>Let’s move on to audio generation models.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Audio</h2>
			<p>Audio generation models are <a id="_idIndexMarker138"/>versatile tools for various applications, as demonstrated through our experience in developing solutions with our customers. The most popular tasks are as follows:</p>
			<ul>
				<li><strong class="bold">Speech synthesis</strong>: Generating human-like <a id="_idIndexMarker139"/>speech from text (text-to-speech) and used in voice assistants, audiobooks, and various accessibility tools</li>
				<li><strong class="bold">Speaker identification</strong>: Recognizing and <a id="_idIndexMarker140"/>differentiating between different speakers in audio recordings, which can be useful in security systems and personalized user experiences</li>
				<li><strong class="bold">Emotion detection</strong>: Identifying <a id="_idIndexMarker141"/>emotions from speech, which can enhance customer service interactions or aid in mental health assessments</li>
				<li><strong class="bold">Sound generation</strong>: Creating <a id="_idIndexMarker142"/>music or sound effects using AI, which has applications in entertainment, gaming, and virtual reality</li>
				<li><strong class="bold">Voice cloning</strong>: Generating a <a id="_idIndexMarker143"/>synthetic voice that sounds like a specific person, which can be used in personalized speech interfaces or entertainment</li>
				<li><strong class="bold">Speech recognition</strong>: Converting <a id="_idIndexMarker144"/>spoken language into text, which is fundamental in creating transcriptions, automated subtitles, and voice commands</li>
				<li><strong class="bold">Speech translation</strong>: Translating <a id="_idIndexMarker145"/>spoken language from one language to another in real-time, facilitating cross-lingual communication</li>
			</ul>
			<p>Audio-based LLMs can <a id="_idIndexMarker146"/>generate various forms of audio, such as speech, music, and sound effects, based on textual or other input. For instance, here we mention a few emerging noteworthy business applications with audio generation models:</p>
			<ul>
				<li><strong class="bold">ChatBot audio and avatar</strong>: Recent <a id="_idIndexMarker147"/>advancements in avatar-based experiences have led organizations to create immersive audio experiences featuring copilots with lifelike avatars</li>
				<li><strong class="bold">Music composition and production</strong>: These <a id="_idIndexMarker148"/>models are used to create new music pieces, simulate various musical styles, and assist composers in exploring new soundscapes and melodies</li>
				<li><strong class="bold">Sound effects and Foley in media production</strong>: They can generate realistic or imaginative sound effects for use in films, video games, and <a id="_idIndexMarker149"/>other multimedia projects, offering a cost-effective alternative to traditional Foley artistry</li>
				<li><strong class="bold">Language learning and pronunciation training</strong>: By generating accurate and diverse speech samples, these models aid in <a id="_idIndexMarker150"/>language learning applications, helping users with pronunciation and listening comprehension</li>
				<li><strong class="bold">Accessibility applications</strong>: Audio <a id="_idIndexMarker151"/>generation models are crucial in developing tools for visually impaired individuals, converting text and visual information into audio, thus enhancing accessibility in various digital platforms</li>
			</ul>
			<p>This space is evolving, but there hasn’t been as much advancement in this domain as with text and image generation models. Here we mention a couple of interesting audio generation models from Google and OpenAI:</p>
			<ul>
				<li><strong class="bold">MusicLM</strong>: From <a id="_idIndexMarker152"/>Google Research, this is a cutting-edge AI model that <a id="_idIndexMarker153"/>transforms music creation using text prompts. It generates high-quality music across genres from simple text inputs. This innovative model utilizes a sophisticated hierarchical sequence-to-sequence approach, trained on a dataset of 5.5K expert-crafted music-text pairs, offering valuable opportunities for researchers and music enthusiasts.</li>
				<li><strong class="bold">Open AI JukeBox</strong>: This model, created by <a id="_idIndexMarker154"/>OpenAI in 2020, generates new <a id="_idIndexMarker155"/>music samples based on <a id="_idIndexMarker156"/>inputs such as genre, artist, and lyrics (<a href="https://github.com/openai/jukebox">https://github.com/openai/jukebox</a>).</li>
			</ul>
			<p>Finally, we look at video generation models.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Video</h2>
			<p>Video generation models, which are <a id="_idIndexMarker157"/>advanced forms of AI designed to create, manipulate, and analyze video content, can perform a wide range of tasks. Some of the key emerging tasks across our customers in this field are as follows:</p>
			<ul>
				<li><strong class="bold">Video synthesis</strong>: Creating new <a id="_idIndexMarker158"/>video content from scratch or based on textual descriptions, which includes generating realistic scenes, animations, or simulations</li>
				<li><strong class="bold">Deepfake generation</strong>: Creating <a id="_idIndexMarker159"/>highly realistic and convincing videos where one person’s likeness is replaced with another, often used in film production, in education, or for entertainment purposes</li>
				<li><strong class="bold">Video editing and enhancement</strong>: Automatically <a id="_idIndexMarker160"/>editing videos to improve their quality, such as enhancing resolution, color correction, and stabilizing shaky footage</li>
				<li><strong class="bold">Video summarization</strong>: Condensing <a id="_idIndexMarker161"/>longer videos into shorter summaries while retaining the essential content, which is useful for quickly conveying information in large video files</li>
				<li><strong class="bold">Object tracking and recognition</strong>: Identifying and <a id="_idIndexMarker162"/>tracking objects or individuals across a video sequence, which is crucial for surveillance, sports analysis, and autonomous vehicles</li>
				<li><strong class="bold">Scene understanding</strong>: Analyzing a video to <a id="_idIndexMarker163"/>understand the context, setting, or events taking place, which can be applied in video indexing and search systems</li>
				<li><strong class="bold">Motion analysis</strong>: Studying the <a id="_idIndexMarker164"/>movement of objects or individuals within a video, applicable in sports training, physical therapy, and animation</li>
				<li><strong class="bold">Facial expression and gesture analysis</strong>: Interpreting facial <a id="_idIndexMarker165"/><a id="_idIndexMarker166"/>expressions and body language to gauge emotions, reactions, or intentions, which is useful in customer service or behavioral studies</li>
				<li><strong class="bold">Video-to-text transcription</strong>: Converting the visual and auditory components of a video into textual descriptions, aiding in <a id="_idIndexMarker167"/>content accessibility and searchability</li>
				<li><strong class="bold">Interactive video creation</strong>: Generating <a id="_idIndexMarker168"/>interactive videos where viewers can influence the storyline or outcome, enhancing user engagement in gaming, education, and marketing</li>
			</ul>
			<p>Text-to-video models are a type of AI technology that generates video content based on textual descriptions. While there have been considerable <a id="_idIndexMarker169"/>advancements in recent <strong class="bold">text-to-video</strong> (<strong class="bold">T2V</strong>) generation techniques, most of these developments are concentrated on creating short video clips that depict a single event set against a single backdrop, essentially limited to single-scene videos. As video generation models evolve, exciting new <a id="_idIndexMarker170"/>applications are beginning to emerge, offering innovative possibilities in this field:</p>
			<ul>
				<li><strong class="bold">Q&amp;A over video archive</strong>: In the media and <a id="_idIndexMarker171"/>entertainment industry, a prominent use case emerging involves embedding video data using models such as CLIP and then creating enhanced search experiences on top of it</li>
				<li><strong class="bold">Film and animation</strong>: These models can <a id="_idIndexMarker172"/>aid in rapidly prototyping scenes and creating short animations, streamlining the filmmaking and animation process</li>
				<li><strong class="bold">Advertising and marketing</strong>: Businesses can <a id="_idIndexMarker173"/>utilize video generation models to create engaging content for marketing campaigns and advertisements tailored to specific audiences</li>
				<li><strong class="bold">Education and training</strong>: They can <a id="_idIndexMarker174"/>enhance educational content by producing custom videos that illustrate complex concepts or simulate real-life scenarios for more effective learning and training</li>
				<li><strong class="bold">Gaming and virtual reality</strong>: In gaming, these <a id="_idIndexMarker175"/>models can be used to generate dynamic environments and characters, enriching the gaming experience, and reducing development time</li>
				<li><strong class="bold">Research and development</strong>: Video generation <a id="_idIndexMarker176"/>models are valuable in visualizing scientific theories, simulating experiments, or <a id="_idIndexMarker177"/>presenting research findings in an interactive format</li>
			</ul>
			<p>This space is evolving and there hasn’t been as much advancement in the video domain as with text and image generation models. Here we mention two models with promising capabilities in the video space:</p>
			<ul>
				<li><strong class="bold">Stable Video Diffusion</strong>: Announced in <a id="_idIndexMarker178"/>November 2023 by Stability AI, this is a model that creates high-resolution videos (576 x 1024) from text or single images. It advances latent diffusion models <a id="_idIndexMarker179"/>previously limited to 2D images to video, maintaining high detail at 14 or 25 frames per second. The research highlights the importance of data curation in enhancing high-resolution video generation performance (<a href="https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt">https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt</a>).</li>
				<li><strong class="bold">GPT-4V</strong>: From OpenAI, this is a <a id="_idIndexMarker180"/>multimodal LLM capable of <a id="_idIndexMarker181"/>analyzing videos but unable to generate videos as of early 2024.</li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout">OpenAI announced <a id="_idIndexMarker182"/>SORA in early 2024, its first text-to-video generation model. Although it has not been released to the public as it is undergoing comprehensive red teaming testing, based on the samples shared by OpenAI, we think this innovation is a significant leap in multimodal LLMs. It allows you to transform text prompts into high-quality, one-minute videos.</p>
			<p>Here’s what SORA brings to the table:</p>
			<ul>
				<li><strong class="bold">Complex scene generation</strong>: SORA excels in creating detailed scenes featuring multiple characters, various motions, and precise subject and background details. The model understands not only what the user has asked for in the prompt, but also how those things exist in the physical world.</li>
				<li><strong class="bold">Advanced language comprehension</strong>: With its profound grasp of language, SORA can bring prompts to life with characters that showcase a range of emotions. Moreover, it can craft multiple shots within a video, maintaining consistency in character and visual style.</li>
			</ul>
			<p>We have highlighted the most prominent LLMs currently known. However, the field is advancing swiftly, and fresh models are continuously emerging. For the latest and trending models, we suggest regularly visiting the Hugging Face website, which maintains an up-to-date list of these innovative and influential models (<a href="https://huggingface.co/models">https://huggingface.co/models</a>).</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Cloud computing for scalability, cost optimization, and security</h1>
			<p>Cloud computing has been <a id="_idIndexMarker183"/>instrumental in bringing LLMs to a wider audience. LLMs use large-scale GPU processing to learn and generate human-like text, image, audio, and video, engaging in increasingly interactive and intelligent ways.</p>
			<p>This section highlights several advantages of <a id="_idIndexMarker184"/>leveraging LLMs in a cloud environment:</p>
			<ul>
				<li><strong class="bold">Scalability</strong>: Cloud computing enables users to access high-performance computing such as GPUs as necessary to run LLMs. This makes it easy to scale applications as required based on consumption needs.<p class="list-inset">Since LLM models such as GPT are heavy API-driven workloads, there is a need for API management services, such as Azure APIM, that help achieve scalability, security, and high availability across regions. They can also capture telemetry that can help determine token usage and error logging across organizations. We discuss scaling strategies on Azure in <a href="B21443_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>.</p></li>
				<li><strong class="bold">Affordability</strong>: There is no need for large upfront infrastructure investment as you can easily access computing power from the cloud, making it more affordable. Utilizing a pay-as-you-go service allows you the flexibility to activate instances for open source models as needed and terminate them at your convenience, ensuring that you have control and adaptability in managing your resources.</li>
				<li><strong class="bold">Data storage</strong>: LLMs may require a large amount of data for training and fine-tuning. Cloud services offer scalable and cheap storage options to manage vast amounts of structured and unstructured data.<p class="list-inset">For instance, Azure Blob Storage provides several cheap and flexible storage options for storing structured and unstructured data and this can be used in conjunction with Azure AI search to enable vector storage with advanced security capabilities.</p></li>
				<li><strong class="bold">Accessibility and collaboration</strong>: Cloud platforms make it easy to access LLMs from anywhere in the world, making it easy for researchers, data scientists, cloud architects, and developers to collaborate.</li>
				<li><strong class="bold">Managed services</strong>: Cloud platforms offer managed services that can simplify deployment and infrastructure management for LLMs on the cloud.<p class="list-inset">For instance, Microsoft’s model-as-a-service allows you to deploy open source models such as Llama 2 as a pay-as-you-go service. Azure handles the infrastructure provisioning and charges you based on token usage. This eliminates the management overhead of provisioning inference computing for open source models.</p></li>
				<li><strong class="bold">Speed</strong>: With access to the cloud, you have access to high-speed computer power, providing you with more options based on the latency needs of your LLM applications.<p class="list-inset">In Azure, you can get access to several GPU-optimized VM sizes options, such as Nvidia A100s V4 series and NCV3 series (<a href="https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu">https://learn.microsoft.com/en-us/azure/virtual-machines/sizes-gpu</a>).</p><p class="list-inset">Different LLMs may necessitate varying sizes of GPU computing power that affect the latency and cost of running the applications.</p></li>
				<li><strong class="bold">Security and compliance</strong>: Top cloud platforms provide comprehensive and industry-leading security and compliance services for your data, thus providing authentication, authorization, encryption, monitoring, and logging capabilities to protect your AI infrastructure. They also provide services to identify potential jailbreak attacks. Jailbreak attacks on LLMs are methods used to bypass or manipulate the model’s safety and ethical guidelines to elicit prohibited or restricted responses. We will learn more about jailbreak attacks in <a href="B21443_08.xhtml#_idTextAnchor163"><em class="italic">Chapter 8</em></a> on security.</li>
				<li><strong class="bold">Responsible AI solutions</strong>: With the advent of new-generation AI applications, implementing robust guardrails to detect and filter out harmful content becomes crucial. Tools such as Azure Content Safety are designed to moderate text and image content, helping to maintain a safe and appropriate user experience. Additionally, the use of safety metaprompts, which are essentially guiding instructions or constraints embedded in the <a id="_idIndexMarker185"/>system messages of LLMs, plays a vital role. These metaprompts can instruct the LLM to avoid generating inappropriate, biased, or harmful content, acting as an integral part of the model’s ethical framework and ensuring responsible AI usage.</li>
			</ul>
			<p>While it’s possible to deploy certain open source models on personal laptops or establish a dedicated infrastructure within an organization, this approach often incurs substantial upfront costs, including significant investment in talent acquisition and ongoing management overhead. Additionally, maintaining the security of such infrastructure might not match the advanced levels offered by cloud service providers. Therefore, cloud services emerge as the more advantageous solution, offering a wide array of flexible, secure, scalable, and ethically responsible options for deploying generative AI solutions. In the next section, we will delve into the process of transforming an innovative idea into reality, examining the various stages involved in deploying it on the cloud and using our experiences as cloud solution architects during the initial stages of generative AI deployments across various organizations.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>From vision to value – navigating the journey 
to production</h1>
			<p>Developing an idea and moving it into <a id="_idIndexMarker186"/>production is a multi-phase process that typically involves ideation, validation, development, testing, and deployment. The multi-phase process of developing an idea and moving it into production is crucial because it methodically transforms a concept into a viable product.</p>
			<p>Take a look at the following image about overlooking a crucial aspect:</p>
			<div><div><img src="img/B21443_01_8.jpg" alt="Figure 1.8 – Two entrepreneurs engaging in a humorous discussion about overlooking expenses"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Two entrepreneurs engaging in a humorous discussion about overlooking expenses</p>
			<p>The above image satirically showcases how some organizations claim to build AI from scratch, when in reality, they're just utilizing API calls to services like OpenAI. It humorously uncovers this exaggeration when asked about the Open AI bills, mocking the notion of starting from scratch.</p>
			<p>Each phase serves a distinct purpose: ideation <a id="_idIndexMarker187"/>fosters innovation, validation ensures market demand and feasibility, development translates validated ideas into tangible products, testing guarantees functionality and user satisfaction, and deployment introduces the product into the market. This structured approach mitigates risks, optimizes the use of resources, assures product quality, and secures market fit. It’s a strategic pathway that allows for informed decision-making and efficient allocation of capital and maximizes the chances of commercial success. Here’s a structured approach:</p>
			<div><div><img src="img/B21443_01_9.jpg" alt="Figure 1.9 – Stages from ideation to deployment"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Stages from ideation to deployment</p>
			<p>Let’s look at <a id="_idIndexMarker188"/>each <a id="_idIndexMarker189"/>stage in more detail.</p>
			<p>The following steps are involved in ideation:</p>
			<ul>
				<li>Generate and brainstorm ideas without constraints to encourage creativity</li>
				<li>Prioritize ideas based on factors such as feasibility, market potential, and alignment with business goals</li>
			</ul>
			<p class="callout-heading">Hackathon events: fostering innovation in generative AI</p>
			<p class="callout">In our early roles as Cloud Architects in the generative AI space, we witnessed a surge of hackathon events across various organizations. These events, integral to the ideation phase, encouraged rapid problem-solving, innovative thinking, and the free exchange of ideas, unencumbered by the usual workplace constraints. Participants were exposed to new perspectives and skills, while the event’s structure promoted quick development and validation of ideas. The combination of collaboration, focused effort, and a supportive community made hackathons an ideal breeding ground for creative solutions and new concepts.</p>
			<p>The following steps are involved in market research and validation:</p>
			<ul>
				<li>Conduct thorough market research to understand the demand and competition</li>
				<li>Validate the idea through customer interviews, surveys, or focus groups</li>
			</ul>
			<p>The following steps are<a id="_idIndexMarker190"/> involved in <strong class="bold">Proof of </strong><strong class="bold">Concept</strong> (<strong class="bold">PoC</strong>):</p>
			<ul>
				<li>Create a PoC to demonstrate the idea’s feasibility</li>
				<li>Use the PoC to gather initial feedback and iterate on the design</li>
				<li>Determine success criteria for the PoC</li>
			</ul>
			<p class="callout-heading">Initial PoCs: leveraging ChatGPT for internal co-pilots</p>
			<p class="callout">Drawing from our experience, the initial PoCs typically involve internal-facing co-pilots utilizing the ‘ChatGPT on your data’ feature on Azure focused on organizational data. These projects were seen as low-hanging fruit, offering rapid wins and valuable lessons learned.</p>
			<p>The following<a id="_idIndexMarker191"/> steps are involved in business case and planning:</p>
			<ul>
				<li>Build a business case by outlining the value proposition, market entry strategy, and financial projections</li>
				<li>Plan the project, including timelines, budget, resources, and risk assessment</li>
				<li>Determine ROI</li>
			</ul>
			<p class="callout-heading">ROI for generative AI workloads</p>
			<p class="callout">Assessing the ROI of generative AI workloads poses a significant challenge, involving not only the calculation of the end-to-end solution cost but also the quantification of returns through automation and the elimination of manual tasks. Adding to this, offering the solution as a white-label product for other companies can substantially enhance ROI. This approach opens new revenue streams, offers cost efficiency for clients, enables scalability, indirectly boosts brand recognition, and provides a rich feedback loop for product improvement. By leveraging white labeling, businesses can maximize the value and reach of their generative AI solutions, making it a strategic move to increase overall returns on investment in a competitive market. In <a href="B21443_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, we discuss a few of the cost optimization strategies companies can leverage to reduce their overall cost of generative AI workloads.</p>
			<p>The following steps<a id="_idIndexMarker192"/> are involved in prototype/MVP development:</p>
			<ul>
				<li>Develop a prototype that’s closer to the product than the PoC</li>
				<li>Iterate on the prototype based on feedback and technical feasibility</li>
				<li>Develop an MVP with the minimal necessary features to satisfy early adopters</li>
				<li>The MVP serves to validate product-market fit and gather user feedback</li>
			</ul>
			<p>The following steps are involved in testing and quality assurance:</p>
			<ul>
				<li>Perform various types of testing (unit, integration, system, user acceptance)</li>
				<li>Ensure that the product meets quality standards and is free of critical bugs</li>
			</ul>
			<p>The following steps are involved in pre-production and staging:</p>
			<ul>
				<li>Deploy the application in a staging environment that closely mimics production</li>
				<li>Conduct further testing, including load and performance tests</li>
			</ul>
			<p>The following steps are involved in the deployment strategy:</p>
			<ul>
				<li>Develop a deployment strategy, such as blue-green deployments and canary releases to minimize risks</li>
				<li>Plan for rollback procedures in the case of failures</li>
			</ul>
			<p>The following steps are involved in the launch:</p>
			<ul>
				<li>Launch the product to the target user base</li>
				<li>Monitor the product closely for any issues or unexpected behaviors</li>
			</ul>
			<p>The following steps are involved in continuous monitoring and feedback loop:</p>
			<ul>
				<li>Establish mechanisms for continuous monitoring, error logging, and performance tracking through LLMOps</li>
				<li>Create feedback channels for users to report issues or suggest improvements</li>
			</ul>
			<p class="callout-heading">Tip</p>
			<p class="callout"><strong class="bold">Large language model operations</strong> (<strong class="bold">LLMOps</strong>) focus on deploying, managing, and scaling LLMs in production to<a id="_idIndexMarker193"/> ensure that they integrate smoothly into applications for optimal performance, security, and cost-effectiveness. This involves practices such as continuous integration and deployment for automated updates, continuous monitoring for performance and cost efficiency, version control for updates without disruption, security measures for compliance, and auto-scaling for demand changes. LLMOps are crucial for organizations using LLMs in production, simplifying operational challenges to foster innovation. More on LLMOps is discussed in <a href="B21443_06.xhtml#_idTextAnchor117"><em class="italic">Chapter 6</em></a>.</p>
			<p>The following steps are involved in iterative improvement:</p>
			<ul>
				<li>Use data and user feedback to make iterative improvements to the product</li>
				<li>Plan for regular updates and feature releases</li>
			</ul>
			<p>The following steps are involved in scalability:</p>
			<ul>
				<li>Ensure that the architecture is scalable to handle growth in users or data</li>
				<li>Regularly<a id="_idIndexMarker194"/> review infrastructure and optimize as necessary</li>
			</ul>
			<p class="callout-heading">We recommend</p>
			<p class="callout">This approach is vital to guarantee superior user experience, ensuring the solution’s high availability and incorporating disaster recovery measures. We discuss these concepts elaborately in <a href="B21443_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>.</p>
			<p>The following steps are involved in maintenance and support:</p>
			<ul>
				<li>Provide ongoing maintenance and support to users</li>
				<li>Keep the product<a id="_idIndexMarker195"/> up to date with the latest security patches and compliance standards</li>
			</ul>
			<p>Throughout this process, it’s essential to stay agile and be prepared to pivot or make changes based on new insights and feedback. Communicate regularly with all stakeholders and ensure that there’s a clear understanding of the vision, progress, and challenges associated with developing the idea and moving it into production.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Summary</h1>
			<p>The aim of this introductory chapter was to highlight the history, core concepts, and other essential information necessary for readers to develop an end-to-end generative AI solution on the cloud. We have explored the evolution of chatbots from simple rule-based systems to multimodal, context-aware, and action-oriented agentic LLMs. We delved into the rise of generative AI, focusing on LLMs and foundation models as well as their relationship and key attributes. The differences between open source and closed source models were examined, alongside trending business applications drawn from our experiences. In the rapidly evolving landscape of AI, we’ve examined a few leading models, including text, image, audio, and video generation. These models represent the forefront of AI technology, showcasing remarkable capabilities in creating high-quality, lifelike content. We then highlighted how cloud computing facilitates the development of secure, scalable, cost-efficient, and ethical generative AI applications. We also outlined a framework for transforming ideas into production-ready solutions. In the next chapter, we’ll dive into the NLP capabilities of LLMs and their transformer architecture, which is fundamental to the functioning of these models.</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>References</h1>
			<ul>
				<li>Nvidia Generative AI:<a href="https://www.nvidia.com/en-us/glossary/data-science/generative-ai/#:~:text=Generative%20AI%20models%20use%20neural,semi%2Dsupervised%20learning%20for%20training"> https://www.nvidia.com/en-us/glossary/data-science/generative-ai/#:~:text=Generative%20AI%20models%20use%20neural,semi%2Dsupervised%20learning%20for%20training</a></li>
				<li>CSET Georgetown University: <a href="https://cset.georgetown.edu/article/what-are-generative-ai-large-language-models-and-foundation-models/#:~:text=Using%20the%20term%20%E2%80%9Cgenerative%20AI,system%20that%20works%20with%20language">https://cset.georgetown.edu/article/what-are-generative-ai-large-language-models-and-foundation-models/#:~:text=Using%20the%20term%20%E2%80%9Cgenerative%20AI,system%20that%20works%20with%20language</a></li>
				<li>Databricks course: <a href="https://microsoft-academy.databricks.com/learn/course/1765/play/12440/llms-and-generative-ai">https://microsoft-academy.databricks.com/learn/course/1765/play/12440/llms-and-generative-ai</a></li>
			</ul>
		</div>
	</body></html>