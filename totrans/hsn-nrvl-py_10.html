<html><head></head><body>
        

                            
                    <h1 class="header-title">Hypercube-Based NEAT for Visual Discrimination</h1>
                
            
            
                
<p class="p1">In this chapter, you will learn about the main concepts behind a hypercube-based NEAT algorithm and about the main challenges it was designed to solve. We take a look at the problems that arise when attempting to use direct genome encoding with large-scale <strong>artificial neural networks</strong> (<strong>ANN</strong>) and how they can be solved with the introduction of an indirect genome encoding scheme. You will learn how a <strong>Compositional Pattern Producing</strong> <strong>Network</strong> (<strong>CPPN</strong>) can be used to store genome encoding information with an extra-high compression rate and how CPPNs are employed by the HyperNEAT algorithm. Finally, you will work with practical examples that demonstrate the power of the HyperNEAT algorithm.</p>
<p class="p1">In this chapter, we discuss the following topics:</p>
<ul class="ul1">
<li class="li1">The problem with the direct encoding of large-scale natural networks using NEAT, and how HyperNEAT can help by introducing the indirect encoding method</li>
<li class="li1">The evolution of CPPNs with NEAT to explore geometric regularities within the hypercube, which allows us to efficiently encode connectivity patterns within the target ANN</li>
<li class="li1">How to use the HyperNEAT method to detect and recognize objects in a visual field</li>
<li class="li1">The definition of the objective function for a visual discrimination experiment</li>
<li class="li1">A discussion of the visual discrimination experiment results</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p class="p1">The following technical requirements should be met in order to execute the experiments described in this chapter:</p>
<ul class="ul1">
<li class="li1">Windows 8/10, macOS 10.13 or newer, modern Linux</li>
<li class="li1">Anaconda Distribution version 2019.03 or newer</li>
</ul>
<div><div><div><p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/tree/master/Chapter7">https:/​/​github.​com/​PacktPublishing/​Hands- on-​Neuroevolution-​with-​Python/​tree/​master/​Chapter7</a></p>
</div>
</div>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Indirect encoding of ANNs with CPPNs</h1>
                
            
            
                
<p class="p1">In the previous chapters, you learned about the direct encoding of ANNs using the nature-inspired conception of a genotype that is mapped to the phenotype in a <kbd>1:1</kbd> ratio to represent the ANN topology. This mapping allows us to use advanced NEAT algorithm features such as an innovation number, which allows us to track when a particular mutation was introduced during the evolution. Each gene in the genome has a specific value of the innovation number, allowing fast and accurate crossover of parent genomes to produce offspring. While this feature introduces immense benefits and also reduces the computational costs needed to match the parent genomes during the recombination, the direct encoding used to encode the ANN topology of the phenotype has a significant drawback as it limits the size of the encoded ANN. The bigger the encoded ANN, the bigger the genome that is evaluated during the evolution, and this involves tremendous computational costs.</p>
<p class="p1">There are many tasks, primarily related to pattern recognition in images or other high-dimensional data sources, that require employing ANNs that have advanced topologies with many layers and nodes within them. Such topological configurations cannot be effectively processed by the classic NEAT algorithm due to the inefficiencies of direct encoding discussed previously.</p>
<p class="p1">The new method of encoding the phenotype ANN was proposed to address this drawback while still having all the benefits provided by the NEAT algorithm. We'll discuss it in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">CPPN encoding</h1>
                
            
            
                
<p class="p1">The proposed encoding scheme employs a method of representing the connectivity patterns within the phenotype ANN by querying another specialized neural network about the weights of the connections between the nodes. This specialized neural network is called a <strong>CPPN</strong>. Its main task is to represent the connectivity patterns of the phenotype ANN as a function of its geometry. The resulting connectivity pattern is represented as a four-dimensional hypercube. Each point of the hypercube encodes the connection between two related nodes within the phenotype ANN and is described by four numbers: the coordinates of the source node and the coordinates of the target node. The connective CPPN takes as input each point of the hypercube and calculates the weights of the connections between every node in the phenotype ANN. Also, a connection between two nodes is not expressed if the magnitude of the connection weight returned by the CPPN is less than a minimal threshold (<img class="fm-editor-equation" src="img/ade9b185-b4ae-44a0-b628-082618ce0fea.png" style="width:2.83em;height:1.08em;"/>). Thus, we can define the connective CPPN as a four-dimensional function returning the connection weight, as given by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/5dd4ef9f-20b1-401a-95e5-35807388b252.png" style="width:13.50em;height:1.42em;"/></p>
<p class="p1">The source node of the phenotype ANN is at <img class="fm-editor-equation" src="img/c248e489-d2cf-4726-93e6-1ba5a39b6518.png" style="width:3.42em;height:1.25em;"/>, and the target node is at <img class="fm-editor-equation" src="img/830c9acc-1001-4637-a7d8-e01d615adf9a.png" style="width:3.42em;height:1.33em;"/>.</p>
<p class="p1">Another essential feature of CPPNs is that unlike conventional ANNs, which employ only one type of activation function for each node (usually from the sigmoid family of functions), CPPNs can use multiple geometric functions as node activators. Due to this, CPPNs can express a rich set of geometric motifs in the produced connectivity patterns:</p>
<ul class="ul1">
<li class="li1">Symmetry (Gaussian function)</li>
<li class="li1">Imperfect symmetry (Gaussian combined with an asymmetric coordinate frame)</li>
<li class="li1">Repetition (sine function)</li>
<li class="li1">Repetition with variations (sine combined with a coordinate frame that doesn't repeat)</li>
</ul>
<p class="p1">Considering the features of the CPPN that we've discussed, we can assume that the connectivity pattern produced by it can represent any network topology for the phenotype ANN. Also, the connectivity pattern can be used to encode large-scale topologies by discovering the regularities in the training data and reusing the same set of genes within the CPPN to encode repetitions in the phenotype ANN.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hypercube-based NeuroEvolution of Augmenting Topologies</h1>
                
            
            
                
<p class="p1">The methodology described in the previous section was invented by Kenneth O. Stanley and was called <strong>Hypercube-based</strong> <strong>NeuroEvolution of Augmenting Topologies</strong> (<strong>HyperNEAT</strong>). As its name suggests, it is an extension of the NEAT algorithm that we have already used in this book. The main difference between these two methods is that the HyperNEAT method uses an indirect encoding scheme based on the CPPN. During the evolution, the HyperNEAT method employs a NEAT algorithm to evolve a population of genomes that encode a topology of the connective CPPN. After that, each created CPPN can be used to establish the connectivity patterns within a specific phenotype ANN. Finally, the phenotype ANN can be evaluated against the problem space.</p>
<p class="p1">So far, we have discussed how connectivity patterns can be evolved using NEAT with a CPPN and can be applied to the nodes of the phenotype ANN. However, we have not mentioned how the geometric layout of the nodes is determined in the first place. The responsibility of defining the nodes and their positions (layout) is assigned to the human architect. The architect analyzes the problem space and utilizes the most appropriate layout.</p>
<p class="p1">By convention, the initial layout of the nodes of the phenotype ANN has a name: substrate. There are several types of substrate configuration (layout), and they have proven their efficiency for particular tasks:</p>
<ul class="ul1">
<li class="li1"><strong>Two-dimensional grid</strong>: A regular grid of network nodes in a two-dimensional Cartesian space centered at (0,0).</li>
<li class="li1"><strong>Three-dimensional grid</strong>: A regular grid of network nodes in a three-dimensional Cartesian space centered at (0,0,0).</li>
<li class="li1"><strong>State-space sandwich</strong>: Two two-dimensional planar grids with corresponding source and target nodes in which one layer can only send connections in the direction of the other one.</li>
<li class="li1"><strong>Circular</strong>: A regular radial structure suited to define regularities in radial geometry based on polar coordinates.</li>
</ul>
<p class="p1">By arranging the ANN nodes in an appropriate layout on the substrate, it is possible to exploit regularities in the geometry of the problem space. That significantly increases the efficiency of the encoding by using the connective CPPN to draw connectivity patterns between the substrate nodes. Let's now look at the basics of a visual discrimination experiment.</p>
<p>For more details about the HyperNEAT method, please refer to <a href="f59c6396-55e5-4495-95c0-7af9a42c2f20.xhtml" target="_blank">Chapter 1</a>, <em>Overview of Neuroevolution Methods.</em></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual discrimination experiment basics</h1>
                
            
            
                
<p class="p1">As we have already mentioned, the main advantage of the indirect encoding employed by the HyperNEAT algorithm is the ability to encode the topology of the large-scale ANN. In this section, we will describe an experiment that can be used to test the capacity of the HyperNEAT method to train a large-scale ANN. Visual pattern recognition tasks typically require large ANNs as detectors due to the high dimensionality of the input data (the image height multiplied by the image width). In this chapter, we consider a variation of this family of computer science problems called visual discrimination tasks.</p>
<p class="p1">The task of visual discrimination is to distinguish a large object from a small object in a two-dimensional visual space, regardless of their positions in the visual space and their positions relative to each other. The visual discrimination task is performed by a specialized discriminator ANN, which is built on a substrate configured as a state-space sandwich with two sheets:</p>
<ul>
<li>The visual field is a two-dimensional array of sensors that can be in two states: on or off (black and white).</li>
<li>The target field is a two-dimensional array of outputs with activation values in the <kbd>[0,1]</kbd> range.</li>
</ul>
<p class="p1">The scheme of the visual discrimination task is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-784 image-border" src="img/c50c3901-45c5-40e5-a73d-774997ad8083.png" style="width:32.33em;height:15.00em;"/></p>
<p>The visual discrimination task</p>
<p class="p1">You can see in the diagram that the objects to be detected are represented as two squares separated by an empty space. The larger object is precisely three times bigger than the other one. The algorithm we are trying to build needs to detect the center of the larger object. The detection is based on measuring the activation values of the ANN nodes in the target field. The position of the node with the highest activation value marks the center of the detected object. Our goal is to discover the right connectivity patterns between visual and target fields that align the output node with the highest activation and the center of the big object in the visual field. Also, the discovered connectivity pattern should be invariant to the relative positions of both objects.</p>
<p class="p1">The algorithm for the task of visual discrimination needs to evaluate a large number of inputs - the values representing the cells in the visual field. Also, the successful algorithm needs to discover the strategy that can process inputs from multiple cells simultaneously. Such a strategy should be based on the general principle that allows the detection of the relative sizes of the objects in the visual field. The visual field in our experiment is represented as a two-dimensional grid. Thus, the general geometric principle to be discovered is the concept of locality.</p>
<p class="p1">We can exploit the locality principle in the substrate configuration of the discriminator ANN that we have chosen by discovering a particular pattern in the scheme of the links that connect the nodes of the visual and the target fields. In this connection scheme, separate nodes of the visual field are connected to the multiple adjacent output nodes around a specific location in the target field. As a result, the more activations the output node collects, the more signals are supplied into it through connections with individual input nodes.</p>
<p class="p1">To effectively exploit the locality principle mentioned previously, the representation of connections should take into account the geometry of the discriminator ANN substrate and the fact that the correct connectivity pattern repeats across it. The best candidate for such a representation is a CPPN, which can discover the local connectivity pattern once and repeat it across the substrate grid at any resolution.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Objective function definition</h1>
                
            
            
                
<p class="p1">The main task of the visual discriminator is to correctly determine the position of a larger object regardless of the relative positions of both objects. Thus, we can define the objective function to guide the neuroevolution process. The objective function should be based on the Euclidean distance between the exact position of the larger object in the visual field and its predicted position in the target field.</p>
<p class="p1">The loss function can be directly represented as the Euclidean distance between the actual and predicted positions as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/699dc1b0-b237-4196-8156-039e0ab72b66.png" style="width:11.33em;height:4.25em;"/></p>
<p class="p1"><img class="fm-editor-equation" src="img/510315d6-bb2f-4e07-bad5-0f1feeb1da7d.png" style="width:0.75em;height:1.00em;"/> is a loss function, <img class="fm-editor-equation" src="img/91dfcca9-9a9a-4677-b363-98a6577866fc.png" style="width:1.08em;height:1.00em;"/> is the ground truth coordinates of the big object, and <img class="fm-editor-equation" src="img/f77190a1-381b-4b78-843f-2a578f739538.png" style="width:1.00em;height:1.00em;"/> is predicted by the discriminator ANN coordinates.</p>
<p class="p1">With the loss function as defined previously, we can write the objective function as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/0ecb5686-c207-44ca-bce5-3c9103ebe605.png" style="width:9.58em;height:3.00em;"/></p>
<p class="p1"><img class="fm-editor-equation" src="img/a5fa600f-b85b-497a-a646-2f398d51b634.png" style="width:2.33em;height:1.00em;"/> is the maximal possible distance between the two points within the target field space. The objective function formula guarantees that the calculated fitness score (<img class="fm-editor-equation" src="img/96b5cab2-09d7-45ab-8cad-e9443b8e07c6.png" style="width:1.33em;height:1.00em;"/> ) always falls within the <kbd>[0,1] </kbd>range. Now that we know the basics of the visual discrimination experiment, let's start with setting it up.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual discrimination experiment setup</h1>
                
            
            
                
<p class="p1">In our experiment, during the training of the discriminator ANN, we use the resolution of the visual and target fields fixed at 11 x 11. Thus, the connective CPPN must learn the correct connectivity pattern between the 121 inputs of the visual field and the 121 outputs of the target fields, which results in a total of 14,641 potential connection weights.</p>
<p class="p1">The following diagram shows the scheme of the substrate for the discriminator ANN:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-785 image-border" src="img/d14d3979-7aaf-4756-803f-23d01cde0226.png" style="width:10.92em;height:12.58em;"/></p>
<p>The state-space sandwich substrate of the discriminator ANN</p>
<p class="p1">The discriminator ANN shown in the diagram has two layers with nodes forming one two-dimensional planar grid per layer. The connective CPPN draws the connectivity patterns by connecting nodes from one layer to another.</p>
<p class="p1">At each generation of the evolution, each individual in the population (the genome encoding the CPPN) is evaluated for its ability to create connectivity patterns of the discriminator ANN. The discriminator ANN is then tested to see whether it can find the center of the large object within the visual field. There are a total of 75 evaluation trials for a particular ANN, in which two objects are placed at different locations in each trial. At each trial, we put a small object in one of the 25 positions uniformly distributed in the visual field. The center of a large object is five steps from the small object to the right, down, or diagonally. If a large object doesn't fit into the visual field completely, then it wraps around to the other side. Thus, considering the logic of the placement of objects relative to each other and the grid, we should be able to evaluate all possible configurations in 75 trials.</p>
<p class="p1">Our experiment setup has two major parts, which we will discuss in the following sections.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual discriminator test environment</h1>
                
            
            
                
<p class="p1">First we need to define the test environment and provide access to the dataset, which contains all the possible visual field configurations as described in the previous section. The dataset used in this experiment is created during the test environment initialization. We will discuss dataset creation later in this section.</p>
<p class="p1">The test environment has two major components:</p>
<ul class="ol1">
<li class="li1">The data structure to maintain the visual field definition </li>
<li class="li1">The test environment manager, which stores the dataset and provides a means to evaluate discriminator ANNs against it</li>
</ul>
<p class="p1">Next, we provide a detailed description of these components.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual field definition</h1>
                
            
            
                
<p class="p1">We store the configuration of the visual field for each of the 75 trials discussed previously in the <kbd>VisualField</kbd> Python class. It has the following constructor:</p>
<pre>    def __init__(self, big_pos, small_pos, field_size):<br/>        self.big_pos = big_pos<br/>        self.small_pos = small_pos<br/>        self.field_size = field_size<br/>        self.data = np.zeros((field_size, field_size))<br/><br/>        # store small object position<br/>        self._set_point(small_pos[0], small_pos[1])<br/><br/>        # store big object points<br/>        offsets = [-1, 0, 1]<br/>        for xo in offsets:<br/>            for yo in offsets:<br/>                self._set_point(big_pos[0] + xo, big_pos[1] + yo)</pre>
<p class="p1">The constructor of <kbd>VisualField</kbd> accepts as parameters the tuple with coordinates (<em>x</em>, <em>y</em>) of the large and small object, as well as the size of the visual field. We consider the square visual field, so the size of the visual field along each axis is equal. The visual field is internally represented as a two-dimensional binary array where ones represent positions occupied by objects, and zeros are empty spaces. It is stored in the <kbd>self.data</kbd> field, which is a NumPy array with the shape (2, 2).</p>
<p class="p1">The small object has a size 1 x 1, and the large object is three times bigger. The following snippet from the constructor's source code creates the representation of the big object in the data array:</p>
<pre>        offsets = [-1, 0, 1]<br/>        for xo in offsets:<br/>            for yo in offsets:<br/>                self._set_point(big_pos[0] + xo, big_pos[1] + yo)</pre>
<p class="p1">The constructor of the <kbd>VisualField</kbd> class receives the coordinates of the center of the big object as a tuple, (<kbd>x</kbd>, <kbd>y</kbd>). The preceding code draws the big object starting from the top-left corner (<kbd>x-1</kbd>, <kbd>y-1</kbd>) and ending at the bottom-right corner (<kbd>x+1</kbd>, <kbd>y+1</kbd>).</p>
<p class="p1">The <kbd>_set_point(self, x, y)</kbd> function referred to in the preceding code sets the <kbd>1.0</kbd> value at the specific position in the <kbd>self.data</kbd> field:</p>
<pre>    def _set_point(self, x, y):<br/>        px, py = x, y<br/>        if px &lt; 0:<br/>            px = self.field_size + px<br/>        elif px &gt;= self.field_size:<br/>            px = px - self.field_size<br/><br/>        if py &lt; 0:<br/>            py = self.field_size + py<br/>        elif py &gt;= self.field_size:<br/>            py = py - self.field_size<br/><br/>        self.data[py, px] = 1 # in Numpy index is: [row, col]</pre>
<p class="p1">The <kbd>_set_point(self, x, y)</kbd> function performs coordinate wrapping when the coordinate value exceeds the allowed number of dimensions per axis. For example, for the <em>x</em> axis, the source code for the coordinate value wrapping is as follows:</p>
<pre>        if px &lt; 0:<br/>            px = self.field_size + px<br/>        elif px &gt;= self.field_size:<br/>            px = px - self.field_size</pre>
<p class="p1">The source code for coordinate wrapping along the <em>y</em> axis is similar.</p>
<p class="p1">After wrapping the coordinates specified as parameters of the function (if needed), we set the corresponding positions in the <kbd>self.data</kbd> field to have a value of <kbd>1.0</kbd>.</p>
<p>NumPy indexes as <kbd>[row, column]</kbd>. Thus, we need to use <em>y</em> in the first position and <em>x</em> in the second position of the index.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual discriminator environment</h1>
                
            
            
                
<p class="p1">The visual discriminator environment holds the generated dataset with the visual field definitions. Also, it provides methods to create the dataset and to evaluate the discriminator ANN against the dataset. The <kbd>VDEnvironment</kbd> Python class holds the definitions of all mentioned methods, as well as related data structures. Next, we'll look at all the significant parts of the <kbd>VDEnvironment</kbd> class definition:</p>
<ul>
<li>The class constructor is defined as follows:</li>
</ul>
<pre style="padding-left: 60px">    def __init__(self, small_object_positions, big_object_offset, <br/>                 field_size):<br/>        self.s_object_pos = small_object_positions<br/>        self.data_set = []<br/>        self.b_object_offset = big_object_offset<br/>        self.field_size = field_size<br/><br/>        self.max_dist = self._distance((0, 0), <br/>                             (field_size - 1, field_size - 1))<br/><br/>        # create test data set<br/>        self._create_data_set()</pre>
<p class="p1" style="padding-left: 60px">The first parameter of the <kbd>VDEnvironment</kbd> constructor is an array with the definitions of all the possible small object positions defined as a sequence of coordinate values for each axis. The second parameter defines the offset of the coordinates of the center of the big object from the small object coordinates. We use <kbd>5</kbd> as the value of this parameter in our experiment. Finally, the third parameter is the visual field size along with both dimensions.</p>
<p class="p1" style="padding-left: 60px">After all the received parameters are saved into object fields, we calculate the maximum possible distance between two points in the visual field as follows:</p>
<pre style="padding-left: 60px">self.max_dist = self._distance((0, 0), <br/>                     (field_size - 1, field_size - 1))</pre>
<p class="p1" style="padding-left: 60px">The Euclidean distance between the top-left and the bottom-right corner of the visual field is then stored in the <kbd>self.max_dist</kbd> field. This value will be used later to normalize the distances between points in the visual field by keeping them in the <kbd>[0, 1]</kbd> range.</p>
<ul>
<li>The <kbd>_create_data_set()</kbd> function creates all possible datasets given the specified environment parameters. The source code of this function is as follows:</li>
</ul>
<pre style="padding-left: 60px">    def _create_data_set(self):<br/>        for x in self.s_object_pos:<br/>            for y in self.s_object_pos:<br/>                # diagonal<br/>                vf = self._create_visual_field(x, y, <br/>                                  x_off=self.b_object_offset, <br/>                                  y_off=self.b_object_offset)<br/>                self.data_set.append(vf)<br/>                # right<br/>                vf = self._create_visual_field(x, y, <br/>                                  x_off=self.b_object_offset,<br/>                                  y_off=0)<br/>                self.data_set.append(vf)<br/>                # down<br/>                vf = self._create_visual_field(x, y, <br/>                                  x_off=0, <br/>                                  y_off=self.b_object_offset)<br/>                self.data_set.append(vf)</pre>
<p class="p1" style="padding-left: 60px">The function iterates over the small object positions along two axes and tries to create the big object at coordinates that are to the right, below, or on a diagonal from the small object coordinates.</p>
<ul>
<li class="p1">The <kbd>_create_visual_field</kbd> function creates the appropriate visual field configuration using the coordinates of the small object (<kbd>sx</kbd>, <kbd>sy</kbd>) and an offset of the big object's center (<kbd>x_off</kbd>, <kbd>y_off</kbd>). The following source code shows how this is implemented:</li>
</ul>
<pre style="padding-left: 60px">    def _create_visual_field(self, sx, sy, x_off, y_off):<br/>        bx = (sx + x_off) % self.field_size # wrap by X coordinate<br/>        by = (sy + y_off) % self.field_size # wrap by Y coordinate<br/><br/>        # create visual field<br/>        return VisualField(big_pos=(bx, by), small_pos=(sx, sy), <br/>                           field_size=self.field_size)</pre>
<p class="p1" style="padding-left: 60px">If the coordinates of the big object calculated by the preceding function are outside the visual field space, we apply the wrapping as follows:</p>
<pre style="padding-left: 60px">        if bx &gt;= self.field_size:<br/>            bx = bx - self.field_size # wrap</pre>
<p class="p1" style="padding-left: 60px">The preceding snippet shows the wrapping along the <em>x</em> axis. The wrapping along the <em>y</em> axis is similar. Finally, the <kbd>VisualField</kbd> object is created and returned to be appended to the dataset.</p>
<ul>
<li class="p1">However, the most exciting part of the <kbd>VDEnvironment</kbd> definition is related to the evaluation of the discriminator ANN, which is defined in the <kbd>evaluate_net(self, net)</kbd> function as follows:</li>
</ul>
<pre style="padding-left: 60px">    def evaluate_net(self, net):<br/>        avg_dist = 0<br/><br/>        # evaluate predicted positions<br/>        for ds in self.data_set:<br/>            # evaluate and get outputs<br/>            outputs, x, y = self.evaluate_net_vf(net, ds)<br/><br/>            # find the distance to the big object<br/>            dist = self._distance((x, y), ds.big_pos)<br/>            avg_dist = avg_dist + dist<br/><br/>        avg_dist /= float(len(self.data_set))<br/>        <br/>        # normalized position error<br/>        error = avg_dist / self.max_dist<br/>        # fitness<br/>        fitness = 1.0 - error<br/><br/>        return fitness, avg_dist</pre>
<p class="p1" style="padding-left: 60px">The preceding function receives the discriminator ANN as a parameter, and returns the evaluated fitness score and the mean distance between the detected coordinates of the big object and the ground truth values calculated for all evaluated visual fields. The average distance is calculated as follows:</p>
<pre class="p1" style="padding-left: 60px">        for ds in self.data_set:<br/>            # evaluate and get outputs<br/>            _, x, y = self.evaluate_net_vf(net, ds)<br/><br/>            # find the distance to the big object<br/>            dist = self._distance((x, y), ds.big_pos)<br/>            avg_dist = avg_dist + dist<br/><br/>        avg_dist /= float(len(self.data_set))</pre>
<p class="p1" style="padding-left: 60px">The preceding source code iterates over all <kbd>VisualField</kbd> objects in the dataset, and uses the discriminator ANN to determine the coordinates of the big object. After that, we calculate the distance (detection error) between the ground truth and the predicted position of the big object. Finally, we find the mean of the detection errors and normalize it as follows:</p>
<pre style="padding-left: 60px">        # normalized detection error<br/>        error = avg_dist / self.max_dist</pre>
<p class="p1" style="padding-left: 60px">The maximum possible error value is <kbd>1.0</kbd>, according to the preceding code. The value of the fitness score is a complement to the <kbd>1.0</kbd> of the error value since it increases as the error decreases:</p>
<pre style="padding-left: 60px">        # fitness<br/>        fitness = 1.0 - error</pre>
<p class="p1" style="padding-left: 60px">The <kbd>evaluate_net</kbd> function returns the calculated fitness score along with the unnormalized detection error.</p>
<ul>
<li class="p1">The <kbd>evaluate_net_vf(self, net, vf)</kbd> function provides a means to evaluate the discriminator ANN against a specific <kbd>VisualField</kbd> object. It is defined as follows:</li>
</ul>
<pre style="padding-left: 60px">   def evaluate_net_vf(self, net, vf):<br/>        depth = 1 # we just have 2 layers<br/><br/>        net.Flush()<br/>        # prepare input<br/>        inputs = vf.get_data()<br/>        net.Input(inputs)<br/>        # activate<br/>        [net.Activate() for _ in range(depth)]<br/><br/>        # get outputs<br/>        outputs = net.Output()<br/>        # find coordinates of big object<br/>        x, y = self._big_object_coordinates(outputs)<br/><br/>        return outputs, x, y</pre>
<p class="p1" style="padding-left: 60px">The preceding function receives the discriminator ANN as the first parameter and the <kbd>VisualField</kbd> object as the second parameter. After that, it obtains the flattened input array from the <kbd>VisualField</kbd> object and uses it as input to the discriminator ANN: </p>
<pre style="padding-left: 60px">        inputs = vf.get_data()<br/>        net.Input(inputs)</pre>
<p class="p1" style="padding-left: 60px">After we set the inputs of the discriminator ANN, it must be activated to propagate input values through all network nodes. Our discriminator ANN has only two layers, as determined by the space-sandwich substrate configuration. Thus we need to activate it twice—once per each layer. After propagation of the activation signal through both layers of the discriminator ANN, we can determine the position of the big object in the target field as an index of the maximal value in the output array. Using the <kbd>_big_object_coordinates(self, outputs)</kbd> function, we can extract the Cartesian coordinates (<em>x</em>, <em>y</em>) of the big object within the target field.</p>
<p class="p1" style="padding-left: 60px">Finally, the <kbd>evaluate_net_vf</kbd> function returns the raw output array along with the extracted Cartesian coordinates (<em>x</em>, <em>y</em>) of the big object in the target field space.</p>
<ul>
<li class="p1">The <kbd>_big_object_coordinates(self, outputs)</kbd> function extracts the Cartesian coordinates of the big object within the target field space from the raw outputs obtained from the discriminator ANN. The function's source code is as follows:</li>
</ul>
<pre style="padding-left: 60px">    def _big_object_coordinates(self, outputs):<br/>        max_activation = -100.0<br/>        max_index = -1<br/>        for i, out in enumerate(outputs):<br/>            if out &gt; max_activation:<br/>                max_activation = out<br/>                max_index = i<br/><br/>        # estimate the maximal activation's coordinates<br/>        x = max_index % self.field_size<br/>        y = int(max_index / self.field_size)<br/><br/>        return (x, y)</pre>
<p class="p1" style="padding-left: 60px">At first, the function enumerates through the output array and finds the index of the maximal value:</p>
<pre style="padding-left: 60px">        max_activation = -100.0<br/>        max_index = -1<br/>        for i, out in enumerate(outputs):<br/>            if out &gt; max_activation:<br/>                max_activation = out<br/>                max_index = I</pre>
<p class="p1" style="padding-left: 60px">After that, it uses the index it finds to estimate the Cartesian coordinates, taking into account the size of the target field:</p>
<pre style="padding-left: 60px">        x = max_index % self.field_size<br/>        y = int(max_index / self.field_size)</pre>
<p class="p1">Finally, the function returns the tuple (<em>x</em>, <em>y</em>) with the Cartesian coordinates of the big object within the target field.</p>
<p>For complete implementation details, please look at <kbd>vd_environment.py</kbd> at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_environment.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_environment.py</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Experiment runner</h1>
                
            
            
                
<p class="p3">As we described earlier, the solution for the visual discrimination task can be found using the HyperNEAT method. Thus, we need to use a library that provides an implementation of the HyperNEAT algorithm. The MultiNEAT Python library is the right candidate for this experiment. As such, we are implementing our experiment using this library.</p>
<p class="p3">Next, we discuss the most critical components of the experiment runner implementation.</p>
<p>For complete implementation details, please refer to <kbd>vd_experiment_multineat.py</kbd> at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_experiment_multineat.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/vd_experiment_multineat.py</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The experiment runner function</h1>
                
            
            
                
<p class="p2">The <kbd>run_experiment</kbd> function allows us to run the experiment using the provided hyperparameters and the initialized visual discriminator test environment. The function implementation has the following parts.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Initializing the first CPPN genome population</h1>
                
            
            
                
<p>In the following code, at first, we initialize the random number generator seed with the current system time. After that, we create the appropriate substrate configuration for the discriminator ANN that is able to operate over the dimensionality of the experiment's visual field. Next, we create the CPPN genome using the created substrate configuration:</p>
<pre>    # random seed<br/>    seed = int(time.time())<br/>    # Create substrate<br/>    substrate = create_substrate(num_dimensions)<br/>    # Create CPPN genome and population<br/>    g = NEAT.Genome(0,<br/>                    substrate.GetMinCPPNInputs(),<br/>                    0,<br/>                    substrate.GetMinCPPNOutputs(),<br/>                    False,<br/>                    NEAT.ActivationFunction.UNSIGNED_SIGMOID,<br/>                    NEAT.ActivationFunction.UNSIGNED_SIGMOID,<br/>                    0,<br/>                    params, 0)<br/>    pop = NEAT.Population(g, params, True, 1.0, seed)<br/>    pop.RNG.Seed(seed)</pre>
<p class="p1">The CPPN genome created in the preceding code has the appropriate number of input and output nodes provided by the substrate. Initially, it uses the unsigned sigmoid as the node activation function. Later, during the evolution, the activation function type at each node of the CPPN will be changed, following the HyperNEAT algorithm routines. Finally, the initial population is created using the initialized CPPN genome and the HyperNEAT hyperparameters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the neuroevolution over the specified number of generations</h1>
                
            
            
                
<p class="p1">At the beginning of this part, we create the intermediate variables to hold the execution results and create the statistics collector (<kbd>Statistics</kbd>). After that, we execute the evolution loop for the number of generations specified in the <kbd>n_generations</kbd> parameter:</p>
<pre>    start_time = time.time()<br/>    best_genome_ser = None<br/>    best_ever_goal_fitness = 0<br/>    best_id = -1<br/>    solution_found = False<br/><br/>    stats = Statistics()<br/>    for generation in range(n_generations):</pre>
<p class="p1">Within the evolution loop, we obtain the list of genomes belonging to the population at the current generation and evaluate all genomes from the list against the test environment as follows:</p>
<pre>        genomes = NEAT.GetGenomeList(pop)<br/>        # evaluate genomes<br/>        genome, fitness, distances = eval_genomes(genomes, <br/>                                      vd_environment=vd_environment, <br/>                                      substrate=substrate, <br/>                                      generation=generation)<br/>        stats.post_evaluate(max_fitness=fitness, distances=distances)<br/>        solution_found = fitness &gt;= FITNESS_THRESHOLD</pre>
<p class="p1">We save the values returned by the <kbd>eval_genomes(genomes, substrate, vd_environment, generation)</kbd> function for the current generation into the statistics collector. Also, we use the fitness score returned by <kbd>eval_genomes</kbd> to estimate whether a successful solution has been found or not. If the fitness score exceeds the <kbd>FITNESS_THRESHOLD</kbd> value, we consider that a successful solution has been found.</p>
<p class="p1">If a successful solution was found or the current fitness score is the maximum fitness score ever achieved, we save the CPPN genome and the current fitness score:</p>
<pre>        if solution_found or best_ever_goal_fitness &lt; fitness:<br/>            best_genome_ser = pickle.dumps(genome)<br/>            best_ever_goal_fitness = fitness<br/>            best_id = genome.GetID()</pre>
<p class="p1">Also, if a successful solution is found, we break the evolution loop and move to the reporting steps, which we will discuss later:</p>
<pre>        if solution_found:<br/>            print('Solution found at generation: %d, best fitness: %f, species count: %d' % (generation, fitness, len(pop.Species)))<br/>            break</pre>
<p class="p1">If a successful solution was not found, we print the statistics for the current generation and advance to the next generation with the following code:</p>
<pre>        # advance to the next generation<br/>        pop.Epoch()<br/>        # print statistics<br/>        gen_elapsed_time = time.time() - gen_time<br/>        print("Best fitness: %f, genome ID: %d" % (fitness, best_id))<br/>        print("Species count: %d" % len(pop.Species))<br/>        print("Generation elapsed time: %.3f sec" % (gen_elapsed_time))<br/>        print("Best fitness ever: %f, genome ID: %d" <br/>               % (best_ever_goal_fitness, best_id))</pre>
<p class="p1"> After the main evolution loop, the results of the experiment are reported, which uses the statistics collected in the loop.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Saving the results of the experiment</h1>
                
            
            
                
<p>The experiment results reported and saved in textual and graphical representations (SVG files). We start by printing general performance statistics as follows:</p>
<pre>    print("\nBest ever fitness: %f, genome ID: %d" <br/>          % (best_ever_goal_fitness, best_id))<br/>    print("\nTrial elapsed time: %.3f sec" % (elapsed_time))<br/>    print("Random seed:", seed)</pre>
<p class="p1">The first three lines of the preceding code print the best ever fitness score obtained among all the generations of evolution to the console. After that, we print the experiment's elapsed time and the random seed value used.</p>
<p class="p1">If we requested to save or show visualizations, the corresponding functions are invoked:</p>
<pre>    # Visualize the experiment results<br/>    show_results = not silent<br/>    if save_results or show_results:<br/>        net = NEAT.NeuralNetwork()<br/>        best_genome.BuildPhenotype(net)<br/>        visualize.draw_net(net, view=show_results, node_names=None, <br/>                           directory=trial_out_dir, fmt='svg')</pre>
<p>The preceding code draws the network graph of the CPPN and prints the statistics of the graph.</p>
<p class="mce-root">Next, we move to the visualization of the output of the discriminator ANN:</p>
<pre><br/>        # Visualize activations from the best genome<br/>        net = NEAT.NeuralNetwork()<br/>        best_genome.BuildHyperNEATPhenotype(net, substrate)<br/>        # select random visual field<br/>        index = random.randint(0, len(vd_environment.data_set) - 1)<br/>        vf = vd_environment.data_set[index]<br/>        # draw activations<br/>        outputs, x, y = vd_environment.evaluate_net_vf(net, vf)<br/>        visualize.draw_activations(outputs, found_object=(x, y), vf=vf,<br/>                  dimns=num_dimensions, view=show_results, <br/>                  filename=os.path.join(trial_out_dir, <br/>                                        "best_activations.svg"))</pre>
<p class="p1">In the preceding code, we create the discriminator ANN using the best CPPN genome found during the evolution. After that, we draw the activation outputs obtained by running the evaluation of the discriminator ANN against the test environment. We use the visual field that is randomly selected from the dataset of the experiment.</p>
<p class="p1">Finally, we render the general statistics collected during the experiment:</p>
<pre>        # Visualize statistics<br/>        visualize.plot_stats(stats, ylog=False, view=show_results, <br/>                  filename=os.path.join(trial_out_dir, 'avg_fitness.svg'))</pre>
<p class="p1">The statistics plot includes the best fitness scores and the average error distances drawn over the generations of evolution.</p>
<p>For implementation details of the visualization functions mentioned in this section, please refer to <kbd>visualize.py</kbd> at <a href="https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/visualize.py">https://github.com/PacktPublishing/Hands-on-Neuroevolution-with-Python/blob/master/Chapter7/visualize.py</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The substrate builder function</h1>
                
            
            
                
<p class="p2">The HyperNEAT method is built around the notion of the substrate that defines the structure of the discriminator ANN. Therefore, it is crucial to create an appropriate substrate configuration to be used during the experiment execution. The substrate creation routines are defined in the following two functions:</p>
<ul>
<li>The substrate builder function <kbd>create_substrate</kbd> creates the substrate object as follows:</li>
</ul>
<pre style="padding-left: 60px">def create_substrate(dim):<br/>    # Building sheet configurations of inputs and outputs<br/>    inputs = create_sheet_space(-1, 1, dim, -1)<br/>    outputs = create_sheet_space(-1, 1, dim, 0)<br/>    substrate = NEAT.Substrate( inputs, [], # hidden outputs)<br/>    substrate.m_allow_input_output_links = True<br/>    ...<br/>    substrate.m_hidden_nodes_activation = \<br/>                  NEAT.ActivationFunction.SIGNED_SIGMOID<br/>    substrate.m_output_nodes_activation = \<br/>                  NEAT.ActivationFunction.UNSIGNED_SIGMOID<br/>    substrate.m_with_distance = True<br/>    substrate.m_max_weight_and_bias = 3.0<br/>    return substrate</pre>
<p class="p1" style="padding-left: 60px">The preceding function first creates the two grid-based Cartesian sheets that represent inputs (the visual field) and outputs (the target field) of the substrate configuration. Remember that for this experiment we selected a state-space sandwich substrate configuration. After that, the substrate instance was initialized using the created field configurations:</p>
<pre style="padding-left: 60px">    inputs = create_sheet_space(-1, 1, dim, -1)<br/>    outputs = create_sheet_space(-1, 1, dim, 0)<br/>    substrate = NEAT.Substrate( inputs, [], # hidden outputs)</pre>
<p>Please note that the substrate doesn't use any hidden nodes; we provide an empty list instead of hidden nodes.</p>
<p class="p1" style="padding-left: 60px">Next, we configure the substrate to only allow connections from input to output nodes and to use a signed sigmoid activation function at the output nodes. Finally, we set the maximum values for the bias and the connection weights.</p>
<ul>
<li class="p1">The <kbd>create_sheet_space</kbd> function invoked by the substrate builder function is defined as follows:</li>
</ul>
<pre style="padding-left: 60px">def create_sheet_space(start, stop, dim, z):<br/>    lin_sp = np.linspace(start, stop, num=dim)<br/>    space = []<br/>    for x in range(dim):<br/>        for y in range(dim):<br/>            space.append((lin_sp[x], lin_sp[y], z))<br/><br/>    return space</pre>
<p class="p1">The <kbd>create_sheet_space</kbd> function receives the start and end coordinates of the grid within one dimension along with the number of grid dimensions. Also, the <em>z</em> coordinate of the sheet is provided. Using the specified parameters, the preceding code creates the uniform linear space with coordinates starting in the <kbd>[start, stop]</kbd> range with a step of <kbd>dim</kbd>:</p>
<pre style="padding-left: 60px">    lin_sp = np.linspace(start, stop, num=dim)</pre>
<p class="p1">After that, we use this linear space to populate the two-dimensional array with the coordinates of the grid nodes as follows:</p>
<pre style="padding-left: 60px">    space = []<br/>    for x in range(dim):<br/>        for y in range(dim):<br/>            space.append((lin_sp[x], lin_sp[y], z))</pre>
<p class="p1">The <kbd>create_sheet_space</kbd> function returns the grid configuration in the form of a two-dimensional array.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Fitness evaluation</h1>
                
            
            
                
<p class="p1">The genome's fitness evaluation is a significant part of any neuroevolution algorithm, including the HyperNEAT method. As you've seen, the main experiment loop invokes the <kbd>eval_genomes</kbd> function to evaluate the fitness of all genomes within a population for each generation. Here, we consider the implementation details of the fitness evaluation routines, which consists of two main functions:</p>
<ul>
<li>The <kbd>eval_genomes</kbd> function evaluates all genomes in the population:</li>
</ul>
<pre style="padding-left: 60px">def eval_genomes(genomes, substrate, vd_environment, generation):<br/>    best_genome = None<br/>    max_fitness = 0<br/>    distances = []<br/>    for genome in genomes:<br/>        fitness, dist = eval_individual(genome, substrate, <br/>                                        vd_environment)<br/>        genome.SetFitness(fitness)<br/>        distances.append(dist)<br/><br/>        if fitness &gt; max_fitness:<br/>            max_fitness = fitness<br/>            best_genome = genome<br/>    <br/>    return best_genome, max_fitness, distances</pre>
<p class="p1" style="padding-left: 60px">The <kbd>eval_genomes</kbd> function takes a list of genomes, the discriminator ANN substrate configuration, the initialized test environment, and the ID of the current generation as parameters. The first lines of the function create intermediate variables, which are used to store the evaluation results:</p>
<pre style="padding-left: 60px">    best_genome = None<br/>    max_fitness = 0<br/>    distances = []</pre>
<p class="p1" style="padding-left: 60px">After that, we iterate over all the genomes in the population and collect appropriate statistics:</p>
<pre style="padding-left: 60px">    for genome in genomes:<br/>        fitness, dist = eval_individual(genome, substrate, <br/>                                        vd_environment)<br/>        genome.SetFitness(fitness)<br/>        distances.append(dist)<br/><br/>        if fitness &gt; max_fitness:<br/>            max_fitness = fitness<br/>            best_genome = genome</pre>
<p class="p1" style="padding-left: 60px">Finally, the <kbd>eval_genomes</kbd> function returns the collected statistics as a tuple, <kbd>(best_genome, max_fitness, distances)</kbd>.</p>
<ul>
<li class="p1">The <kbd>eval_individual</kbd> function allows us to evaluate the fitness of the individual genome as follows:</li>
</ul>
<pre style="padding-left: 60px">def eval_individual(genome, substrate, vd_environment):<br/>    # Create ANN from provided CPPN genome and substrate<br/>    net = NEAT.NeuralNetwork()<br/>    genome.BuildHyperNEATPhenotype(net, substrate)<br/><br/>    fitness, dist = vd_environment.evaluate_net(net)<br/>    return fitness, dist</pre>
<p class="p1">In the beginning, the preceding source code creates the discriminator ANN phenotype using the CPPN genome provided as a parameter. After that, the discriminator ANN phenotype evaluated against the test environment.</p>
<p class="p1">The <kbd>eval_individual</kbd> function returns the fitness score and error distance obtained from the test environment during the phenotype evaluation. Now that we have completed the setup, let us start with the visual discrimination experiment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Visual discrimination experiment</h1>
                
            
            
                
<p class="p1">Having done all of the necessary setup steps, we are ready to start the experiment.</p>
<p class="p1">In the visual discrimination experiment, we use the following configuration of the visual field:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 50.515%">
<p class="p1 CDPAlignCenter CDPAlign"><strong>Parameter</strong></p>
</td>
<td style="width: 47.485%">
<p class="p1 CDPAlignCenter CDPAlign"><strong>Value</strong></p>
</td>
</tr>
<tr>
<td style="width: 50.515%">
<p class="p1">Size of the visual field</p>
</td>
<td style="width: 47.485%">
<p class="p1 CDPAlignCenter CDPAlign">11 x 11</p>
</td>
</tr>
<tr>
<td style="width: 50.515%">
<p class="p1">Positions of the small objects in the visual field along each axis</p>
</td>
<td style="width: 47.485%">
<p class="p1 CDPAlignCenter CDPAlign">[1, 3, 5, 7, 9]</p>
</td>
</tr>
<tr>
<td style="width: 50.515%">
<p class="p1">Size of the small object</p>
</td>
<td style="width: 47.485%">
<p class="p1 CDPAlignCenter CDPAlign">1 x 1</p>
</td>
</tr>
<tr>
<td style="width: 50.515%">
<p class="p1">Size of the big object</p>
</td>
<td style="width: 47.485%">
<p class="p1 CDPAlignCenter CDPAlign">3 x 3</p>
</td>
</tr>
<tr>
<td style="width: 50.515%">
<p class="p1">Offset of the center of the big object from the small object</p>
</td>
<td style="width: 47.485%">
<p class="p1 CDPAlignCenter CDPAlign">5</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="p1">Next, we need to select the appropriate values of the HyperNEAT hyperparameters, allowing us to find a successful solution to the visual discrimination problem.</p>
<p>Note that the hyperparameter that we describe next determines how to evolve the connective CPPN using the neuroevolution process. The discriminator ANN is created by applying the connective CPPN to the substrate.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hyperparameter selection</h1>
                
            
            
                
<p class="p3">The MultiNEAT library uses the <kbd>Parameters</kbd> Python class to hold all the required hyperparameters. To set the appropriate values of the hyperparameters, we define the <kbd>create_hyperparameters</kbd> function in the experiment runner Python script. Here, we describe the essential hyperparameters that have a significant impact on the HyperNEAT algorithm performance in this experiment:</p>
<ol>
<li>The <kbd>create_hyperparameters</kbd> function begins by creating a <kbd>Parameters</kbd> object to hold the HyperNEAT parameters:</li>
</ol>
<pre style="padding-left: 60px">    params = NEAT.Parameters()</pre>
<ol start="2">
<li>We decided to start with a medium-sized population of genomes to keep the computations fast. At the same time, we want to maintain a sufficient number of organisms in the population for evolutionary diversity. The population size is defined as follows:</li>
</ol>
<pre style="padding-left: 60px">    params.PopulationSize = 150</pre>
<ol start="3">
<li>We are interested in producing compact CPPN genomes that have as few nodes as possible to increase the effectiveness of indirect encoding. Thus, we set a tiny probability of adding a new node during evolution, and also keep the probability of creating a new connection quite low:</li>
</ol>
<pre style="padding-left: 60px">    params.MutateAddLinkProb = 0.1<br/>    params.MutateAddNeuronProb = 0.03</pre>
<ol start="4">
<li>The HyperNEAT method produces CPPN genomes with different types of activation functions in the hidden and output nodes. Thus, we define the probability of mutation that changes the type of the node activation. Also, in this experiment, we are interested in using only four types of activation function: signed Gaussian, signed sigmoid, signed sine, and linear. We set the likelihood of choosing any activation type among the four we just mentioned to <kbd>1.0</kbd>, which effectively makes the probability of choosing each type equal. We define this in the hyperparameters as follows:</li>
</ol>
<pre style="padding-left: 60px">    params.MutateNeuronActivationTypeProb = 0.3<br/>    params.ActivationFunction_SignedGauss_Prob = 1.0<br/>    params.ActivationFunction_SignedSigmoid_Prob = 1.0<br/>    params.ActivationFunction_SignedSine_Prob = 1.0<br/>    params.ActivationFunction_Linear_Prob = 1.0</pre>
<ol start="5">
<li>Finally, we define the number of species within the population to be kept in the <kbd>[5,10]</kbd> range and set the value of the species stagnation parameter to <kbd>100</kbd> generations. This configuration maintains moderate species diversity, but keeps species for long enough to allow them to evolve and produce useful CPPN genome configurations:</li>
</ol>
<pre style="padding-left: 60px">    params.SpeciesMaxStagnation = 100<br/>    params.MinSpecies = 5<br/>    params.MaxSpecies = 10</pre>
<p class="p1">The selection of hyperparameters presented here demonstrates the high efficiency of producing successful CPPN genomes during the evolution.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Working environment setup</h1>
                
            
            
                
<p class="p1">In this experiment, we use the MultiNEAT Python library, which provides the implementation of the HyperNEAT algorithm. Thus, we need to create an appropriate Python environment, which includes the MultiNEAT Python library and all the necessary dependencies. This can be done using Anaconda by executing the following commands in the command line:</p>
<pre><strong>$ conda create --name vd_multineat python=3.5</strong><br/><strong>$ conda activate vd_multineat</strong><br/><strong>$ conda install -c conda-forge multineat </strong><br/><strong>$ conda install matplotlib</strong><br/><strong>$ conda install -c anaconda seaborn</strong><br/><strong>$ conda install graphviz</strong><br/><strong>$ conda install python-graphviz</strong></pre>
<p class="p1">These commands create and activate a <kbd>vd_multineat</kbd> virtual environment with Python 3.5. After that, they install the latest version of the MultiNEAT Python library, along with the dependencies that are used by our code for the result visualization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Running the visual discrimination experiment</h1>
                
            
            
                
<p class="p1">To start the experiment, you need to enter the local directory that contains the <kbd>vd_experiment_multineat.py</kbd> script, and execute the following command:</p>
<pre><strong>$ python vd_experiment_multineat.py</strong></pre>
<p>Do not forget to activate the appropriate virtual environment with the following command:<br/>
<br/> <strong><kbd>$ conda activate vd_multineat</kbd></strong></p>
<p class="p1">After a particular number of generations, the successful solution will be found, and you will see lines similar to the following in the console output:</p>
<pre><strong>****** Generation: 16 ******</strong><br/><br/><strong>Best fitness: 0.995286, genome ID: 2410</strong><br/><strong>Species count: 11</strong><br/><strong>Generation elapsed time: 3.328 sec</strong><br/><strong>Best fitness ever: 0.995286, genome ID: 2410</strong><br/><br/><strong>****** Generation: 17 ******</strong><br/><br/><strong>Solution found at generation: 17, best fitness: 1.000000, species count: 11</strong><br/><br/><strong>Best ever fitness: 1.000000, genome ID: 2565</strong><br/><br/><strong>Trial elapsed time: 57.753 sec</strong><br/><strong>Random seed: 1568629572</strong><br/><br/><strong>CPPN nodes: 10, connections: 16</strong><br/><br/><strong>Running test evaluation against random visual field: 41</strong><br/><strong>Substrate nodes: 242, connections: 14641</strong><br/><strong>found (5, 1)</strong><br/><strong>target (5, 1)</strong></pre>
<p class="p1">The console output says that the solution was found at generation <kbd>17</kbd>. The ID of the successful CPPN genome is <kbd>2565</kbd>, and this genome has 10 nodes and 16 connections among them. Also, you can see the results of the evaluation of the discriminator ANN produced by the best CPPN genome against the randomly selected visual field.</p>
<p class="p1">In this case, the detected Cartesian coordinates of the big object in the target field and the actual coordinates in the visual field are the same (5, 1), which means that the solution found is capable of visual discrimination with exact precision.</p>
<p class="p1">Next, it is interesting to take a look at the visualization of the activation outputs of the discriminator ANN obtained during test evaluation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-786 image-border" src="img/c11a26be-bf38-494e-84a5-16ccf7802e91.png" style="width:44.67em;height:22.75em;"/></p>
<p>The target field activations of the discriminator ANN</p>
<p class="p3">The right part of the preceding plot renders the activation values of the target field (the output layer) of the discriminator ANN, which we obtained during evaluation against a random visual field. Also, in the left part of the plot, you can see the actual visual field configuration. As you can see, the maximum target field activation value (the darkest cell) is precisely at the same position as the center of the big object in the visual field, having the coordinates (<kbd>5</kbd>, <kbd>1</kbd>).</p>
<p>As you can see from the preceding graph, the scale of ANN activation values is extremely low: the minimum activation is <kbd>~1e-13</kbd>, and the maximum is only <kbd>~9e-13</kbd>. A human-designed ANN would probably be normalized so that the output is on <kbd>[0,1]</kbd>, having a minimum close to zero and a maximum near one. However, we only require the activation to have a maximum in the right place, and the network is free to choose an output activation scheme that most folks would view as unusual.</p>
<p class="p1">Another plot allows you to study how the evolution process performed over the generations of evolution and how good the produced connective CPPNs were in creating successful discriminator ANNs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-787 image-border" src="img/81b24e28-f535-46a1-ac00-c0303ae401a0.png" style="width:36.75em;height:27.25em;"/></p>
<p>The best fitness scores and average error distances of discriminator ANNs</p>
<p class="p3">The preceding plot renders the change in the fitness scores (the ascending line) and the average error distances (the descending line) for each generation of the evolution. You can see that the fitness scores almost reached the maximum value in the third generation of the evolution and needed seven more generations to elaborate over the CPPN genome configurations to finally find the winner. Also, you can see that the average error distance between the detected and the ground truth position of the big object gradually decreases during the evolution process.</p>
<p class="p1">However, the most exciting part of this experiment is shown in the following diagram of the CPPN phenotype graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-788 image-border" src="img/e49c120b-23b9-443f-80ea-a456227ddb41.png" style="width:30.92em;height:10.58em;"/></p>
<p>The CPPN phenotype graph of the best genome</p>
<p class="p3">The plot demonstrates the network topology of the CPPN phenotype that was used to draw connections over the discriminator ANN producing the successful visual discriminator. In the CPPN phenotype plot, input nodes are marked with squares, output nodes are filled circles, and the bias node is a diamond.</p>
<p class="p1">The two output nodes of the CPPN have the following meaning: </p>
<ul class="ul1">
<li class="li1">The first node (8) provides the weight of the connection.</li>
<li class="li1">The second node (9) determines whether the connection is expressed or not. </li>
</ul>
<p class="p1">The CPPN input nodes are defined as follows:</p>
<ul class="ul1">
<li class="li1">The first two nodes (0 and 1) set the point coordinates (<em>x</em>, <em>y</em>) in the input layer of the substrate.</li>
<li class="li1">The next two nodes (2 and 3) set the point coordinates (<em>x</em>, <em>y</em>) in the hidden layer of the substrate (not used in our experiment).</li>
<li class="li1">The next two nodes (4 and 5) set the point coordinates (<em>x</em>, y) in the output layer of the substrate.</li>
<li class="li1">The last node (6) sets the Euclidean distance from the point in the input layer from the origin of the coordinates.</li>
</ul>
<p class="p1">Also, you can see that the CPPN phenotype doesn't include any hidden nodes. For the visual discrimination task, the neuroevolutionary process was able to find the appropriate activation function types for the output nodes of the CPPN. This finding allows the connective CPPN to draw the correct connectivity patterns within the substrate of the discriminator ANN.</p>
<p class="p1">By counting the number of nodes and connections between them as presented in the preceding plot, you can feel the power of the indirect encoding method introduced by the HyperNEAT algorithm. With only 16 connections between 10 nodes, the CPPN phenotype was able to expose the connectivity pattern of the substrate, which for the visual field at 11 x 11 resolution can have up to 14,641 connections between the nodes in the visual field and the target field. So, we achieved an information compression ratio of about 0.11%, which is pretty impressive.</p>
<p class="p1">Such a high compression rate is possible because of the discovery of the geometric regularities within the connectivity motifs of the substrate by the connective CPPN. Using the regularities of the discovered patterns, the CPPN can store only a few patterns (local connectivity motifs) for the whole connectivity space of the substrate. After that, the CPPN can apply these local patterns multiple times at different substrate positions to draw the full connectivity scheme between the substrate layers, in our case, to draw connections between the input layer (visual field) and the output layer (target field).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<ol class="ol1">
<li class="li3">Try to decrease the value of the <kbd>params.PopulationSize</kbd> hyperparameter and see what happens. How did this affect the algorithm's performance?</li>
<li class="li3">Try to set zero probabilities for the values of the following hyperparameters: <kbd>params.ActivationFunction_SignedGauss_Prob</kbd>, <kbd>params.ActivationFunction_SignedSigmoid_Prob</kbd>, and <kbd>params.ActivationFunction_SignedSine_Prob</kbd>. Was a successful solution found with these changes? How did this affect the configuration of the substrate connections?</li>
<li>Print out the winning genome, try to come up with a visualization, then see how your intuition from looking at the genome matches with the visualized CPPN.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p class="p3">In this chapter, we learned about the method of indirect encoding of the ANN topology using CPPNs. You learned about the HyperNEAT extension of the NEAT algorithm, which uses a connective CPPN to draw connectivity patterns within the substrate of the phenotype of the discriminator ANN. Also, we demonstrated how the indirect encoding scheme allows the HyperNEAT algorithm to work with large-scale ANN topologies, which is common in pattern recognition and visual discrimination tasks.</p>
<p class="p3">With the theoretical background we provided, you have had the chance to improve your coding skills by implementing the solution for a visual discrimination task using Python and the MultiNEAT library. Also, you learned about a new visualization method that renders the activation values of the nodes in the output layer of the discriminator ANN and how this visualization can be used to verify the solution.</p>
<p class="p3">In the next chapter, we will discuss how the HyperNEAT method can be further improved by introducing an automatic way to generate the appropriate substrate configuration. We will consider the <strong>Evolvable Substrate HyperNEAT</strong> (<strong>ES-HyperNEAT</strong>) extension of the NEAT algorithm and see how it can be applied to solve practical tasks that require the modular topologies of the solver ANN.</p>


            

            
        
    </body></html>