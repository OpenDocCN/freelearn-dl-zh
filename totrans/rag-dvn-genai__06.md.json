["```py\n#API Key\n#Store your key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\nfrom google.colab import drive\ndrive.mount('/content/drive') \n```", "```py\nimport os\nimport json\nwith open(os.path.expanduser(\"drive/MyDrive/files/kaggle.json\"), \"r\") as f:\n    kaggle_credentials = json.load(f)\nkaggle_username = kaggle_credentials[\"username\"]\nkaggle_key = kaggle_credentials[\"key\"]\nos.environ[\"KAGGLE_USERNAME\"] = kaggle_username\nos.environ[\"KAGGLE_KEY\"] = kaggle_key \n```", "```py\ntry:\n  import kaggle\nexcept:\n  !pip install kaggle\nimport kaggle\nkaggle.api.authenticate() \n```", "```py\n!kaggle datasets download -d radheshyamkollipara/bank-customer-churn \n```", "```py\nDataset URL: https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn\nLicense(s): other\nbank-customer-churn.zip: Skipping, found more recently modified local copy (use --force to force download) \n```", "```py\nimport zipfile\nwith zipfile.ZipFile('/content/bank-customer-churn.zip', 'r') as zip_ref:\n    zip_ref.extractall('/content/')\nprint(\"File Unzipped!\") \n```", "```py\nFile Unzipped! \n```", "```py\nimport pandas as pd\n# Load the CSV file\nfile_path = '/content/Customer-Churn-Records.csv'\ndata1 = pd.read_csv(file_path) \n```", "```py\n# Drop columns and update the DataFrame in place\ndata1.drop(columns=['RowNumber','Surname', 'Gender','Geography'], inplace=True)\ndata1 \n```", "```py\ndata1.to_csv('data1.csv', index=False)\n!cp /content/data1.csv /content/drive/MyDrive/files/rag_c6/data1.csv \n```", "```py\n#   Column              Non-Null Count  Dtype\n---  ------              --------------  -----\n 0   CustomerId          10000 non-null  int64\n 1   CreditScore         10000 non-null  int64\n **2   Age                 10000 non-null  int64**\n 3   Tenure              10000 non-null  int64\n 4   Balance             10000 non-null  float64\n 5   NumOfProducts       10000 non-null  int64\n 6   HasCrCard           10000 non-null  int64\n 7   IsActiveMember      10000 non-null  int64\n **8   EstimatedSalary     10000 non-null  float64**\n **9   Exited              10000 non-null  int64**\n **10  Complain            10000 non-null  int64**\n 11  Satisfaction Score  10000 non-null  int64\n 12  Card Type           10000 non-null  object\n 13  Point Earned        10000 non-null  int64 \n```", "```py\n# Calculate the percentage of complain over exited\nif sum_exited > 0:  # To avoid division by zero\n    percentage_complain_over_exited = (sum_complain/ sum_exited) * 100\nelse:\n    percentage_complain_over_exited = 0\n# Print results\nprint(f\"Sum of Exited = {sum_exited}\")\nprint(f\"Sum of Complain = {sum_complain}\")\nprint(f\"Percentage of complain over exited = {percentage_complain_over_exited:.2f}%\") \n```", "```py\nSum of Exited = 2038\nSum of Complain = 2044\nPercentage of complain over exited = 100.29% \n```", "```py\n    Sum of Age 50 and Over among Exited = 634\n    Sum of Exited = 2038\n    Percentage of Age 50 and Over among Exited = 31.11% \n    ```", "```py\n    Sum of Estimated Salary over 100000 among Exited = 1045\n    Sum of Exited = 2038\n    Percentage of Estimated Salary over 100000 among Exited = 51.28% \n    ```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Select only numerical columns for the correlation heatmap\nnumerical_columns = data1.select_dtypes(include=['float64', 'int64']).columns\n# Correlation heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(data1[numerical_columns].corr(), annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Correlation Heatmap')\nplt.show() \n```", "```py\n# Copying data1 to data2\ndata2 = data1.copy() \n```", "```py\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import silhouette_score , davies_bouldin_score\n# Assuming you have a dataframe named data1 loaded as described\n# Selecting relevant features\nfeatures = data2[['CreditScore', 'Age', 'EstimatedSalary', 'Exited', 'Complain', 'Point Earned']] \n```", "```py\n# Standardize the features\nscaler = StandardScaler()\nfeatures_scaled = scaler.fit_transform(features) \n```", "```py\n# Experiment with different numbers of clusters\nfor n_clusters in range(2, 5):  # Example range from 2 to 5\n    kmeans = KMeans(n_clusters=n_clusters, n_init=20, random_state=0)\n    cluster_labels = kmeans.fit_predict(features_scaled)\n    silhouette_avg = silhouette_score(features_scaled, cluster_labels)\n    db_index = davies_bouldin_score(features_scaled, cluster_labels)\n    print(f'For n_clusters={n_clusters}, the silhouette score is {silhouette_avg:.4f} and the Davies-Bouldin Index is {db_index:.4f}') \n```", "```py\nFor n_clusters=2, the silhouette score is 0.6129 and the Davies-Bouldin Index is 0.6144\nFor n_clusters=3, the silhouette score is 0.3391 and the Davies-Bouldin Index is 1.1511\nFor n_clusters=4, the silhouette score is 0.3243 and the Davies-Bouldin Index is 1.0802 \n```", "```py\n# Perform K-means clustering with a chosen number of clusters\nkmeans = KMeans(n_clusters=2, n_init=10, random_state=0)  # Explicitly setting n_init to 10\ndata2['class'] = kmeans.fit_predict(features_scaled)\n# Display the first few rows of the dataframe to verify the 'class' column\ndata2 \n```", "```py\n# 1\\. Sum where 'class' == 0\nsum_class_0 = (data2['class'] == 0).sum()\n# 2\\. Sum where 'class' == 0 and 'Complain' == 1\nsum_class_0_complain_1 = data2[(data2['class'] == 0) & (data2['Complain'] == 1)].shape[0]\n# 3\\. Sum where 'class' == 0 and 'Exited' == 1\nsum_class_0_exited_1 = data2[(data2['class'] == 0) & (data2['Exited'] == 1)].shape[0]\n# Print the results\nprint(f\"Sum of 'class' == 0: {sum_class_0}\")\nprint(f\"Sum of 'class' == 0 and 'Complain' == 1: {sum_class_0_complain_1}\")\nprint(f\"Sum of 'class' == 0 and 'Exited' == 1: {sum_class_0_exited_1}\") \n```", "```py\nSum of 'class' == 0: 2039\nSum of 'class' == 0 and 'Complain' == 1: 2036\nSum of 'class' == 0 and 'Exited' == 1: 2037 \n```", "```py\n# 2\\. Sum where 'class' == 1 and 'Complain' == 1\nsum_class_1_complain_1 = data2[(data2['class'] == 1) & (data2['Complain'] == 1)].shape[0] \n```", "```py\nSum of 'class' == 1: 7961\nSum of 'class' == 1 and 'Complain' == 1: 8\nSum of 'class' == 1 and 'Exited' == 1: 1 \n```", "```py\n#API Key\n#Store your key in a file and read it(you can type it directly in the notebook but it will be visible for somebody next to you)\nfrom google.colab import drive\ndrive.mount('/content/drive') \n```", "```py\n!pip install openai==1.40.3\n!pip install pinecone-client==5.0.1 \n```", "```py\nf = open(\"drive/MyDrive/files/pinecone.txt\", \"r\")\nPINECONE_API_KEY=f.readline()\nf.close()\nf = open(\"drive/MyDrive/files/api_key.txt\", \"r\")\nAPI_KEY=f.readline()\nf.close()\n#The OpenAI Key\nimport os\nimport openai\nos.environ['OPENAI_API_KEY'] =API_KEY\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\") \n```", "```py\n!cp /content/drive/MyDrive/files/rag_c6/data1.csv /content/data1.csv \n```", "```py\nimport pandas as pd\n# Load the CSV file\nfile_path = '/content/data1.csv'\ndata1 = pd.read_csv(file_path) \n```", "```py\n# Count the chunks\nnumber_of_lines = len(data1)\nprint(\"Number of lines: \",number_of_lines) \n```", "```py\nNumber of lines:  10000 \n```", "```py\nimport pandas as pd\n# Initialize an empty list to store the lines\noutput_lines = []\n# Iterate over each row in the DataFrame\nfor index, row in data1.iterrows():\n    # Create a list of \"column_name: value\" for each column in the row\n    row_data = [f\"{col}: {row[col]}\" for col in data1.columns]\n    # Join the list into a single string separated by spaces\n    line = ' '.join(row_data)\n    # Append the line to the output list\n    output_lines.append(line)\n# Display or further process `output_lines` as needed\nfor line in output_lines[:5]:  # Displaying first 5 lines for preview\n    print(line) \n```", "```py\nCustomerId: 15634602 CreditScore: 619 Age: 42 Tenure: 2 Balance: 0.0 NumOfProducts: 1 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 101348.88 Exited: 1 Complain: 1 Satisfaction Score: 2 Card Type: DIAMOND Point Earned: 464… \n```", "```py\nlines = output_lines.copy() \n```", "```py\n# Count the lines\nnumber_of_lines = len(lines)\nprint(\"Number of lines: \",number_of_lines) \n```", "```py\nNumber of lines:  10000 \n```", "```py\n# Initialize an empty list for the chunks\nchunks = []\n# Add each line as a separate chunk to the chunks list\nfor line in lines:\n    chunks.append(line)  # Each line becomes its own chunk\n# Now, each line is treated as a separate chunk\nprint(f\"Total number of chunks: {len(chunks)}\") \n```", "```py\nTotal number of chunks: 10000 \n```", "```py\n# Print the length and content of the first 10 chunks\nfor i in range(3):\n    print(len(chunks[i]))\n    print(chunks[i]) \n```", "```py\n224\nCustomerId: 15634602 CreditScore: 619 Age: 42 Tenure: 2 Balance: 0.0 NumOfProducts: 1 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 101348.88 Exited: 1 Complain: 1 Satisfaction Score: 2 Card Type: DIAMOND Point Earned: 464… \n```", "```py\nimport openai\nimport time\nembedding_model=\"text-embedding-3-small\"\n#embedding_model=\"text-embedding-3-large\"\n#embedding_model=\"text-embedding-ada-002\" \n```", "```py\n# Initialize the OpenAI client\nclient = openai.OpenAI()\ndef get_embedding(text, model=embedding_model):\n    text = text.replace(\"\\n\", \" \")\n    response = client.embeddings.create(input=[text], model=model)\n    embedding = response.data[0].embedding\n    return embedding \n```", "```py\nimport openai\nimport time\n# Initialize the OpenAI client\nclient = openai.OpenAI()\n# Initialize variables\nstart_time = time.time()  # Start timing before the request\nchunk_start = 0\nchunk_end = 1000\npause_time = 3\nembeddings = []\ncounter = 1 \n```", "```py\n    while chunk_end <= len(chunks):\n        # Select the current batch of chunks\n        chunks_to_embed = chunks[chunk_start:chunk_end]… \n    ```", "```py\n    for chunk in chunks_to_embed:\n          embedding = get_embedding(chunk, model=embedding_model)\n          current_embeddings.append(embedding)… \n    ```", "```py\n     # Update the chunk indices\n        chunk_start += 1000\n        chunk_end += 1000 \n    ```", "```py\n# Process the remaining chunks if any\nif chunk_end < len(chunks):\n    remaining_chunks = chunks[chunk_end:]\n    remaining_embeddings = [get_embedding(chunk, model=embedding_model) for chunk in remaining_chunks]\n    embeddings.extend(remaining_embeddings) \n```", "```py\nAll chunks processed.\nBatch 1 embedded.\n...\nBatch 10 embedded.\nResponse Time: 2689.46  seconds \n```", "```py\nprint(\"First embedding:\", embeddings[0]) \n```", "```py\nFirst embedding: [-0.024449337273836136, -0.00936567410826683,… \n```", "```py\n# Check the lengths of the chunks and embeddings\nnum_chunks = len(chunks)\nprint(f\"Number of chunks: {num_chunks}\")\nprint(f\"Number of embeddings: {len(embeddings)}\") \n```", "```py\nNumber of chunks: 10000\nNumber of embeddings: 10000 \n```", "```py\n# Define the duplication size\ndsize = 5  # You can set this to any value between 1 and n as per your experimentation requirements\ntotal=dsize * len(chunks)\nprint(\"Total size\", total) \n```", "```py\n# Initialize new lists for duplicated chunks and embeddings\nduplicated_chunks = []\nduplicated_embeddings = []\n# Loop through the original lists and duplicate each entry\nfor i in range(len(chunks)):\n    for _ in range(dsize):\n        duplicated_chunks.append(chunks[i])\n        duplicated_embeddings.append(embeddings[i]) \n```", "```py\n# Checking the lengths of the duplicated lists\nprint(f\"Number of duplicated chunks: {len(duplicated_chunks)}\")\nprint(f\"Number of duplicated embeddings: {len(duplicated_embeddings)}\") \n```", "```py\nTotal size 50000\nNumber of duplicated chunks: 50000\nNumber of duplicated embeddings: 50000 \n```", "```py\nimport os\nfrom pinecone import Pinecone, ServerlessSpec\n# initialize connection to pinecone (get API key at app.pinecone.io)\napi_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\npc = Pinecone(api_key=PINECONE_API_KEY) \n```", "```py\nfrom pinecone import ServerlessSpec\nindex_name = [YOUR INDEX NAME] #'bank-index-900'for example\ncloud = os.environ.get('PINECONE_CLOUD') or 'aws'\nregion = os.environ.get('PINECONE_REGION') or 'us-east-1'\nspec = ServerlessSpec(cloud=cloud, region=region) \n```", "```py\nimport time\nimport pinecone\n# check if index already exists (it shouldn't if this is first time)\nif index_name not in pc.list_indexes().names():\n    # if does not exist, create index\n    pc.create_index(\n        index_name,\n        dimension=1536,  #Dimension of the embedding model\n        metric='cosine',\n        spec=spec\n    )\n    # wait for index to be initialized\n    time.sleep(1)\n# connect to index\nindex = pc.Index(index_name)\n# view index stats\nindex.describe_index_stats() \n```", "```py\n{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {},\n 'total_vector_count': 0} \n```", "```py\n# upsert function\ndef upsert_to_pinecone(data, batch_size):\n    for i in range(0, len(data), batch_size):\n        batch = data[i:i+batch_size]\n        index.upsert(vectors=batch)\n        #time.sleep(1)  # Optional: add delay to avoid rate limits \n```", "```py\nimport pinecone\nimport time\nimport sys\nstart_time = time.time()  # Start timing before the request \n```", "```py\n# Function to calculate the size of a batch\ndef get_batch_size(data, limit=4000000):  # limit set slightly below 4MB to be safe\n    total_size = 0\n    batch_size = 0\n    for item in data:\n        item_size = sum([sys.getsizeof(v) for v in item.values()])\n        if total_size + item_size > limit:\n            break\n        total_size += item_size\n        batch_size += 1\n    return batch_size \n```", "```py\ndef batch_upsert(data):\n    total = len(data)\n    i = 0\n    while i < total:\n        batch_size = get_batch_size(data[i:])\n        batch = data[i:i + batch_size]\n        if batch:\n            upsert_to_pinecone(batch,batch_size)\n            i += batch_size\n            print(f\"Upserted {i}/{total} items...\")  # Display current progress\n        else:\n            break\n    print(\"Upsert complete.\") \n```", "```py\n# Generate IDs for each data item\nids = [str(i) for i in range(1, len(duplicated_chunks) + 1)] \n```", "```py\n# Prepare data for upsert\ndata_for_upsert = [\n    {\"id\": str(id), \"values\": emb, \"metadata\": {\"text\": chunk}}\n    for id, (chunk, emb) in zip(ids, zip(duplicated_chunks, duplicated_embeddings))\n] \n```", "```py\n# Upsert data in batches\nbatch_upsert(data_for_upsert) \n```", "```py\nresponse_time = time.time() - start_time  # Measure response time\nprint(f\"Upsertion response time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nUpserted 316/50000 items...\nUpserted 632/50000 items...\nUpserted 948/50000 items...\n…\nUpserted 49612/50000 items...\nUpserted 49928/50000 items...\nUpserted 50000/50000 items...\nUpsert complete.\nUpsertion response time: 560.66 seconds \n```", "```py\nprint(\"Index stats\")\nprint(index.describe_index_stats(include_metadata=True)) \n```", "```py\nIndex stats\n{'dimension': 1536,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 50000}},\n 'total_vector_count': 50000} \n```", "```py\n# Print the query results along with metadata\ndef display_results(query_results):\n  for match in query_results['matches']:\n    print(f\"ID: {match['id']}, Score: {match['score']}\")\n    if 'metadata' in match and 'text' in match['metadata']:\n        print(f\"Text: {match['metadata']['text']}\")\n    else:\n        print(\"No metadata available.\") \n```", "```py\nembedding_model = \"text-embedding-3-small\"\ndef get_embedding(text, model=embedding_model):\n    text = text.replace(\"\\n\", \" \")\n    response = client.embeddings.create(input=[text], model=model)\n    embedding = response.data[0].embedding\n    return embedding \n```", "```py\nimport openai\n# Initialize the OpenAI client\nclient = openai.OpenAI()\nprint(\"Querying vector store\")\nstart_time = time.time()  # Start timing before the request \n```", "```py\nquery_text = \"Customer Robertson CreditScore 632Age 21 Tenure 2Balance 0.0NumOfProducts 1HasCrCard 1IsActiveMember 1EstimatedSalary 99000 Exited 1Complain 1Satisfaction Score 2Card Type DIAMONDPoint Earned 399\" \n```", "```py\nquery_embedding = get_embedding(query_text,model=embedding_model) \n```", "```py\nquery_results = index.query(vector=query_embedding, top_k=1, include_metadata=True)  # Request metadata\n#print(\"raw query_results\",query_results)\nprint(\"processed query results\")\ndisplay_results(query_results) #display results\nresponse_time = time.time() - start_time              # Measure response time\nprint(f\"Querying response time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nQuerying vector store\nQuerying vector store\nprocessed query results\nID: 46366, Score: 0.823366046\nText: CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance: 0.0 NumOfProducts: 2 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 54706.75 Exited: 0 Complain: 0 Satisfaction Score: 3 Card Type: DIAMOND Point Earned: 852\nQuerying response time: 0.74 seconds \n```", "```py\nimport openai\nimport time\nembedding_model= \"text-embedding-3-small\"\n# Initialize the OpenAI client\nclient = openai.OpenAI()\ndef get_embedding(text, model=embedding_model):\n    text = text.replace(\"\\n\", \" \")\n    response = client.embeddings.create(input=[text], model=model)\n    embedding = response.data[0].embedding\n    return embedding \n```", "```py\nimport time\nstart_time = time.time()  # Start timing before the request\n# Target vector\n \"\n# Target vector\nquery_text = \"Customer Henderson CreditScore 599 Age 37Tenure 2Balance 0.0NumOfProducts 1HasCrCard 1IsActiveMember 1EstimatedSalary 107000.88Exited 1Complain 1Satisfaction Score 2Card Type DIAMONDPoint Earned 501\"\nquery_embedding = get_embedding(text,model=embedding_model) \n```", "```py\n# Perform the query using the embedding\nquery_results = index.query(\n    vector=query_embedding,\n    top_k=5,\n    include_metadata=True,\n) \n```", "```py\n# Print the query results along with metadata\nprint(\"Query Results:\")\nfor match in query_results['matches']:\n    print(f\"ID: {match['id']}, Score: {match['score']}\")\n    if 'metadata' in match and 'text' in match['metadata']:\n        print(f\"Text: {match['metadata']['text']}\")\n    else:\n        print(\"No metadata available.\")\nresponse_time = time.time() - start_time              # Measure response time\nprint(f\"Querying response time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nQuery Results:\nID: 46366, Score: 0.854999781\nText: CustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance: 0.0 NumOfProducts: 2 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 54706.75 Exited: 0 Complain: 0 Satisfaction Score: 3 Card Type: DIAMOND Point Earned: 852\nQuerying response time: 0.63 seconds \n```", "```py\nrelevant_texts = [match['metadata']['text'] for match in query_results['matches'] if 'metadata' in match and 'text' in match['metadata']]\n# Join all items in the list into a single string separated by a specific delimiter (e.g., a newline or space)\ncombined_text = '\\n'.join(relevant_texts)  # Using newline as a separator for readability\nprint(combined_text) \n```", "```py\nCustomerId: 15740160 CreditScore: 616 Age: 31 Tenure: 1 Balance: 0.0 NumOfProducts: 2 HasCrCard: 1 IsActiveMember: 1 EstimatedSalary: 54706.75 Exited: 0 Complain: 0 Satisfaction Score: 3 Card Type: DIAMOND Point Earned: 852 \n```", "```py\n# Combine texts into a single string, separated by new lines\ncombined_context = \"\\n\".join(relevant_texts)\n#prompt\nquery_prompt=\"I have this customer bank record with interesting information on age, credit score and more and similar customers. What could I suggest to keep them in my bank in an email with an url to get new advantages based on the fields for each Customer ID:\"\nitext=query_prompt+ query_text+combined_context\n# Augmented input\nprint(\"Prompt for the Generative AI model:\", itext) \n```", "```py\nPrompt for GPT-4: I have this customer bank record with interesting information on age, credit score and more and similar customers. What could I suggest to keep them in my bank in an email with an url to get new advantages based on the fields for each Customer ID:… \n```", "```py\nfrom openai import OpenAI\nclient = OpenAI()\ngpt_model = \"gpt-4o \n```", "```py\nimport time\nstart_time = time.time()  # Start timing before the request \n```", "```py\nresponse = client.chat.completions.create(\n  model=gpt_model,\n  messages=[ \n```", "```py\n {\n      \"role\": \"system\",\n      \"content\": \"You are the community manager can write engaging email based on the text you have. Do not use a surname but simply Dear Valued Customer instead.\"\n    }, \n```", "```py\n {\n      \"role\": \"user\",\n      \"content\": itext\n    }\n  ], \n```", "```py\n temperature=0,\n  max_tokens=300,\n  top_p=1,\n  frequency_penalty=0,\n  presence_penalty=0\n) \n```", "```py\nprint(response.choices[0].message.content) \n```", "```py\nSubject: Exclusive Benefits Await You at Our Bank!\nDear Valued Customer,\nWe hope this email finds you well. At our bank, we are constantly striving to enhance your banking experience and provide you with the best possible services. We have noticed that you are a valued customer with a DIAMOND card, and we would like to offer you some exclusive benefits tailored just for you!\nBased on your profile, we have identified several opportunities that could enhance your banking experience:\n1\\. **Personalized Financial Advice**: Our financial advisors are available to help you make the most of your finances. Whether it's planning for the future or managing your current assets, we are here to assist you.\n2\\. **Exclusive Rewards and Offers**: As a DIAMOND cardholder, you are eligible for special rewards and offers. Earn more points and enjoy exclusive discounts on various products and services.\n3\\. **Enhanced Credit Options**: With your current credit score, you may qualify for better credit options. We can help you explore these opportunities to improve your financial standing.\n4\\. **Complimentary Financial Health Check**: We understand the importance of financial well-being. Schedule a complimentary financial health check to ensure you are on the right track.\n5\\. **Loyalty Programs**: Participate in our loyalty programs and earn more points for every transaction. Redeem these points for exciting rewards and benefits.\nTo explore these new advantages and more, please visit the following link: [Exclusive Benefits](https://www.yourbank \n```", "```py\nresponse_time = time.time() - start_time              # Measure response time\nprint(f\"Querying response time: {response_time:.2f} seconds\")  # Print response time \n```", "```py\nQuerying response time: 2.83 seconds \n```"]