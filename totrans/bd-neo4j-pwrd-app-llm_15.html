<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer105">&#13;
    <h1 class="chapterNumber"><a id="_idTextAnchor099"/>12</h1>&#13;
    <h1 id="_idParaDest-178" class="chapterTitle">Deploying Your Application on the Google Cloud</h1>&#13;
    <p class="normal">You have come a long way in designing and developing your GenAI application. Now, it is time to take that next crucial step—deployment. While a true production-grade deployment involves various complexities such as CI/CD pipelines, scalability considerations, observability, cost optimization, and security hardening, this chapter is designed to give you a foundational, hands-on introduction to cloud deployment using Google Cloud Run. <strong class="keyWord">Cloud Run</strong> by Google Cloud provides a powerful yet developer-friendly way to deploy containerized applications without managing infrastructure, making it ideal for rapid prototyping and small-scale production use cases.</p>&#13;
    <p class="normal">The deployment steps and services may vary slightly across other cloud platforms, such as AWS or Microsoft Azure, but we will focus on Google Cloud for brevity. However, once you are comfortable with the core concepts here, you are encouraged to experiment with similar workflows on other providers to broaden your cloud deployment expertise.</p>&#13;
    <p class="normal">We will walk through the process of deploying the Haystack chatbot you built in <a href="Chapter_03.xhtml#_idTextAnchor021"><em class="italic">Chapter 5</em></a> as a serverless application on Google Cloud. The process of deploying the intelligent recommendation system using Spring AI is mentioned later in the chapter with the links to the resources to follow the steps. By the end of this chapter, you will have a working chatbot live on the cloud and the confidence to build on this foundation for more advanced deployments.</p>&#13;
    <p class="normal">In this chapter, we are going to cover the following main topics:</p>&#13;
    <ul>&#13;
      <li class="bulletList">Preparing your search chatbot using Haystack for deployment</li>&#13;
      <li class="bulletList">Containerizing the application with Docker</li>&#13;
      <li class="bulletList">Setting up a Google Cloud project and services</li>&#13;
      <li class="bulletList">Deploying to Google Cloud Run</li>&#13;
      <li class="bulletList">Testing and verifying the deployment</li>&#13;
    </ul>&#13;
    <h1 id="_idParaDest-179" class="heading-1">Technical requirements</h1>&#13;
    <p class="normal">To deploy your Haystack chatbot using Google Cloud Run, you need the following:</p>&#13;
    <ul>&#13;
      <li class="bulletList">An active Google Cloud account with billing enabled. If you are new to Google Cloud, you can start by creating an account <a id="_idTextAnchor100"/>at <a href="https://console.cloud.google.com/"><span class="url"> https://console.cloud.google.com/</span></a> and take advantage of the free tier and credits offered for <a id="_idTextAnchor101"/>new users.</li>&#13;
      <li class="bulletList">Access to a Neo4j database:<ul>&#13;
          <li class="bulletList">If you are using a local Neo4j instance, you must expose it publicly using <code class="inlineCode">ngrok</code> or a similar tool, so that the deployed chatbot can connect to it. Here is an example to expose Neo4j’s <code class="inlineCode">bolt</code> port:&#13;
            <pre class="programlisting con"><code class="hljs-con">ngrok tcp 7687&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="bulletList">Update <a id="_idTextAnchor102"/>your <code class="inlineCode">.env</code> with the <code class="inlineCode">ngrok</code> public URL:&#13;
            <pre class="programlisting con"><code class="hljs-con">NEO4J_URI=bolt://0.tcp.ngrok.io:XXXXX&#13;
</code></pre>&#13;
          </li>&#13;
        </ul>&#13;
      </li>&#13;
      <li class="bulletList">If you are using AuraDB Free, the preceding step can be ignored.</li>&#13;
    </ul>&#13;
    <h1 id="_idParaDest-180" class="heading-1">Preparing your Haystack chatbot for deployment</h1>&#13;
    <p class="normal">We will be deploying <a id="_idIndexMarker575"/>our application on Google Cloud. The <a id="_idIndexMarker576"/>approach we will be talking about is similar from a deployment perspective on all the popular cloud environments. We will be looking at building a <code class="inlineCode">docke<a id="_idTextAnchor103"/>r compose</code> and running it in the cloud. Google Cloud was chosen because it is convenient rather than having any technical advantage as such. Once we deploy and run the application on Google Cloud, the official documentation links for other clouds to deploy the same <code class="inlineCode">docker compose</code> will be provided. Just repeating those steps in the book will not make much of a difference. </p>&#13;
    <p class="normal">Before jumping into containerization and deployment, it is important to ensure that your Haystack chatbot code is organized in a way that is compatible with serverless deployment. In this section, you will structure your code base appropriately and prepare the essential configuration files needed for deploying to Google Cloud Run.</p>&#13;
    <p class="normal">We will be reusing the working search chatbot from <a href="Chapter_03.xhtml#_idTextAnchor021"><em class="italic">Chapter 5</em></a> by creating a copy of the main script (<code class="inlineCode">search_chatbot.py</code>), renaming it <code class="inlineCode">app.py</code> (as this is the default entry point many cloud services expect when serving a Python web application), and placing it in a simplified folder ready for containerization. Since the chatbot logic is already functional, we will skip local testing and move directly to packaging and deploying it to the cloud.</p>&#13;
    <div class="note">&#13;
      <p class="normal"> <strong class="keyWord">Note</strong></p>&#13;
      <p class="normal">These steps to help you containerize and deploy your Haystack chatbot to Google Cloud Run are also presented in the <code class="inlineCode">README.md</code> file in the book’s GitHub repo at <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch12"><span class="url">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs/tree/main/ch12</span></a>.</p>&#13;
    </div>&#13;
    <p class="normal">Next, prepare a <code class="inlineCode">requirements.txt</code> file that lists all the necessary Python dependencies your chatbot needs to run. This file allows the container to install the required packages during the build process. The content of this file will look something like this:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">haystack-ai==2.5.0&#13;
openai==1.67.0&#13;
gradio==4.44.1&#13;
python-dotenv&gt;=1.0.0&#13;
neo4j==5.25.0&#13;
neo4j-haystack==2.0.3&#13;
</code></pre>&#13;
    <p class="normal">To manage sensitive credentials and environment-specific configurations securely, it is recommended to use a <code class="inlineCode">.env</code> file. In the <code class="inlineCode">ch12</code> directory of the GitHub repo, you will find a file named <code class="inlineCode">example.env</code> that serves as a template. This file includes placeholders for key variables such as your OpenAI API key and Neo4j database credentials. To use it, simply create a copy of this file, rename it <code class="inlineCode">.env</code>, and <a id="_idIndexMarker577"/>populate it with your actual values. The application leverages the <code class="inlineCode">python-dotenv</code> library to load these variables at runtime, keeping<a id="_idIndexMarker578"/> secrets out of your code base while still making them accessible to your application:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">OPENAI_API_KEY=&lt;insert-your-openai-api-key&gt;&#13;
NEO4J_URI=&lt;insert-your-neo4j-uri&gt;&#13;
NEO4J_USERNAME=neo4j&#13;
NEO4J_PASSWORD=&lt;insert-your-neo4j-password&gt;&#13;
</code></pre>&#13;
    <p class="normal">At this point, you have set up the core components needed for deployment—your application script, dependencies, and environment variables. To ensure everything is organized correctly for containerization and deployment, your project directory should now follow this structure:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">haystack-cloud-app/&#13;
├── app.py                  # Renamed chatbot server file (originally search_chatbot.py)&#13;
├── requirements.txt        # Python dependencies&#13;
├── Dockerfile              # Will be created in the next ste<a id="_idTextAnchor104"/>p&#13;
├── .env                    # For storing configuration variables&#13;
├── example.env             # Template file for environment variables&#13;
</code></pre>&#13;
    <p class="normal">The <code class="inlineCode">example.env</code> file <a id="_idIndexMarker579"/>acts as a reference for users to create their own .<code class="inlineCode">env</code> file with valid credentials and configuration values. With all the core components now <a id="_idIndexMarker580"/>in place—including your application script, dependencies, and environment setup—you are ready to containerize your application for deployment.</p>&#13;
    <p class="normal">Let us move on to the next step, which is to containerize your application with Docker.</p>&#13;
    <h1 id="_idParaDest-181" class="heading-1">Containerizing the application with Docker</h1>&#13;
    <p class="normal">Before deploying your Haystack chatbot to Google Cloud Run, the application must be packaged into a Docker container. <strong class="keyWord">Containerization</strong> allows <a id="_idIndexMarker581"/>you to bundle your code, dependencies, and environment into a single, portable unit that runs consistently across different systems—including the cloud.</p>&#13;
    <p class="normal">In this section, you <a id="_idIndexMarker582"/>will create a Dockerfile, which defines the<a id="_idIndexMarker583"/> steps required to build a Docker image of your chatbot. This image will then be deployed to Cloud Run as a serverless web service.</p>&#13;
    <p class="normal">Here is the Dockerfile used to containerize your Haystack chatbot:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">FROM python:3.11&#13;
EXPOSE 8080&#13;
WORKDIR /app&#13;
COPY . ./&#13;
RUN pip install -r requirements.txt&#13;
CMD ["python", "app.py"]&#13;
</code></pre>&#13;
    <p class="normal">Let us break down what each line does:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><code class="inlineCode">FROM python:3.11</code>: This sets the base image to Python 3.11, which includes everything needed to run Python applications.</li>&#13;
      <li class="bulletList"><code class="inlineCode">EXPOSE 8080</code>: Cloud Run expects the application to listen on port <code class="inlineCode">8080</code>. This line documents the port that the container will expose at runtime.</li>&#13;
      <li class="bulletList"><code class="inlineCode">WORKDIR /app</code>: This sets the working directory inside the container to <code class="inlineCode">/app</code>. All subsequent commands will run from this directory<a id="_idTextAnchor105"/>.</li>&#13;
      <li class="bulletList"><code class="inlineCode">COPY . ./</code>: This copies the entire contents of your local project directory into the container’s <code class="inlineCode">/app</code> directory.</li>&#13;
      <li class="bulletList"><code class="inlineCode">RUN pip install -r requirements.txt</code>: This installs all Python dependencies listed in your <code class="inlineCode">requirements.txt</code> file.</li>&#13;
      <li class="bulletList"><code class="inlineCode">CMD ["python", "app.py"]</code>: This specifies the command to run when the container starts—in this case, it runs your chatbot application using <code class="inlineCode">app.py</code>.</li>&#13;
    </ul>&#13;
    <p class="normal">Once your <a id="_idIndexMarker584"/>Dockerfile is in place, you now have a fully<a id="_idIndexMarker585"/> containerized version of your Haystack chatbot, ready to be deployed to the cloud. The next step is to configure your Google Cloud environment so that you can push your container and run it using Cloud Run.</p>&#13;
    <p class="normal">Let us move on to setting up your Google Cloud project and services.</p>&#13;
    <h1 id="_idParaDest-182" class="heading-1">Setting up a Google Cloud project and services</h1>&#13;
    <p class="normal">Google Cloud provides a robust, developer-friendly platform for deploying modern applications, including GenAI-powered solutions. With tools such as Cloud Run, Artifact Registry, and Cloud Build, Google Cloud enables you to go from code to scalable, serverless deployment with minimal operational overhead.</p>&#13;
    <p class="normal">Although your Haystack chatbot uses OpenAI for language processing, Google Cloud plays a critical role in hosting the application, managing container builds, and securely storing your Docker images. In this section, you will configure your Google Cloud project, enable only the necessary services (such as Cloud Run, Cloud Build, and Artifact Registry), and prepare your environment for deployment.</p>&#13;
    <p class="normal">By the end of this section, your project will be cloud-ready, with all the services and permissions in place to deploy your chatbot using Google Cloud’s serverless infrastructure.</p>&#13;
    <p class="normal">Let us get started by setting up your project and enabling the required APIs.</p>&#13;
    <h2 id="_idParaDest-183" class="heading-2">Creating a project</h2>&#13;
    <p class="normal">In the Google <a id="_idIndexMarker586"/>Cloud console (<a href="https://console.cloud.google.com/"><span class="url">https://console.cloud.google.com/</span></a>), on the project selector page, select or create a Google Cloud project (<a href="https://cloud.google.com/resource-manager/docs/creating-managing-projects"><span class="url">https://cloud.google.com/resource-manager/docs/creating-managing-projects</span></a>).</p>&#13;
    <p class="normal">Make sure that billing is enabled for your Cloud project. Learn how to check whether billing is enabled on a project at <a href="https://cloud.google.com/billing/docs/how-to/verify-billing-enabled"><span class="url">https://cloud.google.com/billing/docs/how-to/verify-billing-enabled</span></a>.</p>&#13;
    <h2 id="_idParaDest-184" class="heading-2">Launching Google Cloud Shell</h2>&#13;
    <p class="normal">To simplify the setup and <a id="_idIndexMarker587"/>avoid installing any tools locally, we will use Google Cloud Shell, which comes pre-installed with Docker, the <code class="inlineCode">gcloud</code> CLI, and Git. Here is how to get started:</p>&#13;
    <ol>&#13;
      <li class="numberedList" value="1">Go to the Google Cloud console (<a href="https://console.cloud.google.com/"><span class="url">https://console.cloud.google.com/</span></a>).</li>&#13;
      <li class="numberedList">Click the Cloud Shell icon in the top-right corner of the navigation bar (the terminal icon).</li>&#13;
    </ol>&#13;
    <p class="normal">A terminal window will open at the bottom of your screen. This is a fully functional shell with access to your Google Cloud project and services.</p>&#13;
    <div class="note">&#13;
      <p class="normal"><strong class="keyWord">Note</strong></p>&#13;
      <p class="normal">Cloud Shell provisions a temporary VM with 5 GB of persistent storage—more than enough for this walkthrough.</p>&#13;
    </div>&#13;
    <h2 id="_idParaDest-185" class="heading-2">Setting your active project</h2>&#13;
    <p class="normal">Make <a id="_idIndexMarker588"/>sure you are working on the right Google Cloud project. You can either create a new one or use an existing one. Set it using the following:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">gcloud config set project YOUR_PROJECT_ID&#13;
</code></pre>&#13;
    <p class="normal">You can verify the active project with the following:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">gcloud config list project&#13;
</code></pre>&#13;
    <h2 id="_idParaDest-186" class="heading-2">Enabling the required services</h2>&#13;
    <p class="normal">Now, enable the Google Cloud services<a id="_idIndexMarker589"/><a id="_idIndexMarker590"/> required for deploying your container:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">gcloud services enable cloudresourcemanager.googleapis.com \&#13;
                       servicenetworking.googleapis.com \&#13;
                       run.googleapis.com \&#13;
                       cloudbuild.googleapis.com \&#13;
                       cloudfunctions.googleapis.com&#13;
</code></pre>&#13;
    <p class="normal">On successful execution of the command, you shoul<a id="_idTextAnchor106"/>d see a message similar to the one shown here:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">Operation "operations/..." finished successfully.&#13;
</code></pre>&#13;
    <p class="normal">The alternative to the preceding <code class="inlineCode">gcloud</code> command is through the console by searching for each product. If any API is missed, you can enable it during the implementation. Refer to the documentation for <code class="inlineCode">gcloud</code> commands and usage: <a href="https://cloud.google.com/sdk/gcloud/reference/config/list"><span class="url">https://cloud.google.com/sdk/gcloud/reference/config/list</span></a>.</p>&#13;
    <h2 id="_idParaDest-187" class="heading-2">Adding your project files to Cloud Shell</h2>&#13;
    <p class="normal">Before continuing with the <a id="_idIndexMarker591"/>deployment steps, make sure your Haystack chatbot files are available in your Google Cloud Shell environment.</p>&#13;
    <p class="normal"> You have two options to do this:</p>&#13;
    <ol>&#13;
      <li class="numberedList" value="1"><strong class="screenText">Upload your existing files</strong>: If you have been developing the project locally (e.g., as part of earlier chapters), you can upload your working directory to Cloud Shell using the <strong class="screenText">Upload</strong> option in the Cloud Shell editor. Just click the <strong class="screenText">Open Editor</strong> button (pencil icon), then use <strong class="screenText">File | Upload Files</strong> or drag and drop your folder directly into the editor.</li>&#13;
      <li class="numberedList"> <strong class="screenText">Clone from GitHub (recommended for clean setup)</strong>: Alternatively, you can clone the <em class="italic">Chapter 12</em> code directly from the official book repository using the following command:&#13;
        <pre class="programlisting code"><code class="hljs-code">git clone <a href="https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs.git"><span class="url">https://github.com/PacktPublishing/Building-Neo4j-Powered-Applications-with-LLMs</span><span class="url">.</span><span class="url">git</span></a>&#13;
cd Building-Neo4j-Powered-Applications-with-LLMs/ch12&#13;
</code></pre>&#13;
      </li>&#13;
    </ol>&#13;
    <p class="normal">Once you are inside the <code class="inlineCode">ch12</code> folder, you will find all the necessary files—<code class="inlineCode">app.py</code>, <code class="inlineCode">requirements.txt</code>, <code class="inlineCode">Dockerfile</code>, and <code class="inlineCode">example.env</code>.</p>&#13;
    <p class="normal">If you decide to clone the repo, make sure to follow the step mentioned earlier to generate the <code class="inlineCode">.env</code> file. Now that your project files are in place and ready inside Cloud Shell, it is time to move on to the final stage—deploying your Haystack chatbot to Google Cloud Run.</p>&#13;
    <h1 id="_idParaDest-188" class="heading-1">Deploying to Google Cloud Run</h1>&#13;
    <p class="normal">In this section, you will walk through the full deployment workflow—from setting environment variables and configuring Artifact Registry to building your container and deploying it live<a id="_idIndexMarker592"/> using Cloud Run. Let us break it down step <a id="_idIndexMarker593"/>by step:</p>&#13;
    <ol>&#13;
      <li class="numberedList" value="1">Set up environment variables. Begin by exporting the key environment variables for your Google Cloud project and deployment region. Replace the placeholders with your actual values:&#13;
        <pre class="programlisting code"><code class="hljs-code"># Set your Google Cloud project ID&#13;
export GCP_PROJE<a id="_idTextAnchor107"/>CT='your-project-id'  # Replace with your actual&#13;
roject ID&#13;
# Set your preferred deployment region&#13;
export GCP_REGION='us-central1'       # You can choose a different supported region&#13;
</code></pre>&#13;
      </li>&#13;
      <li class="numberedList">Create an Artifact Registry instance and build the container. Configure your Artifact Registry repository and build your container image using Cloud Build:&#13;
        <pre class="programlisting code"><code class="hljs-code"># Set Artifact Registry repo name and Cloud Run service name&#13;
export AR_REPO='your-repo-name'       # Choose a name like 'genai-chatbot'&#13;
export SERVICE_N<a id="_idTextAnchor108"/>AME='movies-chatbot'  # Or any descriptive name&#13;
</code></pre>&#13;
        <ol>&#13;
          <li class="alphabeticList" value="1">Create the Docker repository:&#13;
            <pre class="programlisting code"><code class="hljs-code"> gcloud artifacts repositories create "$AR_REPO" \&#13;
  --location="$GCP_REGION" \&#13;
  --repository-format=Docker&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="alphabeticList">Authenticate Docker with Artifact Registry:&#13;
            <pre class="programlisting code"><code class="hljs-code">gcloud auth configure-docker "$GCP_REGION-docker.pkg.dev"&#13;
</code></pre>&#13;
          </li>&#13;
          <li class="alphabeticList">Then, build and push your container image:&#13;
            <pre class="programlisting code"><code class="hljs-code">gcloud builds submit \&#13;
  --tag "$GCP_REGION-docker.pkg.dev/$GCP_PROJECT/$AR_REPO/$SERVICE_NAME"&#13;
</code></pre>&#13;
          </li>&#13;
        </ol>&#13;
      </li>&#13;
    </ol>&#13;
    <p class="normal">This command uses your Dockerfile to package the app and pushes the resulting container image to Artifact Registry.</p>&#13;
    <ol>&#13;
      <li class="numberedList" value="3">Deploy to Cloud Run. Before deploying, make sure your <code class="inlineCode">.env</code> file contains all the required environment variables, such as <code class="inlineCode">OPENAI_API_KEY</code>, <code class="inlineCode">NEO4J_URI</code>, and any project-specific configuration.</li>&#13;
    </ol>&#13;
    <p class="normal">To pass <a id="_idIndexMarker594"/>these variables during deployment, convert<a id="_idIndexMarker595"/> your <code class="inlineCode">.env</code> file into a format compatible with the <code class="inlineCode">--set-env-vars</code> flag:</p>&#13;
    <pre class="programlisting code"><code class="hljs-code">E<a id="_idTextAnchor109"/>NV_VARS=$(grep -v '^#' .env | sed 's/ *= */=/g' | xargs -I{} echo -n "{},")&#13;
ENV_VARS=${ENV_VARS%,}&#13;
</code></pre>&#13;
    <ul>&#13;
      <li class="bulletList">Now, deploy your application to Cloud Run:&#13;
        <pre class="programlisting code"><code class="hljs-code">gcloud run deploy "$SERVICE_NAME" \&#13;
  --port=8080 \&#13;
  --image="$GCP_REGION-&#13;
docker.pkg.dev/$GCP_PROJECT/$AR_REPO/$SERVICE_NAME" \&#13;
  --allow-unauthenticated \&#13;
  --region=$GCP_REGION \&#13;
  --platform=managed \&#13;
  --project=$GCP_PROJECT \&#13;
  --set-env-vars="GCP_PROJECT=$GCP_PROJECT,GCP_REGION=$GCP_REGION,$ENV_VARS"&#13;
</code></pre>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">Once complete, Google Cloud Run will return a URL where your chatbot is live and accessible via the web.</p>&#13;
    <p class="normal">Congratulations—your Haystack chatbot is now successfully deployed and running as a serverless application on Google Cloud!</p>&#13;
    <p class="normal">L<a id="_idTextAnchor110"/>et us move on to the final step: testing and verifying the deployment to ensure everything works as expected.</p>&#13;
    <h1 id="_idParaDest-189" class="heading-1">Testing and verifying the deployment on Google Cloud</h1>&#13;
    <p class="normal">Once your <a id="_idIndexMarker596"/>deployment is complete, Google Cloud Run will return a public service URL, typically in <a id="_idTextAnchor111"/>the following format:</p>&#13;
    <pre class="programlisting con"><code class="hljs-con">https://mov<a id="_idTextAnchor112"/>ies-chatbot-[U<a id="_idTextAnchor113"/>NIQUE_ID].${GCP_REGION}.run.app&#13;
</code></pre>&#13;
    <p class="normal">Open this URL<a id="_idIndexMarker597"/> in your browser. You should see your Gradio-powered chatbot interface live on the web—identical in functionality to your local version. You can now interact with the chatbot, submit queries, and receive movie recommendations just as before, but this time, it is running fully in the cloud.</p>&#13;
    <p class="normal">If something does not work as expected, keep the following checklist ready for troubleshooting:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Dependency check</strong>: Make sure your Dockerfile correctly installs all dependencies using <code class="inlineCode">pip install -r requirements.txt</code>. Missing dependencies can result in build or runtime errors on Cloud Run.</li>&#13;
      <li class="bulletList"><strong class="screenText">Cloud Shell versus local environment</strong>: If you are not using Google Cloud Shell, ensure your local environment is authenticated with Google Cloud using a service account that has appropriate permissions for Cloud Run, Artifact Registry, and (if applicable) Vertex AI.</li>&#13;
      <li class="bulletList"><strong class="screenText">Monitor logs and metrics</strong>: You can monitor your service’s logs, request history, and performance metrics directly from the Google Cloud console under <strong class="screenText">Cloud Run</strong>. This is especially useful for debugging and performance tuning.</li>&#13;
      <li class="bulletList"><strong class="screenText">Cloud Run service management</strong>: Navigate to <strong class="screenText">Cloud Run</strong> in the Cloud console, where you will see a list of deployed services. Your chatbot (e.g., <code class="inlineCode">movies-chatbot</code>) should appear here. Clicking on the service name will give you access to the following:<ul>&#13;
          <li class="bulletList">The public service URL</li>&#13;
          <li class="bulletList">Deployment history</li>&#13;
          <li class="bulletList">Container configuration</li>&#13;
          <li class="bulletList">Environment variables</li>&#13;
          <li class="bulletList">Logs and error reports</li>&#13;
        </ul>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">This visibility makes it <a id="_idIndexMarker598"/>easy to track and manage your application <a id="_idIndexMarker599"/>post-deployment.</p>&#13;
    <p class="normal">With your chatbot now live, deployed on a scalable serverless platform, and publicly accessible, you have successfully completed the deployment journey. Your GenAI-powered movie recommendation chatbot is now ready to be used, shared, and further enhanced.</p>&#13;
    <h2 id="_idParaDest-190" class="heading-2">Deploying the chatbot to other clouds</h2>&#13;
    <p class="normal">As <a id="_idIndexMarker600"/>mentioned in the initial sect<a id="_idTextAnchor114"/>ion, once you have <code class="inlineCode">docker compose</code> prepared, you can follow the instructions provided to deploy the same application to other clouds:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Azure deployment</strong>: Please follow the instructions provided at this link:  <a href="https://techcommunity.microsoft.com/blog/appsonazureblog/how-to-deploy-a-local-docker-container-to-azure-container-apps/3583888"><span class="url">https://techcommunity.microsoft.com/blog/appsonazureblog/how-to-deploy-a-local-docker-container-to-azure-container-apps/3583888</span></a>.</li>&#13;
      <li class="bulletList"><strong class="screenText">AWS deployment</strong>: Docker provides a clear guide on deploying <code class="inlineCode">docker compose</code> files to AWS. Please follow this link for more details: <a href="https://www.docker.com/blog/docker-compose-from-local-to-amazon-ecs/"><span class="url">https://www.docker.com/blog/docker-compose-from-local-to-amazon-ecs/</span></a>.</li>&#13;
    </ul>&#13;
    <p class="normal">When you follow the instructions in these links, you can see the similarity with the deployment approach when we chose to containerize the applications.</p>&#13;
    <p class="normal">There is a lot of information available in each cloud’s official documentation to deploy Spring Boot applications to the cloud. For example, if you are looking to run the application we created in <em class="italic">Chapters 9</em> and <em class="italic">10</em>, you can follow the steps in the documentation provided by various cloud <a id="_idIndexMarker601"/>vendors:</p>&#13;
    <ul>&#13;
      <li class="bulletList"><strong class="screenText">Google Cloud</strong>: This article (<a href="https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-java-service"><span class="url">https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-java-service</span></a>) lists detailed steps to deploy on Google Cloud Run</li>&#13;
      <li class="bulletList"><strong class="screenText">Azure</strong>: This article (<a href="https://learn.microsoft.com/en-us/azure/spring-apps/basic-standard/how-to-maven-deploy-apps"><span class="url">https://learn.microsoft.com/en-us/azure/spring-apps/basic-standard/how-to-maven-deploy-apps</span></a>) shows how to deploy the Spring Boot application on Azure</li>&#13;
      <li class="bulletList"><strong class="screenText">AWS</strong>: For Azure here are some articles for your reference.<ul>&#13;
          <li class="bulletList">This article (<a href="https://community.aws/content/2qk6oFuOPiA4G0N83ocU0REbtdN/step-by-step-guide-to-deploying-a-spring-boot-application-on-aws-ec2-with-best-practices"><span class="url">https://community.aws/content/2qk6oFuOPiA4G0N83ocU0REbtdN/step-by-step-guide-to-deploying-a-spring-boot-application-on-aws-ec2-with-best-practices</span></a>) shows us how to deploy the Spring Boot application on AWS EC2</li>&#13;
          <li class="bulletList">This article (<a href="https://www.geeksforgeeks.org/deploy-a-spring-boot-application-with-aws/"><span class="url">https://www.geeksforgeeks.org/deploy-a-spring-boot-application-with-aws/</span></a>) shows how to deploy the Spring Boot application using AWS Elastic Beanstalk</li>&#13;
        </ul>&#13;
      </li>&#13;
    </ul>&#13;
    <p class="normal">These articles are good enough to guide you through deploying our GenAI application as, at its heart, it is a simple application that does not need scaling as such since it only performs the augmentation—that, too, as a batch process.</p>&#13;
    <p class="normal">The production deployment of these applications is a more complex procedure that requires you to focus on various supporting elements, such as database, monitoring, and so on. Discussing <span id="page12-9" role="doc-pagebreak" aria-label="262" epub:type="pagebreak"/>the whole deployment is out of the scope of this book, but we will highlight the deployment architecture and key considerations to deploy your applications in production in the next section.</p>&#13;
    <h1 id="_idParaDest-191" class="heading-1">Preparing for deployment in production: key considerations</h1>&#13;
    <p class="normal">In this section, we<a id="_idIndexMarker602"/> will look at a typical architecture deployment for intelligent applications. There are a lot of other aspects we need to keep in mind when we are moving to production. For simplicity, we will refer to the augmentation application we built in <em class="italic">Chapters 9</em> and <em class="italic">10</em>.</p>&#13;
    <p class="normal">Let us look at all the tasks we did from loading data to reviewing the results: </p>&#13;
    <ol>&#13;
      <li class="numberedList" value="1">We loaded the data into a graph.</li>&#13;
      <li class="numberedList">The graph was enhanced with seasonal relationships. We augmented the graph using the augmentation application—articles as well as customer behavior aspects.</li>&#13;
      <li class="numberedList">We also utilized KNN similarity and community detection algorithms to enhance the graph and reviewed how this approach gives us better results.</li>&#13;
    </ol>&#13;
    <p class="normal">In a production deployment, all of these aspects may need to be automated and deployed as individual applications. <a id="_idTextAnchor115"/>Let us take a brief look at all of these aspects that we need to take care of.</p>&#13;
    <p class="normal">When we are deploying intelligent applications to production, we need to make sure we take care of data ingestion, data consumption, the LLM or ML pipeline to augment the graph, and the graph database deployment architecture for scale.</p>&#13;
    <p class="normal">Let’s first take a look at the deployment architecture in <em class="italic">Figure 12.1</em>.</p>&#13;
    <figure class="mediaobject"><img src="../Images/B31107_12_01-01.png" alt="Figure 12.1 — Deployment architecture of Neo4j intelligent applications" width="1421" height="697"/></figure>&#13;
    <p class="packt_figref">Figure 12.1 — Deployment architecture of Neo4j intelligent applications</p>&#13;
    <p class="normal">We can see there <a id="_idIndexMarker603"/>are two different Neo4j databases shown here. In Neo4j for regular interaction, we will have <code class="inlineCode">Primary</code>, which can perform both <code class="inlineCode">READ</code> and <code class="inlineCode">WRITE</code> capabilities. For analytical, <strong class="screenText">Graph Data Science</strong> (<strong class="screenText">GDS</strong>), and other uses, we will use <code class="inlineCode">Secondary</code>, which provides only the <code class="inlineCode">READ</code> capability. We can have more than one <code class="inlineCode">Primary</code> database to provide high availability and more than one <code class="inlineCode">Secondary</code> database to provide horizontal scalability. We can see from the diagram that all the regular interactions, including data ingestion and data consumption, are handled by <code class="inlineCode">Primary</code> and the analytical workload that augments the graph is handled by <code class="inlineCode">Secondary</code>.</p>&#13;
    <p class="normal">This type of deployment architecture also makes it easy to maintain and monitor the system. The Neo4j database comes with <strong class="screenText">Neo4j Ops Manager</strong> (<a href="https://neo4j.com/docs/ops-manager/current/"><span class="url">https://neo4j.com/docs/ops-manager/current/</span></a>) for deploying and monitoring Neo4j servers. It provides dashboards to monitor the current health of the system as well as to set up alerts to be notified in case of errors. </p>&#13;
    <p class="normal">For the other applications, we need to have similar monitoring, especially for data ingestion and augmentation applications. When they fail in the middle, we should be able to restart from where we have failed. The augmentation application is built to handle the data that way.</p>&#13;
    <p class="normal">When we<a id="_idTextAnchor116"/> are building the ingestion pipeline, we need to keep i<a id="_idTextAnchor117"/>n mind these aspects: </p>&#13;
    <ul>&#13;
      <li class="bulletList">What is our initial data size?</li>&#13;
      <li class="bulletList">What are our day-to-day changes (incremental data changes) and their size?</li>&#13;
      <li class="bulletList">How does incremental data come? Is it coming in near-real time, as a batch at regular intervals, a big batch at the end of the day, or interactive changes made by end <a id="_idIndexMarker604"/>users via the UI?</li>&#13;
    </ul>&#13;
    <p class="normal">We will look at best practices for these scenarios. </p>&#13;
    <h2 id="_idParaDest-192" class="heading-2">Initial data load</h2>&#13;
    <p class="normal">If we are migrating from another data <a id="_idIndexMarker605"/>source or database, we might have to move the data into Neo4j for the first time. Depending on this data size, we must decide whether we can take a transactional approach to load the data or leverage the offline data import approach called <code class="inlineCode">neo4j-admin</code> import. </p>&#13;
    <p class="normal">In <a href="Chapter_09.xhtml#_idTextAnchor059"><em class="italic">Chapter 9</em></a>, we used a transactional approach to load the data. If the amount of data we are loading is under a few million records, say 100 million, we can load this data in a reasonable amount of time. When we load the data transactionally, the Neo4j database needs to update the indexes and keep transaction logs apart, committing the data to the database. This adds a good amount of overhead to the process. But this approach gives us more flexibility and reusable code to use with incremental data loading. If we are loading data into a cluster, we can use this approach to make <span id="page14-9" role="doc-pagebreak" aria-label="264" epub:type="pagebreak"/>sure the data is available across the cluster, as the Neo4j database server makes sure the changes are replicated across the cluster. We have used the <strong class="screenText">LOAD CSV</strong> option to load the data. You can read more about this at <a href="https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/"><span class="url">https://neo4j.com/docs/cypher-manual/current/clauses/load-csv/</span></a>.</p>&#13;
    <p class="normal">This approach is fine for proofs of concept and ad hoc data load purposes, but for production systems, the data ingestion should be performed using a client by connecting to the database using the Neo4j protocol. While the <strong class="screenText">LOAD CSV</strong> option is simple and attractive, it uses up the database heap to load the data and perform the data ingestion, which might not be desirable. A basic Python client application that can ingest data into a graph can be found at <a href="https://github.com/neo4j-field/pyingest"><span class="url">https://github.com/neo4j-field/pyingest</span></a>. Note that this is a sample client, and you need to build one that suits your production needs.</p>&#13;
    <p class="normal">If the data sizes are <a id="_idIndexMarker606"/>bigger, then using the <strong class="screenText">Neo4j Admin Import</strong> process would be best. For this purpose, we need to prepare CSV files for nodes and relationships in a specific format and use the <code class="inlineCode">neo4j-admin</code> tool to prepare the database. You can read more about the CSV file formats and examples at <a href="https://neo4j.com/docs/operations-manual/current/tools/neo4j-admin/neo4j-admin-import/"><span class="url">https://neo4j.com/docs/operations-manual/current/tools/neo4j-admin/neo4j-admin-import/</span></a>.</p>&#13;
    <h2 id="_idParaDest-193" class="heading-2">Incremental data load</h2>&#13;
    <p class="normal">The approaches for incremental data <a id="_idIndexMarker607"/>loading depend on the framework we will use to load the data. If there is a lot of streaming data, then leveraging a framework such as Apache Kafka might be a good idea. Also, it is easy to build applications to interact with databases to ingest the data using language frameworks such as Java, JavaScript, .NET, or Python.</p>&#13;
    <p class="normal">You can read about building client applications for Neo4j at <a href="https://neo4j.com/docs/create-applications/"><span class="url">https://neo4j.com/docs/create-applications/</span></a>. One thing we need to keep in mind is to leverage the managed transactional functions so that the driver can retry the transactions as needed in a cluster when the cluster topology changes due to network failures or server failures. You can read more about this at <a href="https://neo4j.com/docs/java-manual/current/transactions/"><span class="url">https://neo4j.com/docs/java-manual/current/transactions/</span></a>. This link points to Java usage, but the same feature is available for all supported language frameworks.</p>&#13;
    <h2 id="_idParaDest-194" class="heading-2">Graph augmentation</h2>&#13;
    <p class="normal">While we have built the <a id="_idIndexMarker608"/>article augmentation and customer augmentation as a Spring Boot application, there are other aspects that we have not investigated automating. After we generated embeddings for a specified season for the customer, we ran the ML aspects as individual commands such as KNN similarity and community detection. We might have to automate these aspects, too. Whenever new data is ingested into the graph, we might have to trigger the augmentation appli<span id="page15-8" role="doc-pagebreak" aria-label="265" epub:type="pagebreak"/>cation to generate the embeddings to adapt to the new data we have loaded, and then trigger the GDS algorithms after that is completed. Note that the ML pipeline we looked at is simple chaining of KNN similarity and community detection; they can be much more complex based on the needs.</p>&#13;
    <p class="normal">If you want to read more about ML pipelines with Neo4j, you can visit <a href="https://neo4j.com/docs/graph-data-science/current/machine-learning/machine-learning/"><span class="url">https://neo4j.com/docs/graph-data-science/current/machine-learning/machine-learning/</span></a>.</p>&#13;
    <p class="normal">We explored how to look at recommendations as Cypher queries as part of analysis and validation. Once we are satisfied with queries, we might have to build an application to provide recommendations as needed on demand. </p>&#13;
    <p class="normal">We have built our application on the Spring Framework, which provides various capabilities and options to build a production-grade application that makes it easier to deploy and monitor applications. You can read more about how we can package the Spring applications for production deployment at <a href="https://docs.spring.io/spring-boot/reference/packaging/index.html"><span class="url">https://docs.spring.io/spring-boot/reference/packaging/index.html</span></a>. You can read more about the production-level features that help us monitor the application at <a href="https://docs.spring.io/spring-boot/reference/actuator/index.html"><span class="url">https://docs.spring.io/spring-boot/reference/actuator/index.html</span></a>.</p>&#13;
    <p class="normal">These are the key <a id="_idIndexMarker609"/>principles and considerations for deployment; for production deployment, multiple aspects need to be considered, including a proper deployment architecture along with application development. Several processes need to be deployed for monitoring and performance evaluation to make sure the application scales with usage as data and traffic grows.</p>&#13;
    <h1 id="_idParaDest-195" class="heading-1">Summary</h1>&#13;
    <p class="normal">In this concluding chapter, you learned how to take your Haystack-powered GenAI chatbot from local development to a fully deployed, cloud-hosted application using Google Cloud Run. We walked through preparing your project structure, containerizing your application with Docker, configuring essential Google Cloud services, and deploying your chatbot in a scalable and serverless environment. You also learned how to verify the deployment, monitor performance, and troubleshoot common issues—equipping you with practical skills that extend far beyond just this project.</p>&#13;
    <p class="normal">More importantly, this chapter brought everything full circle. From understanding knowledge graphs and vector search to integrating GenAI workflows using Haystack and Neo4j and, finally, deploying your application to the cloud—you now have a complete end-to-end blueprint for building intelligent, scalable, and production-ready GenAI applications.</p>&#13;
    <p class="normal">We’ve now wrapped up our journey of building Neo4j-powered apps with LLMs. Up next, we’ll take a quick look at the key takeaways of this journey.</p>&#13;
  </div>&#13;
</div></div></body></html>