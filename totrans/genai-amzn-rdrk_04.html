<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-73"><a id="_idTextAnchor073"/>4</h1>
<h1 id="_idParaDest-74"><a id="_idTextAnchor074"/>Customizing Models for Enhanced Performance</h1>
<p>When general-purpose models fall short of delivering satisfactory results for your domain-specific use case, customizing FMs becomes crucial. This chapter delves into the process of customizing FMs while using techniques such as fine-tuning and continued pre-training to enhance their performance. We’ll begin by examining the rationale behind customizing the base FM and exploring the mechanics of fine-tuning. Subsequently, we will delve into data preparation techniques to ensure our data is formatted appropriately for creating a custom model using both the AWS console and APIs. We will understand various components within model customization and different customization APIs that you can call from your application.</p>
<p>Furthermore, we will analyze the model’s behavior and perform inference. Finally, we will conclude this chapter by discussing guidelines and best practices for customizing Bedrock models.</p>
<p>By the end of this chapter, you will be able to understand the importance and process of customizing a model for your domain-specific use case.</p>
<p>The following key topics will be covered in this chapter:</p>
<ul>
<li>Why is customizing FMs important?</li>
<li>Understanding model customization</li>
<li>Preparing the data</li>
<li>Creating a custom model</li>
<li>Analyzing the results</li>
<li>Guidelines and best practices<a id="_idTextAnchor075"/></li>
</ul>
<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>Technical requirements</h1>
<p>For this chapter, you need to have access to an <em class="italic">AWS</em> account. If you don’t have one, you can go to <a href="https://aws.amazon.com/getting-started/">https://aws.amazon.com/getting-started/</a> and create an AWS account.</p>
<p>Once you have access to an AWS account, you will need to install and configure the AWS CLI (<a href="https://aws.amazon.com/cli/">https://aws.amazon.com/cli/</a>) so that you can access Amazon Bedrock FMs from your local machine. In addition, you will need to set up the AWS Python SDK (Boto3) since the majority of the code cells we will be executing require it (<a href="https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html">https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html</a>). You can set up Python by installing it on your local machine, using AWS Cloud9, utilizing AWS Lambda, or leveraging Amazon SageMaker.</p>
<p class="callout-heading">Note</p>
<p class="callout">There will be a charge associated with invocating and customizing the FMs of Amazon Bedrock. Please refer to <a href="https://aws.amazon.com/bedrock/pricing/">https://aws.amazon.com/bedrock/pricing/</a> to learn more.</p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>Why is customizing FMs important?</h1>
<p>In the previous <a id="_idIndexMarker290"/>chapter, we looked at several prompt engineering techniques to improve the performance of a model. As we also saw in <a href="B22045_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a> (and shown in <em class="italic">Figure 4</em><em class="italic">.1</em>), these FMs are trained on massive amounts of data (GBs, TBs, or PBs) with millions to billions of parameters, allowing them to understand relationships between words in context to predict subsequent sequences:</p>
<div><div><img alt="Figure 4.1 – Training an FM" src="img/B22045_04_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Training an FM</p>
<p><em class="italic">So, why do we need to customize </em><em class="italic">these models?</em></p>
<p>That’s a fair question since a lot of use cases can be directly solved by using prompt engineering and RAG techniques (which we will cover in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>). However, consider a situation where<a id="_idIndexMarker291"/> you require the model to adhere to a particular writing style, output format, or domain-specific terminology. For instance, you may need the model to analyze financial earnings reports or medical records accurately. In such cases, the pre-trained models might not have been exposed to the desired writing style or specialized vocabularies, limiting their performance despite effective prompt crafting or RAG implementation.</p>
<p>To bridge this gap and enhance the model’s domain-specific language understanding and generation capabilities, customization becomes essential. By fine-tuning the pre-trained models on domain-specific data or adapting them to the desired writing style or output format, you can tailor their performance to meet your unique requirements, ensuring more accurate and relevant responses:</p>
<div><div><img alt="Figure 4.2 – Generative AI performance techniques" src="img/B22045_04_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Generative AI performance techniques</p>
<p>If you look at the spectrum of generative AI performance techniques shown in <em class="italic">Figure 4</em><em class="italic">.2</em> for improving the performance of FMs, it ranges from prompt engineering to training the model from scratch. For domain-specific data, prompt engineering techniques may provide low accuracy, but they involve less effort and are cost-effective. Prompt engineering is a better option if you have a simple task and don’t need a new domain-specific dataset. If you would like to understand how prompt engineering works, please go back to <a href="B22045_03.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>.</p>
<p>Next on the spectrum with a little bit of increasing complexity, cost, and accuracy is RAG. This technique fetches data from outside the language model, such as from internal knowledge bases or external sources. It is a particularly useful technique when you have large corpora of documents that do not fit the context length of the model. We will discuss RAG in more detail in <a href="B22045_05.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>.</p>
<p>Further on the <a id="_idIndexMarker292"/>spectrum, customizing the model is essentially more time-consuming and costly. However, it provides greater accuracy to your specialized use case.</p>
<p>There are two customization techniques within Amazon Bedrock: fine-tuning and continued pretraining.</p>
<p>In <em class="italic">fine-tuning</em>, the model is trained with the labeled dataset – a supervised learning approach. The labeled dataset that you provide will be specific to your use case. Whether you’re working in healthcare, finance, or any other field, you can fine-tune your model to become an expert in that particular domain. In healthcare, for example, the model can be fine-tuned for medical specialization, allowing it to understand and interpret medical records with greater accuracy. Similarly, a financial analysis model can be fine-tuned for niche financial analysis, enabling it to identify patterns and trends in financial data that may be missed by traditional algorithms.</p>
<p>To fine-tune a model using your own data, you need to have a sufficient amount of high-quality data that is relevant to the task you want to perform. This data should be labeled and annotated to provide the model with the necessary information for training. As shown in <em class="italic">Figure 4</em><em class="italic">.3</em>, we can use this labeled dataset to fine-tune the base FM, which then generates a custom model. You can then use the custom model to generate responses that are tailored to your specific domain and use case:</p>
<div><div><img alt="Figure 4.3 – Fine-tuning" src="img/B22045_04_03.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Fine-tuning</p>
<p>For example, let’s say you work in the medical industry and would like to summarize a dialog between two doctors discussing the medical report of the patient, extract the information to<a id="_idIndexMarker293"/> be put into medical forms, and maybe write it in layman’s terms.</p>
<p>In this case, the base FMs might not be trained on the domain-specific dataset. Hence, this is an example scenario where when we perform fine-tuning, we will provide the model with labeled examples of how the prompt and response should look like.</p>
<p>In <em class="italic">continued pre-training</em>, we<a id="_idIndexMarker294"/> adapt to a new domain or train the model to learn the terminologies of an unfamiliar domain. This involves providing additional continuous training to an FM while utilizing large amounts of unlabeled data. When we say unlabeled data, we mean that there is no target label, and the model will learn the patterns from the provided texts. This contrasts with fine-tuning, which involves using smaller quantities of labeled data. <em class="italic">Figure 4</em><em class="italic">.4</em> highlights the difference between the labeled and unlabeled data that’s required for continued pre-training and fine-tuning, respectively:</p>
<div><div><img alt="Figure 4.4 – Unlabeled versus labeled data" src="img/B22045_04_04.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Unlabeled versus labeled data</p>
<p>Examples of continued pre-training can include training the model to learn the terminologies of the financial industry so that it can understand financial reports, or training the model to learn quantum physics by giving it abundant information from books so that it will be able to evaluate/predict the tokens associated with string theory with greater accuracy. Let’s say that two physicists are having a dialog around string theory, and we <a id="_idIndexMarker295"/>pass that dialog as a context to the base FM (as shown in <em class="italic">Figure 4</em><em class="italic">.5</em>):</p>
<div><div><img alt="Figure 4.5 – Quantum physicist dialog and question" src="img/B22045_04_05.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Quantum physicist dialog and question</p>
<p>It could be possible that the base FM we are using here isn’t familiar with quantum physics – that is, the base FM hasn’t been trained on a dataset related to quantum physics.</p>
<p>So, when we ask the model a question such as <code>What are E8 x E8 symmetry groups?</code>, the model hallucinates and doesn’t explain this concept since it doesn’t know about string theory.</p>
<p>With continued pre-training, we train the model on an unfamiliar domain by providing the base FM with a large amount of unlabeled datasets. For example, we could train the model on textbooks about quantum computing in the desired format, as explained in the <em class="italic">Preparing the data</em> section, which<a id="_idIndexMarker296"/> then creates a custom model (as shown in <em class="italic">Figure 4</em><em class="italic">.6</em>):</p>
<div><div><img alt="Figure 4.6 – Continued pre-training" src="img/B22045_04_06.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – Continued pre-training</p>
<p>Continued pre-training presents certain challenges. As we are training the entire model, the weights and biases are what demand heavy computational resources and diverse unlabeled text data.</p>
<p>When you’re deciding whether to use custom models over other methods, such as prompt engineering and RAG, several factors come into play. These include the task that you are working on, the availability of data, computational resources, and cost. Here are some guidelines to help you make an informed decision:</p>
<ul>
<li><strong class="bold">Complexity level</strong>: Creating custom models is particularly useful when you have tasks that are complex and require the model to understand intricate details.</li>
<li><strong class="bold">Specialized data</strong>: Having a sufficient amount of specialized data for creating custom models will provide remarkable results. Make sure your data is clean (free from errors, inconsistencies, and duplicates) and prepared (formatted, transformed, and split into appropriate subsets) before you start the training process.</li>
<li><strong class="bold">Computational resources and cost</strong>: When you create custom models, you’ll need to purchase Provisioned Throughput, which gives you a dedicated capacity to deploy the model. Make sure you review the pricing based on the model type and commitment terms. We will discuss Provisioned Throughput in detail in the <em class="italic">Analyzing the results</em> section of this chapter.</li>
</ul>
<p>In addition, creating custom models provides you with greater control over how you want the model to respond. You can customize it precisely to your needs, making it suitable for tasks that <a id="_idIndexMarker297"/>require fine-grained customization, such as responding in a specific tone, dialect, or inclusive language.</p>
<p>Let’s understand some key concepts of model customization before we start our first model customization job.</p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor078"/>Understanding model customization</h1>
<p>The principle behind fine-tuning <a id="_idIndexMarker298"/>and continued pre-training comes from the broad concept of <strong class="bold">transfer learning</strong>, which, as<a id="_idIndexMarker299"/> its name suggests, entails transferring knowledge that’s been acquired from one problem to other often related but distinct problems. This practice is widely employed in the field of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) to <a id="_idIndexMarker300"/>enhance the performance of models on new tasks or domains.</p>
<p>Model customization is a five-step process:</p>
<ol>
<li><strong class="bold">Identify your use case and data</strong>: Identifying the use case/task and how it solves your organization’s business objectives is a critical step. Do you want to summarize legal documents, perform Q&amp;A on medical reports, or do something else? Once you’ve identified the use case, you must gather enough relevant datasets that you can use for model customization. The dataset should contain examples that the model can learn intricate details from. Remember, how your custom model performs on your task-specific use case depends on the quality of the dataset that you provide for training.</li>
<li><strong class="bold">Prepare the dataset</strong>: Once you’ve gathered the dataset, you have to clean and preprocess it. For fine-tuning, you need to have labeled examples in <strong class="bold">JSON lines</strong> (<strong class="bold">JSONL</strong>) format. For continued<a id="_idIndexMarker301"/> pre-training, you need to have unlabeled examples in JSONL format. We will discuss this in more detail in the <em class="italic">Preparing the </em><em class="italic">data</em> section.</li>
<li><strong class="bold">Select the base pre-trained model</strong>: Once the dataset has been prepared, you have to select an existing base pretrained model that you would like to fine-tune. You can look at the website of the model provider to understand the model attributes. If it is fit for your use case, try prompt engineering techniques to check which <a id="_idIndexMarker302"/>model responds closest to what you are looking for, and also evaluate the FMs<a id="_idIndexMarker303"/> using <strong class="bold">Model evaluation</strong> within Amazon Bedrock or <a id="_idIndexMarker304"/>using <strong class="bold">Model leaderboards</strong>:<ul><li><em class="italic">Model evaluation</em>: Bedrock provides two distinct evaluation methods: automatic evaluation and human evaluation. Automatic evaluation utilizes predefined metrics such as accuracy, robustness, and toxicity screening, whereas with human evaluation, you can define custom metrics such as friendliness, stylistic adherence, or alignment with brand voice. We will have a more detailed discussion on model evaluation in <a href="B22045_11.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a>.</li><li><em class="italic">Model leaderboards</em>: Several leaderboards are available that rank models based on their performance on various tasks, such as text generation, summarization, sentiment analysis, and more. Some of the most popular leaderboards<a id="_idIndexMarker305"/> include <strong class="bold">General Language Understanding Evaluation</strong> (<strong class="bold">GLUE</strong>), SuperGLUE, HELM, and OpenLLM by HuggingFace.</li></ul><p class="list-inset">Please note that although it’s good to understand the performance of the FM through leaderboards, for real-world use cases, you have to be cautious and not rely solely on leaderboards as they may lack the robustness required to mirror the complexity of real-world use.</p></li>
<li><strong class="bold">Configure and start the fine-tuning job</strong>: Once you’ve identified the base FM and the dataset is ready, you can configure the fine-tuning job by specifying hyperparameters, the input and output S3 path for the dataset and store metrics, respectively, and networking and security settings. We will discuss this in more detail in the <em class="italic">Creating a custom </em><em class="italic">model</em> section.</li>
<li><strong class="bold">Evaluate and iterate</strong>: Once the model is ready, you can evaluate and analyze it based on the metrics and logs stored by the model. To do so, you can put aside a validation set that provides the performance metric of the custom model you’ve created. We will discuss this in more detail in the <em class="italic">Analyzing the </em><em class="italic">results</em> section.</li>
</ol>
<p>When we are customizing a <a id="_idIndexMarker306"/>model, Amazon Bedrock creates a copy of the base FM, on which we essentially update its model weights. <strong class="bold">Weights</strong> are<a id="_idIndexMarker307"/> key <a id="_idIndexMarker308"/>components in <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>) and are attached to the inputs (or features). These weights define which features are important in predicting the output and getting better at specific tasks. <em class="italic">Figure 4</em><em class="italic">.7</em> shows a simplified ANN architecture where these inputs, along with their weights, are processed by <strong class="bold">summation</strong> and the <strong class="bold">activation function</strong> (both<a id="_idIndexMarker309"/> defined in the model algorithm) to get the output (<strong class="bold">Y</strong>).</p>
<div><div><img alt="Figure 4.7 – Simplified ANN" src="img/B22045_04_07.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Simplified ANN</p>
<p>For a deeper dive into ANNs, there are numerous online tutorials and courses available that provide in-depth explanations and examples of neural network concepts, architectures, and training techniques. Additionally, textbook classics such as <em class="italic">Neural Networks and Deep Learning, Michael Nielsen, Determination Press</em> and <em class="italic">Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press</em> offer comprehensive theoretical and mathematical foundations.</p>
<p>When we perform model customization (fine-tuning or continued pre-training), we update the model weights. While updating the model weights, a common problem can occur called <strong class="bold">catastrophic forgetting</strong>. This is <a id="_idIndexMarker310"/>when the model starts to forget some information it was originally trained on due to weight modifications, which can lead to degraded performance on more generalized tasks. In general, this can happen due to overfitting the training data, which means the model provides an accurate response to the training data but can’t generalize well and provides degraded performance on new information. In addition, customizing the model can be costly and resource-intensive, something that requires extensive memory utilization.</p>
<p>To overcome these challenges, a technique called <strong class="bold">Parameter-efficient Fine-tuning</strong> (<strong class="bold">PEFT</strong>) was introduced in the paper <em class="italic">Parameter-Efficient Transfer Learning for </em><em class="italic">NLP</em> (<a href="https://arxiv.org/pdf/1902.00751">https://arxiv.org/pdf/1902.00751</a>).</p>
<p>Note that at the<a id="_idIndexMarker311"/> time of writing, Bedrock does not support PEFT. However, it’s good to have an understanding of the PEFT technique.</p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/>PEFT</h2>
<p>In PEFT, you<a id="_idIndexMarker312"/> don’t <a id="_idIndexMarker313"/>need to fine-tune all the model parameters, something that can be quite time-consuming, resource-intensive, and costly. Instead, it freezes much of the model weights and you only need to train a small number of them. This makes it memory and compute-efficient, less susceptible to catastrophic forgetting, and cheaper to store the model on the hardware.</p>
<p>When fine-tuning LLMs, various techniques can reduce the number of trainable parameters to improve efficiency. We can categorize these PEFT methods into three main classes:</p>
<ul>
<li><strong class="bold">Selective methods</strong>: These<a id="_idIndexMarker314"/> update only certain components or layers of the original LLM during fine-tuning. This allows you to focus on the most relevant parts of the model. However, it can result in suboptimal performance compared to full fine-tuning.</li>
<li><strong class="bold">Reparameterization methods</strong>: These introduce low-rank matrices to compress the original weights. Examples include such as <strong class="bold">Low-Rank Adaptation of Large Language Models</strong> (<strong class="bold">LoRA</strong>). This <a id="_idIndexMarker315"/>reduces parameters while still modifying the whole model. The trade-off is increased memory usage during training.</li>
<li><strong class="bold">Additive methods</strong>: These keep the original weights of the LLM frozen and add new trainable layers for task-specific adaptation. Additive methods such as <strong class="bold">adapters</strong> add the <a id="_idIndexMarker316"/>trainable layer inside the encoder or decoder component of the transformer architecture.</li>
</ul>
<p>The choice of the PEFT approach involves balancing metrics such as parameter and memory efficiency against model quality, training speed, and cost. Selectively updating parts of a model offers one end of this trade-off, while adapters and prompts maximize parameter <a id="_idIndexMarker317"/>efficiency at the cost of some architectural <a id="_idIndexMarker318"/>changes.</p>
<p>With that, we’ve covered PEFT and its techniques at a very high level. However, if you are interested in learning more about it, go to <a href="https://github.com/huggingface/peft">https://github.com/huggingface/peft</a>. In addition, the <em class="italic">Generative AI with Large Language Models</em> course provides in-depth information about PEFT methods: <a href="https://www.deeplearning.ai/courses/generative-ai-with-llms/">https://www.deeplearning.ai/courses/generative-ai-with-llms/</a>.</p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Hyperparameter tuning</h2>
<p>In addition to fine-tuning<a id="_idIndexMarker319"/> techniques such as PEFT, <strong class="bold">hyperparameter tuning</strong> also <a id="_idIndexMarker320"/>plays a big role in ensuring a model retains its pretrained knowledge. Hyperparameters are configuration settings that control the model training process, much like knobs that can be tweaked and tuned. Models have various hyperparameters, including the learning rate, number of epochs, batch size, beta, gamma, and more. Each model may require a different set of optimal hyperparameter values, found through experimentation, to achieve the best performance and accuracy.</p>
<p>The <strong class="bold">learning rate</strong> hyperparameter<a id="_idIndexMarker321"/> controls how quickly the model is adapted to the task. It also controls how much the model’s parameters are adjusted during each iteration of the training process. It determines the step size at which the model’s parameters are updated based on the calculated gradients (which represent the direction and magnitude of the changes needed to minimize the loss function).</p>
<p>Let’s consider an analogy that might help you visualize the learning rate.</p>
<p>Imagine that you’re trying to find the lowest point in a hilly landscape, but you’re blindfolded. You can only sense the steepness of the slope you’re standing on (the gradient) and take steps accordingly. The learning rate determines how big or small those steps should be:</p>
<ul>
<li>If the learning rate is too high, you might overshoot the lowest point and end up on the other side of the hill, continually overshooting and never converging to the optimal solution</li>
<li>If the learning rate is too low, you might take tiny steps and get stuck on a plateau or make painfully slow progress toward the lowest point</li>
</ul>
<p>The ideal learning rate allows you to take reasonably sized steps that bring you progressively closer to the lowest point (the optimal set of model parameters) without overshooting or getting stuck.</p>
<p>In practice, finding the optimal learning rate is often a matter of experimentation and tuning. Different models and datasets may require different learning rates for the training process to converge effectively.</p>
<p>Now that we understand the concepts behind fine-tuning, let’s start the customization process by<a id="_idIndexMarker322"/> preparing<a id="_idIndexMarker323"/> the data.</p>
<h1 id="_idParaDest-80"><a id="_idTextAnchor081"/>Preparing the data</h1>
<p>We’ve already <a id="_idIndexMarker324"/>seen why customizing the model is important to improve its accuracy and performance. We’ve also seen that continued pre-training is an unsupervised learning approach that needs unlabeled data, whereas fine-tuning is a supervised learning approach that needs labeled data.</p>
<p>The type of data we provide to the model can change the way the model responds. If the data is biased or has highly correlated features, you might not get the right responses from the trained custom model. This is true for any ML models you are training, so it is essential to provide high-quality data. While I won’t cover data processing and feature engineering concepts in this book, I wanted to highlight their importance. If you wish to learn more about these concepts, you can go through any ML courses and books, such as <em class="italic">Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em> by Aurélien Géron, and <em class="italic">Feature Engineering for Machine Learning</em> by Alice Zheng and Amanda Casari.</p>
<p>The dataset that you need for continued pre-training and fine-tuning should be in JSONL format. The following documentation explains what JSONL format is, its requirements, sample examples, and its validator: <a href="https://jsonlines.org/">https://jsonlines.org/</a>.</p>
<p>Now, let’s look at the data preparation techniques we can use for both methods.</p>
<p>Continued pre-training expects the data to be in <code>{"input": "&lt;raw_text&gt;"}</code> format, whereas fine-tuning expects the data to be in <code>{"prompt": "&lt;prompt text&gt;", "completion": "&lt;expected generated </code><code>text&gt;"}</code> format.</p>
<p>Here are some examples:</p>
<ul>
<li><code>{"input": "EBITDA stands for Earnings Before Interest, Tax, Depreciation </code><code>and Amortization"}</code></li>
<li><code>{"prompt": "What's EBITDA?", "completion": "Earnings Before Interest, Tax, Depreciation </code><code>and Amortization"}</code></li>
</ul>
<p>If your dataset comprises images, then you can fine-tune the text-to-image or image-to-embedding model using Titan Image Generator as the base model. At the time of writing, continued pre-training only supports text-to-text models, not image-generation models.</p>
<p>For image data, fine-tuning expects the data to be in <code>{"image-ref": "s3://path/file1.png", "caption": "caption </code><code>text"}</code> format.</p>
<p>Once you’ve prepared the data, you must split it into train and validation datasets and store it in <a id="_idIndexMarker325"/>an Amazon S3 bucket. Once you’ve done this, you can create a custom model.</p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor082"/>Creating a custom model</h1>
<p>To create a custom<a id="_idIndexMarker326"/> model via the AWS console, go to <strong class="bold">Custom models</strong> on the Amazon Bedrock console page (<a href="https://console.aws.amazon.com/bedrock/home">https://console.aws.amazon.com/bedrock/home</a>). <em class="italic">Figure 4</em><em class="italic">.8</em> shows what the <strong class="bold">Custom models</strong> page looks like. It provides information on how the customization process works, as well as two tabs called <strong class="bold">Models</strong> and <strong class="bold">Training jobs</strong>:</p>
<div><div><img alt="Figure 4.8 – The Bedrock console – Custom models" src="img/B22045_04_08.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – The Bedrock console – Custom models</p>
<p>Under <strong class="bold">Customize model</strong> in the <strong class="bold">Models</strong> tab, you can select <strong class="bold">Create Fine-tuning job</strong> or <strong class="bold">Create Continued Pre-training job</strong>. When you select either of these options, you can view details about <a id="_idIndexMarker327"/>the job, including its status, under the <strong class="bold">Training </strong><strong class="bold">jobs</strong> tab.</p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>Components of model customization</h2>
<p>The main components<a id="_idIndexMarker328"/> of model customization (fine-tuning or continued pre-training) include the source model, hyperparameters, and input data, as demonstrated in <em class="italic">Figure 4</em><em class="italic">.9</em>. These inputs are used to create a training job, which outputs the custom model alongside its metrics and logs:</p>
<div><div><img alt="Figure 4.9 – Components of customization job" src="img/B22045_04_09.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Components of customization job</p>
<p>Let’s learn more about these:</p>
<ul>
<li><strong class="bold">Source model</strong>: A key component of any customization job is selecting the source model that you wish to customize. You can find a list of all the supported models under the <strong class="bold">Model details</strong> section of the <strong class="bold">Create Fine-tuning job</strong> and <strong class="bold">Create Continued Pre-training job</strong> pages, as shown in <em class="italic">Figure 4</em><em class="italic">.10</em>:</li>
</ul>
<div><div><img alt="Figure 4.10 – Selecting a model for a customization job" src="img/B22045_04_10.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Selecting a model for a customization job</p>
<ul>
<li><strong class="bold">Hyperparameters</strong>: Along with the source models, you can specify a set of hyperparameters. These act like external knobs that control how the model is trained. These are different from inference parameters, which are set during the inference process.</li>
<li><strong class="bold">Input data</strong>: The dataset that is used to train the model is in JSONL format, and it’s prepared and <a id="_idIndexMarker329"/>stored in an Amazon S3 bucket.</li>
<li><strong class="bold">Training job</strong>: The inputs (source model, hyperparameters, and input data) are used to create a training job. There are other configuration details, such as VPC settings, which you can use to securely control access to the data in an Amazon S3 bucket, an IAM service role, which provides access to Bedrock to write to an S3 bucket, and model encryption, which you can use encrypt the custom model at rest using a KMS key. We will cover security and privacy in Amazon Bedrock in <a href="B22045_12.xhtml#_idTextAnchor226"><em class="italic">Chapter 12</em></a>.</li>
<li><strong class="bold">Custom model</strong>: Once the training process is completed, the custom model is stored in the AWS account owned by the AWS Bedrock Service team.</li>
<li><code>step_wise_training_metrics.csv</code> and <code>validation_metrics.csv</code> files inside the S3 output path. We will learn how to evaluate and analyze results in the <em class="italic">Analyzing the </em><em class="italic">results</em> section.</li>
</ul>
<p>For now, let’s look <a id="_idIndexMarker330"/>at the API calls we can use to create a custom model.</p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor084"/>APIs</h2>
<p>Amazon Bedrock <a id="_idIndexMarker331"/>provides several APIs that allow you to create, monitor, and<a id="_idIndexMarker332"/> stop customization jobs. This section will examine some of these key APIs: <strong class="bold">CreateModelCustomizationJob</strong>, <strong class="bold">ListModelCustomizationJob</strong>, <strong class="bold">GetModelCustomizationJob</strong>, and <strong class="bold">StopModelCustomizationJob</strong>.</p>
<p> Let’s dive deeper into each of these API calls:</p>
<ul>
<li><code>customizationType</code> to <code>FINE_TUNING</code> or <code>CONTINUED_PRE_TRAINING</code>, <code>baseModelIdentifier</code> as the source model you wish to use, relevant hyperparameters, and the input data (training and validation dataset). Here’s an example of the job being used in the Python SDK (Boto3):<pre class="source-code">
import boto3</pre><pre class="source-code">
import json</pre><pre class="source-code">
llm = boto3.client(service_name='bedrock')</pre><pre class="source-code">
# Setting customization type</pre><pre class="source-code">
customizationType = "FINE_TUNING"</pre><pre class="source-code">
# Creating customization job</pre><pre class="source-code">
llm.create_model_customization_job(</pre><pre class="source-code">
    jobName="fine-tuning-job",</pre><pre class="source-code">
    customModelName="fine-tuned model",</pre><pre class="source-code">
    roleArn="arn:aws:iam::arn-for-MyBedrockModelCustomizationRole",</pre><pre class="source-code">
    baseModelIdentifier="arn:aws:bedrock:us-east-1::foundation-model/foundation-model-id",</pre><pre class="source-code">
    hyperParameters={</pre><pre class="source-code">
        "epochCount": "1",</pre><pre class="source-code">
        "batchSize": "1",</pre><pre class="source-code">
        "learningRate": "0.007",</pre><pre class="source-code">
        "learningRateWarmupSteps": "0"</pre><pre class="source-code">
    },</pre><pre class="source-code">
    trainingDataConfig={"s3Uri": "s3://bucket/path/to/train.jsonl"},</pre><pre class="source-code">
    validationDataConfig={</pre><pre class="source-code">
        "validators": [{</pre><pre class="source-code">
            "s3Uri": "s3://bucket/folder/validation-file.jsonl"</pre><pre class="source-code">
        }]</pre><pre class="source-code">
    },</pre><pre class="source-code">
    outputDataConfig={"s3Uri": "s3://bucket/folder/outputdataconfig/"}</pre><pre class="source-code">
)</pre><p class="list-inset">Once you <a id="_idIndexMarker333"/>run<a id="_idIndexMarker334"/> the preceding code, the training job will start.</p></li>
<li><strong class="bold">ListModelCustomizationJob</strong>: You can use this API call to retrieve a list of all the customization jobs that you are running:<pre class="source-code">
import boto3</pre><pre class="source-code">
llm = boto3.client(service_name='bedrock')</pre><pre class="source-code">
llm.list_model_customization_jobs()</pre></li>
<li><code>IN_PROGRESS</code>, <code>STOPPED</code>, <code>FAILED</code>, or <code>COMPLETE</code>. If the model has a status of <code>FAILED</code>, you will <em class="italic">not</em> be charged:<pre class="source-code">
import boto3</pre><pre class="source-code">
llm = boto3.client(service_name='bedrock')</pre><pre class="source-code">
fine_tune_job = llm.get_model_customization_job(jobIdentifier='arn:aws:bedrock:job-arn-from-create-model-customization')</pre><pre class="source-code">
print(fine_tune_job['status'])</pre><p class="list-inset">Amazon Bedrock also has integration with Amazon EventBridge, where you can receive a notification whenever there is a status change. We will dive deeper into the EventBridge integration in <a href="B22045_11.xhtml#_idTextAnchor207"><em class="italic">Chapter 11</em></a>.</p></li>
<li><code>IN_PROGRESS</code>, and you would like to stop the job for any reason, you can run this API call:<pre class="source-code">
import boto3</pre><pre class="source-code">
llm = boto3.client(service_name='bedrock')</pre><pre class="source-code">
llm.stop_model_customization_job(jobIdentifier='arn:aws:bedrock:job-arn-from-create-model-customization')</pre></li>
</ul>
<p>Once you start the customization job, the time it takes to complete will vary depending on the size of the training dataset you provide. If your dataset contains a few thousand records, the training job can take about an hour, while if the dataset contains millions of records, the <a id="_idIndexMarker335"/>training<a id="_idIndexMarker336"/> job can take a few days to complete.</p>
<p>Once the customization job has been completed and a custom model has been created, we can analyze the <a id="_idIndexMarker337"/>results <a id="_idIndexMarker338"/>and perform inference on our model.</p>
<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/>Analyzing the results</h1>
<p>As mentioned <a id="_idIndexMarker339"/>previously, when creating a customization job, we provide an output S3 path, where the metrics and logs are stored by the training job. You will see the <code>step_wise_training_metrics.csv</code> and <code>validation_metrics.csv</code> files inside the S3 output path. Within these files, you will see information such as the step number, epoch number, loss, and perplexity. You will see these details in both the training and validation sets. Although providing a validation set is optional, doing so allows the performance metrics of the custom model that’s been created to be evaluated.</p>
<p>Depending on the size of the dataset, you can decide how much of the validation dataset you would like to hold. If your dataset is small (for example, it contains hundreds or thousands of records), you can use 90% as the training set and 10% as the validation set. If your dataset size is large (for example, it contains hundreds of thousands of records), you can reduce the validation set. So, if you have hundreds of thousands of records, you can use 99% of them as the training set and 1% as the validation set.</p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor086"/>Metrics for training and validation</h2>
<p>There are two<a id="_idIndexMarker340"/> key types of metrics that provide valuable insights into how well the model is learning: loss and perplexity. Let’s take a closer look:</p>
<ul>
<li><strong class="bold">Loss</strong>: This ranges from 0 to infinity. The loss value that’s calculated during training indicates how well the model fits the training data. Meanwhile, the validation loss shows how effectively the model generalizes to new, unseen examples after training is completed. Loss is one of the most commonly used metrics for evaluating the performance of a model during training. In general, lower loss values are preferable and indicate that the model is fitting the data well. Higher loss values suggest that the model’s prediction is far off from the actual response and it’s making a lot of errors.</li>
<li><strong class="bold">Perplexity</strong>: This ranges from 1 to infinity. It measures a language model’s ability to accurately predict the next token in a sequence. A lower perplexity score corresponds to better predictions and the model’s capabilities.</li>
</ul>
<p>Both loss and perplexity are important metrics for data scientists to analyze when training models with Bedrock. A well-performing training run will show the training and validation loss<a id="_idIndexMarker341"/> values converging over time. This convergence indicates that the model is learning from the training data without overfitting.</p>
<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Inference</h2>
<p>Once the job is successful<a id="_idIndexMarker342"/> and we’ve verified the training and validation metrics, we are ready to perform inference on our model. The first thing we need to do is purchase Provisioned Throughput, which gives us a dedicated capacity to deploy the model. At the time of writing, custom Bedrock models can only be deployed through Provisioned Throughput. However, you can also use Provisioned Throughput for base FMs supported by Bedrock.</p>
<p>At the time of writing, three commitment terms are available with Bedrock.</p>
<ul>
<li><strong class="bold">No commitment</strong> (priced hourly)</li>
<li><strong class="bold">1 month</strong></li>
<li><strong class="bold">6 months</strong>:</li>
</ul>
<div><div><img alt="Figure 4.11 – Model units &amp; commitment term" src="img/B22045_04_11.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Model units &amp; commitment term</p>
<p>The <code>1</code>. <strong class="bold">Model units</strong> are<a id="_idIndexMarker343"/> a way to define a throughput that’s measured in terms of the maximum number of input and output tokens processed per minute:</p>
<div><div><img alt="Figure 4.12 – Provisioned Throughput" src="img/B22045_04_12.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Provisioned Throughput</p>
<p>Once you’ve purchased Provisioned Throughput, you can see its details in the Bedrock console and via the <strong class="bold">ListProvisionedModelThroughputs</strong> and <strong class="bold">GetProvisionedModelThroughput</strong> APIs.</p>
<p>Once Provisioned Throughput has an <em class="italic">Active</em> status, the custom model that you’ve created will be<a id="_idIndexMarker344"/> deployed to an endpoint. At this point, you can perform inference on the model using either the playground experience or through an API. Both options will be discussed next.</p>
<h3>Amazon Bedrock playground</h3>
<p>Performing inference<a id="_idIndexMarker345"/> via the <a id="_idIndexMarker346"/>playground experience is pretty straightforward and similar to how you perform inference on base FMs.</p>
<p>Instead of using the base model, you can select the custom model that you’ve created, at which point you’re ready to ask questions or provide a prompt to your model. <em class="italic">Figure 4</em><em class="italic">.13</em> depicts the process of selecting the <strong class="bold">custom-titan-1705116361</strong> model from the Bedrock playground, where it can be fine-tuned on user-provisioned training data:</p>
<div><div><img alt="Figure 4.13 – Select model" src="img/B22045_04_13.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Select model</p>
<h3>Amazon Bedrock API</h3>
<p>Bedrock also <a id="_idIndexMarker347"/>provides the <code>modelId</code>, we should provide the <code>arn</code> model of the provisioned endpoint. You can attain this from the <strong class="bold">Bedrock Console – Provisioned Throughput</strong> tab or via the <strong class="bold">GetProvisionedModelThroughput</strong> API:</p>
<pre class="source-code">
Bedrock_runtime.invoke_model(
    modelId=arn-provisioned-throughput,
    body="""
{
  "inputText": "Classify this statement as Positive, Neutral, or Negative:\\n'I really do not like this!'",
  "textGenerationConfig":{
    "maxTokenCount": 1,
    "stopSequences": [],
    "temperature": 1,
    "topP": 0.9
  }
"""
)
response_body = response["body"].read().decode('utf8')
print(response_body)
print(json.loads(response_body)["results"][0]["outputText"])</pre>
<p>Now that we understand how to fine-tune models with Amazon Bedrock and leverage Provisioned Throughput, let’s learn how to import selective custom models.</p>
<h3>Importing custom models in Amazon Bedrock</h3>
<p>To leverage <a id="_idIndexMarker349"/>the <strong class="bold">Import Models</strong> capability <a id="_idIndexMarker350"/>within Amazon Bedrock, navigate to the Bedrock console. On the left-hand side panel, under <strong class="bold">Foundation models</strong>, click <strong class="bold">Imported models</strong>.</p>
<p>Once you land on the <strong class="bold">Imported models</strong> page, as shown in <em class="italic">Figure 4</em><em class="italic">.14</em>, you will be able to create a custom model by importing a model directly from Amazon SageMaker (where you might have customized FMs already) or by importing the model files from an Amazon S3 bucket:</p>
<div><div><img alt="Figure 4.14 – Imported models" src="img/B22045_04_14.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Imported models</p>
<p>At the time of writing, importing a model into Amazon Bedrock creates a custom model that supports the following patterns:</p>
<ul>
<li><strong class="bold">Continued pre-training or fine-tuned model</strong>: As explained previously, you can refine the pre-trained model by utilizing proprietary data while the maintaining structural integrity of the original model configuration.</li>
<li><strong class="bold">Domain adaptation</strong>: You can tailor the custom-imported model to a specific domain. This adaptation process will enhance the model’s performance within a target domain by addressing domain-specific variations. For instance, language adaptation can be undertaken so that responses can be generated in regional dialects or languages, such as Tamil or Portuguese.</li>
<li><strong class="bold">Pre-training from scratch</strong>: As you are aware by now, this approach extends beyond merely customizing weights and vocabulary. This approach provides you with the opportunity to modify fundamental model parameters, including the number of attention heads, hidden layers, or context length. Additionally, techniques such as post-training quantization or integrating base and adapter weights enable further refinement and optimization of the model’s architecture.</li>
</ul>
<p>To initiate the <strong class="bold">Import model</strong> job, you can provide the model details, including a relevant model name, import job name, and model import settings.</p>
<p>At the time of writing this book, the imported model can support the Mistral, Flan, Llama2, and Llama3 architectures. As the generative AI landscape evolves, Bedrock may expand the list of supported architectures for model import in the future.</p>
<p>Once a model import job has been completed successfully, the imported model will be listed on the <strong class="bold">Models</strong> tab of the <strong class="bold">Imported models</strong> page. Here, you can view key details about the imported model, such as its ARN, model ID, and status. From this page, you can also use the<a id="_idIndexMarker351"/> imported model for <a id="_idIndexMarker352"/>inference by invoking it through the Bedrock API.</p>
<p>Detailed information regarding the different model types and open source architectures that Amazon Bedrock’s custom model capability supports can be found at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html#model-customization-import-model-architecture">https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html#model-customization-import-model-architecture</a>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Please ensure that your account has sufficient quota limits to execute the <strong class="bold">CreateModelImportJob</strong> action. If it<a id="_idIndexMarker353"/> doesn’t, the following error will be displayed:</p>
<div><div><img alt="Figure 4.15 – Error" src="img/B22045_04_15.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Error</p>
<p>You can request for your quota limit to be increased by navigating to <a href="https://us-east-1.console.aws.amazon.com/servicequotas/home/services/bedrock/quotas">https://us-east-1.console.aws.amazon.com/servicequotas/home/services/bedrock/quotas</a>.</p>
<p>Throughout this chapter, we’ve learned how to prepare a dataset, customize an FM, and then check its performance and perform inference. Now, let’s look at some of the guidelines and best <a id="_idIndexMarker354"/>practices we need to consider <a id="_idIndexMarker355"/>while trying to customize a model.</p>
<h1 id="_idParaDest-87"><a id="_idTextAnchor088"/>Guidelines and best practices</h1>
<p>While customizing a <a id="_idIndexMarker356"/>model, it’s ideal to consider the following practices for optimal results:</p>
<ul>
<li><strong class="bold">Providing the dataset</strong>: The most important thing in ML is the dataset. Most of the time, how your model performs depends on the dataset you provide to train the model. So, providing quality data that’s aligned with your use case is important. If you’ve studied ML in university or worked in this field, you might have learned about various feature engineering and data processing techniques you can use to clean and process the data. For example, you can handle missing values in the dataset, make sure you don’t provide biased data, or ensure that the dataset follows the format that the model expects. If you would like to learn more about providing quality data, please read <em class="italic">Feature Engineering for Machine Learning</em> by Alice Zheng and Amanda Casari. This same principle applies to generative AI since it is essentially a subset of ML.</li>
<li><strong class="bold">Choosing the right FM</strong>: Next, you need to select the base FM that you are looking to customize. Make sure you look at its attributes, how many tokens it supports, what type of data it’s been trained on, and the size of the model. Go through the model cards in the Bedrock console, read through the websites of these models, and look at their performance by using standardized benchmarks such as GLUE, SuperGLUE, HELM, and OpenLLM by HuggingFace. However, keep in mind that you shouldn’t completely rely on these benchmark tools as they may not represent the complexity and diversity of real-world applications.</li>
<li><strong class="bold">Identifying hyperparameters</strong>: Once you have a quality dataset and the right base model has been selected, you need to identify the right hyperparameters for customization. Your goal should be to avoid overfitting; the model should be able to generalize well to the new unseen information. There are several hyperparameters that you can adjust, such as the number of epochs, batch size, learning rate, early stopping, and others. You can find a list of hyperparameters that all the Bedrock models support at <a href="https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html">https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models-hp.html</a>.</li>
<li><strong class="bold">Evaluating performance</strong>: Once you’ve fine-tuned the model, evaluate its performance using the validation dataset. The <strong class="bold">validation dataset</strong> is the dataset <a id="_idIndexMarker357"/>that’s held back from training the model and is used for evaluating it instead. To learn more about data splitting, go to <a href="https://mlu-explain.github.io/train-test-validation/">https://mlu-explain.github.io/train-test-validation/</a>. Here, you can look at different metrics, such as loss and perplexity, or use techniques such as accuracy, <strong class="bold">Bilingual Evaluation Understudy</strong> (<strong class="bold">BLEU</strong>), and <strong class="bold">Recall-Oriented Understudy for Gisting Evaluation</strong> (<strong class="bold">ROUGE</strong>) scores. For context, BLEU scores <a id="_idIndexMarker358"/>indicate the quality assessment <a id="_idIndexMarker359"/>of machine-generated translations compared to reference translations set provided by human translators. The ROUGE score is useful for text summarization tasks, wherein evaluation is conducted based on the quality of machine-generated summaries compared to the respective reference summaries created by humans.  If the model doesn’t provide the desired performance results, you have to readjust the hyperparameters or bring in more datasets. Once the model is ready to be used and provides the desired evaluation results, you can perform inference on the model.</li>
<li><strong class="bold">Adapting the model for specific domains</strong>: Customizing the model to a business domain is a promising approach for improving productivity and efficiency. By tailoring the model to the specific needs of a particular industry, we can enable it to perform tasks that were previously impossible or inefficient and create a more competitive and successful business.</li>
</ul>
<p>Adopting these <a id="_idIndexMarker360"/>practices can help you make the most of customizing an FM and harnessing the true power of generative AI.</p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor089"/>Summary</h1>
<p>In this chapter, we explored two model customization techniques, fine-tuning and continued pre-training, the need to customize a model, and understood the concepts behind fine-tuning and continued pre-training. Further, we prepared our dataset, created a custom model, evaluated the model, and performed inference.</p>
<p>Lastly, we discussed some of the guidelines and best practices you need to consider when customizing your FM.</p>
<p>In the next chapter, we’re going to uncover the power of RAG in solving real-world business problems by using an external data source. We will delve into the various use cases and sample architectures and implement RAG with Amazon Bedrock.</p>
</div>
</body></html>