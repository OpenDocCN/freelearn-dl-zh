<html><head></head><body>
		<div><h1 id="_idParaDest-215" class="chapter-number"><a id="_idTextAnchor357"/>18</h1>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor358"/>Applications – Object Editing and Style Transferring</h1>
			<p><strong class="bold">Stable Diffusion</strong> (<strong class="bold">SD</strong>) is not only capable of generating a variety of images but it can also be utilized for image editing and style transfer from one image to another. In this chapter, we will explore solutions for image editing and style transfer.</p>
			<p>Along the way, we will also introduce the tools that enable us to achieve these goals:<strong class="bold"> CLIPSeg</strong>, which<a id="_idIndexMarker544"/> is used to detect the content of <a id="_idIndexMarker545"/>an image; <strong class="bold">Rembg</strong>, which is a tool that flawlessly removes the background of an image; and <strong class="bold">IP-Adapter</strong>, which <a id="_idIndexMarker546"/>is used to transfer the style from one image to another.</p>
			<p>In this chapter, we are going to cover the following topics:</p>
			<ul>
				<li>Editing images using Stable Diffusion</li>
				<li>Object and style transferring</li>
			</ul>
			<p>Let’s star<a id="_idTextAnchor359"/>t.</p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor360"/>Editing images using Stable Diffusion</h1>
			<p>Do you recall the background <a id="_idIndexMarker547"/>swap example we discussed in <a href="B21263_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>? In this section, we will introduce a solution that can assist you in editing the content of an image.</p>
			<p>Before we can edit anything, we need to identify the boundary of the object we want to edit. In our case, to obtain the background mask, we will use the CLIPSeg [1] model. <strong class="bold">CLIPSeg</strong>, which <a id="_idIndexMarker548"/>stands for <strong class="bold">CLIP-based Image Segmentation</strong>, is a model trained to segment images based on text prompts or reference images. Unlike traditional segmentation models that require a large amount of labeled data, CLIPSeg can achieve impressive results with little to no training data.</p>
			<p>CLIPSeg builds <a id="_idIndexMarker549"/>upon the success of CLIP, the same model used by SD. CLIP is a powerful pre-trained model that learns to connect text and images. The CLIPSeg model adds a small decoder module on top of CLIP, allowing it to translate the learned relationships into pixel-level segmentation. This means we <a id="_idIndexMarker550"/>can provide CLIPSeg with a simple description such as “the background of this picture,” and CLIPSeg will return the mask of the targeted objects.</p>
			<p>Now, let’s see how we can use CLIPSeg to accomplish some <a id="_idTextAnchor361"/>tasks.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor362"/>Replacing image background content</h2>
			<p>We will first load up the <a id="_idIndexMarker551"/>CLIPSeg processor and model, then provide both the prompt and image to the model to generate the mask data, and finally, use the SD inpainting pipeline to redraw the background. Let’s do it step by step:</p>
			<ol>
				<li>Load the CLIPSeg model.<p class="list-inset">The following code will load up the <code>CLIPSegProcessor</code> processor and <code>CLIPSegForImageSegmentation</code> model:</p><pre class="source-code">
from transformers import( </pre><pre class="source-code">
    CLIPSegProcessor,CLIPSegForImageSegmentation)</pre><pre class="source-code">
processor = CLIPSegProcessor.from_pretrained(</pre><pre class="source-code">
    "CIDAS/clipseg-rd64-refined"</pre><pre class="source-code">
)</pre><pre class="source-code">
model = CLIPSegForImageSegmentation.from_pretrained(</pre><pre class="source-code">
    "CIDAS/clipseg-rd64-refined"</pre><pre class="source-code">
)</pre><p class="list-inset">The <code>processor</code> will be used to preprocess both the prompt and images input. The <code>model</code> will be the one responsible for model inference.</p></li>
				<li>Generate the grayscale mask.<p class="list-inset">By default, the CLIPSeg model will return logits of its result. By applying the <code>torch.sigmoid()</code> function, we can then have the grayscale mask of the target object in the image. The grayscale mask can then enable us to generate the binary mask, which<a id="_idIndexMarker552"/> will be used in the SD inpainting pipeline:</p><pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
from diffusers.utils.pil_utils import numpy_to_pil</pre><pre class="source-code">
import torch</pre><pre class="source-code">
source_image = load_image("./images/clipseg_source_image.png")</pre><pre class="source-code">
prompts = ['the background']</pre><pre class="source-code">
inputs = processor(</pre><pre class="source-code">
    text = prompts,</pre><pre class="source-code">
    images = [source_image] * len(prompts),</pre><pre class="source-code">
    padding = True,</pre><pre class="source-code">
    return_tensors = "pt"</pre><pre class="source-code">
)</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    outputs = model(**inputs)</pre><pre class="source-code">
preds = outputs.logits</pre><pre class="source-code">
mask_data = torch.sigmoid(preds)</pre><pre class="source-code">
mask_data_numpy = mask_data.detach().unsqueeze(-1).numpy()</pre><pre class="source-code">
mask_pil = numpy_to_pil(</pre><pre class="source-code">
    mask_data_numpy)[0].resize(source_image.size)</pre><p class="list-inset">The preceding code will <a id="_idIndexMarker553"/>generate a grayscale mask image that highlights the background, as shown in <em class="italic">Figure 18</em><em class="italic">.1</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_18_01.jpg" alt="Figure 18.1: Background grayscale mask"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.1: Background grayscale mask</p>
			<p class="list-inset">This mask is still not the one we want; we need a binary mask. Why do we need a binary mask? Because SD v1.5 inpainting works better with a binary mask than a grayscale mask. You may also add the grayscale mask to the SD pipeline to see the result; there’s nothing to lose by trying different combinations and inputs.</p>
			<ol>
				<li value="3">Generate a binary mask.<p class="list-inset">We will use the following code to convert a grayscale mask into a 0-1 binary mask image:</p><pre class="source-code">
bw_thresh = 100</pre><pre class="source-code">
bw_fn = lambda x : 255 if x &gt; bw_thresh else 0</pre><pre class="source-code">
bw_mask_pil = mask_pil.convert("L").point(bw_fn, mode="1")</pre><p class="list-inset">Let me explain the key elements we presented in the preceding code:</p><ul><li><code>bw_thresh</code>: This<a id="_idIndexMarker554"/> defines the threshold of treating a pixel as black or white. In the preceding code, any grayscale pixel value higher than 100 will be treated as a white highlight.</li><li><code>mask_pil.convert("L")</code>: This converts the <code>mask_pil</code> image into grayscale mode. Grayscale images have only one channel, representing pixel intensity values from 0 (black) to 255 (white).</li><li><code>.point(bw_fn, mode="1")</code>: This applies the <code>bw_fn</code> thresholding function to each pixel of the grayscale image. The <code>mode="1"</code> argument ensures that the output image is a 1-bit binary image (black and white only).</li></ul><p class="list-inset">We will see the result shown in <em class="italic">Figure 18</em><em class="italic">.2</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_18_02.jpg" alt="Figure 18.2: Background binary mask"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.2: Background binary mask</p>
			<ol>
				<li value="4">Redraw the <a id="_idIndexMarker555"/>background using the SD inpainting model:<pre class="source-code">
from diffusers import(StableDiffusionInpaintPipeline, </pre><pre class="source-code">
    EulerDiscreteScheduler)</pre><pre class="source-code">
inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(</pre><pre class="source-code">
    "CompVis/stable-diffusion-v1-4",</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    safety_checker = None</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
sd_prompt = "blue sky and mountains"</pre><pre class="source-code">
out_image = inpaint_pipe(</pre><pre class="source-code">
    prompt = sd_prompt,</pre><pre class="source-code">
    image = source_image,</pre><pre class="source-code">
    mask_image = bw_mask_pil,</pre><pre class="source-code">
    strength = 0.9,</pre><pre class="source-code">
    generator = torch.Generator("cuda:0").manual_seed(7)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
out_image</pre></li>
			</ol>
			<p>In the preceding code, we <a id="_idIndexMarker556"/>use the SD v1.4 model as the inpainting model because it generates better results than the SD v1.5 model. If you execute it, you will see the exact result we presented in <a href="B21263_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>. The background is now no longer a vast planetary universe but blue sky and mountains.</p>
			<p>The same technique can be used for many other purposes, such as editing clothing in a photo and adding items t<a id="_idTextAnchor363"/>o a photo.</p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor364"/>Removing the image background</h2>
			<p>Many times, we want to just remove the<a id="_idIndexMarker557"/> background of an image. With the binary mask in hand, removing the background isn’t hard at all. We can do it using the following code:</p>
			<pre class="source-code">
from PIL import Image, ImageOps
output_image = Image.new("RGBA", source_image.size,
    (255,255,255,255))
inverse_bw_mask_pil = ImageOps.invert(bw_mask_pil)
r = Image.composite(source_image ,output_image,
    inverse_bw_mask_pil)</pre>
			<p>Here’s a breakdown of what each line does:</p>
			<ul>
				<li><code>from PIL import Image, ImageOps</code>: This line imports the <code>Image</code> and <code>ImageOps</code> modules from PIL. The <code>Image</code> module provides a class with the same name that is used<a id="_idIndexMarker558"/> to represent a PIL image. The <code>ImageOps</code> module contains a number of “ready-made” image-processing operations.</li>
				<li><code>output_image = Image.new("RGBA", source_image.size, (255,255,255,255))</code>: This line creates a new image with the same size as <code>source_image</code>. The new image will be in RGBA mode, meaning it includes channels for red, green, blue, and alpha (transparency). The initial color of all pixels in the image is set to white <code>(255,255,255)</code> with full opacity <code>(255)</code>.</li>
				<li><code>inverse_bw_mask_pil = ImageOps.invert(bw_mask_pil)</code>: This line inverts the colors of the <code>bw_mask_pil</code> image using the <code>invert</code> function from ImageOps. If <code>bw_mask_pil</code> is a black and white image, the result will be a negative of the original image, that is, black becomes white and white becomes black.</li>
				<li><code>r = Image.composite(source_image ,output_image, inverse_bw_mask_pil)</code>: This line creates a new image by blending <code>source_image</code> and <code>output_image</code> based on the <code>inverse_bw_mask_pil</code> mask image. Where the mask image is white (or shades of gray), the corresponding pixels from <code>source_image</code> are used, and where the mask image is black, the corresponding pixels from <code>output_image</code> are used. The result is assigned to <code>r</code>.</li>
			</ul>
			<p>Simply four lines of code enable the replacement of the background with pure white, as shown in <em class="italic">Figure 18</em><em class="italic">.3</em>:</p>
			<div><div><img src="img/B21263_18_03.jpg" alt="Figure 18.3: Remove background using CLIPSeg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.3: Remove background using CLIPSeg</p>
			<p>But, we will see jagged edges; this is<a id="_idIndexMarker559"/> not good and can’t be perfectly solved using CLIPSeg. If you are going to feed this image into the diffusion pipeline again, SD will help fix the jagged edges problem by using another image-to-image pipeline. Based on the nature of the diffusion model, the background edges will be either blurred or rerendered with other pixels. To remove the background neatly, we will need other tools to help, for example, the Rembg project [2]. Its usage is also simple:</p>
			<ol>
				<li>Install the package:<pre class="source-code">
pip install rembg</pre></li>
				<li>Remove the background with two lines of code:<pre class="source-code">
from rembg import remove</pre><pre class="source-code">
remove(source_image)</pre><p class="list-inset">And we see the background<a id="_idIndexMarker560"/> is completely removed, as shown in <em class="italic">Figure 18</em><em class="italic">.4</em>:</p></li>
			</ol>
			<div><div><img src="img/B21263_18_04.jpg" alt="Figure 18.4: Remove background using Rembg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.4: Remove background using Rembg</p>
			<p>To set the background as white, use three more lines of code, as shown below:</p>
			<pre class="source-code">
from rembg import remove
from PIL import Image
white_bg = Image.new("RGBA", source_image.size, (255,255,255))
image_wo_bg = remove(source_image)
Image.alpha_composite(white_bg, image_wo_bg)</pre>
			<p>We can find the background is completely replaced with a white background. An object with a pure white background can be useful in some cases; for instance, we are going to use the object as a guidance<a id="_idIndexMarker561"/> embedding. No, you did not read that wrong; we can use the image as the input prompt. Let’s explore this in the <a id="_idTextAnchor365"/>next section.</p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor366"/>Object and style transferring</h1>
			<p>When we introduced the theory behind SD in <em class="italic">Chapters 4</em> and <em class="italic">5</em>, we learned that only text embedding is involved in the UNet diffusion process. Even if we provide an initial image as the starting point, the initial image is simply used as the starting noise or concatenated with initial noises. It does not have any influence on the steps of the diffusion process.</p>
			<p>That is until the IP-Adapter<a id="_idIndexMarker562"/> project [3] came about. IP-Adapter is a tool that lets you use an existing image as a reference for text prompts. In other words, we can take the image as another piece of prompt work together with text guidance to generate an image. Unlike Textual Inversion, which usually works well for certain concepts or styles, IP-Adapter works with any images.</p>
			<p>With the help of IP-Adapter, we can magically transfer an object from one image to a completely different one.</p>
			<p>Next, let’s start using IP-Adapter to transfer an object from one image t<a id="_idTextAnchor367"/>o another one.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor368"/>Loading up a Stable Diffusion pipeline with IP-Adapter</h2>
			<p>Using <a id="_idIndexMarker563"/>IP-Adapter in Diffusers is simple enough, you don’t need <a id="_idIndexMarker564"/>to install any additional packages or manually download any model files:</p>
			<ol>
				<li>Load the image encoder. It is this dedicated image encoder that plays a key role in turning the image into a guidance prompt embedding:<pre class="source-code">
import torch</pre><pre class="source-code">
from transformers import CLIPVisionModelWithProjection</pre><pre class="source-code">
image_encoder = CLIPVisionModelWithProjection.from_pretrained(</pre><pre class="source-code">
    "h94/IP-Adapter",</pre><pre class="source-code">
    subfolder = "models/image_encoder",</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
).to("cuda:0")</pre></li>
				<li>Load a vanilla SD pipeline but with one additional <code>image_encoder</code> parameter:<pre class="source-code">
from diffusers import StableDiffusionImg2ImgPipeline</pre><pre class="source-code">
pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    image_encoder = image_encoder,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    safety_checker = None</pre><pre class="source-code">
).to("cuda:0")</pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">We will use the image encoder model from <code>models/image_encoder</code> even when loading an SDXL pipeline, rather than <code>sdxl_models/image_encoder</code>; otherwise, an error message will be thrown. You can also replace the SD v1.5 base model with any other community-shared models.</p>
			<ol>
				<li value="3">Apply<a id="_idIndexMarker565"/> IP-Adapter to the UNet <a id="_idIndexMarker566"/>pipeline:<pre class="source-code">
pipeline.load_ip_adapter(</pre><pre class="source-code">
    "h94/IP-Adapter",</pre><pre class="source-code">
    Subfolder = "models",</pre><pre class="source-code">
    weight_name = "ip-adapter_sd15.bin"</pre><pre class="source-code">
)</pre><p class="list-inset">If you are using an SDXL pipeline, replace <code>models</code> with <code>sdxl_models</code>, and replace <code>ip-adapter_sd15.bin</code> with <code>ip-adapter_sdxl.bin</code>.</p></li>
			</ol>
			<p>That is all; now we <a id="_idIndexMarker567"/>can use the pipeline just like any other<a id="_idIndexMarker568"/> pipeline. Diffusers will help you download the model files automatically if no IP-Adapter models exist. In the next section, we are going to use the IP-Adapter model to transfer a style from o<a id="_idTextAnchor369"/>ne image to another.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor370"/>Transferring style</h2>
			<p>In this section, we are<a id="_idIndexMarker569"/> going to write code to transfer the famous <em class="italic">Girl with a Pearl Earring</em> by Johannes Vermeer (see <em class="italic">Figure 18</em><em class="italic">.5</em>) to the <em class="italic">astronaut riding a </em><em class="italic">horse</em> image:</p>
			<div><div><img src="img/B21263_18_05.jpg" alt="Figure 18.5: Girl with a Pearl Earring by Johannes Vermeer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.5: Girl with a Pearl Earring by Johannes Vermeer</p>
			<p>Here, let’s kick off the pipeline to<a id="_idIndexMarker570"/> transfer style:</p>
			<pre class="source-code">
from diffusers.utils import load_image
source_image = load_image("./images/clipseg_source_image.png")
ip_image = load_image("./images/vermeer.png")
pipeline.to("cuda:0")
image = pipeline(
    prompt = 'best quality, high quality',
    negative_prompt = "monochrome,lowres, bad anatomy,low quality" ,
    image = source_image,
    ip_adapter_image = ip_image ,
    num_images_per_prompt = 1 ,
    num_inference_steps = 50,
    strength = 0.5,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
pipeline.to("cpu")
torch.cuda.empty_cache()
image</pre>
			<p>In the preceding code, we used the<a id="_idIndexMarker571"/> original astronaut image – <code>source_image</code> – as the base, and the oil painting image as the IP-Adapter image prompt – <code>ip_image</code> (we want its style). Amazingly, we get the result shown in <em class="italic">Figure 18</em><em class="italic">.6</em>:</p>
			<div><div><img src="img/B21263_18_06.jpg" alt="Figure 18.6: Astronaut riding a horse with a new style"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.6: Astronaut riding a horse with a new style</p>
			<p>The style and <a id="_idIndexMarker572"/>feel of the <em class="italic">Girl with a Pearl Earring</em> image have successfully been applied to another image.</p>
			<p>IP-Adapter’s potential is huge. We can even transfer the clothing and face from one image to another. More usage samples can be found in the original IP-Adapter repository [3] and the <a id="_idTextAnchor371"/>Diffusers PR page [5].</p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor372"/>Summary</h1>
			<p>In this chapter, the focus was on using SD for image editing and style transferring. The chapter introduced tools such as CLIPSeg for image content detection, Rembg for background removal, and IP-Adapter for transferring styles between images.</p>
			<p>The first section covered image editing, specifically replacing or removing the background. CLIPSeg is used to generate a mask of the background, which is then converted to a binary mask. The background is either replaced using SD or removed, with the latter option showing jagged edges. Rembg was introduced as a solution for smoother background removal.</p>
			<p>The second section explored object and style transferring using IP-Adapter. The process involves loading an image encoder, incorporating it into an SD pipeline, and applying IP-Adapter to the UNet of the pipeline. The chapter concluded with an example of transferring the style of Vermeer’s <em class="italic">Girl with a Pearl Earring</em> onto an image of an astronaut riding a horse.</p>
			<p>In the next chapter, we are going to explore solutions to save and read the parameters and prompt information to and from the <a id="_idTextAnchor373"/>generated image files.</p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor374"/>References</h1>
			<ol>
				<li>CLIPSeg GitHub repository: <a href="https://github.com/timojl/clipseg">https://github.com/timojl/clipseg</a></li>
				<li>Timo Lüddecke and Alexander S. Ecker, <em class="italic">Image Segmentation Using Text and Image </em><em class="italic">Prompts</em>: <a href="https://arxiv.org/abs/2112.10003">https://arxiv.org/abs/2112.10003</a></li>
				<li>IP-Adapter GitHub repository: <a href="https://github.com/tencent-ailab/IP-Adapter">https://github.com/tencent-ailab/IP-Adapter</a></li>
				<li>Rembg, a tool to remove image backgrounds: <a href="https://github.com/danielgatis/rembg">https://github.com/danielgatis/rembg</a></li>
				<li>IP-Adapters original samples: <a href="https://github.com/huggingface/diffusers/pull/5713">https://github.com/huggingface/diffusers/pull/5713</a></li>
			</ol>
		</div>
	</body></html>