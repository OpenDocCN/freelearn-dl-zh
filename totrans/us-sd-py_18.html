<html><head></head><body>
		<div id="_idContainer137">
			<h1 id="_idParaDest-215" class="chapter-number"><a id="_idTextAnchor357"/>18</h1>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor358"/>Applications – Object Editing and Style Transferring</h1>
			<p><strong class="bold">Stable Diffusion</strong> (<strong class="bold">SD</strong>) is not only capable of generating a variety of images but it can also be utilized for image editing and style transfer from one image to another. In this chapter, we will explore solutions for image editing and <span class="No-Break">style transfer.</span></p>
			<p>Along the way, we will also introduce the tools that enable us to achieve these goals:<strong class="bold"> CLIPSeg</strong>, which<a id="_idIndexMarker544"/> is used to detect the content of <a id="_idIndexMarker545"/>an image; <strong class="bold">Rembg</strong>, which is a tool that flawlessly removes the background of an image; and <strong class="bold">IP-Adapter</strong>, which <a id="_idIndexMarker546"/>is used to transfer the style from one image <span class="No-Break">to another.</span></p>
			<p>In this chapter, we are going to cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Editing images using <span class="No-Break">Stable Diffusion</span></li>
				<li>Object and <span class="No-Break">style transferring</span></li>
			</ul>
			<p><span class="No-Break">Let’s star<a id="_idTextAnchor359"/>t.</span></p>
			<h1 id="_idParaDest-217"><a id="_idTextAnchor360"/>Editing images using Stable Diffusion</h1>
			<p>Do you recall the background <a id="_idIndexMarker547"/>swap example we discussed in <a href="B21263_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>? In this section, we will introduce a solution that can assist you in editing the content of <span class="No-Break">an image.</span></p>
			<p>Before we can edit anything, we need to identify the boundary of the object we want to edit. In our case, to obtain the background mask, we will use the CLIPSeg [1] model. <strong class="bold">CLIPSeg</strong>, which <a id="_idIndexMarker548"/>stands for <strong class="bold">CLIP-based Image Segmentation</strong>, is a model trained to segment images based on text prompts or reference images. Unlike traditional segmentation models that require a large amount of labeled data, CLIPSeg can achieve impressive results with little to no <span class="No-Break">training data.</span></p>
			<p>CLIPSeg builds <a id="_idIndexMarker549"/>upon the success of CLIP, the same model used by SD. CLIP is a powerful pre-trained model that learns to connect text and images. The CLIPSeg model adds a small decoder module on top of CLIP, allowing it to translate the learned relationships into pixel-level segmentation. This means we <a id="_idIndexMarker550"/>can provide CLIPSeg with a simple description such as “the background of this picture,” and CLIPSeg will return the mask of the <span class="No-Break">targeted objects.</span></p>
			<p>Now, let’s see how we can use CLIPSeg to accomplish <span class="No-Break">some <a id="_idTextAnchor361"/>tasks.</span></p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor362"/>Replacing image background content</h2>
			<p>We will first load up the <a id="_idIndexMarker551"/>CLIPSeg processor and model, then provide both the prompt and image to the model to generate the mask data, and finally, use the SD inpainting pipeline to redraw the background. Let’s do it step <span class="No-Break">by step:</span></p>
			<ol>
				<li>Load the <span class="No-Break">CLIPSeg model.</span><p class="list-inset">The following code will load up the <strong class="source-inline">CLIPSegProcessor</strong> processor and <span class="No-Break"><strong class="source-inline">CLIPSegForImageSegmentation</strong></span><span class="No-Break"> model</span><span class="No-Break">:</span></p><pre class="source-code">
from transformers import( </pre><pre class="source-code">
    CLIPSegProcessor,CLIPSegForImageSegmentation)</pre><pre class="source-code">
processor = CLIPSegProcessor.from_pretrained(</pre><pre class="source-code">
    "CIDAS/clipseg-rd64-refined"</pre><pre class="source-code">
)</pre><pre class="source-code">
model = CLIPSegForImageSegmentation.from_pretrained(</pre><pre class="source-code">
    "CIDAS/clipseg-rd64-refined"</pre><pre class="source-code">
)</pre><p class="list-inset">The <strong class="source-inline">processor</strong> will be used to preprocess both the prompt and images input. The <strong class="source-inline">model</strong> will be the one responsible for <span class="No-Break">model inference</span><span class="No-Break">.</span></p></li>
				<li>Generate the <span class="No-Break">grayscale mask.</span><p class="list-inset">By default, the CLIPSeg model will return logits of its result. By applying the <strong class="source-inline">torch.sigmoid()</strong> function, we can then have the grayscale mask of the target object in the image. The grayscale mask can then enable us to generate the binary mask, which<a id="_idIndexMarker552"/> will be used in the SD <span class="No-Break">inpainting pipeline:</span></p><pre class="source-code">
from diffusers.utils import load_image</pre><pre class="source-code">
from diffusers.utils.pil_utils import numpy_to_pil</pre><pre class="source-code">
import torch</pre><pre class="source-code">
source_image = load_image("./images/clipseg_source_image.png")</pre><pre class="source-code">
prompts = ['the background']</pre><pre class="source-code">
inputs = processor(</pre><pre class="source-code">
    text = prompts,</pre><pre class="source-code">
    images = [source_image] * len(prompts),</pre><pre class="source-code">
    padding = True,</pre><pre class="source-code">
    return_tensors = "pt"</pre><pre class="source-code">
)</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    outputs = model(**inputs)</pre><pre class="source-code">
preds = outputs.logits</pre><pre class="source-code">
mask_data = torch.sigmoid(preds)</pre><pre class="source-code">
mask_data_numpy = mask_data.detach().unsqueeze(-1).numpy()</pre><pre class="source-code">
mask_pil = numpy_to_pil(</pre><pre class="source-code">
    mask_data_numpy)[0].resize(source_image.size)</pre><p class="list-inset">The preceding code will <a id="_idIndexMarker553"/>generate a grayscale mask image that highlights the background, as shown in <span class="No-Break"><em class="italic">Figure 18</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B21263_18_01.jpg" alt="Figure 18.1: Background grayscale mask"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.1: Background grayscale mask</p>
			<p class="list-inset">This mask is still not the one we want; we need a binary mask. Why do we need a binary mask? Because SD v1.5 inpainting works better with a binary mask than a grayscale mask. You may also add the grayscale mask to the SD pipeline to see the result; there’s nothing to lose by trying different combinations <span class="No-Break">and inputs.</span></p>
			<ol>
				<li value="3">Generate a <span class="No-Break">binary mask.</span><p class="list-inset">We will use the following code to convert a grayscale mask into a 0-1 binary <span class="No-Break">mask image:</span></p><pre class="source-code">
bw_thresh = 100</pre><pre class="source-code">
bw_fn = lambda x : 255 if x &gt; bw_thresh else 0</pre><pre class="source-code">
bw_mask_pil = mask_pil.convert("L").point(bw_fn, mode="1")</pre><p class="list-inset">Let me explain the key elements we presented in the <span class="No-Break">preceding code:</span></p><ul><li><strong class="source-inline">bw_thresh</strong>: This<a id="_idIndexMarker554"/> defines the threshold of treating a pixel as black or white. In the preceding code, any grayscale pixel value higher than 100 will be treated as a <span class="No-Break">white highlight.</span></li><li><strong class="source-inline">mask_pil.convert("L")</strong>: This converts the <strong class="source-inline">mask_pil</strong> image into grayscale mode. Grayscale images have only one channel, representing pixel intensity values from 0 (black) to <span class="No-Break">255 (white).</span></li><li><strong class="source-inline">.point(bw_fn, mode="1")</strong>: This applies the <strong class="source-inline">bw_fn</strong> thresholding function to each pixel of the grayscale image. The <strong class="source-inline">mode="1"</strong> argument ensures that the output image is a 1-bit binary image (black and <span class="No-Break">white only).</span></li></ul><p class="list-inset">We will see the result shown in <span class="No-Break"><em class="italic">Figure 18</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B21263_18_02.jpg" alt="Figure 18.2: Background binary mask"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.2: Background binary mask</p>
			<ol>
				<li value="4">Redraw the <a id="_idIndexMarker555"/>background using the SD <span class="No-Break">inpainting model:</span><pre class="source-code">
from diffusers import(StableDiffusionInpaintPipeline, </pre><pre class="source-code">
    EulerDiscreteScheduler)</pre><pre class="source-code">
inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(</pre><pre class="source-code">
    "CompVis/stable-diffusion-v1-4",</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    safety_checker = None</pre><pre class="source-code">
).to("cuda:0")</pre><pre class="source-code">
sd_prompt = "blue sky and mountains"</pre><pre class="source-code">
out_image = inpaint_pipe(</pre><pre class="source-code">
    prompt = sd_prompt,</pre><pre class="source-code">
    image = source_image,</pre><pre class="source-code">
    mask_image = bw_mask_pil,</pre><pre class="source-code">
    strength = 0.9,</pre><pre class="source-code">
    generator = torch.Generator("cuda:0").manual_seed(7)</pre><pre class="source-code">
).images[0]</pre><pre class="source-code">
out_image</pre></li>
			</ol>
			<p>In the preceding code, we <a id="_idIndexMarker556"/>use the SD v1.4 model as the inpainting model because it generates better results than the SD v1.5 model. If you execute it, you will see the exact result we presented in <a href="B21263_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. The background is now no longer a vast planetary universe but blue sky <span class="No-Break">and mountains.</span></p>
			<p>The same technique can be used for many other purposes, such as editing clothing in a photo and adding items t<a id="_idTextAnchor363"/>o <span class="No-Break">a photo.</span></p>
			<h2 id="_idParaDest-219"><a id="_idTextAnchor364"/>Removing the image background</h2>
			<p>Many times, we want to just remove the<a id="_idIndexMarker557"/> background of an image. With the binary mask in hand, removing the background isn’t hard at all. We can do it using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
from PIL import Image, ImageOps
output_image = Image.new("RGBA", source_image.size,
    (255,255,255,255))
inverse_bw_mask_pil = ImageOps.invert(bw_mask_pil)
r = Image.composite(source_image ,output_image,
    inverse_bw_mask_pil)</pre>
			<p>Here’s a breakdown of what each <span class="No-Break">line does:</span></p>
			<ul>
				<li><strong class="source-inline">from PIL import Image, ImageOps</strong>: This line imports the <strong class="source-inline">Image</strong> and <strong class="source-inline">ImageOps</strong> modules from PIL. The <strong class="source-inline">Image</strong> module provides a class with the same name that is used<a id="_idIndexMarker558"/> to represent a PIL image. The <strong class="source-inline">ImageOps</strong> module contains a number of “ready-made” <span class="No-Break">image-processing operations.</span></li>
				<li><strong class="source-inline">output_image = Image.new("RGBA", source_image.size, (255,255,255,255))</strong>: This line creates a new image with the same size as <strong class="source-inline">source_image</strong>. The new image will be in RGBA mode, meaning it includes channels for red, green, blue, and alpha (transparency). The initial color of all pixels in the image is set to white <strong class="source-inline">(255,255,255)</strong> with full <span class="No-Break">opacity </span><span class="No-Break"><strong class="source-inline">(255)</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">inverse_bw_mask_pil = ImageOps.invert(bw_mask_pil)</strong>: This line inverts the colors of the <strong class="source-inline">bw_mask_pil</strong> image using the <strong class="source-inline">invert</strong> function from ImageOps. If <strong class="source-inline">bw_mask_pil</strong> is a black and white image, the result will be a negative of the original image, that is, black becomes white and white <span class="No-Break">becomes black.</span></li>
				<li><strong class="source-inline">r = Image.composite(source_image ,output_image, inverse_bw_mask_pil)</strong>: This line creates a new image by blending <strong class="source-inline">source_image</strong> and <strong class="source-inline">output_image</strong> based on the <strong class="source-inline">inverse_bw_mask_pil</strong> mask image. Where the mask image is white (or shades of gray), the corresponding pixels from <strong class="source-inline">source_image</strong> are used, and where the mask image is black, the corresponding pixels from <strong class="source-inline">output_image</strong> are used. The result is assigned <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">r</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>Simply four lines of code enable the replacement of the background with pure white, as shown in <span class="No-Break"><em class="italic">Figure 18</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B21263_18_03.jpg" alt="Figure 18.3: Remove background using CLIPSeg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.3: Remove background using CLIPSeg</p>
			<p>But, we will see jagged edges; this is<a id="_idIndexMarker559"/> not good and can’t be perfectly solved using CLIPSeg. If you are going to feed this image into the diffusion pipeline again, SD will help fix the jagged edges problem by using another image-to-image pipeline. Based on the nature of the diffusion model, the background edges will be either blurred or rerendered with other pixels. To remove the background neatly, we will need other tools to help, for example, the Rembg project [2]. Its usage is <span class="No-Break">also simple:</span></p>
			<ol>
				<li>Install <span class="No-Break">the package:</span><pre class="source-code">
pip install rembg</pre></li>
				<li>Remove the background with two lines <span class="No-Break">of code:</span><pre class="source-code">
from rembg import remove</pre><pre class="source-code">
remove(source_image)</pre><p class="list-inset">And we see the background<a id="_idIndexMarker560"/> is completely removed, as shown in <span class="No-Break"><em class="italic">Figure 18</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B21263_18_04.jpg" alt="Figure 18.4: Remove background using Rembg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.4: Remove background using Rembg</p>
			<p>To set the background as white, use three more lines of code, as <span class="No-Break">shown below:</span></p>
			<pre class="source-code">
from rembg import remove
from PIL import Image
white_bg = Image.new("RGBA", source_image.size, (255,255,255))
image_wo_bg = remove(source_image)
Image.alpha_composite(white_bg, image_wo_bg)</pre>
			<p>We can find the background is completely replaced with a white background. An object with a pure white background can be useful in some cases; for instance, we are going to use the object as a guidance<a id="_idIndexMarker561"/> embedding. No, you did not read that wrong; we can use the image as the input prompt. Let’s explore this in the <a id="_idTextAnchor365"/><span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor366"/>Object and style transferring</h1>
			<p>When we introduced the theory behind SD in <em class="italic">Chapters 4</em> and <em class="italic">5</em>, we learned that only text embedding is involved in the UNet diffusion process. Even if we provide an initial image as the starting point, the initial image is simply used as the starting noise or concatenated with initial noises. It does not have any influence on the steps of the <span class="No-Break">diffusion process.</span></p>
			<p>That is until the IP-Adapter<a id="_idIndexMarker562"/> project [3] came about. IP-Adapter is a tool that lets you use an existing image as a reference for text prompts. In other words, we can take the image as another piece of prompt work together with text guidance to generate an image. Unlike Textual Inversion, which usually works well for certain concepts or styles, IP-Adapter works with <span class="No-Break">any images.</span></p>
			<p>With the help of IP-Adapter, we can magically transfer an object from one image to a completely <span class="No-Break">different one.</span></p>
			<p>Next, let’s start using IP-Adapter to transfer an object from one image t<a id="_idTextAnchor367"/>o <span class="No-Break">another one.</span></p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor368"/>Loading up a Stable Diffusion pipeline with IP-Adapter</h2>
			<p>Using <a id="_idIndexMarker563"/>IP-Adapter in Diffusers is simple enough, you don’t need <a id="_idIndexMarker564"/>to install any additional packages or manually download any <span class="No-Break">model files:</span></p>
			<ol>
				<li>Load the image encoder. It is this dedicated image encoder that plays a key role in turning the image into a guidance <span class="No-Break">prompt embedding:</span><pre class="source-code">
import torch</pre><pre class="source-code">
from transformers import CLIPVisionModelWithProjection</pre><pre class="source-code">
image_encoder = CLIPVisionModelWithProjection.from_pretrained(</pre><pre class="source-code">
    "h94/IP-Adapter",</pre><pre class="source-code">
    subfolder = "models/image_encoder",</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
).to("cuda:0")</pre></li>
				<li>Load a vanilla SD pipeline but with one additional <span class="No-Break"><strong class="source-inline">image_encoder</strong></span><span class="No-Break"> parameter:</span><pre class="source-code">
from diffusers import StableDiffusionImg2ImgPipeline</pre><pre class="source-code">
pipeline = StableDiffusionImg2ImgPipeline.from_pretrained(</pre><pre class="source-code">
    "runwayml/stable-diffusion-v1-5",</pre><pre class="source-code">
    image_encoder = image_encoder,</pre><pre class="source-code">
    torch_dtype = torch.float16,</pre><pre class="source-code">
    safety_checker = None</pre><pre class="source-code">
).to("cuda:0")</pre></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">We will use the image encoder model from <strong class="source-inline">models/image_encoder</strong> even when loading an SDXL pipeline, rather than <strong class="source-inline">sdxl_models/image_encoder</strong>; otherwise, an error message will be thrown. You can also replace the SD v1.5 base model with any other <span class="No-Break">community-shared models.</span></p>
			<ol>
				<li value="3">Apply<a id="_idIndexMarker565"/> IP-Adapter to the <span class="No-Break">UNet </span><span class="No-Break"><a id="_idIndexMarker566"/></span><span class="No-Break">pipeline:</span><pre class="source-code">
pipeline.load_ip_adapter(</pre><pre class="source-code">
    "h94/IP-Adapter",</pre><pre class="source-code">
    Subfolder = "models",</pre><pre class="source-code">
    weight_name = "ip-adapter_sd15.bin"</pre><pre class="source-code">
)</pre><p class="list-inset">If you are using an SDXL pipeline, replace <strong class="source-inline">models</strong> with <strong class="source-inline">sdxl_models</strong>, and replace <strong class="source-inline">ip-adapter_sd15.bin</strong> <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">ip-adapter_sdxl.bin</strong></span><span class="No-Break">.</span></p></li>
			</ol>
			<p>That is all; now we <a id="_idIndexMarker567"/>can use the pipeline just like any other<a id="_idIndexMarker568"/> pipeline. Diffusers will help you download the model files automatically if no IP-Adapter models exist. In the next section, we are going to use the IP-Adapter model to transfer a style from o<a id="_idTextAnchor369"/>ne image <span class="No-Break">to another.</span></p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor370"/>Transferring style</h2>
			<p>In this section, we are<a id="_idIndexMarker569"/> going to write code to transfer the famous <em class="italic">Girl with a Pearl Earring</em> by Johannes Vermeer (see <span class="No-Break"><em class="italic">Figure 18</em></span><em class="italic">.5</em>) to the <em class="italic">astronaut riding a </em><span class="No-Break"><em class="italic">horse</em></span><span class="No-Break"> image:</span></p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B21263_18_05.jpg" alt="Figure 18.5: Girl with a Pearl Earring by Johannes Vermeer"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.5: Girl with a Pearl Earring by Johannes Vermeer</p>
			<p>Here, let’s kick off the pipeline to<a id="_idIndexMarker570"/> <span class="No-Break">transfer style:</span></p>
			<pre class="source-code">
from diffusers.utils import load_image
source_image = load_image("./images/clipseg_source_image.png")
ip_image = load_image("./images/vermeer.png")
pipeline.to("cuda:0")
image = pipeline(
    prompt = 'best quality, high quality',
    negative_prompt = "monochrome,lowres, bad anatomy,low quality" ,
    image = source_image,
    ip_adapter_image = ip_image ,
    num_images_per_prompt = 1 ,
    num_inference_steps = 50,
    strength = 0.5,
    generator = torch.Generator("cuda:0").manual_seed(1)
).images[0]
pipeline.to("cpu")
torch.cuda.empty_cache()
image</pre>
			<p>In the preceding code, we used the<a id="_idIndexMarker571"/> original astronaut image – <strong class="source-inline">source_image</strong> – as the base, and the oil painting image as the IP-Adapter image prompt – <strong class="source-inline">ip_image</strong> (we want its style). Amazingly, we get the result shown in <span class="No-Break"><em class="italic">Figure 18</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B21263_18_06.jpg" alt="Figure 18.6: Astronaut riding a horse with a new style"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 18.6: Astronaut riding a horse with a new style</p>
			<p>The style and <a id="_idIndexMarker572"/>feel of the <em class="italic">Girl with a Pearl Earring</em> image have successfully been applied to <span class="No-Break">another image.</span></p>
			<p>IP-Adapter’s potential is huge. We can even transfer the clothing and face from one image to another. More usage samples can be found in the original IP-Adapter repository [3] and the <a id="_idTextAnchor371"/>Diffusers PR <span class="No-Break">page [5].</span></p>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor372"/>Summary</h1>
			<p>In this chapter, the focus was on using SD for image editing and style transferring. The chapter introduced tools such as CLIPSeg for image content detection, Rembg for background removal, and IP-Adapter for transferring styles <span class="No-Break">between images.</span></p>
			<p>The first section covered image editing, specifically replacing or removing the background. CLIPSeg is used to generate a mask of the background, which is then converted to a binary mask. The background is either replaced using SD or removed, with the latter option showing jagged edges. Rembg was introduced as a solution for smoother <span class="No-Break">background removal.</span></p>
			<p>The second section explored object and style transferring using IP-Adapter. The process involves loading an image encoder, incorporating it into an SD pipeline, and applying IP-Adapter to the UNet of the pipeline. The chapter concluded with an example of transferring the style of Vermeer’s <em class="italic">Girl with a Pearl Earring</em> onto an image of an astronaut riding <span class="No-Break">a horse.</span></p>
			<p>In the next chapter, we are going to explore solutions to save and read the parameters and prompt information to and from the <a id="_idTextAnchor373"/>generated <span class="No-Break">image files.</span></p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor374"/>References</h1>
			<ol>
				<li>CLIPSeg GitHub <span class="No-Break">repository: </span><a href="https://github.com/timojl/clipseg"><span class="No-Break">https://github.com/timojl/clipseg</span></a></li>
				<li>Timo Lüddecke and Alexander S. Ecker, <em class="italic">Image Segmentation Using Text and Image </em><span class="No-Break"><em class="italic">Prompts</em></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/2112.10003"><span class="No-Break">https://arxiv.org/abs/2112.10003</span></a></li>
				<li>IP-Adapter GitHub <span class="No-Break">repository: </span><a href="https://github.com/tencent-ailab/IP-Adapter"><span class="No-Break">https://github.com/tencent-ailab/IP-Adapter</span></a></li>
				<li>Rembg, a tool to remove image <span class="No-Break">backgrounds: </span><a href="https://github.com/danielgatis/rembg"><span class="No-Break">https://github.com/danielgatis/rembg</span></a></li>
				<li>IP-Adapters original <span class="No-Break">samples: </span><a href="https://github.com/huggingface/diffusers/pull/5713"><span class="No-Break">https://github.com/huggingface/diffusers/pull/5713</span></a></li>
			</ol>
		</div>
	</body></html>