<html><head></head><body>
  <div id="_idContainer110">
   <h1 class="chapter-number" id="_idParaDest-135">
    <a id="_idTextAnchor134">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     10
    </span>
   </h1>
   <h1 id="_idParaDest-136">
    <a id="_idTextAnchor135">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Training an Entity Linker Model with spaCy
    </span>
   </h1>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.3.1">
      Entity linking
     </span>
    </strong>
    <span class="koboSpan" id="kobo.4.1">
     is the NLP
    </span>
    <a id="_idIndexMarker478">
    </a>
    <span class="koboSpan" id="kobo.5.1">
     task that maps textual mentions to unique identifiers in external knowledge bases.
    </span>
    <span class="koboSpan" id="kobo.5.2">
     This chapter explores how to train an entity linking model using spaCy and the best practices on how to create good datasets for NLP training.
    </span>
    <span class="koboSpan" id="kobo.5.3">
     We will also learn how to use a custom corpus reader to train a spaCy component.
    </span>
    <span class="koboSpan" id="kobo.5.4">
     With this knowledge, you can customize any of the spaCy components to use while training
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.6.1">
      your models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.7.1">
     We will cover the following in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.8.1">
      this chapter:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.9.1">
      Understanding the concept and importance of entity linking
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.10.1">
       in NLP
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.11.1">
      Best practices for creating high-quality datasets for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.12.1">
       NLP training
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.13.1">
      Training an
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.14.1">
       EntityLinker
      </span>
     </strong>
     <span class="koboSpan" id="kobo.15.1">
      component
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.16.1">
       with spaCy
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.17.1">
      Utilizing a custom corpus reader to train a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.18.1">
       spaCy component
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.19.1">
     By the end of this chapter, you will be able to develop NLP models that integrate with external knowledge bases, enhancing accuracy and applicability in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.20.1">
      real-world scenarios.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-137">
    <a id="_idTextAnchor136">
    </a>
    <span class="koboSpan" id="kobo.21.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.22.1">
     All the data and the code for this chapter can be found
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.23.1">
      at
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.24.1">
       https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.25.1">
      .
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-138">
    <a id="_idTextAnchor137">
    </a>
    <span class="koboSpan" id="kobo.26.1">
     Understanding the entity linking task
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.27.1">
     Entity linking is the
    </span>
    <a id="_idIndexMarker479">
    </a>
    <span class="koboSpan" id="kobo.28.1">
     task of identifying the entity mentioned and linking it to the corresponding entry in each knowledge base.
    </span>
    <span class="koboSpan" id="kobo.28.2">
     For example, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.29.1">
      Washington
     </span>
    </strong>
    <span class="koboSpan" id="kobo.30.1">
     entity can refer to the person George Washington or the US state.
    </span>
    <span class="koboSpan" id="kobo.30.2">
     With entity linking or entity resolution, our goal is to map the entity to the correct real-world representation.
    </span>
    <span class="koboSpan" id="kobo.30.3">
     As spaCy’s documentation says, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.31.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.32.1">
     spaCy architecture requires three
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.33.1">
      main components:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.34.1">
      A
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.35.1">
       knowledge base
      </span>
     </strong>
     <span class="koboSpan" id="kobo.36.1">
      (
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.37.1">
       KB
      </span>
     </strong>
     <span class="koboSpan" id="kobo.38.1">
      ) to store
     </span>
     <a id="_idIndexMarker480">
     </a>
     <span class="koboSpan" id="kobo.39.1">
      the unique identifiers, synonyms, and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.40.1">
       prior probabilities
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.41.1">
      A
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.42.1">
       candidate generation step
      </span>
     </strong>
     <span class="koboSpan" id="kobo.43.1">
      to produce the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.44.1">
       likely identifiers
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.45.1">
      A
     </span>
     <strong class="bold">
      <span class="koboSpan" id="kobo.46.1">
       machine learning model
      </span>
     </strong>
     <span class="koboSpan" id="kobo.47.1">
      to select the most likely ID from the list
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.48.1">
       of candidates
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.49.1">
     In KB, each textual mention (alias) is represented as a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.50.1">
      Candidate
     </span>
    </strong>
    <span class="koboSpan" id="kobo.51.1">
     object that may or may not be linked to an entity.
    </span>
    <span class="koboSpan" id="kobo.51.2">
     A prior probability is assigned to each candidate
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.52.1">
      (alias,
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.53.1">
       entity)
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.54.1">
      pair.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.55.1">
     In the spaCy
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.56.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.57.1">
     architecture, first, we initialize a KB with the language shared
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.58.1">
      Vocab
     </span>
    </strong>
    <span class="koboSpan" id="kobo.59.1">
     and the length of the fixed-size entity vectors.
    </span>
    <span class="koboSpan" id="kobo.59.2">
     Then, we need to set the model.
    </span>
    <span class="koboSpan" id="kobo.59.3">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.60.1">
      spacy.EntityLinker.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.61.1">
     class uses the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.62.1">
      spacy.HashEmbedCNN.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.63.1">
     model as the default
    </span>
    <a id="_idIndexMarker481">
    </a>
    <span class="koboSpan" id="kobo.64.1">
     model architecture, which is spaCy’s standard
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.65.1">
       tok2vec
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.66.1">
      layer.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.67.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.68.1">
      spacy.HashEmbedCNN.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.69.1">
     architecture is defined by a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.70.1">
      MultiHashEmbed
     </span>
    </strong>
    <span class="koboSpan" id="kobo.71.1">
     embedding layer and a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.72.1">
      MaxoutWindowEncoder
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.73.1">
      encoding layer.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.74.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.75.1">
      MultiHashEmbed
     </span>
    </strong>
    <span class="koboSpan" id="kobo.76.1">
     embedding layer uses subword features and constructs an embedding layer that separately embeds lexical attributes (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.77.1">
      "NORM"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.78.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.79.1">
      "PREFIX"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.80.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.81.1">
      "SUFFIX"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.82.1">
     , etc.) using hash embedding.
    </span>
    <span class="koboSpan" id="kobo.82.2">
     The hash embedding is intended to solve the problem of embedding size.
    </span>
    <span class="koboSpan" id="kobo.82.3">
     In the paper
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.83.1">
      Hash Embeddings for Efficient Word Representations
     </span>
    </em>
    <span class="koboSpan" id="kobo.84.1">
     (
    </span>
    <a href="https://arxiv.org/pdf/1709.03933">
     <span class="koboSpan" id="kobo.85.1">
      https://arxiv.org/pdf/1709.03933
     </span>
    </a>
    <span class="koboSpan" id="kobo.86.1">
     ), Svenstrup, Hansen, and Winther point out that even for moderately small embedding sizes (300 dimensions), if the vocabulary is large (3 million words and phrases as in Google Word2Vec), the total number of parameters is close to 1 billion.
    </span>
    <span class="koboSpan" id="kobo.86.2">
     Using hash embedding is memory-efficient because it’s a compact data structure, requiring less storage space than
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.87.1">
      bag-of-words representation.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.88.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.89.1">
      MaxoutWindowEncoder
     </span>
    </strong>
    <span class="koboSpan" id="kobo.90.1">
     encoding layer encodes context using convolutions.
    </span>
    <span class="koboSpan" id="kobo.90.2">
     The two main parameters this layer takes are
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.91.1">
      window_size
     </span>
    </strong>
    <span class="koboSpan" id="kobo.92.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.93.1">
      depth
     </span>
    </strong>
    <span class="koboSpan" id="kobo.94.1">
     .
    </span>
    <span class="koboSpan" id="kobo.94.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.95.1">
      window_size
     </span>
    </strong>
    <span class="koboSpan" id="kobo.96.1">
     parameter sets the number of words to concatenate around each token to construct the convolution.
    </span>
    <span class="koboSpan" id="kobo.96.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.97.1">
      depth
     </span>
    </strong>
    <span class="koboSpan" id="kobo.98.1">
     parameter sets the number of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.99.1">
      convolutional layers.
     </span>
    </span>
   </p>
   <p>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.100.1">
      spacy.HashEmbedCNN.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.101.1">
     and all the other spaCy layers are defined using the Thinc API.
    </span>
    <span class="koboSpan" id="kobo.101.2">
     Let’s see the code that defines these layers (for learning purposes only since we just need to point to
    </span>
    <a id="_idIndexMarker482">
    </a>
    <span class="koboSpan" id="kobo.102.1">
     this definition in our
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.103.1">
       config.cfg
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.104.1">
      file):
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.105.1">
@registry.architectures("spacy.HashEmbedCNN.v2")
def build_hash_embed_cnn_tok2vec(
    *,
    width: int,
    depth: int,
    embed_size: int,
    window_size: int,
    maxout_pieces: int,
    subword_features: bool,
    pretrained_vectors: Optional[bool],
) -&gt; Model[List[Doc], List[Floats2d]]:
    if subword_features:
        attrs = ["NORM", "PREFIX", "SUFFIX", "SHAPE"]
        row_sizes = [embed_size, embed_size // 2, 
                     embed_size // 2, embed_size // 2]
    else:
        attrs = ["NORM"]
        row_sizes = [embed_size]
    return build_Tok2Vec_model(
        embed=MultiHashEmbed(
            width=width,
            rows=row_sizes,
            attrs=attrs,
            include_static_vectors=bool(pretrained_vectors),
        ),
        encode=MaxoutWindowEncoder(
            width=width,
            depth=depth,
            window_size=window_size,
            maxout_pieces=maxout_pieces,
        ),
    )</span></pre>
   <p>
    <span class="koboSpan" id="kobo.106.1">
     We can see that the layer is using
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.107.1">
      MultiHashEmbed
     </span>
    </strong>
    <span class="koboSpan" id="kobo.108.1">
     to embed the text and the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.109.1">
      MaxoutWindowEncoder
     </span>
    </strong>
    <span class="koboSpan" id="kobo.110.1">
     layer to encode the embeddings providing a final linear output for the model.
    </span>
    <span class="koboSpan" id="kobo.110.2">
     Next, let’s
    </span>
    <a id="_idIndexMarker483">
    </a>
    <span class="koboSpan" id="kobo.111.1">
     see the code for the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.112.1">
      EntityLinker.v2
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.113.1">
      architecture itself:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.114.1">
@registry.architectures("spacy.EntityLinker.v2")
def build_nel_encoder(
    tok2vec: Model, nO: Optional[int] = None
) -&gt; Model[List[Doc], Floats2d]:
    with Model.define_operators({"&gt;&gt;": chain, "&amp;": tuplify}):
        token_width = tok2vec.maybe_get_dim("nO")
        output_layer = Linear(nO=nO, nI=token_width)
        model = (
            ((tok2vec &gt;&gt; list2ragged()) &amp; build_span_maker())
            &gt;&gt; extract_spans()
            &gt;&gt; reduce_mean()
            &gt;&gt; residual(Maxout(nO=token_width, nI=token_width, 
                               nP=2, dropout=0.0))  # type: ignore
            &gt;&gt; output_layer
        )
        model.set_ref("output_layer", output_layer)
        model.set_ref("tok2vec", tok2vec)
    # flag to show this isn't legacy
    model.attrs["include_span_maker"] = True
    return model</span></pre>
   <p>
    <span class="koboSpan" id="kobo.115.1">
     The parameters of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.116.1">
      build_nel_encoder()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.117.1">
     method are the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.118.1">
      tok2vec
     </span>
    </strong>
    <span class="koboSpan" id="kobo.119.1">
     model and the output dimension,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.120.1">
      nO
     </span>
    </strong>
    <span class="koboSpan" id="kobo.121.1">
     , determined by the length of the vectors encoding each entity in the KB.
    </span>
    <span class="koboSpan" id="kobo.121.2">
     When we don’t set
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.122.1">
      nO
     </span>
    </strong>
    <span class="koboSpan" id="kobo.123.1">
     , it’s automatically set when
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.124.1">
      initialize
     </span>
    </strong>
    <span class="koboSpan" id="kobo.125.1">
     is called.
    </span>
    <span class="koboSpan" id="kobo.125.2">
     We will
    </span>
    <a id="_idIndexMarker484">
    </a>
    <span class="koboSpan" id="kobo.126.1">
     define the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.127.1">
      tok2vec
     </span>
    </strong>
    <span class="koboSpan" id="kobo.128.1">
     model in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.129.1">
      config.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.130.1">
     file
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.131.1">
      like this:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.132.1">
[components.entity_linker.model]
@architectures = "spacy.EntityLinker.v2"
nO = null
[components.entity_linker.model.tok2vec]
@architectures = "spacy.HashEmbedCNN.v1"
pretrained_vectors = null
width = 96
depth = 2
embed_size = 2000
window_size = 1
maxout_pieces = 3
subword_features = true</span></pre>
   <p>
    <span class="koboSpan" id="kobo.133.1">
     Now you have an idea of how spaCy and Thinc interact under the hood to create the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.134.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.135.1">
     model architecture.
    </span>
    <span class="koboSpan" id="kobo.135.2">
     If you ever need to change the parameters or try different models, you have an idea of where to go to change
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.136.1">
      these settings.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.137.1">
     In business settings, we usually start with no datasets available, forcing us to create our own datasets.
    </span>
    <span class="koboSpan" id="kobo.137.2">
     Having good quality datasets is a key component to training models with good performance, so it’s
    </span>
    <a id="_idIndexMarker485">
    </a>
    <span class="koboSpan" id="kobo.138.1">
     important to learn the fundamentals for creating good datasets.
    </span>
    <span class="koboSpan" id="kobo.138.2">
     Let’s talk about this in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.139.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-139">
    <a id="_idTextAnchor138">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     Best practices for creating a good NLP corpus
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.141.1">
     Due to fine-tuning
    </span>
    <a id="_idIndexMarker486">
    </a>
    <span class="koboSpan" id="kobo.142.1">
     techniques, fortunately, we don’t need huge amounts of data to train models today.
    </span>
    <span class="koboSpan" id="kobo.142.2">
     However, good datasets are still very important because they are critical to guaranteeing and evaluating the performance of our NLP systems.
    </span>
    <span class="koboSpan" id="kobo.142.3">
     For instance, biases and cultural nuances deeply embedded within language can inadvertently shape AI outputs if not carefully curated.
    </span>
    <span class="koboSpan" id="kobo.142.4">
     In the article
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.143.1">
      How to make a racist AI without really trying
     </span>
    </em>
    <span class="koboSpan" id="kobo.144.1">
     (
    </span>
    <a href="https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/">
     <span class="koboSpan" id="kobo.145.1">
      https://blog.conceptnet.io/posts/2017/how-to-make-a-racist-ai-without-really-trying/
     </span>
    </a>
    <span class="koboSpan" id="kobo.146.1">
     ), the researcher Robyn Speer provides a great tutorial on sentiment analysis and shows us that we can produce racist AI solutions by simply reusing embeddings that were trained on
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.147.1">
      biased data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.148.1">
     Usually, the annotation process for NLP tasks consists mainly of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.149.1">
      following steps:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.150.1">
      Define the problem or the task.
     </span>
     <span class="koboSpan" id="kobo.150.2">
      What problem are we trying to solve?
     </span>
     <span class="koboSpan" id="kobo.150.3">
      This question will guide us on how to select samples for labeling, how to label the data consistently, and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.151.1">
       so on.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.152.1">
      Identify and prepare a selection of the representative texts as starting material for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.153.1">
       the corpus.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.154.1">
      Define an annotation scheme and annotate a fragment of the corpus to determine the feasibility of the data to solve
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.155.1">
       the task.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.156.1">
      Annotate a large portion of
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.157.1">
       the corpus.
      </span>
     </span>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.158.1">
     It’s always a good practice to define the annotation instructions to ensure the dataset is annotated consistently.
    </span>
    <span class="koboSpan" id="kobo.158.2">
     This is important because we need this consistency so the machine learning algorithms can generalize effectively.
    </span>
    <span class="koboSpan" id="kobo.158.3">
     Andrew Ng tells an example of the importance of consistency in the annotation process in the course
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.159.1">
      Machine Learning in Production
     </span>
    </em>
    <span class="koboSpan" id="kobo.160.1">
     (
    </span>
    <a href="https://www.coursera.org/learn/introduction-to-machine-learning-in-production">
     <span class="koboSpan" id="kobo.161.1">
      https://www.coursera.org/learn/introduction-to-machine-learning-in-production
     </span>
    </a>
    <span class="koboSpan" id="kobo.162.1">
     ), where a computer vision model was not achieving the performance they were expecting, and after running some error analysis, they found out that it was due to inconsistencies in the labeling process.
    </span>
    <span class="koboSpan" id="kobo.162.2">
     The task was to find flaws in steel sheet images, and some annotations labeled a whole area with flaws while others labeled the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.163.1">
      flaws individually.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.164.1">
     To build good quality NLP datasets, we can borrow some principles from data quality dimensions.
    </span>
    <span class="koboSpan" id="kobo.164.2">
     The first characteristic of a good labeled dataset is
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.165.1">
      consistency
     </span>
    </strong>
    <span class="koboSpan" id="kobo.166.1">
     .
    </span>
    <span class="koboSpan" id="kobo.166.2">
     The data should have a consistent structure and unity in terms of formatting, labeling,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.167.1">
      and categorization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.168.1">
     A good dataset should also be
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.169.1">
      representative
     </span>
    </strong>
    <span class="koboSpan" id="kobo.170.1">
     of the target application domain.
    </span>
    <span class="koboSpan" id="kobo.170.2">
     If we’re working on a pipeline to extract information from the business documentation of a company, it may not be a good idea to use legal texts to train an embedding model
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.171.1">
      for that.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.172.1">
     The third and final characteristic of a good dataset is that it’s
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.173.1">
      well-documented
     </span>
    </strong>
    <span class="koboSpan" id="kobo.174.1">
     .
    </span>
    <span class="koboSpan" id="kobo.174.2">
     We should be explicit and transparent about how we gathered and selected the data, what the labeling instructions were, and who annotated the data.
    </span>
    <span class="koboSpan" id="kobo.174.3">
     This documentation is very important because it allows for transparency, reproducibility, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.175.1">
      bias management.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.176.1">
     In the upcoming section we create a small dataset, we’ve created a small dataset with sentences from news websites that mention the entities we want to disambiguate (Taylor Swift, Taylor Lautner, and Taylor Fritz).
    </span>
    <span class="koboSpan" id="kobo.176.2">
     The labeling task was to label the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.177.1">
      Taylor
     </span>
    </strong>
    <span class="koboSpan" id="kobo.178.1">
     mentions in the news sentences to each of the persons mentioned.
    </span>
    <span class="koboSpan" id="kobo.178.2">
     Unfortunately, in real life, we usually don’t deal with easy scenarios like this, and these are the moments when following the consistency, representativeness, and
    </span>
    <a id="_idIndexMarker487">
    </a>
    <span class="koboSpan" id="kobo.179.1">
     well-documented principles to build the datasets will become much
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.180.1">
      more important.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.181.1">
     Now that we know how to create a corpus, let’s get back to spaCy and learn how to train our
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.182.1">
       EntityLinker
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.183.1">
      component.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-140">
    <a id="_idTextAnchor139">
    </a>
    <span class="koboSpan" id="kobo.184.1">
     Training an EntityLinker component with spaCy
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.185.1">
     The first step to
    </span>
    <a id="_idIndexMarker488">
    </a>
    <span class="koboSpan" id="kobo.186.1">
     train the model is to create
    </span>
    <a id="_idIndexMarker489">
    </a>
    <span class="koboSpan" id="kobo.187.1">
     the KB.
    </span>
    <span class="koboSpan" id="kobo.187.2">
     We want to create a pipeline that will detect whether a reference to
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.188.1">
      Taylor
     </span>
    </em>
    <span class="koboSpan" id="kobo.189.1">
     means a reference to Taylor Swift (singer), Taylor Lautner (actor), or Taylor Fritz (tennis player).
    </span>
    <span class="koboSpan" id="kobo.189.2">
     Each of them has its own page and identifier on Wikidata so we will use Wikidata as our KB source.
    </span>
    <span class="koboSpan" id="kobo.189.3">
     To create the KB, we need to create an instance of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.190.1">
      InMemoryLookupKB
     </span>
    </strong>
    <span class="koboSpan" id="kobo.191.1">
     class passing the shared
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.192.1">
      Vocab
     </span>
    </strong>
    <span class="koboSpan" id="kobo.193.1">
     object and the size of the embeddings that we`ll use to encode the entities.
    </span>
    <span class="koboSpan" id="kobo.193.2">
     Let’s create
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.194.1">
      our KB:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.195.1">
      First, we will choose the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.196.1">
       Language
      </span>
     </strong>
     <span class="koboSpan" id="kobo.197.1">
      object (
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.198.1">
       en_core_web_md
      </span>
     </strong>
     <span class="koboSpan" id="kobo.199.1">
      ) and add a
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.200.1">
       SpanRuler
      </span>
     </strong>
     <span class="koboSpan" id="kobo.201.1">
      component to match all the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.202.1">
       taylor
      </span>
     </strong>
     <span class="koboSpan" id="kobo.203.1">
      mentions (this will be used to create
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.204.1">
       the corpus):
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.205.1">
import spacy
nlp = spacy.load("en_core_web_md")
ruler = nlp.add_pipe("span_ruler", after="ner")
patterns = [{"label": "PERSON", "pattern": [{"LOWER": "taylor"}]}]
ruler.add_patterns(patterns)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.206.1">
      We will save both the KB and the model to disk, so let’s define
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.207.1">
       these files:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.208.1">
kb_loc = "chapter_10/nel_taylor/my_kb"
nlp_dir = "chapter_10/nel_taylor/my_nlp"</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.209.1">
      And finally, we can start the KB creation.
     </span>
     <span class="koboSpan" id="kobo.209.2">
      First, we instantiate the kb object passing
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.210.1">
       Vocab
      </span>
     </strong>
     <span class="koboSpan" id="kobo.211.1">
      and the
     </span>
     <a id="_idIndexMarker490">
     </a>
     <span class="koboSpan" id="kobo.212.1">
      size
     </span>
     <a id="_idIndexMarker491">
     </a>
     <span class="koboSpan" id="kobo.213.1">
      of the vectors.
     </span>
     <span class="koboSpan" id="kobo.213.2">
      The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.214.1">
       en_core_web_md
      </span>
     </strong>
     <span class="koboSpan" id="kobo.215.1">
      model has vectors of 300 dimensions so we set this size for the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.216.1">
       entity vectors:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.217.1">
import os
from spacy.kb import InMemoryLookupKB
kb = InMemoryLookupKB(vocab=nlp.vocab, entity_vector_length=300)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.218.1">
     Now that we have the KB, we can use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.219.1">
      add_entity()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.220.1">
     method to add the entities and the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.221.1">
      add_alias()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.222.1">
     method to add the mention (in our case, the mention will be
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.223.1">
      Taylor
     </span>
    </strong>
    <span class="koboSpan" id="kobo.224.1">
     ) with the prior probabilities for each entity.
    </span>
    <span class="koboSpan" id="kobo.224.2">
     Let’s create the code to do all
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.225.1">
      of that:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.226.1">
      First, we create two dictionaries, one with Wikidata IDs for each of our entities and another with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.227.1">
       their descriptions:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.228.1">
entities = {'Q26876': 'Taylor Swift', 'Q23359': 'Taylor Lautner', 'Q17660516': 'Taylor Fritz'}
descriptions = {'Q26876': 'American singer-songwriter (born 1989)', 'Q23359': 'American actor', 'Q17660516': 'American tennis player'}</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.229.1">
      Now, it’s time to add the entities to the KB.
     </span>
     <span class="koboSpan" id="kobo.229.2">
      Each Wikidata QID will have a vector representation of the entity’s description.
     </span>
     <span class="koboSpan" id="kobo.229.3">
      To add the entity, we use the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.230.1">
       add_entity()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.231.1">
      method and, for now, we can set an arbitrary value for the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.232.1">
       freq
      </span>
     </strong>
     <span class="koboSpan" id="kobo.233.1">
      parameter (we will tell spaCy to ignore this frequency on the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.234.1">
       config.cfg
      </span>
     </strong>
     <span class="koboSpan" id="kobo.235.1">
      file we’ll use to train
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.236.1">
       the model):
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.237.1">
for qid, desc in descriptions.items():
    desc_doc = nlp(desc)
    desc_vector = desc_doc.vector
    kb.add_entity(entity=qid, entity_vector=desc_vector,
                  freq=111)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.238.1">
      Now, we can add
     </span>
     <a id="_idIndexMarker492">
     </a>
     <span class="koboSpan" id="kobo.239.1">
      the
     </span>
     <a id="_idIndexMarker493">
     </a>
     <span class="koboSpan" id="kobo.240.1">
      mentions (
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.241.1">
       alias
      </span>
     </strong>
     <span class="koboSpan" id="kobo.242.1">
      ) to the KB.
     </span>
     <span class="koboSpan" id="kobo.242.2">
      If the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.243.1">
       Taylor Swift
      </span>
     </strong>
     <span class="koboSpan" id="kobo.244.1">
      entity appears in the text, we have no doubt it’s referring to Taylor Swift the singer.
     </span>
     <span class="koboSpan" id="kobo.244.2">
      Similarly, if the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.245.1">
       Taylor Lautner
      </span>
     </strong>
     <span class="koboSpan" id="kobo.246.1">
      entity appears in the text, we have no doubt it’s referring to the actor Taylor Lautner.
     </span>
     <span class="koboSpan" id="kobo.246.2">
      We add this information to the KB by setting the probabilities of these entities
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.247.1">
       to
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.248.1">
        1
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.249.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.250.1">
for qid, name in entities.items():
    kb.add_alias(alias=name, entities=[qid], probabilities=[1])</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.251.1">
      When the name and the surname of the entities are present, we have no doubt about who is who, but what if the text mentions only
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.252.1">
       Taylor
      </span>
     </strong>
     <span class="koboSpan" id="kobo.253.1">
      ?
     </span>
     <span class="koboSpan" id="kobo.253.2">
      We will set the initial probabilities to be equal for all our three entities (30% for each since the sum of the probabilities must not
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.254.1">
       exceed 100%):
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.255.1">
qids = entities.keys()
kb.add_alias(alias="Taylor", entities=qids, 
             probabilities=[0.3, 0.3, 0.3])</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.256.1">
      Our KB is not all set.
     </span>
     <span class="koboSpan" id="kobo.256.2">
      Let’s print the entities and the aliases to check whether everything
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.257.1">
       is okay:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.258.1">
print(f"Entities in the KB: {kb.get_entity_strings()}")
&gt;&gt;&gt; Entities in the KB: ['Q23359', 'Q17660516', 'Q26876']
print(f"Aliases in the KB: {kb.get_alias_strings()}")
&gt;&gt;&gt; Aliases in the KB: ['Taylor Lautner', 'Taylor', 'Taylor Swift', 'Taylor Fritz']</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.259.1">
      Now, it’s time to save the
     </span>
     <a id="_idIndexMarker494">
     </a>
     <span class="koboSpan" id="kobo.260.1">
      KB
     </span>
     <a id="_idIndexMarker495">
     </a>
     <span class="koboSpan" id="kobo.261.1">
      and the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.262.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.263.1">
      model to disk so we can use
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.264.1">
       them later:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.265.1">
kb.to_disk(kb_loc)
if not os.path.exists(nlp_dir):
    os.mkdir(nlp_dir)
nlp.to_disk(nlp_dir)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.266.1">
     With
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.267.1">
      InMemoryLookupKB
     </span>
    </strong>
    <span class="koboSpan" id="kobo.268.1">
     all set, we can now prepare the training data for spaCy.
    </span>
    <span class="koboSpan" id="kobo.268.2">
     The CSV file we’ll work with has columns called
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.269.1">
      text
     </span>
    </strong>
    <span class="koboSpan" id="kobo.270.1">
     (the sentence),
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.271.1">
      person
     </span>
    </strong>
    <span class="koboSpan" id="kobo.272.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.273.1">
      label
     </span>
    </strong>
    <span class="koboSpan" id="kobo.274.1">
     (the name of the entity),
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.275.1">
      ent_start
     </span>
    </strong>
    <span class="koboSpan" id="kobo.276.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.277.1">
      ent_end
     </span>
    </strong>
    <span class="koboSpan" id="kobo.278.1">
     (the position of the tokens in the sentence), and the Wikidata
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.279.1">
      QID
     </span>
    </strong>
    <span class="koboSpan" id="kobo.280.1">
     .
    </span>
    <span class="koboSpan" id="kobo.280.2">
     The data has 49 sentences and we will use 80% for training (we will split this into
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.281.1">
      train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.282.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.283.1">
      dev
     </span>
    </strong>
    <span class="koboSpan" id="kobo.284.1">
     sets) and 20% for testing.
    </span>
    <span class="koboSpan" id="kobo.284.2">
     We will prepare the data in two steps: first, creating the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.285.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.286.1">
     objects and then adding them to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.287.1">
      DocBin
     </span>
    </strong>
    <span class="koboSpan" id="kobo.288.1">
     train and
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.289.1">
       dev
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.290.1">
      objects.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.291.1">
     To create the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.292.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.293.1">
     objects, we will wrap each sentence in a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.294.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.295.1">
     object and create
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.296.1">
      Span
     </span>
    </strong>
    <span class="koboSpan" id="kobo.297.1">
     objects using the data from the CSV file.
    </span>
    <span class="koboSpan" id="kobo.297.2">
     This
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.298.1">
      Span
     </span>
    </strong>
    <span class="koboSpan" id="kobo.299.1">
     object has a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.300.1">
      kb_id
     </span>
    </strong>
    <span class="koboSpan" id="kobo.301.1">
     parameter we will use to set the Wikidata QID of the entity.
    </span>
    <span class="koboSpan" id="kobo.301.2">
     Let’s go ahead and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.302.1">
      do that:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.303.1">
      First, we load the CSV file and get 80% of the rows for training and the rest
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.304.1">
       for testing:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.305.1">
import pandas as pd
df_labeled = pd.read_csv("https://raw.githubusercontent.com/PacktPublishing/Mastering-spaCy-Second-Edition/main/chapter_10/taylor_labeled_dataset.csv")
df_train = df_labeled.sample(frac=0.8, random_state=123)
df_test = df_labeled.drop(df_train.index)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.306.1">
      Now, we instantiate the pipeline we were working with to create the KB, wrap the sentences in it to create the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.307.1">
       Doc
      </span>
     </strong>
     <span class="koboSpan" id="kobo.308.1">
      objects, and create the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.309.1">
       Span
      </span>
     </strong>
     <span class="koboSpan" id="kobo.310.1">
      objects for the entities.
     </span>
     <span class="koboSpan" id="kobo.310.2">
      We will
     </span>
     <a id="_idIndexMarker496">
     </a>
     <span class="koboSpan" id="kobo.311.1">
      use
     </span>
     <a id="_idIndexMarker497">
     </a>
     <span class="koboSpan" id="kobo.312.1">
      two lists, one to store
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.313.1">
       docs
      </span>
     </strong>
     <span class="koboSpan" id="kobo.314.1">
      and the other to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.315.1">
       store
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.316.1">
        QIDs
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.317.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.318.1">
import spacy
from spacy.tokens import Span
from collections import Counter
nlp_dir = "chapter_10/nel_taylor/my_nlp"
nlp = spacy.load(nlp_dir)
docs = []
QIDs = []
for _,row in df_train.iterrows():
    sentence = row["text"]
    QID = row["QID"]
    span_start = row["ent_start"]
    span_end = row["ent_end"]
    doc = nlp(sentence)
    QIDs.append(QID)
    label_ent = "PERSON"
    ent_span = Span(doc, span_start, span_end, label_ent, 
                    kb_id=QID)
    doc.ents = [ent_span]
    docs.append(doc)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.319.1">
     We will use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.320.1">
      QIDs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.321.1">
     list to split the sentences into
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.322.1">
      train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.323.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.324.1">
      dev
     </span>
    </strong>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.325.1">
      DocBin
     </span>
    </strong>
    <span class="koboSpan" id="kobo.326.1">
     objects.
    </span>
    <span class="koboSpan" id="kobo.326.2">
     To do that, we will get the indexes of each QID, use the first eight sentences for
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.327.1">
      train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.328.1">
     , and leave the rest for
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.329.1">
      dev
     </span>
    </strong>
    <span class="koboSpan" id="kobo.330.1">
     .
    </span>
    <span class="koboSpan" id="kobo.330.2">
     Let’s
    </span>
    <a id="_idIndexMarker498">
    </a>
    <span class="koboSpan" id="kobo.331.1">
     do
    </span>
    <a id="_idIndexMarker499">
    </a>
    <span class="koboSpan" id="kobo.332.1">
     that in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.333.1">
      the code:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.334.1">
      First, we import the objects and create our empty
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.335.1">
        DocBin
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.336.1">
       objects:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.337.1">
from spacy.tokens import DocBin
import math
train_docs = DocBin()
dev_docs = DocBin()</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.338.1">
      Now, we loop through each entity QID, get the indexes of their sentences, and add them to each
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.339.1">
        DocBin
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.340.1">
       object:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.341.1">
entities = {'Q26876': 'Taylor Swift', 
            'Q23359': 'Taylor Lautner',
            'Q17660516': 'Taylor Fritz'}
for QID in entities.keys():
    indexes_sentences_qid = [i for i, j in enumerate(QIDs) 
                             if j == QID]
    for index in indexes_sentences_qid[0:8]:
        train_docs.add(docs[index])
    for index in indexes_sentences_qid[8:]:
        dev_docs.add(docs[index])</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.342.1">
      And now we can save the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.343.1">
       DocBin
      </span>
     </strong>
     <span class="koboSpan" id="kobo.344.1">
      files
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.345.1">
       to disk:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.346.1">
train_corpus = "chapter_10/nel_taylor/train.spacy"
dev_corpus = "chapter_10/nel_taylor/dev.spacy"
train_docs.to_disk(train_corpus)
dev_docs.to_disk(dev_corpus)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.347.1">
     With the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.348.1">
      train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.349.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.350.1">
      dev
     </span>
    </strong>
    <span class="koboSpan" id="kobo.351.1">
     sets ready, we can go ahead and train the model.
    </span>
    <span class="koboSpan" id="kobo.351.2">
     We will use the same configuration file that spaCy’s Nel Emerson tutorial uses (
    </span>
    <a href="https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson">
     <span class="koboSpan" id="kobo.352.1">
      https://github.com/explosion/projects/tree/v3/tutorials/nel_emerson
     </span>
    </a>
    <span class="koboSpan" id="kobo.353.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.353.2">
     You can get this file in the Github repository
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.354.1">
      here:
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/tree/main/chapter_10">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.355.1">
       https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/tree/main/chapter_10
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.356.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.357.1">
     If you need to refresh your understanding of spaCy’s training process, you can refer to
    </span>
    <a href="B22441_06.xhtml#_idTextAnchor087">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.358.1">
        Chapter 6
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.359.1">
     .
    </span>
    <span class="koboSpan" id="kobo.359.2">
     One new thing we will need to do to train the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.360.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.361.1">
     component is to use a custom file with additional code that should be used for training.
    </span>
    <span class="koboSpan" id="kobo.361.2">
     We’ll
    </span>
    <a id="_idIndexMarker500">
    </a>
    <span class="koboSpan" id="kobo.362.1">
     do
    </span>
    <a id="_idIndexMarker501">
    </a>
    <span class="koboSpan" id="kobo.363.1">
     this because we need to create
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.364.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.365.1">
     objects the way
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.366.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.367.1">
     needs.
    </span>
    <span class="koboSpan" id="kobo.367.2">
     Let’s talk more about this in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.368.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-141">
    <a id="_idTextAnchor140">
    </a>
    <span class="koboSpan" id="kobo.369.1">
     Training with a custom corpus reader
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.370.1">
     spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.371.1">
      Corpus
     </span>
    </strong>
    <span class="koboSpan" id="kobo.372.1">
     class manages
    </span>
    <a id="_idIndexMarker502">
    </a>
    <span class="koboSpan" id="kobo.373.1">
     annotated corpora for data loading during training.
    </span>
    <span class="koboSpan" id="kobo.373.2">
     The default corpus reader (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.374.1">
      spacy.Corpus.v1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.375.1">
     ) creates the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.376.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.377.1">
     objects using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.378.1">
      make_doc()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.379.1">
     method of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.380.1">
      Language
     </span>
    </strong>
    <span class="koboSpan" id="kobo.381.1">
     class.
    </span>
    <span class="koboSpan" id="kobo.381.2">
     This method only tokenizes the text.
    </span>
    <span class="koboSpan" id="kobo.381.3">
     To train the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.382.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.383.1">
     component, it needs to have the entities available in the doc.
    </span>
    <span class="koboSpan" id="kobo.383.2">
     That’s why we will create our own corpus reader in a file called
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.384.1">
      custom_functions.py
     </span>
    </strong>
    <span class="koboSpan" id="kobo.385.1">
     .
    </span>
    <span class="koboSpan" id="kobo.385.2">
     The reader should receive as parameters the path to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.386.1">
      DocBin
     </span>
    </strong>
    <span class="koboSpan" id="kobo.387.1">
     file and the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.388.1">
      nlp
     </span>
    </strong>
    <span class="koboSpan" id="kobo.389.1">
     object.
    </span>
    <span class="koboSpan" id="kobo.389.2">
     Inside the method, we will loop through each
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.390.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.391.1">
     to create the examples.
    </span>
    <span class="koboSpan" id="kobo.391.2">
     Let’s go ahead and create
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.392.1">
      this method:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.393.1">
      First, we disable the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.394.1">
       EntityLinker
      </span>
     </strong>
     <span class="koboSpan" id="kobo.395.1">
      component of the pipeline and then get all the docs from the
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.396.1">
        DocBin
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.397.1">
       file:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.398.1">
def read_files(file: Path, nlp: "Language") -&gt; Iterable[Example]:
   with nlp.select_pipes(disable="entity_linker"):
      doc_bin = DocBin().from_disk(file)
      docs = doc_bin.get_docs(nlp.vocab)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.399.1">
      Now, we will create the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.400.1">
       Example
      </span>
     </strong>
     <span class="koboSpan" id="kobo.401.1">
      objects for each doc.
     </span>
     <span class="koboSpan" id="kobo.401.2">
      The first parameter of
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.402.1">
       Example
      </span>
     </strong>
     <span class="koboSpan" id="kobo.403.1">
      is the text processed by the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.404.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.405.1">
      object and the second is the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.406.1">
       doc
      </span>
     </strong>
     <span class="koboSpan" id="kobo.407.1">
      with the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.408.1">
       annotated entities:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.409.1">
# ...
</span><span class="koboSpan" id="kobo.409.2">      for doc in docs:
         yield Example(nlp(doc.text), doc)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.410.1">
      To reference this reader
     </span>
     <a id="_idIndexMarker503">
     </a>
     <span class="koboSpan" id="kobo.411.1">
      in the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.412.1">
       config.cfg
      </span>
     </strong>
     <span class="koboSpan" id="kobo.413.1">
      file, we need to register it using the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.414.1">
       @spacy.registry
      </span>
     </strong>
     <span class="koboSpan" id="kobo.415.1">
      decorator.
     </span>
     <span class="koboSpan" id="kobo.415.2">
      Let’s import the libraries we will need and register
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.416.1">
       the reader:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.417.1">
from functools import partial
from pathlib import Path
from typing import Iterable, Callable
import spacy
from spacy.training import Example
from spacy.tokens import DocBin
@spacy.registry.readers("MyCorpus.v1")
def create_docbin_reader(file: Path) -&gt; Callable[["Language"], Iterable[Example]]:
    return partial(read_files, file)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.418.1">
      We use the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.419.1">
       partial
      </span>
     </strong>
     <span class="koboSpan" id="kobo.420.1">
      function so that when the spaCy code uses the reader internally, we only need to pass the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.421.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.422.1">
      object.
     </span>
     <span class="koboSpan" id="kobo.422.2">
      Now, we can reference this
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.423.1">
       MyCorpus.v1
      </span>
     </strong>
     <span class="koboSpan" id="kobo.424.1">
      reader in the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.425.1">
       config.cfg
      </span>
     </strong>
     <span class="koboSpan" id="kobo.426.1">
      file
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.427.1">
       like this:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.428.1">
#config.cfg snippet
[corpora]
[corpora.train]
@readers = "MyCorpus.v1"
file = ${paths.train}
[corpora.dev]
@readers = "MyCorpus.v1"
file = ${paths.dev}</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.429.1">
      Here is the full
     </span>
     <a id="_idIndexMarker504">
     </a>
     <span class="koboSpan" id="kobo.430.1">
      source code of our
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.431.1">
        custom_functions.py
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.432.1">
       file:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.433.1">
from functools import partial
from pathlib import Path
from typing import Iterable, Callable
import spacy
from spacy.training import Example
from spacy.tokens import DocBin
@spacy.registry.readers("MyCorpus.v1")
def create_docbin_reader(file: Path) -&gt; Callable[["Language"],
                                                 
Iterable[Example]]:
    return partial(read_files, file)
def read_files(file: Path, nlp: "Language") -&gt; Iterable[Example]:
    # we run the full pipeline and not just nlp.make_doc to
    # ensure we have entities and sentences
    # which are needed during training of the entity linker
    with nlp.select_pipes(disable="entity_linker"):
        doc_bin = DocBin().from_disk(file)
        docs = doc_bin.get_docs(nlp.vocab)
        for doc in docs:
            yield Example(nlp(doc.text), doc)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.434.1">
      To make the reader available when we call the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.435.1">
       train
      </span>
     </strong>
     <span class="koboSpan" id="kobo.436.1">
      CLI command, we need to provide the source code that contains the Python code we’ve created.
     </span>
     <span class="koboSpan" id="kobo.436.2">
      We do this using the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.437.1">
       code
      </span>
     </strong>
     <span class="koboSpan" id="kobo.438.1">
      parameter.
     </span>
     <span class="koboSpan" id="kobo.438.2">
      First, we get the
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.439.1">
        config.cfg
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.440.1">
       file:
      </span>
     </span>
     <pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.441.1">!curl https://raw.githubusercontent.com/PacktPublishing/Mastering-spaCy-Second-Edition/main/chapter_10/config.cfg -o config.cfg</span></strong></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.442.1">
      Now, we can run the full command to train our
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.443.1">
        EntityLinker
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.444.1">
       pipeline:
      </span>
     </span>
     <pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.445.1">python -m spacy train ./config.cfg --output entity_linking_taylor --paths.train ./nel_taylor/train.spacy --paths.dev ./nel_taylor/dev.spacy --paths.kb nel_taylor/my_kb --paths.base_nlp ./nel_taylor/my_nlp --code custom_functions.py</span></strong></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.446.1">
     Now that we have the
    </span>
    <a id="_idIndexMarker505">
    </a>
    <span class="koboSpan" id="kobo.447.1">
     trained model, we need to test it.
    </span>
    <span class="koboSpan" id="kobo.447.2">
     Let’s do that in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.448.1">
      next section.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-142">
    <a id="_idTextAnchor141">
    </a>
    <span class="koboSpan" id="kobo.449.1">
     Testing the entity linking model
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.450.1">
     To test the model, we will
    </span>
    <a id="_idIndexMarker506">
    </a>
    <span class="koboSpan" id="kobo.451.1">
     load it from the path we saved it on using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.452.1">
      train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.453.1">
     command and call the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.454.1">
      nlp
     </span>
    </strong>
    <span class="koboSpan" id="kobo.455.1">
     object on the doc we want to disambiguate the entities.
    </span>
    <span class="koboSpan" id="kobo.455.2">
     The entity linker model adds
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.456.1">
      kb_id
     </span>
    </strong>
    <span class="koboSpan" id="kobo.457.1">
     to the entities and we can use it to see which of the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.458.1">
      Taylors
     </span>
    </em>
    <span class="koboSpan" id="kobo.459.1">
     the model predicted.
    </span>
    <span class="koboSpan" id="kobo.459.2">
     Let’s use some of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.460.1">
      df_test
     </span>
    </strong>
    <span class="koboSpan" id="kobo.461.1">
     sentences to evaluate
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.462.1">
      the model:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.463.1">
      First, we load the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.464.1">
       model :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.465.1">
nlp = spacy.load("entity_linking_taylor/model-best")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.466.1">
      Now, we process the sentences.
     </span>
     <span class="koboSpan" id="kobo.466.2">
      The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.467.1">
       doc.ents
      </span>
     </strong>
     <span class="koboSpan" id="kobo.468.1">
      entities have the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.469.1">
       kb_id
      </span>
     </strong>
     <span class="koboSpan" id="kobo.470.1">
      attribute with the hash of the entity and the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.471.1">
       kb_id_
      </span>
     </strong>
     <span class="koboSpan" id="kobo.472.1">
      attribute with the plain text QID.
     </span>
     <span class="koboSpan" id="kobo.472.2">
      Let’s process
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.473.1">
       the text:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.474.1">
text = 'Taylor struggled with chilly temperatures in Edinburgh, pausing the show to warm up her hands and to assist a distressed fan.'
</span><span class="koboSpan" id="kobo.474.2">doc = nlp(text)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.475.1">
      Now, we can show the entities
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.476.1">
       using
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.477.1">
        displacy
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.478.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.479.1">
from spacy import displacy
displacy.serve(doc, style="ent")</span></pre>
    </li>
    <li>
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.480.1">
        Figure 10
       </span>
      </em>
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.481.1">
       .1
      </span>
     </em>
     <span class="koboSpan" id="kobo.482.1">
      shows
     </span>
     <a id="_idIndexMarker507">
     </a>
     <span class="koboSpan" id="kobo.483.1">
      the result.
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.484.1">
       Q26876
      </span>
     </strong>
     <span class="koboSpan" id="kobo.485.1">
      is the Wikidata ID for Taylor Swift, so the model was right
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.486.1">
       this time.
      </span>
     </span>
    </li>
   </ol>
   <div>
    <div class="IMG---Figure" id="_idContainer108">
     <span class="koboSpan" id="kobo.487.1">
      <img alt="Figure 10.1 – The Taylor Swift entity disambiguated correctly by the model" src="image/B22441_10_01.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.488.1">
     Figure 10.1 – The Taylor Swift entity disambiguated correctly by the model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.489.1">
     Let’s test the model with
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.490.1">
      another sentence:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.491.1">
text = 'Now, Taylor has revealed that he had to re-audition for the part because the producers wanted to go in a different direction.'
</span><span class="koboSpan" id="kobo.491.2">doc = nlp(text)</span></pre>
   <ol>
    <li value="5">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.492.1">
        Figure 10
       </span>
      </em>
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.493.1">
       .2
      </span>
     </em>
     <span class="koboSpan" id="kobo.494.1">
      shows the result.
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.495.1">
       Q23359
      </span>
     </strong>
     <span class="koboSpan" id="kobo.496.1">
      is the Wikidata ID for Taylor Lauter, so the model was also right on
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.497.1">
       this one.
      </span>
     </span>
    </li>
   </ol>
   <div>
    <div class="IMG---Figure" id="_idContainer109">
     <span class="koboSpan" id="kobo.498.1">
      <img alt="Figure 10.2 – The Taylor Lautner entity disambiguated correctly by the model" src="image/B22441_10_02.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.499.1">
     Figure 10.2 – The Taylor Lautner entity disambiguated correctly by the model
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.500.1">
     We’re taking it easy on the model by only evaluating these simple sentences because the goal here was only to
    </span>
    <a id="_idIndexMarker508">
    </a>
    <span class="koboSpan" id="kobo.501.1">
     demonstrate how to obtain the results from the model.
    </span>
    <span class="koboSpan" id="kobo.501.2">
     Training this component was quite a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.502.1">
      journey; congratulations!
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-143">
    <a id="_idTextAnchor142">
    </a>
    <span class="koboSpan" id="kobo.503.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.504.1">
     In this chapter, we learned how to train an
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.505.1">
      EntityLinker
     </span>
    </strong>
    <span class="koboSpan" id="kobo.506.1">
     component using spaCy.
    </span>
    <span class="koboSpan" id="kobo.506.2">
     We saw some implementation details to learn more about the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.507.1">
      HashEmbedCNN.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.508.1">
     layer and the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.509.1">
       EntityLinker.v2
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.510.1">
      architecture.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.511.1">
     We also discussed some characteristics of high-quality datasets for NLP training, stressing the importance of consistency, representativeness, and thorough documentation.
    </span>
    <span class="koboSpan" id="kobo.511.2">
     Finally, we saw how to create a custom corpus reader used to train the entity linking model.
    </span>
    <span class="koboSpan" id="kobo.511.3">
     With this knowledge, you can customize any other
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.512.1">
      spaCy component.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.513.1">
     In the next and final chapter, you will learn how to combine spaCy with other cool open source libraries to create great NLP applications.
    </span>
    <span class="koboSpan" id="kobo.513.2">
     See
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.514.1">
      you there!
     </span>
    </span>
   </p>
  </div>
 </body></html>