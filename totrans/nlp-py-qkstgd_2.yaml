- en: Tidying your Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data cleaning is one of the most important and time-consuming tasks when it
    comes to **natural language processing** (**NLP**):'
  prefs: []
  type: TYPE_NORMAL
- en: '"There''s the joke that 80 percent of data science is cleaning the data and
    20 percent is complaining about cleaning the data."'
  prefs: []
  type: TYPE_NORMAL
- en: – Kaggle founder and CEO Anthony Goldbloom in a [Verge Interview](https://www.theverge.com/2017/11/1/16589246/machine-learning-data-science-dirty-data-kaggle-survey-2017)
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss some of the most common text pre-processing
    ideas. This task is universal, tedious, and unavoidable. Most people working in
    data science or NLP understand that it's an underrated value addition. Some of
    these tasks don't work well in isolation but have a powerful effect when used
    in the right combination and order. This chapter will introduce several new words
    and tools, since the field has a rich history from two worlds. It borrows from
    both traditional NLP and machine learning. We'll meet spaCy, a fast industry-grade
    toolkit for natural language processing in Python. We will use it for tokenization,
    sentence extraction, and lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn to use regex functions, which are useful for text mining. Python's
    regex replaces can be slow for larger data sizes. Instead, we will use FlashText
    for substitution and expansion.
  prefs: []
  type: TYPE_NORMAL
- en: This is the only book to cover FlashText. More broadly, we will share how you
    can start thinking about manipulating and cleaning text. This is not an in-depth
    coverage of any one technique, but a jump start for you to think about what might
    work for you.
  prefs: []
  type: TYPE_NORMAL
- en: Bread and butter – most common tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several well-known text cleaning ideas. They have all made their
    way into the most popular tools today such as NLTK, Stanford CoreNLP, and spaCy.
    I like spaCy for two main reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It's an industry-grade NLP, unlike NLTK, which is mainly meant for teaching.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has good speed-to-performance trade-off. spaCy is written in Cython, which
    gives it C-like performance with Python code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spaCy is actively maintained and developed, and incorporates the best methods
    available for most challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 'By the end of this section, you will be able to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Understand tokenization and do it manually yourself using spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand why stop word removal and case standardization works, with spaCy
    examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between stemming and lemmatization, with spaCy lemmatization examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'I have always liked *The Adventures of Sherlock Holmes* by Sir Arthur Conan
    Doyle. Let''s download the book and save it locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s actually download the file. You only need to do this once, but this
    download utility can be used whenever you are downloading other datasets, too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving on, let''s check whether we got the correct file in place with shell
    syntax inside our Jupyter notebook. This ability to run basic shell commands –
    on both Windows and Linux – is really useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The file contains header and footer information from Project Gutenberg. We
    are not interested in this, and will discard the copyright and other legal notices.
    This is what we want to do:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete the header and footer information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the new file as `sherlock_clean.txt`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I opened the text file and found that I need to remove the first 33 lines.
    Let''s do that using shell commands – which also work on Windows inside Jupyter
    notebook. You remember this now, don''t you? Marching on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: I used the `sed` syntax. The `-i` flag tells you to make the necessary changes.
    `1,33d` instructs you to delete lines 1 to 33.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s double-check this. We expect the book to now begin with the iconic book
    title/cover:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows the first five lines of the book. They are as we expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: What do I see?
  prefs: []
  type: TYPE_NORMAL
- en: 'Before I move on to text cleaning for any NLP task, I would like to spend a
    few seconds taking a quick glance at the data itself. I noted down some of the
    things I spotted in the following list. Of course, a keener eye will be able to
    see a lot more than I did:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dates are written in a mixed format: *twentieth of March, 1888*; times are
    too: *three o''clock*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text is wrapped at around 70 columns, so no line can be longer than 70 characters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a lot of proper nouns. These include names such as *Atkinson* and
    *Trepoff*, in addition to locations such as *Trincomalee* and *Baker Street*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The index is in Roman numerals such as *I* and *IV*, and not *1* and *4*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot of dialogues such as *You have carte blanche,* with no narrative
    around them. This storytelling style switches freely from being narrative to dialogue-driven.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The grammar and vocabulary is slightly unusual because of the time when Doyle
    wrote.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These subjective observations are helpful in understanding the nature and edge
    cases in your text. Let''s move on and load the book into Python for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the first five characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Let's quickly verify that we have loaded the data into useful data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check our own data types, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: There is a major improvement between Py2.7 and Py3.6 on how strings are handled.
    They are now all Unicode by default.
  prefs: []
  type: TYPE_NORMAL
- en: In Python 3, `str` are Unicode strings, and it is more convenient for the NLP
    of non-English texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a small relevant example to highlight the differences between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the loaded data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How many unique characters can we see?
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, ASCII has 127 characters in it, so we expect this to have, at
    most, 127 characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For our machine learning models, we often need the words to occur as individual
    tokens or single words. Let's explain what this means in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a character sequence and a defined document unit, tokenization is the
    task of chopping it up into pieces, called tokens , perhaps at the same time throwing
    away certain characters, such as punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It is, in fact, sometimes useful to distinguish between tokens and words. But
    here, for ease of understanding, we will use them interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: We will convert the raw text into a list of words. This should preserve the
    original ordering of the text.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to do this, so let's try a few of them out. We will program
    two methods from scratch to build our intuition, and then check how spaCy handles
    tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Intuitive – split by whitespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following lines of code simply segment or *split* the entire text body
    on space `'' ''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s preview a rather large segment from our list of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The way punctuation is split here is not desirable. It often appears with the
    word itself, such as the full stop at end of `Adler**.**` and a comma being part
    of `emotions**,**`. Quite often we want words to be separated from punctuation,
    because words convey a lot more meaning than punctuation in most datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a shorter example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note how the words *red-headed* were not split. This is something we may or
    may not want to keep. We will come back to this, so keep this in mind.
  prefs: []
  type: TYPE_NORMAL
- en: One way to tackle this punctuation challenge is to simply extract words and
    discard everything else. This means that we will discard all non-ASCII characters
    and punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: The hack – splitting by word extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word extraction can be done in several ways. In turn, we can use word extraction
    for splitting the words into tokens. We will look at Regex, or Regular Expressions
    for doing word extractions. It is a pattern driven string search mechanism where
    the pattern grammar is defined by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Regexes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regular expressions can be a little challenging at first, but they are very
    powerful. They are generic abstractions, and work across multiple languages beyond
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The regular expression `\W+` means *a word character (A-Z* etc.*) repeated one
    or more times:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output of the preceding code is `(109111, 107431)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s preview the words we extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output we got from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We notice how `Adler` no longer has the punctuation mark alongside it. This
    is what we wanted. Mission accomplished?
  prefs: []
  type: TYPE_NORMAL
- en: 'What was the trade-off we made here? To understand that, let''s look at another
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output we got from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We have split `Isn''t` to `Isn` and `t`. This isn''t good if you''re working
    with, say, email or Twitter data, because you would have a lot more of these contractions
    and abbreviations. As a minor annoyance, we have an extra empty token, `''''`,
    at the end. Similarly, because we neglected punctuation, `red-headed` is broken
    into two words: `red` and `headed`. We have no straightforward way to restore
    this connection if we are only given the tokenized version.'
  prefs: []
  type: TYPE_NORMAL
- en: We can write custom rules in our tokenization strategy to cover most of these
    edge cases. Or, we can use something that has already been written for us.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy for tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'spaCy loads the English *model* using the preceding `.load` syntax. This tells
    spaCy what rules, logic, weights, and other information to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'While we use only `''en''` or English examples in this book, spaCy supports
    these features for more languages. I have used their multi-language tokenizer
    for Hindi as well, and have been satisfied with the same:'
  prefs: []
  type: TYPE_NORMAL
- en: The `%%time` syntax measures the CPU and Wall time at your runtime execution
    for the cell in a Jupyter not ebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This creates a spaCy object, `doc`. The object stores pre-computed linguistic
    features, including tokens. Some NLP libraries, especially in the Java and C ecosystem,
    compute linguistic features such as tokens, lemmas, and parts of speech when that
    specific function is called. Instead, spaCy computes them all at initialization
    when the `text` is passed to it.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy pre-computes most linguistic features – all you have to do is retrieve
    them from the object.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can retrieve them by calling the object iterator. In the following code,
    we call the iterator and *list* it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Conveniently, spaCy tokenizes all punctuation and words. They are returned
    as individual tokens. Let''s try the example that we didn''t like earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy got the `Isn''t` split correct: `Is` and `n''t`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`red-headed` was broken into three tokens: `red`, `-`, and `headed`. Since
    the punctuation information isn''t lost, we can restore the original `red-headed`
    token if we want to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the spaCy tokenizer work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest explanation is from the spaCy docs ([spacy-101](https://spacy.io/usage/spacy-101))
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the raw text is split on whitespace characters, similar to text.split
    (`'' ''`). Then, the tokenizer processes the text from left to right. On each
    substring, it performs two checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Does the substring match a tokenizer exception rule?* For example, *don''t*
    does not contain whitespace, but should be split into two tokens, *do* and *n''t*,
    while *U.K.* should always remain one token.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Can a prefix, suffix, or infix be split off?* For example, punctuation such
    as commas, periods, hyphens, or quotes:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/3c84ff70-690a-4ebb-b74b-29065083cf1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Sentence tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also use spaCy to extract one sentence at a time, instead of one word
    at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Stop words removal and case change
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These simple ideas are widespread and fairly effective for a lot of tasks. They
    are particularly useful in reducing the number of unique tokens in a document
    for your processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy has already marked each token as a stop word or not and stored it in
    the `is_stop` attribute of each token. This makes it very handy for text cleaning.
    Let''s take a quick look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting back to our Sherlock example, let’s take a look at the first few lines
    and whether they count as stop words or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Interesting – while *the* and *of* were marked as stop words, `THE` and `OF`
    were not. This is not a bug, but by design. spaCy doesn't remove words that are
    different because of their capitals or title case automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, we can force this behavior by converting our original text to lowercase
    before we pass it to spaCy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at what stop words exist in the spaCy dictionary, and then how
    to extend the same programmatically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: We want to expand the stop words dictionary according to our domain and problem.
    For instance, if you were using this code to process the text of an NLP book,
    we might want to add words such as `NLP`, *Processing*, `AGI`, *Data,* and so
    on to the stop words list.
  prefs: []
  type: TYPE_NORMAL
- en: 'spaCy has an intuitive `.add()` API to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try running the same example as earlier with these added stop words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output from running the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Exactly as expected, `NLP` and `AGI` are now marked as stop words too.
  prefs: []
  type: TYPE_NORMAL
- en: Let's pull out string tokens which are not stop words into a Python list or
    similar data structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some NLP tasks that come after text pre-processing expect string tokens and
    not spaCy token objects as a datatype. Removing both stop words and punctuation
    here for demonstration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Or just removing stop words, while retaining punctuation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Stemming and lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stemming and lemmatization are very two very popular ideas that are used to
    reduce the vocabulary size of your corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stemming** usually refers to a crude heuristic process that chops off the
    ends of words in the hope of achieving this goal correctly most of the time, and
    often includes the removal of derivational affixes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lemmatization** usually refers to doing things properly with the use of a
    vocabulary and morphological analysis of words, normally aiming to remove inflectional
    endings only and to return the base or dictionary form of a word, which is known
    as the lemma.'
  prefs: []
  type: TYPE_NORMAL
- en: If confronted with the token saw, stemming might return just s, whereas lemmatization
    would attempt to return either see or saw, depending on whether the use of the
    token was as a verb or a noun.
  prefs: []
  type: TYPE_NORMAL
- en: '- Dr. Christopher Manning et al, 2008, [[IR-Book](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)]'
  prefs: []
  type: TYPE_NORMAL
- en: (Chris Manning is a Professor in machine learning at the Departments of Computer
    Science and Linguistics at Stanford University)
  prefs: []
  type: TYPE_NORMAL
- en: spaCy for lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'spaCy only supports lemmatization. As discussed by spaCy creator Matt Honnibal
    in [issue #327](https://github.com/explosion/spaCy/issues/327) on GitHub, stemmers
    are rarely a good idea.'
  prefs: []
  type: TYPE_NORMAL
- en: We want to treat `meet/NOUN` differently from `meeting/VERB`. Unlike Stanford
    NLTK, which was created to *teach and introduce* as many NLP ideas as possible,
    spaCy takes an opinionated stand against stemming.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy does lemmatization for you by default when you process the text with the
    `nlp` object. This information is stored in the `lemma` attribute for each token.
    spaCy stores the internal hash or identifier, which spaCy stores in `token.lemma`.
    This numerical hash has no meaning for us. This numerical representation helps
    spaCy access and manipulate information much faster than its other Pythonic components.
  prefs: []
  type: TYPE_NORMAL
- en: 'An underscore at the attribute end, such as `lemma_`, tells spaCy that we are
    looking for something that is human-readable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: There's quite a few things going on here. Let's discuss them.
  prefs: []
  type: TYPE_NORMAL
- en: -PRON-
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'spaCy has a slightly annoying lemma (recall that lemma is the output of lemmatization):
    -PRON-. This is used as the lemma for all pronouns such as `Their`, `you`, `me`,
    and `I`. Other NLP tools lemmatize these to `I` instead of a placeholder, such
    as `-PRON-`.'
  prefs: []
  type: TYPE_NORMAL
- en: Case-insensitive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While checking for stop words, spaCy did not automatically lowercase our input.
    On the other hand, lemmatization does this for us. It converted "Apple" to "apple"
    and "Banana" to "banana".
  prefs: []
  type: TYPE_NORMAL
- en: This is one of the ways spaCy makes our lives easier, though slightly inconsistent.
    While removing stop words, we want to preserve THE in "THE ADVENTURES OF SHERLOCK
    HOLMES" while removing *the* in "the street was black". The opposite is usually
    true in lemmatization; we care more about how the word was used in context and
    use a proper lemma accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion – meeting to meet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lemmatization is aware of the linguistic role that words play in context. "Meeting"
    is converted to "meet" because it's a verb. spaCy does expose part of speech tagging
    and other linguistic features for us to use. We will learn how to query those
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy compared with NLTK and CoreNLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is a comparison of the NLTK and CoreNLP:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Spacy** | **NLTK** | **CoreNLP** |'
  prefs: []
  type: TYPE_TB
- en: '| Native Python support/API | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-language support | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Tokenization | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Part-of-speech tagging | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Sentence segmentation | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Dependency parsing | Y | N | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Entity recognition | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Integrated word vectors | Y | N | N |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment analysis | Y | Y | Y |'
  prefs: []
  type: TYPE_TB
- en: '| Coreference resolution | N | N | Y |'
  prefs: []
  type: TYPE_TB
- en: Correcting spelling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most frequently seen text challenges is correcting spelling errors.
    This is all the more true when data is entered by casual human users, for instance,
    shipping addresses or similar.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example. We want to correct Gujrat, Gujart, and other minor
    misspellings to Gujarat. There are several good ways to do this, depending on
    your dataset and level of expertise. We will discuss two or three popular ways,
    and discuss their pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Before I begin, we need to pay homage to the legendary [Peter Norvig's Spell
    Correct](https://norvig.com/spell-correct.html). It's still worth a read on how
    to *think* about solving a problem and *exploring* implementations. Even the way
    he refactors his code and writes functions is educational.
  prefs: []
  type: TYPE_NORMAL
- en: 'His spell-correction module is not the simplest or best way of doing this.
    I recommend two packages: one with a bias toward simplicity, one with a bias toward
    giving you all the knives, bells, and whistles to try:'
  prefs: []
  type: TYPE_NORMAL
- en: '**[FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy)** is easy to use. It
    gives a simple similarity score between two strings, capped to 100\. Higher numbers
    mean that the words are more similar.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**[Jellyfish](https://github.com/jamesturk/jellyfish)** supports six edit distance
    functions and four phonetic encoding options that you can use as per your use
    case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FuzzyWuzzy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's see how we can use FuzzyWuzzy to correct our misspellings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to install FuzzyWuzzy on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'FuzzyWuzzy has two main modules that will come in useful: fuzz and process.
    Let''s import fuzz first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We can see how the ratio function is confused by the trailing `Bangalore` used
    in the preceding address, but really the two strings refer to the same address/entity.
    This is captured by `partial_ratio`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Do you see how both `ratio` and `partial_ratio` are sensitive to the ordering
    of the words? This is useful for comparing addresses that follow some order. On
    the other hand, if we want to compare something else, for example, person names,
    it might give counter-intuitive results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, just because we had an extra `D.` token, our logic is not applicable
    anymore. We want something that is less order-sensitive. The authors of FuzzyWuzzy
    have us covered.
  prefs: []
  type: TYPE_NORMAL
- en: 'FuzzyWuzzy supports functions that tokenize our input on space and remove punctuation,
    numbers, and non-ASCII characters. This is then used to calculate similarity.
    Let''s try this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This will work perfectly for us. In case we have a list of options and we want
    to find the closest match(es), we can use the process module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at another example. Here, we have `Bangalore` misspelled as `Banglore`
    – we are missing an `a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take an example of a common search typo in online shopping. Users have
    misspelled `chilli` as `chili`; note the missing `l`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Jellyfish
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Jellyfish supports reasonably fast implementations of almost all popular edit
    distance functions (Recall how the edit distance functions tell you how similar
    two sequences/strings are). While FuzzyWuzzy supported mainly Levenshtein distance,
    this package supports some more string comparison utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Levenshtein distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Damerau-Levenshtein distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaro distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jaro-Winkler distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Match rating approach comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamming distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, it supports **phonetic encodings** for English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following code to install Jellyfish on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s try importing the package and setting up some examples to try out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to try multiple distance functions with all of our examples. The smarter
    thing to do is build a utility function for this. Let''s do that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note that `calculate_distance` takes the distance function as input. We can
    leave `examples` as implicitly picked from what we had declared previously in
    the global namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Levenshtein distance, which is probably the most famous string similarity function,
    is sometimes synonymous with edit distance function, but we consider this to be
    a particular implementation of the edit distance family of functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The Damerau–Levenshtein distance adds transpositions to the Levenshtein edit
    operations of insertion, deletion, and substitution. Let''s try this out and see
    if it changes anything for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We note that the `Narendra` and `Narendar` distance value changed from `3` to
    `2`. This is because we now count at least `a` to be transposed with `r` or vice
    versa. The other character is a substitution, so 1+1 = 2\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next distance function that we will try is hamming distance. This counts
    the minimum number of substitutions required to change one string into the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '**Jaro and Jaro-Winkler** return a value of similarity – and not dissimilarity.
    This means that the perfect match returns 1.0 and a totally unrelated match would
    tend to be 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Trying the other variation of Jaro similarity, that is, Jaro-Winkler, we get
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: These are extremely useful and diverse techniques. Yet, their overemphasis on
    written text creates one problem that is unique to English. We don't write English
    in the same way we speak. This means that we do not capture the range of all similarities.
    To solve this challenge, which is typically encountered in chatbots used by non-native
    English speakers, we can look at the phonetic similarity of words, which is what
    we will do next.
  prefs: []
  type: TYPE_NORMAL
- en: Phonetic word similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The way we say a word makes up its phonetics. Phonetics is the information of
    speech sounds. For instance, soul and sole sound identical in a lot of British-derived
    accents, such as Indian accents.
  prefs: []
  type: TYPE_NORMAL
- en: Quite often, words might be misspelled a little bit because the typist was trying
    to make it *sound right*. In this case, we leverage this phonetic information
    to map this typo back to the correct spelling.
  prefs: []
  type: TYPE_NORMAL
- en: What is a phonetic encoding?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can convert a word into a representation of its pronunciation. Of course,
    this might vary by accents, and by the conversion technique as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yet, over time, two or three popular ways have emerged so that we can do this.
    Each of these methods takes a single string and returns a coded representation.
    I encourage you to Google each of these terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**American Soundex (the 1930s)**: Implemented in popular database software
    such as PostgreSQL, MySQL, and SQLite'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NYSIIS (New York State Identification and Intelligence System) (the 1970s)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metaphone (the 1990s)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Match rating codex (the early 2000s)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s take a quick preview of the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'For NYSIIS, we will use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the slightly more updated metaphone, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The matching rate codex gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We can now use the string comparison utility that we saw earlier to compare
    two strings phonetically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'For instance, `write` and `right`should have zero phonetic Levenshtein distance
    because they are pronounced in the same way. Let''s try this out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'This worked as expected. Let''s add some examples to our old examples list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s encapsulate this into a utility function, like we did earlier. We will
    use two function parameters now: `phonetic_func` and `distance_func`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Note that Delhi and Dilli are separated, which is not nice. On the other hand,
    Narendra and Narendar are marked as similar to zero edit distance, which is quite
    cool. Let's try a different technique and see how it goes.
  prefs: []
  type: TYPE_NORMAL
- en: '**American soundex**'
  prefs: []
  type: TYPE_NORMAL
- en: We note that the Soundex is aware of common similar-sounding words and gives
    them separate phonetic encoding. This allows us to separate `right` from `write`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will only work on American/English words though. Indian sounds such as
    `Narendra Modi` and `Narendra D. Modi` are now considered similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the changes from the previous code in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Runtime complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have the ability to find the correct spellings of words or mark them
    as similar. While processing a large corpus, we can extract all unique words and
    compare each token against every other token.
  prefs: []
  type: TYPE_NORMAL
- en: It would take O(n²), where *n* is the number of unique tokens in a corpus. This
    might make the process too slow for a large corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The alternative is to use a standard dictionary and expand the same for your
    corpus. If the dictionary has *m* unique words, this process now will be O(m∗n).
    Assuming that m<<n*m<<n², this will be much faster than the previous approach.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning a corpus with FlashText
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: But what about a web-scale corpus with millions of documents and a few thousand
    keywords? Regex can take several days to run over such exact searches because
    of its linear time complexity. How can we improve this?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use FlashText for this very specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: A few million documents with a few thousand keywords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exact keyword matches – either by replacing or searching for the presence of
    those keywords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, there are several different possible solutions to this problem. I
    recommend this for its simplicity and focus on solving one problem. It does not
    require us to learn new syntax or set up specific tools such as ElasticSearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table gives you a comparison of using Flashtext versus compiled
    regex for searching:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a629566c-ec3d-427b-a4de-f22c8b7ee9a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following tables gives you a comparison of using FlashText versus compiled
    regex for substitutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d090d72d-edef-4635-9035-0979e2af849c.png)'
  prefs: []
  type: TYPE_IMG
- en: We note that while the time taken by Regex scales almost linearly, Flashtext
    is relatively flat. Now, we know that we need Flashtext for speed and scale. FlashText
    has seen a lot of love from the community. Adopters include [NLProc](https://github.com/NIHOPA/NLPre)
    – the NLP Preprocessing Toolkit from the National Institute of Health.
  prefs: []
  type: TYPE_NORMAL
- en: Follow these instructions to install FlashText onto your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will install pip on our conda environment. We will do this from our
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'The FlashText source code is available on GitHub ([https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02](https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02)),
    and the documents are pretty easy to navigate and use. We will only consider two
    basic examples here. Let''s figure out the syntax for finding keywords that exist
    in a corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: How about we replace them now?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Unfortunately, FlashText only supports English for the time being. Regex can
    search for keywords based on special characters such as `^,$,*,\d`, which are
    not supported in FlashText. So, to match partial words such as `word\dvec`, we
    would still have to use regex. However, FlashText is still excellent for extracting
    complete words like `word2vec`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered a lot of new ground. We started by performing linguistic
    processing on our text. We met **spaCy**, which we will continue to dive deeper
    into as we move on in this book. We covered the following foundational ideas from
    linguistics, tokenization doing this with and without spaCy, stop word removal,
    case standardization, lemmatization (we skipped stemming) – using spaCy and its
    peculiarities such as*-PRON-*
  prefs: []
  type: TYPE_NORMAL
- en: But what do we do with spaCy, other than text cleaning? Can we build something?
    Yes!
  prefs: []
  type: TYPE_NORMAL
- en: Not only can we extend our simple linguistics based text cleaning using spaCy
    pipelines but also do parts of speech tagging, named entity recognition, and other
    common tasks. We will look at this in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at spelling correction or the closest word match problem. We discussed
    **FuzzyWuzzy** and **Jellyfish** in this context. To ensure that we can scale
    beyond more than a few hundred keywords, we also looked at **FlashText**. I encourage
    you to dive deeper into any of these excellent libraries to learn about the best
    software engineering practices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will tie all these together with other linguistic tools
    to build an end-to-end toy program.
  prefs: []
  type: TYPE_NORMAL
