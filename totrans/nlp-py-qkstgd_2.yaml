- en: Tidying your Text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 整理你的文本
- en: 'Data cleaning is one of the most important and time-consuming tasks when it
    comes to **natural language processing** (**NLP**):'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是自然语言处理（**NLP**）中最重要且耗时的工作之一：
- en: '"There''s the joke that 80 percent of data science is cleaning the data and
    20 percent is complaining about cleaning the data."'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “有这样一个笑话，80%的数据科学是数据清洗，20%是抱怨数据清洗。”
- en: – Kaggle founder and CEO Anthony Goldbloom in a [Verge Interview](https://www.theverge.com/2017/11/1/16589246/machine-learning-data-science-dirty-data-kaggle-survey-2017)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: – Kaggle创始人兼首席执行官安东尼·戈德布卢姆在[Verge访谈](https://www.theverge.com/2017/11/1/16589246/machine-learning-data-science-dirty-data-kaggle-survey-2017)中提到
- en: In this chapter, we will discuss some of the most common text pre-processing
    ideas. This task is universal, tedious, and unavoidable. Most people working in
    data science or NLP understand that it's an underrated value addition. Some of
    these tasks don't work well in isolation but have a powerful effect when used
    in the right combination and order. This chapter will introduce several new words
    and tools, since the field has a rich history from two worlds. It borrows from
    both traditional NLP and machine learning. We'll meet spaCy, a fast industry-grade
    toolkit for natural language processing in Python. We will use it for tokenization,
    sentence extraction, and lemmatization.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一些最常见的文本预处理想法。这项任务是普遍的、繁琐的、不可避免的。大多数在数据科学或NLP领域工作的人都知道，这是一个被低估的价值增加。其中一些任务在单独使用时效果不佳，但在正确的组合和顺序中使用时具有强大的效果。由于该领域有两个世界的丰富历史，本章将介绍几个新词和工具。它从传统NLP和机器学习两个领域借鉴。我们将遇到spaCy，这是一个用于Python的自然语言处理的快速行业级工具包。我们将用它进行分词、句子提取和词形还原。
- en: We will learn to use regex functions, which are useful for text mining. Python's
    regex replaces can be slow for larger data sizes. Instead, we will use FlashText
    for substitution and expansion.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习使用正则表达式函数，这对于文本挖掘很有用。Python的正则表达式替换对于较大的数据集来说可能很慢。相反，我们将使用FlashText进行替换和扩展。
- en: This is the only book to cover FlashText. More broadly, we will share how you
    can start thinking about manipulating and cleaning text. This is not an in-depth
    coverage of any one technique, but a jump start for you to think about what might
    work for you.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这是唯一一本涵盖FlashText的书籍。更广泛地说，我们将分享你如何开始思考操纵和清洗文本。这不是对任何一种技术的深入覆盖，而是为你提供一个起点，让你思考哪些可能对你有效。
- en: Bread and butter – most common tasks
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常规任务 - 面包和黄油
- en: 'There are several well-known text cleaning ideas. They have all made their
    way into the most popular tools today such as NLTK, Stanford CoreNLP, and spaCy.
    I like spaCy for two main reasons:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个著名的文本清洗想法。它们都已经进入了今天最流行的工具中，如NLTK、Stanford CoreNLP和spaCy。我喜欢spaCy的两个主要原因：
- en: It's an industry-grade NLP, unlike NLTK, which is mainly meant for teaching.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是一个行业级NLP，与NLTK不同，NLTK主要是为了教学。
- en: It has good speed-to-performance trade-off. spaCy is written in Cython, which
    gives it C-like performance with Python code.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有良好的速度与性能权衡。spaCy是用Cython编写的，这使得它具有类似C的性能，同时使用Python代码。
- en: spaCy is actively maintained and developed, and incorporates the best methods
    available for most challenges.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy正在积极维护和开发，并集成了大多数挑战的最佳可用方法。
- en: 'By the end of this section, you will be able to do the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到本节结束时，你将能够做到以下几件事情：
- en: Understand tokenization and do it manually yourself using spaCy
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分词并使用spaCy手动进行
- en: Understand why stop word removal and case standardization works, with spaCy
    examples
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解为什么停用词去除和大小写标准化是有效的，以spaCy为例
- en: Differentiate between stemming and lemmatization, with spaCy lemmatization examples
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分词干提取和词形还原，以spaCy词形还原为例
- en: Loading the data
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'I have always liked *The Adventures of Sherlock Holmes* by Sir Arthur Conan
    Doyle. Let''s download the book and save it locally:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我一直喜欢亚瑟·柯南·道尔的*福尔摩斯探案集*。让我们下载这本书并保存在本地：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s actually download the file. You only need to do this once, but this
    download utility can be used whenever you are downloading other datasets, too:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际下载文件。你只需要做一次，但这个下载实用程序也可以在下载其他数据集时使用：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Moving on, let''s check whether we got the correct file in place with shell
    syntax inside our Jupyter notebook. This ability to run basic shell commands –
    on both Windows and Linux – is really useful:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查在Jupyter笔记本内部是否正确放置了文件，使用shell语法。这种在Windows和Linux上运行基本shell命令的能力非常有用：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding command returns the following output:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令返回以下输出：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The file contains header and footer information from Project Gutenberg. We
    are not interested in this, and will discard the copyright and other legal notices.
    This is what we want to do:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文件包含来自Project Gutenberg的标题和页脚信息。我们对此不感兴趣，并将丢弃版权和其他法律声明。这是我们想要做的：
- en: Open the file.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开文件。
- en: Delete the header and footer information.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除标题和页脚信息。
- en: Save the new file as `sherlock_clean.txt`.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新文件保存为`sherlock_clean.txt`。
- en: 'I opened the text file and found that I need to remove the first 33 lines.
    Let''s do that using shell commands – which also work on Windows inside Jupyter
    notebook. You remember this now, don''t you? Marching on:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我打开了文本文件，发现我需要删除前33行。让我们使用shell命令来完成这个任务——这些命令在Jupyter笔记本中的Windows上也同样适用。你现在还记得这个吗？继续前进：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: I used the `sed` syntax. The `-i` flag tells you to make the necessary changes.
    `1,33d` instructs you to delete lines 1 to 33.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了`sed`语法。`-i`标志告诉你要进行必要的更改。`1,33d`指示你删除第1到33行。
- en: 'Let''s double-check this. We expect the book to now begin with the iconic book
    title/cover:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次检查一下。我们期望这本书现在以标志性的书名/封面开始：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This shows the first five lines of the book. They are as we expect:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了这本书的前五行。它们正如我们所预期的那样：
- en: '[PRE6]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: What do I see?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我看到了什么？
- en: 'Before I move on to text cleaning for any NLP task, I would like to spend a
    few seconds taking a quick glance at the data itself. I noted down some of the
    things I spotted in the following list. Of course, a keener eye will be able to
    see a lot more than I did:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我继续进行任何NLP任务前的文本清理之前，我想花几秒钟快速看一下数据本身。我在以下列表中记下了我注意到的一些事情。当然，更敏锐的眼睛能看到比我更多的东西：
- en: 'Dates are written in a mixed format: *twentieth of March, 1888*; times are
    too: *three o''clock*.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期以混合格式书写：*1888年3月20日*；时间也是如此：*三点钟*。
- en: The text is wrapped at around 70 columns, so no line can be longer than 70 characters.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本在大约70列处换行，所以没有一行可以超过70个字符。
- en: There are a lot of proper nouns. These include names such as *Atkinson* and
    *Trepoff*, in addition to locations such as *Trincomalee* and *Baker Street*.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多专有名词。这些包括像*Atkinson*和*Trepoff*这样的名字，以及像*Trincomalee*和*Baker Street*这样的地点。
- en: The index is in Roman numerals such as *I* and *IV*, and not *1* and *4*.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 索引使用罗马数字，如*I*和*IV*，而不是*1*和*4*。
- en: There is a lot of dialogues such as *You have carte blanche,* with no narrative
    around them. This storytelling style switches freely from being narrative to dialogue-driven.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有很多对话，如*你有全权*，周围没有叙述。这种叙事风格可以自由地从叙述切换到对话驱动。
- en: The grammar and vocabulary is slightly unusual because of the time when Doyle
    wrote.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于道尔写作的时代，语法和词汇稍微有些不寻常。
- en: 'These subjective observations are helpful in understanding the nature and edge
    cases in your text. Let''s move on and load the book into Python for processing:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些主观观察有助于理解文本的性质和边缘情况。让我们继续，并将这本书加载到Python中进行处理：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This returns the first five characters:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了前五个字符：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Let's quickly verify that we have loaded the data into useful data types.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速验证一下我们是否已将数据加载到有用的数据类型中。
- en: 'To check our own data types, use the following command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查我们的数据类型，请使用以下命令：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding command returns the following output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令返回以下输出：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There is a major improvement between Py2.7 and Py3.6 on how strings are handled.
    They are now all Unicode by default.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理字符串方面，Py2.7和Py3.6之间有一个重大的改进。现在它们默认都是Unicode。
- en: In Python 3, `str` are Unicode strings, and it is more convenient for the NLP
    of non-English texts.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python 3中，`str`是Unicode字符串，这对于非英语文本的NLP来说更方便。
- en: 'Here is a small relevant example to highlight the differences between the two:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个小的相关示例，以突出两种方法之间的差异：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Exploring the loaded data
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索加载的数据
- en: How many unique characters can we see?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能看到多少个独特的字符？
- en: 'For reference, ASCII has 127 characters in it, so we expect this to have, at
    most, 127 characters:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，ASCII中有127个字符，所以我们预计这最多有127个字符：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code returns the following output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码返回以下输出：
- en: '[PRE13]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For our machine learning models, we often need the words to occur as individual
    tokens or single words. Let's explain what this means in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的机器学习模型，我们通常需要单词以单个标记或单个单词的形式出现。让我们在下一节中解释这意味着什么。
- en: Tokenization
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分词
- en: Given a character sequence and a defined document unit, tokenization is the
    task of chopping it up into pieces, called tokens , perhaps at the same time throwing
    away certain characters, such as punctuation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个字符序列和一个定义的文档单元，分词是将它切分成称为标记的片段的任务，也许同时丢弃某些字符，如标点符号。
- en: 'Here is an example of tokenization:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个分词的示例：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It is, in fact, sometimes useful to distinguish between tokens and words. But
    here, for ease of understanding, we will use them interchangeably.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，有时区分标记和单词是有用的。但在这里，为了便于理解，我们将它们互换使用。
- en: We will convert the raw text into a list of words. This should preserve the
    original ordering of the text.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把原始文本转换为单词列表。这应该保留文本的原始顺序。
- en: There are several ways to do this, so let's try a few of them out. We will program
    two methods from scratch to build our intuition, and then check how spaCy handles
    tokenization.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以做到这一点，所以让我们尝试其中几种。我们将从头开始编写两个方法来建立我们的直觉，然后检查spaCy如何处理分词。
- en: Intuitive – split by whitespace
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直观 – 通过空格拆分
- en: 'The following lines of code simply segment or *split* the entire text body
    on space `'' ''`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码行简单地通过空格 `' '` 将整个文本主体进行分割或 *拆分*：
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let''s preview a rather large segment from our list of tokens:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们预览列表中的一个较大片段：
- en: '[PRE16]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The way punctuation is split here is not desirable. It often appears with the
    word itself, such as the full stop at end of `Adler**.**` and a comma being part
    of `emotions**,**`. Quite often we want words to be separated from punctuation,
    because words convey a lot more meaning than punctuation in most datasets.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里标点符号的拆分方式并不理想。它经常与单词本身一起出现，例如 `Adler**.**` 结尾的全停点和 `emotions**,**` 中的逗号。很多时候，我们希望单词与标点符号分开，因为单词在大多数数据集中比标点符号传达的意义更多。
- en: 'Let''s look at a shorter example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更短的例子：
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following is the output from the preceding code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从前面代码中得到的输出：
- en: '[PRE18]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note how the words *red-headed* were not split. This is something we may or
    may not want to keep. We will come back to this, so keep this in mind.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到单词 *red-headed* 没有被拆分。这可能是我们想要保留的，也可能不是。我们将回过头来讨论这个问题，所以请记住这一点。
- en: One way to tackle this punctuation challenge is to simply extract words and
    discard everything else. This means that we will discard all non-ASCII characters
    and punctuation.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个标点符号挑战的一种方法简单地提取单词并丢弃其他所有内容。这意味着我们将丢弃所有非ASCII字符和标点符号。
- en: The hack – splitting by word extraction
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 诡计 – 通过单词提取拆分
- en: Word extraction can be done in several ways. In turn, we can use word extraction
    for splitting the words into tokens. We will look at Regex, or Regular Expressions
    for doing word extractions. It is a pattern driven string search mechanism where
    the pattern grammar is defined by the user.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 单词提取可以通过几种方式完成。反过来，我们可以使用单词提取将单词拆分成标记。我们将探讨正则表达式，或正则表达式进行单词提取。它是一种由用户定义的模式驱动的字符串搜索机制。
- en: Introducing Regexes
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍正则表达式
- en: 'Regular expressions can be a little challenging at first, but they are very
    powerful. They are generic abstractions, and work across multiple languages beyond
    Python:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式一开始可能有点挑战性，但它们非常强大。它们是通用的抽象，并且可以在Python以外的多种语言中使用：
- en: '[PRE19]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The regular expression `\W+` means *a word character (A-Z* etc.*) repeated one
    or more times:*
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式 `\W+` 表示 *一个单词字符（A-Z等）重复一次或多次：*
- en: '[PRE20]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output of the preceding code is `(109111, 107431)`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出是 `(109111, 107431)`。
- en: 'Let’s preview the words we extracted:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们预览一下我们提取的单词：
- en: '[PRE21]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The following is the output we got from the preceding code:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从前面代码中得到的输出：
- en: '[PRE22]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We notice how `Adler` no longer has the punctuation mark alongside it. This
    is what we wanted. Mission accomplished?
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到 `Adler` 不再与标点符号一起出现。这正是我们想要的。任务完成了吗？
- en: 'What was the trade-off we made here? To understand that, let''s look at another
    example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做了什么权衡？为了理解这一点，让我们看看另一个例子：
- en: '[PRE23]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The following is the output we got from the preceding code:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从前面的代码中得到的输出：
- en: '[PRE24]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We have split `Isn''t` to `Isn` and `t`. This isn''t good if you''re working
    with, say, email or Twitter data, because you would have a lot more of these contractions
    and abbreviations. As a minor annoyance, we have an extra empty token, `''''`,
    at the end. Similarly, because we neglected punctuation, `red-headed` is broken
    into two words: `red` and `headed`. We have no straightforward way to restore
    this connection if we are only given the tokenized version.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `Isn't` 拆分为 `Isn` 和 `t`。如果你处理的是电子邮件或推特数据，这就不太好了，因为你会有很多这样的缩写和缩写。作为一个小烦恼，我们在末尾有一个额外的空标记
    `''`。同样，因为我们忽略了标点符号，`red-headed` 被拆分为两个单词：`red` 和 `headed`。如果我们只有标记化版本，我们就没有简单的方法来恢复这种联系。
- en: We can write custom rules in our tokenization strategy to cover most of these
    edge cases. Or, we can use something that has already been written for us.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在我们的分词策略中编写自定义规则来覆盖这些边缘情况的大部分。或者，我们可以使用已经为我们编写好的东西。
- en: spaCy for tokenization
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy 用于分词
- en: 'spaCy loads the English *model* using the preceding `.load` syntax. This tells
    spaCy what rules, logic, weights, and other information to use:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 使用前面的 `.load` 语法加载英语 *模型*。这告诉 spaCy 使用哪些规则、逻辑、权重和其他信息：
- en: '[PRE25]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'While we use only `''en''` or English examples in this book, spaCy supports
    these features for more languages. I have used their multi-language tokenizer
    for Hindi as well, and have been satisfied with the same:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在这本书中只使用 `'en'` 或英语示例，但 spaCy 支持更多语言的这些功能。我已使用他们的多语言分词器为印地语，并且对结果感到满意：
- en: The `%%time` syntax measures the CPU and Wall time at your runtime execution
    for the cell in a Jupyter not ebook.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`%%time` 语法测量你在 Jupyter Notebook 运行时执行的单元格的 CPU 和 Wall 时间。'
- en: '[PRE26]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This creates a spaCy object, `doc`. The object stores pre-computed linguistic
    features, including tokens. Some NLP libraries, especially in the Java and C ecosystem,
    compute linguistic features such as tokens, lemmas, and parts of speech when that
    specific function is called. Instead, spaCy computes them all at initialization
    when the `text` is passed to it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个 spaCy 对象，`doc`。该对象存储了预先计算的语料库特征，包括标记。一些 NLP 库，尤其是在 Java 和 C 生态系统中，在调用特定功能时计算语料库特征，如标记、词元和词性。相反，spaCy
    在将 `text` 传递给它时，在初始化时计算所有这些特征。
- en: spaCy pre-computes most linguistic features – all you have to do is retrieve
    them from the object.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 预先计算了大多数语料库特征——你只需要从对象中检索它们。
- en: 'We can retrieve them by calling the object iterator. In the following code,
    we call the iterator and *list* it:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过调用对象迭代器来检索它们。在下面的代码中，我们调用迭代器并将其 *列表化*：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following is the output from the preceding code:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面代码的结果：
- en: '[PRE28]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Conveniently, spaCy tokenizes all punctuation and words. They are returned
    as individual tokens. Let''s try the example that we didn''t like earlier:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 便利的是，spaCy 将所有标点符号和单词都分词。它们作为单独的标记返回。让我们尝试一下我们之前不喜欢的例子：
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here are the observations:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是观察结果：
- en: 'spaCy got the `Isn''t` split correct: `Is` and `n''t`.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy 正确地将 `Isn't` 分割为 `Is` 和 `n't`。
- en: '`red-headed` was broken into three tokens: `red`, `-`, and `headed`. Since
    the punctuation information isn''t lost, we can restore the original `red-headed`
    token if we want to.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`red-headed` 被分割成三个标记：`red`、`-` 和 `headed`。由于标点信息没有丢失，如果我们想的话，可以恢复原始的 `red-headed`
    标记。'
- en: How does the spaCy tokenizer work?
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy 分词器是如何工作的？
- en: The simplest explanation is from the spaCy docs ([spacy-101](https://spacy.io/usage/spacy-101))
    itself.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的解释来自 spaCy 文档（[spacy-101](https://spacy.io/usage/spacy-101)）本身。
- en: 'First, the raw text is split on whitespace characters, similar to text.split
    (`'' ''`). Then, the tokenizer processes the text from left to right. On each
    substring, it performs two checks:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，原始文本在空白字符上分割，类似于 text.split (`' '`)。然后，分词器从左到右处理文本。在每个子字符串上，它执行两个检查：
- en: '*Does the substring match a tokenizer exception rule?* For example, *don''t*
    does not contain whitespace, but should be split into two tokens, *do* and *n''t*,
    while *U.K.* should always remain one token.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*子字符串是否匹配分词器的异常规则？* 例如，*don''t* 不包含空格，但应该分割成两个标记，*do* 和 *n''t*，而 *U.K.* 应始终作为一个标记。'
- en: '*Can a prefix, suffix, or infix be split off?* For example, punctuation such
    as commas, periods, hyphens, or quotes:'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*前缀、后缀或中缀可以被分割开吗？* 例如，如逗号、句号、连字符或引号之类的标点符号：'
- en: '![](img/3c84ff70-690a-4ebb-b74b-29065083cf1d.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c84ff70-690a-4ebb-b74b-29065083cf1d.png)'
- en: Sentence tokenization
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 句子分词
- en: 'We can also use spaCy to extract one sentence at a time, instead of one word
    at a time:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用 spaCy 一次提取一句话，而不是一次提取一个单词：
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following is the output from the preceding code:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是前面代码的结果：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Stop words removal and case change
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 停用词去除和大小写转换
- en: These simple ideas are widespread and fairly effective for a lot of tasks. They
    are particularly useful in reducing the number of unique tokens in a document
    for your processing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的想法在许多任务中都很普遍，并且相当有效。它们特别有助于在处理文档时减少唯一标记的数量。
- en: 'spaCy has already marked each token as a stop word or not and stored it in
    the `is_stop` attribute of each token. This makes it very handy for text cleaning.
    Let''s take a quick look:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 已经将每个标记标记为停用词或非停用词，并将其存储在每个标记的 `is_stop` 属性中。这使得它在文本清理中非常方便。让我们快速看一下：
- en: '[PRE32]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Getting back to our Sherlock example, let’s take a look at the first few lines
    and whether they count as stop words or not:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的夏洛克示例，让我们看看前几行是否算作停用词：
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Interesting – while *the* and *of* were marked as stop words, `THE` and `OF`
    were not. This is not a bug, but by design. spaCy doesn't remove words that are
    different because of their capitals or title case automatically.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，虽然*the*和*of*被标记为停用词，但`THE`和`OF`没有被标记。这不是一个错误，而是设计上的选择。spaCy不会自动移除因为大小写或标题化而不同的单词。
- en: 'Instead, we can force this behavior by converting our original text to lowercase
    before we pass it to spaCy:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以在将原始文本传递给spaCy之前将其转换为小写，以强制这种行为：
- en: '[PRE34]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s look at what stop words exist in the spaCy dictionary, and then how
    to extend the same programmatically:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看spaCy字典中存在哪些停用词，然后如何以编程方式扩展：
- en: '[PRE35]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We want to expand the stop words dictionary according to our domain and problem.
    For instance, if you were using this code to process the text of an NLP book,
    we might want to add words such as `NLP`, *Processing*, `AGI`, *Data,* and so
    on to the stop words list.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望根据我们的领域和问题来扩展停用词字典。例如，如果你使用这段代码来处理NLP书籍的文本，我们可能希望将诸如`NLP`、*Processing*、`AGI`、*Data*等词添加到停用词列表中。
- en: 'spaCy has an intuitive `.add()` API to do this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy有一个直观的`.add()` API来做这件事：
- en: '[PRE36]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let''s try running the same example as earlier with these added stop words:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试运行之前相同的示例，并添加这些停用词：
- en: '[PRE37]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is the output from running the preceding code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从运行前面的代码中得到的输出：
- en: '[PRE38]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Exactly as expected, `NLP` and `AGI` are now marked as stop words too.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，`NLP`和`AGI`现在也被标记为停用词。
- en: Let's pull out string tokens which are not stop words into a Python list or
    similar data structure.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提取出不是停用词的字符串标记到一个Python列表或类似的数据结构中。
- en: 'Some NLP tasks that come after text pre-processing expect string tokens and
    not spaCy token objects as a datatype. Removing both stop words and punctuation
    here for demonstration:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一些在文本预处理之后的NLP任务期望字符串标记而不是spaCy标记对象作为数据类型。为了演示，这里同时移除停用词和标点符号：
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Or just removing stop words, while retaining punctuation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 或者只是移除停用词，同时保留标点符号：
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Stemming and lemmatization
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词干提取和词形还原
- en: Stemming and lemmatization are very two very popular ideas that are used to
    reduce the vocabulary size of your corpus.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取和词形还原是非常流行的两种方法，用于减少语料库的词汇量。
- en: '**Stemming** usually refers to a crude heuristic process that chops off the
    ends of words in the hope of achieving this goal correctly most of the time, and
    often includes the removal of derivational affixes.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**词干提取**通常指的是一种粗略的启发式过程，它通过截断单词的末尾来希望大多数时候正确地实现这一目标，并且通常包括移除派生词缀。'
- en: '**Lemmatization** usually refers to doing things properly with the use of a
    vocabulary and morphological analysis of words, normally aiming to remove inflectional
    endings only and to return the base or dictionary form of a word, which is known
    as the lemma.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**词形还原**通常指的是使用词汇和词形分析来正确地处理事物，通常旨在仅删除屈折词尾，并返回单词的基本或词典形式，这被称为词元。'
- en: If confronted with the token saw, stemming might return just s, whereas lemmatization
    would attempt to return either see or saw, depending on whether the use of the
    token was as a verb or a noun.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到token saw，词干提取可能只返回s，而词形还原会尝试返回see或saw，这取决于token的使用是作为动词还是名词。
- en: '- Dr. Christopher Manning et al, 2008, [[IR-Book](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)]'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '- 克里斯托弗·曼宁博士等，2008年，[[IR-Book](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)]'
- en: (Chris Manning is a Professor in machine learning at the Departments of Computer
    Science and Linguistics at Stanford University)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: (克里斯·曼宁是斯坦福大学计算机科学和语言学系的机器学习教授)
- en: spaCy for lemmatization
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy用于词形还原
- en: 'spaCy only supports lemmatization. As discussed by spaCy creator Matt Honnibal
    in [issue #327](https://github.com/explosion/spaCy/issues/327) on GitHub, stemmers
    are rarely a good idea.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy仅支持词形还原。如spaCy创建者马特·霍尼巴尔在GitHub上的[问题#327](https://github.com/explosion/spaCy/issues/327)中讨论的那样，词干提取器很少是一个好主意。
- en: We want to treat `meet/NOUN` differently from `meeting/VERB`. Unlike Stanford
    NLTK, which was created to *teach and introduce* as many NLP ideas as possible,
    spaCy takes an opinionated stand against stemming.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将`meet/NOUN`与`meeting/VERB`区别对待。与旨在*教授和介绍*尽可能多的NLP思想的斯坦福NLTK不同，spaCy对词干提取持反对意见。
- en: spaCy does lemmatization for you by default when you process the text with the
    `nlp` object. This information is stored in the `lemma` attribute for each token.
    spaCy stores the internal hash or identifier, which spaCy stores in `token.lemma`.
    This numerical hash has no meaning for us. This numerical representation helps
    spaCy access and manipulate information much faster than its other Pythonic components.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `nlp` 对象处理文本时，spaCy 会默认为你进行词形还原。这些信息存储在每个标记的 `lemma` 属性中。spaCy 存储内部哈希或标识符，它存储在
    `token.lemma` 中。这个数字哈希对我们没有意义。这种数字表示有助于 spaCy 比其其他 Python 组件更快地访问和操作信息。
- en: 'An underscore at the attribute end, such as `lemma_`, tells spaCy that we are
    looking for something that is human-readable:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 属性末尾的下划线，例如 `lemma_`，告诉 spaCy 我们正在寻找的是可读性强的内容：
- en: '[PRE41]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: There's quite a few things going on here. Let's discuss them.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有很多事情在进行中。让我们来讨论它们。
- en: -PRON-
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: -PRON-
- en: 'spaCy has a slightly annoying lemma (recall that lemma is the output of lemmatization):
    -PRON-. This is used as the lemma for all pronouns such as `Their`, `you`, `me`,
    and `I`. Other NLP tools lemmatize these to `I` instead of a placeholder, such
    as `-PRON-`.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 'spaCy 有一个稍微有些令人烦恼的词形还原（回想一下，词形还原是词形还原的输出）: -PRON-。这被用作所有代词（如 `Their`、`you`、`me`
    和 `I`）的词形。其他 NLP 工具将这些词形还原为 `I` 而不是占位符，如 `-PRON-`。'
- en: Case-insensitive
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不区分大小写
- en: While checking for stop words, spaCy did not automatically lowercase our input.
    On the other hand, lemmatization does this for us. It converted "Apple" to "apple"
    and "Banana" to "banana".
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查停用词时，spaCy 并没有自动将我们的输入转换为小写。另一方面，词形还原为我们做了这件事。它将 "Apple" 转换为 "apple"，将 "Banana"
    转换为 "banana"。
- en: This is one of the ways spaCy makes our lives easier, though slightly inconsistent.
    While removing stop words, we want to preserve THE in "THE ADVENTURES OF SHERLOCK
    HOLMES" while removing *the* in "the street was black". The opposite is usually
    true in lemmatization; we care more about how the word was used in context and
    use a proper lemma accordingly.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 spaCy 使我们的生活变得更轻松的一种方式，尽管有些不一致。当我们移除停用词时，我们希望保留 "THE ADVENTURES OF SHERLOCK
    HOLMES" 中的 THE，同时移除 "the street was black" 中的 *the*。在词形还原中，通常情况相反；我们更关心词语在上下文中的使用方式，并相应地使用正确的词形。
- en: Conversion – meeting to meet
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换 - 会议到会议
- en: Lemmatization is aware of the linguistic role that words play in context. "Meeting"
    is converted to "meet" because it's a verb. spaCy does expose part of speech tagging
    and other linguistic features for us to use. We will learn how to query those
    soon.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 词形还原（Lemmatization）意识到了词语在上下文中的语言角色。"Meeting" 转换为 "meet" 因为它是一个动词。spaCy 为我们提供了部分词性标注和其他语言特征供我们使用。我们很快就会学习如何查询这些信息。
- en: spaCy compared with NLTK and CoreNLP
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy 与 NLTK 和 CoreNLP 的比较
- en: 'The following is a comparison of the NLTK and CoreNLP:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对 NLTK 和 CoreNLP 的比较：
- en: '| **Feature** | **Spacy** | **NLTK** | **CoreNLP** |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **Spacy** | **NLTK** | **CoreNLP** |'
- en: '| Native Python support/API | Y | Y | Y |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 原生 Python 支持/API | Y | Y | Y |'
- en: '| Multi-language support | Y | Y | Y |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 多语言支持 | Y | Y | Y |'
- en: '| Tokenization | Y | Y | Y |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 分词 | Y | Y | Y |'
- en: '| Part-of-speech tagging | Y | Y | Y |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 词性标注 | Y | Y | Y |'
- en: '| Sentence segmentation | Y | Y | Y |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 句子分割 | Y | Y | Y |'
- en: '| Dependency parsing | Y | N | Y |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 依存句法分析 | Y | N | Y |'
- en: '| Entity recognition | Y | Y | Y |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 实体识别 | Y | Y | Y |'
- en: '| Integrated word vectors | Y | N | N |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 集成词向量 | Y | N | N |'
- en: '| Sentiment analysis | Y | Y | Y |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 情感分析 | Y | Y | Y |'
- en: '| Coreference resolution | N | N | Y |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 语义消歧 | N | N | Y |'
- en: Correcting spelling
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 修正拼写
- en: One of the most frequently seen text challenges is correcting spelling errors.
    This is all the more true when data is entered by casual human users, for instance,
    shipping addresses or similar.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的文本挑战之一是纠正拼写错误。当数据由非专业用户输入时，这一点尤其正确，例如，运输地址或类似内容。
- en: Let's look at an example. We want to correct Gujrat, Gujart, and other minor
    misspellings to Gujarat. There are several good ways to do this, depending on
    your dataset and level of expertise. We will discuss two or three popular ways,
    and discuss their pros and cons.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们希望将 Gujrat、Gujart 和其他小错误拼写更正为 Gujarat。根据你的数据集和专业知识水平，有几种好的方法可以做到这一点。我们将讨论两种或三种流行的方法，并讨论它们的优缺点。
- en: Before I begin, we need to pay homage to the legendary [Peter Norvig's Spell
    Correct](https://norvig.com/spell-correct.html). It's still worth a read on how
    to *think* about solving a problem and *exploring* implementations. Even the way
    he refactors his code and writes functions is educational.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在我开始之前，我们需要向传奇人物 [Peter Norvig 的拼写纠正](https://norvig.com/spell-correct.html)
    表示敬意。关于如何 *思考* 解决问题和 *探索* 实现方法，它仍然值得一读。即使他重构代码和编写函数的方式也是教育性的。
- en: 'His spell-correction module is not the simplest or best way of doing this.
    I recommend two packages: one with a bias toward simplicity, one with a bias toward
    giving you all the knives, bells, and whistles to try:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 他的拼写纠正模块不是最简单或最好的方法。我推荐两个包：一个侧重于简单性，一个侧重于给您所有刀、铃和哨子来尝试：
- en: '**[FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy)** is easy to use. It
    gives a simple similarity score between two strings, capped to 100\. Higher numbers
    mean that the words are more similar.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[FuzzyWuzzy](https://github.com/seatgeek/fuzzywuzzy)** 使用简单。它给出两个字符串之间简单的相似度评分，上限为
    100。数字越高，表示单词越相似。'
- en: '**[Jellyfish](https://github.com/jamesturk/jellyfish)** supports six edit distance
    functions and four phonetic encoding options that you can use as per your use
    case.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**[Jellyfish](https://github.com/jamesturk/jellyfish)** 支持六种编辑距离函数和四种音标编码选项，您可以根据您的用例使用它们。'
- en: FuzzyWuzzy
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FuzzyWuzzy
- en: Let's see how we can use FuzzyWuzzy to correct our misspellings.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何使用 FuzzyWuzzy 来纠正我们的拼写错误。
- en: 'Use the following code to install FuzzyWuzzy on your machine:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码在您的机器上安装 FuzzyWuzzy：
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'FuzzyWuzzy has two main modules that will come in useful: fuzz and process.
    Let''s import fuzz first:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: FuzzyWuzzy 有两个主要的模块将非常有用：fuzz 和 process。让我们首先导入 fuzz：
- en: '[PRE43]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can see how the ratio function is confused by the trailing `Bangalore` used
    in the preceding address, but really the two strings refer to the same address/entity.
    This is captured by `partial_ratio`.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，比率函数被前面地址中使用的尾随 `Bangalore` 搅乱了，但实际上这两个字符串指的是同一个地址/实体。这被 `partial_ratio`
    捕获。
- en: 'Do you see how both `ratio` and `partial_ratio` are sensitive to the ordering
    of the words? This is useful for comparing addresses that follow some order. On
    the other hand, if we want to compare something else, for example, person names,
    it might give counter-intuitive results:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到没有，`ratio` 和 `partial_ratio` 都对单词的顺序很敏感？这对于比较遵循某种顺序的地址很有用。另一方面，如果我们想比较其他东西，例如人名，可能会得到反直觉的结果：
- en: '[PRE44]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As you can see, just because we had an extra `D.` token, our logic is not applicable
    anymore. We want something that is less order-sensitive. The authors of FuzzyWuzzy
    have us covered.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，仅仅因为我们有一个额外的 `D.` 标记，我们的逻辑就不再适用了。我们想要的是对顺序不那么敏感的东西。FuzzyWuzzy 的作者已经为我们解决了这个问题。
- en: 'FuzzyWuzzy supports functions that tokenize our input on space and remove punctuation,
    numbers, and non-ASCII characters. This is then used to calculate similarity.
    Let''s try this out:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: FuzzyWuzzy 支持将我们的输入在空格上进行标记化，并删除标点符号、数字和非 ASCII 字符的功能。然后这被用来计算相似度。让我们试试这个：
- en: '[PRE45]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This will work perfectly for us. In case we have a list of options and we want
    to find the closest match(es), we can use the process module:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们来说将完美无缺。如果我们有一个选项列表，并且我们想找到最接近的匹配项，我们可以使用 process 模块：
- en: '[PRE46]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s look at another example. Here, we have `Bangalore` misspelled as `Banglore`
    – we are missing an `a`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看另一个例子。在这里，我们将 `Bangalore` 拼写错误为 `Banglore` – 我们缺少一个 `a`：
- en: '[PRE47]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s take an example of a common search typo in online shopping. Users have
    misspelled `chilli` as `chili`; note the missing `l`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个在线购物中常见的搜索拼写错误为例。用户将 `chilli` 拼写为 `chili`；注意缺少的 `l`：
- en: '[PRE48]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Jellyfish
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jellyfish
- en: 'Jellyfish supports reasonably fast implementations of almost all popular edit
    distance functions (Recall how the edit distance functions tell you how similar
    two sequences/strings are). While FuzzyWuzzy supported mainly Levenshtein distance,
    this package supports some more string comparison utilities:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Jellyfish 支持几乎所有流行编辑距离函数的合理快速实现（回想一下编辑距离函数是如何告诉你两个序列/字符串相似度的）。虽然 FuzzyWuzzy
    主要支持 Levenshtein 距离，但这个包支持一些更多的字符串比较工具：
- en: Levenshtein distance
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levenshtein 距离
- en: Damerau-Levenshtein distance
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Damerau-Levenshtein 距离
- en: Jaro distance
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaro 距离
- en: Jaro-Winkler distance
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaro-Winkler 距离
- en: Match rating approach comparison
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匹配评分方法比较
- en: Hamming distance
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamming 距离
- en: Additionally, it supports **phonetic encodings** for English.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还支持英语的**音标编码**。
- en: 'Use the following code to install Jellyfish on your machine:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码在您的机器上安装 Jellyfish：
- en: '[PRE49]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s try importing the package and setting up some examples to try out:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试导入包并设置一些示例来尝试：
- en: '[PRE50]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We want to try multiple distance functions with all of our examples. The smarter
    thing to do is build a utility function for this. Let''s do that now:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望尝试所有示例中的多个距离函数。更聪明的方法是为此构建一个实用函数。我们现在就来做这件事：
- en: '[PRE51]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note that `calculate_distance` takes the distance function as input. We can
    leave `examples` as implicitly picked from what we had declared previously in
    the global namespace.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`calculate_distance`函数接受距离函数作为输入。我们可以将`examples`保留为从我们在全局命名空间中之前声明的什么中隐式选择。
- en: 'Levenshtein distance, which is probably the most famous string similarity function,
    is sometimes synonymous with edit distance function, but we consider this to be
    a particular implementation of the edit distance family of functions:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Levenshtein距离，这可能是最著名的字符串相似度函数，有时与编辑距离函数同义，但我们将这视为编辑距离函数家族的一个特定实现：
- en: '[PRE52]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The Damerau–Levenshtein distance adds transpositions to the Levenshtein edit
    operations of insertion, deletion, and substitution. Let''s try this out and see
    if it changes anything for us:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Damerau–Levenshtein距离在Levenshtein编辑操作（插入、删除和替换）中添加了交换。让我们尝试这个，看看是否对我们有什么影响：
- en: '[PRE53]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We note that the `Narendra` and `Narendar` distance value changed from `3` to
    `2`. This is because we now count at least `a` to be transposed with `r` or vice
    versa. The other character is a substitution, so 1+1 = 2\.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到`Narendra`和`Narendar`的距离值从`3`变为`2`。这是因为我们现在至少将`a`与`r`或反之进行交换。其他字符是替换，所以1+1=2。
- en: 'The next distance function that we will try is hamming distance. This counts
    the minimum number of substitutions required to change one string into the other:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要尝试的距离函数是汉明距离。这计算了将一个字符串转换为另一个字符串所需的最小替换次数：
- en: '[PRE54]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '**Jaro and Jaro-Winkler** return a value of similarity – and not dissimilarity.
    This means that the perfect match returns 1.0 and a totally unrelated match would
    tend to be 0:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jaro和Jaro-Winkler**返回相似度值——而不是不相似度。这意味着完美匹配返回1.0，而完全不相关的匹配往往会接近0：'
- en: '[PRE55]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Trying the other variation of Jaro similarity, that is, Jaro-Winkler, we get
    the following:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试Jaro相似度的另一种变体，即Jaro-Winkler，我们得到以下结果：
- en: '[PRE56]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: These are extremely useful and diverse techniques. Yet, their overemphasis on
    written text creates one problem that is unique to English. We don't write English
    in the same way we speak. This means that we do not capture the range of all similarities.
    To solve this challenge, which is typically encountered in chatbots used by non-native
    English speakers, we can look at the phonetic similarity of words, which is what
    we will do next.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术非常实用且多样化。然而，它们对书面文本的过度强调为英语创造了一个独特的问题。我们不会像说话那样写英语。这意味着我们没有捕捉到所有相似性的范围。为了解决这个挑战，这是在非母语英语使用者的聊天机器人中通常遇到的挑战，我们可以查看单词的音位相似性，这正是我们接下来要做的。
- en: Phonetic word similarity
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音位词相似度
- en: The way we say a word makes up its phonetics. Phonetics is the information of
    speech sounds. For instance, soul and sole sound identical in a lot of British-derived
    accents, such as Indian accents.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说一个词的方式构成了它的音位。音位是语音信息。例如，在许多源自英国口音中，如印度口音中，soul和sole听起来相同。
- en: Quite often, words might be misspelled a little bit because the typist was trying
    to make it *sound right*. In this case, we leverage this phonetic information
    to map this typo back to the correct spelling.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 很常见，单词可能因为打字员试图让它**听起来正确**而稍微拼错。在这种情况下，我们利用这种音位信息将这个错误拼写的单词映射回正确的拼写。
- en: What is a phonetic encoding?
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是音位编码？
- en: We can convert a word into a representation of its pronunciation. Of course,
    this might vary by accents, and by the conversion technique as well.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将一个单词转换为其发音的表示。当然，这可能会因口音和转换技术而有所不同。
- en: 'Yet, over time, two or three popular ways have emerged so that we can do this.
    Each of these methods takes a single string and returns a coded representation.
    I encourage you to Google each of these terms:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着时间的推移，已经出现了两种或三种流行的方法，我们可以这样做。每种方法都接受一个字符串并返回一个编码表示。我鼓励您在Google上搜索这些术语：
- en: '**American Soundex (the 1930s)**: Implemented in popular database software
    such as PostgreSQL, MySQL, and SQLite'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**美国Soundex（20世纪30年代**）：在流行的数据库软件中实现，如PostgreSQL、MySQL和SQLite'
- en: '**NYSIIS (New York State Identification and Intelligence System) (the 1970s)**'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NYSIIS（纽约州身份和情报系统）（20世纪70年代**）'
- en: Metaphone (the 1990s)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaphone（20世纪90年代）
- en: '**Match rating codex (the early 2000s)**'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**匹配评分码（20世纪初期**）'
- en: 'Let''s take a quick preview of the same:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速预览一下同样的内容：
- en: '[PRE57]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'For NYSIIS, we will use the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NYSIIS，我们将使用以下方法：
- en: '[PRE58]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Using the slightly more updated metaphone, we get the following output:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 使用稍微更新一点的 metaphone，我们得到以下输出：
- en: '[PRE59]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The matching rate codex gives us the following output:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配率编码器给出了以下输出：
- en: '[PRE60]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We can now use the string comparison utility that we saw earlier to compare
    two strings phonetically.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用之前看到的字符串比较工具来比较两个字符串的音位。
- en: '[PRE61]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'For instance, `write` and `right`should have zero phonetic Levenshtein distance
    because they are pronounced in the same way. Let''s try this out:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`write` 和 `right` 应该具有零音位 Levenshtein 距离，因为它们的发音相同。让我们来试试看：
- en: '[PRE62]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'This worked as expected. Let''s add some examples to our old examples list:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这正如预期的那样工作。让我们将一些例子添加到我们的旧例子列表中：
- en: '[PRE63]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Let''s encapsulate this into a utility function, like we did earlier. We will
    use two function parameters now: `phonetic_func` and `distance_func`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个封装成一个实用函数，就像我们之前做的那样。现在我们将使用两个函数参数：`phonetic_func` 和 `distance_func`：
- en: '[PRE64]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This returns the following table:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下表格：
- en: '[PRE65]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Note that Delhi and Dilli are separated, which is not nice. On the other hand,
    Narendra and Narendar are marked as similar to zero edit distance, which is quite
    cool. Let's try a different technique and see how it goes.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意德里（Delhi）和达利（Dilli）被分开，这并不好。另一方面，纳伦德拉（Narendra）和纳伦达尔（Narendar）被标记为零编辑距离相似，这相当酷。让我们尝试不同的技术，看看效果如何。
- en: '**American soundex**'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**美国 Soundex**'
- en: We note that the Soundex is aware of common similar-sounding words and gives
    them separate phonetic encoding. This allows us to separate `right` from `write`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到 Soundex 能够识别常见的发音相似的单词，并为它们提供单独的音位编码。这使得我们可以区分 `right` 和 `write`。
- en: 'This will only work on American/English words though. Indian sounds such as
    `Narendra Modi` and `Narendra D. Modi` are now considered similar:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这只适用于美国/英语单词。例如，印度名字如 `Narendra Modi` 和 `Narendra D. Modi` 现在被认为是相似的：
- en: '[PRE66]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Note the changes from the previous code in the following table:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下表格中与之前代码的变化：
- en: '[PRE67]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Runtime complexity
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行时间复杂度
- en: We now have the ability to find the correct spellings of words or mark them
    as similar. While processing a large corpus, we can extract all unique words and
    compare each token against every other token.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有找到单词正确拼写或标记它们为相似的能力。在处理大型语料库时，我们可以提取所有唯一的单词，并将每个标记与每个其他标记进行比较。
- en: It would take O(n²), where *n* is the number of unique tokens in a corpus. This
    might make the process too slow for a large corpus.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要 O(n²)，其中 *n* 是语料库中唯一标记的数量。这可能会使大型语料库的处理过程变得太慢。
- en: The alternative is to use a standard dictionary and expand the same for your
    corpus. If the dictionary has *m* unique words, this process now will be O(m∗n).
    Assuming that m<<n*m<<n², this will be much faster than the previous approach.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是使用标准词典，并扩展你的语料库。如果词典有 *m* 个独特的单词，这个过程现在将是 O(m∗n)。假设 m << n，这将比之前的方法快得多。
- en: Cleaning a corpus with FlashText
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FlashText 清理语料库
- en: But what about a web-scale corpus with millions of documents and a few thousand
    keywords? Regex can take several days to run over such exact searches because
    of its linear time complexity. How can we improve this?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于一个包含数百万份文档和数千个关键词的网页规模语料库呢？由于正则表达式的线性时间复杂度，它可能需要几天时间才能完成这样的精确搜索。我们如何改进这一点？
- en: 'We can use FlashText for this very specific use case:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 FlashText 来处理这个非常具体的用例：
- en: A few million documents with a few thousand keywords
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几百万份文档和数千个关键词
- en: Exact keyword matches – either by replacing or searching for the presence of
    those keywords
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确关键词匹配 - 要么替换，要么搜索这些关键词的存在
- en: Of course, there are several different possible solutions to this problem. I
    recommend this for its simplicity and focus on solving one problem. It does not
    require us to learn new syntax or set up specific tools such as ElasticSearch.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个问题有几种不同的可能解决方案。我推荐这个方案，因为它简单，专注于解决一个问题。它不需要我们学习新的语法或设置特定的工具，如 ElasticSearch。
- en: 'The following table gives you a comparison of using Flashtext versus compiled
    regex for searching:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了使用 Flashtext 与编译正则表达式进行搜索的比较：
- en: '![](img/a629566c-ec3d-427b-a4de-f22c8b7ee9a0.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a629566c-ec3d-427b-a4de-f22c8b7ee9a0.png)'
- en: 'The following tables gives you a comparison of using FlashText versus compiled
    regex for substitutions:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了使用 FlashText 与编译正则表达式进行替换的比较：
- en: '![](img/d090d72d-edef-4635-9035-0979e2af849c.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d090d72d-edef-4635-9035-0979e2af849c.png)'
- en: We note that while the time taken by Regex scales almost linearly, Flashtext
    is relatively flat. Now, we know that we need Flashtext for speed and scale. FlashText
    has seen a lot of love from the community. Adopters include [NLProc](https://github.com/NIHOPA/NLPre)
    – the NLP Preprocessing Toolkit from the National Institute of Health.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，虽然正则表达式的运行时间几乎呈线性增长，但Flashtext则相对平稳。现在，我们知道我们需要Flashtext来提高速度和扩展性。FlashText得到了社区的广泛喜爱。使用者包括[NLProc](https://github.com/NIHOPA/NLPre)
    – 来自国家卫生研究院的自然语言处理预处理工具包。
- en: Follow these instructions to install FlashText onto your machine.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下说明将FlashText安装到您的机器上。
- en: 'First, we will install pip on our conda environment. We will do this from our
    notebook:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将在我们的conda环境中安装pip。我们将从我们的笔记本中这样做：
- en: '[PRE68]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The FlashText source code is available on GitHub ([https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02](https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02)),
    and the documents are pretty easy to navigate and use. We will only consider two
    basic examples here. Let''s figure out the syntax for finding keywords that exist
    in a corpus:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: FlashText的源代码可在GitHub上找到（[https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02](https://github.com/PacktPublishing/Natural-Language-Processing-with-Python-Quick-Start/tree/master/Chapter02)），文档导航和使用都很简单。在这里，我们只考虑两个基本示例。让我们找出在语料库中查找关键词的语法：
- en: '[PRE69]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: How about we replace them now?
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们就替换它们怎么样？
- en: '[PRE70]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: Unfortunately, FlashText only supports English for the time being. Regex can
    search for keywords based on special characters such as `^,$,*,\d`, which are
    not supported in FlashText. So, to match partial words such as `word\dvec`, we
    would still have to use regex. However, FlashText is still excellent for extracting
    complete words like `word2vec`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前FlashText只支持英语。正则表达式可以根据特殊字符如`^,$,*,\d`来搜索关键词，这些在FlashText中是不支持的。因此，为了匹配像`word\dvec`这样的部分单词，我们仍然需要使用正则表达式。然而，FlashText在提取完整的单词如`word2vec`方面仍然非常出色。
- en: Summary
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered a lot of new ground. We started by performing linguistic
    processing on our text. We met **spaCy**, which we will continue to dive deeper
    into as we move on in this book. We covered the following foundational ideas from
    linguistics, tokenization doing this with and without spaCy, stop word removal,
    case standardization, lemmatization (we skipped stemming) – using spaCy and its
    peculiarities such as*-PRON-*
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了大量的新内容。我们首先对文本进行了语言处理。我们遇到了**spaCy**，随着我们在本书中的深入，我们将继续深入研究。我们涵盖了以下语言学基础概念：分词（使用和未使用spaCy进行），去除停用词，大小写标准化，词形还原（我们跳过了词干提取）
    – 使用spaCy及其特性，如`*-PRON-*`
- en: But what do we do with spaCy, other than text cleaning? Can we build something?
    Yes!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们除了进行文本清理之外，还能用spaCy做什么？我们能构建一些东西吗？当然可以！
- en: Not only can we extend our simple linguistics based text cleaning using spaCy
    pipelines but also do parts of speech tagging, named entity recognition, and other
    common tasks. We will look at this in the next chapter.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以用spaCy的管道扩展我们的简单语言文本清理，还可以进行词性标注、命名实体识别和其他常见任务。我们将在下一章中探讨这一点。
- en: We looked at spelling correction or the closest word match problem. We discussed
    **FuzzyWuzzy** and **Jellyfish** in this context. To ensure that we can scale
    beyond more than a few hundred keywords, we also looked at **FlashText**. I encourage
    you to dive deeper into any of these excellent libraries to learn about the best
    software engineering practices.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了拼写纠正或最接近的单词匹配问题。在这个背景下，我们讨论了**FuzzyWuzzy**和**Jellyfish**。为了确保我们可以扩展到超过几百个关键词，我们还研究了**FlashText**。我鼓励您深入研究这些优秀的库，以了解最佳软件工程实践。
- en: In the next chapter, we will tie all these together with other linguistic tools
    to build an end-to-end toy program.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将与其他语言工具结合，构建一个端到端的玩具程序。
