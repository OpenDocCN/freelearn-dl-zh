["```py\nfrom random import *\nfrom math import sqrt\n\nins = 0\nn = 100000\n\nfor i in range(0, n):\n   x = (random()-.5) * 2\n    y = (random()-.5) * 2\n    if sqrt(x*x+y*y)<=1:\n        ins+=1\n\npi = 4 * ins / n\nprint(pi)\n```", "```py\nfor i in range(0, n):\n  x = (random()-.5) * 2\n  y = (random()-.5) * 2\n  if sqrt(x*x+y*y)<=1:\n   ins+=1\n```", "```py\npip install matplotlib\n```", "```py\nimport matplotlib.pyplot as plt\nfrom random import random\n\nins = 0\nn = 1000\n\nx_ins = []\ny_ins = []\nx_outs = []\ny_outs = []\n\nfor _ in range(n):\n    x = (random()-.5) * 2\n    y = (random()-.5) * 2 \n    if (x**2+y**2) <= 1:\n        ins += 1\n        x_ins.append(x)\n y_ins.append(y)\n    else:\n        x_outs.append(x)\n y_outs.append(y)\n\npi = 4 * ins/n\nprint(pi)\n\nfig, ax = plt.subplots()\nax.set_aspect('equal')\nax.scatter(x_ins, y_ins, color='g', marker='s')\nax.scatter(x_outs, y_outs, color='r', marker='s')\nplt.show()\n```", "```py\nenv = gym.make('FrozenLake8x8-v0')\npolicy = monte_carlo_e_soft(env,episodes=50000)print(test_policy(policy, env))\n```", "```py\nif not policy:\n  policy = create_random_policy(env)\nQ = create_state_action_dictionary(env, policy)\nreturns = {}\n```", "```py\ndef create_random_policy(env):\n  policy = {}\n  for key in range(0, env.observation_space.n):\n    p = {}\n    for action in range(0, env.action_space.n):\n      p[action] = 1 / env.action_space.n\n      policy[key] = p\n  return policy\n```", "```py\ndef create_state_action_dictionary(env, policy):\n  Q = {}\n  for key in policy.keys():\n    Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n  return Q\n```", "```py\nfor e in range(episodes): \n  G = 0 \n  episode = play_game(env=env, policy=policy, display=False)\n  evaluate_policy_check(env, e, policy, test_policy_freq)\n```", "```py\nepisode = play_game(env=env, policy=policy, display=True)\n```", "```py\ndef play_game(env, policy, display=True):\n  env.reset()\n  episode = []\n  finished = False\n  while not finished:\n    s = env.env.s\n    if display:\n      clear_output(True)\n      env.render()\n      sleep(1)\n    timestep = []\n    timestep.append(s)\n    n = random.uniform(0, sum(policy[s].values()))\n    top_range = 0\n    action = 0\n    for prob in policy[s].items():\n      top_range += prob[1]            \n      if n < top_range:\n        action = prob[0]\n        break \n    state, reward, finished, info = env.step(action)\n\n    timestep.append(action)\n    timestep.append(reward)\n    episode.append(timestep)\n if display:\n   clear_output(True)\n   env.render()\n   sleep(1)\n return episode\n```", "```py\nwhile not finished:\n```", "```py\ns = env.env.s\nif display:\n clear_output(True)\n env.render()\n sleep(1)\ntimestep = []\ntimestep.append(s)\n```", "```py\nn = random.uniform(0, sum(policy[s].values())) top_range = 0\naction = 0\nfor prob in policy[s].items():\n  top_range += prob[1] \n  if n < top_range:\n    action = prob[0]\n    break \n```", "```py\nstate, reward, finished, info = env.step(action) timestep.append(action)\ntimestep.append(reward)\nepisode.append(timestep)\n```", "```py\nepisode = play_game(env=env, policy=policy, display=False)\nevaluate_policy_check(env, e, policy, test_policy_freq)\n```", "```py\ndef test_policy(policy, env):\n  wins = 0\n  r = 100\n  for i in range(r):\n    w = play_game(env, policy, display=False)[-1][-1]\n    if w == 1:\n      wins += 1\n  return wins / r\n```", "```py\nfor i in reversed(range(0, len(episode))):\n  s_t, a_t, r_t = episode[i] \n  state_action = (s_t, a_t)\n  G += r_t\n```", "```py\nif not state_action in [(x[0], x[1]) for x in episode[0:i]]:\n  if returns.get(state_action):\n    returns[state_action].append(G)\n  else:\n    returns[state_action] = [G] \n\n  Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) \n  Q_list = list(map(lambda x: x[1], Q[s_t].items())) \n  indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n  max_Q = random.choice(indices) \nA_star = max_Q\n```", "```py\nfor a in policy[s_t].items(): \n  if a[0] == A_star:\n    policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n  else:\n    policy[s_t][a[0]] = (epsilong / abs(sum(policy[s_t].values())))\n```", "```py\npolicy[s_t][a[0]] = 1 - alpha + (alpha / abs(sum(policy[s_t].values())))\n```", "```py\npolicy[s_t][a[0]] = (alpha / abs(sum(policy[s_t].values())))\n```"]