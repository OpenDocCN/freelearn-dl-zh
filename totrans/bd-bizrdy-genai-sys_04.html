<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer139">
    <h1 class="chapterNumber"><a id="_idTextAnchor110"/>4</h1>
    <h1 class="chapterTitle" id="_idParaDest-106"><a id="_idTextAnchor111"/>Building the AI Controller Orchestration Interface</h1>
    <p class="normal">Businesses today need to design, produce, and deliver goods and services at a speed never attained before. Responsiveness has become key in nearly every field, from online cloud services to delivering food, medication, clothing, and so on. Such an event-driven economy produces an endless stream of tasks, and only an equally event-driven, human-centered <strong class="keyWord">generative AI system</strong> (<strong class="keyWord">GenAISys</strong>) can keep pace.</p>
    <p class="normal">Human judgment still anchors even the most automated workflows: when fires break out, storms destroy infrastructure, or supply chains falter, teams—not algorithms alone—must act. An advanced GenAISys that leaves people out of the loop is a myth. This chapter, therefore, begins by outlining an architecture that tears down the walls between users and AI to create a collaborative, multi-user chatbot.</p>
    <p class="normal">First, we sketch the event-driven GenAISys interface at a high level, showing how the building blocks from earlier chapters—short-term, episodic, and long-term memory, the multi-turn conversational agent, and twin RAG pipelines for instruction scenarios and data—fit together. To then implement the responsive system, we must code the processes of the GenAISys and then the conversational agent that will manage the generative AI agent. Once our GenAISys interface is built, we will run a multi-user, multi-turn conversation with three users working in an online travel agency. Their online meeting will include a conversational AI agent as a participant. </p>
    <p class="normal">These users will be able to have an online meeting with or without the AI agent. They will be able to utilize RAG to find instruction scenarios or simply ask the generative AI agent to answer a question. By the end of the chapter, we will have a fully working GenAISys interface ready for the multimodal chain-of-thought extensions in <a href="Chapter_5.xhtml#_idTextAnchor140"><em class="italic">Chapter 5</em></a>.</p>
    <p class="normal">In a nutshell, this chapter covers the following topics:</p>
    <ul>
      <li class="bulletList">A high-level view of the architecture of an event-driven GenAISys interface</li>
      <li class="bulletList">A low-level, hands-on flowchart of the GenAISys interface</li>
      <li class="bulletList">Implementing response widgets for inputs, the AI agent, and active users</li>
      <li class="bulletList">The chatbot’s event-driven flow in a multi-turn conversation</li>
      <li class="bulletList">Multi-user GenAISys conversation with an AI agent as a participant</li>
      <li class="bulletList">The response RAG features of the conversational agent</li>
      <li class="bulletList">The orchestration capabilities of the GenAISys interface and AI agent</li>
    </ul>
    <p class="normal">Our first task is to define an event-driven GenAISys interface.</p>
    <h1 class="heading-1" id="_idParaDest-107"><a id="_idTextAnchor112"/>Architecture of an event-driven GenAISys interface</h1>
    <p class="normal">Our event-driven GenAISys interface integrates the functionality we built in the previous chapters. The <a id="_idIndexMarker272"/>interface will leverage the flexibility of IPython widgets to create a reactive event-driven environment in which the following apply:</p>
    <ul>
      <li class="bulletList">The high-level tasks will be event-driven, triggered by user inputs</li>
      <li class="bulletList">Generative AI tasks will trigger generative AI agent functions</li>
    </ul>
    <p class="normal">We will first examine the program we are building at a high level, as represented in <em class="italic">Figure 4.1</em>:</p>
    <figure class="mediaobject"><img alt="Figure 4.1: High-level architecture of the GenAISys interface" src="../Images/B32304_04_1.png"/></figure>
    <p class="packt_figref">Figure 4.1: High-level architecture of the GenAISys interface</p>
    <p class="normal">Let’s go through the functions we have already built in the previous chapters and also list the key ones we are adding in this chapter:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">I1 – AI controller</strong>: This chapter’s main new component is the generative AI Python interface with responsive widgets, which will be run as an AI controller and orchestrator</li>
      <li class="bulletList"><strong class="keyWord">I2 – Multi-user chatbot</strong>: The chat surface through which several users interact concurrently</li>
      <li class="bulletList"><strong class="keyWord">F1 – Generative AI model</strong>: Inherited from all the previous chapters, especially <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>, in which we ran generative AI calls with GPT-4o</li>
      <li class="bulletList"><strong class="keyWord">F2 – Memory retention</strong>: Inherited from <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>, which introduced different types of memory</li>
      <li class="bulletList"><strong class="keyWord">F3 – Modular RAG</strong>: The instruction-and-data pipelines inherited from <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a></li>
      <li class="bulletList"><strong class="keyWord">F4 – Multifunctional capabilities</strong>: Semantic and sentiment analysis from <em class="italic">Chapters</em> <em class="italic">2</em> and <em class="italic">3</em>, to be expanded in <a href="Chapter_5.xhtml#_idTextAnchor140"><em class="italic">Chapter 5</em></a> with image, audio, web search, and ML features</li>
    </ul>
    <p class="normal">To build <a id="_idIndexMarker273"/>this architecture, we will do the following:</p>
    <ul>
      <li class="bulletList">Build the processes of an event-driven GenAISys interface</li>
      <li class="bulletList">Implement the conversational agent with GPT-4o and an OpenAI embedding model</li>
      <li class="bulletList">Run a multi-user, multi-turn session exploring the main features of the GenAISys AI controller and orchestrator</li>
    </ul>
    <div class="note">
      <p class="normal"> The decision to present the main components of the GenAISys architecture (in this chapter and the next) without arrows is a deliberate choice designed to convey a core concept: modularity and architectural flexibility. The figure is not a rigid blueprint but rather a conceptual toolkit. It shows you the powerful components at your disposal—<strong class="keyWord">I1. AI controller</strong>, <strong class="keyWord">I2. Multi-user chatbot</strong>, <strong class="keyWord">F1. Generative AI model</strong>, <strong class="keyWord">F2. Memory retention</strong>, <strong class="keyWord">F3. Modular RAG</strong>, and <strong class="keyWord">F4. Multifunctional capabilities</strong>—as independent, interoperable blocks. This empowers you, illustrating that you are free to design your own system architecture. For instance, a user could choose to run some functional components, such as <strong class="keyWord">F4. Multifunctional capabilities</strong>, as independent, distributed agents that are called upon by the controller. Alternatively, they could implement a completely different interface or even run the system headlessly without one.</p>
      <p class="normal">However, the focus of this architecture is on demonstrating a human-centered GenAISys. In this configuration, <strong class="keyWord">I1. AI controller</strong> (the generative AI IPython interface) serves as the central hub and orchestrator. This human-centered architecture guarantees full control and transparency. This is essential to build trust in risk-averse corporate environments. The control flow, while not drawn with arrows, is implicit: user interactions from <strong class="keyWord">I2. Multi-user chatbot</strong> are managed by the AI controller, which then strategically delegates tasks to the various functional components (<strong class="keyWord">F1</strong> to <strong class="keyWord">F4</strong>) to generate responses, access memory, perform RAG, or execute specific functions. This approach provides a clear, stable, and explainable pathway to building a business-ready generative AI system.</p>
    </div>
    <p class="normal">Let’s first explore scenario-driven task execution.</p>
    <h1 class="heading-1" id="_idParaDest-108"><a id="_idTextAnchor113"/>Building the processes of an event-driven GenAISys interface</h1>
    <p class="normal">Let’s begin by building the GenAISys interface shown in <em class="italic">Figure 4.2</em>, using IPython widgets to create a <a id="_idIndexMarker274"/>responsive, event-driven environment. The result will be a dynamic multi-user chat surface with drop-down menus, text-input fields, and a checkbox—everything needed for real-time collaboration between people/users and the generative AI agent.</p>
    <p class="normal">Open <code class="inlineCode">Event-driven_GenAISys_framework.ipynb</code> notebook within the Chapter04 directory on GitHub (<a href="https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main"><span class="url">https://github.com/Denis2054/Building-Business-Ready-Generative-AI-Systems/tree/main</span></a>). Setting up the environment is the same as described in the previous chapters:</p>
    <ul>
      <li class="bulletList">To set up OpenAI, refer to <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>, including the custom OpenAI API call used here: <code class="inlineCode">openai_api.make_openai_api_call</code></li>
      <li class="bulletList">Refer to <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a> for setting up Pinecone, connecting to the index, and querying it</li>
    </ul>
    <p class="normal">An additional package (<code class="inlineCode">ipython</code>) is required for the notebook environment. IPython is pre-installed in Google Colab; if needed, install it using the following:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install ipython
</code></pre>
    <p class="normal">The code we’ll build demonstrates core concepts such as event-driven interactions, dynamic content updating, and modular function organization. By the end of this section, you will have learned how to bridge the gap between AI functionality and end user engagement.</p>
    <figure class="mediaobject"><img alt="Figure 4.2: The flowchart of an event-driven GenAISys interface" src="../Images/B32304_04_2.png"/></figure>
    <p class="packt_figref">Figure 4.2: The flowchart of an event-driven GenAISys interface</p>
    <p class="normal">The main <a id="_idIndexMarker275"/>groups of functions required to build this interface are the following:</p>
    <ul>
      <li class="bulletList">Initializing widgets</li>
      <li class="bulletList">Handling user input and selection changes</li>
      <li class="bulletList">Processing chat messages, including triggering functions and exit commands</li>
      <li class="bulletList">Generating and processing AI responses</li>
      <li class="bulletList">Updating the UI dynamically</li>
      <li class="bulletList">Saving the conversation history</li>
    </ul>
    <p class="normal">Before diving into the code from a developer’s perspective, let’s keep the user’s point of view in mind. We must build an intuitive interface that can seamlessly execute the flow outlined in <em class="italic">Figure 4.2</em>.</p>
    <figure class="mediaobject"><img alt="Figure 4.3: GenAISys from a user’s perspective" src="../Images/B32304_04_3.png"/></figure>
    <p class="packt_figref">Figure 4.3: GenAISys from a user’s perspective</p>
    <div class="packt_tip">
      <p class="normal"><img alt="A magnifying glass on a black background  AI-generated content may be incorrect." src="../Images/1.png"/><strong class="keyWord">Quick tip</strong>: Need to see a high-resolution version of this image? Open this book in the next-gen Packt Reader or view it in the PDF/ePub copy.</p>
      <p class="normal"><img alt="" src="../Images/2.png"/><strong class="keyWord">The next-gen Packt Reader</strong> is included for free with the purchase of this book. Scan the QR code OR go to <a href="http://packtpub.com/unlock"><span class="url">packtpub.com/unlock</span></a>, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.</p>
      <p class="normal"><img alt="A qr code on a white background  AI-generated content may be incorrect." src="../Images/Unlock_Code1.png"/></p>
    </div>
    <p class="normal">The UI contains only three widgets: an input box for entering prompts, a drop-down list for selecting active users, and a checkbox for activating and deactivating the conversational AI agent.</p>
    <p class="normal">Let’s walk through the process of setting up and running this interactive GenAISys environment.</p>
    <h2 class="heading-2" id="_idParaDest-109"><a id="_idTextAnchor114"/>1. Start</h2>
    <p class="normal">The program <a id="_idIndexMarker276"/>starts from the <em class="italic">Multi-user conversation with the agent as a participant</em> cell. We first import the modules and libraries we need, starting with <code class="inlineCode">IPython</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display, HTML, clear_output
</code></pre>
    <p class="normal">Let’s go through each functionality we will be implementing in Google Colab:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">display</code> and <code class="inlineCode">HTML</code> to display objects such as widgets, images, and rich HTML outputs</li>
      <li class="bulletList"><code class="inlineCode">clear_output</code> to clear the output of a cell</li>
    </ul>
    <p class="normal">Then, we import <code class="inlineCode">ipywidgets</code> managed by the Jupyter project:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> ipywidgets <span class="hljs-keyword">import</span> Dropdown, Text, Checkbox, VBox, Layout
</code></pre>
    <p class="normal"><code class="inlineCode">ipywidgets</code> is the core <a id="_idIndexMarker277"/>component of the interactive interface in this notebook, in which we will use the following widgets:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Dropdown</code>: A drop-down widget to select a value from a list of options</li>
      <li class="bulletList"><code class="inlineCode">Text</code>: A widget for text input from a user</li>
      <li class="bulletList"><code class="inlineCode">Checkbox</code>: A widget for Boolean checked/unchecked input</li>
      <li class="bulletList"><code class="inlineCode">Vbox</code>: A container widget to arrange child widgets in a vertical box layout</li>
      <li class="bulletList"><code class="inlineCode">Layout</code>: To customize the style of the widgets with layout properties such as width, height, and margin</li>
    </ul>
    <p class="normal">Finally, we import JSON, used to store multi-user conversation histories:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> json
</code></pre>
    <p class="normal">We then initialize the conversation histories for all users, define the first active user, and set the active conversation to <code class="inlineCode">True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Initialize conversation histories for all users and active user</span>
user_histories = {<span class="hljs-string">"User01"</span>: [], <span class="hljs-string">"User02"</span>: [], <span class="hljs-string">"User03"</span>: []}
active_user = <span class="hljs-string">"User01"</span>  <span class="hljs-comment"># Default user</span>
conversation_active = <span class="hljs-literal">True</span>
</code></pre>
    <p class="normal">We are thus, from the start, initializing a multi-user collaborative GenAISys in which the users can <a id="_idIndexMarker278"/>be human prompts and system prompts. For example, a “user” could be a message from another system and triggered in this interface by an event that reads pending messages. The user list can be expanded, stored in variables, or utilized in any user management system that suits a project’s needs, including access rights, passwords, and roles for various applications. Next, we initialize the widgets themselves.</p>
    <h2 class="heading-2" id="_idParaDest-110"><a id="_idTextAnchor115"/>2. Initialize widgets</h2>
    <p class="normal">The code now sets up the <code class="inlineCode">Dropdown</code>, <code class="inlineCode">Text</code>, and <code class="inlineCode">Checkbox</code> widgets we need. The widgets are also <a id="_idIndexMarker279"/>linked to event handlers. The <code class="inlineCode">Dropdown</code> widget for the users defines the three users initialized at the start of the conversation:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create a dropdown to select the user</span>
user_selector = Dropdown(
    options=[<span class="hljs-string">"User01"</span>, <span class="hljs-string">"User02"</span>, <span class="hljs-string">"User03"</span>],
    value=active_user,
    description=<span class="hljs-string">'User:'</span>,
    layout=Layout(width=<span class="hljs-string">'50%'</span>)
)
</code></pre>
    <p class="normal">The selector has four parameters:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">options</code> lists the available users that can be expanded and can access any user management repository as needed for your project.</li>
      <li class="bulletList"><code class="inlineCode">value</code> determines the active user. The program started with <code class="inlineCode">User01</code> as the initial user. This can be automated when an authorized user first connects to the GenAISys.</li>
      <li class="bulletList"><code class="inlineCode">description</code> provides a label for the drop-down list that will be displayed.</li>
      <li class="bulletList"><code class="inlineCode">layout</code> sets the width of the widget that will be displayed.</li>
    </ul>
    <p class="normal">Note that we are creating a core GenAISys, not a platform. The goal is to grasp the inner workings of a GenAISys. Once it works as expected, we can then add the classical layers of user management (names, roles, and rights). In this case, we are remaining focused on the flexible core concepts of GenAISys, not how they will be encapsulated in a specific platform and framework. We are learning how to be generative AI agentic architects, not operators of a specific framework.</p>
    <p class="normal">The next step is to insert an event handler. In this case, it is an event listener that will detect when the value of <code class="inlineCode">user_selector</code> changes. When another user is selected, the <code class="inlineCode">on_user_change</code> function is automatically called, and <code class="inlineCode">value</code> becomes the new user:</p>
    <pre class="programlisting code"><code class="hljs-code">user_selector.observe(on_user_change, names=<span class="hljs-string">'value'</span>)
</code></pre>
    <p class="normal">This dynamic change in users within a GenAISys conversation represents a major evolution from <a id="_idIndexMarker280"/>the one-on-one chatbots. It introduces a whole new dimension to collaborative teamwork with AI as a co-participant.</p>
    <p class="normal">The second widget to activate is the input widget:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create the input box widget</span>
input_box = Text(placeholder=<span class="hljs-string">"Type your message here or type 'exit' to end the conversation."</span>, layout=Layout(width=<span class="hljs-string">'100%'</span>))
</code></pre>
    <p class="normal">The input can be any text and will occupy 100% of the UI layout. The conversation ends when a user enters <code class="inlineCode">exit</code> or <code class="inlineCode">quit</code>. When the text is typed and the <em class="italic">Enter</em> button is pressed, the event handler takes over:</p>
    <pre class="programlisting code"><code class="hljs-code">input_box.on_submit(handle_submit)  <span class="hljs-comment"># Attach the on_submit event handler</span>
</code></pre>
    <p class="normal"><code class="inlineCode">on_submit</code> is a <a id="_idIndexMarker281"/>method of the <code class="inlineCode">input_box</code> widget. <code class="inlineCode">handle_submit</code> is a callback function that we can write as we wish and will be described later in this section.</p>
    <p class="normal">The third widget is the checkbox for the AI conversational agent:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create a checkbox to toggle agent response</span>
agent_checkbox = Checkbox(
    value=<span class="hljs-literal">True</span>,
    description=<span class="hljs-string">'Agent'</span>,
    layout=Layout(width=<span class="hljs-string">'20%'</span>)
)
</code></pre>
    <p class="normal">The checkbox displays the description label, which is an agent in this case. The layout will occupy 20% of the UI. If <code class="inlineCode">value</code> is set to <code class="inlineCode">True</code>, then the conversational AI agent will be activated. We will build the AI agent in the <em class="italic">Conversational agent</em> section of this chapter. The AI agent will also be event-driven.</p>
    <p class="normal">The UI box is now ready to be displayed.</p>
    <h2 class="heading-2" id="_idParaDest-111"><a id="_idTextAnchor116"/>3. Display the UI</h2>
    <p class="normal">The UI <a id="_idIndexMarker282"/>container widget now combines the three event-driven widgets we defined in <code class="inlineCode">VBox</code> (<code class="inlineCode">V</code> stands for vertical; i.e., in a vertical box). The three widgets are in brackets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Display the initial interface</span>
display<code class="codeHighlighted" style="font-weight: bold;">(</code>
    <code class="codeHighlighted" style="font-weight: bold;">VBox(</code>
    <code class="codeHighlighted" style="font-weight: bold;"> </code>   <code class="codeHighlighted" style="font-weight: bold;">[user_selector, input_box, agent_checkbox]</code>,
        layout=Layout(
            display=<span class="hljs-string">'flex'</span>, flex_flow=<span class="hljs-string">'column'</span>,
            align_items=<span class="hljs-string">'flex-start'</span>, width=<span class="hljs-string">'100%'</span>
        )
    ))
</code></pre>
    <p class="normal">The layout is then defined:</p>
    <pre class="programlisting code"><code class="hljs-code">layout=Layout(
    display=<span class="hljs-string">'flex'</span>, flex_flow=<span class="hljs-string">'column'</span>,
    align_items=<span class="hljs-string">'flex-start'</span>, width=<span class="hljs-string">'100%'</span>
)))
</code></pre>
    <p class="normal">The parameters of this responsive UI are the following:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">display='flex'</code> activates the CSS flexbox model for layouts dynamically without specifying the sizes of the items</li>
      <li class="bulletList"><code class="inlineCode">flex_flow='column'</code> arranges the child widgets vertically</li>
      <li class="bulletList"><code class="inlineCode">align_items='flex-start'</code> aligns the widgets to the start (left side) of the UI (left side) container</li>
      <li class="bulletList"><code class="inlineCode">width='100%'</code> makes the container take up the full width of the available space</li>
    </ul>
    <p class="normal">With that, the UI is ready. We can choose to begin with any of the three widgets. The user selector can be run before the input, as well as the AI agent checkbox. In this case, the user selector was set to a default value, <code class="inlineCode">User01</code>, and the AI agent checkbox was set to the default value, <code class="inlineCode">True</code>.</p>
    <p class="normal">The three widgets and their processes can be built into any classical web or software interface, depending on your project’s needs. Since there is no default value for the input, let’s continue with the input widget.</p>
    <h2 class="heading-2" id="_idParaDest-112"><a id="_idTextAnchor117"/>4. Input box event</h2>
    <p class="normal">The input <a id="_idIndexMarker283"/>text is managed by the UI described in the previous section, which triggers <code class="inlineCode">input_box.on_submit(handle_submit)</code> when a user enters text. In turn, the <code class="inlineCode">submit</code> method calls the <code class="inlineCode">handle_submit</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Function to handle the submission of the input</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">handle_submit</span>(<span class="hljs-params">sender</span>):
    user_message = sender.value
    <span class="hljs-keyword">if</span> user_message.strip():
        sender.value = <span class="hljs-string">""</span>  <span class="hljs-comment"># Clear the input box</span>
        chat(user_message)
</code></pre>
    <p class="normal">Now, the <a id="_idIndexMarker284"/>function does three things:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">user_message = sender.value</code> processes the text received from the input widget</li>
      <li class="bulletList"><code class="inlineCode">if user_message.strip()</code> checks whether there is a message and clears the input box for the next input with <code class="inlineCode">sender.value = "" # Clear the input box</code></li>
      <li class="bulletList"><code class="inlineCode">chat(user_message)</code> is called if there is a message</li>
    </ul>
    <p class="normal"><code class="inlineCode">chat(user_message)</code> is the next process and a key event processing hub for the GenAISys. Let’s go through it.</p>
    <h2 class="heading-2" id="_idParaDest-113"><a id="_idTextAnchor118"/>5. chat(user_message) function</h2>
    <p class="normal">The <code class="inlineCode">chat(user_message)</code> function <a id="_idIndexMarker285"/>is an <em class="italic">orchestrator</em> component of our event-driven GenAISys. It should remain human-centered for critical human <a id="_idIndexMarker286"/>control. Once the system has gained the trust of the users and after careful consideration, some of the actions it manages can be triggered by system messages. The orchestrator contains important decisions when it processes the user message it receives from the <code class="inlineCode">handle_submit(sender)</code> function. It encapsulates several choices and functions, as represented in <em class="italic">Figure 4.2</em>: deciding whether to continue the conversation, appending or saving the conversation history to a file, determining whether to call the AI conversational agent, and updating the UI display.</p>
    <p class="normal">It first inherits the global status of the conversation variable (<code class="inlineCode">conversation_active = True</code>) we initialized at the start of the conversation (in node <strong class="keyWord">1</strong> of <em class="italic">Figure 4.2</em>):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Function to handle user input and optional bot response</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">chat</span>(<span class="hljs-params">user_message</span>):
    <span class="hljs-keyword">global</span> conversation_active
</code></pre>
    <p class="normal">It continues to determine whether the multiple-turn conversation is over or not by checking whether the user has exited or quit the conversation (see <strong class="keyWord">6</strong> in <em class="italic">Figure 4.2</em>):</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> user_message.lower() <span class="hljs-keyword">in</span> [<span class="hljs-string">'exit'</span>, <span class="hljs-string">'quit'</span>]:
</code></pre>
    <p class="normal">Let’s see what happens if the user chooses to exit the conversation.</p>
    <h2 class="heading-2" id="_idParaDest-114"><a id="_idTextAnchor119"/>6. If ‘exit’ is chosen</h2>
    <p class="normal">Suppose the user enters <code class="inlineCode">exit</code> or <code class="inlineCode">quit</code>; then the <code class="inlineCode">conversation_active</code> variable we set to <code class="inlineCode">True</code> at the <a id="_idIndexMarker287"/>start of the conversation (in node <strong class="keyWord">1</strong> of <em class="italic">Figure 4.2</em>) will now be set to <code class="inlineCode">False</code>. The system now knows that there is no need to update the display anymore. It then tells the <code class="inlineCode">clear_output</code> function to wait until the next conversation turn to clear the output to avoid flickering effects:</p>
    <pre class="programlisting code"><code class="hljs-code">clear_output(wait=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">The exit process continues by displaying a message signaling the end of the conversation and indicating that the conversation history is being saved:</p>
    <pre class="programlisting code"><code class="hljs-code">display(HTML(<span class="hljs-string">"&lt;div style='color: red;'&gt;&lt;strong&gt;Conversation ended. Saving history...&lt;/strong&gt;&lt;/div&gt;"</span>))
</code></pre>
    <p class="normal">The exit process ends by calling the <em class="italic">save</em> function of the conversation, which will save all history to a file (see node <strong class="keyWord">7</strong> in <em class="italic">Figure 4.2</em>):</p>
    <pre class="programlisting code"><code class="hljs-code">save_conversation_history()
</code></pre>
    <p class="normal">The conversation is thus saved at the end of the session for further use (for a new session or a meeting summary), as shown in node <strong class="keyWord">7</strong> of <em class="italic">Figure 4.2</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Function to save conversation history to a file</span>
def save_conversation_history():
    filename = <span class="hljs-string">"conversation_history.json"</span>  <span class="hljs-comment"># Define the filename</span>
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(filename, <span class="hljs-string">'w'</span>) <span class="hljs-keyword">as</span> <span class="hljs-built_in">file</span>:
        json.dump(user_histories, <span class="hljs-built_in">file</span>, indent=<span class="hljs-number">4</span>)  <span class="hljs-comment"># Write the user histories dictionary to the file in JSON format</span>
    display(HTML(f<span class="hljs-string">"&lt;div style='color: green;'&gt;&lt;strong&gt;Conversation history saved to {filename}.&lt;/strong&gt;&lt;/div&gt;"</span>))
</code></pre>
    <p class="normal">Now, let’s go through the process when the user(s) chooses to continue the conversation.</p>
    <h2 class="heading-2" id="_idParaDest-115"><a id="_idTextAnchor120"/>7. If user(s) continue the conversation</h2>
    <p class="normal">If the user input does not contain <code class="inlineCode">exit</code> or <code class="inlineCode">quit</code>, then the multi-turn, multi-user conversation will continue. We have some big decisions to make with this function, however. Do we append it to <a id="_idIndexMarker288"/>each user request or not? If we append it to each user request, at some point, the context window will be complete, but the number of tokens we send through the API will increase processing time and costs.</p>
    <p class="normal">The first step is to append the history of the conversation we initialized at the start (in node <strong class="keyWord">1</strong> of <em class="italic">Figure 4.2</em>):</p>
    <pre class="programlisting code"><code class="hljs-code"># Append user message to the active user’s history 
user_histories[active_user].append(
    {<span class="hljs-string">“role”: “user”</span>, <span class="hljs-string">“content”:</span> user_message}
)
</code></pre>
    <p class="normal">So, in the hybrid scenario of this notebook, at this point, we will save the user history in memory until the end of the session, and we will thus augment each user’s input with their input history, as seen in node <strong class="keyWord">11 </strong>of <em class="italic">Figure 4.2</em>. If the user input does not contain <code class="inlineCode">exit</code> or <code class="inlineCode">quit</code>, then the multi-turn, multi-user conversation will continue. It will append the user message to the history (in node <strong class="keyWord">8 </strong>of <em class="italic">Figure 4.2</em>) of the user.</p>
    <p class="normal">However, if we don’t want to append a user request to it but still want to keep a record of the entire conversation for context, we can also summarize the conversation at the midpoint or the end. If we summarize it during the conversation, we can add a function to append it to the user <a id="_idIndexMarker289"/>input each time. If we summarize after the end of a session, we can continue with a new, fresh session with a summary of the previous session’s history.</p>
    <p class="normal">In this notebook, we will implement a hybrid short- and long-term memory process. We can continue the conversation by not entering <code class="inlineCode">'quit'</code> or <code class="inlineCode">'exit'</code>. Now, the <code class="inlineCode">chat(user_message)</code> function will check the conversational agent’s checkbox value:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> agent_checkbox.value:
</code></pre>
    <p class="normal">This verification is shown in node <strong class="keyWord">9</strong> in <em class="italic">Figure 4.2</em>. If the checkbox is checked, then the functions we created in the previous chapters are activated by calling <code class="inlineCode">chat_with_gpt</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    response = chat_with_gpt(user_histories[active_user],
        user_message)
 
</code></pre>
    <p class="normal">Once the response is returned, it is appended to the history of the response described previously:</p>
    <pre class="programlisting code"><code class="hljs-code">        user_histories[active_user].append( 
            {<span class="hljs-string">“role”</span>: <span class="hljs-string">“assistant”</span>, <span class="hljs-string">“content”</span>: response} 
        ) 
</code></pre>
    <p class="normal">We now have an entry-point memory framework. The program then calls <code class="inlineCode">update_display()</code>, another key function that is shown in node <strong class="keyWord">14</strong> of <em class="italic">Figure 4.2</em>. If the agent checkbox is checked, <code class="inlineCode">chat_with_gpt</code> will be called.</p>
    <h2 class="heading-2" id="_idParaDest-116"><a id="_idTextAnchor121"/>8. Generate bot response</h2>
    <p class="normal">The <code class="inlineCode">chat_with_gpt</code> function assembles the work we did in the previous chapters to create a conversational <a id="_idIndexMarker290"/>AI agent with the Pinecone-based RAG functionality. We will fully implement this integration in the <em class="italic">Conversational agent</em> section of this chapter.</p>
    <p class="normal"><code class="inlineCode">chat_with_gpt</code> orchestrates the AI conversational agent by providing information, enabling it to be dynamic and responsive. The user history of this conversation and the user message are sent to the <code class="inlineCode">chat_with_gpt</code> conversational agent function:</p>
    <pre class="programlisting code"><code class="hljs-code">response = chat_with_gpt(user_histories[active_user], user_message)
</code></pre>
    <p class="normal">Once the response is returned, the <code class="inlineCode">update_display</code> function is called from <code class="inlineCode">chat(user_message)</code>.</p>
    <h2 class="heading-2" id="_idParaDest-117"><a id="_idTextAnchor122"/>9. Update display</h2>
    <p class="normal">The <code class="inlineCode">update_display</code> function refreshes the UI with the updated conversation history and also <a id="_idIndexMarker291"/>displays the status of the widgets. It first tells the UI to wait until a new output arrives by setting <code class="inlineCode">wait</code> to <code class="inlineCode">True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">update_display</span>():
    clear_output(wait=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">The function then filters and displays the active user’s history (see node <strong class="keyWord">15 </strong>of <em class="italic">Figure 4.2</em>):</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> entry <span class="hljs-keyword">in</span> user_histories[active_user]:  <span class="hljs-comment"># Show only the active user's history</span>
        <span class="hljs-keyword">if</span> entry[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'user'</span>:
            display(HTML(<span class="hljs-string">f"&lt;div style='text-align: left; margin-left: 20px; color: blue;'&gt;&lt;strong&gt;</span><span class="hljs-subst">{active_user}</span><span class="hljs-string">:&lt;/strong&gt; </span><span class="hljs-subst">{entry[</span><span class="hljs-string">'content'</span><span class="hljs-subst">]}</span><span class="hljs-string">&lt;/div&gt;"</span>))
        <span class="hljs-keyword">elif</span> entry[<span class="hljs-string">'role'</span>] == <span class="hljs-string">'assistant'</span>:
            display(HTML(<span class="hljs-string">f"&lt;div style='text-align: left; margin-left: 20px; color: green;'&gt;&lt;strong&gt;Agent:&lt;/strong&gt; </span><span class="hljs-subst">{entry[</span><span class="hljs-string">'content'</span><span class="hljs-subst">]}</span><span class="hljs-string">&lt;/div&gt;"</span>))
</code></pre>
    <p class="normal">If the conversation is active, the UI <code class="inlineCode">VBox</code> is displayed along with the status of the widgets:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> conversation_active:
        display(VBox([user_selector, input_box, agent_checkbox]))  <span class="hljs-comment"># Keep input box, selector, and checkbox visible if active</span>
</code></pre>
    <p class="normal">The input box is cleared, the agent checkbox has been checked independently by the user, and the system has verified its status. The active user will be displayed based on the independent decision of the user. In this case, the active user, <code class="inlineCode">active_user</code>, who was initialized at the start (<strong class="keyWord">1</strong>) of the conversation, remains the same. If the user changed, the <code class="inlineCode">on_user_change</code> drop-down event<strong class="keyWord"> </strong>(<strong class="keyWord">13</strong>) would have been triggered by the <code class="inlineCode">observe</code> method of the <code class="inlineCode">user_selector</code> widget:</p>
    <pre class="programlisting code"><code class="hljs-code">user_selector.observe(on_user_change, names=<span class="hljs-string">'value'</span>)
</code></pre>
    <p class="normal">In that case, <code class="inlineCode">user_selector.observe</code> will independently call the <code class="inlineCode">update active_user</code> function (<strong class="keyWord">14</strong>) and first make sure that the active user is a global variable:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">on_user_change</span>(<span class="hljs-params">change</span>):
    <span class="hljs-keyword">global</span> active_user
</code></pre>
    <p class="normal">Then, it will <a id="_idIndexMarker292"/>make the new user the active user:</p>
    <pre class="programlisting code"><code class="hljs-code">    active_user = change[<span class="hljs-string">'new'</span>]
</code></pre>
    <p class="normal">Finally, it will call the <code class="inlineCode">update_display</code> function we built in this subsection:</p>
    <pre class="programlisting code"><code class="hljs-code">    update_display()
</code></pre>
    <p class="normal">Now that we have our dynamic UI and event-driven functions in place, let’s implement the conversational agent logic called by <code class="inlineCode">chat_with_gpt</code>.</p>
    <h1 class="heading-1" id="_idParaDest-118"><a id="_idTextAnchor123"/>Conversational agent</h1>
    <p class="normal">We implemented an AI conversational agent in <em class="italic">Chapters 1</em> and <em class="italic">2</em> and built the query Pinecone functionality in <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>. Go to the <em class="italic">Conversational agent</em> section of the notebook. If needed, take the <a id="_idIndexMarker293"/>time to revisit those chapters before proceeding. In this section, it’s time we integrate those components, preparing our GenAISys conversational agent for multi-user sessions.</p>
    <p class="normal">We begin by importing OpenAI and initializing the client:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI
<span class="hljs-comment"># Initialize the OpenAI client</span>
client = OpenAI()
</code></pre>
    <p class="normal">Next, we make a decision to store or not to store all of the user’s conversation history for each call to optimize context window size for cost and clarity:</p>
    <pre class="programlisting code"><code class="hljs-code">user_memory = <span class="hljs-literal">True</span> <span class="hljs-comment"># True=User messages are memorized False=User messages are not memorized</span>
</code></pre>
    <p class="normal">The memory setting should be strategically monitored in production environments. For example, here we set <code class="inlineCode">user_memory</code> to <code class="inlineCode">True</code>, but we avoid applying it during RAG queries, as historical context could confuse the Pinecone similarity searches. We then define the <code class="inlineCode">chat_with_gpt</code> function, which is called in node <strong class="keyWord">10</strong> of <em class="italic">Figure 4.2</em>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">chat_with_gpt</span>(<span class="hljs-params">messages, user_message</span>):
</code></pre>
    <p class="normal">The function first searches the input text for a keyword to trigger a RAG retrieval from the Pinecone index as implemented in <code class="inlineCode">Query_Pinecone.ipynb</code> and described in <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>. The code first determines the namespace:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">try</span>:
      namespace=<span class="hljs-string">""</span>
      <span class="hljs-keyword">if</span> <span class="hljs-string">"Pinecone"</span> <span class="hljs-keyword">in</span> user_message <span class="hljs-keyword">or</span> <span class="hljs-string">"RAG"</span> <span class="hljs-keyword">in</span> user_message:
        <span class="hljs-comment"># Determine the keyword</span>
        <span class="hljs-keyword">if</span> <span class="hljs-string">"Pinecone"</span> <span class="hljs-keyword">in</span> user_message:
            namespace=<span class="hljs-string">"genaisys"</span>
        <span class="hljs-keyword">elif</span> <span class="hljs-string">"RAG"</span> <span class="hljs-keyword">in</span> user_message:
            namespace=<span class="hljs-string">"data01"</span>
        <span class="hljs-built_in">print</span>(namespace)
…
</code></pre>
    <p class="normal">If the user message contains “Pinecone,” the query will target the <code class="inlineCode">genaisys</code> namespace, which contains the instruction scenarios. The <code class="inlineCode">genaisys</code> namespace implementation departs from static <a id="_idIndexMarker294"/>data retrieval and takes us into agentic, dynamic decision-making to trigger an instruction or a task. If the user message contains “RAG,” the query will target the <code class="inlineCode">data01</code> namespace, which contains static data. The queries and content of the Pinecone index are those implemented in <a href="Chapter_3.xhtml#_idTextAnchor085"><em class="italic">Chapter 3</em></a>:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-comment">#define query text</span>
        query_text=user_message
        <span class="hljs-comment"># Retrieve query results</span>
        query_results = get_query_results(query_text, namespace)
        <span class="hljs-comment"># Process and display the results</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"Processed query results:"</span>)
        qtext, target_id = display_results(query_results)
        <span class="hljs-built_in">print</span>(qtext)
</code></pre>
    <p class="normal">Once the query result is returned, we append the user message to it to augment the input:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-comment">#run task</span>
        sc_input=qtext + <span class="hljs-string">" "</span> + user_message
        mrole = <span class="hljs-string">"system"</span>
        mcontent = <span class="hljs-string">"You are an assistant who executes the tasks you are asked to do."</span>
        user_role = <span class="hljs-string">"user"</span>
</code></pre>
    <p class="normal">The message parameters and the OpenAI API call are described in the <em class="italic">Setting up the environment</em> section of <a href="Chapter_1.xhtml#_idTextAnchor021"><em class="italic">Chapter 1</em></a>. The OpenAI response is stored in <code class="inlineCode">task response</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">        task_response = openai_api.make_openai_api_call(
            sc_input,mrole,mcontent,user_role
        )
        <span class="hljs-built_in">print</span>(task_response)
</code></pre>
    <p class="normal">The response returned by the OpenAI API call, augmented with the result of the Pinecone query, is stored in <code class="inlineCode">aug_output</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">        aug_output=namespace + <span class="hljs-string">":"</span> +task_response
</code></pre>
    <p class="normal">If the user message does not contain a keyword to trigger the RAG function, the user request will be <a id="_idIndexMarker295"/>sent directly to the OpenAI API call, and the response will be stored in <code class="inlineCode">aug_output</code>. However, the system must first check whether <code class="inlineCode">user_memory</code> is <code class="inlineCode">True</code> or not. The system must also extract the text content of <code class="inlineCode">user_message</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">      <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">if</span> user_memory:
                <span class="hljs-comment"># Extract ALL user messages from the conversation history</span>
                user_messages_content = [
                    msg[<span class="hljs-string">"content"</span>] <span class="hljs-keyword">for</span> msg <span class="hljs-keyword">in</span> messages
                    <span class="hljs-keyword">if</span> msg[<span class="hljs-string">"role"</span>] == <span class="hljs-string">"user"</span> <span class="hljs-keyword">and</span> <span class="hljs-string">"content"</span> <span class="hljs-keyword">in</span> msg
                ]
                <span class="hljs-comment"># Combine all extracted user messages into a single string</span>
                combined_user_messages = <span class="hljs-string">" "</span>.join(user_messages_content)
                <span class="hljs-comment"># Add the current user_message to the combined text</span>
                umessage = <span class="hljs-string">f"</span><span class="hljs-subst">{combined_user_messages}</span><span class="hljs-string"> </span><span class="hljs-subst">{user_message}</span><span class="hljs-string">"</span>
   
</code></pre>
    <p class="normal">In this case, <code class="inlineCode">umessage</code> now contains a concatenation of the conversation history of the active user extracted and stored in <code class="inlineCode">combined_user_messages</code> and the user message itself in <code class="inlineCode">user_message</code>. The generative AI model now has complete context about the dialogue with this user.</p>
    <div class="note">
      <p class="normal"> The strategy for managing conversation history will depend heavily on each real-world use case. For example, we might choose to extract the history of all users involved in a session or only specific users. Alternatively, a team could decide to use a single shared username throughout an entire conversation. Generally, the best practice is to organize workshops with end users to define and configure the conversation-memory strategies that best fit their workflow.</p>
    </div>
    <p class="normal">In some cases, we might decide to ignore the conversation history altogether. In that scenario, we set the <code class="inlineCode">user_memory</code> parameter to <code class="inlineCode">False</code>, and the system disregards prior exchanges:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">else</span>:
                umessage = user_message
</code></pre>
    <p class="normal">The <code class="inlineCode">umessage</code> variable is now ready to be sent directly to the generative AI model:</p>
    <pre class="programlisting code"><code class="hljs-code">        mrole = <span class="hljs-string">"system"</span>
        mcontent = <span class="hljs-string">"You are an assistant who executes the tasks you are asked to do."</span>
        user_role = <span class="hljs-string">"user"</span>
        task_response =openai_api.make_openai_api_call(
            umessage,mrole,mcontent,user_role
        )
        aug_output=task_response
</code></pre>
    <p class="normal">The response <a id="_idIndexMarker296"/>from the OpenAI API call is then returned to the <code class="inlineCode">chat_with_gpt</code> function (in node <strong class="keyWord">10</strong> of <em class="italic">Figure 4.2</em>):</p>
    <pre class="programlisting code"><code class="hljs-code">      <span class="hljs-comment"># Return the augmented output</span>
      <span class="hljs-keyword">return</span> aug_output
</code></pre>
    <p class="normal">If the OpenAI API call fails, an exception is raised and returned:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">except</span> Exception <span class="hljs-keyword">as</span> e:
        <span class="hljs-comment"># Return the error message in case of an exception</span>
        <span class="hljs-keyword">return</span> <span class="hljs-string">f"An error occurred: </span><span class="hljs-subst">{</span><span class="hljs-built_in">str</span><span class="hljs-subst">(e)}</span><span class="hljs-string">"</span>
</code></pre>
    <p class="normal">And with that, we have assembled the generative AI functionalities developed across the previous three chapters. At this stage, we’ve built a responsive GenAISys interface and integrated a generative agent, together forming a cohesive AI controller and orchestrator. Let’s now put our GenAISys into motion.</p>
    <h1 class="heading-1" id="_idParaDest-119"><a id="_idTextAnchor124"/>Multi-user, multi-turn GenAISys session</h1>
    <p class="normal">We now have a responsive, event-driven GenAISys capable of <a id="_idIndexMarker297"/>executing multiple tasks in diverse ways, as illustrated in <em class="italic">Figure 4.4</em>. We will explore the flexibility of this GenAISys interface we built using IPython and assemble the OpenAI and Pinecone components from previous chapters.</p>
    <figure class="mediaobject"><img alt="Figure 4.4: Summing up the components we have built and assembled in this chapter" src="../Images/B32304_04_4.png"/></figure>
    <p class="packt_figref">Figure 4.4: Summing up the components we have built and assembled in this chapter</p>
    <p class="normal">Since the functions within GenAISys are event-driven, a user (human or system) or a group of users can leverage this framework to address multiple cross-domain tasks. The system is human-centric, creating a collaborative, frictionless environment between humans and a generative AI agent. Importantly, there is no competition between humans and AI in this framework. Teams can maintain human social relationships with co-workers while using the GenAISys to boost their performance and productivity exponentially. This human-centric approach is one I have always advocated throughout my decades of <a id="_idIndexMarker298"/>experience providing AI-driven automation solutions for global corporations, mid-sized businesses, and smaller organizations. When teams adopt AI as a collaborative tool rather than a competitor, it fosters a positive atmosphere that leads to quick wins—demonstrating the combined effectiveness of teamwork and technology.</p>
    <p class="normal">If we look deeper into how the GenAISys framework can be leveraged in teamwork scenarios, we can establish several fundamental sequences of events typically needed in real-world projects:</p>
    <ol>
      <li class="alphabeticList" value="1">User selection =&gt; Input =&gt; Agent checked =&gt; RAG instruction =&gt; GenAI agent =&gt; Output</li>
      <li class="alphabeticList">User selection =&gt; Input =&gt; Agent checked =&gt; RAG data =&gt; GenAI agent =&gt; Output</li>
      <li class="alphabeticList">User selection =&gt; Input =&gt; Agent checked =&gt; User history =&gt; GenAI agent =&gt; Output</li>
      <li class="alphabeticList">User selection =&gt; Input =&gt; Agent checked =&gt; No user history =&gt; GenAI agent =&gt; Output</li>
      <li class="alphabeticList">User selection =&gt; Input =&gt; Agent unchecked =&gt; Output</li>
    </ol>
    <p class="normal">These basic sequences constitute a set of sequences, <strong class="bold-italic" style="font-style: italic;">S</strong>:</p>
    <p class="normal"><a id="_idIndexMarker299"/><img alt="" src="../Images/B32304_Equation.png"/></p>
    <p class="normal">To achieve a goal for a single user or a group of users, the sequences can be assembled as follows:</p>
    <ul>
      <li class="bulletList">{a, b}: Running a sentiment analysis with RAG, followed by the retrieval of an episodic memory of a past meeting.</li>
      <li class="bulletList">{d, e}: Running an OpenAI API request and then making a comment for other users. The novelty in this case is that the AI agent remains a co-worker in a team and sometimes doesn’t express itself, allowing the team to ponder the ideas it suggested.</li>
    </ul>
    <p class="normal">These sequences <a id="_idIndexMarker300"/>can be arranged into longer session flows as required by the specific tasks and scenarios. Because sequences can repeat themselves, we have an indefinite number of possible dynamic combinations. For instance, here’s a glimpse into the flexibility that this provides:</p>
    <ul>
      <li class="bulletList">Set of three members, such as {a, c, e}, {b, d, e}, {a, b, c}</li>
      <li class="bulletList">Set of four members, such as {a, b, c, d}, {b, c, d, e}, {a, c, d, e}</li>
      <li class="bulletList">Set of five members, such as {a, b, c, d, e}</li>
    </ul>
    <p class="normal">We could add exiting the session and summarizing to these sequences, as well as reloading a saved file and continuing the session. There can also be a repetition of sets, sets with different users, and sets with more functions. In the following chapters, we will add new features, including image generation, audio, web search, and ML, that will expand the scope of the GenAISys framework we have built.</p>
    <p class="normal">In this section, however, we will run a session with two users in a simple sequence of events. Then, we will run a scenario with multiple users and some basic sequences. Let’s begin with a straightforward sequence of events.</p>
    <h2 class="heading-2" id="_idParaDest-120"><a id="_idTextAnchor125"/>A session with two users</h2>
    <p class="normal">In this example session, two users collaborate to brainstorm ideas for attractive travel destinations they could recommend to customers on their online travel website. We start by running an interface session, then display the conversation history, and finally summarize the discussion. To begin the session, open <code class="inlineCode">Event-driven_GenAISys_framework.ipynb</code> and run these sections of cells:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Setting up the environment</strong>: Run all cells</li>
      <li class="bulletList"><strong class="keyWord">Conversational agent</strong>: Run the single cell</li>
      <li class="bulletList"><strong class="keyWord">Running the interface in the GenAISys IPython interface</strong>: This will initialize the conversation</li>
    </ul>
    <div class="note">
      <p class="normal"> Due to the stochastic nature of generative AI models, the outputs might vary slightly with each execution. Likewise, minor differences may appear between this notebook and the printed chapter, as multiple runs are performed during quality control.</p>
    </div>
    <p class="normal">With the conversation initialized, let’s now run the interactive session.</p>
    <h3 class="heading-3" id="_idParaDest-121"><a id="_idTextAnchor126"/>The interactive conversation</h3>
    <p class="normal">The conversation starts with <code class="inlineCode">User01</code> by default, displaying the input box and the activated agent checkbox. The <a id="_idIndexMarker301"/>sequence of events and functions triggered in this scenario is illustrated in <em class="italic">Figure 4.5</em>.</p>
    <figure class="mediaobject"><img alt="Figure 4.5: The GenAI agent performs a task with the user’s history" src="../Images/B32304_04_5.png"/></figure>
    <p class="packt_figref">Figure 4.5: The GenAI agent performs a task with the user’s history</p>
    <p class="normal">The flow follows this sequence:</p>
    <p class="normal">User selection =&gt; Input =&gt; Agent checked =&gt; User history =&gt; GenAI agent =&gt; Output</p>
    <p class="normal">To the user, this process is seamless, as illustrated in <em class="italic">Figure 4.6</em>. However, the underlying functions required careful design and development to produce this smooth effect.</p>
    <figure class="mediaobject"><img alt="Figure 4.6: UI with the GenAI agent checked" src="../Images/B32304_04_6.png"/></figure>
    <p class="packt_figref">Figure 4.6: UI with the GenAI agent checked</p>
    <p class="normal">From the user’s perspective, the process is straightforward. <code class="inlineCode">User01</code> types a prompt into the input box: <code class="inlineCode">What is the capital of France?</code>.</p>
    <figure class="mediaobject"><img alt="Figure 4.7: User entering a simple prompt" src="../Images/B32304_04_7.png"/></figure>
    <p class="packt_figref">Figure 4.7: User entering a simple prompt</p>
    <p class="normal">The output will be displayed above the input widget, as shown here:</p>
    <figure class="mediaobject"><img alt="Figure 4.8: Output is displayed above the input box" src="../Images/B32304_04_8.png"/></figure>
    <p class="packt_figref">Figure 4.8: Output is displayed above the input box</p>
    <p class="normal">At this point, even an <a id="_idIndexMarker302"/>untrained user can intuitively run the GenAISys with the basic information provided in a one-page document. <code class="inlineCode">User01</code> continues with a follow-up question:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="codeHighlighted" style="font-weight: bold;">User01:</code> What is there to visit?
</code></pre>
    <p class="normal">Since the user history option is set to <code class="inlineCode">True</code> in the conversational agent, the agent knows that the question is about Paris and provides information on some iconic locations to visit:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="code-highlight"><strong class="hljs-con-slc">Agent:</strong></span> The capital of France is Paris. There are numerous attractions to visit in Paris, including: 1. **Eiffel Tower** - An iconic symbol of Paris, offering stunning views of the city. 2. **Louvre Museum** - The world's largest art museum, home to the Mona Lisa…
</code></pre>
    <p class="normal"><code class="inlineCode">User02</code> now enters the dialogue by selecting <strong class="screenText">User02</strong> in the drop-down menu, as shown in <em class="italic">Figure 4.9</em>. <strong class="screenText">User02</strong> then asks what the capital of Spain is, the agent responds, and the output is displayed above the input box.</p>
    <figure class="mediaobject"><img alt="Figure 4.9: Input: Another user can enter the conversation" src="../Images/B32304_04_9.png"/></figure>
    <p class="packt_figref">Figure 4.9: Input: Another user can enter the conversation</p>
    <p class="normal">At this point, the GenAISys core framework is set, and we will progressively enhance it in subsequent chapters by addressing features such as security, user access controls, and data privacy. <code class="inlineCode">User02</code> goes further and asks about the places worth visiting. The agent responds correctly because the user history option is activated:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="code-highlight"><strong class="hljs-slc">User02:</strong></span> What <span class="hljs-keyword">is</span> there to visit?
</code></pre>
    <pre class="programlisting con"><code class="hljs-con"><span class="code-highlight"><strong class="hljs-con-slc">Agent:</strong></span> The capital of Spain is Madrid. There are many attractions to visit in Madrid, including: 1. **The Royal Palace of Madrid** - The official residence of the Spanish royal family, although it is now used only for state ceremonies. 2. **Prado Museum** …
</code></pre>
    <p class="normal">Now, a key feature comes into play. <code class="inlineCode">User02</code> deactivates the AI agent, as shown in <em class="italic">Figure 4.10</em>, and directly <a id="_idIndexMarker303"/>addresses <code class="inlineCode">User01</code>, similar to an interaction on social media or remote collaboration platforms.</p>
    <figure class="mediaobject"><img alt="Figure 4.10: Direct communication in a team without an AI agent" src="../Images/B32304_04_10.png"/></figure>
    <p class="packt_figref">Figure 4.10: Direct communication in a team without an AI agent</p>
    <p class="normal"><code class="inlineCode">User01</code> responds informally: “Yes, that’s a good idea, but let’s think it over.” Subsequently, <code class="inlineCode">User01</code> ends the session by typing <code class="inlineCode">exit</code>. This capability, as illustrated in <em class="italic">Figure 4.11</em>, takes our GenAISys to a new level for the use cases we will explore in this book, such as the following configurations:</p>
    <ul>
      <li class="bulletList">A user can communicate with GenAISys alone in a one-to-one conversation</li>
      <li class="bulletList">A team can work together, enhancing their performance with the AI agent as a collaborator</li>
      <li class="bulletList">The users can be AI agents playing the role of managers from different locations when the human managers are not available</li>
      <li class="bulletList">The users can be systems providing information in real-time to human users</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 4.11: A team communicates directly and then ends the session" src="../Images/B32304_04_11.png"/></figure>
    <p class="packt_figref">Figure 4.11: A team communicates directly and then ends the session</p>
    <p class="normal">Upon exiting, the session ends, and the conversation history is saved to the <code class="inlineCode">conversation_history.json</code> file:</p>
    <figure class="mediaobject"><img alt="Figure 4.12: Saving and concluding the conversation" src="../Images/B32304_04_12.png"/></figure>
    <p class="packt_figref">Figure 4.12: Saving and concluding the conversation</p>
    <p class="normal">Like all <a id="_idIndexMarker304"/>other features in this framework, the exit behavior can be customized for individual projects. Take the following examples:</p>
    <ul>
      <li class="bulletList">The conversation history can be saved or not</li>
      <li class="bulletList">Only parts of the conversation history can be saved</li>
      <li class="bulletList">The name of the saved conversation history file can contain a timestamp</li>
      <li class="bulletList">Going <code class="inlineCode">"…to the next cell" </code>is optional</li>
    </ul>
    <p class="normal">These are decisions to make for each use case. They will not modify the overall framework of the GenAISys but allow for a high level of customization.</p>
    <p class="normal">In this case, the team wants to display the conversation they just had.</p>
    <h3 class="heading-3" id="_idParaDest-122"><a id="_idTextAnchor127"/>Loading and displaying the conversation</h3>
    <p class="normal">The code <a id="_idIndexMarker305"/>for this function is a standard <a id="_idIndexMarker306"/>IPython display function to convert the JSON file, <code class="inlineCode">conversation_history.json</code>, into Markdown format. Let’s first check whether the conversation history parameter and/or the summary parameter is activated:</p>
    <pre class="programlisting code"><code class="hljs-code"><code class="inlineCode">display_conversation_history=True</code>
<code class="inlineCode">summary=True</code>
</code></pre>
    <p class="normal">In this case, the conversation history and the summary function are both activated. Now, we will check whether a conversation history file is present or not:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> json
<span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> display, Markdown
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">if</span> display_conversation_history == <span class="hljs-literal">True</span> <span class="hljs-keyword">or</span> summary==<span class="hljs-literal">True</span>:
    <span class="hljs-comment"># File path</span>
    file_path = <span class="hljs-string">'conversation_history.json'</span>
    <span class="hljs-comment"># Check if the file exists</span>
    <span class="hljs-keyword">if</span> os.path.exists(file_path):
        display_conversation_history=<span class="hljs-literal">True</span>
        summary=<span class="hljs-literal">True</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"The file '</span><span class="hljs-subst">{file_path}</span><span class="hljs-string">' exists."</span>)
    <span class="hljs-keyword">else</span>:
        display_conversation_history=<span class="hljs-literal">False</span>
        summary=<span class="hljs-literal">False</span>
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"The file '</span><span class="hljs-subst">{file_path}</span><span class="hljs-string">' does not exist."</span>)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"The conversation history will not be processed."</span>)
</code></pre>
    <p class="normal">If a <a id="_idIndexMarker307"/>file exists, <code class="inlineCode">display_conversation_history</code> will be set to <code class="inlineCode">True</code> and <code class="inlineCode">summary=True</code> (even if it was set to <code class="inlineCode">False</code> previously). A <a id="_idIndexMarker308"/>message will signal that the file exists:</p>
    <pre class="programlisting con"><code class="hljs-con">The file 'conversation_history.json' exists.
</code></pre>
    <p class="normal">If <code class="inlineCode">display_conversation_history==True</code>, then the conversation will be displayed:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Display option</span>
<span class="hljs-keyword">if</span> display_conversation_history==<span class="hljs-literal">True</span>:
  <span class="hljs-comment"># File path</span>
  file_path = <span class="hljs-string">'conversation_history.json'</span>
  <span class="hljs-comment"># Open the file and read its content into the 'dialog' variable</span>
  <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(file_path, <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>) <span class="hljs-keyword">as</span> file:
      dialog = json.load(file)  <span class="hljs-comment"># Parse JSON content</span>
…
<span class="hljs-comment"># Function to format JSON content as markdown</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">format_json_as_markdown</span>(<span class="hljs-params">data, level=</span><span class="hljs-number">0</span>):
    html_output = <span class="hljs-string">""</span>
    indent = <span class="hljs-string">"  "</span> * level
…
<span class="hljs-keyword">return</span> html_output
<span class="hljs-comment"># Format the JSON into markdown</span>
formatted_markdown = format_json_as_markdown(dialog)
<span class="hljs-comment"># Display formatted JSON as Markdown</span>
display(Markdown(formatted_markdown))
</code></pre>
    <p class="normal">The output is nicely formatted:</p>
    <pre class="programlisting con"><code class="hljs-con">…
<span class="code-highlight"><strong class="hljs-con-slc">User01</strong></span>:
<span class="code-highlight"><strong class="hljs-con-slc">role</strong></span>:
user
<span class="code-highlight"><strong class="hljs-con-slc">content</strong></span>:
What is the capital of France?
<span class="code-highlight"><strong class="hljs-con-slc">role</strong></span>:
assistant
<span class="code-highlight"><strong class="hljs-con-slc">content</strong></span>:
The capital of France is Paris.
…
<span class="code-highlight"><strong class="hljs-con-slc">Content</strong></span>:
The capital of Spain is Madrid.
<span class="code-highlight"><strong class="hljs-con-slc">role</strong></span>:
user
<span class="code-highlight"><strong class="hljs-con-slc">content</strong></span>:
What is there to visit?
<span class="code-highlight"><strong class="hljs-con-slc">role</strong></span>:
assistant
<span class="code-highlight"><strong class="hljs-con-slc">content</strong></span>:
The capital of Spain is Madrid. There are many attractions to visit in Madrid, including:
<span class="code-highlight"><strong class="hljs-con-slc">The Royal Palace of Madrid</strong></span> – …
</code></pre>
    <p class="normal">The <a id="_idIndexMarker309"/>team has displayed the conversation <a id="_idIndexMarker310"/>but wants to take the process further and summarize this online meeting that included an AI agent as a participant.</p>
    <h3 class="heading-3" id="_idParaDest-123"><a id="_idTextAnchor128"/>Loading and summarizing the conversation</h3>
    <p class="normal">The conversation we are summarizing shows how to merge an AI agent into an existing human team <a id="_idIndexMarker311"/>to boost productivity. In some cases, the GenAISys will have worked on automated tasks alone. In other cases, the GenAISys will be the copilot of one or several users. In others, in the many critical moments of the life of an organization, teams of humans and AI agents will be able to work together to make decisions.</p>
    <p class="normal">In this section, we will ask the AI agent to summarize the conversation. We will integrate this feature as a function in the GenAISys in the following chapters. For the moment, we will run it separately after displaying the conversation, as shown in <em class="italic">Figure 4.13</em>.</p>
    <figure class="mediaobject"><img alt="Figure 4.13: Displaying and summarizing a conversation" src="../Images/B32304_04_13.png"/></figure>
    <p class="packt_figref">Figure 4.13: Displaying and summarizing a conversation</p>
    <p class="normal">The code <a id="_idIndexMarker312"/>first loads the <code class="inlineCode">conversation_history.json</code> file as in the display function. Then, we define a function that converts <a id="_idIndexMarker313"/>the conversation history content into an optimal format for the OpenAI API:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Function to construct dialog string from the JSON conversation history</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">construct_dialog_for_summary</span>(<span class="hljs-params">conversation_history_json</span>):
    dialog = <span class="hljs-string">""</span>
    <span class="hljs-keyword">for</span> user, messages <span class="hljs-keyword">in</span> conversation_history_json.items():
        dialog += <span class="hljs-string">f"\n</span><span class="hljs-subst">{user}</span><span class="hljs-string">:\n"</span>
        <span class="hljs-keyword">for</span> message <span class="hljs-keyword">in</span> messages:
            role = message[<span class="hljs-string">"role"</span>]
            content = message[<span class="hljs-string">"content"</span>]
            dialog += <span class="hljs-string">f"- </span><span class="hljs-subst">{role}</span><span class="hljs-string">: </span><span class="hljs-subst">{content}</span><span class="hljs-string">\n"</span>
    <span class="hljs-keyword">return</span> dialog
</code></pre>
    <p class="normal">The function to construct the full conversation history is called:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Construct the full dialog from the JSON history</span>
formatted_dialog = construct_dialog_for_summary(conversation_history_json)
</code></pre>
    <p class="normal">Now, we prepare the complete message for the custom GenAISys API call built for the system and imported in the <em class="italic">OpenAI</em> subsection of the <em class="italic">Setting the environment </em>section in our notebook:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Task to summarize the conversation</span>
mrole = <span class="hljs-string">"system"</span>
mcontent = <span class="hljs-string">"Your task is to read this JSON formatted text and summarize it."</span>
user_role = <span class="hljs-string">"user"</span>
task = <span class="hljs-string">f"Read this JSON formatted text and make a very detailed summary of it with a list of actions:\n</span><span class="hljs-subst">{formatted_dialog}</span><span class="hljs-string">"</span>
</code></pre>
    <p class="normal">Finally, we call the GenAISys OpenAI function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># The make_openai_api_call function is called</span>
task_response = openai_api.make_openai_api_call(
    task, mrole, mcontent, user_role
)
</code></pre>
    <p class="normal">The API <a id="_idIndexMarker314"/>response code will be displayed in Markdown format:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> Markdown, display
<span class="hljs-comment"># Display the task response as Markdown</span>
display(Markdown(task_response))
</code></pre>
    <p class="normal">Now, everything <a id="_idIndexMarker315"/>is ready. We can call the summarizing function if <code class="inlineCode">summary==True</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> summary==<span class="hljs-literal">True</span>:
    <span class="hljs-comment"># File path to the JSON file</span>
    file_path = <span class="hljs-string">'/content/conversation_history.json'</span>
    <span class="hljs-comment"># Check if the file exists before calling the function</span>
    <span class="hljs-keyword">if</span> os.path.exists(file_path):
        summarize_conversation(file_path)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">f"File '</span><span class="hljs-subst">{file_path}</span><span class="hljs-string">' does not exist. Please provide a valid file path."</span>)
</code></pre>
    <div class="note">
      <p class="normal">Note that in Google Colab, <code class="inlineCode">/content/</code> is the default directory. So, the following file paths point to the same directory:</p>
      <pre class="programlisting code"><code class="hljs-code">  file_path = ‘/content/conversation_history.json’ or
 file_path = ‘conversation_history.json’
</code></pre>
      <p class="normal">In another environment, you may need absolute paths.</p>
    </div>
    <p class="normal">The output is a summary of the conversation history that contains an introduction and then a detailed summary. The prompt for this summary can be modified to request shorter or longer lengths. We can also design a prompt asking the generative AI model to target part of the conversation or design any other specific prompt for a given project. In this case, the output is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con">The JSON formatted text contains interactions between users and an assistant, where users inquire about the capitals of France and Spain and seek recommendations for attractions to visit in these cities. Below is a detailed summary with a list of actions:
<span class="code-highlight"><strong class="hljs-con-slc">User01 Interaction:</strong></span>
<span class="code-highlight"><strong class="hljs-con-slc">1. Question about the Capital of France:</strong></span>
    User01 asks for the capital of France.
    The assistant responds that the capital of France is Paris.
<span class="code-highlight"><strong class="hljs-con-slc">2. Inquiry about Attractions in Paris:</strong></span>
    User01 asks what there is to visit in Paris.
    The assistant provides a list of notable attractions in Paris:
<span class="code-highlight"><strong class="hljs-con-slc">    1. Eiffel Tower</strong></span> - Iconic symbol and must-visit landmark.
<span class="code-highlight"><strong class="hljs-con-slc">    2. Louvre Museum</strong></span> - Largest art museum, home to the Mona Lisa….
</code></pre>
    <p class="normal">By running <a id="_idIndexMarker316"/>through the many possible <a id="_idIndexMarker317"/>sequences of tasks and events, we have seen the flexibility that the GenAISys offers us. Let’s run a more complex multi-user session.</p>
    <h2 class="heading-2" id="_idParaDest-124"><a id="_idTextAnchor129"/>Multi-user session</h2>
    <p class="normal">In this section, we will <a id="_idIndexMarker318"/>run a technical session that activates the main functions we have built in the previous chapters and this chapter:</p>
    <ul>
      <li class="bulletList">Semantic and sentiment analysis</li>
      <li class="bulletList">RAG for episodic memory retrieval</li>
      <li class="bulletList">A dialogue without an AI conversational agent</li>
      <li class="bulletList">Loading, displaying, and summarizing the conversation history</li>
    </ul>
    <p class="normal">If you haven’t interrupted the previous session, then simply run the <em class="italic">Running the interface section in the GenAISys IPython interface</em> cell again in our notebook, which will start a new conversation.</p>
    <p class="normal">If you are starting from scratch, then to start the session, open <code class="inlineCode">Event-driven_GenAISys_framework.ipynb</code> and run the following sections of cells:</p>
    <ul>
      <li class="bulletList">Setting up the environment: All the cells</li>
      <li class="bulletList">Conversational agent: Contains one cell</li>
      <li class="bulletList">Running the interface in the GenAISys IPython interface: Will start the conversation</li>
    </ul>
    <p class="normal">We are ready to explore some advanced features of the GenAISys. We will highlight the events and functions that are activated by each prompt. The first sequence in the session is semantic and sentiment analysis.</p>
    <h3 class="heading-3" id="_idParaDest-125"><a id="_idTextAnchor130"/>Semantic and sentiment analysis</h3>
    <p class="normal">To perform <a id="_idIndexMarker319"/>semantic and sentiment analysis, we will need <a id="_idIndexMarker320"/>to run the following sequence orchestrated by the GenAISys as shown in <em class="italic">Figure 4.14</em>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">1. User selection</strong> is not activated because <code class="inlineCode">User01</code> is the default user at the beginning of a session. We could call this user the “host” if we wish, depending on the use case.</li>
      <li class="bulletList"><code class="inlineCode">User01</code> enters an input at <strong class="keyWord">2. Input</strong><em class="italic"> </em>triggering <strong class="keyWord">3. Agent checked</strong>, which is checked as the default value when the session starts.</li>
      <li class="bulletList">The AI conversational AI controller takes over, parses the prompt, finds the <code class="inlineCode">Pinecone</code> keyword in the prompt, triggers a Pinecone query in the instruction scenario namespace, augments the prompt, and triggers <strong class="keyWord">4. GenAI agent</strong>.</li>
      <li class="bulletList"><strong class="keyWord">4. GenAI agent</strong> triggers an API call to GPT-4o and returns the response.</li>
      <li class="bulletList"><strong class="keyWord">5. Output</strong> triggers the updating of the display. The system is ready for a new input.</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 4.14: The sequence of events and functions to perform semantic and semantic analysis" src="../Images/B32304_04_14.png"/></figure>
    <p class="packt_figref">Figure 4.14: The sequence of events and functions to perform semantic and semantic analysis</p>
    <p class="normal">The prompt that triggers this sequence of functions and events is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">A customer said that our travel agency was pretty good but should have more activities. Let's ask Pinecone for ideas.
</code></pre>
    <p class="normal">The generative AI controller correctly identified <code class="inlineCode">Pinecone</code> as a trigger to query the instruction scenario namespace, which GPT-4o used to produce a satisfactory response:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="code-highlight"><strong class="hljs-con-slc">Agent:</strong></span> genaisys:To enhance your travel agency's offerings based on the customer's feedback, … 3. **Gather Data**: Collect data from various sources such as travel blogs, customer reviews, social media, and travel forums. This data can be used to train or query your semantic search model. 4. **Generate Ideas**: Use the results from the semantic search to generate a list of potential activities. For example, if you're looking to expand adventure activities, the search might suggest zip-lining, rock climbing, or guided hiking tours….
</code></pre>
    <p class="normal">Note that the AI agent begins the response with <code class="inlineCode">genaisys</code>, signaling that the proper namespace <a id="_idIndexMarker321"/>was queried. This is an optional quality control feature that is good practice to implement when developing the GenAISys.</p>
    <p class="normal">The prompt 2 sentiment <a id="_idIndexMarker322"/>analysis sequence represented in <em class="italic">Figure 4.15</em> is identical to the semantic analysis sequence with two differences:</p>
    <ul>
      <li class="bulletList">The <strong class="keyWord">1. User selection</strong> event is activated because <code class="inlineCode">User02</code> is selected</li>
      <li class="bulletList">The prompt will contain an allusion to sentiment analysis</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 4.15: Task orchestration for sentiment analysis" src="../Images/B32304_04_15.png"/></figure>
    <p class="packt_figref">Figure 4.15: Task orchestration for sentiment analysis</p>
    <p class="normal">The prompt contains the <code class="inlineCode">Pinecone</code> keyword to activate a Pinecone query and the word <code class="inlineCode">sentiment</code>, which is an indicator for similarity search:</p>
    <pre class="programlisting code"><code class="hljs-code">`A customer said that our travel agency was worse than our competition and should have better service. Let's ask Pinecone what its sentiment is.`
</code></pre>
    <p class="normal">We could add an explicit drop-down list for all the tasks requested, and we will in some cases. But in this case, we are relying on implicit keyword searches. The balance between explicit (choosing the task in a drop-down list, for example) and implicit (using a keyword or relying on the content of the prompt) should be decided in workshops with the users.</p>
    <p class="normal">In this case, the <code class="inlineCode">Pinecone</code> keyword <code class="inlineCode">genaisys</code> namespace is selected, and the output is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="code-highlight"><strong class="hljs-con-slc">Agent:</strong></span> genaisys:Sentiment analysis score: 0.2 Explanation: The text expresses a negative sentiment towards the travel agency. The customer explicitly states that the agency is "worse than our competition" and suggests that it "should have better service." These statements indicate dissatisfaction and a negative comparison to competitors, which justifies a low sentiment score.
</code></pre>
    <p class="normal">The session continues to trigger episodic memory retrieval.</p>
    <h3 class="heading-3" id="_idParaDest-126"><a id="_idTextAnchor131"/>RAG for episodic memory retrieval</h3>
    <p class="normal">The sequence for episodic memory retrieval will search the Pinecone index for a personal memory <a id="_idIndexMarker323"/>of the team members. The memory is related to a past meeting that was saved and upserted to the Pinecone index in the <code class="inlineCode">data01</code> namespace. <code class="inlineCode">User03</code> will trigger the following sequence of functions and events, as represented in <em class="italic">Figure 4.16</em>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">1. User selection</strong> is triggered independently because the user has changed.</li>
      <li class="bulletList"><strong class="keyWord">2. Input</strong> triggers the verification of <strong class="keyWord">3. Agent checked</strong>.</li>
      <li class="bulletList"><strong class="keyWord">3. Agent checked</strong> triggers the conversational AI agent controller, which in turn triggers <strong class="keyWord">3.2. RAG data</strong> retrieval in the <code class="inlineCode">data01</code> namespace based on the <code class="inlineCode">RAG</code> keyword in the prompt.</li>
      <li class="bulletList">Finally, the generative AI agent, GPT-4o, runs a request with the augmented input and returns an output.</li>
      <li class="bulletList"><strong class="keyWord">4. Output</strong> will trigger a display update, and the system is ready for a new turn in the conversation.</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 4.16: Episodic memory retrieval" src="../Images/B32304_04_16.png"/></figure>
    <p class="packt_figref">Figure 4.16: Episodic memory retrieval</p>
    <p class="normal">The prompt clearly refers to a past event that is related to the personal experience of the team. The trace of this event is an episodic memory stored in the Pinecone index:</p>
    <pre class="programlisting code"><code class="hljs-code">The CTO was talking about leveraging different kind of memories the other day. What did he mean by that? Let's search RAG.
</code></pre>
    <p class="normal">The agent’s response is satisfactory:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="code-highlight"><strong class="hljs-con-slc">Agent:</strong></span> data01:RAG, or Retrieval-Augmented Generation, is a framework that combines retrieval-based methods with generative models to enhance the capabilities of AI systems. When your CTO mentioned leveraging different kinds of memories, they were likely referring to the various ways in which information can be stored, accessed, and utilized within such a system. Here are some types of "memories" that might be relevant in this context: 1. **Short-term Memory**:
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker324"/>see that the agent correctly found the namespace and also uses the phrase <code class="inlineCode">your CTO mentioned</code>, recognizing that this is an event linked to the personal experience of the team, not impersonal semantic data.</p>
    <p class="normal">The session continues with ideation.</p>
    <h3 class="heading-3" id="_idParaDest-127"><a id="_idTextAnchor132"/>Generative AI agent for ideation</h3>
    <p class="normal">The session <a id="_idIndexMarker325"/>continues with ideation. We want to leverage the power of the generative AI model, GPT-4o, in this case, to give us ideas. As such, the sequence will not use the Pinecone index to retrieve instructions or data, as shown in <em class="italic">Figure 4.17</em>:</p>
    <ul>
      <li class="bulletList">The user remains unchanged, <strong class="keyWord">2. Input</strong> goes directly to <strong class="keyWord">3. Agent checked</strong>.</li>
      <li class="bulletList">The system then ignores the Pinecone index but takes <strong class="keyWord">3.2. User history </strong>into account.</li>
      <li class="bulletList">Finally, <strong class="keyWord">4. GenAI agent</strong> triggers the generative AI call and returns the output.</li>
      <li class="bulletList"><strong class="keyWord">5. Output</strong> triggers the display update and the system is ready for another conversation turn.</li>
    </ul>
    <figure class="mediaobject"><img alt="Figure 4.17: GenAISys as an ideation generator" src="../Images/B32304_04_17.png"/></figure>
    <p class="packt_figref">Figure 4.17: GenAISys as an ideation generator</p>
    <p class="normal">The prompt asks the AI agent for help to get some ideas:</p>
    <pre class="programlisting code"><code class="hljs-code">But what do you, the AI Agent, suggest we do to leverage these types of memories in our traveling promotion campaigns?
</code></pre>
    <p class="normal">The AI <a id="_idIndexMarker326"/>agent inherits episodic memory since it refers to the CTO’s reflections in the conversation history and now gives its suggestions based on the history of the conversation:</p>
    <pre class="programlisting con"><code class="hljs-con">When the CTO mentioned leveraging different kinds of memories, they were likely referring to the concept of using various types of memory systems in artificial intelligence and computing to enhance performance and capabilities.…
…consider the following strategies: 1. **Personalized Recommendations**: Use short-term and long-term memory to analyze customer preferences and past interactions to provide personalized travel recommendations and offers. 2. **Dynamic Content**: Utilize episodic memory to tailor marketing content based on past customer interactions and experiences, making the promotions more relevant and engaging. …
</code></pre>
    <p class="normal">The GenAISys has provided ideas for the team. Now, the team wants to think these ideas over.</p>
    <h3 class="heading-3" id="_idParaDest-128"><a id="_idTextAnchor133"/>Dialogue without an AI conversational agent</h3>
    <p class="normal">The team <a id="_idIndexMarker327"/>has now had a short conversation that could have continued as long as they needed to. <code class="inlineCode">User01</code> takes over and communicates directly with the team. The GenAISys is now used as a collaborative remote meeting tool, as shown in <em class="italic">Figure 4.18</em>:</p>
    <ol>
      <li class="numberedList" value="1">User selection is triggered because <code class="inlineCode">User01</code> is stepping in.</li>
      <li class="numberedList"><code class="inlineCode">User01</code> unchecks the <strong class="screenText">Agent</strong> widget.</li>
      <li class="numberedList">A message is entered, but the prompt is for other users, not the AI agent</li>
      <li class="numberedList">Then, <code class="inlineCode">User01</code> ends the conversation, which is saved.</li>
    </ol>
    <figure class="mediaobject"><img alt="Figure 4.18: A dialogue without an AI agent" src="../Images/B32304_04_18.png"/></figure>
    <p class="packt_figref">Figure 4.18: A dialogue without an AI agent</p>
    <p class="normal"><code class="inlineCode">User01</code> enters <a id="_idIndexMarker328"/>a message for the others:</p>
    <pre class="programlisting code"><code class="hljs-code">OK. Let's stop here, get a summary, and go see the manager to get some green lights to move ahead.
</code></pre>
    <p class="normal"><em class="italic">Figure 4.19</em> shows that <code class="inlineCode">User01</code> has unchecked the AI agent to send the message and is now ready to end the session by entering <code class="inlineCode">exit</code>.</p>
    <figure class="mediaobject"><img alt="Figure 4.19: The user ends the conversation" src="../Images/B32304_04_19.png"/></figure>
    <p class="packt_figref">Figure 4.19: The user ends the conversation</p>
    <p class="normal">The GenAISys displays the <em class="italic">conversation ended</em> message, as shown in <em class="italic">Figure 4.20</em>.</p>
    <figure class="mediaobject"><img alt="Figure 4.20: Conversation ends" src="../Images/B32304_04_20.png"/></figure>
    <p class="packt_figref">Figure 4.20: Conversation ends</p>
    <div class="packt_tip">
      <p class="normal"><img alt="A magnifying glass on a black background  AI-generated content may be incorrect." src="../Images/1.png"/><strong class="keyWord">Quick tip</strong>: Need to see a high-resolution version of this image? Open this book in the next-gen Packt Reader or view it in the PDF/ePub copy.</p>
      <p class="normal"><img alt="" src="../Images/2.png"/><strong class="keyWord">The next-gen Packt Reader</strong> is included for free with the purchase of this book. Scan the QR code OR go to <a href="http://packtpub.com/unlock"><span class="url">packtpub.com/unlock</span></a>, then use the search bar to find this book by name. Double-check the edition shown to make sure you get the right one.</p>
      <p class="normal"><img alt="A qr code on a white background  AI-generated content may be incorrect." src="../Images/Unlock_Code1.png"/></p>
    </div>
    <p class="normal">The message instructs the users to proceed to the next cell to display and summarize the conversation.</p>
    <h3 class="heading-3" id="_idParaDest-129"><a id="_idTextAnchor134"/>Loading, displaying, and summarizing the conversation</h3>
    <p class="normal">The display <a id="_idIndexMarker329"/>and summarization of a conversation <a id="_idIndexMarker330"/>will be integrated into the functions <a id="_idIndexMarker331"/>of the GenAISys framework in <a href="Chapter_5.xhtml#_idTextAnchor140"><em class="italic">Chapter 5</em></a>, <em class="italic">Adding Multimodal, Multifunctional Reasoning with Chain of Thought</em>.</p>
    <p class="normal">In this notebook, we will proceed to the next cells as described in the <em class="italic">A session with two users</em> section.</p>
    <p class="normal">The output of the display function provides Markdown text of the conversation:</p>
    <pre class="programlisting con"><code class="hljs-con">…assistant
<span class="code-highlight"><strong class="hljs-con-slc">content</strong></span>:
When the CTO mentioned leveraging different kinds of memories, they were likely referring to the concept of…
<span class="code-highlight"><strong class="hljs-con-slc">Episodic Memory</strong></span>: This involves storing information about specific events or experiences. In AI, episodic memory can be used to recall past interactions or events to inform future decisions…
…To leverage these types of memories in your travel promotion campaigns, consider the following strategies:
<span class="code-highlight"><strong class="hljs-con-slc">Personalized Recommendations</strong></span>: Use short-term and long-term memory to analyze customer preferences and past interactions to provide personalized travel recommendations and offers.
<span class="code-highlight"><strong class="hljs-con-slc">Dynamic Content</strong></span>: Utilize episodic memory to tailor marketing content based on past customer interactions and experiences, making the promotions more relevant and engaging….
The summary is interesting because it provides useful suggestions for this online travel agency:
<span class="code-highlight"><strong class="hljs-con-slc">AI Suggestion for Travel Promotion</strong></span>:
<span class="code-highlight"><strong class="hljs-con-slc">1.Personalized Recommendations</strong></span>: Use short-term and long-term memory for personalized travel offers.
<span class="code-highlight"><strong class="hljs-con-slc">2.Dynamic Content</strong></span>: Utilize episodic memory for tailored marketing content.
<span class="code-highlight"><strong class="hljs-con-slc">3.Knowledge-Based Insights</strong></span>: Leverage semantic memory for travel tips and destination information.
<span class="code-highlight"><strong class="hljs-con-slc">4.Real-Time Engagement</strong></span>: Use working memory for real-time customer interactions.
<span class="code-highlight"><strong class="hljs-con-slc">5.Feedback and Improvement</strong></span>: Implement long-term memory systems to analyze feedback and improve campaigns.
</code></pre>
    <p class="normal">We built the fundamental structure of the GenAISys framework we will be enhancing throughout the next chapters. We also ran some basic conversations. Let’s summarize this chapter and move up to the next level.</p>
    <h1 class="heading-1" id="_idParaDest-130"><a id="_idTextAnchor135"/>Summary</h1>
    <p class="normal">A complex, event-driven, fast-moving economy requires powerful automation for the hundreds of tasks generated by just-in-time consumer needs. A GenAISys can satisfy those requirements with a responsive interface and generative AI capabilities. The challenge is providing a dynamic, intuitive system. No matter how generative AI automates tasks—and they can be tremendously automated—the final decisions will be made by humans. Humans need to communicate in meetings, whether they are organized physically or online. The challenge then evolves to provide an organization with multi-user GenAISys.</p>
    <p class="normal">In this chapter, we first explored a high-level framework to build multi-user, multi-turn, multifunctional, and RAG features. The framework includes real-time memory features and long-term knowledge stored in a vector store. The overall ChatGPT-like system requires a response interface and conversational agent that we will enhance in the following chapters.</p>
    <p class="normal">We then build an event-driven GenAISys response interface with IPython. The interface was seamless for an end user who can use the system with three widgets. The first widget managed the users’ input, the second one the active user, and the third an agent checkbox to activate or deactivate the AI conversational agent built with GPT-4o.</p>
    <p class="normal">Finally, we ran a multi-user, multi-turn GenAISys session centered on traveling for an online travel agency team. The first goal was to run a seamless GenAISys for the users with three widgets. The second goal was to explore the scope of short-term, long-term, semantic, and episodic memory. The third goal was to run RAG to retrieve instructions and data. Finally, the goal was to let the users communicate with or without the AI agent. We concluded the session by saving and summarizing it.</p>
    <p class="normal">We now have a framework that we can configure and enhance in the following chapters, starting by adding multimodal functions and external extensions to the GenAISys in <a href="Chapter_5.xhtml#_idTextAnchor140"><em class="italic">Chapter 5</em></a>, <em class="italic">Adding Multimodal, Multifunctional Reasoning with Chain of Thought</em>.</p>
    <h1 class="heading-1" id="_idParaDest-131"><a id="_idTextAnchor136"/>Questions</h1>
    <ol>
      <li class="numberedList" value="1">The interface of a GenAISys must be seamless for the users. (True or False)</li>
      <li class="numberedList">IPython is the only tool available to build a GenAISys interface. (True or False)</li>
      <li class="numberedList">The AI conversational AI agent built with GPT-4o must be enhanced with RAG. (True or False).</li>
      <li class="numberedList">GPT-4o can provide sufficient information and perform tasks quite well. (True or False)</li>
      <li class="numberedList">Pinecone can be used to retrieve instruction scenarios. (True or False)</li>
      <li class="numberedList">A namespace is only for data in Pinecone. (True or False)</li>
      <li class="numberedList">A vector store such as Pinecone is a good way to store episodic memory. (True or False)</li>
      <li class="numberedList">We don’t need an agent checkbox option. (True or False)</li>
      <li class="numberedList">Querying Pinecone is done by the user in a GenAISys. (True or False)</li>
      <li class="numberedList">GenAISys is a complex system that should be seamless for the user. (True or False)</li>
    </ol>
    <h1 class="heading-1" id="_idParaDest-132"><a id="_idTextAnchor137"/>References</h1>
    <ul>
      <li class="bulletList">IPython documentation: <a href="https://ipython.org/"><span class="url">https://ipython.org/</span></a><a href="https://ipython.org/ "/></li>
      <li class="bulletList">OpenAI multi-turn conversations: <a href="https://platform.openai.com/docs/guides/audio/multi-turn-conversations/"><span class="url">https://platform.openai.com/docs/guides/audio/multi-turn-conversations/</span></a><a href="https://platform.openai.com/docs/guides/audio/multi-turn-conversations "/></li>
      <li class="bulletList">Google Colab functionality: <a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com/</span></a><a href="https://colab.research.google.com/ "/></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-133"><a id="_idTextAnchor138"/>Further reading</h1>
    <ul>
      <li class="bulletList">Liu, J., Tan, Y. K., Fu, B., &amp; Lim, K. H. (n.d.). <em class="italic">Balancing accuracy and efficiency in multi-turn intent classification for LLM-powered dialog systems in production</em>: <a href="https://arxiv.org/abs/2411.12307"><span class="url">https://arxiv.org/abs/2411.12307</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-134"><a id="_idTextAnchor139"/>Subscribe for a Free eBook</h1>
    <p class="normal">New frameworks, evolving architectures, research drops, production breakdowns—<em class="italic">AI_Distilled</em> filters the noise into a weekly briefing for engineers and researchers working hands-on with LLMs and GenAI systems. Subscribe now and receive a free eBook, along with weekly insights that help you stay focused and informed.</p>
    <p class="normal">Subscribe at <a href="Chapter_4.xhtml"><span class="url">https://packt.link/TRO5B</span></a> or scan the QR code below.</p>
    <p class="normal"><img alt="" src="../Images/Newsletter_QR_Code1.png"/></p>
  </div>
</body></html>