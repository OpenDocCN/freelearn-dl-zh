- en: All about Rainbow DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we have learned how the various threads in **Reinforcement
    Learning** (**RL**) combined to form modern RL and then advanced to **Deep Reinforcement
    Learning** (**DRL**) with the inclusion of **Deep Learning** (**DL**). Like most
    other specialized fields from this convergence, we now see a divergence back to
    specialized methods for specific classes of environments. We started to see this
    in the chapters where we covered **Policy Gradient** (**PG**) methods and the
    environments it specialized on are continuous control. The flip side of this is
    the more typical episodic game environment, which is episodic with some form of
    discrete control mechanism. These environments typically perform better with DQN
    but the problem then becomes about DQN. Well, in this chapter, we will look at
    how smart people solved that by introducing Rainbow DQN.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce Rainbow DQN and understand the problems it
    works to address and the solutions it provides. Since we have already covered
    a majority of those solutions in other chapters, we will cover the few that make
    Rainbow special, by first looking at noisy or fuzzy networks for a better understanding
    of sampling and exploration. Then, we will look at distributed RL and how it can
    be used to improve value estimates by predicting distributions, not unlike our
    policy networks from PG, combining these improvements and others into Rainbow
    and seeing how well that performs. Finally, we will look at hierarchical DQN for
    understanding tasks and sub-tasks for possible training environments, with the
    plan to use this advanced DQN on more sophisticated environments later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the main topics we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Rainbow – combining improvements in deep reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing distributional RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding noisy networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unveiling Rainbow DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be sure to brush up on your understanding of probability, statistics, stochastic
    processes, and/or Bayesian inference and variational inference methods. This is
    something you should have already been doing in previous chapters but that knowledge
    will now be essential as we move to some of the more advanced content in DRL—and
    DL, for that matter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look to understand what is Rainbow DQN and why
    it was needed.
  prefs: []
  type: TYPE_NORMAL
- en: Rainbow – combining improvements in deep reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The paper that introduced Rainbow DQN, *Rainbow: Combining Improvements in
    Deep Reinforcement Learning*, by DeepMind in October 2017 was developed to address
    several failings in DQN. DQN was introduced by the same group at DeepMind, led
    by David Silver to beat Atari games better than humans. However, as we learned
    over several chapters, while the algorithm was groundbreaking, it did suffer from
    some shortcomings. Some of these we have already addressed with advances such
    as DDQN and experience replay. To understand what encompasses all of Rainbow,
    let''s look at the main elements it contributes to RL/DRL:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DQN**: This is, of course, the core algorithm, something we should have a
    good understanding of by now. We covered DQN in [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml),
    *Going Deep with DQN*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Double DQN**: This is not to be confused with DDQN or dueling DQN. Again,
    we already covered this in [Chapter 7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml),
    *Going Deeper with DDQN*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritized Experience Replay:** This is another improvement we already covered
    in [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dueling Network Architecture** (**DDQN**): This is an element we have covered
    already and is mentioned previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-step returns**: This is our calculation of TD lambda and estimations
    of expectation or advantage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributional RL**: This tries to understand the value distribution, not
    unlike our policy model in actor-critic except, in this case, we use values. This
    enhancement will be covered in its own section in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Noisy Nets**: Noisy or fuzzy networks are DL networks that learn to balance
    a distribution of weight parameters rather than actual discriminant values of
    network weights. These advanced DL networks have been used to better understand
    data distributions and hence data by modeling the weights it uses as distributions.
    We will cover these advanced DL networks in a further section in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of these improvements we have already covered in previous chapters, apart
    from distributional RL and noisy nets. We will cover both of these improvements
    in this chapter starting with distributional RL in a future section. Before we
    do that though, let's take a step back and improve our logging output capabilities
    with TensorBoard in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point in this book, we need to move beyond building toy examples and
    look to building modules or frameworks you can use to train your own agents in
    the future. In fact, we will use the code in this chapter for training agents
    to solve other challenge environments we present in later chapters. That means
    we need a more general way to capture our progress, preferably to log files that
    we can view later. Since building such frameworks is such a common task to machine
    learning as a whole, Google developed a very useful logging framework called TensorBoard.
    TensorBoard was originally developed as a subset of the other DL framework we
    mentioned earlier, TensorFlow. Fortunately, for us, PyTorch includes an extension
    that supports logging to TensorBoard. So, in this section, we are going to set
    up and install TensorBoard for use as a logging and graphing platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next exercise, we will install TensorBoard for use with PyTorch. If
    you have only ever used PyTorch, you likely need to follow these instructions.
    For those of you that have already installed TensorFlow previously, you will already
    be good to go and can skip this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open an Anaconda or Python shell and switch to your virtual environment. You
    likely already have one open. You may want to create an entirely separate clean
    virtual environment for TensorBoard. This minimizes the amount of code that can
    or will break in that environment. TensorBoard is a server application and is
    best treated as such, meaning the environment it runs on should be pristine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we need to install TensorBoard with the following command run from your
    Anaconda or Python shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This installs TensorBoard into your virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, to avoid dependency issues that may arise, we need to run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After that is all installed, we can run TensorBoard with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start a server application on port `6006`, by default, and pull logs
    generated from a folder called `runs`, the default used by PyTorch. If you need
    to customize the port or input log folder, you can use the following command options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'TensorBoard is a server application with a web interface. This is quite common
    for applications that we always want on and pulling from some output log or other
    data processing folder. The following diagram shows TensorBoard running in the
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/73b2b78f-0a04-403d-a8b0-af0d601d02fa.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard starting up
  prefs: []
  type: TYPE_NORMAL
- en: When TB first runs, it will output the address that you can use in your browser
    to see the interface. Copy the URL as shown in the diagram and paste it to your
    favorite web browser. Note that if you have trouble accessing the page, it may
    be a binding issue, meaning your computer may be preventing this access. A couple
    of things to try are using a `localhost:6006` or `127.0.0.1:6006` address and/or
    use the bind all option for TB.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When your browser opens and assuming you have not run TensorBoard before or
    put output in the data folder, you will see something like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/75b8dfd3-8c5d-4cf0-ba31-7cd6e6ebf197.png)'
  prefs: []
  type: TYPE_IMG
- en: Empty TensorBoard running
  prefs: []
  type: TYPE_NORMAL
- en: 'The one important point to note when running TB for the first time is that
    it is using the right data folder. You should see the `Data location: runs` label
    designating the folder that will contain the logging output.'
  prefs: []
  type: TYPE_NORMAL
- en: With TB set up, we can now move on to exploring the innovations that made Rainbow
    DQN so much better than vanilla DQN. While we have already used and explored a
    few of those innovations already, we can now move on to understanding the remaining
    two innovations that were included in Rainbow. We start with distributional RL
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing distributional RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name distributional RL can be a bit misleading and may conjure up images
    of multilayer distributed networks of DQN all working together. Well, that indeed
    may be a description of distributed RL, but distribution RL is where we try and
    find the value distribution that DQN is predicting, that is, not just find the
    maximum or mean value but understanding the data distribution that generated it.
    This is quite similar to both intuition and purpose for PG methods. We do this
    by projecting our known or previously predicted distribution into a future or
    future predicted distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'This definitely requires us to review a code example, so open `Chapter_10_QRDQN.py`
    and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire code listing is too big to drop here, so we will look at sections
    of importance. We will start with the **QRDQN** or **Quantile Regressive DQN**.
    Quantile regression is a technique to predict distributions from observations.
    The QRDQN listing follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Most of this code looks the same as before, but one thing to note and not get
    confused by is `qvalues` denotes a Q value (state-action) and not a Q policy value
    as we saw with PG methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will scroll down to the `projection_distribution` function, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This code is quite mathematical and outside the scope of this book. It essentially
    just extracts what it believes to be the distribution of Q values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After that, we can see the construction of our two models, denoting that we
    are building a DDQN model here using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we get the computation of the TD loss with the `computer_td_loss`
    function, shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This loss calculation function is similar to other DQN implementations we have
    seen before although this one does expose a few twists and turns. Most of the
    twists are introduced by using **Quantile Regression** (**QR**). QR is essentially
    about predicting the distribution using quants or quantiles, that is, slices of
    the probability to iteratively determine the predicted distribution. This predicted
    distribution is then used to determine the network loss and train it back through
    the DL network. If you scroll back up, you can note the introduction of the three
    new hyperparameters that allow us to tune that search. These new values, shown
    here, allow us to define the number iterations, `num_quants`, and search range,
    `Vmin` and `Vmax`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can see how the training code is run by scrolling to the bottom
    of the code and reviewing it here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have seen very similar code in Chapters 6 and 7 when we previously looked
    at DQN so we won''t review it here. Instead, familiarize yourself with the DQN
    model again if you need to. Note the differences between it and PG methods. When
    you are ready, run the code as you normally would. The output of running the sample
    is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/973d2e7f-a4f7-427c-a407-f0b192ec7e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Example output from Chapter_10_QRDQN.py
  prefs: []
  type: TYPE_NORMAL
- en: The output generated from this sample is just a reminder that we have more information
    being output to a log folder. To see that log folder, we need to run TensorBoard
    again and we will do that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Back to TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the sample from the last exercise still running, we want to return to
    TensorBoard and now see the output from our sample running. To do that, open a
    new Python/Anaconda command shell and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the shell to the same folder you are running your previous exercise code
    example in. Switch to your virtual environment or a special one just for TB and
    then run the following command to start the process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start TB in the current folder using that `runs` folder as the data
    dump directory. After the sample is running for a while, you may see something
    like the following when you visit the TB web interface now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/14068085-91c6-41a9-bd47-290e50f4f0c6.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard output from Chapter_10_QRDQN.py
  prefs: []
  type: TYPE_NORMAL
- en: Turn up the **Smoothing** control as shown in the screenshot to see the visual
    trend of the data. Seeing a general trend in the data allows you to extrapolate
    if or when your agent may be fully trained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we need to go back to the `Chapter_10_QRDQN.py` example code and see how
    we generated this output data. First, notice the new `import` and declaration
    of a new variable, `writer`, of the `SummaryWriter` class imported from `torch.utils.tensorboard`
    and shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `writer` object is used to output to the log files that get constructed
    in the `run` folder. Every time we run this example piece of code now, this writer
    will output to the `run` folder. You can alter this behavior by inputting a directory
    into the `SummaryWriter` constructor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, scroll down to the revised `plot` function. This function, shown here,
    now generates the log output we can visualize with TB:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This updated block of code now outputs results using TB `writer` and not `matplotlib
    plot`, as we did before. Each `writer.add_scalar` call adds a value to the data
    plot we visualized earlier. There are plenty of other functions you can call on
    to add many different types of output. Considering the ease with which we can
    generate impressive output, you likely may ever find a need to use `matplotlib`
    again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to your TB web interface and observe the continued training output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This code sample may need some tuning to be able to tune the agent to a successful
    policy. However, you now have at your disposal even more powerful tools TensorBoard
    to assist you in doing that. In the next section, we will look at the last improvement
    introduced by Rainbow, noisy networks.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding noisy networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Noisy networks are not those networks that need to know everything—those would
    be nosey networks. Instead, noisy networks introduce the concept of noise into
    the weights used to predict the output through the network. So, instead of having
    a single scalar value to denote the weight in a perceptron, we now think of weights
    as being pulled from some form of distribution. Obviously, we have a common theme
    going on here and that is going from working with numbers as single scalar values
    to what is better described as a distribution of data. If you have studied the
    subject of Bayesian or variational inference, you will likely understand this
    concept concretely.
  prefs: []
  type: TYPE_NORMAL
- en: 'For those without that background, let''s look at what a distribution could
    be in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08f99ae7-4012-4212-8ed5-be3f45e89823.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of different data distributions
  prefs: []
  type: TYPE_NORMAL
- en: The source for the preceding diagram comes from a blog post by Akshay Sharma
    ([https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec](https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec)).
    What is shown in the diagram is a sampling pattern of various well-known data
    distributions. Basic statistics assumes that all data is always evenly or normally
    distributed. In statistics, you will learn of other distributions you use to define
    various tests of variance or fit such as the Chi or Student's t. You are also
    likely quite familiar with a uniform distribution if you have ever sampled a random
    number in a computer program. Most computer programs always assume a uniform distribution,
    which in some ways is the problem we have in machine learning and hence the move
    to better understanding how real data or actions/events are distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Variational inference or quantitative risk analysis is a technique whereby we
    use typical equations of engineering, economics, or other disciplines and assume
    their inputs are distributions rather that discriminant values. By using distributions
    of data as input we, therefore, assume our output is also a distribution in some
    form. That distribution can then be used to evaluate terms of risk or reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s time for another exercise, so open `Chapter_10_NDQN.py` and follow the
    next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is another big example so we will only focus on what is important. Let''s
    start by scrolling down and looking at the `NoisyDQN` class here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is quite similar to our previous DQN samples but with a key difference:
    the addition of a new specialized DL network layer type called `NoisyLinear`**.**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scrolling down further, we can see the `td_compute_loss` function updated to
    handle the noisy or fuzzy layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This function is quite close to our previous vanilla DQN examples and that is
    because all of the work/difference is going on in the new noisy layers, which
    we will get to shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scroll back up the definition of the `NoisyLinear` class, as seen here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `NoisyLinear` class is a layer that uses a normal distribution to define
    each of the weights in the layers. This distribution is assumed to be normal,
    which means it is defined by a mean, mu, and standard variation, sigma. So, if
    we assumed 100 weights in a layer previously, we would now have two values (mu
    and sigma) that now define how the weight is sampled. In turn, the values for
    mu and sigma also become the values we train the network on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With other frameworks, building in the ability to apply variational weights
    to layers is quite difficult and often requires more code. Fortunately, this is
    one of the strengths with PyTorch and it boosts a built-in probabilistic framework
    designed to predict and handle distributional data.
  prefs: []
  type: TYPE_NORMAL
- en: Different distributions may use different descriptive values to define them.
    The normal or Gaussian distribution is defined by mu and sigma, while the uniform
    distribution is often just defined by a min/max values and the triangle would
    be min/max and peak value for instance. We almost always prefer to use a normal
    distribution for most natural events.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down to the training code and you will see virtually the same code as
    the last exercise with one key difference. Instead of epsilon for exploration,
    we introduce a term called beta. Beta becomes our de-facto exploration term and
    replaces epsilon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the application as you normally would and observe the training in TensorBoard,
    as shown in the screenshot here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/79eca78e-3c2f-490c-aa79-4dede0817696.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard output from sample Chapter_10_NDQN.py
  prefs: []
  type: TYPE_NORMAL
- en: The arrows and the direction the arrow is pointing denotes the direction in
    which we want our agent/algorithm to move. We get these trend plots by increasing
    the **Smoothing** parameter to the max, .99\. By doing this, it is easier to see
    the general or median trend.
  prefs: []
  type: TYPE_NORMAL
- en: One thing we need to revisit before moving on to Rainbow is how exploration
    is managed when using noisy networks. This will also help to explain the use of
    that the new beta parameter for *z*.
  prefs: []
  type: TYPE_NORMAL
- en: Noisy networks for exploration and importance sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using noisy networks also introduces fuzziness in our action prediction. That
    is, since the weights of the network are now being pulled from a distribution
    that also means that they equally becoming distributional. We can also say they
    are stochastic and that stochasticity is defined by a distribution, basically
    meaning that the same input could yield two completely different results, which
    means we can no longer take just the max or best action because that is now just
    fuzzy. Instead, we need a way to decrease the size of the sampling distributions
    we use for weights and therefore the uncertainty we have in the actions the agent
    selects.
  prefs: []
  type: TYPE_NORMAL
- en: Decreasing the size of a distribution is more or less the same as reducing the
    uncertainty in that data. This is a cornerstone of data science and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'We reduce this uncertainty by introducing a factor called beta that is increased
    over time. This increase is not unlike epsilon but just in reverse. Let''s see
    how this looks in code by opening `Chapter_10_NDQN.py` back up and follow the
    exercise here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see how beta is defined by looking at the main code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This setup and equation are again not unlike how we defined epsilon previously.
    The difference here is that beta increases gradually.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Beta is used to correct the weights being trained and hence introduces the concept
    of importance sampling. Importance sampling is about how much importance we have
    on the weights before correcting/sampling them. Beta then becomes the importance
    sampling factor where a value of 1.0 means 100% important and 0 means no importance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open up the `replay_buffer.py` file found in the `common` folder in the same
    project. Scroll down to the `sample` function and notice the code, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The `sample` function is part of the `PrioritizedExperienceReplay` class we
    are using to hold experiences. There's no need for us to review this whole class
    other than to realize it orders experiences in terms of priority. Sampling weights
    for the network based on the importance factor, `beta`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, jump back to the sample code and review the plot function. The line
    that generates our plot of beta in TensorBoard now looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you can review more of the code or try and tune the new hyperparameters
    before continuing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That completes our look at noisy and not nosey networks for exploration. We
    saw how we could introduce distributions to be used as the weights for our DL
    network. Then, we saw how, to compensate for that, we needed to introduce a new
    training parameter, beta. In the next section, we see how all these pieces come
    together in Rainbow DQN.
  prefs: []
  type: TYPE_NORMAL
- en: Unveiling Rainbow DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The author of *Rainbow: Combining Improvements in Deep Reinforcement Learning*,
    Matteo Hessel ([https://arxiv.org/search/cs?searchtype=author&amp;query=Hessel%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+M)),
    did several comparisons against other state-of-the-art models in DRL, many of
    which we have already looked at. They performed these comparisons against the
    standard 2D classic Atari games with impressive results. Rainbow DQN outperformed
    all of the current state-of-the-art algorithms. In the paper, they used the familiar
    classic Atari environment. This is fine since DeepMind has a lot of data for that
    environment that is specific to applicable models to compare with. However, many
    have observed that the paper lacks a comparison between PG methods, such as PPO.
    Of course, PPO is an OpenAI advancement and it may have been perceived by Google
    DeepMind to be an infringement or just wanting to avoid acknowledgment by comparing
    it at all. Unfortunately, this also suggests that even a highly intellectual pursuit
    such as DRL cannot be removed from politics.'
  prefs: []
  type: TYPE_NORMAL
- en: Methods such as PPO have been used to beat or best some of the biggest challenges
    in DRL currently. PPO was in fact responsible for taking the 100 thousand dollar
    grand prize in the Unity Obstacle Tower Challenge. For that reason, you should
    not discount PG methods anytime soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that previous plot, we should be expecting some big things from Rainbow.
    So, let''s open up `Chapter_10_Rainbow.py` and follow the next exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example will be very familiar by now and we will limit ourselves to looking
    at just the differences, starting with the main implementation of the `RainbowDQN`
    class itself here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code defines the network structure for the RainbowDQN. This network
    is a bit complicated so we have put the major elements in the diagram here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/5542d332-f67b-4fe3-9c44-113580d656d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Rainbow Network Architecture
  prefs: []
  type: TYPE_NORMAL
- en: If you go over the `init` and `forward` functions, you should be able to see
    how this diagram was built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can''t leave the preceding code just yet and we need to review the act function
    again and shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `act` function shows how the agent selects an action. We have refined the
    action selection strategy here and now use the values for `Vmin`, `Vmax`, and
    `num_atoms` . We use these values as inputs into `torch.linspace` as a way to
    create a discrete distribution ranging in value from `Vmin` to `Vmax` and in steps
    defined by `num_atoms`. This outputs scaling values within the min/max ranges
    that are then multiplied by the original distribution, `dist`, output from the
    `forward` function. This multiplying a distribution returned by the `forward`
    function and the one generated from `torch.linspace` applies a type of scaling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You may have noticed that the hyperparameters, `num_atoms`, `Vmin`, and `Vmax`,
    now perform dual purposes in tuning parameters in the model. This is generally
    a bad thing. That is, you always want the hyperparameters you define to be single-purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will scroll down and look at the differences in the `projection_distribution`
    function. Remember this function is what performs the distributional part of finding
    the distribution rather than a discrete value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This code is quite different than the quantile regression code we looked at
    previously. The primary difference here is the use of the PyTorch libraries here
    whereas before, the code was more low-level. Using the libraries is a bit more
    verbose but hopefully, you can appreciate how more explanatory the code is now
    compared to the previous example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One thing to note here is that we continue to use `epsilon` for exploration,
    as the following code shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Run the example as you normally would and observe the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep in mind that since this example lacks a prioritized replay buffer, it fails
    to be a complete RainbowDQN implementation. However, it does cover the 80/20 rule
    and implementing a prioritized replay buffer is left as an exercise to the reader.
    Let the sample keep running while we jump to the next section on observing training.
  prefs: []
  type: TYPE_NORMAL
- en: When does training fail?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One thing that trips any newcomer to DL and certainly deep reinforcement learning
    is when to know whether your model is failing, is just being a bit stubborn, or
    is not ever going to work. It is a question that causes frustration and angst
    in the AI field and often leaves you to wonder: *what if I let that agent train
    a day longer*? Unfortunately, if you speak to experts, they will often say just
    be patient and keep training, but this perhaps builds on those frustrations. After
    all, what if what you built has no hope of ever doing anything—are you wasting
    time and energy to keep it going?'
  prefs: []
  type: TYPE_NORMAL
- en: Another issue that many face is that the more complex an algorithm/model gets,
    the more time it takes to train, except you never know how long that is unless
    you trained it before or read a really well-written paper that uses the exact
    same model. Even with the same exact model, the environment may also differ perhaps
    being more complex as well. With all of these factors at play, as well as the
    pain of tuning hyperparameters, it is a wonder why anyone of sane mind would want
    to work in DRL at all.
  prefs: []
  type: TYPE_NORMAL
- en: The author hosts a Deep Learning Meetup support group for RL and DRL. One of
    the frequent discussions in this group is how DL researchers can keep their sanity
    and/or reduce their stress levels. If you work in AI, you understand the constant
    need to live up to the hype overcoming the world. This hype is a good thing but
    can also be a bad thing when it involves investors or impatient bosses.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, with advanced tools such as TensorBoard, we can gain insights into
    how are agent trains or hopes to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up TensorBoard and follow the next exercise to see how to effectively
    diagnose training problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from TB is shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/eea7ff49-3611-4ef4-a3a5-4503e588c03a.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorBoard output from Rainbow DQN
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding screenshot, where the **Smoothing** has been upped to .99,
    we can see training is failing. Remember, the graphs in the screenshot are annotated
    to show the preferred direction. For all of those plots, that is not the case.
    However, don't assume that if the plot is going in the opposite direction that
    it is necessarily bad—it isn't. Instead, any movement is a better indication of
    some training activity. This is also the reason we smooth these plots so highly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The one key plot that often dictates future training performance is the **Losses**
    plot. An agent will be learning when losses are decreasing and will be forgetting/confused
    if the losses are increasing. If losses remain constant, then the agent is stagnant
    and could be confused or stuck. A helpful summary of this is shown in the screenshot
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bc442dbe-4558-4574-a52b-73049901ac6a.png)'
  prefs: []
  type: TYPE_IMG
- en: The Losses plot summarized
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the ideal training over 500 thousand episodes
    and for this environment, you can expect to train double or triple that amount.
    As a general rule, it is best to consider no movement, positive or negative, over
    10% or the training time to be a failure. For example, if you are training an
    agent for 1 million iterations, then your 10% window would be about 100 thousand
    iterations. If your agent is training constantly or flat-lining in any plot, aside
    from Advantage, over a period equal to or larger than the 10% window size, it
    may be best to tune hyperparameters and start again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Again, pay special attention to the Losses plot as this provides the strongest
    indicator for training problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can run view the results of multiple training efforts side by side by just
    running the sample repeatedly for the same number of iterations, as the screenshot
    here shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/0cbc852e-2d96-4c24-ae03-0a8cd81e8af4.png)'
  prefs: []
  type: TYPE_IMG
- en: Examples of multiple training outputs on the same graph
  prefs: []
  type: TYPE_NORMAL
- en: Stop the current sample change some hyperparameters and run it again to see
    the preceding example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These simple rules will hopefully help you to avoid frustrations on building/training
    your own models on new or different environments. Fortunately, we have several
    more chapters to work through and that includes plenty of more exercises like
    those featured in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to working in the real world, the experience you build from doing
    these exercises may mean the difference between not getting that job and certainly
    keeping it. As a programmer, you don''t have the luxury of just understanding
    how something works; you''re a mechanic/engineer that needs to get their hands
    dirty and actually do the work:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the hyperparameters for `Chapter_10_QRDQN.py` and see what effect this
    has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for `Chapter_10_NDQN.py` and see what effect this has
    on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters for `Chapter_10_Rainbow.py` and see what effect this
    has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run and tune the hyperparameters for any of this chapter's samples on another
    environment such as CartPole or FrozenLake or something more complex such as Atari.
    Reducing the complexity of an environment is also helpful if your computer is
    older and needs to work harder training agents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This chapter also includes sample code for Hierarchical DQNs and Categorical
    DQNs in the `Chapter_10_HDQN.py` and `Chapter_10_C51.py` samples. Run these examples,
    review the code, and do some investigation on your own on what improvements these
    samples bring to DRL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to save/load the trained model from any of the examples. Can
    you now use the trained model to show the agent playing the game?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add the ability to output other training values to TensorBoard that you may
    think are important to training an agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `NoisyLinear` layers to the `Chapter_10_QRDQN.py` example. There may already
    be code that is just commented out in the example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a prioritized replay buffer to the `Chapter_10_Rainbow.py` example. You
    can use the same method found in the `Chapter_10_NDQN.py` example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: TensorBoard allows you to output and visualize a trained model. Use TensorBoard
    to output the trained model from one of the examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obviously, the number of exercises has increased to reflect your increasing
    skill level and/or interest in DRL. You certainly don't need to complete all of
    these exercises but 2-3 will go a long way. In the next section, we will summarize
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked specifically at one of the more state-of-the-art
    advances in DRL from DeepMind called Rainbow DQN. Rainbow combines several improvements
    layered on top of DQN that allow dramatic increases in training performance. As
    we have already covered many of these improvements, we only needed to review a
    couple of new advances. Before doing that though, we installed TensorBoard as
    a tool to investigate training performance. Then, we looked at the first advancement
    in distributional RL and how to model the action by understanding the sampling
    distribution. Continuing with distributions, we then looked at noisy network layers—network
    layers that don't have individual weights but rather individual distributions
    to describe each weight. Building on this example, we moved onto Rainbow DQN with
    our last example, finishing off with a quick discussion on when to determine whether
    an agent is not trainable or flat-lining.
  prefs: []
  type: TYPE_NORMAL
- en: For the next chapter, we will move from building DRL algorithms/agents to building
    environments with Unity and constructing agents in those environments with the
    ML-Agents toolkit.
  prefs: []
  type: TYPE_NORMAL
