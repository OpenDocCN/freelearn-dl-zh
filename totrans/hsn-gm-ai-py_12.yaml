- en: All about Rainbow DQN
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全部关于 Rainbow DQN
- en: Throughout this book, we have learned how the various threads in **Reinforcement
    Learning** (**RL**) combined to form modern RL and then advanced to **Deep Reinforcement
    Learning** (**DRL**) with the inclusion of **Deep Learning** (**DL**). Like most
    other specialized fields from this convergence, we now see a divergence back to
    specialized methods for specific classes of environments. We started to see this
    in the chapters where we covered **Policy Gradient** (**PG**) methods and the
    environments it specialized on are continuous control. The flip side of this is
    the more typical episodic game environment, which is episodic with some form of
    discrete control mechanism. These environments typically perform better with DQN
    but the problem then becomes about DQN. Well, in this chapter, we will look at
    how smart people solved that by introducing Rainbow DQN.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经学习了各种 **强化学习**（**RL**）的线索是如何结合形成现代强化学习，然后通过加入 **深度学习**（**DL**）而发展到
    **深度强化学习**（**DRL**）。像大多数其他从这种融合中产生的专业领域一样，我们现在看到的是回到针对特定环境类别的专门方法。我们在涵盖 **策略梯度**（**PG**）方法和它专门化的连续控制环境章节中开始看到这一点。这一面的反面是更典型的周期性游戏环境，它具有某种形式的离散控制机制。这些环境通常使用
    DQN 表现更好，但问题变成了关于 DQN 的。好吧，在这一章中，我们将探讨聪明人如何通过引入 Rainbow DQN 来解决这个问题。
- en: In this chapter, we will introduce Rainbow DQN and understand the problems it
    works to address and the solutions it provides. Since we have already covered
    a majority of those solutions in other chapters, we will cover the few that make
    Rainbow special, by first looking at noisy or fuzzy networks for a better understanding
    of sampling and exploration. Then, we will look at distributed RL and how it can
    be used to improve value estimates by predicting distributions, not unlike our
    policy networks from PG, combining these improvements and others into Rainbow
    and seeing how well that performs. Finally, we will look at hierarchical DQN for
    understanding tasks and sub-tasks for possible training environments, with the
    plan to use this advanced DQN on more sophisticated environments later.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将介绍 Rainbow DQN，并理解它试图解决的问题以及它提供的解决方案。由于我们已经在其他章节中涵盖了大多数这些解决方案，我们将通过首先查看用于更好地理解采样和探索的噪声或模糊网络来介绍使
    Rainbow 不同的少数几个解决方案。然后，我们将探讨分布式强化学习以及它如何通过预测分布来提高价值估计，这与我们的策略网络从 PG 类似，将这些改进和其他改进结合到
    Rainbow 中，并观察其表现如何。最后，我们将探讨分层 DQN 以理解任务和子任务，为可能的训练环境提供计划，并计划在以后使用这种高级 DQN 在更复杂的环境中。
- en: 'Here are the main topics we will cover in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖的主要主题包括：
- en: Rainbow – combining improvements in deep reinforcement learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rainbow – 结合深度强化学习中的改进
- en: Using TensorBoard
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorBoard
- en: Introducing distributional RL
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍分布式强化学习
- en: Understanding noisy networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解噪声网络
- en: Unveiling Rainbow DQN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 揭示 Rainbow DQN
- en: Be sure to brush up on your understanding of probability, statistics, stochastic
    processes, and/or Bayesian inference and variational inference methods. This is
    something you should have already been doing in previous chapters but that knowledge
    will now be essential as we move to some of the more advanced content in DRL—and
    DL, for that matter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要复习你对概率、统计学、随机过程以及/或贝叶斯推理和变分推理方法的理解。这应该是你在前面的章节中就已经在做的事情，但随着我们转向 DRL 以及 DL
    的更高级内容，这些知识现在将变得至关重要。
- en: In the next section, we will look to understand what is Rainbow DQN and why
    it was needed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨理解 Rainbow DQN 是什么以及为什么需要它。
- en: Rainbow – combining improvements in deep reinforcement learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rainbow – 结合深度强化学习中的改进
- en: 'The paper that introduced Rainbow DQN, *Rainbow: Combining Improvements in
    Deep Reinforcement Learning*, by DeepMind in October 2017 was developed to address
    several failings in DQN. DQN was introduced by the same group at DeepMind, led
    by David Silver to beat Atari games better than humans. However, as we learned
    over several chapters, while the algorithm was groundbreaking, it did suffer from
    some shortcomings. Some of these we have already addressed with advances such
    as DDQN and experience replay. To understand what encompasses all of Rainbow,
    let''s look at the main elements it contributes to RL/DRL:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年10月，DeepMind发表的介绍Rainbow DQN的论文，*Rainbow：结合深度强化学习的改进*，旨在解决DQN的几个不足。DQN是由DeepMind的同一组人引入的，由David
    Silver领导，旨在在Atari游戏中打败人类。然而，正如我们在几个章节中学到的，虽然该算法具有开创性，但它确实存在一些不足。其中一些我们已经通过DDQN和经验回放等进步解决了。为了理解Rainbow所包含的所有内容，让我们看看它对RL/DRL的主要贡献：
- en: '**DQN**: This is, of course, the core algorithm, something we should have a
    good understanding of by now. We covered DQN in [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml),
    *Going Deep with DQN*.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DQN**：当然，这是核心算法，我们到现在应该已经很好地理解了。我们已经在[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，“深入探索DQN”中介绍了DQN。'
- en: '**Double DQN**: This is not to be confused with DDQN or dueling DQN. Again,
    we already covered this in [Chapter 7](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml),
    *Going Deeper with DDQN*.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双DQN**：这不要与DDQN或对抗DQN混淆。再次强调，我们已经在[第7章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，“深入探索DDQN”中介绍过这一点。'
- en: '**Prioritized Experience Replay:** This is another improvement we already covered
    in [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优先经验回放**：这是我们已经在[第6章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，“深入探索DQN”中介绍过的另一个改进。'
- en: '**Dueling Network Architecture** (**DDQN**): This is an element we have covered
    already and is mentioned previously.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对抗网络架构**（**DDQN**）：这是我们之前已经介绍过的元素，并在之前提到过。'
- en: '**Multi-step returns**: This is our calculation of TD lambda and estimations
    of expectation or advantage.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多步回报**：这是我们计算TD lambda和期望或优势估计的方法。'
- en: '**Distributional RL**: This tries to understand the value distribution, not
    unlike our policy model in actor-critic except, in this case, we use values. This
    enhancement will be covered in its own section in this chapter.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式强化学习**：这试图理解价值分布，与我们的策略模型在演员-评论家中的作用类似，但在这个情况下，我们使用的是价值。这种增强将在本章的单独部分进行介绍。'
- en: '**Noisy Nets**: Noisy or fuzzy networks are DL networks that learn to balance
    a distribution of weight parameters rather than actual discriminant values of
    network weights. These advanced DL networks have been used to better understand
    data distributions and hence data by modeling the weights it uses as distributions.
    We will cover these advanced DL networks in a further section in this chapter.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声网络**：噪声或模糊网络是深度学习网络，它们学习平衡权重参数的分布，而不是网络权重的实际判别值。这些高级深度学习网络已被用于更好地理解数据分布及其数据，通过将使用的权重建模为分布来实现。我们将在本章的后续部分介绍这些高级深度学习网络。'
- en: Many of these improvements we have already covered in previous chapters, apart
    from distributional RL and noisy nets. We will cover both of these improvements
    in this chapter starting with distributional RL in a future section. Before we
    do that though, let's take a step back and improve our logging output capabilities
    with TensorBoard in the next section.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中已经介绍了许多这些改进，除了分布式强化学习和噪声网络。我们将在本章中介绍这两个改进，从未来的部分开始介绍分布式强化学习。但在我们这样做之前，让我们退一步，在下一节中通过TensorBoard提高我们的日志输出能力。
- en: Using TensorBoard
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorBoard
- en: At this point in this book, we need to move beyond building toy examples and
    look to building modules or frameworks you can use to train your own agents in
    the future. In fact, we will use the code in this chapter for training agents
    to solve other challenge environments we present in later chapters. That means
    we need a more general way to capture our progress, preferably to log files that
    we can view later. Since building such frameworks is such a common task to machine
    learning as a whole, Google developed a very useful logging framework called TensorBoard.
    TensorBoard was originally developed as a subset of the other DL framework we
    mentioned earlier, TensorFlow. Fortunately, for us, PyTorch includes an extension
    that supports logging to TensorBoard. So, in this section, we are going to set
    up and install TensorBoard for use as a logging and graphing platform.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这个阶段，我们需要超越构建玩具示例，转向构建你可以用来在未来训练自己的代理的模块或框架。实际上，我们将使用本章中的代码来训练代理解决我们在后续章节中提出的其他挑战环境。这意味着我们需要一种更通用的方式来记录我们的进度，最好是记录到我们以后可以查看的日志文件中。由于构建这样的框架对于整个机器学习来说是一个如此常见的任务，因此Google开发了一个非常有用的日志框架，称为TensorBoard。TensorBoard最初是作为我们之前提到的其他深度学习框架的一个子集开发的。幸运的是，对于我们来说，PyTorch包含了一个支持将日志记录到TensorBoard的扩展。因此，在本节中，我们将设置和安装TensorBoard，用作日志记录和绘图平台。
- en: 'In the next exercise, we will install TensorBoard for use with PyTorch. If
    you have only ever used PyTorch, you likely need to follow these instructions.
    For those of you that have already installed TensorFlow previously, you will already
    be good to go and can skip this exercise:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，我们将为与PyTorch一起使用安装TensorBoard。如果你只使用过PyTorch，你可能需要遵循这些说明。对于那些之前已经安装过TensorFlow的你们，你们已经可以开始了，可以跳过这个练习：
- en: Open an Anaconda or Python shell and switch to your virtual environment. You
    likely already have one open. You may want to create an entirely separate clean
    virtual environment for TensorBoard. This minimizes the amount of code that can
    or will break in that environment. TensorBoard is a server application and is
    best treated as such, meaning the environment it runs on should be pristine.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个Anaconda或Python shell并切换到你的虚拟环境。你很可能已经有一个打开的环境了。你可能想为TensorBoard创建一个完全独立的干净虚拟环境。这可以最小化在该环境中可能或将会破坏的代码量。TensorBoard是一个服务器应用程序，最好将其视为服务器应用程序，这意味着它运行的应该是一个纯净的环境。
- en: 'Then, we need to install TensorBoard with the following command run from your
    Anaconda or Python shell:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要使用以下命令从你的Anaconda或Python shell中安装TensorBoard：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This installs TensorBoard into your virtual environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你的虚拟环境中安装TensorBoard。
- en: 'Next, to avoid dependency issues that may arise, we need to run the following
    command:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了避免可能出现的依赖性问题，我们需要运行以下命令：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After that is all installed, we can run TensorBoard with the following command:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装完成后，我们可以使用以下命令运行TensorBoard：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will start a server application on port `6006`, by default, and pull logs
    generated from a folder called `runs`, the default used by PyTorch. If you need
    to customize the port or input log folder, you can use the following command options:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将在默认的`6006`端口启动一个服务器应用程序，并从名为`runs`的文件夹中拉取日志，这是PyTorch默认使用的。如果你需要自定义端口或输入日志文件夹，可以使用以下命令选项：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'TensorBoard is a server application with a web interface. This is quite common
    for applications that we always want on and pulling from some output log or other
    data processing folder. The following diagram shows TensorBoard running in the
    shell:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorBoard是一个具有网络界面的服务器应用程序。这对于我们总是希望运行并从某些输出日志或其他数据处理文件夹中提取数据的应用程序来说是很常见的。以下图显示了在shell中运行的TensorBoard：
- en: '![](img/73b2b78f-0a04-403d-a8b0-af0d601d02fa.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/73b2b78f-0a04-403d-a8b0-af0d601d02fa.png)'
- en: TensorBoard starting up
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard启动中
- en: When TB first runs, it will output the address that you can use in your browser
    to see the interface. Copy the URL as shown in the diagram and paste it to your
    favorite web browser. Note that if you have trouble accessing the page, it may
    be a binding issue, meaning your computer may be preventing this access. A couple
    of things to try are using a `localhost:6006` or `127.0.0.1:6006` address and/or
    use the bind all option for TB.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当TB首次运行时，它将输出你可以在浏览器中使用的地址以查看界面。按照图示复制URL并粘贴到你喜欢的网页浏览器中。请注意，如果你访问页面有困难，可能是一个绑定问题，意味着你的电脑可能阻止了这种访问。你可以尝试使用`localhost:6006`或`127.0.0.1:6006`地址，或者为TB使用绑定所有选项。
- en: 'When your browser opens and assuming you have not run TensorBoard before or
    put output in the data folder, you will see something like the following:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你的浏览器打开，并且假设你之前没有运行过TensorBoard或者没有在数据文件夹中放置输出，你将看到如下内容：
- en: '![](img/75b8dfd3-8c5d-4cf0-ba31-7cd6e6ebf197.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/75b8dfd3-8c5d-4cf0-ba31-7cd6e6ebf197.png)'
- en: Empty TensorBoard running
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 空的TensorBoard运行
- en: 'The one important point to note when running TB for the first time is that
    it is using the right data folder. You should see the `Data location: runs` label
    designating the folder that will contain the logging output.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一次运行TB时，需要注意的一个重要点是它正在使用正确的数据文件夹。你应该会看到`数据位置：runs`标签，指明将包含日志输出的文件夹。
- en: With TB set up, we can now move on to exploring the innovations that made Rainbow
    DQN so much better than vanilla DQN. While we have already used and explored a
    few of those innovations already, we can now move on to understanding the remaining
    two innovations that were included in Rainbow. We start with distributional RL
    in the next section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好TB之后，我们现在可以继续探索使Rainbow DQN比vanilla DQN好得多的创新之处。虽然我们已经在使用和探索了一些这些创新，但现在我们可以继续理解Rainbow中包含的剩余两个创新。我们将在下一节中从分布式RL开始。
- en: Introducing distributional RL
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式强化学习介绍
- en: The name distributional RL can be a bit misleading and may conjure up images
    of multilayer distributed networks of DQN all working together. Well, that indeed
    may be a description of distributed RL, but distribution RL is where we try and
    find the value distribution that DQN is predicting, that is, not just find the
    maximum or mean value but understanding the data distribution that generated it.
    This is quite similar to both intuition and purpose for PG methods. We do this
    by projecting our known or previously predicted distribution into a future or
    future predicted distribution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式强化学习这个名字可能有点误导，可能会让人联想到多层分布式网络中的DQN一起工作。好吧，这确实可能是分布式RL的描述，但分布RL是我们试图找到DQN预测的价值分布，也就是说，不仅仅是找到最大值或平均值，而是理解生成它的数据分布。这与PG方法的直觉和目的非常相似。我们通过将已知的或先前预测的分布投影到未来的或未来预测的分布来实现这一点。
- en: 'This definitely requires us to review a code example, so open `Chapter_10_QRDQN.py`
    and follow the next exercise:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实需要我们回顾一个代码示例，所以打开`Chapter_10_QRDQN.py`并跟随下一个练习：
- en: 'The entire code listing is too big to drop here, so we will look at sections
    of importance. We will start with the **QRDQN** or **Quantile Regressive DQN**.
    Quantile regression is a technique to predict distributions from observations.
    The QRDQN listing follows:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 整个代码列表太长了，无法在这里全部展示，所以我们将查看重要的部分。我们将从**QRDQN**或**Quantile Regressive DQN**开始。分位数回归是一种从观察中预测分布的技术。QRDQN的列表如下：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Most of this code looks the same as before, but one thing to note and not get
    confused by is `qvalues` denotes a Q value (state-action) and not a Q policy value
    as we saw with PG methods.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码的大部分看起来和之前一样，但需要注意的一点是`qvalues`表示一个Q值（状态-动作），而不是像PG方法中我们看到的那样表示Q策略值。
- en: 'Next, we will scroll down to the `projection_distribution` function, as shown
    here:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将滚动到`projection_distribution`函数，如下所示：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code is quite mathematical and outside the scope of this book. It essentially
    just extracts what it believes to be the distribution of Q values.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码相当数学化，并且超出了本书的范围。它本质上只是提取它认为的Q值的分布。
- en: 'After that, we can see the construction of our two models, denoting that we
    are building a DDQN model here using the following code:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们可以看到我们两个模型的构建，这表明我们在这里正在使用以下代码构建一个DDQN模型：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After that, we get the computation of the TD loss with the `computer_td_loss`
    function, shown here:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们得到使用`computer_td_loss`函数计算TD损失，如下所示：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This loss calculation function is similar to other DQN implementations we have
    seen before although this one does expose a few twists and turns. Most of the
    twists are introduced by using **Quantile Regression** (**QR**). QR is essentially
    about predicting the distribution using quants or quantiles, that is, slices of
    the probability to iteratively determine the predicted distribution. This predicted
    distribution is then used to determine the network loss and train it back through
    the DL network. If you scroll back up, you can note the introduction of the three
    new hyperparameters that allow us to tune that search. These new values, shown
    here, allow us to define the number iterations, `num_quants`, and search range,
    `Vmin` and `Vmax`:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个损失计算函数与我们之前看到的其他DQN实现类似，尽管这个实现确实暴露了一些曲折。大多数曲折都是通过使用**分位数回归**（**QR**）引入的。QR本质上是通过使用分位数或分位数来预测分布，即概率的切片，以迭代地确定预测分布。然后使用这个预测分布来确定网络损失，并通过深度学习网络进行训练。如果你向上滚动，你可以注意到引入了三个新的超参数，允许我们调整搜索。这里显示的新值允许我们定义迭代次数`num_quants`和搜索范围`Vmin`和`Vmax`：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can see how the training code is run by scrolling to the bottom
    of the code and reviewing it here:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过滚动到代码的底部并在这里查看，来了解训练代码是如何运行的：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have seen very similar code in Chapters 6 and 7 when we previously looked
    at DQN so we won''t review it here. Instead, familiarize yourself with the DQN
    model again if you need to. Note the differences between it and PG methods. When
    you are ready, run the code as you normally would. The output of running the sample
    is shown here:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在第六章和第七章中已经看到过非常类似的代码，当时我们之前查看DQN时，所以这里不会进行回顾。相反，如果你需要的话，再次熟悉一下DQN模型。注意它与PG方法之间的区别。当你准备好时，像平常一样运行代码。运行样本的输出如下所示：
- en: '![](img/973d2e7f-a4f7-427c-a407-f0b192ec7e6a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/973d2e7f-a4f7-427c-a407-f0b192ec7e6a.png)'
- en: Example output from Chapter_10_QRDQN.py
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Chapter_10_QRDQN.py的示例输出
- en: The output generated from this sample is just a reminder that we have more information
    being output to a log folder. To see that log folder, we need to run TensorBoard
    again and we will do that in the next section.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个样本生成的输出只是一个提醒，我们还有更多信息被输出到日志文件夹中。要查看该日志文件夹，我们需要再次运行TensorBoard，我们将在下一节中这样做。
- en: Back to TensorBoard
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回到TensorBoard
- en: 'With the sample from the last exercise still running, we want to return to
    TensorBoard and now see the output from our sample running. To do that, open a
    new Python/Anaconda command shell and follow the next exercise:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个练习的样本仍在运行的情况下，我们想回到TensorBoard，并现在查看样本运行的输出。为此，打开一个新的Python/Anaconda命令行窗口，并按照下一个练习进行操作：
- en: 'Open the shell to the same folder you are running your previous exercise code
    example in. Switch to your virtual environment or a special one just for TB and
    then run the following command to start the process:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开与你在运行之前的练习代码示例相同的文件夹的shell。切换到你的虚拟环境或专门用于TB的特殊环境，然后运行以下命令以启动过程：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will start TB in the current folder using that `runs` folder as the data
    dump directory. After the sample is running for a while, you may see something
    like the following when you visit the TB web interface now:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将在当前文件夹中启动TB，使用该`runs`文件夹作为数据转储目录。样本运行一段时间后，当你现在访问TB网络界面时，你可能看到以下类似的内容：
- en: '![](img/14068085-91c6-41a9-bd47-290e50f4f0c6.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14068085-91c6-41a9-bd47-290e50f4f0c6.png)'
- en: TensorBoard output from Chapter_10_QRDQN.py
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard输出来自Chapter_10_QRDQN.py
- en: Turn up the **Smoothing** control as shown in the screenshot to see the visual
    trend of the data. Seeing a general trend in the data allows you to extrapolate
    if or when your agent may be fully trained.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将截图所示的**平滑**控制调高，以查看数据的可视化趋势。看到数据的一般趋势允许你推断你的智能体何时可能完全训练。
- en: 'Now, we need to go back to the `Chapter_10_QRDQN.py` example code and see how
    we generated this output data. First, notice the new `import` and declaration
    of a new variable, `writer`, of the `SummaryWriter` class imported from `torch.utils.tensorboard`
    and shown here:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要回到`Chapter_10_QRDQN.py`示例代码，看看我们是如何生成这些输出数据的。首先，注意新的`import`和声明一个新变量`writer`，它是从`torch.utils.tensorboard`导入的`SummaryWriter`类，如下所示：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `writer` object is used to output to the log files that get constructed
    in the `run` folder. Every time we run this example piece of code now, this writer
    will output to the `run` folder. You can alter this behavior by inputting a directory
    into the `SummaryWriter` constructor.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`writer` 对象用于输出到在 `run` 文件夹中构建的日志文件。现在每次我们运行这个示例代码块时，这个 writer 都会输出到 `run`
    文件夹。你可以通过将目录输入到 `SummaryWriter` 构造函数中来改变这种行为。'
- en: 'Next, scroll down to the revised `plot` function. This function, shown here,
    now generates the log output we can visualize with TB:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，向下滚动到修订的 `plot` 函数。这个函数，如这里所示，现在生成我们可以用 TB 可视化的日志输出：
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This updated block of code now outputs results using TB `writer` and not `matplotlib
    plot`, as we did before. Each `writer.add_scalar` call adds a value to the data
    plot we visualized earlier. There are plenty of other functions you can call on
    to add many different types of output. Considering the ease with which we can
    generate impressive output, you likely may ever find a need to use `matplotlib`
    again.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个更新的代码块现在使用 TB `writer` 而不是我们之前使用的 `matplotlib plot` 输出结果。每次调用 `writer.add_scalar`
    都会将一个值添加到我们之前可视化的数据图中。有许多其他你可以调用的函数来添加许多不同类型的输出。考虑到我们生成令人印象深刻输出的容易程度，你可能永远不会再次需要使用
    `matplotlib`。
- en: Go back to your TB web interface and observe the continued training output.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回你的 TB 网页界面，并观察持续的训练输出。
- en: This code sample may need some tuning to be able to tune the agent to a successful
    policy. However, you now have at your disposal even more powerful tools TensorBoard
    to assist you in doing that. In the next section, we will look at the last improvement
    introduced by Rainbow, noisy networks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码示例可能需要一些调整才能将智能体调整到成功的策略。然而，你现在有更多更强大的工具 TensorBoard 来帮助你做到这一点。在下一节中，我们将探讨
    Rainbow 引入的最后一个改进：噪声网络。
- en: Understanding noisy networks
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解噪声网络
- en: Noisy networks are not those networks that need to know everything—those would
    be nosey networks. Instead, noisy networks introduce the concept of noise into
    the weights used to predict the output through the network. So, instead of having
    a single scalar value to denote the weight in a perceptron, we now think of weights
    as being pulled from some form of distribution. Obviously, we have a common theme
    going on here and that is going from working with numbers as single scalar values
    to what is better described as a distribution of data. If you have studied the
    subject of Bayesian or variational inference, you will likely understand this
    concept concretely.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声网络并不是指那些需要知道一切的网络——那些会是好奇的网络。相反，噪声网络引入了噪声的概念，用于通过网络预测输出时的权重。因此，我们不再只有一个标量值来表示感知器中的权重，我们现在认为权重是从某种形式的分布中抽取的。显然，我们在这里有一个共同的主题，那就是从处理单个标量值作为数字到更好的描述为数据分布。如果你研究过贝叶斯或变分推理的主题，你可能会具体理解这个概念。
- en: 'For those without that background, let''s look at what a distribution could
    be in the following diagram:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于没有那个背景的人来说，让我们看看以下图中分布可能是什么：
- en: '![](img/08f99ae7-4012-4212-8ed5-be3f45e89823.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08f99ae7-4012-4212-8ed5-be3f45e89823.png)'
- en: Example of different data distributions
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 不同数据分布的示例
- en: The source for the preceding diagram comes from a blog post by Akshay Sharma
    ([https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec](https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec)).
    What is shown in the diagram is a sampling pattern of various well-known data
    distributions. Basic statistics assumes that all data is always evenly or normally
    distributed. In statistics, you will learn of other distributions you use to define
    various tests of variance or fit such as the Chi or Student's t. You are also
    likely quite familiar with a uniform distribution if you have ever sampled a random
    number in a computer program. Most computer programs always assume a uniform distribution,
    which in some ways is the problem we have in machine learning and hence the move
    to better understanding how real data or actions/events are distributed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图表的来源是Akshay Sharma的一篇博客文章（[https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec](https://medium.com/mytake/understanding-different-types-of-distributions-you-will-encounter-as-a-data-scientist-27ea4c375eec)）。图表中显示的是各种知名数据分布的采样模式。基本统计学假设所有数据总是均匀或正态分布的。在统计学中，你将了解到其他分布，你将使用它们来定义各种方差测试或拟合测试，如卡方或学生t分布。如果你在计算机程序中随机采样过数字，你很可能非常熟悉均匀分布。大多数计算机程序总是假设均匀分布，这在某种程度上是机器学习中的问题，因此转向更好地理解真实数据或动作/事件是如何分布的。
- en: Variational inference or quantitative risk analysis is a technique whereby we
    use typical equations of engineering, economics, or other disciplines and assume
    their inputs are distributions rather that discriminant values. By using distributions
    of data as input we, therefore, assume our output is also a distribution in some
    form. That distribution can then be used to evaluate terms of risk or reward.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推理或定量风险分析是一种技术，我们使用工程、经济学或其他学科的典型方程，并假设它们的输入是分布而不是判别值。因此，使用数据的分布作为输入，我们假设我们的输出也是某种形式的分布。这个分布可以用来评估风险或奖励的术语。
- en: 'It''s time for another exercise, so open `Chapter_10_NDQN.py` and follow the
    next exercise:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进行另一个练习了，所以打开`Chapter_10_NDQN.py`并遵循下一个练习：
- en: 'This is another big example so we will only focus on what is important. Let''s
    start by scrolling down and looking at the `NoisyDQN` class here:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是一个很大的例子，所以我们只会关注重要的部分。让我们先向下滚动，看看这里的`NoisyDQN`类：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This is quite similar to our previous DQN samples but with a key difference:
    the addition of a new specialized DL network layer type called `NoisyLinear`**.**'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这与我们的之前的DQN示例非常相似，但有一个关键的区别：添加了一个新的专业深度学习网络层类型，称为`NoisyLinear`**。**
- en: 'Scrolling down further, we can see the `td_compute_loss` function updated to
    handle the noisy or fuzzy layers:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续向下滚动，我们可以看到`td_compute_loss`函数已更新以处理噪声或模糊层：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This function is quite close to our previous vanilla DQN examples and that is
    because all of the work/difference is going on in the new noisy layers, which
    we will get to shortly.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个函数与我们之前的vanilla DQN示例非常相似，这是因为所有的工作/差异都在新的噪声层中，我们将在稍后讨论。
- en: 'Scroll back up the definition of the `NoisyLinear` class, as seen here:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动回`NoisyLinear`类的定义，如下所示：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `NoisyLinear` class is a layer that uses a normal distribution to define
    each of the weights in the layers. This distribution is assumed to be normal,
    which means it is defined by a mean, mu, and standard variation, sigma. So, if
    we assumed 100 weights in a layer previously, we would now have two values (mu
    and sigma) that now define how the weight is sampled. In turn, the values for
    mu and sigma also become the values we train the network on.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NoisyLinear`类是一个使用正态分布来定义层中每个权重的层。这个分布被假定为正态分布，这意味着它由均值、mu和标准差、sigma定义。因此，如果我们之前在一个层中假设有100个权重，我们现在将有两个值（mu和sigma）来定义权重的采样方式。反过来，mu和sigma的值也成为了我们在网络上训练的值。'
- en: With other frameworks, building in the ability to apply variational weights
    to layers is quite difficult and often requires more code. Fortunately, this is
    one of the strengths with PyTorch and it boosts a built-in probabilistic framework
    designed to predict and handle distributional data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他框架中，将应用变分权重到层中的能力构建进去相当困难，通常需要更多的代码。幸运的是，这是PyTorch的一个优势，它增强了一个内置的概率框架，旨在预测和处理分布数据。
- en: Different distributions may use different descriptive values to define them.
    The normal or Gaussian distribution is defined by mu and sigma, while the uniform
    distribution is often just defined by a min/max values and the triangle would
    be min/max and peak value for instance. We almost always prefer to use a normal
    distribution for most natural events.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同的分布可能使用不同的描述性值来定义它们。正态分布或高斯分布由mu和sigma定义，而均匀分布通常只由最小/最大值定义，而三角形分布则是由最小/最大值和峰值定义的。我们几乎总是更喜欢使用正态分布来描述大多数自然事件。
- en: Scroll down to the training code and you will see virtually the same code as
    the last exercise with one key difference. Instead of epsilon for exploration,
    we introduce a term called beta. Beta becomes our de-facto exploration term and
    replaces epsilon.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动到训练代码，你会看到几乎与上一个练习相同的代码，只有一个关键的区别。在探索中，我们不是使用epsilon，而是引入了一个称为beta的术语。beta成为我们的实际探索术语，并取代了epsilon。
- en: 'Run the application as you normally would and observe the training in TensorBoard,
    as shown in the screenshot here:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照正常方式运行应用程序，并在TensorBoard中观察训练，如图中所示截图：
- en: '![](img/79eca78e-3c2f-490c-aa79-4dede0817696.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/79eca78e-3c2f-490c-aa79-4dede0817696.png)'
- en: TensorBoard output from sample Chapter_10_NDQN.py
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 从样本`Chapter_10_NDQN.py`生成的TensorBoard输出
- en: The arrows and the direction the arrow is pointing denotes the direction in
    which we want our agent/algorithm to move. We get these trend plots by increasing
    the **Smoothing** parameter to the max, .99\. By doing this, it is easier to see
    the general or median trend.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 箭头及其指向的方向表示我们希望我们的代理/算法移动的方向。我们通过将**平滑**参数增加到最大值，即.99，来获得这些趋势图。通过这样做，更容易看到一般或中值趋势。
- en: One thing we need to revisit before moving on to Rainbow is how exploration
    is managed when using noisy networks. This will also help to explain the use of
    that the new beta parameter for *z*.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续到Rainbow之前，我们需要重新审视一下在使用噪声网络时如何管理探索。这也有助于解释新beta参数对于*z*的使用。
- en: Noisy networks for exploration and importance sampling
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于探索和重要性采样的噪声网络
- en: Using noisy networks also introduces fuzziness in our action prediction. That
    is, since the weights of the network are now being pulled from a distribution
    that also means that they equally becoming distributional. We can also say they
    are stochastic and that stochasticity is defined by a distribution, basically
    meaning that the same input could yield two completely different results, which
    means we can no longer take just the max or best action because that is now just
    fuzzy. Instead, we need a way to decrease the size of the sampling distributions
    we use for weights and therefore the uncertainty we have in the actions the agent
    selects.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 使用噪声网络也会在我们的动作预测中引入模糊性。也就是说，由于网络的权重现在是从一个分布中抽取的，这也意味着它们正在变得分布化。我们也可以说它们是随机的，而随机性是由一个分布定义的，基本上意味着相同的输入可能会产生两个完全不同的结果，这意味着我们不能再仅仅取最大或最佳动作，因为现在这只是一个模糊的概念。相反，我们需要一种方法来减小我们用于权重的采样分布的大小，因此减少我们在代理选择的动作中的不确定性。
- en: Decreasing the size of a distribution is more or less the same as reducing the
    uncertainty in that data. This is a cornerstone of data science and machine learning.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 减小分布的大小基本上等同于减少该数据的不确定性。这是数据科学和机器学习的一个基石。
- en: 'We reduce this uncertainty by introducing a factor called beta that is increased
    over time. This increase is not unlike epsilon but just in reverse. Let''s see
    how this looks in code by opening `Chapter_10_NDQN.py` back up and follow the
    exercise here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过引入一个称为beta的因子来减少这种不确定性，这个因子会随着时间的推移而增加。这种增加与epsilon不同，只是方向相反。让我们通过重新打开`Chapter_10_NDQN.py`文件并跟随这里的练习来看看这看起来像什么代码：
- en: 'We can see how beta is defined by looking at the main code here:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过查看主要代码来了解beta是如何定义的：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This setup and equation are again not unlike how we defined epsilon previously.
    The difference here is that beta increases gradually.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个设置和方程再次与之前定义的epsilon类似。这里的区别在于beta是逐渐增加的。
- en: Beta is used to correct the weights being trained and hence introduces the concept
    of importance sampling. Importance sampling is about how much importance we have
    on the weights before correcting/sampling them. Beta then becomes the importance
    sampling factor where a value of 1.0 means 100% important and 0 means no importance.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Beta用于纠正正在训练的权重，从而引入了重要性采样的概念。重要性采样是关于我们在纠正/采样权重之前对它们的重视程度。Beta因此成为重要性采样因子，其中1.0的值表示100%重要，而0表示没有重要性。
- en: 'Open up the `replay_buffer.py` file found in the `common` folder in the same
    project. Scroll down to the `sample` function and notice the code, as shown here:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开同一项目中 `common` 文件夹中的 `replay_buffer.py` 文件。向下滚动到 `sample` 函数，并注意代码，如图所示：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `sample` function is part of the `PrioritizedExperienceReplay` class we
    are using to hold experiences. There's no need for us to review this whole class
    other than to realize it orders experiences in terms of priority. Sampling weights
    for the network based on the importance factor, `beta`.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sample` 函数是我们使用的 `PrioritizedExperienceReplay` 类的一部分，用于存储经验。除了意识到它按优先级排序经验之外，我们不需要审查这个类的全部内容。'
- en: 'Finally, jump back to the sample code and review the plot function. The line
    that generates our plot of beta in TensorBoard now looks like this:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，回到示例代码并回顾一下绘图函数。现在TensorBoard中生成我们的beta绘图的那一行看起来是这样的：
- en: '[PRE18]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: At this point, you can review more of the code or try and tune the new hyperparameters
    before continuing.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，你可以回顾更多的代码，或者尝试调整新的超参数，然后再继续。
- en: That completes our look at noisy and not nosey networks for exploration. We
    saw how we could introduce distributions to be used as the weights for our DL
    network. Then, we saw how, to compensate for that, we needed to introduce a new
    training parameter, beta. In the next section, we see how all these pieces come
    together in Rainbow DQN.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就完成了对噪声和非噪声网络探索的考察。我们看到了如何引入分布作为我们深度学习网络权重的使用。然后，我们看到为了补偿这一点，我们需要引入一个新的训练参数，beta。在下一节中，我们将看到所有这些部分如何在Rainbow
    DQN中结合在一起。
- en: Unveiling Rainbow DQN
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示Rainbow DQN
- en: 'The author of *Rainbow: Combining Improvements in Deep Reinforcement Learning*,
    Matteo Hessel ([https://arxiv.org/search/cs?searchtype=author&amp;query=Hessel%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+M)),
    did several comparisons against other state-of-the-art models in DRL, many of
    which we have already looked at. They performed these comparisons against the
    standard 2D classic Atari games with impressive results. Rainbow DQN outperformed
    all of the current state-of-the-art algorithms. In the paper, they used the familiar
    classic Atari environment. This is fine since DeepMind has a lot of data for that
    environment that is specific to applicable models to compare with. However, many
    have observed that the paper lacks a comparison between PG methods, such as PPO.
    Of course, PPO is an OpenAI advancement and it may have been perceived by Google
    DeepMind to be an infringement or just wanting to avoid acknowledgment by comparing
    it at all. Unfortunately, this also suggests that even a highly intellectual pursuit
    such as DRL cannot be removed from politics.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 《Rainbow：结合深度强化学习中的改进》一书的作者Matteo Hessel ([https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+M](https://arxiv.org/search/cs?searchtype=author&query=Hessel%2C+M))，与其他一些最先进的DRL模型进行了多次比较，其中许多我们已经看过。他们使用标准的2D经典Atari游戏进行了这些比较，并取得了令人印象深刻的结果。Rainbow
    DQN优于所有当前最先进的算法。在论文中，他们使用了熟悉的经典Atari环境。这是可以的，因为DeepMind为该环境有大量数据，可以与比较的模型相关联。然而，许多人观察到，论文缺乏PG方法，如PPO的比较。当然，PPO是OpenAI的进步，它可能被Google
    DeepMind视为侵权，或者只是想通过完全不进行比较来避免认可。不幸的是，这也表明，即使是像DRL这样高度智力追求的东西也无法摆脱政治。
- en: Methods such as PPO have been used to beat or best some of the biggest challenges
    in DRL currently. PPO was in fact responsible for taking the 100 thousand dollar
    grand prize in the Unity Obstacle Tower Challenge. For that reason, you should
    not discount PG methods anytime soon.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: PPO等方法已被用于克服DRL当前的一些最大挑战。实际上，PPO在Unity Obstacle Tower Challenge中赢得了10万美元的大奖。因此，你很快就不应该低估PG方法。
- en: 'Given that previous plot, we should be expecting some big things from Rainbow.
    So, let''s open up `Chapter_10_Rainbow.py` and follow the next exercise:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的绘图，我们应该对Rainbow抱有很高的期望。所以，让我们打开 `Chapter_10_Rainbow.py` 文件，并跟随下一个练习：
- en: 'This example will be very familiar by now and we will limit ourselves to looking
    at just the differences, starting with the main implementation of the `RainbowDQN`
    class itself here:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到现在为止，这个例子应该已经很熟悉了，我们将限制自己只查看差异，从下面这里 `RainbowDQN` 类的主要实现开始：
- en: '[PRE19]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The preceding code defines the network structure for the RainbowDQN. This network
    is a bit complicated so we have put the major elements in the diagram here:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述代码定义了Rainbow DQN的网络结构。这个网络有点复杂，所以我们已经将主要元素放在这里的图中：
- en: '![](img/5542d332-f67b-4fe3-9c44-113580d656d9.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5542d332-f67b-4fe3-9c44-113580d656d9.png)'
- en: Rainbow Network Architecture
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow网络架构
- en: If you go over the `init` and `forward` functions, you should be able to see
    how this diagram was built.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你查看`init`和`forward`函数，你应该能够看到这个图是如何构建的。
- en: 'We can''t leave the preceding code just yet and we need to review the act function
    again and shown here:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还不能离开前面的代码，我们需要再次审查act函数，如下所示：
- en: '[PRE20]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `act` function shows how the agent selects an action. We have refined the
    action selection strategy here and now use the values for `Vmin`, `Vmax`, and
    `num_atoms` . We use these values as inputs into `torch.linspace` as a way to
    create a discrete distribution ranging in value from `Vmin` to `Vmax` and in steps
    defined by `num_atoms`. This outputs scaling values within the min/max ranges
    that are then multiplied by the original distribution, `dist`, output from the
    `forward` function. This multiplying a distribution returned by the `forward`
    function and the one generated from `torch.linspace` applies a type of scaling.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`act`函数展示了智能体如何选择动作。我们在这里已经细化了动作选择策略，现在使用`Vmin`、`Vmax`和`num_atoms`的值。我们将这些值作为输入传递给`torch.linspace`，以此创建一个从`Vmin`到`Vmax`的离散分布，步长由`num_atoms`定义。这会输出最小/最大范围内的缩放值，然后这些值乘以`forward`函数输出的原始分布`dist`。将`forward`函数返回的分布与由`torch.linspace`生成的分布相乘，这是一种缩放类型。'
- en: You may have noticed that the hyperparameters, `num_atoms`, `Vmin`, and `Vmax`,
    now perform dual purposes in tuning parameters in the model. This is generally
    a bad thing. That is, you always want the hyperparameters you define to be single-purpose.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，超参数`num_atoms`、`Vmin`和`Vmax`现在在调整模型参数时具有双重作用。这通常是一件坏事。也就是说，你总是希望定义的超参数具有单一目的。
- en: 'Next, we will scroll down and look at the differences in the `projection_distribution`
    function. Remember this function is what performs the distributional part of finding
    the distribution rather than a discrete value:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将向下滚动并查看`projection_distribution`函数中的差异。记住这个函数是执行寻找分布的分布部分，而不是离散值：
- en: '[PRE21]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This code is quite different than the quantile regression code we looked at
    previously. The primary difference here is the use of the PyTorch libraries here
    whereas before, the code was more low-level. Using the libraries is a bit more
    verbose but hopefully, you can appreciate how more explanatory the code is now
    compared to the previous example.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这段代码与我们之前查看的量分回归代码有很大不同。这里的主要区别是使用了PyTorch库，而之前代码更底层。使用库会使代码更加冗长，但希望你现在可以欣赏到代码的说明性比之前的示例更清晰。
- en: 'One thing to note here is that we continue to use `epsilon` for exploration,
    as the following code shows:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这里需要注意的是，我们继续使用`epsilon`进行探索，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Run the example as you normally would and observe the output.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照常规方式运行示例，并观察输出。
- en: Keep in mind that since this example lacks a prioritized replay buffer, it fails
    to be a complete RainbowDQN implementation. However, it does cover the 80/20 rule
    and implementing a prioritized replay buffer is left as an exercise to the reader.
    Let the sample keep running while we jump to the next section on observing training.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，由于这个示例缺少优先级回放缓冲区，它无法成为完整的RainbowDQN实现。然而，它确实涵盖了80/20规则，实现优先级回放缓冲区被留作读者的练习。在我们跳转到观察训练的下一节时，让样本继续运行。
- en: When does training fail?
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练何时会失败？
- en: 'One thing that trips any newcomer to DL and certainly deep reinforcement learning
    is when to know whether your model is failing, is just being a bit stubborn, or
    is not ever going to work. It is a question that causes frustration and angst
    in the AI field and often leaves you to wonder: *what if I let that agent train
    a day longer*? Unfortunately, if you speak to experts, they will often say just
    be patient and keep training, but this perhaps builds on those frustrations. After
    all, what if what you built has no hope of ever doing anything—are you wasting
    time and energy to keep it going?'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何新进入深度学习领域的人来说，尤其是对于深度强化学习，一个常见的问题就是何时知道你的模型是失败的，只是有点固执，或者永远都不会工作。这是一个在人工智能领域引起挫败感和焦虑的问题，常常让你想知道：“如果我让那个智能体再训练一天会怎样？”不幸的是，如果你和专家交谈，他们通常会说你只需要耐心，继续训练，但这可能是在那些挫败感的基础上。毕竟，如果你构建的东西永远没有希望做任何事情，你是在浪费时间精力去维持它吗？
- en: Another issue that many face is that the more complex an algorithm/model gets,
    the more time it takes to train, except you never know how long that is unless
    you trained it before or read a really well-written paper that uses the exact
    same model. Even with the same exact model, the environment may also differ perhaps
    being more complex as well. With all of these factors at play, as well as the
    pain of tuning hyperparameters, it is a wonder why anyone of sane mind would want
    to work in DRL at all.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人都面临的一个问题是，算法/模型越复杂，训练所需的时间就越长，除非你之前训练过它或者阅读过一篇使用完全相同模型的非常优秀的论文。即使使用完全相同的模型，环境也可能有所不同，可能更加复杂。考虑到所有这些因素，以及调整超参数的痛苦，真让人惊讶，为什么任何理智的人会想要在强化学习（DRL）领域工作。
- en: The author hosts a Deep Learning Meetup support group for RL and DRL. One of
    the frequent discussions in this group is how DL researchers can keep their sanity
    and/or reduce their stress levels. If you work in AI, you understand the constant
    need to live up to the hype overcoming the world. This hype is a good thing but
    can also be a bad thing when it involves investors or impatient bosses.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 作者主持了一个针对强化学习（RL）和深度强化学习（DRL）的深度学习Meetup支持小组。这个小组中经常讨论的一个话题是，深度学习研究人员如何保持他们的理智和/或降低他们的压力水平。如果你从事人工智能工作，你就会理解不断满足世界期望的必要性，这种期望是好事，但涉及投资者或缺乏耐心的老板时，也可能是一件坏事。
- en: Fortunately, with advanced tools such as TensorBoard, we can gain insights into
    how are agent trains or hopes to train.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有了像TensorBoard这样的高级工具，我们可以深入了解代理是如何训练或希望如何训练的。
- en: 'Open up TensorBoard and follow the next exercise to see how to effectively
    diagnose training problems:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 打开TensorBoard并遵循下一个练习，了解如何有效地诊断训练问题：
- en: 'The output from TB is shown in the following screenshot:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下截图显示了TB的输出：
- en: '![](img/eea7ff49-3611-4ef4-a3a5-4503e588c03a.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eea7ff49-3611-4ef4-a3a5-4503e588c03a.png)'
- en: TensorBoard output from Rainbow DQN
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Rainbow DQN的TensorBoard输出
- en: From the preceding screenshot, where the **Smoothing** has been upped to .99,
    we can see training is failing. Remember, the graphs in the screenshot are annotated
    to show the preferred direction. For all of those plots, that is not the case.
    However, don't assume that if the plot is going in the opposite direction that
    it is necessarily bad—it isn't. Instead, any movement is a better indication of
    some training activity. This is also the reason we smooth these plots so highly.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从前面的截图可以看出，**平滑**已经提高到.99，我们可以看到训练失败了。记住，截图中的图表都有注释，以显示首选的方向。对于所有这些图表，情况并非如此。然而，不要假设如果图表的方向相反，它就一定是坏的——它不是。相反，任何移动都是一些训练活动的更好指标。这也是我们为什么要高度平滑这些图表的原因。
- en: 'The one key plot that often dictates future training performance is the **Losses**
    plot. An agent will be learning when losses are decreasing and will be forgetting/confused
    if the losses are increasing. If losses remain constant, then the agent is stagnant
    and could be confused or stuck. A helpful summary of this is shown in the screenshot
    here:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 常常决定未来训练性能的一个关键图是**损失**图。当损失减少时，代理在学习；当损失增加时，代理可能会忘记/困惑。如果损失保持不变，那么代理可能停滞不前，可能会困惑或陷入困境。这里展示的截图是对这一点的有益总结：
- en: '![](img/bc442dbe-4558-4574-a52b-73049901ac6a.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bc442dbe-4558-4574-a52b-73049901ac6a.png)'
- en: The Losses plot summarized
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 损失图总结
- en: The preceding screenshot shows the ideal training over 500 thousand episodes
    and for this environment, you can expect to train double or triple that amount.
    As a general rule, it is best to consider no movement, positive or negative, over
    10% or the training time to be a failure. For example, if you are training an
    agent for 1 million iterations, then your 10% window would be about 100 thousand
    iterations. If your agent is training constantly or flat-lining in any plot, aside
    from Advantage, over a period equal to or larger than the 10% window size, it
    may be best to tune hyperparameters and start again.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述截图显示了在超过50万次回合的理想训练过程中，对于这个环境，你可以预期训练的回合数是原来的两倍或三倍。一般来说，如果训练时间超过10%，无论是正向还是负向的移动，都应被视为失败。例如，如果你正在训练一个代理进行100万次迭代，那么你的10%窗口大约是10万次迭代。如果你的代理在某个图上持续训练或处于平坦状态，除了优势之外，在等于或大于10%窗口大小的期间，可能最好调整超参数并重新开始。
- en: Again, pay special attention to the Losses plot as this provides the strongest
    indicator for training problems.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次强调，特别关注损失图，因为它提供了训练问题的最强指标。
- en: 'You can run view the results of multiple training efforts side by side by just
    running the sample repeatedly for the same number of iterations, as the screenshot
    here shows:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过重复运行样本相同的迭代次数来并排查看多个训练尝试的结果，如图中所示：
- en: '![](img/0cbc852e-2d96-4c24-ae03-0a8cd81e8af4.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0cbc852e-2d96-4c24-ae03-0a8cd81e8af4.png)'
- en: Examples of multiple training outputs on the same graph
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 同一图表上的多个训练输出示例
- en: Stop the current sample change some hyperparameters and run it again to see
    the preceding example.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止当前样本，更改一些超参数，然后再次运行以查看前面的示例。
- en: These simple rules will hopefully help you to avoid frustrations on building/training
    your own models on new or different environments. Fortunately, we have several
    more chapters to work through and that includes plenty of more exercises like
    those featured in the next section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的规则可能会帮助您在构建/训练新或不同环境中的模型时避免挫败感。幸运的是，我们还有更多章节要学习，其中包括大量类似下一节中展示的练习。
- en: Exercises
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: 'When it comes to working in the real world, the experience you build from doing
    these exercises may mean the difference between not getting that job and certainly
    keeping it. As a programmer, you don''t have the luxury of just understanding
    how something works; you''re a mechanic/engineer that needs to get their hands
    dirty and actually do the work:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到在现实世界中工作时，您从这些练习中获得的经验可能意味着得到那份工作与保住那份工作的区别。作为一名程序员，您不仅需要理解某物是如何工作的；您是一个需要亲自动手并实际工作的机械师/工程师：
- en: Tune the hyperparameters for `Chapter_10_QRDQN.py` and see what effect this
    has on training.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_10_QRDQN.py`的超参数，并查看这对训练有什么影响。
- en: Tune the hyperparameters for `Chapter_10_NDQN.py` and see what effect this has
    on training.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_10_NDQN.py`的超参数，并查看这对训练有什么影响。
- en: Tune the hyperparameters for `Chapter_10_Rainbow.py` and see what effect this
    has on training.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整`Chapter_10_Rainbow.py`的超参数，并查看这对训练有什么影响。
- en: Run and tune the hyperparameters for any of this chapter's samples on another
    environment such as CartPole or FrozenLake or something more complex such as Atari.
    Reducing the complexity of an environment is also helpful if your computer is
    older and needs to work harder training agents.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在另一个环境（如CartPole、FrozenLake或更复杂的环境如Atari）上运行和调整本章样本的超参数。如果您的电脑较旧且需要更努力地训练智能体，降低环境的复杂性也是有帮助的。
- en: This chapter also includes sample code for Hierarchical DQNs and Categorical
    DQNs in the `Chapter_10_HDQN.py` and `Chapter_10_C51.py` samples. Run these examples,
    review the code, and do some investigation on your own on what improvements these
    samples bring to DRL.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本章还包括`Chapter_10_HDQN.py`和`Chapter_10_C51.py`示例中的分层DQNs和分类DQNs的示例代码。运行这些示例，审查代码，并自行研究这些示例为DRL带来的改进。
- en: Add the ability to save/load the trained model from any of the examples. Can
    you now use the trained model to show the agent playing the game?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加从任何示例中保存/加载训练模型的功能。现在您可以使用训练好的模型来展示智能体玩游戏吗？
- en: Add the ability to output other training values to TensorBoard that you may
    think are important to training an agent.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加将其他可能认为对训练智能体重要的训练值输出到TensorBoard的功能。
- en: Add `NoisyLinear` layers to the `Chapter_10_QRDQN.py` example. There may already
    be code that is just commented out in the example.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`NoisyLinear`层添加到`Chapter_10_QRDQN.py`示例中。示例中可能已经存在一些只是被注释掉的代码。
- en: Add a prioritized replay buffer to the `Chapter_10_Rainbow.py` example. You
    can use the same method found in the `Chapter_10_NDQN.py` example.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优先级回放缓冲区添加到`Chapter_10_Rainbow.py`示例中。您可以使用在`Chapter_10_NDQN.py`示例中找到的相同方法。
- en: TensorBoard allows you to output and visualize a trained model. Use TensorBoard
    to output the trained model from one of the examples.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TensorBoard允许您输出和可视化训练模型。使用TensorBoard从示例中输出训练模型。
- en: Obviously, the number of exercises has increased to reflect your increasing
    skill level and/or interest in DRL. You certainly don't need to complete all of
    these exercises but 2-3 will go a long way. In the next section, we will summarize
    the chapter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，练习的数量已经增加，以反映您在DRL技能水平和/或兴趣上的增长。您当然不需要完成所有这些练习，但2-3个就足够了。在下一节中，我们将总结本章内容。
- en: Summary
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked specifically at one of the more state-of-the-art
    advances in DRL from DeepMind called Rainbow DQN. Rainbow combines several improvements
    layered on top of DQN that allow dramatic increases in training performance. As
    we have already covered many of these improvements, we only needed to review a
    couple of new advances. Before doing that though, we installed TensorBoard as
    a tool to investigate training performance. Then, we looked at the first advancement
    in distributional RL and how to model the action by understanding the sampling
    distribution. Continuing with distributions, we then looked at noisy network layers—network
    layers that don't have individual weights but rather individual distributions
    to describe each weight. Building on this example, we moved onto Rainbow DQN with
    our last example, finishing off with a quick discussion on when to determine whether
    an agent is not trainable or flat-lining.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们特别关注了DeepMind在DRL领域的一项更先进的进展，称为Rainbow DQN。Rainbow在DQN的基础上结合了多项改进，这些改进显著提高了训练性能。由于我们已经涵盖了这些改进中的许多，我们只需要回顾一些新的进展。然而，在这样做之前，我们安装了TensorBoard作为调查训练性能的工具。然后，我们探讨了分布式RL的第一个进展以及如何通过理解采样分布来建模动作。继续探讨分布，我们接下来研究了有噪声的网络层——这些网络层没有单个权重，而是有描述每个权重的单个分布。基于这个例子，我们转向了Rainbow
    DQN，在我们的最后一个例子中完成，并快速讨论了何时确定智能体不可训练或表现停滞不前。
- en: For the next chapter, we will move from building DRL algorithms/agents to building
    environments with Unity and constructing agents in those environments with the
    ML-Agents toolkit.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从构建DRL算法/智能体转向使用Unity构建环境，并使用ML-Agents工具包在这些环境中构建智能体。
