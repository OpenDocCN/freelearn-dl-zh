<html><head></head><body>
		<div id="_idContainer093">
			<h1 class="chapter-number" id="_idParaDest-190"><a id="_idTextAnchor232"/><a id="_idTextAnchor233"/>11</h1>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor234"/>Common Failures of Generative AI</h1>
			<p>If you have just built your <strong class="bold">generative AI</strong> (<strong class="bold">GenAI</strong>) application, then you may be so fascinated by what it can do that you lose sight of answer quality and accuracy. Discovering how often GenAI is incorrect is a challenge <span class="No-Break">in itself.</span></p>
			<p>Many tend to believe that when a computer gives an answer, it gives an accurate answer—usually, more accurate than a human being. For example, most people feel relieved that machines, and not just people, fly airplanes today. Airplanes may be much safer now compared to 15 years ago because of this advancement, but when it comes to GenAI, the results are not nearly as accurate as the onboard systems of a <span class="No-Break">flight craft.</span></p>
			<p>This chapter takes a detailed look at the top five challenges with GenAI applications and why they occur. Understanding these challenges is crucial for developers to devise effective solutions. By the end of this chapter, you will have a good understanding of these challenges, how they influence your outcomes, how they relate to each other, and why this particular set of technologies, despite these challenges, is still highly valuable <span class="No-Break">to users.</span></p>
			<p>This chapter will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Hallucinations</span></li>
				<li><span class="No-Break">Sycophancy</span></li>
				<li><span class="No-Break">Data leakage</span></li>
				<li><span class="No-Break">Cost optimization</span></li>
				<li><span class="No-Break">Performance issues</span></li>
			</ul>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor235"/>Technical requirements</h1>
			<p>Most of the examples in this chapter can be demonstrated by simply repeating the prompt or example <span class="No-Break">in ChatGPT.</span></p>
			<h1 id="_idParaDest-193"><a id="_idTextAnchor236"/>Hallucinations</h1>
			<p>One of the greatest challenges of working with GenAI, and perhaps the most well-known, is <strong class="bold">hallucination</strong>. Hallucination in GenAI refers to the phenomenon where the AI model generates content that sounds plausible but is factually incorrect, nonsensical, or not grounded in the provided input data. This issue is particularly prevalent in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) models, such as those used for text generation, but can also occur in other generative models such as image generation and LLMs such <span class="No-Break">as GPT-4.</span></p>
			<p>In the worst case, both the developers and their users do not know whether the answer given by GenAI is correct, partially correct, mostly incorrect, or a <span class="No-Break">complete fabrication.</span></p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor237"/>Causes of hallucinations</h2>
			<p>Much of the data that organizations capture is either <strong class="bold">redundant, obsolete, trivial</strong> (<strong class="bold">ROT</strong>), or altogether unclassified. As a portion, <em class="italic">good</em> data forms a small fraction of the data lakes, warehouses, and databases that most companies have. Whenever beginning your GenAI application journey, one of the first things you’re likely to notice is that much of the data you’d like to use to train your GenAI application is poor quality. Shortly thereafter, you’ll learn that hallucinations are caused by <strong class="bold">poor-quality </strong><span class="No-Break"><strong class="bold">training data</strong></span><span class="No-Break">.</span></p>
			<p>Engineers can best think of this as a <strong class="bold">garbage in, garbage out</strong> problem. When training data has errors, inconsistencies, irrelevancy, outdated information, biased information, and other issues, the model will learn to replicate those problems. The accuracy of an AI model is heavily dependent on the quality of training data, and the following data issues are more likely to cause output problems <span class="No-Break">and hallucinations:</span></p>
			<ul>
				<li><strong class="bold">Inaccurate data</strong>: Errors in the input will propagate and compound in the system, so it is critical to know that any automated or real-time data streaming to your GenAI application has accurate information. For example, if you’re ingesting sensor data to predict when equipment will fail but receive inaccurate sensor readings, then your GenAI application may not predict the failure correctly or in a <span class="No-Break">timely way.</span></li>
				<li><strong class="bold">Incomplete data</strong>: Training on incomplete datasets can cause the model to generate plausible but incorrect content to fill <span class="No-Break">perceived gaps.</span></li>
				<li><strong class="bold">Outdated or obsolete data</strong>: At its heart, obsolete data is often simply no longer accurate, providing AI with false information. Relevant data updates ensure that your GenAI application continues to provide your users with <span class="No-Break">accurate outputs.</span></li>
				<li><strong class="bold">Irrelevant data</strong>: It can be tempting to stuff your GenAI application with as much data as possible so that it can use that information for analysis; however, this is a way to increase costs without <span class="No-Break">improving accuracy.</span></li>
				<li><strong class="bold">Misleading or misrepresentative data</strong>: If a machine learning model is trained on images that are poorly labeled or unrepresentative of real-world scenarios, it will struggle to correctly identify or classify images <span class="No-Break">when deployed.</span></li>
				<li><strong class="bold">Duplicated data</strong>: This also includes poorly integrated datasets. Redundant data can give AI the impression that something is more important than it is because <span class="No-Break">it’s repeated.</span></li>
				<li><strong class="bold">Model architecture and objectives</strong>: Models such as GPT-4 are trained to predict the next word in a sequence based on context, and not necessarily to verify facts. This objective can lead to the model generating fluent text that is not <span class="No-Break">factually accurate.</span></li>
			</ul>
			<p>Each of these causes slightly different issues and, in combination, can make your GenAI application incapable of producing satisfactory results. Therefore, your training data must be accurate, comprehensive, and representative of the diverse conditions the model will encounter in real-world applications. Much of GenAI is continuously self-learning, so maintaining data quality is an ongoing issue, not a <span class="No-Break"><strong class="bold">first-deploying-to-production</strong></span><span class="No-Break"> issue.</span></p>
			<p>Generative models focus on producing outputs that are coherent and contextually relevant, which sometimes comes at the expense of factual correctness. These models are also excellent at recognizing and replicating patterns in data. However, this can result in outputs that follow learned patterns even when those patterns do not align with factual reality. This is the <strong class="bold">correlation, not </strong><span class="No-Break"><strong class="bold">causation</strong></span><span class="No-Break"> issue.</span></p>
			<p>Also, models are trained on static datasets and lack real-time access to updated information, which can lead to outdated or incorrect outputs. For instance, GPT and its ilk are trained on data scraped from the web several months (or even years!) ago. Products, insights, and world news from yesterday are not available. When asking questions about recent events, in the best case, the user receives an answer such as <strong class="source-inline">I do not have this information</strong>. In the worst case, the GenAI application simply hallucinates a response. Generative models may not fully understand the context or possess the real-world knowledge required to validate the correctness of <span class="No-Break">generated information.</span></p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor238"/>Implications of hallucinations</h2>
			<p>Besides just “being wrong” and “making up answers,” hallucinations can have other unexpected implications. Misinformation can easily be propagated to thousands of people, some of whom may find it difficult to turn around later. For instance, if today, ChatGPT (a popular GenAI model) started telling every person who asked that a popular open-source project has a critical vulnerability, then the news would spread like wildfire, making damage control difficult. It would reach many more people than the statement put out on the company blog about how the information wasn’t true. Many users trust the AI’s output <span class="No-Break">without verification.</span></p>
			<p>Hallucinations undermine the reliability of AI systems, particularly in fields such as healthcare, legal, or financial services, where accuracy is paramount. Moreover, consistent hallucinations can erode user trust in AI applications, leading to reduced adoption and skepticism regarding <span class="No-Break">AI capabilities.</span></p>
			<p>Incorrect information can lead to ethical dilemmas and potential legal liabilities, especially if the AI’s output influences critical decisions or public opinion. As GenAI is added into all sorts of applications, it becomes more and more difficult to both opt out of (for the users) and discern whether the answers <span class="No-Break">are legitimate.</span></p>
			<p>It is worth saying also that receiving an answer that is not a hallucination is far different from receiving the <span class="No-Break">best answer.</span></p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor239"/>Sycophancy</h1>
			<p>A sycophant is a person who does whatever they can to win your approval, even at the cost of their ethics or knowledge of what is true. AI models demonstrate this behavior often enough for AI researchers and developers to use the same term—<strong class="bold">sycophancy</strong>—to describe how models respond to human feedback and prompting in deceptive or problematic ways. Human feedback is commonly utilized to fine-tune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a trait known as sycophancy. Sycophancy exists in multiple ways, such as mirroring feedback, easily being swayed, and changing correct answers if the user pushes back. If users share their beliefs and views on a topic, AI assistants will provide answers that align with the <span class="No-Break">user’s beliefs.</span></p>
			<p>Sycophancy can be observed and described on multiple levels, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Feedback sycophancy</strong>: When users express likes or dislikes about a text, AI assistants may provide more positive or negative <span class="No-Break">feedback accordingly</span></li>
				<li><strong class="bold">Swaying easily</strong>: After answering a question correctly, AI assistants may change their answer when users challenge them, even if the original answer <span class="No-Break">was correct</span></li>
				<li><strong class="bold">Belief conformity</strong>: When users share their views on a topic, AI assistants tend to provide answers that align with those beliefs, leading to <span class="No-Break">decreased accuracy</span></li>
			</ul>
			<p>In testing, researchers Mrinank Sharma et al. demonstrated sycophantic answers generated by Claude (<a href="https://arxiv.org/abs/2310.13548"><span class="P---URL">https://arxiv.org/abs/2310.13548</span></a>), as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer092">
					<img alt="" role="presentation" src="image/B22495_11_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1: Example responses demonstrating sycophancy</p>
			<p>It is worth noting that repeated testing of the same and similar questions in ChatGPT did not yield <span class="No-Break">consistent results.</span></p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor240"/>Causes of sycophancy</h2>
			<p>The exact causes of sycophancy are not well understood. This phenomenon exists in many LLMs because these models have been instructed to take in contextual and parametric information to inform their responses. GenAI applications have a <em class="italic">learning</em> feature where the more they interact with users, the more they learn about syntax, context, and providing sufficient answers. As they do so, the applications exhibit what can only be described as <em class="italic">people-pleasing behaviors</em>, causing them to deviate from a purely factual relaying <span class="No-Break">of information.</span></p>
			<p>In the above research, it was found that sycophancy is a side effect of RLHF-like alignment training. <strong class="bold">Reinforcement learning from human feedback</strong> (<strong class="bold">RLHF</strong>) is a technique that is used to train LLMs to align the agent or machine with human preferences. This is particularly important in areas such as language models. To illustrate this, let’s look at some examples of what this means and why <span class="No-Break">it matters.</span></p>
			<p>Consider <span class="No-Break">the following:</span></p>
			<p>When you greet a coworker, you might say “Hello, sir/madam,” “Hello,” “Good morning,” “Good day,” “Hi,” “What’s up,” “Greetings,” or many other potential salutations. Hypothetically, all are appropriate, but there are human preferences as to which is <span class="No-Break">more suitable.</span></p>
			<p>To further understand this, let’s begin with cultural preference. In some cultures, it would be shocking indeed if you did not include the coworker’s name, as in “Good morning, Mr. Smith.” Yet in other cultures, to address someone in this manner would seem exceedingly strange. The human preference on which greeting is preferred has some basis, part of which is cultural, part of which is situational and contextual (is Mr. Smith the president? Is he your 20-year-old new hire?), and part of which is purely you, <span class="No-Break">the individual.</span></p>
			<p>Engineers decided that when people interact with GenAI, they prefer that their conversations and interactions feel human. To do that, the machines must consider cultural, situational, behavioral, and, to some extent, <span class="No-Break">individual preferences.</span></p>
			<p>Training models have access to vast amounts of information, both contextual (passages of text from websites, books, research, etc.) and parametric (embeddings of nearest-neighbor words). They will use any cultural, contextual, or behavioral clues that the user provides to help inform their answer. That is, how the user phrases the question influences <span class="No-Break">the answer.</span></p>
			<p>ChatGPT confirms this. When asked how it arrives at an answer, it states the <span class="No-Break">following clearly:</span></p>
			<pre class="source-code">
I assess the context of your question. For instance, if you've mentioned the setting (formal or informal), the relationship with the coworker, or any specific preferences, I take those into account.
If we've interacted before, I consider any speech patterns or preferences you've shown in previous conversations. This helps tailor the response to your style and needs.
I use general knowledge about cultural and social norms to gauge what might be most appropriate. For example, formal greetings are more suitable in professional settings, while casual greetings work better in relaxed environments.</pre>			<p>It is possible to request GenAI to disregard your previous interactions, personal preferences, syntax, and/or any data it has concluded about you before it creates answers to your questions, but, of course, this would require the user to know that this is happening in the <span class="No-Break">first place.</span></p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor241"/>Implications of sycophancy</h2>
			<p>As helpful as this functionality is, it has real-world implications for the outputs of GenAI applications. In the same research paper cited earlier in this chapter (<a href="https://arxiv.org/abs/2310.13548"><span class="P---URL">https://arxiv.org/abs/2310.13548</span></a>), it was determined that the consequences of sycophancy, while machine in origin, can result in incorrect deference to user opinion, propagation of user-created errors, and biased responses. Therefore, instead of helping create a more factual and consistent understanding of the world, GenAI perpetuates and perhaps accelerates the spread <span class="No-Break">of misinformation.</span></p>
			<p>Researchers at Google DeepMind found that the problem grew worse as the model became bigger (<a href="https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/"><span class="P---URL">https://www.newscientist.com/article/2386915-ai-chatbots-become-more-sycophantic-as-they-get-more-advanced/</span></a>). LLMs with more parametric inputs had a greater tendency to agree with objectively false statements than smaller ones. This tendency held true even for mathematical equations, that is, questions where there is only one <span class="No-Break">correct answer.</span></p>
			<p>LLMs are constantly learning, evolving, and being improved by their creators. In the future, perhaps LLMs will weigh the objective truth of a statement higher than the opinion or preferences of the user, but as of 2023, that is yet to happen. Ongoing research and testing will make them ever more adept at balancing user expectations, user opinions, and facts. Still, as of the time this book was written, sycophancy remains a primary concern with GenAI applications, particularly where the outputs consider opinions and user preferences before generating their response. Further testing using synthetic data and retraining models has reduced the tendency of sycophancy by up to 10%, which is still not 100% (<a href="https://arxiv.org/abs/2308.03958"><span class="P---URL">https://arxiv.org/abs/2308.03958</span></a>). This means that the tendency persists, even with fairly substantial modifications to <span class="No-Break">the fine-tuning.</span></p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor242"/>Data leakage</h1>
			<p><strong class="bold">Data leakage</strong>, in the context of GenAI, refers to situations where information from outside the desired training dataset is used to create the model, leading to overly optimistic performance metrics and potentially flawed or misleading predictions. This can happen at various stages of model development, from data collection to model evaluation, and can significantly compromise the validity of the AI system. There are multiple types of datasets with <span class="No-Break">different purposes:</span></p>
			<ul>
				<li>Training datasets, which are used to train <span class="No-Break">the LLM</span></li>
				<li>Fine-tuning datasets, which can be used to improve LLM responses and <span class="No-Break">reduce hallucinations</span></li>
				<li>Evaluation datasets, which can be useful in evaluating the accuracy <span class="No-Break">of responses</span></li>
			</ul>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor243"/>Causes of data leakage</h2>
			<p>The causes of data leakage are straightforward and easily avoided, as long as the developers of these applications are aware of these causes. First, let’s understand at a high level what leads to <span class="No-Break">data leakage:</span></p>
			<ul>
				<li><strong class="bold">Inappropriate dataset overlap</strong>: Each dataset should be used at the appropriate training and evaluation stage. When this is not true, you have data leakage. For example, when the training dataset overlaps with the evaluation dataset, GenAI applications will, of course, perform better during testing because they already know the exact answers. In this scenario, your stock price predictor application would have had duplicated historical data points present in its training and evaluation datasets; therefore, its performance when testing its outputs will be unrealistically high because it has already seen <span class="No-Break">the answers.</span></li>
				<li><strong class="bold">Future information</strong>: Each dataset should only include information that would be available at the time of prediction. For instance, you would not include real or hypothetical information in your training dataset from a period in the future, or data that the model would not typically have access to <span class="No-Break">in production.</span></li>
				<li><strong class="bold">Data normalization and transformation efforts</strong>: When transformations or feature-engineering steps inadvertently introduce data from outside the training set, it is possible for information to leak from evaluation datasets into the training process. For GenAI, you want training data that is as close to <em class="italic">real life</em> as possible, both in terms of user interaction and whatever context the application will be operating within, so that your application has truly <span class="No-Break">representative data.</span></li>
			</ul>
			<p>To illustrate these causes, let’s use a hypothetical GenAI application that predicts stock prices upon request using historical data. In this scenario, it is May 2024, and your application is in the final testing phases. Before pushing to production, you’d like to determine how accurate its predictions are. You begin by checking your application’s response to the following <span class="No-Break">user request.</span></p>
			<p><span class="No-Break"><strong class="bold">User request</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
Predict the average stock price for $TSLA in May 2024.</pre>			<p><span class="No-Break"><strong class="bold">Output answer</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
The average stock price for $TSLA in May 2024 is expected to be $176.</pre>			<p>In this example, note <span class="No-Break">the following:</span></p>
			<ul>
				<li>The <strong class="bold">training</strong> data fed to the model should <strong class="bold">not</strong> include any data points from <span class="No-Break">May 2024.</span></li>
				<li>The <strong class="bold">evaluation</strong> dataset should include all prices from May 2024 and could include the actual calculated value of the average stock price. This is because you would like to compare the model’s estimate to the actual value, and then give it a score for accuracy, then plot that month over month, in order to see whether the application consistently makes low or <span class="No-Break">high estimates.</span></li>
			</ul>
			<p>If you’re trying for accuracy with your May 2024 estimate, but you’ve already fed it the May 2024 data in the training phase, this would be considered inappropriate dataset overlap. Let’s look at <span class="No-Break">another example.</span></p>
			<p><span class="No-Break"><strong class="bold">User request</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
Predict the average annual price for $TSLA in 2024.</pre>			<p><span class="No-Break"><strong class="bold">Output answer</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
The average annual stock price for $TSLA in May 2024 is predicted to be $205.</pre>			<p>You would not provide a training dataset that already includes an annual average because that information is not yet available. While you could include a year-to-date average in the training dataset, you should not include an annual average with synthetic or generated forward-looking data. If you created an estimated annual stock price and included that in the training data, then you would be using future information. Now, let’s consider a <span class="No-Break">final example.</span></p>
			<p><span class="No-Break"><strong class="bold">User request</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
What is the average stock price for $TSLA in May 2024?</pre>			<p><span class="No-Break"><strong class="bold">Output answer</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
The average stock price for $TSLA in May 2024 is expected to be $176.</pre>			<p>Notice how the user query is worded differently here as compared to the first example, though it leads to the same answer. LLMs are quite skilled at inferring user intention. Remember that users asking even fairly simple questions will phrase them in many different ways (<em class="italic">estimate</em>, <em class="italic">predict</em>, <em class="italic">forecast</em>, <em class="italic">imagine</em>, <em class="italic">guess</em>, and <em class="italic">projection</em> are all words they <span class="No-Break">might use).</span></p>
			<p>For your training dataset, you might include a prompt-and-answer pairing in the style of <strong class="bold">frequently asked questions</strong> (<strong class="bold">FAQs</strong>) for your entire support database. However, resist the urge to correct aspects such as wording and spelling. While you want to be aware of “garbage in, garbage out” problems, you do not want to shield your GenAI application so much that it won’t know how to respond when your users inevitably input garbage. This is particularly relevant for GenAI chatbots. Users have so many ways of asking a question. Those questions are presented usually without proper syntax, terminology, or contextual awareness, and their knowledge may also be outdated. Data normalization and transformation efforts should not normalize and cleanse your training data so much so that it becomes <span class="No-Break">less useful.</span></p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor244"/>Implications of data leakage</h2>
			<p>The implications of data leakage vary widely, depending on whether you’ve leaked a teardrop or a waterfall. If there is data leakage, then the results of your GenAI evaluation and testing prior to production will be wrong and misrepresentative of your application’s actual performance, leading to overly optimistic tests or misleading conclusions. In all data overlap cases, the most obvious consequence of overlapping the training and test datasets is that the model may learn to simply <em class="italic">memorize</em> the training data and perform poorly on any new data from which it must <span class="No-Break">make predictions.</span></p>
			<p>This can give application developers and testers a false sense of confidence in the model’s performance. Later, when real-world data is offered and users are asking questions in production, the application will perform <span class="No-Break">markedly worse.</span></p>
			<p>Avoiding data leakage is simple, and it begins with splitting your datasets into distinct entities, then doing <span class="No-Break">the following:</span></p>
			<ul>
				<li>Ensure that training, validation, and test datasets are strictly separated. Use techniques such as time-based splitting for time-series data to prevent future information from leaking into the <span class="No-Break">training set.</span></li>
				<li>Use tools to ensure that data transformations are only applied to the training set during model training and applied to the test set independently <span class="No-Break">during evaluation.</span></li>
				<li>Engineer features in a way that prevents future data from being used. Avoid using future values or aggregated future statistics as part of your <span class="No-Break">training data.</span></li>
			</ul>
			<p>Returning to the stock price prediction application, you would ideally want the data for your training and test sets to be based on time, ensuring that stock prices in the training set occur chronologically before those in the test set. Then, your application would only have features that were used in the historical stock data available up to the point of the stock price being predicted, marking a clear delineation between authentic prior stock prices and predicted future stock prices. Next, to validate your application, use time-based cross-validation to ensure that model performance is evaluated on data that simulates real-world prediction scenarios or the scenarios your application <span class="No-Break">would allow.</span></p>
			<p>By rigorously managing how data is handled throughout the model development process, you can minimize the risk of data leakage and ensure that your GenAI model provides reliable and <span class="No-Break">valid predictions.</span></p>
			<h1 id="_idParaDest-202"><a id="_idTextAnchor245"/>Cost</h1>
			<p>With so many distinct, complex, and potentially expensive moving parts, it is critical for engineers to know the costs of their GenAI application and how to contain these costs. While you will learn more about cost optimization strategies in <a href="B22495_12.xhtml#_idTextAnchor253"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, <em class="italic">Correcting and Optimizing Your Generative AI Application</em>, this section will serve as an introduction to understanding the financial costs of GenAI applications, which are in some ways different from web <span class="No-Break">development applications.</span></p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor246"/>Types of costs</h2>
			<p>When using GenAI, costs can arise from several different areas. These costs can be broadly categorized into computational, storage, data acquisition, development, and <span class="No-Break">maintenance costs:</span></p>
			<ul>
				<li><strong class="bold">Training costs</strong>: Training GenAI models requires significant computational resources. This is especially true for large models such as GPT-4. These resources often include <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>) or <strong class="bold">tensor processing units</strong> (<strong class="bold">TPUs</strong>), which are optimized for parallel processing tasks. The infrastructure to support these setups consumes a lot of electricity and requires cooling systems to maintain operational temperatures. Most engineers may not be in the position to pay these costs and, instead, will utilize models from vendors, such as OpenAI, Anthropic, Google, Meta, <span class="No-Break">or others.</span></li>
				<li><strong class="bold">Inference, or real-time computation</strong>: Generating responses or outputs from a trained model, which is called <strong class="bold">inference</strong>, also incurs computational costs, especially for models that need to provide real-time answers. Bigger models <span class="No-Break">cost more.</span></li>
				<li><strong class="bold">Storage costs</strong>: Storing large datasets required for training GenAI models incurs costs. This includes raw data, preprocessed data, user interaction data, observability data, and the <span class="No-Break">models themselves.</span></li>
				<li><strong class="bold">Data collection</strong>: Acquiring high-quality datasets can be expensive. This can include purchasing data from third-party providers or generating <span class="No-Break">proprietary datasets.</span></li>
				<li><strong class="bold">Data labeling and cleaning</strong>: Preprocessing data to ensure it is suitable for training involves costs. This can include paying for human annotators to label data or developing algorithms to clean and prepare the data as either training or <span class="No-Break">evaluation datasets.</span></li>
				<li><strong class="bold">Software development</strong>: Writing and maintaining the code base for training and deploying GenAI applications requires skilled engineers and <span class="No-Break">data analysts.</span></li>
				<li><strong class="bold">Experimentation and testing</strong>: Developing GenAI often involves extensive experimentation and fine-tuning, which requires time <span class="No-Break">and resources.</span></li>
				<li><strong class="bold">Data updates</strong>: Training and evaluation datasets require periodic updates to maintain their accuracy and relevance, which involves additional computational and <span class="No-Break">human resources.</span></li>
				<li><strong class="bold">Monitoring and support</strong>: The continuous monitoring of AI systems to ensure they are performing correctly and handling issues as they arise involves <span class="No-Break">operational costs.</span></li>
				<li><strong class="bold">Compliance and security</strong>: Ensuring data privacy and security and complying with regulations (such as GDPR) involves <span class="No-Break">additional costs.</span></li>
			</ul>
			<p>This is not an exhaustive list. Therefore, estimating your expected costs is complex and a non-trivial endeavor. But let’s hone in on the most important cost driver, which is text, and therefore tokens. Next, you will learn how to estimate and control <span class="No-Break">costs here.</span></p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor247"/>Tokens</h2>
			<p>LLMs process text using <strong class="bold">tokens</strong>, which are common sequences of characters found in a set of text. Tokens are the currency of the GenAI application. Each user input and output is a <em class="italic">token</em>, and both the question and response token count can be controlled. The cost per token is tiny. GenAI vendors look to make their money <em class="italic">per transaction</em>, which can add <span class="No-Break">up quickly.</span></p>
			<p>Let’s understand this concept with an example. The statement <strong class="source-inline">Hello how are you</strong> is 5 tokens. A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so, 100 tokens ~= 75 words). The example of <strong class="source-inline">Hello how are you</strong> has 18 characters including spaces, therefore 18/4 = 4.5, ergo <span class="No-Break">5 tokens.</span></p>
			<p>Each input and output for the GenAI application is reduced down to this simple unit <span class="No-Break">of measurement.</span></p>
			<p>ChatGPT and other LLMs have a token limit, thus capping how much text the user can enter as their prompt and limiting the output response. These limits, however, are generous. For most use cases, it is unlikely the average consumer would hit <span class="No-Break">these limits.</span></p>
			<p>For instance, GPT-4 has a token limit of 32,768 per interaction and an estimated word count of 25,000 words, whereas Claude 3 (the LLM hosted by Anthropic) has a token limit of 100,000+ as of the time of writing. For a simple customer service chatbot, it is very unlikely that you would hit this limit, but it <em class="italic">is</em> possible. Let’s look at two examples to explain how this might <span class="No-Break">be true.</span></p>
			<p><strong class="bold">Example 1</strong>: A customer asks a GenAI chatbot a <span class="No-Break">simple question.</span></p>
			<p><span class="No-Break"><strong class="bold">Inquiry</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
what is the latest version of python</pre>			<p><span class="No-Break"><strong class="bold">Response</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
The latest stable version of Python is 3.12.4, released on June 6, 2024. This release is part of the Python 3.12 series, which includes many new features and optimizations. Key improvements in this version include enhanced f-string parsing, support for isolated subinterpreters with separate Global Interpreter Locks, and numerous performance enhancements (Python.org) (Python.org).
Additionally, Python 3.13 is currently in development, with the first beta release, 3.13.0b1, made available in May 2024. This upcoming version introduces new features such as an improved interactive interpreter, experimental JIT support, and an incremental garbage collector (Python Insider). For more details, you can visit the official Python website here.</pre>			<p>The initial inquiry is 36 characters and 7 tokens. The response is 162 tokens, or 741 characters. If you were to have interactions limited to 300 tokens, you would still be well under the limit that you have allowed for <span class="No-Break">your users.</span></p>
			<p><strong class="bold">Example 2</strong>: A server experiences an out-of-memory error, and the GenAI automatically analyzes the stack traces and logs an analysis for a human to <span class="No-Break">review later.</span></p>
			<p><span class="No-Break"><strong class="bold">Inquiry</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
serverId.nodeId.processID reports error for analysis.
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
    at java.util.HashMap.resize(HashMap.java:704)
    at java.util.HashMap.putVal(HashMap.java:663)
    at java.util.HashMap.put(HashMap.java:612)
    at com.example.server.RequestHandler.handleRequest(RequestHandler.java:45)
    at com.example.server.Server.processRequest(Server.java:78)
    at com.example.server.Server.run(Server.java:61)
    at java.lang.Thread.run(Thread.java:834)
2024-06-18 14:32:21.456 ERROR [Server] - Server encountered an unrecoverable error and will shut down.
java.lang.OutOfMemoryError: Java heap space
    at java.util.Arrays.copyOf(Arrays.java:3332)
    at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
    at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:674)
    at java.lang.StringBuilder.append(StringBuilder.java:214)
    at com.example.server.Logger.log(Logger.java:27)
    at com.example.server.RequestHandler.handleRequest(RequestHandler.java:47)
    at com.example.server.Server.processRequest(Server.java:78)
    at com.example.server.Server.run(Server.java:61)
    at java.lang.Thread.run(Thread.java:834)</pre>			<p><span class="No-Break"><strong class="bold">Response</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
Exception: The OutOfMemoryError exception indicates that the Java Virtual Machine (JVM) has run out of memory.
Error Message: Java heap space specifies that the heap memory allocated for the JVM is exhausted.
Method Calls: The trace shows the sequence of method calls that led to the error.
The error starts in the HashMap.resize method.
The putVal and put methods of HashMap are called.
These methods are used in RequestHandler.handleRequest.
Server.processRequest and Server.run handle the processing and running of the server, respectively.
The Thread.run method is the entry point of the thread that encountered the error.
Timestamp: The timestamp (2024-06-18 14:32:21.456) provides the exact time of the error occurrence.
Logger: The logger in Logger.log attempted to log the error but also encountered the same OutOfMemoryError.
Causes and Solutions:
Causes: Excessive memory usage due to large data structures. Memory leaks where objects are not properly disposed of. Insufficient heap size allocated to the JVM.
Solutions: Increase the heap size using JVM options (-Xmx for maximum heap size). Optimize the code to use memory more efficiently. Use profiling tools to detect and fix memory leaks.</pre>			<p>This analyzer is useful to a diagnostics engineer, as the analyzer quickly reviews a stack trace, summarizes its findings, and generates recommendations for solving <span class="No-Break">the issue.</span></p>
			<p>However, the prompt is 275 tokens (1,240 characters), and the response is 248 tokens (1,205 characters). As this is a sample stack trace, the real-life implementation would possibly be more detailed with real information. So, if the control setting was still 300 tokens for input or output, you would be quite close to this <span class="No-Break">limit already.</span></p>
			<p>Choosing the input and output token limits for your application is critically important. While you want to control costs, you also do not want to fundamentally limit functionality. If the token limit is too low, the LLM may not be able to generate the <span class="No-Break">desired output.</span></p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor248"/>Performance issues in generative AI applications</h1>
			<p>The most obvious failures of GenAI are performance- and reliability-related issues. Since you’ve learned about accuracy in <a href="B22495_10.xhtml#_idTextAnchor214"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Refining the Semantic Data Model to Improve Accuracy</em>, performance in this chapter’s context means slowness. If a user asks your AI application a question and there is either no response, a metered response, or a partial response, it is typically much more apparent than if the response was hallucinated <span class="No-Break">or sycophantic.</span></p>
			<p>Several factors can contribute to the slowness of a GenAI application. Some of the most common causes of performance issues in GenAI are computational load, network latency, model serving strategies, and high <strong class="bold">input/output</strong> (<span class="No-Break"><strong class="bold">I/O</strong></span><span class="No-Break">) operations.</span></p>
			<p>There can be many more causes, of course. The rest of this section will explain some of these performance killers in detail and their impact on your application <span class="No-Break">and users.</span></p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor249"/>Computational load</h2>
			<p>As you already know, LLMs require significant computational power. The time required to generate responses to queries increases with the complexity and the size of the model. Poorly formed requests significantly increase the computational load for a GenAI application. Let’s look at a few examples of this so that you’re able to understand how this failure mode <span class="No-Break">can happen.</span></p>
			<h3>Extensive data processing and calculations</h3>
			<p>Requests that require processing large datasets or performing extensive calculations can be computationally demanding, as happens in the <span class="No-Break">following example.</span></p>
			<p><span class="No-Break"><strong class="bold">User request</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
Evaluate a sample of the last 20,000 stock prices for TSLA, sort it from highest to lowest, and let me know on which days and times it had the highest price.</pre>			<p>Fetching 20,000 random stock prices sounds simple, but the user does not specify a timeframe. For what period should the model evaluate the last 20,000 stock prices? Over the last month? Last year at random? The sorting of those values is computationally expensive and adds further processing to the <span class="No-Break">returned list.</span></p>
			<h3>High-complexity requests</h3>
			<p>Complex requests that involve evaluating a large amount of data, summarizing, and then returning many results are also taxing. Often, this involves chaining multiple LLM calls through advanced prompting techniques, such as the ReACT pattern and <span class="No-Break">function calling.</span></p>
			<p>The <strong class="bold">reasoning and acting</strong> (<strong class="bold">ReACT</strong>) pattern is an advanced prompting technique used in GenAI models to handle complex tasks that require multiple steps of reasoning and interaction. This pattern involves a sequence where the model reasons about the task, generates intermediate actions, and then produces the final output. The ReACT pattern helps the model break down complex requests into manageable steps, improving accuracy and coherence in the <span class="No-Break">final response.</span></p>
			<p><strong class="bold">Function calling</strong> in the context of LLMs involves instructing the model to execute specific functions or actions as part of its response generation process. This can be particularly useful for tasks that require structured outputs, calculations, data retrieval, or interactions with external systems. As an example, the developer specifies functions within the prompt that the model can call to perform specific tasks. These functions are predefined and can handle various operations, such as querying databases, performing calculations, or fetching <span class="No-Break">external data.</span></p>
			<p>Let’s look at a high-complexity request to <span class="No-Break">illustrate this.</span></p>
			<p><span class="No-Break"><strong class="bold">User request</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
Generate a detailed and historically accurate list of the top three priorities for every US president, but do not include their policies related to South America.</pre>			<p>In this scenario, the GenAI must first create the list of every US president, then seek information about each one, and then create a detailed summary of their policies and events during their terms in office. It must also retrieve content related to which things the presidents prioritized, identify consensus on what pieces of content were the top priorities, compile and summarize all that information, and then output it to the user. This is extensive knowledge retrieval, analysis, and text generation. Most likely, this information would require multiple LLM queries, and more queries equate to <span class="No-Break">more spend.</span></p>
			<p>These examples illustrate how certain types of user requests can significantly increase the computational load for GenAI applications. Let’s now see how model serving strategies can impact <span class="No-Break">GenAI performance.</span></p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor250"/>Model serving strategies</h2>
			<p>Generating responses for every request individually can be inefficient, depending on volume. If the application is not designed to handle multiple requests concurrently, it will become slower the more users you have. If the application relies on cloud-based services, network latency can affect performance. Slow internet connections or high latency between the client and the server can cause delays. Frequent or complex API calls to external services can add to the response time, especially if those services are experiencing a high load or are <span class="No-Break">geographically distant.</span></p>
			<p>Let’s return to the stock predictor application for <span class="No-Break">an example.</span></p>
			<p>Because your GenAI application receives some news coverage, your website experiences a surge in traffic, and the number of customers interacting with the application increases dramatically. But, since your application handles each request individually and cannot process multiple requests concurrently, the response time for each user increases as the system becomes overwhelmed. Users experience slower response times, leading <span class="No-Break">to frustration.</span></p>
			<p>The news coverage was from an influencer in Sydney, Australia, so the surge in users is from Asia. Your servers are in the US East region, and network latency due to the geographical distance between the server and the clients causes delays. Customers with slow internet connections experience even longer wait times, further degrading the <span class="No-Break">user experience.</span></p>
			<p>Your application frequently calls external APIs to fetch real-time data for stock prices and financial market news. If these external services are experiencing high load, the API calls take longer <span class="No-Break">to complete.</span></p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor251"/>High I/O operations</h2>
			<p>Poor data-handling practices, such as reading large datasets inefficiently or not using appropriate data structures, can slow down performance. Frequent read/write operations to disk can be a bottleneck, as can poorly optimized database interactions and malformed queries. The example stock price predictor application frequently reads large historical stock price datasets to make predictions. Let’s walk through some potential issues with data handling that result in high <span class="No-Break">I/O operations:</span></p>
			<ul>
				<li>The application reads large datasets inefficiently, such as loading the entire dataset into memory even when only a subset is needed, which consumes excessive memory and processing power, slowing <span class="No-Break">down performance.</span></li>
				<li>The application saves intermediate prediction results and logs to disk after every prediction cycle. Frequent read/write operations to disk form a bottleneck, which significantly increases the time it takes to complete each <span class="No-Break">prediction cycle.</span></li>
				<li>The application queries a database to fetch recent financial news and other relevant data before making predictions. However, a lack of indexes means that query results are slowly delivered. This increases response times, making the application slow to respond to <span class="No-Break">user requests.</span></li>
			</ul>
			<p>Assuming you have a large dataset, you’ll want to avoid these practices as they will affect user experience and <span class="No-Break">increase costs.</span></p>
			<h1 id="_idParaDest-209"><a id="_idTextAnchor252"/>Summary</h1>
			<p>Now that you have navigated through these GenAI challenges, you can appreciate some of the complexities and nuances that accompany these powerful technologies. The issues of hallucinations, sycophancy, data leakage, cost, and performance present formidable obstacles that demand a critical eye and innovative solutions. Each challenge offers a unique perspective on the limitations and potential pitfalls inherent in <span class="No-Break">GenAI applications.</span></p>
			<p>Despite these hurdles, GenAI remains unequivocally valuable. It continues to transform industries, enhance productivity, and open new avenues for creativity and innovation. By understanding and addressing these challenges, developers can harness the full potential of GenAI, delivering robust, reliable, and responsible applications. At the same time, it’s also important to note that applications can be useful even when they are not always correct. To take ChatGPT as an example: it has greatly improved the productivity of millions of users already, even though its deficiencies are well-known (and some not so easily worked around). Your GenAI application could be just as useful and popular but with <span class="No-Break">similar caveats.</span></p>
			<p>In the next chapter, you’ll look at ways to optimize your GenAI application, improving its outputs and performance for a better user experience as well as combatting some of the issues <span class="No-Break">discussed here.</span></p>
		</div>
	</body></html>