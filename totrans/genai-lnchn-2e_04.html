<html><head></head><body>
<div><div><h1 class="chapterNumber"><a id="_idTextAnchor107"/>3</h1>
<h1 class="chapterTitle" id="_idParaDest-64"><a id="_idTextAnchor108"/>Building Workflows with LangGraph</h1>
<p class="normal">So far, we’ve learned about LLMs, LangChain as a framework, and how to use LLMs with LangChain in a vanilla mode (just asking to generate a text output based on a prompt). In this chapter, we’ll start with a quick introduction to LangGraph as a framework and how to develop more complex workflows with LangChain and LangGraph by chaining together multiple steps. As an example, we’ll discuss parsing LLM outputs and look into error handling patterns with LangChain and LangGraph. Then, we’ll continue with more advanced ways to develop prompts and explore what building blocks LangChain offers for few-shot prompting and other techniques. </p>
<p class="normal">We’re also going to cover working with multimodal inputs, utilizing the long context, and adjusting your workloads to overcome limitations related to the context window size. Finally, we’ll look into the basic mechanisms of managing memory with LangChain. Understanding these fundamental and key techniques will help us read LangGraph code, understand tutorials and code samples, and develop our own complex workflows. We’ll, of course, discuss what LangGraph workflows are and will continue building on that skill in <em class="italic">Chapters 5</em> and <em class="italic">6</em>.</p>
<p class="normal">In a nutshell, we’ll cover the following main topics in this chapter:</p>
<ul>
<li class="b lletList">LangGraph fundamentals</li>
<li class="b lletList">Prompt engineering</li>
<li class="b lletList">Working with short context windows</li>
<li class="b lletList">Understanding memory mechanisms</li>
</ul>
<div><div><div><p class="normal">As always, you can find all the code samples on our public GitHub repository as Jupyter notebooks: <a href="https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3">https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3</a>.</p>
</div>
</div>
<h1 class="heading-1" id="_idParaDest-65"><a id="_idTextAnchor109"/>LangGraph fundamentals</h1>
<p class="normal">LangGraph is a framework developed by LangChain (as a company) that helps control and orchestrate <a id="_idIndexMarker177"/>workflows. Why do we need another orchestration framework? Let’s park this question until <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>, where we’ll touch on agents and agentic workflows, but for now, let us mention the flexibility of LangGraph as an orchestration framework and its robustness in handling complex scenarios.</p>
<p class="normal">Unlike many other frameworks, LangGraph allows cycles (most other orchestration frameworks operate only with directly acyclic graphs), supports streaming out of the box, and has many pre-built loops and components dedicated to generative AI applications (for example, human moderation). LangGraph also has a very rich API that allows you to have very granular control of your execution flow if needed. This is not fully covered in our book, but just keep in mind that you can always use a more low-level API if you need to.</p>
<div><div><p class="normal">A <strong class="keyWord">Directed Acyclic Graph (DAG)</strong> is a <a id="_idIndexMarker178"/>special type of graph in graph theory and computer science. Its edges (connections between nodes) have a direction, which means that the connection from node A to node B is different from the connection from node B to node A. It has no cycles. In other words, there is no path that starts at a node and returns to the same node by following the directed edges.</p>
<p class="normal">DAGs are often used as a model of workflows in data engineering, where nodes are tasks and edges are dependencies between these tasks. For example, an edge from node A to node B means that we need output from node A to execute node B.</p>
</div>
</div>
<p class="normal">For now, let’s start with the basics. If you’re new to this framework, we would also highly recommend a free online course on LangGraph that is available at <a href="https://academy.langchain.com/">https://academy.langchain.com/</a> to deepen your understanding.</p>
<div><h2 class="heading-2" id="_idParaDest-66"><a id="_idTextAnchor110"/>State management</h2>
<p class="normal">State management <a id="_idIndexMarker179"/>is crucial in real-world AI applications. For example, in a <a id="_idIndexMarker180"/>customer service chatbot, the state might track information such as customer ID, conversation history, and outstanding issues. LangGraph’s state management lets you maintain this context across a complex workflow of multiple AI components.</p>
<p class="normal">LangGraph allows <a id="_idIndexMarker181"/>you to develop and execute complex workflows called <strong class="keyWord">graphs</strong>. We will use the words <em class="italic">graph</em> and <em class="italic">workflow</em> interchangeably in this chapter. A graph consists of nodes and edges between them. Nodes are components of your workflow, and a workflow has a <em class="italic">state</em>. What is it? Firstly, a state makes your nodes aware of the current context by keeping track of the user input and previous computations. Secondly, a state allows you to persist your workflow execution at any point in time. Thirdly, a state makes your workflow truly interactive since a node <a id="_idIndexMarker182"/>can change the workflow’s <a id="_idIndexMarker183"/>behavior by updating the state. For simplicity, think about a state as a Python dictionary. Nodes are Python functions that operate on this dictionary. They take a dictionary as input and return another dictionary that contains keys and values to be updated in the state of the workflow.</p>
<p class="normal">Let’s understand that with a simple example. First, we need to define a state’s schema:</p>
<pre>from typing_extensions import TypedDict
class JobApplicationState(TypedDict):
 job_description: str
 is_suitable: bool
 application: str</pre>
<p class="normal">A <code class="inlineCode">TypedDict</code> is a Python <a id="_idIndexMarker184"/>type constructor that allows to define dictionaries with a predefined set of keys and each key can have its own type (as opposed to a <code class="inlineCode">Dict[str, str]</code> construction).</p>
<div><div><p class="normal">LangGraph state’s schema shouldn’t necessarily be defined as a <code class="inlineCode">TypedDict</code>; you can use data classes or Pydantic models too.</p>
</div>
</div>
<div><p class="normal">After we have defined a schema for a state, we can define our first simple workflow:</p>
<pre>from langgraph.graph import StateGraph, START, END, Graph
def analyze_job_description(state):
   print("...Analyzing a provided job description ...")
   return {"is_suitable": len(state["job_description"]) &gt; 100}
def generate_application(state):
   print("...generating application...")
   return {"application": "some_fake_application"}
builder = StateGraph(JobApplicationState)
builder.add_node("analyze_job_description", analyze_job_description)
builder.add_node("generate_application", generate_application)
builder.add_edge(START, "analyze_job_description")
builder.add_edge("analyze_job_description", "generate_application")
builder.add_edge("generate_application", END)
graph = builder.compile()</pre>
<p class="normal">Here, we defined two Python functions that are components of our workflow. Then, we defined our <a id="_idIndexMarker185"/>workflow by providing a state’s schema, adding nodes and edges between them. <code class="inlineCode">add_node</code> is a convenient way to add a component <a id="_idIndexMarker186"/>to your graph (by providing its name and a corresponding Python function), and you can reference this name later when you define edges with <code class="inlineCode">add_edge</code>. <code class="inlineCode">START</code> and <code class="inlineCode">END</code> are reserved built-in nodes that define the beginning and end of the workflow accordingly.</p>
<p class="normal">Let’s take a look at our workflow by using a built-in visualization mechanism:</p>
<pre>from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))</pre>
<div><figure class="mediaobject"><img alt="Figure 3.1: LangGraph built-in visualization of our first workflow" src="img/B32363_03_01.png"/></figure>
<p class="packt_figref">Figure 3.1: LangGraph built-in visualization of our first workflow</p>
<p class="normal">Our function accesses the state by simply reading from the dictionary that LangGraph automatically provides as input. LangGraph isolates state updates. When a node receives the state, it gets an immutable copy, not a reference to the actual state object. The node must return a dictionary containing the specific keys and values it wants to update. LangGraph then handles merging these updates into the master state. This pattern prevents side effects and ensures that state changes are explicit and traceable.</p>
<p class="normal">The only way <a id="_idIndexMarker187"/>for a node to modify a state is to provide an output dictionary <a id="_idIndexMarker188"/>with key-value pairs to be updated, and LangGraph will handle it. A node should modify at least one key in the state. A <code class="inlineCode">graph</code> instance itself is a <code class="inlineCode">Runnable</code> (to be precise, it inherits from <code class="inlineCode">Runnable</code>) and we can execute it. We should provide a dictionary with the initial state, and we’ll get the final state as an output:</p>
<pre>res = graph.invoke({"job_description":"fake_jd"})
print(res)
&gt;&gt;...Analyzing a provided job description ...
...generating application...
{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_fake_application'}</pre>
<div><p class="normal">We used a very simple graph as an example. With your real workflows, you can define parallel steps (for example, you can easily connect one node with multiple nodes) and even cycles. LangGraph executes the <a id="_idIndexMarker189"/>workflow in so-called <em class="italic">supersteps</em> that can call multiple nodes at the same time (and then merge state updates from these nodes). You can control the depth of recursion and amount of overall supersteps in the graph, which helps you avoid cycles running forever, especially because the LLMs output is non-deterministic.</p>
<div><div><p class="normal"><strong class="keyWord">A superstep</strong> on LangGraph represents a discrete iteration over one or a few nodes, and it’s inspired by Pregel, a system built by Google for processing large graphs at scale. It handles parallel execution of nodes and updates sent to the central gr<a id="_idTextAnchor111"/>aph’s state.</p>
</div>
</div>
<p class="normal">In our example, we used direct edges from one node to another. It makes our graph no different from a sequential chain that we could have defined with LangChain. One of the key LangGraph features is the ability to create conditional edges that can direct the execution flow to one or another node depending on the current state. A conditional edge is a Python function that gets the current state as an input and returns a string with the node’s name to be executed.</p>
<p class="normal">Let’s look at an example:</p>
<pre>from typing import Literal
builder = StateGraph(JobApplicationState)
builder.add_node("analyze_job_description", analyze_job_description)
builder.add_node("generate_application", generate_application)
def is_suitable_condition(state: StateGraph) -&gt; Literal["generate_application", END]:
 if state.get("is_suitable"):
 return "generate_application"
 return END
builder.add_edge(START, "analyze_job_description")
builder.add_conditional_edges("analyze_job_description", is_suitable_condition)
builder.add_edge("generate_application", END)
graph = builder.compile()</pre>
<div><pre>from IPython.display import Image, display
display(Image(graph.get_graph().draw_mermaid_png()))</pre>
<p class="normal">We’ve defined <a id="_idIndexMarker190"/>an edge <code class="inlineCode">is_suitable_condition</code> that takes a state and returns either an <code class="inlineCode">END</code> or <code class="inlineCode">generate_application</code> string by analyzing the current state. We used a <code class="inlineCode">Literal</code> type hint since it’s <a id="_idIndexMarker191"/>used by LangGraph to determine which destination nodes to connect the source node with when it’s creating conditional edges. If you don’t use a type hint, you can provide a list of destination nodes directly to the <code class="inlineCode">add_conditional_edges</code> function; otherwise, LangGraph will connect the source node with all other nodes in the graph (since it doesn’t analyze the code of an edge function itself when creating a graph). The following figure shows the output generated:</p>
<figure class="mediaobject"> <img alt="Figure 3.2: A workflow with conditional edges (represented as dotted lines)" src="img/B32363_03_02.png"/></figure>
<p class="packt_figref">Figure 3.2: A workflow with conditional edges (represented as dotted lines)</p>
<p class="normal">Conditional <a id="_idIndexMarker192"/>edges are visualized with dotted lines, and <a id="_idIndexMarker193"/>now we can see that, depending on the output of the <code class="inlineCode">analyze_job_description</code> step, our graph can perform diff<a id="_idTextAnchor112"/>erent actions.</p>
<h2 class="heading-2" id="_idParaDest-67"><a id="_idTextAnchor113"/>Reducers</h2>
<p class="normal">So far, our nodes have changed the state by updating the value for a corresponding key. From another <a id="_idIndexMarker194"/>point of view, at each superstep, LangGraph can produce a new value for a given key. In other words, for every key in the state, there’s a sequence of values, and from a functional <a id="_idIndexMarker195"/>programming perspective, a <code class="inlineCode">reduce</code> function can be applied to this sequence. The default reducer on LangGraph always replaces the final value with the new value. Let’s imagine we want to track custom actions (produced by nodes) and compare three options.</p>
<div><p class="normal">With the first option, a node should return a list as a value for the key <code class="inlineCode">actions</code>. We provide short code samples just for illustration purposes, but you can find full ones on Github. If such a value already exists in the state, it will be replaced with the new one:</p>
<pre>class JobApplicationState(TypedDict):
   ...
   actions: list[str]</pre>
<p class="normal">Another option is to use the default <code class="inlineCode">add</code> method with the <code class="inlineCode">Annotated</code> type hint. By using this type hint, we tell the LangGraph compiler that the type of our variable in the state is a list of strings, and it should use the <code class="inlineCode">add</code> method to concatenate two lists (if the value already exists in the state and a node produces a new one):</p>
<pre>from typing import Annotated, Optional
from operator import add
class JobApplicationState(TypedDict):
   ...
   actions: Annotated[list[str], add]</pre>
<p class="normal">The last option <a id="_idIndexMarker196"/>is to write your own custom reducer. In this <a id="_idIndexMarker197"/>example, we write a custom reducer that accepts not only a list from the node (as a new value) but also a single string that would be converted to a list:</p>
<pre>from typing import Annotated, Optional, Union
def my_reducer(left: list[str], right: Optional[Union[str, list[str]]]) -&gt; list[str]:
 if right:
 return left + [right] if isinstance(right, str) else left + right
 return left
class JobApplicationState(TypedDict):
   ...
   actions: Annotated[list[str], my_reducer]</pre>
<p class="normal">LangGraph has a few built-in reducers, and we’ll also demonstrate how you can implement your own. One of the important ones is <code class="inlineCode">add_messages</code>, which allows us to merge messages. Many of your nodes would be LLM agents, and LLMs typically work with messages. Therefore, according to the conversational programming paradigm we’ll talk about in more detail in <em class="italic">Chapters 5</em> and <em class="italic">6</em>, you typically need to keep track of these messages:</p>
<div><pre>from langchain_core.messages import AnyMessage
from langgraph.graph.message import add_messages 
class JobApplicationState(TypedDict): 
  ...
  messages: Annotated[list[AnyMessage], add_messages]</pre>
<p class="normal">Since this is such an important reducer, there’s a built-in state that you can inherit from:</p>
<pre>from langgraph.graph import MessagesState 
class JobApplicationState(MessagesState): 
  ...</pre>
<p class="normal">Now, as we have <a id="_idIndexMarker198"/>discussed reducers, let’s talk about another important concept for <a id="_idIndexMarker199"/>any developer – how to write reusable and modular workflows by passing con<a id="_idTextAnchor114"/>figurations to them.</p>
<h2 class="heading-2" id="_idParaDest-68"><a id="_idTextAnchor115"/>Making graphs configurable</h2>
<p class="normal">LangGraph provides a powerful API that allows you to make your graph configurable. It allows you to <a id="_idIndexMarker200"/>separate parameters from user input – for example, to experiment between different LLM providers or pass custom callbacks. A node can <a id="_idIndexMarker201"/>also access the configuration by accepting it as a second argument. The configuration will be passed as an instance of <code class="inlineCode">RunnableConfig.</code></p>
<p class="normal"><code class="inlineCode">RunnableConfig</code> is a typed dictionary that gives you control over execution control settings. For example, you can control the maximum number of supersteps with the <code class="inlineCode">recursion_limit </code>parameter. <code class="inlineCode">RunnableConfig</code> also allows you to pass custom parameters as a separate dictionary under a <code class="inlineCode">configurable</code> key.</p>
<p class="normal">Let’s allow our node to use different LLMs during application generation:</p>
<pre>from langchain_core.runnables.config import RunnableConfig
def generate_application(state: JobApplicationState, config: RunnableConfig):
   model_provider = config["configurable"].get("model_provider", "Google")
   model_name = config["configurable"].get("model_name", "gemini-1.5-flash-002")
 print(f"...generating application with {model_provider} and {model_name} ...")
 return {"application": "some_fake_application", "actions": ["action2", "action3"]}</pre>
<div><p class="normal">Let’s now compile and execute our graph with a custom configuration (if you don’t provide any, LangGraph will use the default one):</p>
<pre>res = graph.invoke({"job_description":"fake_jd"}, config={"configurable": {"model_provider": "OpenAI", "model_name": "gpt-4o"}})
print(res)
&gt;&gt; ...Analyzing a provided job description ...
...generating application with OpenAI and OpenAI ...
{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_fake_application', 'actions': ['action1', 'action2', 'action3']}</pre>
<p class="normal">Now that <a id="_idIndexMarker202"/>we’ve established how to structure <a id="_idIndexMarker203"/>complex workflows with LangGraph, let’s look at a common challenge these workflows face: ensuring LLM outputs follow the exact structure needed by downstream components. Robust output parsing and graceful error handling are essential for reliable AI pipelines.</p>
<h2 class="heading-2" id="_idParaDest-69"><a id="_idTextAnchor116"/>Controlled output generation</h2>
<p class="normal">When you develop complex workflows, one of the common tasks you need to solve is to force an LLM to <a id="_idIndexMarker204"/>generate an output that follows a certain structure. This is called a controlled generation. This way, it can be consumed <a id="_idIndexMarker205"/>programmatically by the next steps further down the workflow. For example, we can ask the LLM to generate JSON or XML for an API call, extract certain attributes from a text, or generate a CSV table. There are multiple ways to achieve this, and we’ll start exploring them in this chapter and continue in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a>. Since an LLM might not always follow the exact output structure, the next step might fail, and you’ll need to recover from the error. Hence, we’ll also begin discussing error handling in this section.</p>
<h3 class="heading-3" id="_idParaDest-70"><a id="_idTextAnchor117"/>Output parsing</h3>
<p class="normal">Output parsing is essential when integrating LLMs into larger workflows, where subsequent steps require <a id="_idIndexMarker206"/>structured data rather than natural <a id="_idIndexMarker207"/>language responses. One way to do that is to add corresponding instructions to the prompt and parse the output.</p>
<p class="normal">Let’s see a simple task. We’d like to classify whether a certain job description is suitable for a junior Java programmer as a step of our pipeline and, based on the LLM’s decision, we’d like to either continue with an application or ignore this specific job description. We can start with a simple prompt:</p>
<pre>from langchain_google_vertexai import ChatVertexAI
llm = ChatVertexAI(model="gemini-1.5-flash-002")</pre>
<div><pre>job_description: str = ...  # put your JD here
prompt_template = (
 "Given a job description, decide whether it suits a junior Java developer."
 "\nJOB DESCRIPTION:\n{job_description}\n"
)
result = llm.invoke(prompt_template.format(job_description=job_description))
print(result.content)
&gt;&gt; No, this job description is not suitable for a junior Java developer.\n\nThe key reasons are:\n\n* … (output reduced)</pre>
<p class="normal">As you <a id="_idIndexMarker208"/>can see, the output of the LLM is free text, which <a id="_idIndexMarker209"/>might be difficult to parse or interpret in subsequent pipeline steps. What if we add a specific instruction to a prompt?</p>
<pre>prompt_template_enum = (
 "Given a job description, decide whether it suits a junior Java developer."
 "\nJOB DESCRIPTION:\n{job_description}\n\nAnswer only YES or NO."
)
result = llm.invoke(prompt_template_enum.format(job_description=job_description))
print(result.content)
&gt;&gt; NO</pre>
<p class="normal">Now, how can we parse this output? Of course, our next step can be to just look at the text and have a condition based on a string comparison. But that won’t work for more complex use cases – for example, if the next step expects the output to be a JSON object. To deal with that, LangChain offers plenty of OutputParsers that take the output generated by the LLM and try to parse it into a desired format (by checking a schema if needed) – a list, CSV, enum, pandas DatafFrame, Pydantic model, JSON, XML, and so on. Each parser implements a <code class="inlineCode">BaseGenerationOutputParser</code> interface, which extends the <code class="inlineCode">Runnable</code> interface with an additional <code class="inlineCode">parse_result</code> method.</p>
<p class="normal">Let’s build a parser that parses an output into an enum:</p>
<pre>from enum import Enum
from langchain.output_parsers import EnumOutputParser
from langchain_core.messages import HumanMessage</pre>
<div><pre>class IsSuitableJobEnum(Enum):
   YES = "YES"
   NO = "NO"
parser = EnumOutputParser(enum=IsSuitableJobEnum)
assert parser.invoke("NO") == IsSuitableJobEnum.NO
assert parser.invoke("YES\n") == IsSuitableJobEnum.YES
assert parser.invoke(" YES \n") == IsSuitableJobEnum.YES
assert parser.invoke(HumanMessage(content="YES")) == IsSuitableJobEnum.YES</pre>
<p class="normal">The <code class="inlineCode">EnumOutputParser</code> converts text output into a corresponding <code class="inlineCode">Enum</code> instance. Note that the parser <a id="_idIndexMarker210"/>handles any generation-like output (not only strings), and it actually also strips the output.</p>
<div><div><p class="normal">You can find a full list of parsers in the documentation at <a href="https://python.langchain.com/docs/concepts/output_parsers/">https://python.langchain.com/docs/concepts/output_parsers/</a>, and if you need your own parser, you can always build a new one!</p>
</div>
</div>
<p class="normal">As a <a id="_idIndexMarker211"/>final step, let’s combine everything into a chain:</p>
<pre>chain = llm | parser
result = chain.invoke(prompt_template_enum.format(job_description=job_description))
print(result)
&gt;&gt; NO</pre>
<p class="normal">Now let’s make this chain part of our LangGraph workflow:</p>
<pre>class JobApplicationState(TypedDict):
   job_description: str
   is_suitable: IsSuitableJobEnum
   application: str
analyze_chain = llm | parser
def analyze_job_description(state):
   prompt = prompt_template_enum.format(job_description=state["job_description"])</pre>
<div><pre>   result = analyze_chain.invoke(prompt)
 return {"is_suitable": result}
def is_suitable_condition(state: StateGraph):
 return state["is_suitable"] == IsSuitableJobEnum.YES
builder = StateGraph(JobApplicationState)
builder.add_node("analyze_job_description", analyze_job_description)
builder.add_node("generate_application", generate_application)
builder.add_edge(START, "analyze_job_description")
builder.add_conditional_edges(
 "analyze_job_description", is_suitable_condition,
    {True: "generate_application", False: END})
builder.add_edge("generate_application", END)</pre>
<p class="normal">We made <a id="_idIndexMarker212"/>two important changes. First, our newly <a id="_idIndexMarker213"/>built chain is now part of a Python function that represents the <code class="inlineCode">analyze_job_description</code> node, and that’s how we implement the logic within the node. Second, our conditional edge function doesn’t return a string anymore, but we added a mapping of returned values to destination edges to the <code class="inlineCode">add_conditional_edges</code> function, and that’s an example of how you could implement a branching of your workflow.</p>
<p class="normal">Let’s take some time to discuss how to handle pote<a id="_idTextAnchor118"/>ntial errors if our parsing fails!</p>
<h3 class="heading-3" id="_idParaDest-71"><a id="_idTextAnchor119"/>Error handling</h3>
<p class="normal">Effective error <a id="_idIndexMarker214"/>management is essential in any LangChain workflow, including <a id="_idIndexMarker215"/>when handling tool failures (which we’ll explore in <a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic">Chapter 5</em></a> when we get to tools). When developing LangChain applications, remember that failures can occur at any stage:</p>
<ul>
<li class="b lletList">API calls to foundation models may fail</li>
<li class="b lletList">LLMs might generate unexpected outputs</li>
<li class="b lletList">External services could become unavailable</li>
</ul>
<p class="normal">One of the possible approaches would be to use a basic Python mechanism for catching exceptions, logging them for further analysis, and continuing your workflow either by wrapping an exception as a text or by returning a default value. If your LangChain chain calls some custom Python function, think about appropriate exception handling. The same goes for your LangGraph nodes.</p>
<div><p class="normal">Logging is essential, especially as you approach production deployment. Proper logging ensures that exceptions don’t go unnoticed, allowing you to monitor their occurrence. Modern observability tools provide alerting mechanisms that group similar errors and notify you about frequently occurring issues.</p>
<p class="normal">Converting exceptions to text enables your workflow to continue execution while providing downstream LLMs with valuable context about what went wrong and potential recovery paths. Here is a simple example of how you can log the exception but continue executing your workflow by sticking to the default behavior:</p>
<pre>import logging
logger = logging.getLogger(__name__)
llms = {
 "fake": fake_llm,
 "Google": llm
}
def analyze_job_description(state, config: RunnableConfig):
 try:
     llm = config["configurable"].get("model_provider", "Google")
     llm = llms[model_provider]
     analyze_chain = llm | parser
     prompt = prompt_template_enum.format(job_description=job_description)
     result = analyze_chain.invoke(prompt)
 return {"is_suitable": result}
 except Exception as e:
     logger.error(f"Exception {e} occurred while executing analyze_job_description")
 return {"is_suitable": False}</pre>
<p class="normal">To test <a id="_idIndexMarker216"/>our error handling, we need to simulate <a id="_idIndexMarker217"/>LLM failures. LangChain has a few <code class="inlineCode">FakeChatModel</code> classes that help you to test your chain:</p>
<ul>
<li class="b lletList"><code class="inlineCode">GenericFakeChatModel</code> returns messages based on a provided iterator</li>
<li class="b lletList"><code class="inlineCode">FakeChatModel</code> always returns a <code class="inlineCode">"fake_response"</code> string</li>
<li class="b lletList"><code class="inlineCode">FakeListChatModel</code> takes a list of messages and returns them one by one on each invocation</li>
</ul>
<div><p class="normal">Let’s create a fake LLM that fails every second time:</p>
<pre>from langchain_core.language_models import GenericFakeChatModel
from langchain_core.messages import AIMessage
class MessagesIterator:
 def __init__(self):
 self._count = 0
 def __iter__(self):
 return self
 def __next__(self):
 self._count += 1
 if self._count % 2 == 1:
 raise ValueError("Something went wrong")
 return AIMessage(content="False")
fake_llm = GenericFakeChatModel(messages=MessagesIterator())</pre>
<p class="normal">When we <a id="_idIndexMarker218"/>provide this to our graph (the full code sample is available in our GitHub repo), we can <a id="_idIndexMarker219"/>see that the workflow continues despite encountering an exception:</p>
<pre>res = graph.invoke({"job_description":"fake_jd"}, config={"configurable": {"model_provider": "fake"}})
print(res)
&gt;&gt; ERROR:__main__:Exception Expected a Runnable, callable or dict.Instead got an unsupported type: &lt;class 'str'&gt; occured while executing analyze_job_description
{'job_descri<a id="_idTextAnchor120"/>ption': 'fake_jd', 'is_suitable': False}</pre>
<p class="normal">When an error occurs, sometimes it helps to try again. LLMs have a non-deterministic nature, and the next attempt might be successful; also, if you’re using third-party APIs, various failures might happen on the provider’s side. Let’s discuss how to implement proper retries with LangGraph.</p>
<div><h4 class="heading-4">Retries</h4>
<p class="normal">There are <a id="_idIndexMarker220"/>three distinct retry approaches, each suited to different scenarios:</p>
<ul>
<li class="b lletList">Generic retry with Runnable</li>
<li class="b lletList">Node-specific retry policies</li>
<li class="b lletList">Semantic output repair</li>
</ul>
<p class="normal">Let’s look <a id="_idIndexMarker221"/>at these in turn, starting with generic retries that are available for every <code class="inlineCode">Runnable.</code></p>
<p class="normal">You can retry any <code class="inlineCode">Runnable</code><em class="italic"> </em>or LangGraph node using a built-in mechanism:</p>
<pre>fake_llm_retry = fake_llm.with_retry(
   retry_if_exception_type=(ValueError,),
   wait_exponential_jitter=True,
   stop_after_attempt=2,
)
analyze_chain_fake_retries = fake_llm_retry | parser</pre>
<p class="normal">With LangGraph, you can also describe specific retries for every node. For example, let’s retry our <code class="inlineCode">analyze_job_description</code> node two times in case of a <code class="inlineCode">ValueError</code>:</p>
<pre>from langgraph.pregel import RetryPolicy
builder.add_node(
 "analyze_job_description", analyze_job_description,
  retry=RetryPolicy(retry_on=ValueError, max_attempts=2))</pre>
<p class="normal">The components you’re using, often known as building blocks, might have their own retry mechanism that tries to algorithmically fix the problem by giving an LLM additional input on what went wrong. For example, many chat models on LangChain have client-side retries on specific server-side errors.</p>
<p class="normal">ChatAnthropic has a <code class="inlineCode">max_retries</code> parameter that you can define either per instance or per request. Another good <a id="_idIndexMarker222"/>example of a more advanced building block is trying to recover from <a id="_idIndexMarker223"/>a parsing error. Retrying a parsing step won’t help since typically parsing errors are related to the incomplete LLM output. What if we retry the generation step and hope for the best, or actually give LLM a hint about what went wrong? That’s exactly what a <code class="inlineCode">RetryWithErrorOutputParser</code> is doing.</p>
<div><figure class="mediaobject"><img alt="Figure 3.3: Adding a retry mechanism to a chain that has multiple steps" src="img/B32363_03_03.png"/></figure>
<p class="packt_figref">Figure 3.3: Adding a retry mechanism to a chain that has multiple steps</p>
<p class="normal">In order to use <code class="inlineCode">RetryWithErrorOutputParser</code>, we need to first initialize it with an LLM (used to fix the output) and our parser. Then, if our parsing fails, we run it and provide our initial prompt (with all substituted parameters), generated response, and parsing error:</p>
<pre>from langchain.output_parsers import RetryWithErrorOutputParser
fix_parser = RetryWithErrorOutputParser.from_llm(
  llm=llm, # provide llm here
  parser=parser, # your original parser that failed
  prompt=retry_prompt, # an optional parameter, you can redefine the default prompt 
)
fixed_output = fix_parser.parse_with_prompt(
  completion=original_response, prompt_value=original_prompt)</pre>
<p class="normal">We can read <a id="_idIndexMarker224"/>the source code on GitHub to better understand what’s going on, but in essence, that’s an example of a pseudo-code without too many details. We illustrate <a id="_idIndexMarker225"/>how we can pass the parsing error and the original output that led to this error back to an LLM and ask it to fix the problem:</p>
<pre>prompt = """
Prompt: {prompt} Completion: {completion} Above, the Completion did not satisfy the constraints given in the Prompt. Details: {error} Please try again:
""" 
retry_chain = prompt | llm | StrOutputParser()
# try to parse a completion with a provided parser
parser.parse(completion)
# if it fails, catch an error and try to recover max_retries attempts
completion = retry_chain.invoke(original_prompt, completion, error)</pre>
<div><p class="normal">We introduced the <code class="inlineCode">StrOutputParser</code> in <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a> to convert the output of the ChatModel from an AIMessage to a string so that we can easily pass it to the next step in the chain.</p>
<p class="normal">Another thing to keep in mind is that LangChain building blocks allow you to redefine parameters, including default prompts. You can always check them on Github; sometimes it’s a good idea <a id="_idIndexMarker226"/>to customize default prompts for your workflows.</p>
<div><div><p class="normal">You can read about other available output-fixing parsers here: <a href="https://python.l﻿angchain.com/docs/how_to/output_parser_retry/">https://python.langchain.com/docs/how_to/output_parser_retry/</a>.</p>
</div>
</div>
<h4 class="heading-4">Fallbacks</h4>
<p class="normal">In software development, a <strong class="keyWord">fallback</strong> is an alternative program that allows you to recover if your base <a id="_idIndexMarker227"/>one fails. LangChain allows you to define fallbacks on a <code class="inlineCode">Runnable</code> level. If execution <a id="_idIndexMarker228"/>fails, an alternative chain is triggered with the same input parameters. For example, if the LLM you’re using is not available for a short period of time, your chain will automatically switch to a different one that uses an alternative provider (and probably different prompts).</p>
<p class="normal">Our fake model fails every second time, so let’s add a fallback to it. It’s just a lambda that prints a statement. As we can see, every second time, the fallback is executed:</p>
<pre>from langchain_core.runnables import RunnableLambda
chain_fallback = RunnableLambda(lambda _: print("running fallback"))
chain = fake_llm | RunnableLambda(lambda _: print("running main chain"))
chain_with_fb = chain.with_fallbacks([chain_fallback])
chain_with_fb.invoke("test")
chain_with_fb.invoke("test")
&gt;&gt; running fallback
running main chain</pre>
<p class="normal">Generating complex <a id="_idIndexMarker229"/>outcomes that can follow a certain template and can be parsed reliably <a id="_idIndexMarker230"/>is called structured generation (or controlled generation). This can <a id="_idIndexMarker231"/>help to build more complex workflows, where an output of one LLM-driven step can be consumed by another programmatic step. We’ll pick this up again in more detail in <em class="italic">Chapters 5</em> and <em class="italic">6</em>.</p>
<div><p class="normal">Prompts that you send to an LLM are one of the most important building blocks of your workflows. Hence, let’s discuss some basics of prompt engineering next a<a id="_idTextAnchor122"/>nd see how to organize your prompts with LangChain.</p>
<h1 class="heading-1" id="_idParaDest-72"><a id="_idTextAnchor123"/>Prompt engineering</h1>
<p class="normal">Let’s continue by looking into prompt engineering and exploring various LangChain syntaxes related to it. But first, let’s discuss how prompt engineering is different from prompt design. These terms <a id="_idIndexMarker232"/>are sometimes used interchangeably, and it creates a certain level of confusion. As we discussed in <a href="E_Chapter_1.xhtml#_idTextAnchor001"><em class="italic">Chapter 1</em></a>, one of the big discoveries about LLMs was that they have the capability of domain adaptation by <em class="italic">in-context learning</em>. It’s often enough to describe the task we’d like it to perform in a natural language, and even though the LLM wasn’t trained on this specific task, it performs extremely well. But as we can imagine, there are multiple ways of describing the same task, and LLMs are sensitive to this. Improving our prompt (or prompt template, to be specific) to increase performance on a specific task is called prompt engineering. However, developing more universal prompts that guide LLMs to generate generally better responses on a broad set of tasks is called prompt design.</p>
<p class="normal">There exists a large variety of different prompt engineering techniques. We won’t discuss many of them in detail in this section, but we’ll touch on just a few of them to illustrate key LangChain capabilities that would allow you to construct any prompts you want.</p>
<div><div><p class="normal">You can find a good overview of prompt taxonomy in the paper <em class="italic">The Prompt Report: A Systematic Survey of Prompt Engineering Techniques</em>, published by Sander Schulho<a id="_idTextAnchor124"/>ff and colleagues: <a href="https://arxiv.org/abs/2406.06608">https://arxiv.org/abs/2406.06608</a>.</p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-73"><a id="_idTextAnchor125"/>Prompt templates</h2>
<p class="normal">What we did in <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a> is <a id="_idIndexMarker233"/>called <em class="italic">zero-shot prompting</em>. We created a <a id="_idIndexMarker234"/>prompt template that contained a description of each task. When we run the workflow, we substitute certain values of this prompt <a id="_idIndexMarker235"/>template with runtime arguments. LangChain has some very useful abstractions to help with that.</p>
<div><p class="normal">In <a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic">Chapter 2</em></a>, we introduced <code class="inlineCode">PromptTemplate</code>, which is a <code class="inlineCode">RunnableSerializable</code>. Remember that it substitutes a string template during invocation – for example, you can create a template based on f-string and add your chain, and LangChain would pass parameters from the input, substitute them in the template, and pass the string to the next step in the chain:</p>
<pre>from langchain_core.output_parsers import StrOutputParser
lc_prompt_template = PromptTemplate.from_template(prompt_template)
chain = lc_prompt_template | llm | StrOutputParser()
chain.invoke({"job_description": job_description})</pre>
<p class="normal">For chat models, an input can not only be a string but also a list of <code class="inlineCode">messages</code> – for example, a system message followed by a history of the conversation. Therefore, we can also create a template that prepares a list of messages, and a template itself can be created based on a list of messages or message templates, as in this example:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate
from langchain_core.messages import SystemMessage, HumanMessage
msg_template = HumanMessagePromptTemplate.from_template(
  prompt_template)
msg_example = msg_template.format(job_description="fake_jd")
chat_prompt_template = ChatPromptTemplate.from_messages([
  SystemMessage(content="You are a helpful assistant."),
  msg_template])
chain = chat_prompt_template | llm | StrOutputParser()
chain.invoke({"job_description": job_description})</pre>
<p class="normal">You can also do the same more conveniently without using chat prompt templates but by submitting a tuple (just because it’s faster and more convenient sometimes) with a type of message and a templated string instead:</p>
<pre>chat_prompt_template = ChatPromptTemplate.from_messages(
   [("system", "You are a helpful assistant."),
    ("human", prompt_template)])</pre>
<div><p class="normal">Another <a id="_idIndexMarker236"/>important concept is a <em class="italic">placeholder</em>. This substitutes a variable with <a id="_idIndexMarker237"/>a list of messages provided in real time. You can add a placeholder to your prompt by using a <code class="inlineCode">placeholder</code> hint, or adding a <code class="inlineCode">MessagesPlaceholder</code>:</p>
<pre>from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
chat_prompt_template = ChatPromptTemplate.from_messages(
   [("system", "You are a helpful assistant."),
    ("placeholder", "{history}"),
 # same as MessagesPlaceholder("history"),
    ("human", prompt_template)])
len(chat_prompt_template.invoke({"job_description": "fake", "history": [("human", "hi!"), ("ai", "hi!")]}).messages)
&gt;&gt; 4</pre>
<p class="normal">Now our input consists of four messages – a system message, two history messages that we provided, and one human message from a templated prompt. The best example of using a placeholder is to input a history of a chat, but we’ll see more advanced ones later in this book when we’ll talk about how an LLM interacts with an external world or how d<a id="_idTextAnchor126"/>ifferent LLMs coordinate together in a multi-agent setup.</p>
<h2 class="heading-2" id="_idParaDest-74"><a id="_idTextAnchor127"/>Zero-shot vs. few-shot prompting</h2>
<p class="normal">As we have discussed, the first thing that we want to experiment with is improving the task description <a id="_idIndexMarker238"/>itself. A description of a task without <a id="_idIndexMarker239"/>examples of solutions is called <strong class="keyWord">zero-shot</strong> prompting, and <a id="_idIndexMarker240"/>there are multiple tricks that you can try.</p>
<p class="normal">What <a id="_idIndexMarker241"/>typically works well is assigning the LLM <a id="_idIndexMarker242"/>a certain role (for example, “<em class="italic">You are a useful enterprise assistant working for XXX Fortune-500 company</em>”) and giving some additional instruction (for example, whether the LLM should be creative, concise, or factual). Remember that LLMs have seen various data and they can do different tasks, from writing a fantasy book to answering complex reasoning questions. But your goal is to instruct them, and if you want them to stick to the facts, you’d better give very specific instructions as part of their role profile. For chat models, such role setting typically happens through a system message (but remember that, even for a chat model, everything is combined to a single input prompt formatted on the server side).</p>
<div><p class="normal">The Gemini prompting guide recommends that each prompt should have four parts: a persona, a task, a relevant context, and a desired format. Keep in mind that different model providers might have different recommendations on prompt writing or formatting, hence if you have <a id="_idIndexMarker243"/>complex prompts, always check the <a id="_idIndexMarker244"/>documentation of the model provider, evaluate the performance of your workflows before switching to a new model provider, and adjust prompts accordingly if needed. If you want to use multiple model providers in production, you might end up with multiple prompt templates and select them dynamically based on the model provider.</p>
<p class="normal">Another big improvement can be to provide an LLM with a few examples of this specific task as input-output pairs as part of the prompt. This is called few-shot prompting. Typically, few-shot prompting is difficult to use in scenarios that require a long input (such as RAG, which we’ll talk about in the next chapter) but it’s still very useful for tasks with relatively short prompts, such as classification, extraction, etc.</p>
<p class="normal">Of course, you can always hard-code examples in the prompt template itself, but this makes it difficult to manage them as your system grows. A better way might be to store examples in a separa<a id="_idTextAnchor128"/>te file on disk or in a database and load them into your prompt.</p>
<h3 class="heading-3" id="_idParaDest-75"><a id="_idTextAnchor129"/>Chaining prompts together</h3>
<p class="normal">As your prompts become more advanced, they tend to grow in size and complexity. One common <a id="_idIndexMarker245"/>scenario is to partially format your prompts, and you can do this either by string or function substitution. The latter is relevant if some parts of your prompt depend on dynamically changing variables (for example, current date, user name, etc.). Below, you can find an example of a partial substitution in a prompt template:</p>
<pre>system_template = PromptTemplate.from_template("a: {a} b: {b}")
system_template_part = system_template.partial(
   a="a" # you also can provide a function here
)
print(system_template_part.invoke({"b": "b"}).text)
&gt;&gt; a: a b: b</pre>
<div><p class="normal">Another way to make your prompts more manageable is to split them into pieces and chain them together:</p>
<pre>system_template_part1 = PromptTemplate.from_template("a: {a}")
system_template_part2 = PromptTemplate.from_template("b: {b}")
system_template = system_template_part1 + system_template_part2
print(system_template_part.invoke({"a": "a", "b": "b"}).text)
&gt;&gt; a: a b: b</pre>
<p class="normal">You can <a id="_idIndexMarker246"/>also build more complex substitutions by using the class <code class="inlineCode">langchain_core.prompts.PipelinePromptTemplate</code>. Additionally, you can pass templates into a <code class="inlineCode">ChatPromptTemplate</code> and they will automatically be composed together:</p>
<pre>system_prompt_template = PromptTemplate.from_template("a: {a} b: {b}")
chat_prompt_template = ChatPromptTemplate.from_messages(
   [("system", system_prompt_template.template),
    ("human", "hi"),
    ("ai", "{c}")])
messages = chat_prompt_template.invoke({"a": "a", "b": "b", "c": "c"}).messa<a id="_idTextAnchor130"/>ges
print(len(messages))
print(messages[0].content)
&gt;&gt; 3
a: a b: b</pre>
<h3 class="heading-3" id="_idParaDest-76"><a id="_idTextAnchor131"/>Dynamic few-shot prompting</h3>
<p class="normal">As the number of examples used in your few-shot prompts continues to grow, you might limit the number <a id="_idIndexMarker247"/>of examples to be passed into a specific prompt’s template substitution. We select examples for every input – by searching for examples similar to the user’s input (we’ll talk more about semantic similarity and embeddings in <a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a>), limiting them by length, taking the freshest ones, etc.</p>
<div><figure class="mediaobject"><img alt="Figure 3.4: An example of a workflow with a dynamic retrieval of examples to be passed to a few-shot prompt" src="img/B32363_03_04.png"/></figure>
<p class="packt_figref">Figure 3.4: An example of a workflow with a dynamic retrieval of examples to be passed to a few-shot prompt</p>
<p class="normal">There are <a id="_idIndexMarker248"/>a few already built-in selectors under <code class="inlineCode">langchain_core.example_selectors</code>. You can directly pass an instance of an example s<a id="_idTextAnchor132"/>elector to the <code class="inlineCode">FewShotPromptTemplate</code> instance during instantiation.</p>
<h2 class="heading-2" id="_idParaDest-77"><a id="_idTextAnchor133"/>Chain of Thought</h2>
<p class="normal">The Google Research team introduced the <strong class="keyWord">Chain-of-Thought</strong> (<strong class="keyWord">CoT</strong>) technique early in 2022. They demonstrated <a id="_idIndexMarker249"/>that a relatively simple modification to a prompt <a id="_idIndexMarker250"/>that encouraged a model to generate intermediate step-by-step reasoning steps significantly increased the LLM’s performance on complex symbolic reasoning, common sense, and math tasks. Such an increase in performance has been replicated multiple times since then.</p>
<div><div><p class="normal">You can read the original paper introducing CoT, <em class="italic">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>, published by Jason Wei and colleagues: <a href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>.</p>
</div>
</div>
<p class="normal">There are different modifications of CoT prompting, and because it has long outputs, typically, CoT prompts are zero-shot. You add instructions that encourage an LLM to think about the problem first instead of immediately generating tokens representing the answer. A very simple example of CoT is just to add to your prompt template something like “Let’s think step by step.”</p>
<div><p class="normal">There are various CoT prompts reported in different papers. You can also explore the CoT template available on LangSmith. For our learning purposes, let’s use a CoT prompt with few-shot examples:</p>
<pre>from langchain import hub
math_cot_prompt = hub.pull("arietem/math_cot")
cot_chain = math_cot_prompt | llm | StrOutputParser()
print(cot_chain.invoke("Solve equation 2*x+5=15"))
&gt;&gt; Answer: Let's think step by step
Subtract 5 from both sides:
2x + 5 - 5 = 15 - 5
2x = 10
Divide both sides by 2:
2x / 2 = 10 / 2
x = 5</pre>
<p class="normal">We used a <a id="_idIndexMarker251"/>prompt from LangSmith Hub – a collection <a id="_idIndexMarker252"/>of private and public artifacts that you can use with LangChain. You can explore the prompt itself here: <a href="https://smith.langchain.com/hub. ">https://smith.langchain.com/hub.</a></p>
<p class="normal">In practice, you might want to wrap a CoT invocation with an extraction step to provide a concise answer to the user. For example, let us first run a <code class="inlineCode">cot_chain</code> and then pass its output (please note that we pass a dictionary with an initial <code class="inlineCode">question</code> and a <code class="inlineCode">cot_output</code> to the next step) to an LLM that will use a prompt to create a final answer based on CoT reasoning:</p>
<pre>from operator import itemgetter
parse_prompt_template = (
 "Given the initial question and a full answer, "
 "extract the concise answer. Do not assume anything and "
 "only use a provided full answer.\n\nQUESTION:\n{question}\n"
 "FULL ANSWER:\n{full_answer}\n\nCONCISE ANSWER:\n"
)
parse_prompt = PromptTemplate.from_template(
   parse_prompt_template
)
final_chain = (
 {"full_answer": itemgetter("question") | cot_chain,
 "question": itemgetter("question"),
 }
 | parse_prompt</pre>
<div><pre> | llm
 | StrOutputParser()
)
print(final_chain.invoke({"question": "Solve equation 2*x+5=15"}))
&gt;&gt; 5</pre>
<p class="normal">Although a CoT prompt seems to be relatively simple, it’s extremely powerful since, as we’ve mentioned, it has been demonstrated multiple times that it significantly increases performance in many cases. We will see its evolution and expansion when we discuss agents in <em class="italic">Chapters 5</em> and <em class="italic">6</em>.</p>
<p class="normal">These days, we can observe how the CoT pattern gets more and more application with so-called reasoning models <a id="_idIndexMarker253"/>such as o3-mini or gemini-flash-thinking. To a certain extent, these models do exactly the same (but often in a more advanced manner) – they think <a id="_idIndexMarker254"/>before they answer, and this is achieved not only by changing the <a id="_idIndexMarker255"/>prompt but also by preparing training data (sometimes synthetic) that follows a CoT format.</p>
<p class="normal">Please note that alternatively to using reasoning models, we can use CoT modification with additional instructions by asking an LLM to first generate output tokens that represent a reasoning process:</p>
<pre>template = ChatPromptTemplate.from_messages([
    ("system", """You are a problem-solving assistant that shows its reasoning process. First, walk through your thought process step by step, labeling this section as 'THINKING:'. After completing your analysis, provide your final answer labeled as 'ANSWER:'."""),
    ("user", "{problem}")
])</pre>
<h2 class="heading-2" id="_idParaDest-78"><a id="_idTextAnchor134"/>Self-consistency</h2>
<p class="normal">The idea <a id="_idIndexMarker256"/>behind self-consistency is simple: let’s increase an <a id="_idIndexMarker257"/>LLM’s temperature, sample the answer multiple times, and then take the most frequent answer from the distribution. This has been demonstrated to improve the performance of LLM-based workflows on certain tasks, and it works especially well on tasks such as classification or entity extraction, where the output’s dimensionality is low.</p>
<p class="normal">Let’s use a chain from a previous example and try a quadratic equation. Even with CoT prompting, the first attempt might give us a wrong answer, but if we sample from a distribution, we will be more likely to get the right one:</p>
<div><pre>generations = []
for _ in range(20):
 generations.append(final_chain.invoke({"question": "Solve equation 2*x**2-96*x+1152"}, temperature=2.0).strip())
from collections import Counter
print(Counter(generations).most_common(1)[0][0])
&gt;&gt; x = 24</pre>
<p class="normal">As you can see, we first created a list containing multiple outputs generated by an LLM for the same input <a id="_idIndexMarker258"/>and then created a <code class="inlineCode">Counter</code> class that allowed us to easily find the <a id="_idIndexMarker259"/>most common element in this list, and we took it as a final answer.</p>
<div><div><p class="normal"><strong class="keyWord">Switching between model providers</strong></p>
<p class="normal">Different providers might have slightly different guidance on how to construct the best working prompts. Always check the documentation on the provider’s side – for example, Anthropic emphasizes the importance of XML tags to structure your prompts. Reasoning models have different prompting guidelines (for example, typically, you should not use either CoT or few-shot prompting with such models).</p>
<p class="normal">Last but not least, if you’re changing the model provider, we highly recommend running an evaluation and estimating the quality of your end-to-end application.</p>
</div>
</div>
<p class="normal">Now that we have learned how to efficiently organize your prompt and use different prompt engineering approaches with LangChain, let’s talk about what can we do <a id="_idTextAnchor135"/>if prompts become too long and they don’t fit into the model’s context window.</p>
<h1 class="heading-1" id="_idParaDest-79"><a id="_idTextAnchor136"/>Working with short context windows</h1>
<p class="normal">A context window of 1 or 2 million tokens seems to be enough for almost any task we could imagine. With <a id="_idIndexMarker260"/>multimodal models, you can just ask the model questions <a id="_idIndexMarker261"/>about one, two, or many PDFs, images, or even videos. To process multiple documents (for summarization or question answering), you can use what’s known as the <strong class="keyWord">stuff</strong> approach. This approach is straightforward: use prompt templates to combine all inputs into a single prompt. Then, send this consolidated prompt to an LLM. This works well when the combined content fits within your model’s context window. In the coming chapter, we’ll discuss further ways of using external data to improve models’ responses.</p>
<div><div><div><p class="normal">Keep in mind that, typically, PDFs are treated as images by a multimodal LLM.</p>
</div>
</div>
<p class="normal">Compared to the context window length of 4096 input tokens that we were working with only 2 years ago, the current context window of 1 or 2 million tokens is tremendous progress. But it is still <a id="_idIndexMarker262"/>relevant to discuss techniques of overcoming limitations of context window size for a few reasons:</p>
<ul>
<li class="b lletList">Not all models have long context windows, especially open-sourced ones or the ones served on edge.</li>
<li class="b lletList">Our knowledge bases and the complexity of tasks we’re handling with LLMs are also expanding since we might be facing limitations even with current context windows.</li>
<li class="b lletList">Shorter inputs also help reduce costs and latency.</li>
<li class="b lletList">Inputs like audio or video are used more and more, and there are additional limitations o<a id="_idTextAnchor137"/>n the input length (total size of PDF files, length of the video or audio, etc.).</li>
</ul>
<p class="normal">Hence, let’s take a close look at what we can do to work with a context that is larger than a context window that an LLM can handle – summarization is a good example of such a task. Handling a long context is similar to a classical Map-Reduce (a technique that was actively developed in the 2000s to handle computations on large datasets in a distributed and parallel manner). In general, we have two phases:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Map</strong>: We split <a id="_idIndexMarker263"/>the incoming context into smaller pieces and apply the same task to every one of them in a parallel manner. We can repeat this phase a few times if needed.</li>
<li class="b lletList"><strong class="keyWord">Reduce</strong>: We <a id="_idIndexMarker264"/>combine outputs of previous tasks together.</li>
</ul>
<figure class="mediaobject"><img alt="Figure 3.5: A Map-Reduce summarization pipeline" src="img/B32363_03_05.png"/></figure>
<p class="packt_figref">Figure 3.5: A Map-Reduce summarization pipeline</p>
<div><h2 class="heading-2" id="_idParaDest-80"><a id="_idTextAnchor138"/>Summarizing long video</h2>
<p class="normal">Let’s build a LangGraph workflow that implements the Map-Reduce approach presented above. First, let’s <a id="_idIndexMarker265"/>define the state of the graph that keeps track of the video in question, the intermediate summaries we produce during the phase step, and the final summary:</p>
<pre>from langgraph.constants import Send
import operator
class AgentState(TypedDict):
   video_uri: str
   chunks: int
   interval_secs: int
   summaries: Annotated[list, operator.add]
   final_summary: str
class _ChunkState(TypedDict):
   video_uri: str
   start_offset: int
   interval_secs: int</pre>
<p class="normal">Our state schema now tracks all input arguments (so that they can be accessed by various nodes) and intermediate results so that we can pass them across nodes. However, the Map-Reduce pattern presents another challenge: we need to schedule many similar tasks that process different parts of the original video in parallel. LangGraph provides a special <code class="inlineCode">Send</code> node that enables dynamic scheduling of execution on a node with a specific state. For this approach, we need an additional state schema called <code class="inlineCode">_ChunkState</code> to represent a map step. It’s worth mentioning that ordering is guaranteed – results are collected (in other words, applied to the main state) in exactly the same order as nodes are scheduled.</p>
<p class="normal">Let’s define two nodes:</p>
<ul>
<li class="b lletList"><code class="inlineCode">summarize_video_chunk</code> for the Map phase</li>
<li class="b lletList"><code class="inlineCode">_generate_final_summary</code> for the Reduce phase</li>
</ul>
<div><p class="normal">The first node operates on a state different from the main state, but its output is added to the main state. We run this node multiple times and outputs are combined into a list within the main graph. To schedule <a id="_idIndexMarker266"/>these map tasks, we will create a conditional edge connecting the <code class="inlineCode">START</code> and <code class="inlineCode">_summarize_video_chunk</code> nodes with an edge based on a <code class="inlineCode">_map_summaries</code> function:</p>
<pre>human_part = {"type": "text", "text": "Provide a summary of the video."}
async def _summarize_video_chunk(state:  _ChunkState):
   start_offset = state["start_offset"]
   interval_secs = state["interval_secs"]
   video_part = {
 "type": "media", "file_uri": state["video_uri"], "mime_type": "video/mp4",
 "video_metadata": {
 "start_offset": {"seconds": start_offset*interval_secs},
 "end_offset": {"seconds": (start_offset+1)*interval_secs}}
   }
   response = await llm.ainvoke(
       [HumanMessage(content=[human_part, video_part])])
 return {"summaries": [response.content]}
async def _generate_final_summary(state: AgentState):
   summary = _merge_summaries(
       summaries=state["summaries"], interval_secs=state["interval_secs"])
   final_summary = await (reduce_prompt | llm | StrOutputParser()).ainvoke({"summaries": summary})
 return {"final_summary": final_summary}
def _map_summaries(state: AgentState):
   chunks = state["chunks"]
   payloads = [
       {
 "video_uri": state["video_uri"],
 "interval_secs": state["interval_secs"],
 "start_offset": i
       } for i in range(state["chunks"])
   ] 
 return [Send("summarize_video_chunk", payload) for payload in payloads]</pre>
<div><p class="normal">Now, let’s put everything toget<a id="_idTextAnchor139"/>her and run our graph. We can pass all arguments to the pipeline in a simple manner:</p>
<pre>graph = StateGraph(AgentState)
graph.add_node("summarize_video_chunk", _summarize_video_chunk)
graph.add_node("generate_final_summary", _generate_final_summary)
graph.add_conditional_edges(START, _map_summaries, ["summarize_video_chunk"])
graph.add_edge("summarize_video_chunk", "generate_final_summary")
graph.add_edge("generate_final_summary", END)
app = graph.compile()
result = await app.ainvoke(
   {"video_uri": video_uri, "chunks": 5, "interval_secs": 600},
   {"max_concurrency": 3}
)["final_summary"]</pre>
<p class="normal">Now, as we’re <a id="_idIndexMarker267"/>prepared to build our first workflows with LangGraph, there’s one last important topic to discuss. What if your history of conversations becomes too long and won’t fit into the context window or it would start distracting an LLM from the last input? Let’s discuss the various memory mechanisms LangChain offers.</p>
<h1 class="heading-1" id="_idParaDest-81"><a id="_idTextAnchor140"/>Understanding memory mechanisms</h1>
<p class="normal">LangChain chains and any code you wrap them with are stateless. When you deploy LangChain applications <a id="_idIndexMarker268"/>to production, they should also be kept stateless to allow horizontal scaling (more about this in <a href="E_Chapter_9.xhtml#_idTextAnchor448"><em class="italic">Chapter 9</em></a>). In this section, we’ll discuss how to organize memory to <a id="_idTextAnchor141"/>keep track of interactions between your generative AI application and a specific user.</p>
<h2 class="heading-2" id="_idParaDest-82"><a id="_idTextAnchor142"/>Trimming chat history</h2>
<p class="normal">Every chat application should preserve a dialogue history. In prototype applications, you can store it in <a id="_idIndexMarker269"/>a variable, though this won’t work for <a id="_idIndexMarker270"/>production applications, which we’ll address in the next section.</p>
<p class="normal">The chat history is essentially a list of messages, but there are situations where trimming this history becomes necessary. While this was a very important design pattern when LLMs had a limited context window, these days, it’s not that relevant since most of the models (even small open-sourced models) now support 8192 tokens or even more. Nevertheless, understanding trimming techniques remains valuable for specific use cases.</p>
<div><p class="normal">There are <a id="_idIndexMarker271"/>five ways to trim the chat history:</p>
<ul>
<li class="b lletList"><strong class="keyWord">Discard messages based on length</strong> (like tokens or messages count): You keep only the most recent messages so their total length is shorter than a threshold. The special LangChain function <code class="inlineCode">from langchain_core.messages import trim_messages</code> allows you to trim a sequence of messages. You can provide a function or an LLM instance as a <code class="inlineCode">token_counter</code> argument to this function (and a corresponding LLM integration should support a <code class="inlineCode">get_token_ids</code> method; otherwise, a default tokenizer might be used and results might differ from token counts for this specific LLM provider). This function also allows you to customize how to trim the messages – for example, whether to keep a system message and whether a human message should always come first since many model providers require that a chat always starts with a human message (or with a system message). In that case, you should trim the original sequence of <code class="inlineCode">human, ai, human, ai</code> to a <code class="inlineCode">human, ai</code> one and not <code class="inlineCode">ai, human, ai</code> even if all three messages do fit within the context window threshold.</li>
<li class="b lletList"><strong class="keyWord">Summarize the previous conversation</strong>: On each turn, you can summarize the previous conversation to a single message that you prepend to the next user’s input. LangChain offered some building blocks for a running memory implementation but, as of March 2025, the recommended way is to build your own summarization node with LangGraph.You can find a detailed guide in the LangChain documentation section: <a href="https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/">https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/</a>).</li>
</ul>
<p class="normal-one">When implementing summarization or trimming, think about whether you should keep both histories in your database for further debugging, analytics, etc. You might want to keep the short-memory history of the latest summary and the message after that summary for the application itself, and you probably want to keep track of the whole history (all raw messages and all the summaries) for further analysis. If yes, design your application carefully. For example, you probably don’t need to load all the raw history and summary messages; it’s enough to dump new messages into the database keeping track of the raw history.</p>
<div><ul>
<li class="b lletList"><strong class="keyWord">Combine both trimming and summarization</strong>: Instead of simply discarding old messages <a id="_idIndexMarker272"/>that make the context window too long, you could <a id="_idIndexMarker273"/>summarize these messages and prepend the remaining history.</li>
<li class="b lletList"><strong class="keyWord">Summarize long messages into a short one</strong>: You could also summarize long messages. This might be especially relevant for RAG use cases, which we’re going to discuss in the next chapter, when your input to the model might include a lot of additional context added on top of the actual user’s input.</li>
<li class="b lletList"><strong class="keyWord">Implement your own trimming logic</strong>: The recommended way is to implement your own tokenizer that can be passed to a <code class="inlineCode">trim_messages</code> function since you can reuse a lot of logic that this function already cares for.</li>
</ul>
<p class="normal">Of cou<a id="_idTextAnchor143"/>rse, the question remains on how you can persist the chat history. Let’s examine that next.</p>
<h2 class="heading-2" id="_idParaDest-83"><a id="_idTextAnchor144"/>Saving history to a database</h2>
<p class="normal">As mentioned above, an application deployed to production can’t store chat history in a local memory. If you have your code running on more than one machine, there’s no guarantee that a request <a id="_idIndexMarker274"/>from the same user will hit the same server at the next turn. Of course, you can store history on the frontend and send it back and forth each time, but that also makes sessions not sharable, increases the request size, etc.</p>
<p class="normal">Various database providers might offer an implementation that inherits from the <code class="inlineCode">langchain_core.chat_history.BaseChatMessageHistory</code>, which allows you to store and retrieve a chat history by <code class="inlineCode">session_id</code>. If you’re saving a history to a local variable while prototyping, we recommend using <code class="inlineCode">InMemoryChatMessageHistory</code> instead of a list to be able to later switch to integration with a database.</p>
<p class="normal">Let’s look at an example. We create a fake chat model with a callback that prints out the amount of input messages each time it’s called. Then we initialize the dictionary that keeps histories, and we create a separate function that returns a history given the <code class="inlineCode">session_id</code>:</p>
<pre>from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.language_models import FakeListChatModel
from langchain.callbacks.base import BaseCallbackHandler
class PrintOutputCallback(BaseCallbackHandler):
 def on_chat_model_start(self, serialized, messages, **kwargs):
 print(f"Amount of input messages: {len(messages)}")
sessions = {}
handler = PrintOutputCallback()
llm = FakeListChatModel(responses=["ai1", "ai2", "ai3"])
def get_session_history(session_id: str):
 if session_id not in sessions:
       sessions[session_id] = InMemoryChatMessageHistory()
 return sessions[session_id]</pre>
<div><p class="normal">Now we <a id="_idIndexMarker275"/>create a trimmer that uses a <code class="inlineCode">len</code> function and threshold <code class="inlineCode">1</code> – i.e., it always removes the entire history and keeps a system message only:</p>
<pre>trimmer = trim_messages(
   max_tokens=1,
   strategy="last",
   token_counter=len,
   include_system=True,
   start_on="human",
)
raw_chain = trimmer | llm
chain = RunnableWithMessageHistory(raw_chain, get_session_history)</pre>
<p class="normal">Now let’s run it and make sure that our history keeps all the interactions with the user but a trimmed history is passed to the LLM:</p>
<pre>config = {"callbacks": [PrintOutputCallback()], "configurable": {"session_id": "1"}}
_ = chain.invoke(
   [HumanMessage("Hi!")],
   config=config,
)
print(f"History length: {len(sessions['1'].messages)}")
_ = chain.invoke(
   [HumanMessage("How are you?")],
   config=config,
)
print(f"History length: {len(sessions['1'].messages)}")
&gt;&gt; Amount of input messages: 1
History length: 2
Amount of input messages: 1
History length: 4</pre>
<p class="normal">We used a <code class="inlineCode">RunnableWithMessageHistory</code> that takes a chain and wraps it (like a decorator) with calls to history before executing the chain (to retrieve the history and pass it to the chain) and after finishing the chain (to add new messages to the history).</p>
<div><p class="normal">Database <a id="_idIndexMarker276"/>providers might have their integrations as part of the <code class="inlineCode">langchain_commuity</code> package or outside of it – for example, in libraries such as <code class="inlineCode">langchain_postgres</code> for a standalone PostgreSQL database or <code class="inlineCode">langchain-google-cloud-sql-pg</code> for a managed one.</p>
<div><div><p class="normal">You can find the full list of integrations to store chat history on the documentation page: <a href="https://python.langchain.com/api_reference/community/chat_message_histories.html">python.langchain.com/api_reference/community/chat_message_histories.html</a>.</p>
</div>
</div>
<p class="normal">When designing a real application, you should be cautious about managing access to somebody’s sessions. For example, if you use a sequential <code class="inlineCode">session_id</code>, users might easily access sessions that don’t belong to them. Practically, it might be enough to use a <code class="inlineCode">uuid</code> (a uniquely generated long identifier) instead of a sequential <code class="inlineCode">session_id</code>, <a id="_idTextAnchor145"/>or, depending on your security requirements, add other permissions validations during runtime.</p>
<h2 class="heading-2" id="_idParaDest-84"><a id="_idTextAnchor146"/>LangGraph checkpoints</h2>
<p class="normal">A checkpoint is a snapshot of the current state of the graph. It keeps all the information to continue <a id="_idIndexMarker277"/>running the workflow from the moment when the snapshot has <a id="_idIndexMarker278"/>been taken – including the full state, metadata, nodes that were planned to be executed, and tasks that failed. This is a different mechanism from storing the chat history since you can store the workflow at any given point in time and later restore from the checkpoint to continue. It is important for multiple reasons:</p>
<ul>
<li class="b lletList">Checkpoints allow deep debugging and “time travel.”</li>
<li class="b lletList">Checkpoints allow you to experiment with different paths in your complex workflow without the need to rerun it each time.</li>
<li class="b lletList">Checkpoints facilitate human-in-the-loop workflows by making it possible to implement human intervention at a given point and continue further.</li>
<li class="b lletList">Checkpoints help to implement production-ready systems since they add a required level of persistence and fault tolerance.</li>
</ul>
<p class="normal">Let’s build a simple example with a single node that prints the amount of messages in the state and returns a fake <code class="inlineCode">AIMessage</code>. We use a built-in <code class="inlineCode">MessageGraph</code> that represents a state with only a list of messages, and we initiate a <code class="inlineCode">MemorySaver</code> that will keep checkpoints in local memory and pass it to the graph during compilation:</p>
<pre>from langgraph.graph import MessageGraph
from langgraph.checkpoint.memory import MemorySaver</pre>
<div><pre>def test_node(state):
 # ignore the last message since it's an input one
 print(f"History length = {len(state[:-1])}")
 return [AIMessage(content="Hello!")]
builder = MessageGraph()
builder.add_node("test_node", test_node)
builder.add_edge(START, "test_node")
builder.add_edge("test_node", END)
memory = MemorySaver()
graph = builder.compile(checkpointer=memory)</pre>
<p class="normal">Now, each time <a id="_idIndexMarker279"/>we invoke the graph, we should provide <a id="_idIndexMarker280"/>either a specific checkpoint or a thread-id (a unique identifier of each run). We invoke our graph two times with different <code class="inlineCode">thread-id</code> values, make sure they each start with an empty history, and then check that the first thread has a history when we invoke it for the second time:</p>
<pre>_ = graph.invoke([HumanMessage(content="test")],
  config={"configurable": {"thread_id": "thread-a"}})
_ = graph.invoke([HumanMessage(content="test")]
  config={"configurable": {"thread_id": "thread-b"}})
_ = graph.invoke([HumanMessage(content="test")]
  config={"configurable": {"thread_id": "thread-a"}})
&gt;&gt; History length = 0
History length = 0
History length = 2</pre>
<p class="normal">We can inspect checkpoints for a given thread:</p>
<pre>checkpoints = list(memory.list(config={"configurable": {"thread_id": "thread-a"}}))
for check_point in checkpoints:
 print(check_point.config["configurable"]["checkpoint_id"])</pre>
<p class="normal">Let’s also restore from the initial checkpoint for <code class="inlineCode">thread-a</code>. We’ll see that we start with an empty history:</p>
<pre>checkpoint_id = checkpoints[-1].config["configurable"]["checkpoint_id"]
_ = graph.invoke(</pre>
<div><pre>   [HumanMessage(content="test")],
   config={"configurable": {"thread_id": "thread-a", "checkpoint_id": checkpoint_id}})
&gt;&gt; History length = 0</pre>
<p class="normal">We can <a id="_idIndexMarker281"/>also start from an intermediate checkpoint, as shown here:</p>
<pre>checkpoint_id = checkpoints[-3].config["configurable"]["checkpoint_id"]
_ = graph.invoke(
   [HumanMessage(content="test")],
   config={"configurable": {"thread_id": "thread-a", "checkpoint_id": checkpoint_id}})
&gt;&gt; History length = 2</pre>
<p class="normal">One obvious <a id="_idIndexMarker282"/>use case for checkpoints is implementing workflows that require additional input from the user. We’ll run into exactly the same problem as above – when deploying our production to multiple instances, we can’t guarantee that the next request from the user hits the same server as before. Our graph is stateful (during the execution), but the application that wraps it as a web service should remain stateless. Hence, we can’t store checkpoints in local memory, and we should write them to the database instead. LangGraph offers two integrations: <code class="inlineCode">SqliteSaver</code> and <code class="inlineCode">PostgresSaver</code>. You can always use them as a starting point and build your own integration if you’d like to use another database provider since all you need to implement is storing and retrieving dictionaries that represent a checkpoint.</p>
<p class="normal">Now, you’ve learned the basics and are fully equipped to develop you<a id="_idTextAnchor147"/>r own workflows. We’ll continue to look at more complex examples and techniques in the next chapter.</p>
<h1 class="heading-1" id="_idParaDest-85"><a id="_idTextAnchor148"/>Summary</h1>
<p class="normal">In this chapter, we dived into building complex workflows with LangChain and LangGraph, going beyond simple text generation. We introduced LangGraph as an orchestration framework designed to handle agentic workflows and also created a basic workflow with nodes and edges, and conditional edges, that allow workflow to branch based on the current state. Next, we shifted to output parsing and error handling, where we saw how to use built-in LangChain output parsers and emphasized the importance of graceful error handling.</p>
<div><p class="normal">We then looked into prompt engineering and discussed how to use zero-shot and dynamic few-shot prompting with LangChain, how to construct advanced prompts such as CoT prompting, and how to use substitution mechanisms. Finally, we discussed how to work with long and short contexts, exploring techniques for managing large contexts by splitting the input into smaller pieces and combining the outputs in a Map-Reduce fashion, and worked on an example of processing a large video that doesn’t fit into a context.</p>
<p class="normal">Finally, we covered memory mechanisms in LangChain, emphasized the need for statelessness in production deployments, and discussed methods for managing chat history, including trimming based on length and summarizing conversations.</p>
<p class="normal">We will use what we l<a id="_idTextAnchor149"/>earned here to develop a RAG system in <a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic">Chapter 4</em></a> and more complex agentic workflows in <em class="italic">Chapters 5</em> and <em class="italic">6</em>.</p>
<h1 class="heading-1" id="_idParaDest-86"><a id="_idTextAnchor150"/>Questions</h1>
<ol>
<li class="numberedList" value="1">What is LangGraph, and how does LangGraph workflow differ from LangChain’s vanilla chains?</li>
<li class="numberedList">What is a “state” in LangGraph, and what are its main functions?</li>
<li class="numberedList">Explain the purpose of <em class="italic">add_node</em> and <em class="italic">add_edge</em> in LangGraph.</li>
<li class="numberedList">What are “supersteps” in LangGraph, and how do they relate to parallel execution?</li>
<li class="numberedList">How do conditional edges enhance LangGraph workflows compared to sequential chains?</li>
<li class="numberedList">What is the purpose of the Literal type hint when defining conditional edges?</li>
<li class="numberedList">What are reducers in LangGraph, and how do they allow modification of the state?</li>
<li class="numberedList">Why is error handling crucial in LangChain workflows, and what are some strategies for achieving it?</li>
<li class="numberedList">How can memory mechanisms be used to trim the history of a conversational bot?</li>
<li class="numberedList">What is the use case of LangGraph checkpoints?</li>
</ol>
<div><h1 class="heading-1" id="_idParaDest-87"><a id="_idTextAnchor151"/>Subscribe to our weekly newsletter</h1>
<p class="normal">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at <a href="E_Chapter_3.xhtml">https://packt.link/Q5UyU</a>.</p>
<p class="normal"><img alt="" src="img/Newsletter_QRcode1.jpg"/></p>
</div>
</body></html>