<html><head></head><body>
<div aria-label="67" epub:type="pagebreak" id="page1-5" role="doc-pagebreak"/>
<div id="_idContainer052">
<h1 class="chapterNumber"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 class="chapterTitle" id="_idParaDest-64"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.2.1">Building Workflows with LangGraph</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.3.1">So far, we’ve learned about LLMs, LangChain as a framework, and how to use LLMs with LangChain in a vanilla mode (just asking to generate a text output based on a prompt). </span><span class="koboSpan" id="kobo.3.2">In this chapter, we’ll start with a quick introduction to LangGraph as a framework and how to develop more complex workflows with LangChain and LangGraph by chaining together multiple steps. </span><span class="koboSpan" id="kobo.3.3">As an example, we’ll discuss parsing LLM outputs and look into error handling patterns with LangChain and LangGraph. </span><span class="koboSpan" id="kobo.3.4">Then, we’ll continue with more advanced ways to develop prompts and explore what building blocks LangChain offers for few-shot prompting and other techniques. </span></p>
<p class="normal"><span class="koboSpan" id="kobo.4.1">We’re also going to cover working with multimodal inputs, utilizing the long context, and adjusting your workloads to overcome limitations related to the context window size. </span><span class="koboSpan" id="kobo.4.2">Finally, we’ll look into the basic mechanisms of managing memory with LangChain. </span><span class="koboSpan" id="kobo.4.3">Understanding these fundamental and key techniques will help us read LangGraph code, understand tutorials and code samples, and develop our own complex workflows. </span><span class="koboSpan" id="kobo.4.4">We’ll, of course, discuss what LangGraph workflows are and will continue building on that skill in </span><em class="italic"><span class="koboSpan" id="kobo.5.1">Chapters 5</span></em><span class="koboSpan" id="kobo.6.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.7.1">6</span></em><span class="koboSpan" id="kobo.8.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.9.1">In a nutshell, we’ll cover the following main topics in this chapter:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.10.1">LangGraph fundamentals</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.11.1">Prompt engineering</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.12.1">Working with short context windows</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.13.1">Understanding memory mechanisms</span></li>
</ul>
<div>
<div class="note" id="_idContainer035">
<div aria-label="68" epub:type="pagebreak" id="page2-5" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.14.1">As always, you can find all the code samples on our public GitHub repository as Jupyter notebooks: </span><a href="https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3"><span class="url"><span class="koboSpan" id="kobo.15.1">https://github.com/benman1/generative_ai_with_langchain/tree/second_edition/chapter3</span></span></a><span class="koboSpan" id="kobo.16.1">.</span></p>
</div>
</div>
<h1 class="heading-1" id="_idParaDest-65"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.17.1">LangGraph fundamentals</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.18.1">LangGraph is a framework developed by LangChain (as a company) that helps control and orchestrate </span><a id="_idIndexMarker177"/><span class="koboSpan" id="kobo.19.1">workflows. </span><span class="koboSpan" id="kobo.19.2">Why do we need another orchestration framework? </span><span class="koboSpan" id="kobo.19.3">Let’s park this question until </span><a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic"><span class="koboSpan" id="kobo.20.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.21.1">, where we’ll touch on agents and agentic workflows, but for now, let us mention the flexibility of LangGraph as an orchestration framework and its robustness in handling complex scenarios.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.22.1">Unlike many other frameworks, LangGraph allows cycles (most other orchestration frameworks operate only with directly acyclic graphs), supports streaming out of the box, and has many pre-built loops and components dedicated to generative AI applications (for example, human moderation). </span><span class="koboSpan" id="kobo.22.2">LangGraph also has a very rich API that allows you to have very granular control of your execution flow if needed. </span><span class="koboSpan" id="kobo.22.3">This is not fully covered in our book, but just keep in mind that you can always use a more low-level API if you need to.</span></p>
<div>
<div class="note" id="_idContainer036">
<p class="normal"><span class="koboSpan" id="kobo.23.1">A </span><strong class="keyWord"><span class="koboSpan" id="kobo.24.1">Directed Acyclic Graph (DAG)</span></strong><span class="koboSpan" id="kobo.25.1"> is a </span><a id="_idIndexMarker178"/><span class="koboSpan" id="kobo.26.1">special type of graph in graph theory and computer science. </span><span class="koboSpan" id="kobo.26.2">Its edges (connections between nodes) have a direction, which means that the connection from node A to node B is different from the connection from node B to node A. </span><span class="koboSpan" id="kobo.26.3">It has no cycles. </span><span class="koboSpan" id="kobo.26.4">In other words, there is no path that starts at a node and returns to the same node by following the directed edges.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.27.1">DAGs are often used as a model of workflows in data engineering, where nodes are tasks and edges are dependencies between these tasks. </span><span class="koboSpan" id="kobo.27.2">For example, an edge from node A to node B means that we need output from node A to execute node B.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.28.1">For now, let’s start with the basics. </span><span class="koboSpan" id="kobo.28.2">If you’re new to this framework, we would also highly recommend a free online course on LangGraph that is available at </span><a href="https://academy.langchain.com/"><span class="url"><span class="koboSpan" id="kobo.29.1">https://academy.langchain.com/</span></span></a><span class="koboSpan" id="kobo.30.1"> to deepen your understanding.</span></p>
<div aria-label="69" epub:type="pagebreak" id="page3-5" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-66"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.31.1">State management</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.32.1">State management </span><a id="_idIndexMarker179"/><span class="koboSpan" id="kobo.33.1">is crucial in real-world AI applications. </span><span class="koboSpan" id="kobo.33.2">For example, in a </span><a id="_idIndexMarker180"/><span class="koboSpan" id="kobo.34.1">customer service chatbot, the state might track information such as customer ID, conversation history, and outstanding issues. </span><span class="koboSpan" id="kobo.34.2">LangGraph’s state management lets you maintain this context across a complex workflow of multiple AI components.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.35.1">LangGraph allows </span><a id="_idIndexMarker181"/><span class="koboSpan" id="kobo.36.1">you to develop and execute complex workflows called </span><strong class="keyWord"><span class="koboSpan" id="kobo.37.1">graphs</span></strong><span class="koboSpan" id="kobo.38.1">. </span><span class="koboSpan" id="kobo.38.2">We will use the words </span><em class="italic"><span class="koboSpan" id="kobo.39.1">graph</span></em><span class="koboSpan" id="kobo.40.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.41.1">workflow</span></em><span class="koboSpan" id="kobo.42.1"> interchangeably in this chapter. </span><span class="koboSpan" id="kobo.42.2">A graph consists of nodes and edges between them. </span><span class="koboSpan" id="kobo.42.3">Nodes are components of your workflow, and a workflow has a </span><em class="italic"><span class="koboSpan" id="kobo.43.1">state</span></em><span class="koboSpan" id="kobo.44.1">. </span><span class="koboSpan" id="kobo.44.2">What is it? </span><span class="koboSpan" id="kobo.44.3">Firstly, a state makes your nodes aware of the current context by keeping track of the user input and previous computations. </span><span class="koboSpan" id="kobo.44.4">Secondly, a state allows you to persist your workflow execution at any point in time. </span><span class="koboSpan" id="kobo.44.5">Thirdly, a state makes your workflow truly interactive since a node </span><a id="_idIndexMarker182"/><span class="koboSpan" id="kobo.45.1">can change the workflow’s </span><a id="_idIndexMarker183"/><span class="koboSpan" id="kobo.46.1">behavior by updating the state. </span><span class="koboSpan" id="kobo.46.2">For simplicity, think about a state as a Python dictionary. </span><span class="koboSpan" id="kobo.46.3">Nodes are Python functions that operate on this dictionary. </span><span class="koboSpan" id="kobo.46.4">They take a dictionary as input and return another dictionary that contains keys and values to be updated in the state of the workflow.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.47.1">Let’s understand that with a simple example. </span><span class="koboSpan" id="kobo.47.2">First, we need to define a state’s schema:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.48.1">from</span></span><span class="koboSpan" id="kobo.49.1"> typing_extensions </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.50.1">import</span></span><span class="koboSpan" id="kobo.51.1"> TypedDict</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.52.1">class</span></span> <span class="hljs-symbol"><span class="koboSpan" id="kobo.53.1">JobApplicationState</span></span><span class="koboSpan" id="kobo.54.1">(</span><span class="hljs-symbol"><span class="koboSpan" id="kobo.55.1">TypedDict</span></span><span class="koboSpan" id="kobo.56.1">):</span></p>
<p class="snippet-code"> <span class="hljs-symbol"><span class="koboSpan" id="kobo.57.1">job_description: str</span></span></p>
<p class="snippet-code"> <span class="hljs-symbol"><span class="koboSpan" id="kobo.58.1">is_suitable: bool</span></span></p>
<p class="snippet-code"> <span class="hljs-symbol"><span class="koboSpan" id="kobo.59.1">application: str</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.60.1">A </span><code class="inlineCode"><span class="koboSpan" id="kobo.61.1">TypedDict</span></code><span class="koboSpan" id="kobo.62.1"> is a Python </span><a id="_idIndexMarker184"/><span class="koboSpan" id="kobo.63.1">type constructor that allows to define dictionaries with a predefined set of keys and each key can have its own type (as opposed to a </span><code class="inlineCode"><span class="koboSpan" id="kobo.64.1">Dict[str, str]</span></code><span class="koboSpan" id="kobo.65.1"> construction).</span></p>
<div>
<div class="note" id="_idContainer037">
<p class="normal"><span class="koboSpan" id="kobo.66.1">LangGraph state’s schema shouldn’t necessarily be defined as a </span><code class="inlineCode"><span class="koboSpan" id="kobo.67.1">TypedDict</span></code><span class="koboSpan" id="kobo.68.1">; you can use data classes or Pydantic models too.</span></p>
</div>
</div>
<div aria-label="70" epub:type="pagebreak" id="page4-5" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.69.1">After we have defined a schema for a state, we can define our first simple workflow:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.70.1">from</span></span><span class="koboSpan" id="kobo.71.1"> langgraph.graph import StateGraph, START, END, Graph</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.72.1">def analyze_job_description(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.73.1">state</span></span><span class="koboSpan" id="kobo.74.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.75.1">   print(</span><span class="hljs-string"><span class="koboSpan" id="kobo.76.1">"...Analyzing a provided job description ..."</span></span><span class="koboSpan" id="kobo.77.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.78.1">   return {</span><span class="hljs-string"><span class="koboSpan" id="kobo.79.1">"is_suitable"</span></span><span class="koboSpan" id="kobo.80.1">: len(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.81.1">state</span></span><span class="koboSpan" id="kobo.82.1">[</span><span class="hljs-string"><span class="koboSpan" id="kobo.83.1">"job_description"</span></span><span class="koboSpan" id="kobo.84.1">]) &gt; </span><span class="hljs-number"><span class="koboSpan" id="kobo.85.1">100</span></span><span class="koboSpan" id="kobo.86.1">}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.87.1">def generate_application(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.88.1">state</span></span><span class="koboSpan" id="kobo.89.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.90.1">   print(</span><span class="hljs-string"><span class="koboSpan" id="kobo.91.1">"...generating application..."</span></span><span class="koboSpan" id="kobo.92.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.93.1">   return {</span><span class="hljs-string"><span class="koboSpan" id="kobo.94.1">"application"</span></span><span class="koboSpan" id="kobo.95.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.96.1">"some_fake_application"</span></span><span class="koboSpan" id="kobo.97.1">}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.98.1">builder = StateGraph(JobApplicationState)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.99.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.100.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.101.1">, analyze_job_description)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.102.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.103.1">"generate_application"</span></span><span class="koboSpan" id="kobo.104.1">, generate_application)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.105.1">builder.add_edge(START, </span><span class="hljs-string"><span class="koboSpan" id="kobo.106.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.107.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.108.1">builder.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.109.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.110.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.111.1">"generate_application"</span></span><span class="koboSpan" id="kobo.112.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.113.1">builder.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.114.1">"generate_application"</span></span><span class="koboSpan" id="kobo.115.1">, END)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.116.1">graph = builder.compile()</span></p>
<p class="normal"><span class="koboSpan" id="kobo.117.1">Here, we defined two Python functions that are components of our workflow. </span><span class="koboSpan" id="kobo.117.2">Then, we defined our </span><a id="_idIndexMarker185"/><span class="koboSpan" id="kobo.118.1">workflow by providing a state’s schema, adding nodes and edges between them. </span><code class="inlineCode"><span class="koboSpan" id="kobo.119.1">add_node</span></code><span class="koboSpan" id="kobo.120.1"> is a convenient way to add a component </span><a id="_idIndexMarker186"/><span class="koboSpan" id="kobo.121.1">to your graph (by providing its name and a corresponding Python function), and you can reference this name later when you define edges with </span><code class="inlineCode"><span class="koboSpan" id="kobo.122.1">add_edge</span></code><span class="koboSpan" id="kobo.123.1">. </span><code class="inlineCode"><span class="koboSpan" id="kobo.124.1">START</span></code><span class="koboSpan" id="kobo.125.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.126.1">END</span></code><span class="koboSpan" id="kobo.127.1"> are reserved built-in nodes that define the beginning and end of the workflow accordingly.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.128.1">Let’s take a look at our workflow by using a built-in visualization mechanism:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.129.1">from IPython</span><span class="hljs-selector-class"><span class="koboSpan" id="kobo.130.1">.display</span></span><span class="koboSpan" id="kobo.131.1"> import Image, </span><span class="hljs-attribute"><span class="koboSpan" id="kobo.132.1">display</span></span></p>
<p class="snippet-code"><span class="hljs-title"><span class="koboSpan" id="kobo.133.1">display</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.134.1">(Image(graph.get_graph()</span></span><span class="hljs-selector-class"><span class="koboSpan" id="kobo.135.1">.draw_mermaid_png</span></span><span class="koboSpan" id="kobo.136.1">()))</span></p>
<div aria-label="71" epub:type="pagebreak" id="page5-5" role="doc-pagebreak"/>
<figure class="mediaobject"><span class="koboSpan" id="kobo.137.1"><img alt="Figure 3.1: LangGraph built-in visualization of our first workflow" src="../Images/B32363_03_01.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.138.1">Figure 3.1: LangGraph built-in visualization of our first workflow</span></p>
<p class="normal"><span class="koboSpan" id="kobo.139.1">Our function accesses the state by simply reading from the dictionary that LangGraph automatically provides as input. </span><span class="koboSpan" id="kobo.139.2">LangGraph isolates state updates. </span><span class="koboSpan" id="kobo.139.3">When a node receives the state, it gets an immutable copy, not a reference to the actual state object. </span><span class="koboSpan" id="kobo.139.4">The node must return a dictionary containing the specific keys and values it wants to update. </span><span class="koboSpan" id="kobo.139.5">LangGraph then handles merging these updates into the master state. </span><span class="koboSpan" id="kobo.139.6">This pattern prevents side effects and ensures that state changes are explicit and traceable.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.140.1">The only way </span><a id="_idIndexMarker187"/><span class="koboSpan" id="kobo.141.1">for a node to modify a state is to provide an output dictionary </span><a id="_idIndexMarker188"/><span class="koboSpan" id="kobo.142.1">with key-value pairs to be updated, and LangGraph will handle it. </span><span class="koboSpan" id="kobo.142.2">A node should modify at least one key in the state. </span><span class="koboSpan" id="kobo.142.3">A </span><code class="inlineCode"><span class="koboSpan" id="kobo.143.1">graph</span></code><span class="koboSpan" id="kobo.144.1"> instance itself is a </span><code class="inlineCode"><span class="koboSpan" id="kobo.145.1">Runnable</span></code><span class="koboSpan" id="kobo.146.1"> (to be precise, it inherits from </span><code class="inlineCode"><span class="koboSpan" id="kobo.147.1">Runnable</span></code><span class="koboSpan" id="kobo.148.1">) and we can execute it. </span><span class="koboSpan" id="kobo.148.2">We should provide a dictionary with the initial state, and we’ll get the final state as an output:</span></p>
<p class="snippet-code"><span class="hljs-attr"><span class="koboSpan" id="kobo.149.1">res</span></span><span class="koboSpan" id="kobo.150.1"> = graph.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.151.1">"job_description"</span></span><span class="koboSpan" id="kobo.152.1">:</span><span class="hljs-string"><span class="koboSpan" id="kobo.153.1">"fake_jd"</span></span><span class="koboSpan" id="kobo.154.1">})</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.155.1">print(res)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.156.1">&gt;&gt;...Analyzing a provided job description ...</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.157.1">...generating application...</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.158.1">{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_fake_application'}</span></p>
<div aria-label="72" epub:type="pagebreak" id="page6-5" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.159.1">We used a very simple graph as an example. </span><span class="koboSpan" id="kobo.159.2">With your real workflows, you can define parallel steps (for example, you can easily connect one node with multiple nodes) and even cycles. </span><span class="koboSpan" id="kobo.159.3">LangGraph executes the </span><a id="_idIndexMarker189"/><span class="koboSpan" id="kobo.160.1">workflow in so-called </span><em class="italic"><span class="koboSpan" id="kobo.161.1">supersteps</span></em><span class="koboSpan" id="kobo.162.1"> that can call multiple nodes at the same time (and then merge state updates from these nodes). </span><span class="koboSpan" id="kobo.162.2">You can control the depth of recursion and amount of overall supersteps in the graph, which helps you avoid cycles running forever, especially because the LLMs output is non-deterministic.</span></p>
<div>
<div class="note" id="_idContainer039">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.163.1">A superstep</span></strong><span class="koboSpan" id="kobo.164.1"> on LangGraph represents a discrete iteration over one or a few nodes, and it’s inspired by Pregel, a system built by Google for processing large graphs at scale. </span><span class="koboSpan" id="kobo.164.2">It handles parallel execution of nodes and updates sent to the central gr</span><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.165.1">aph’s state.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.166.1">In our example, we used direct edges from one node to another. </span><span class="koboSpan" id="kobo.166.2">It makes our graph no different from a sequential chain that we could have defined with LangChain. </span><span class="koboSpan" id="kobo.166.3">One of the key LangGraph features is the ability to create conditional edges that can direct the execution flow to one or another node depending on the current state. </span><span class="koboSpan" id="kobo.166.4">A conditional edge is a Python function that gets the current state as an input and returns a string with the node’s name to be executed.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.167.1">Let’s look at an example:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.168.1">from</span></span><span class="koboSpan" id="kobo.169.1"> typing </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.170.1">import</span></span> <span class="hljs-type"><span class="koboSpan" id="kobo.171.1">Literal</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.172.1">builder = StateGraph(JobApplicationState)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.173.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.174.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.175.1">analyze_job_description"</span></span><span class="koboSpan" id="kobo.176.1">, analyze_job_description)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.177.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.178.1">"generate_application"</span></span><span class="koboSpan" id="kobo.179.1">, generate_application)</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.180.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.181.1">is_suitable_condition</span></span><span class="koboSpan" id="kobo.182.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.183.1">state: StateGraph</span></span><span class="koboSpan" id="kobo.184.1">) -&gt; </span><span class="hljs-type"><span class="koboSpan" id="kobo.185.1">Literal</span></span><span class="koboSpan" id="kobo.186.1">[</span><span class="hljs-string"><span class="koboSpan" id="kobo.187.1">"generate_application"</span></span><span class="koboSpan" id="kobo.188.1">, END]:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.189.1">if</span></span><span class="koboSpan" id="kobo.190.1"> state.get(</span><span class="hljs-string"><span class="koboSpan" id="kobo.191.1">"is_suitable"</span></span><span class="koboSpan" id="kobo.192.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.193.1">return</span></span> <span class="hljs-string"><span class="koboSpan" id="kobo.194.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.195.1">generate_application"</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.196.1">return</span></span><span class="koboSpan" id="kobo.197.1"> END</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.198.1">builder.add_edge(START, </span><span class="hljs-string"><span class="koboSpan" id="kobo.199.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.200.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.201.1">builder.add_conditional_edges(</span><span class="hljs-string"><span class="koboSpan" id="kobo.202.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.203.1">, is_suitable_condition)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.204.1">builder.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.205.1">"generate_application"</span></span><span class="koboSpan" id="kobo.206.1">, END)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.207.1">graph = builder.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.208.1">compile</span></span><span class="koboSpan" id="kobo.209.1">()</span></p>
<div aria-label="73" epub:type="pagebreak" id="page7-4" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.210.1">from</span></span><span class="koboSpan" id="kobo.211.1"> IPython.display </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.212.1">import</span></span><span class="koboSpan" id="kobo.213.1"> Image, display</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.214.1">display(Image(graph.get_graph().draw_mermaid_png()))</span></p>
<p class="normal"><span class="koboSpan" id="kobo.215.1">We’ve defined </span><a id="_idIndexMarker190"/><span class="koboSpan" id="kobo.216.1">an edge </span><code class="inlineCode"><span class="koboSpan" id="kobo.217.1">is_suitable_condition</span></code><span class="koboSpan" id="kobo.218.1"> that takes a state and returns either an </span><code class="inlineCode"><span class="koboSpan" id="kobo.219.1">END</span></code><span class="koboSpan" id="kobo.220.1"> or </span><code class="inlineCode"><span class="koboSpan" id="kobo.221.1">generate_application</span></code><span class="koboSpan" id="kobo.222.1"> string by analyzing the current state. </span><span class="koboSpan" id="kobo.222.2">We used a </span><code class="inlineCode"><span class="koboSpan" id="kobo.223.1">Literal</span></code><span class="koboSpan" id="kobo.224.1"> type hint since it’s </span><a id="_idIndexMarker191"/><span class="koboSpan" id="kobo.225.1">used by LangGraph to determine which destination nodes to connect the source node with when it’s creating conditional edges. </span><span class="koboSpan" id="kobo.225.2">If you don’t use a type hint, you can provide a list of destination nodes directly to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.226.1">add_conditional_edges</span></code><span class="koboSpan" id="kobo.227.1"> function; otherwise, LangGraph will connect the source node with all other nodes in the graph (since it doesn’t analyze the code of an edge function itself when creating a graph). </span><span class="koboSpan" id="kobo.227.2">The following figure shows the output generated:</span></p>
<figure class="mediaobject"> <span class="koboSpan" id="kobo.228.1"><img alt="Figure 3.2: A workflow with conditional edges (represented as dotted lines)" src="../Images/B32363_03_02.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.229.1">Figure 3.2: A workflow with conditional edges (represented as dotted lines)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.230.1">Conditional </span><a id="_idIndexMarker192"/><span class="koboSpan" id="kobo.231.1">edges are visualized with dotted lines, and </span><a id="_idIndexMarker193"/><span class="koboSpan" id="kobo.232.1">now we can see that, depending on the output of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.233.1">analyze_job_description</span></code><span class="koboSpan" id="kobo.234.1"> step, our graph can perform diff</span><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.235.1">erent actions.</span></p>
<h2 class="heading-2" id="_idParaDest-67"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.236.1">Reducers</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.237.1">So far, our nodes have changed the state by updating the value for a corresponding key. </span><span class="koboSpan" id="kobo.237.2">From another </span><a id="_idIndexMarker194"/><span class="koboSpan" id="kobo.238.1">point of view, at each superstep, LangGraph can produce a new value for a given key. </span><span class="koboSpan" id="kobo.238.2">In other words, for every key in the state, there’s a sequence of values, and from a functional </span><a id="_idIndexMarker195"/><span class="koboSpan" id="kobo.239.1">programming perspective, a </span><code class="inlineCode"><span class="koboSpan" id="kobo.240.1">reduce</span></code><span class="koboSpan" id="kobo.241.1"> function can be applied to this sequence. </span><span class="koboSpan" id="kobo.241.2">The default reducer on LangGraph always replaces the final value with the new value. </span><span class="koboSpan" id="kobo.241.3">Let’s imagine we want to track custom actions (produced by nodes) and compare three options.</span></p>
<div aria-label="74" epub:type="pagebreak" id="page8-4" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.242.1">With the first option, a node should return a list as a value for the key </span><code class="inlineCode"><span class="koboSpan" id="kobo.243.1">actions</span></code><span class="koboSpan" id="kobo.244.1">. </span><span class="koboSpan" id="kobo.244.2">We provide short code samples just for illustration purposes, but you can find full ones on Github. </span><span class="koboSpan" id="kobo.244.3">If such a value already exists in the state, it will be replaced with the new one:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.245.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.246.1">JobApplicationState</span></span><span class="koboSpan" id="kobo.247.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.248.1">TypedDict</span></span><span class="koboSpan" id="kobo.249.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.250.1">   ...</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.251.1">   actions: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.252.1">list</span></span><span class="koboSpan" id="kobo.253.1">[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.254.1">str</span></span><span class="koboSpan" id="kobo.255.1">]</span></p>
<p class="normal"><span class="koboSpan" id="kobo.256.1">Another option is to use the default </span><code class="inlineCode"><span class="koboSpan" id="kobo.257.1">add</span></code><span class="koboSpan" id="kobo.258.1"> method with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.259.1">Annotated</span></code><span class="koboSpan" id="kobo.260.1"> type hint. </span><span class="koboSpan" id="kobo.260.2">By using this type hint, we tell the LangGraph compiler that the type of our variable in the state is a list of strings, and it should use the </span><code class="inlineCode"><span class="koboSpan" id="kobo.261.1">add</span></code><span class="koboSpan" id="kobo.262.1"> method to concatenate two lists (if the value already exists in the state and a node produces a new one):</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.263.1">from</span></span><span class="koboSpan" id="kobo.264.1"> typing </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.265.1">import</span></span><span class="koboSpan" id="kobo.266.1"> Annotated, </span><span class="hljs-type"><span class="koboSpan" id="kobo.267.1">Optional</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.268.1">from</span></span><span class="koboSpan" id="kobo.269.1"> operator </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.270.1">import</span></span><span class="koboSpan" id="kobo.271.1"> add</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.272.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.273.1">JobApplicationState</span></span><span class="koboSpan" id="kobo.274.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.275.1">TypedDict</span></span><span class="koboSpan" id="kobo.276.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.277.1">   ...</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.278.1">   actions: Annotated[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.279.1">list</span></span><span class="koboSpan" id="kobo.280.1">[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.281.1">str</span></span><span class="koboSpan" id="kobo.282.1">], add]</span></p>
<p class="normal"><span class="koboSpan" id="kobo.283.1">The last option </span><a id="_idIndexMarker196"/><span class="koboSpan" id="kobo.284.1">is to write your own custom reducer. </span><span class="koboSpan" id="kobo.284.2">In this </span><a id="_idIndexMarker197"/><span class="koboSpan" id="kobo.285.1">example, we write a custom reducer that accepts not only a list from the node (as a new value) but also a single string that would be converted to a list:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.286.1">from</span></span><span class="koboSpan" id="kobo.287.1"> typing </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.288.1">import</span></span><span class="koboSpan" id="kobo.289.1"> Annotated, </span><span class="hljs-type"><span class="koboSpan" id="kobo.290.1">Optional</span></span><span class="koboSpan" id="kobo.291.1">, </span><span class="hljs-type"><span class="koboSpan" id="kobo.292.1">Union</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.293.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.294.1">my_reducer</span></span><span class="koboSpan" id="kobo.295.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.296.1">left: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.297.1">list</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.298.1">[</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.299.1">str</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.300.1">], right: </span></span><span class="hljs-type"><span class="koboSpan" id="kobo.301.1">Optional</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.302.1">[</span></span><span class="hljs-type"><span class="koboSpan" id="kobo.303.1">Union</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.304.1">[</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.305.1">str</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.306.1">, </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.307.1">list</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.308.1">[</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.309.1">str</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.310.1">]]]</span></span><span class="koboSpan" id="kobo.311.1">) -&gt; </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.312.1">list</span></span><span class="koboSpan" id="kobo.313.1">[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.314.1">str</span></span><span class="koboSpan" id="kobo.315.1">]:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.316.1">if</span></span><span class="koboSpan" id="kobo.317.1"> right:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.318.1">return</span></span><span class="koboSpan" id="kobo.319.1"> left + [right] </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.320.1">if</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.321.1">isinstance</span></span><span class="koboSpan" id="kobo.322.1">(right, </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.323.1">str</span></span><span class="koboSpan" id="kobo.324.1">) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.325.1">else</span></span><span class="koboSpan" id="kobo.326.1"> left + right</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.327.1">return</span></span><span class="koboSpan" id="kobo.328.1"> left</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.329.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.330.1">JobApplicationState</span></span><span class="koboSpan" id="kobo.331.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.332.1">TypedDict</span></span><span class="koboSpan" id="kobo.333.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.334.1">   ...</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.335.1">   actions: Annotated[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.336.1">list</span></span><span class="koboSpan" id="kobo.337.1">[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.338.1">str</span></span><span class="koboSpan" id="kobo.339.1">], my_reducer]</span></p>
<p class="normal"><span class="koboSpan" id="kobo.340.1">LangGraph has a few built-in reducers, and we’ll also demonstrate how you can implement your own. </span><span class="koboSpan" id="kobo.340.2">One of the important ones is </span><code class="inlineCode"><span class="koboSpan" id="kobo.341.1">add_messages</span></code><span class="koboSpan" id="kobo.342.1">, which allows us to merge messages. </span><span class="koboSpan" id="kobo.342.2">Many of your nodes would be LLM agents, and LLMs typically work with messages. </span><span class="koboSpan" id="kobo.342.3">Therefore, according to the conversational programming paradigm we’ll talk about in more detail in </span><em class="italic"><span class="koboSpan" id="kobo.343.1">Chapters 5</span></em><span class="koboSpan" id="kobo.344.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.345.1">6</span></em><span class="koboSpan" id="kobo.346.1">, you typically need to keep track of these messages:</span></p>
<div aria-label="75" epub:type="pagebreak" id="page9-3" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-title"><span class="koboSpan" id="kobo.347.1">from</span></span><span class="koboSpan" id="kobo.348.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.349.1">import</span></span><span class="koboSpan" id="kobo.350.1"> AnyMessage</span></p>
<p class="snippet-code"><span class="hljs-title"><span class="koboSpan" id="kobo.351.1">from</span></span><span class="koboSpan" id="kobo.352.1"> langgraph.graph.message </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.353.1">import</span></span><span class="koboSpan" id="kobo.354.1"> add_messages </span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.355.1">class</span></span><span class="hljs-class"> </span><span class="hljs-type"><span class="koboSpan" id="kobo.356.1">JobApplicationState</span></span><span class="hljs-class"><span class="koboSpan" id="kobo.357.1">(</span></span><span class="hljs-type"><span class="koboSpan" id="kobo.358.1">TypedDict</span></span><span class="hljs-class"><span class="koboSpan" id="kobo.359.1">): </span></span></p>
<p class="snippet-code"><span class="hljs-class"><span class="koboSpan" id="kobo.360.1">  ...</span></span></p>
<p class="snippet-code"><span class="hljs-class"><span class="koboSpan" id="kobo.361.1">  messages: </span></span><span class="hljs-type"><span class="koboSpan" id="kobo.362.1">Annotated</span></span><span class="hljs-class"><span class="koboSpan" id="kobo.363.1">[list[</span></span><span class="hljs-type"><span class="koboSpan" id="kobo.364.1">AnyMessage</span></span><span class="hljs-class"><span class="koboSpan" id="kobo.365.1">], add_messages]</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.366.1">Since this is such an important reducer, there’s a built-in state that you can inherit from:</span></p>
<p class="snippet-code"><span class="hljs-title"><span class="koboSpan" id="kobo.367.1">from</span></span><span class="koboSpan" id="kobo.368.1"> langgraph.graph </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.369.1">import</span></span><span class="koboSpan" id="kobo.370.1"> MessagesState </span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.371.1">class</span></span><span class="hljs-class"> </span><span class="hljs-type"><span class="koboSpan" id="kobo.372.1">JobApplicationState</span></span><span class="hljs-class"><span class="koboSpan" id="kobo.373.1">(</span></span><span class="hljs-type"><span class="koboSpan" id="kobo.374.1">MessagesState</span></span><span class="hljs-class"><span class="koboSpan" id="kobo.375.1">): </span></span></p>
<p class="snippet-code"><span class="hljs-class"><span class="koboSpan" id="kobo.376.1">  ...</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.377.1">Now, as we have </span><a id="_idIndexMarker198"/><span class="koboSpan" id="kobo.378.1">discussed reducers, let’s talk about another important concept for </span><a id="_idIndexMarker199"/><span class="koboSpan" id="kobo.379.1">any developer – how to write reusable and modular workflows by passing con</span><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.380.1">figurations to them.</span></p>
<h2 class="heading-2" id="_idParaDest-68"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.381.1">Making graphs configurable</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.382.1">LangGraph provides a powerful API that allows you to make your graph configurable. </span><span class="koboSpan" id="kobo.382.2">It allows you to </span><a id="_idIndexMarker200"/><span class="koboSpan" id="kobo.383.1">separate parameters from user input – for example, to experiment between different LLM providers or pass custom callbacks. </span><span class="koboSpan" id="kobo.383.2">A node can </span><a id="_idIndexMarker201"/><span class="koboSpan" id="kobo.384.1">also access the configuration by accepting it as a second argument. </span><span class="koboSpan" id="kobo.384.2">The configuration will be passed as an instance of </span><code class="inlineCode"><span class="koboSpan" id="kobo.385.1">RunnableConfig.</span></code></p>
<p class="normal"><code class="inlineCode"><span class="koboSpan" id="kobo.386.1">RunnableConfig</span></code><span class="koboSpan" id="kobo.387.1"> is a typed dictionary that gives you control over execution control settings. </span><span class="koboSpan" id="kobo.387.2">For example, you can control the maximum number of supersteps with the </span><code class="inlineCode"><span class="koboSpan" id="kobo.388.1">recursion_limit </span></code><span class="koboSpan" id="kobo.389.1">parameter. </span><code class="inlineCode"><span class="koboSpan" id="kobo.390.1">RunnableConfig</span></code><span class="koboSpan" id="kobo.391.1"> also allows you to pass custom parameters as a separate dictionary under a </span><code class="inlineCode"><span class="koboSpan" id="kobo.392.1">configurable</span></code><span class="koboSpan" id="kobo.393.1"> key.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.394.1">Let’s allow our node to use different LLMs during application generation:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.395.1">from langchain_core.runnables.</span><span class="hljs-function"><span class="koboSpan" id="kobo.396.1">config </span></span><span class="hljs-keyword"><span class="koboSpan" id="kobo.397.1">import</span></span><span class="hljs-function"><span class="koboSpan" id="kobo.398.1"> RunnableConfig</span></span></p>
<p class="snippet-code"><span class="hljs-function"><span class="koboSpan" id="kobo.399.1">def </span></span><span class="hljs-title"><span class="koboSpan" id="kobo.400.1">generate_application</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.401.1">(state: JobApplicationState, config: RunnableConfig)</span></span><span class="hljs-function"><span class="koboSpan" id="kobo.402.1">:</span></span></p>
<p class="snippet-code"><span class="hljs-function"><span class="koboSpan" id="kobo.403.1">   model_provider =</span></span><span class="koboSpan" id="kobo.404.1"> config[</span><span class="hljs-string"><span class="koboSpan" id="kobo.405.1">"configurable"</span></span><span class="koboSpan" id="kobo.406.1">].</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.407.1">get</span></span><span class="koboSpan" id="kobo.408.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.409.1">"model_provider"</span></span><span class="koboSpan" id="kobo.410.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.411.1">"Google"</span></span><span class="koboSpan" id="kobo.412.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.413.1">   model_name = config[</span><span class="hljs-string"><span class="koboSpan" id="kobo.414.1">"configurable"</span></span><span class="koboSpan" id="kobo.415.1">].</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.416.1">get</span></span><span class="koboSpan" id="kobo.417.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.418.1">"model_name"</span></span><span class="koboSpan" id="kobo.419.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.420.1">"gemini-1.5-flash-002"</span></span><span class="koboSpan" id="kobo.421.1">)</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.422.1">print</span></span><span class="koboSpan" id="kobo.423.1">(f</span><span class="hljs-string"><span class="koboSpan" id="kobo.424.1">"...generating application with {model_provider} and {model_name} ..."</span></span><span class="koboSpan" id="kobo.425.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.426.1">return</span></span><span class="koboSpan" id="kobo.427.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.428.1">"application"</span></span><span class="koboSpan" id="kobo.429.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.430.1">"some_fake_application"</span></span><span class="koboSpan" id="kobo.431.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.432.1">"actions"</span></span><span class="koboSpan" id="kobo.433.1">: [</span><span class="hljs-string"><span class="koboSpan" id="kobo.434.1">"action2"</span></span><span class="koboSpan" id="kobo.435.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.436.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.437.1">action3"</span></span><span class="koboSpan" id="kobo.438.1">]}</span></p>
<div aria-label="76" epub:type="pagebreak" id="page10-3" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.439.1">Let’s now compile and execute our graph with a custom configuration (if you don’t provide any, LangGraph will use the default one):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.440.1">res = graph</span><span class="hljs-selector-class"><span class="koboSpan" id="kobo.441.1">.invoke</span></span><span class="koboSpan" id="kobo.442.1">({</span><span class="hljs-string"><span class="koboSpan" id="kobo.443.1">"job_description"</span></span><span class="koboSpan" id="kobo.444.1">:</span><span class="hljs-string"><span class="koboSpan" id="kobo.445.1">"fake_jd"</span></span><span class="koboSpan" id="kobo.446.1">}, config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.447.1">"configurable"</span></span><span class="koboSpan" id="kobo.448.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.449.1">"model_provider"</span></span><span class="koboSpan" id="kobo.450.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.451.1">"OpenAI"</span></span><span class="koboSpan" id="kobo.452.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.453.1">"model_name"</span></span><span class="koboSpan" id="kobo.454.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.455.1">"gpt-4o"</span></span><span class="koboSpan" id="kobo.456.1">}})</span></p>
<p class="snippet-code"><span class="hljs-title"><span class="koboSpan" id="kobo.457.1">print</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.458.1">(res)</span></span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.459.1">&gt;&gt; ...Analyzing a provided job description ...</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.460.1">...generating application with OpenAI and OpenAI ...</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.461.1">{'job_description': 'fake_jd', 'is_suitable': True, 'application': 'some_fake_application', 'actions': ['action1', 'action2', 'action3']}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.462.1">Now that </span><a id="_idIndexMarker202"/><span class="koboSpan" id="kobo.463.1">we’ve established how to structure </span><a id="_idIndexMarker203"/><span class="koboSpan" id="kobo.464.1">complex workflows with LangGraph, let’s look at a common challenge these workflows face: ensuring LLM outputs follow the exact structure needed by downstream components. </span><span class="koboSpan" id="kobo.464.2">Robust output parsing and graceful error handling are essential for reliable AI pipelines.</span></p>
<h2 class="heading-2" id="_idParaDest-69"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.465.1">Controlled output generation</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.466.1">When you develop complex workflows, one of the common tasks you need to solve is to force an LLM to </span><a id="_idIndexMarker204"/><span class="koboSpan" id="kobo.467.1">generate an output that follows a certain structure. </span><span class="koboSpan" id="kobo.467.2">This is called a controlled generation. </span><span class="koboSpan" id="kobo.467.3">This way, it can be consumed </span><a id="_idIndexMarker205"/><span class="koboSpan" id="kobo.468.1">programmatically by the next steps further down the workflow. </span><span class="koboSpan" id="kobo.468.2">For example, we can ask the LLM to generate JSON or XML for an API call, extract certain attributes from a text, or generate a CSV table. </span><span class="koboSpan" id="kobo.468.3">There are multiple ways to achieve this, and we’ll start exploring them in this chapter and continue in </span><a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic"><span class="koboSpan" id="kobo.469.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.470.1">. </span><span class="koboSpan" id="kobo.470.2">Since an LLM might not always follow the exact output structure, the next step might fail, and you’ll need to recover from the error. </span><span class="koboSpan" id="kobo.470.3">Hence, we’ll also begin discussing error handling in this section.</span></p>
<h3 class="heading-3" id="_idParaDest-70"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.471.1">Output parsing</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.472.1">Output parsing is essential when integrating LLMs into larger workflows, where subsequent steps require </span><a id="_idIndexMarker206"/><span class="koboSpan" id="kobo.473.1">structured data rather than natural </span><a id="_idIndexMarker207"/><span class="koboSpan" id="kobo.474.1">language responses. </span><span class="koboSpan" id="kobo.474.2">One way to do that is to add corresponding instructions to the prompt and parse the output.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.475.1">Let’s see a simple task. </span><span class="koboSpan" id="kobo.475.2">We’d like to classify whether a certain job description is suitable for a junior Java programmer as a step of our pipeline and, based on the LLM’s decision, we’d like to either continue with an application or ignore this specific job description. </span><span class="koboSpan" id="kobo.475.3">We can start with a simple prompt:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.476.1">from</span></span><span class="koboSpan" id="kobo.477.1"> langchain_google_vertexai import ChatVertexAI</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.478.1">llm = ChatVertexAI(</span><span class="hljs-attribute"><span class="koboSpan" id="kobo.479.1">model</span></span><span class="koboSpan" id="kobo.480.1">=</span><span class="hljs-string"><span class="koboSpan" id="kobo.481.1">"gemini-1.5-flash-002"</span></span><span class="koboSpan" id="kobo.482.1">)</span></p>
<div aria-label="77" epub:type="pagebreak" id="page11-2" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.483.1">job_description: str = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.484.1">..</span></span><span class="koboSpan" id="kobo.485.1">.  </span><span class="koboSpan" id="kobo.485.2"># put your JD here</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.486.1">prompt_template = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.487.1">"Given a job description, decide whether it suits a junior Java developer."</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.488.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.489.1">\nJOB DESCRIPTION:\n{job_description}\n"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.490.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.491.1">result = llm.invoke(prompt_template.format(</span><span class="hljs-attribute"><span class="koboSpan" id="kobo.492.1">job_description</span></span><span class="koboSpan" id="kobo.493.1">=job_description))</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.494.1">print</span></span><span class="koboSpan" id="kobo.495.1">(result.content)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.496.1">&gt;&gt; No, this job description is not suitable for a junior Java developer.\n\nThe key reasons are:\n\n* … (output reduced)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.497.1">As you </span><a id="_idIndexMarker208"/><span class="koboSpan" id="kobo.498.1">can see, the output of the LLM is free text, which </span><a id="_idIndexMarker209"/><span class="koboSpan" id="kobo.499.1">might be difficult to parse or interpret in subsequent pipeline steps. </span><span class="koboSpan" id="kobo.499.2">What if we add a specific instruction to a prompt?</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.500.1">prompt_template_enum = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.501.1">"Given a job description, decide whether it suits a junior Java developer."</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.502.1">"\nJOB DESCRIPTION:\n{job_description}\n\nAnswer only YES or NO."</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.503.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.504.1">result = llm</span><span class="hljs-selector-class"><span class="koboSpan" id="kobo.505.1">.invoke</span></span><span class="koboSpan" id="kobo.506.1">(prompt_template_enum</span><span class="hljs-selector-class"><span class="koboSpan" id="kobo.507.1">.format</span></span><span class="koboSpan" id="kobo.508.1">(job_description=job_description))</span></p>
<p class="snippet-code"><span class="hljs-title"><span class="koboSpan" id="kobo.509.1">print</span></span><span class="hljs-params"><span class="koboSpan" id="kobo.510.1">(result.content)</span></span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.511.1">&gt;&gt; NO</span></p>
<p class="normal"><span class="koboSpan" id="kobo.512.1">Now, how can we parse this output? </span><span class="koboSpan" id="kobo.512.2">Of course, our next step can be to just look at the text and have a condition based on a string comparison. </span><span class="koboSpan" id="kobo.512.3">But that won’t work for more complex use cases – for example, if the next step expects the output to be a JSON object. </span><span class="koboSpan" id="kobo.512.4">To deal with that, LangChain offers plenty of OutputParsers that take the output generated by the LLM and try to parse it into a desired format (by checking a schema if needed) – a list, CSV, enum, pandas DatafFrame, Pydantic model, JSON, XML, and so on. </span><span class="koboSpan" id="kobo.512.5">Each parser implements a </span><code class="inlineCode"><span class="koboSpan" id="kobo.513.1">BaseGenerationOutputParser</span></code><span class="koboSpan" id="kobo.514.1"> interface, which extends the </span><code class="inlineCode"><span class="koboSpan" id="kobo.515.1">Runnable</span></code><span class="koboSpan" id="kobo.516.1"> interface with an additional </span><code class="inlineCode"><span class="koboSpan" id="kobo.517.1">parse_result</span></code><span class="koboSpan" id="kobo.518.1"> method.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.519.1">Let’s build a parser that parses an output into an enum:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.520.1">from</span></span><span class="koboSpan" id="kobo.521.1"> enum </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.522.1">import</span></span><span class="koboSpan" id="kobo.523.1"> Enum</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.524.1">from</span></span><span class="koboSpan" id="kobo.525.1"> langchain.output_parsers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.526.1">import</span></span><span class="koboSpan" id="kobo.527.1"> EnumOutputParser</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.528.1">from</span></span><span class="koboSpan" id="kobo.529.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.530.1">import</span></span><span class="koboSpan" id="kobo.531.1"> HumanMessage</span></p>
<div aria-label="78" epub:type="pagebreak" id="page12-2" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.532.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.533.1">IsSuitableJobEnum</span></span><span class="koboSpan" id="kobo.534.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.535.1">Enum</span></span><span class="koboSpan" id="kobo.536.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.537.1">   YES = </span><span class="hljs-string"><span class="koboSpan" id="kobo.538.1">"YES"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.539.1">   NO = </span><span class="hljs-string"><span class="koboSpan" id="kobo.540.1">"NO"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.541.1">parser = EnumOutputParser(enum=IsSuitableJobEnum)</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.542.1">assert</span></span><span class="koboSpan" id="kobo.543.1"> parser.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.544.1">"NO"</span></span><span class="koboSpan" id="kobo.545.1">) == IsSuitableJobEnum.NO</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.546.1">assert</span></span><span class="koboSpan" id="kobo.547.1"> parser.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.548.1">"YES\n"</span></span><span class="koboSpan" id="kobo.549.1">) == IsSuitableJobEnum.YES</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.550.1">assert</span></span><span class="koboSpan" id="kobo.551.1"> parser.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.552.1">" YES \n"</span></span><span class="koboSpan" id="kobo.553.1">) == IsSuitableJobEnum.YES</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.554.1">assert</span></span><span class="koboSpan" id="kobo.555.1"> parser.invoke(HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.556.1">"YES"</span></span><span class="koboSpan" id="kobo.557.1">)) == IsSuitableJobEnum.YES</span></p>
<p class="normal"><span class="koboSpan" id="kobo.558.1">The </span><code class="inlineCode"><span class="koboSpan" id="kobo.559.1">EnumOutputParser</span></code><span class="koboSpan" id="kobo.560.1"> converts text output into a corresponding </span><code class="inlineCode"><span class="koboSpan" id="kobo.561.1">Enum</span></code><span class="koboSpan" id="kobo.562.1"> instance. </span><span class="koboSpan" id="kobo.562.2">Note that the parser </span><a id="_idIndexMarker210"/><span class="koboSpan" id="kobo.563.1">handles any generation-like output (not only strings), and it actually also strips the output.</span></p>
<div>
<div class="note" id="_idContainer041">
<p class="normal"><span class="koboSpan" id="kobo.564.1">You can find a full list of parsers in the documentation at </span><a href="https://python.langchain.com/docs/concepts/output_parsers/"><span class="url"><span class="koboSpan" id="kobo.565.1">https://python.langchain.com/docs/concepts/output_parsers/</span></span></a><span class="koboSpan" id="kobo.566.1">, and if you need your own parser, you can always build a new one!</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.567.1">As a </span><a id="_idIndexMarker211"/><span class="koboSpan" id="kobo.568.1">final step, let’s combine everything into a chain:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.569.1">chain = llm | parser</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.570.1">result = chain.invoke(prompt_template_enum.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.571.1">format</span></span><span class="koboSpan" id="kobo.572.1">(job_description=job_description))</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.573.1">print</span></span><span class="koboSpan" id="kobo.574.1">(result)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.575.1">&gt;&gt; NO</span></p>
<p class="normal"><span class="koboSpan" id="kobo.576.1">Now let’s make this chain part of our LangGraph workflow:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.577.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.578.1">JobApplicationState</span></span><span class="koboSpan" id="kobo.579.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.580.1">TypedDict</span></span><span class="koboSpan" id="kobo.581.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.582.1">   job_description: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.583.1">str</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.584.1">   is_suitable: IsSuitableJobEnum</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.585.1">   application: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.586.1">str</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.587.1">analyze_chain = llm | parser</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.588.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.589.1">analyze_job_description</span></span><span class="koboSpan" id="kobo.590.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.591.1">state</span></span><span class="koboSpan" id="kobo.592.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.593.1">   prompt = prompt_template_enum.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.594.1">format</span></span><span class="koboSpan" id="kobo.595.1">(job_description=state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.596.1">"job_description"</span></span><span class="koboSpan" id="kobo.597.1">])</span></p>
<div aria-label="79" epub:type="pagebreak" id="page13-2" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.598.1">   result = analyze_chain.invoke(prompt)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.599.1">return</span></span><span class="koboSpan" id="kobo.600.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.601.1">"is_suitable"</span></span><span class="koboSpan" id="kobo.602.1">: result}</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.603.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.604.1">is_suitable_condition</span></span><span class="koboSpan" id="kobo.605.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.606.1">state: StateGraph</span></span><span class="koboSpan" id="kobo.607.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.608.1">return</span></span><span class="koboSpan" id="kobo.609.1"> state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.610.1">"is_suitable"</span></span><span class="koboSpan" id="kobo.611.1">] == IsSuitableJobEnum.YES</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.612.1">builder = StateGraph(JobApplicationState)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.613.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.614.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.615.1">, analyze_job_description)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.616.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.617.1">"generate_application"</span></span><span class="koboSpan" id="kobo.618.1">, generate_application)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.619.1">builder.add_edge(START, </span><span class="hljs-string"><span class="koboSpan" id="kobo.620.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.621.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.622.1">builder.add_conditional_edges(</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.623.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.624.1">, is_suitable_condition,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.625.1">    {</span><span class="hljs-literal"><span class="koboSpan" id="kobo.626.1">True</span></span><span class="koboSpan" id="kobo.627.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.628.1">"generate_application"</span></span><span class="koboSpan" id="kobo.629.1">, </span><span class="hljs-literal"><span class="koboSpan" id="kobo.630.1">False</span></span><span class="koboSpan" id="kobo.631.1">: END})</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.632.1">builder.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.633.1">"generate_application"</span></span><span class="koboSpan" id="kobo.634.1">, END)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.635.1">We made </span><a id="_idIndexMarker212"/><span class="koboSpan" id="kobo.636.1">two important changes. </span><span class="koboSpan" id="kobo.636.2">First, our newly </span><a id="_idIndexMarker213"/><span class="koboSpan" id="kobo.637.1">built chain is now part of a Python function that represents the </span><code class="inlineCode"><span class="koboSpan" id="kobo.638.1">analyze_job_description</span></code><span class="koboSpan" id="kobo.639.1"> node, and that’s how we implement the logic within the node. </span><span class="koboSpan" id="kobo.639.2">Second, our conditional edge function doesn’t return a string anymore, but we added a mapping of returned values to destination edges to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.640.1">add_conditional_edges</span></code><span class="koboSpan" id="kobo.641.1"> function, and that’s an example of how you could implement a branching of your workflow.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.642.1">Let’s take some time to discuss how to handle pote</span><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.643.1">ntial errors if our parsing fails!</span></p>
<h3 class="heading-3" id="_idParaDest-71"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.644.1">Error handling</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.645.1">Effective error </span><a id="_idIndexMarker214"/><span class="koboSpan" id="kobo.646.1">management is essential in any LangChain workflow, including </span><a id="_idIndexMarker215"/><span class="koboSpan" id="kobo.647.1">when handling tool failures (which we’ll explore in </span><a href="E_Chapter_5.xhtml#_idTextAnchor231"><em class="italic"><span class="koboSpan" id="kobo.648.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.649.1"> when we get to tools). </span><span class="koboSpan" id="kobo.649.2">When developing LangChain applications, remember that failures can occur at any stage:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.650.1">API calls to foundation models may fail</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.651.1">LLMs might generate unexpected outputs</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.652.1">External services could become unavailable</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.653.1">One of the possible approaches would be to use a basic Python mechanism for catching exceptions, logging them for further analysis, and continuing your workflow either by wrapping an exception as a text or by returning a default value. </span><span class="koboSpan" id="kobo.653.2">If your LangChain chain calls some custom Python function, think about appropriate exception handling. </span><span class="koboSpan" id="kobo.653.3">The same goes for your LangGraph nodes.</span></p>
<div aria-label="80" epub:type="pagebreak" id="page14-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.654.1">Logging is essential, especially as you approach production deployment. </span><span class="koboSpan" id="kobo.654.2">Proper logging ensures that exceptions don’t go unnoticed, allowing you to monitor their occurrence. </span><span class="koboSpan" id="kobo.654.3">Modern observability tools provide alerting mechanisms that group similar errors and notify you about frequently occurring issues.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.655.1">Converting exceptions to text enables your workflow to continue execution while providing downstream LLMs with valuable context about what went wrong and potential recovery paths. </span><span class="koboSpan" id="kobo.655.2">Here is a simple example of how you can log the exception but continue executing your workflow by sticking to the default behavior:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.656.1">import</span></span><span class="koboSpan" id="kobo.657.1"> logging</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.658.1">logger = logging.getLogger(__name__)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.659.1">llms = {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.660.1">"fake"</span></span><span class="koboSpan" id="kobo.661.1">: fake_llm,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.662.1">"Google"</span></span><span class="koboSpan" id="kobo.663.1">: llm</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.664.1">}</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.665.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.666.1">analyze_job_description</span></span><span class="koboSpan" id="kobo.667.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.668.1">state, config: RunnableConfig</span></span><span class="koboSpan" id="kobo.669.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.670.1">try</span></span><span class="koboSpan" id="kobo.671.1">:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.672.1">     llm = config[</span><span class="hljs-string"><span class="koboSpan" id="kobo.673.1">"configurable"</span></span><span class="koboSpan" id="kobo.674.1">].get(</span><span class="hljs-string"><span class="koboSpan" id="kobo.675.1">"model_provider"</span></span><span class="koboSpan" id="kobo.676.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.677.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.678.1">Google"</span></span><span class="koboSpan" id="kobo.679.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.680.1">     llm = llms[model_provider]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.681.1">     analyze_chain = llm | parser</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.682.1">     prompt = prompt_template_enum.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.683.1">format</span></span><span class="koboSpan" id="kobo.684.1">(job_description=job_description)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.685.1">     result = analyze_chain.invoke(prompt)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.686.1">return</span></span><span class="koboSpan" id="kobo.687.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.688.1">"is_suitable"</span></span><span class="koboSpan" id="kobo.689.1">: result}</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.690.1">except</span></span><span class="koboSpan" id="kobo.691.1"> Exception </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.692.1">as</span></span><span class="koboSpan" id="kobo.693.1"> e:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.694.1">     logger.error(</span><span class="hljs-string"><span class="koboSpan" id="kobo.695.1">f"Exception </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.696.1">{e}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.697.1"> occurred while executing analyze_job_description"</span></span><span class="koboSpan" id="kobo.698.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.699.1">return</span></span><span class="koboSpan" id="kobo.700.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.701.1">"is_suitable"</span></span><span class="koboSpan" id="kobo.702.1">: </span><span class="hljs-literal"><span class="koboSpan" id="kobo.703.1">False</span></span><span class="koboSpan" id="kobo.704.1">}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.705.1">To test </span><a id="_idIndexMarker216"/><span class="koboSpan" id="kobo.706.1">our error handling, we need to simulate </span><a id="_idIndexMarker217"/><span class="koboSpan" id="kobo.707.1">LLM failures. </span><span class="koboSpan" id="kobo.707.2">LangChain has a few </span><code class="inlineCode"><span class="koboSpan" id="kobo.708.1">FakeChatModel</span></code><span class="koboSpan" id="kobo.709.1"> classes that help you to test your chain:</span></p>
<ul>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.710.1">GenericFakeChatModel</span></code><span class="koboSpan" id="kobo.711.1"> returns messages based on a provided iterator</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.712.1">FakeChatModel</span></code><span class="koboSpan" id="kobo.713.1"> always returns a </span><code class="inlineCode"><span class="koboSpan" id="kobo.714.1">"fake_response"</span></code><span class="koboSpan" id="kobo.715.1"> string</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.716.1">FakeListChatModel</span></code><span class="koboSpan" id="kobo.717.1"> takes a list of messages and returns them one by one on each invocation</span></li>
</ul>
<div aria-label="81" epub:type="pagebreak" id="page15-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.718.1">Let’s create a fake LLM that fails every second time:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.719.1">from</span></span><span class="koboSpan" id="kobo.720.1"> langchain_core.language_models </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.721.1">import</span></span><span class="koboSpan" id="kobo.722.1"> GenericFakeChatModel</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.723.1">from</span></span><span class="koboSpan" id="kobo.724.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.725.1">import</span></span><span class="koboSpan" id="kobo.726.1"> AIMessage</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.727.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.728.1">MessagesIterator</span></span><span class="koboSpan" id="kobo.729.1">:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.730.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.731.1">__init__</span></span><span class="koboSpan" id="kobo.732.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.733.1">self</span></span><span class="koboSpan" id="kobo.734.1">):</span></p>
<p class="snippet-code"> <span class="hljs-variable"><span class="koboSpan" id="kobo.735.1">self</span></span><span class="koboSpan" id="kobo.736.1">._count = </span><span class="hljs-number"><span class="koboSpan" id="kobo.737.1">0</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.738.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.739.1">__iter__</span></span><span class="koboSpan" id="kobo.740.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.741.1">self</span></span><span class="koboSpan" id="kobo.742.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.743.1">return</span></span> <span class="hljs-variable"><span class="koboSpan" id="kobo.744.1">self</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.745.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.746.1">__next__</span></span><span class="koboSpan" id="kobo.747.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.748.1">self</span></span><span class="koboSpan" id="kobo.749.1">):</span></p>
<p class="snippet-code"> <span class="hljs-variable"><span class="koboSpan" id="kobo.750.1">self</span></span><span class="koboSpan" id="kobo.751.1">._count += </span><span class="hljs-number"><span class="koboSpan" id="kobo.752.1">1</span></span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.753.1">if</span></span> <span class="hljs-variable"><span class="koboSpan" id="kobo.754.1">self</span></span><span class="koboSpan" id="kobo.755.1">._count % </span><span class="hljs-number"><span class="koboSpan" id="kobo.756.1">2</span></span><span class="koboSpan" id="kobo.757.1"> == </span><span class="hljs-number"><span class="koboSpan" id="kobo.758.1">1</span></span><span class="koboSpan" id="kobo.759.1">:</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.760.1">raise</span></span><span class="koboSpan" id="kobo.761.1"> ValueError(</span><span class="hljs-string"><span class="koboSpan" id="kobo.762.1">"Something went wrong"</span></span><span class="koboSpan" id="kobo.763.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.764.1">return</span></span><span class="koboSpan" id="kobo.765.1"> AIMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.766.1">"False"</span></span><span class="koboSpan" id="kobo.767.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.768.1">fake_llm = GenericFakeChatModel(messages=MessagesIterator())</span></p>
<p class="normal"><span class="koboSpan" id="kobo.769.1">When we </span><a id="_idIndexMarker218"/><span class="koboSpan" id="kobo.770.1">provide this to our graph (the full code sample is available in our GitHub repo), we can </span><a id="_idIndexMarker219"/><span class="koboSpan" id="kobo.771.1">see that the workflow continues despite encountering an exception:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.772.1">res = graph.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.773.1">"job_description"</span></span><span class="koboSpan" id="kobo.774.1">:</span><span class="hljs-string"><span class="koboSpan" id="kobo.775.1">"fake_jd"</span></span><span class="koboSpan" id="kobo.776.1">}, config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.777.1">"configurable"</span></span><span class="koboSpan" id="kobo.778.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.779.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.780.1">model_provider"</span></span><span class="koboSpan" id="kobo.781.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.782.1">"fake"</span></span><span class="koboSpan" id="kobo.783.1">}})</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.784.1">print</span></span><span class="koboSpan" id="kobo.785.1">(res)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.786.1">&gt;&gt; ERROR:__main__:Exception Expected a Runnable, callable or dict.Instead got an unsupported type: &lt;class 'str'&gt; occured while executing analyze_job_description</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.787.1">{'job_descri</span><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.788.1">ption': 'fake_jd', 'is_suitable': False}</span></p>
<p class="normal"><span class="koboSpan" id="kobo.789.1">When an error occurs, sometimes it helps to try again. </span><span class="koboSpan" id="kobo.789.2">LLMs have a non-deterministic nature, and the next attempt might be successful; also, if you’re using third-party APIs, various failures might happen on the provider’s side. </span><span class="koboSpan" id="kobo.789.3">Let’s discuss how to implement proper retries with LangGraph.</span></p>
<div aria-label="82" epub:type="pagebreak" id="page16-2" role="doc-pagebreak"/>
<h4 class="heading-4"><span class="koboSpan" id="kobo.790.1">Retries</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.791.1">There are </span><a id="_idIndexMarker220"/><span class="koboSpan" id="kobo.792.1">three distinct retry approaches, each suited to different scenarios:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.793.1">Generic retry with Runnable</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.794.1">Node-specific retry policies</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.795.1">Semantic output repair</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.796.1">Let’s look </span><a id="_idIndexMarker221"/><span class="koboSpan" id="kobo.797.1">at these in turn, starting with generic retries that are available for every </span><code class="inlineCode"><span class="koboSpan" id="kobo.798.1">Runnable.</span></code></p>
<p class="normal"><span class="koboSpan" id="kobo.799.1">You can retry any </span><code class="inlineCode"><span class="koboSpan" id="kobo.800.1">Runnable</span></code><em class="italic"> </em><span class="koboSpan" id="kobo.801.1">or LangGraph node using a built-in mechanism:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.802.1">fake_llm_retry = fake_llm.with_retry(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.803.1">   retry_if_exception_type=(ValueError,),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.804.1">   wait_exponential_jitter=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.805.1">True</span></span><span class="koboSpan" id="kobo.806.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.807.1">   stop_after_attempt=</span><span class="hljs-number"><span class="koboSpan" id="kobo.808.1">2</span></span><span class="koboSpan" id="kobo.809.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.810.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.811.1">analyze_chain_fake_retries = fake_llm_retry | parser</span></p>
<p class="normal"><span class="koboSpan" id="kobo.812.1">With LangGraph, you can also describe specific retries for every node. </span><span class="koboSpan" id="kobo.812.2">For example, let’s retry our </span><code class="inlineCode"><span class="koboSpan" id="kobo.813.1">analyze_job_description</span></code><span class="koboSpan" id="kobo.814.1"> node two times in case of a </span><code class="inlineCode"><span class="koboSpan" id="kobo.815.1">ValueError</span></code><span class="koboSpan" id="kobo.816.1">:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.817.1">from</span></span><span class="koboSpan" id="kobo.818.1"> langgraph.pregel </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.819.1">import</span></span><span class="koboSpan" id="kobo.820.1"> RetryPolicy</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.821.1">builder.add_node(</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.822.1">"analyze_job_description"</span></span><span class="koboSpan" id="kobo.823.1">, analyze_job_description,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.824.1">  retry=RetryPolicy(retry_on=ValueError, max_attempts=</span><span class="hljs-number"><span class="koboSpan" id="kobo.825.1">2</span></span><span class="koboSpan" id="kobo.826.1">))</span></p>
<p class="normal"><span class="koboSpan" id="kobo.827.1">The components you’re using, often known as building blocks, might have their own retry mechanism that tries to algorithmically fix the problem by giving an LLM additional input on what went wrong. </span><span class="koboSpan" id="kobo.827.2">For example, many chat models on LangChain have client-side retries on specific server-side errors.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.828.1">ChatAnthropic has a </span><code class="inlineCode"><span class="koboSpan" id="kobo.829.1">max_retries</span></code><span class="koboSpan" id="kobo.830.1"> parameter that you can define either per instance or per request. </span><span class="koboSpan" id="kobo.830.2">Another good </span><a id="_idIndexMarker222"/><span class="koboSpan" id="kobo.831.1">example of a more advanced building block is trying to recover from </span><a id="_idIndexMarker223"/><span class="koboSpan" id="kobo.832.1">a parsing error. </span><span class="koboSpan" id="kobo.832.2">Retrying a parsing step won’t help since typically parsing errors are related to the incomplete LLM output. </span><span class="koboSpan" id="kobo.832.3">What if we retry the generation step and hope for the best, or actually give LLM a hint about what went wrong? </span><span class="koboSpan" id="kobo.832.4">That’s exactly what a </span><code class="inlineCode"><span class="koboSpan" id="kobo.833.1">RetryWithErrorOutputParser</span></code><span class="koboSpan" id="kobo.834.1"> is doing.</span></p>
<div aria-label="83" epub:type="pagebreak" id="page17-2" role="doc-pagebreak"/>
<figure class="mediaobject"><span class="koboSpan" id="kobo.835.1"><img alt="Figure 3.3: Adding a retry mechanism to a chain that has multiple steps" src="../Images/B32363_03_03.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.836.1">Figure 3.3: Adding a retry mechanism to a chain that has multiple steps</span></p>
<p class="normal"><span class="koboSpan" id="kobo.837.1">In order to use </span><code class="inlineCode"><span class="koboSpan" id="kobo.838.1">RetryWithErrorOutputParser</span></code><span class="koboSpan" id="kobo.839.1">, we need to first initialize it with an LLM (used to fix the output) and our parser. </span><span class="koboSpan" id="kobo.839.2">Then, if our parsing fails, we run it and provide our initial prompt (with all substituted parameters), generated response, and parsing error:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.840.1">from</span></span><span class="koboSpan" id="kobo.841.1"> langchain.output_parsers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.842.1">import</span></span><span class="koboSpan" id="kobo.843.1"> RetryWithErrorOutputParser</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.844.1">fix_parser = RetryWithErrorOutputParser.from_llm(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.845.1">  llm=llm, </span><span class="hljs-comment"><span class="koboSpan" id="kobo.846.1"># provide llm here</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.847.1">  parser=parser, </span><span class="hljs-comment"><span class="koboSpan" id="kobo.848.1"># your original parser that failed</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.849.1">  prompt=retry_prompt, </span><span class="hljs-comment"><span class="koboSpan" id="kobo.850.1"># an optional parameter, you can redefine the default prompt </span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.851.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.852.1">fixed_output = fix_parser.parse_with_prompt(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.853.1">  completion=original_response, prompt_value=original_prompt)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.854.1">We can read </span><a id="_idIndexMarker224"/><span class="koboSpan" id="kobo.855.1">the source code on GitHub to better understand what’s going on, but in essence, that’s an example of a pseudo-code without too many details. </span><span class="koboSpan" id="kobo.855.2">We illustrate </span><a id="_idIndexMarker225"/><span class="koboSpan" id="kobo.856.1">how we can pass the parsing error and the original output that led to this error back to an LLM and ask it to fix the problem:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.857.1">prompt = </span><span class="hljs-string"><span class="koboSpan" id="kobo.858.1">"""</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.859.1">Prompt: {prompt} Completion: {completion} Above, the Completion did not satisfy the constraints given in the Prompt. </span><span class="koboSpan" id="kobo.859.2">Details: {error} Please try again:</span></span></p>
<p class="snippet-code"><span class="hljs-string"><span class="koboSpan" id="kobo.860.1">"""</span></span> </p>
<p class="snippet-code"><span class="koboSpan" id="kobo.861.1">retry_chain = prompt | llm | StrOutputParser()</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.862.1"># try to parse a completion with a provided parser</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.863.1">parser.parse(completion)</span></p>
<p class="snippet-code"><span class="hljs-comment"><span class="koboSpan" id="kobo.864.1"># if it fails, catch an error and try to recover max_retries attempts</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.865.1">completion = retry_chain.invoke(original_prompt, completion, error)</span></p>
<div aria-label="84" epub:type="pagebreak" id="page18-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.866.1">We introduced the </span><code class="inlineCode"><span class="koboSpan" id="kobo.867.1">StrOutputParser</span></code><span class="koboSpan" id="kobo.868.1"> in </span><a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic"><span class="koboSpan" id="kobo.869.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.870.1"> to convert the output of the ChatModel from an AIMessage to a string so that we can easily pass it to the next step in the chain.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.871.1">Another thing to keep in mind is that LangChain building blocks allow you to redefine parameters, including default prompts. </span><span class="koboSpan" id="kobo.871.2">You can always check them on Github; sometimes it’s a good idea </span><a id="_idIndexMarker226"/><span class="koboSpan" id="kobo.872.1">to customize default prompts for your workflows.</span></p>
<div>
<div class="note" id="_idContainer043">
<p class="normal"><span class="koboSpan" id="kobo.873.1">You can read about other available output-fixing parsers here: </span><a href="https://python.l﻿angchain.com/docs/how_to/output_parser_retry/"><span class="url"><span class="koboSpan" id="kobo.874.1">https://python.l</span><span id="_idTextAnchor121"/><span class="koboSpan" id="kobo.875.1">angchain.com/docs/how_to/output_parser_retry/</span></span></a><span class="koboSpan" id="kobo.876.1">.</span></p>
</div>
</div>
<h4 class="heading-4"><span class="koboSpan" id="kobo.877.1">Fallbacks</span></h4>
<p class="normal"><span class="koboSpan" id="kobo.878.1">In software development, a </span><strong class="keyWord"><span class="koboSpan" id="kobo.879.1">fallback</span></strong><span class="koboSpan" id="kobo.880.1"> is an alternative program that allows you to recover if your base </span><a id="_idIndexMarker227"/><span class="koboSpan" id="kobo.881.1">one fails. </span><span class="koboSpan" id="kobo.881.2">LangChain allows you to define fallbacks on a </span><code class="inlineCode"><span class="koboSpan" id="kobo.882.1">Runnable</span></code><span class="koboSpan" id="kobo.883.1"> level. </span><span class="koboSpan" id="kobo.883.2">If execution </span><a id="_idIndexMarker228"/><span class="koboSpan" id="kobo.884.1">fails, an alternative chain is triggered with the same input parameters. </span><span class="koboSpan" id="kobo.884.2">For example, if the LLM you’re using is not available for a short period of time, your chain will automatically switch to a different one that uses an alternative provider (and probably different prompts).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.885.1">Our fake model fails every second time, so let’s add a fallback to it. </span><span class="koboSpan" id="kobo.885.2">It’s just a lambda that prints a statement. </span><span class="koboSpan" id="kobo.885.3">As we can see, every second time, the fallback is executed:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.886.1">from</span></span><span class="koboSpan" id="kobo.887.1"> langchain_core.runnables </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.888.1">import</span></span><span class="koboSpan" id="kobo.889.1"> RunnableLambda</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.890.1">chain_fallback = RunnableLambda(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.891.1">lambda</span></span><span class="koboSpan" id="kobo.892.1"> _: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.893.1">print</span></span><span class="koboSpan" id="kobo.894.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.895.1">"running fallback"</span></span><span class="koboSpan" id="kobo.896.1">))</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.897.1">chain = fake_llm | RunnableLambda(</span><span class="hljs-keyword"><span class="koboSpan" id="kobo.898.1">lambda</span></span><span class="koboSpan" id="kobo.899.1"> _: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.900.1">print</span></span><span class="koboSpan" id="kobo.901.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.902.1">"running main chain"</span></span><span class="koboSpan" id="kobo.903.1">))</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.904.1">chain_with_fb = chain.with_fallbacks([chain_fallback])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.905.1">chain_with_fb.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.906.1">"test"</span></span><span class="koboSpan" id="kobo.907.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.908.1">chain_with_fb.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.909.1">"test"</span></span><span class="koboSpan" id="kobo.910.1">)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.911.1">&gt;&gt; running fallback</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.912.1">running main chain</span></p>
<p class="normal"><span class="koboSpan" id="kobo.913.1">Generating complex </span><a id="_idIndexMarker229"/><span class="koboSpan" id="kobo.914.1">outcomes that can follow a certain template and can be parsed reliably </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.915.1">is called structured generation (or controlled generation). </span><span class="koboSpan" id="kobo.915.2">This can </span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.916.1">help to build more complex workflows, where an output of one LLM-driven step can be consumed by another programmatic step. </span><span class="koboSpan" id="kobo.916.2">We’ll pick this up again in more detail in </span><em class="italic"><span class="koboSpan" id="kobo.917.1">Chapters 5</span></em><span class="koboSpan" id="kobo.918.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.919.1">6</span></em><span class="koboSpan" id="kobo.920.1">.</span></p>
<div aria-label="85" epub:type="pagebreak" id="page19-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.921.1">Prompts that you send to an LLM are one of the most important building blocks of your workflows. </span><span class="koboSpan" id="kobo.921.2">Hence, let’s discuss some basics of prompt engineering next a</span><a id="_idTextAnchor122"/><span class="koboSpan" id="kobo.922.1">nd see how to organize your prompts with LangChain.</span></p>
<h1 class="heading-1" id="_idParaDest-72"><a id="_idTextAnchor123"/><span class="koboSpan" id="kobo.923.1">Prompt engineering</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.924.1">Let’s continue by looking into prompt engineering and exploring various LangChain syntaxes related to it. </span><span class="koboSpan" id="kobo.924.2">But first, let’s discuss how prompt engineering is different from prompt design. </span><span class="koboSpan" id="kobo.924.3">These terms </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.925.1">are sometimes used interchangeably, and it creates a certain level of confusion. </span><span class="koboSpan" id="kobo.925.2">As we discussed in </span><a href="E_Chapter_1.xhtml#_idTextAnchor001"><em class="italic"><span class="koboSpan" id="kobo.926.1">Chapter 1</span></em></a><span class="koboSpan" id="kobo.927.1">, one of the big discoveries about LLMs was that they have the capability of domain adaptation by </span><em class="italic"><span class="koboSpan" id="kobo.928.1">in-context learning</span></em><span class="koboSpan" id="kobo.929.1">. </span><span class="koboSpan" id="kobo.929.2">It’s often enough to describe the task we’d like it to perform in a natural language, and even though the LLM wasn’t trained on this specific task, it performs extremely well. </span><span class="koboSpan" id="kobo.929.3">But as we can imagine, there are multiple ways of describing the same task, and LLMs are sensitive to this. </span><span class="koboSpan" id="kobo.929.4">Improving our prompt (or prompt template, to be specific) to increase performance on a specific task is called prompt engineering. </span><span class="koboSpan" id="kobo.929.5">However, developing more universal prompts that guide LLMs to generate generally better responses on a broad set of tasks is called prompt design.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.930.1">There exists a large variety of different prompt engineering techniques. </span><span class="koboSpan" id="kobo.930.2">We won’t discuss many of them in detail in this section, but we’ll touch on just a few of them to illustrate key LangChain capabilities that would allow you to construct any prompts you want.</span></p>
<div>
<div class="note" id="_idContainer044">
<p class="normal"><span class="koboSpan" id="kobo.931.1">You can find a good overview of prompt taxonomy in the paper </span><em class="italic"><span class="koboSpan" id="kobo.932.1">The Prompt Report: A Systematic Survey of Prompt Engineering Techniques</span></em><span class="koboSpan" id="kobo.933.1">, published by Sander Schulho</span><a id="_idTextAnchor124"/><span class="koboSpan" id="kobo.934.1">ff and colleagues: </span><a href="https://arxiv.org/abs/2406.06608"><span class="url"><span class="koboSpan" id="kobo.935.1">https://arxiv.org/abs/2406.06608</span></span></a><span class="koboSpan" id="kobo.936.1">.</span></p>
</div>
</div>
<h2 class="heading-2" id="_idParaDest-73"><a id="_idTextAnchor125"/><span class="koboSpan" id="kobo.937.1">Prompt templates</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.938.1">What we did in </span><a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic"><span class="koboSpan" id="kobo.939.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.940.1"> is </span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.941.1">called </span><em class="italic"><span class="koboSpan" id="kobo.942.1">zero-shot prompting</span></em><span class="koboSpan" id="kobo.943.1">. </span><span class="koboSpan" id="kobo.943.2">We created a </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.944.1">prompt template that contained a description of each task. </span><span class="koboSpan" id="kobo.944.2">When we run the workflow, we substitute certain values of this prompt </span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.945.1">template with runtime arguments. </span><span class="koboSpan" id="kobo.945.2">LangChain has some very useful abstractions to help with that.</span></p>
<div aria-label="86" epub:type="pagebreak" id="page20-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.946.1">In </span><a href="E_Chapter_2.xhtml#_idTextAnchor044"><em class="italic"><span class="koboSpan" id="kobo.947.1">Chapter 2</span></em></a><span class="koboSpan" id="kobo.948.1">, we introduced </span><code class="inlineCode"><span class="koboSpan" id="kobo.949.1">PromptTemplate</span></code><span class="koboSpan" id="kobo.950.1">, which is a </span><code class="inlineCode"><span class="koboSpan" id="kobo.951.1">RunnableSerializable</span></code><span class="koboSpan" id="kobo.952.1">. </span><span class="koboSpan" id="kobo.952.2">Remember that it substitutes a string template during invocation – for example, you can create a template based on f-string and add your chain, and LangChain would pass parameters from the input, substitute them in the template, and pass the string to the next step in the chain:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.953.1">from</span></span><span class="koboSpan" id="kobo.954.1"> langchain_core.output_parsers </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.955.1">import</span></span><span class="koboSpan" id="kobo.956.1"> StrOutputParser</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.957.1">lc_prompt_template = PromptTemplate.from_template(prompt_template)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.958.1">chain = lc_prompt_template | llm | StrOutputParser()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.959.1">chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.960.1">"job_description"</span></span><span class="koboSpan" id="kobo.961.1">: job_description})</span></p>
<p class="normal"><span class="koboSpan" id="kobo.962.1">For chat models, an input can not only be a string but also a list of </span><code class="inlineCode"><span class="koboSpan" id="kobo.963.1">messages</span></code><span class="koboSpan" id="kobo.964.1"> – for example, a system message followed by a history of the conversation. </span><span class="koboSpan" id="kobo.964.2">Therefore, we can also create a template that prepares a list of messages, and a template itself can be created based on a list of messages or message templates, as in this example:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.965.1">from</span></span><span class="koboSpan" id="kobo.966.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.967.1">import</span></span><span class="koboSpan" id="kobo.968.1"> ChatPromptTemplate, HumanMessagePromptTemplate</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.969.1">from</span></span><span class="koboSpan" id="kobo.970.1"> langchain_core.messages </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.971.1">import</span></span><span class="koboSpan" id="kobo.972.1"> SystemMessage, HumanMessage</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.973.1">msg_template = HumanMessagePromptTemplate.from_template(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.974.1">  prompt_template)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.975.1">msg_example = msg_template.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.976.1">format</span></span><span class="koboSpan" id="kobo.977.1">(job_description=</span><span class="hljs-string"><span class="koboSpan" id="kobo.978.1">"fake_jd"</span></span><span class="koboSpan" id="kobo.979.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.980.1">chat_prompt_template = ChatPromptTemplate.from_messages([</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.981.1">  SystemMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.982.1">"You are a helpful assistant."</span></span><span class="koboSpan" id="kobo.983.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.984.1">  msg_template])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.985.1">chain = chat_prompt_template | llm | StrOutputParser()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.986.1">chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.987.1">"job_description"</span></span><span class="koboSpan" id="kobo.988.1">: job_description})</span></p>
<p class="normal"><span class="koboSpan" id="kobo.989.1">You can also do the same more conveniently without using chat prompt templates but by submitting a tuple (just because it’s faster and more convenient sometimes) with a type of message and a templated string instead:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.990.1">chat_prompt_template = ChatPromptTemplate.from_messages(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.991.1">   [(</span><span class="hljs-string"><span class="koboSpan" id="kobo.992.1">"system"</span></span><span class="koboSpan" id="kobo.993.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.994.1">"You are a helpful assistant."</span></span><span class="koboSpan" id="kobo.995.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.996.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.997.1">"human"</span></span><span class="koboSpan" id="kobo.998.1">, prompt_template)])</span></p>
<div aria-label="87" epub:type="pagebreak" id="page21-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.999.1">Another </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.1000.1">important concept is a </span><em class="italic"><span class="koboSpan" id="kobo.1001.1">placeholder</span></em><span class="koboSpan" id="kobo.1002.1">. </span><span class="koboSpan" id="kobo.1002.2">This substitutes a variable with </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.1003.1">a list of messages provided in real time. </span><span class="koboSpan" id="kobo.1003.2">You can add a placeholder to your prompt by using a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1004.1">placeholder</span></code><span class="koboSpan" id="kobo.1005.1"> hint, or adding a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1006.1">MessagesPlaceholder</span></code><span class="koboSpan" id="kobo.1007.1">:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1008.1">from</span></span><span class="koboSpan" id="kobo.1009.1"> langchain_core.prompts </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1010.1">import</span></span><span class="koboSpan" id="kobo.1011.1"> ChatPromptTemplate, MessagesPlaceholder</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1012.1">chat_prompt_template = ChatPromptTemplate.from_messages(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1013.1">   [(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1014.1">"system"</span></span><span class="koboSpan" id="kobo.1015.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1016.1">"You are a helpful assistant."</span></span><span class="koboSpan" id="kobo.1017.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1018.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1019.1">"placeholder"</span></span><span class="koboSpan" id="kobo.1020.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1021.1">"{history}"</span></span><span class="koboSpan" id="kobo.1022.1">),</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.1023.1"># same as MessagesPlaceholder("history"),</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1024.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1025.1">"human"</span></span><span class="koboSpan" id="kobo.1026.1">, prompt_template)])</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1027.1">len</span></span><span class="koboSpan" id="kobo.1028.1">(chat_prompt_template.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1029.1">"job_description"</span></span><span class="koboSpan" id="kobo.1030.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1031.1">"fake"</span></span><span class="koboSpan" id="kobo.1032.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1033.1">"history"</span></span><span class="koboSpan" id="kobo.1034.1">: [(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1035.1">"human"</span></span><span class="koboSpan" id="kobo.1036.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1037.1">"hi!"</span></span><span class="koboSpan" id="kobo.1038.1">), (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1039.1">"ai"</span></span><span class="koboSpan" id="kobo.1040.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1041.1">"hi!"</span></span><span class="koboSpan" id="kobo.1042.1">)]}).messages)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1043.1">&gt;&gt; 4</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1044.1">Now our input consists of four messages – a system message, two history messages that we provided, and one human message from a templated prompt. </span><span class="koboSpan" id="kobo.1044.2">The best example of using a placeholder is to input a history of a chat, but we’ll see more advanced ones later in this book when we’ll talk about how an LLM interacts with an external world or how d</span><a id="_idTextAnchor126"/><span class="koboSpan" id="kobo.1045.1">ifferent LLMs coordinate together in a multi-agent setup.</span></p>
<h2 class="heading-2" id="_idParaDest-74"><a id="_idTextAnchor127"/><span class="koboSpan" id="kobo.1046.1">Zero-shot vs. </span><span class="koboSpan" id="kobo.1046.2">few-shot prompting</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1047.1">As we have discussed, the first thing that we want to experiment with is improving the task description </span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.1048.1">itself. </span><span class="koboSpan" id="kobo.1048.2">A description of a task without </span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.1049.1">examples of solutions is called </span><strong class="keyWord"><span class="koboSpan" id="kobo.1050.1">zero-shot</span></strong><span class="koboSpan" id="kobo.1051.1"> prompting, and </span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.1052.1">there are multiple tricks that you can try.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1053.1">What </span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.1054.1">typically works well is assigning the LLM </span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.1055.1">a certain role (for example, “</span><em class="italic"><span class="koboSpan" id="kobo.1056.1">You are a useful enterprise assistant working for XXX Fortune-500 company</span></em><span class="koboSpan" id="kobo.1057.1">”) and giving some additional instruction (for example, whether the LLM should be creative, concise, or factual). </span><span class="koboSpan" id="kobo.1057.2">Remember that LLMs have seen various data and they can do different tasks, from writing a fantasy book to answering complex reasoning questions. </span><span class="koboSpan" id="kobo.1057.3">But your goal is to instruct them, and if you want them to stick to the facts, you’d better give very specific instructions as part of their role profile. </span><span class="koboSpan" id="kobo.1057.4">For chat models, such role setting typically happens through a system message (but remember that, even for a chat model, everything is combined to a single input prompt formatted on the server side).</span></p>
<div aria-label="88" epub:type="pagebreak" id="page22-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1058.1">The Gemini prompting guide recommends that each prompt should have four parts: a persona, a task, a relevant context, and a desired format. </span><span class="koboSpan" id="kobo.1058.2">Keep in mind that different model providers might have different recommendations on prompt writing or formatting, hence if you have </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.1059.1">complex prompts, always check the </span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.1060.1">documentation of the model provider, evaluate the performance of your workflows before switching to a new model provider, and adjust prompts accordingly if needed. </span><span class="koboSpan" id="kobo.1060.2">If you want to use multiple model providers in production, you might end up with multiple prompt templates and select them dynamically based on the model provider.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1061.1">Another big improvement can be to provide an LLM with a few examples of this specific task as input-output pairs as part of the prompt. </span><span class="koboSpan" id="kobo.1061.2">This is called few-shot prompting. </span><span class="koboSpan" id="kobo.1061.3">Typically, few-shot prompting is difficult to use in scenarios that require a long input (such as RAG, which we’ll talk about in the next chapter) but it’s still very useful for tasks with relatively short prompts, such as classification, extraction, etc.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1062.1">Of course, you can always hard-code examples in the prompt template itself, but this makes it difficult to manage them as your system grows. </span><span class="koboSpan" id="kobo.1062.2">A better way might be to store examples in a separa</span><a id="_idTextAnchor128"/><span class="koboSpan" id="kobo.1063.1">te file on disk or in a database and load them into your prompt.</span></p>
<h3 class="heading-3" id="_idParaDest-75"><a id="_idTextAnchor129"/><span class="koboSpan" id="kobo.1064.1">Chaining prompts together</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1065.1">As your prompts become more advanced, they tend to grow in size and complexity. </span><span class="koboSpan" id="kobo.1065.2">One common </span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.1066.1">scenario is to partially format your prompts, and you can do this either by string or function substitution. </span><span class="koboSpan" id="kobo.1066.2">The latter is relevant if some parts of your prompt depend on dynamically changing variables (for example, current date, user name, etc.). </span><span class="koboSpan" id="kobo.1066.3">Below, you can find an example of a partial substitution in a prompt template:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1067.1">system_template = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1068.1">"a: {a} b: {b}"</span></span><span class="koboSpan" id="kobo.1069.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1070.1">system_template_part = system_template.partial(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1071.1">   a=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1072.1">"a"</span></span> <span class="hljs-comment"><span class="koboSpan" id="kobo.1073.1"># you also can provide a function here</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1074.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1075.1">print</span></span><span class="koboSpan" id="kobo.1076.1">(system_template_part.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1077.1">"b"</span></span><span class="koboSpan" id="kobo.1078.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1079.1">"b"</span></span><span class="koboSpan" id="kobo.1080.1">}).text)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1081.1">&gt;&gt; a: a b: b</span></p>
<div aria-label="89" epub:type="pagebreak" id="page23-2" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1082.1">Another way to make your prompts more manageable is to split them into pieces and chain them together:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1083.1">system_template_part1 = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1084.1">"a: {a}"</span></span><span class="koboSpan" id="kobo.1085.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1086.1">system_template_part2 = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1087.1">"b: {b}"</span></span><span class="koboSpan" id="kobo.1088.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1089.1">system_template = system_template_part1 + system_template_part2</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1090.1">print</span></span><span class="koboSpan" id="kobo.1091.1">(system_template_part.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1092.1">"a"</span></span><span class="koboSpan" id="kobo.1093.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1094.1">"a"</span></span><span class="koboSpan" id="kobo.1095.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1096.1">"b"</span></span><span class="koboSpan" id="kobo.1097.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1098.1">"b"</span></span><span class="koboSpan" id="kobo.1099.1">}).text)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1100.1">&gt;&gt; a: a b: b</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1101.1">You can </span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.1102.1">also build more complex substitutions by using the class </span><code class="inlineCode"><span class="koboSpan" id="kobo.1103.1">langchain_core.prompts.PipelinePromptTemplate</span></code><span class="koboSpan" id="kobo.1104.1">. </span><span class="koboSpan" id="kobo.1104.2">Additionally, you can pass templates into a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1105.1">ChatPromptTemplate</span></code><span class="koboSpan" id="kobo.1106.1"> and they will automatically be composed together:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1107.1">system_prompt_template = PromptTemplate.from_template(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1108.1">"a: {a} b: {b}"</span></span><span class="koboSpan" id="kobo.1109.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1110.1">chat_prompt_template = ChatPromptTemplate.from_messages(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1111.1">   [(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1112.1">"system"</span></span><span class="koboSpan" id="kobo.1113.1">, system_prompt_template.template),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1114.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1115.1">"human"</span></span><span class="koboSpan" id="kobo.1116.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1117.1">"hi"</span></span><span class="koboSpan" id="kobo.1118.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1119.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1120.1">"ai"</span></span><span class="koboSpan" id="kobo.1121.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1122.1">"{c}"</span></span><span class="koboSpan" id="kobo.1123.1">)])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1124.1">messages = chat_prompt_template.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1125.1">"a"</span></span><span class="koboSpan" id="kobo.1126.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1127.1">"a"</span></span><span class="koboSpan" id="kobo.1128.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1129.1">"b"</span></span><span class="koboSpan" id="kobo.1130.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1131.1">"b"</span></span><span class="koboSpan" id="kobo.1132.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1133.1">"c"</span></span><span class="koboSpan" id="kobo.1134.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1135.1">"c"</span></span><span class="koboSpan" id="kobo.1136.1">}).messa</span><a id="_idTextAnchor130"/><span class="koboSpan" id="kobo.1137.1">ges</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1138.1">print</span></span><span class="koboSpan" id="kobo.1139.1">(</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1140.1">len</span></span><span class="koboSpan" id="kobo.1141.1">(messages))</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1142.1">print</span></span><span class="koboSpan" id="kobo.1143.1">(messages[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1144.1">0</span></span><span class="koboSpan" id="kobo.1145.1">].content)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1146.1">&gt;&gt; 3</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1147.1">a: a b: b</span></p>
<h3 class="heading-3" id="_idParaDest-76"><a id="_idTextAnchor131"/><span class="koboSpan" id="kobo.1148.1">Dynamic few-shot prompting</span></h3>
<p class="normal"><span class="koboSpan" id="kobo.1149.1">As the number of examples used in your few-shot prompts continues to grow, you might limit the number </span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.1150.1">of examples to be passed into a specific prompt’s template substitution. </span><span class="koboSpan" id="kobo.1150.2">We select examples for every input – by searching for examples similar to the user’s input (we’ll talk more about semantic similarity and embeddings in </span><a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic"><span class="koboSpan" id="kobo.1151.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.1152.1">), limiting them by length, taking the freshest ones, etc.</span></p>
<div aria-label="90" epub:type="pagebreak" id="page24-2" role="doc-pagebreak"/>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1153.1"><img alt="Figure 3.4: An example of a workflow with a dynamic retrieval of examples to be passed to a few-shot prompt" src="../Images/B32363_03_04.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1154.1">Figure 3.4: An example of a workflow with a dynamic retrieval of examples to be passed to a few-shot prompt</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1155.1">There are </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.1156.1">a few already built-in selectors under </span><code class="inlineCode"><span class="koboSpan" id="kobo.1157.1">langchain_core.example_selectors</span></code><span class="koboSpan" id="kobo.1158.1">. </span><span class="koboSpan" id="kobo.1158.2">You can directly pass an instance of an example s</span><a id="_idTextAnchor132"/><span class="koboSpan" id="kobo.1159.1">elector to the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1160.1">FewShotPromptTemplate</span></code><span class="koboSpan" id="kobo.1161.1"> instance during instantiation.</span></p>
<h2 class="heading-2" id="_idParaDest-77"><a id="_idTextAnchor133"/><span class="koboSpan" id="kobo.1162.1">Chain of Thought</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1163.1">The Google Research team introduced the </span><strong class="keyWord"><span class="koboSpan" id="kobo.1164.1">Chain-of-Thought</span></strong><span class="koboSpan" id="kobo.1165.1"> (</span><strong class="keyWord"><span class="koboSpan" id="kobo.1166.1">CoT</span></strong><span class="koboSpan" id="kobo.1167.1">) technique early in 2022. </span><span class="koboSpan" id="kobo.1167.2">They demonstrated </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.1168.1">that a relatively simple modification to a prompt </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.1169.1">that encouraged a model to generate intermediate step-by-step reasoning steps significantly increased the LLM’s performance on complex symbolic reasoning, common sense, and math tasks. </span><span class="koboSpan" id="kobo.1169.2">Such an increase in performance has been replicated multiple times since then.</span></p>
<div>
<div class="note" id="_idContainer046">
<p class="normal"><span class="koboSpan" id="kobo.1170.1">You can read the original paper introducing CoT, </span><em class="italic"><span class="koboSpan" id="kobo.1171.1">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</span></em><span class="koboSpan" id="kobo.1172.1">, published by Jason Wei and colleagues: </span><a href="https://arxiv.org/abs/2201.11903"><span class="url"><span class="koboSpan" id="kobo.1173.1">https://arxiv.org/abs/2201.11903</span></span></a><span class="koboSpan" id="kobo.1174.1">.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.1175.1">There are different modifications of CoT prompting, and because it has long outputs, typically, CoT prompts are zero-shot. </span><span class="koboSpan" id="kobo.1175.2">You add instructions that encourage an LLM to think about the problem first instead of immediately generating tokens representing the answer. </span><span class="koboSpan" id="kobo.1175.3">A very simple example of CoT is just to add to your prompt template something like “Let’s think step by step.”</span></p>
<div aria-label="91" epub:type="pagebreak" id="page25-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1176.1">There are various CoT prompts reported in different papers. </span><span class="koboSpan" id="kobo.1176.2">You can also explore the CoT template available on LangSmith. </span><span class="koboSpan" id="kobo.1176.3">For our learning purposes, let’s use a CoT prompt with few-shot examples:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1177.1">from</span></span><span class="koboSpan" id="kobo.1178.1"> langchain </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1179.1">import</span></span><span class="koboSpan" id="kobo.1180.1"> hub</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1181.1">math_cot_prompt = hub.pull(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1182.1">"arietem/math_cot"</span></span><span class="koboSpan" id="kobo.1183.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1184.1">cot_chain = math_cot_prompt | llm | StrOutputParser()</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1185.1">print</span></span><span class="koboSpan" id="kobo.1186.1">(cot_chain.invoke(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1187.1">"Solve equation 2*x+5=15"</span></span><span class="koboSpan" id="kobo.1188.1">))</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1189.1">&gt;&gt; Answer: Let's think step by step</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1190.1">Subtract 5 from both sides:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1191.1">2x + 5 - 5 = 15 - 5</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1192.1">2x = 10</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1193.1">Divide both sides by 2:</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1194.1">2x / 2 = 10 / 2</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1195.1">x = 5</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1196.1">We used a </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.1197.1">prompt from LangSmith Hub – a collection </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.1198.1">of private and public artifacts that you can use with LangChain. </span><span class="koboSpan" id="kobo.1198.2">You can explore the prompt itself here: </span><a href="https://smith.langchain.com/hub. "><span class="url"><span class="koboSpan" id="kobo.1199.1">https://smith.langchain.com/hub.</span></span></a></p>
<p class="normal"><span class="koboSpan" id="kobo.1200.1">In practice, you might want to wrap a CoT invocation with an extraction step to provide a concise answer to the user. </span><span class="koboSpan" id="kobo.1200.2">For example, let us first run a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1201.1">cot_chain</span></code><span class="koboSpan" id="kobo.1202.1"> and then pass its output (please note that we pass a dictionary with an initial </span><code class="inlineCode"><span class="koboSpan" id="kobo.1203.1">question</span></code><span class="koboSpan" id="kobo.1204.1"> and a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1205.1">cot_output</span></code><span class="koboSpan" id="kobo.1206.1"> to the next step) to an LLM that will use a prompt to create a final answer based on CoT reasoning:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1207.1">from</span></span><span class="koboSpan" id="kobo.1208.1"> operator </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1209.1">import</span></span><span class="koboSpan" id="kobo.1210.1"> itemgetter</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1211.1">parse_prompt_template = (</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1212.1">"Given the initial question and a full answer, "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1213.1">"extract the concise answer. </span><span class="koboSpan" id="kobo.1213.2">Do not assume anything and "</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1214.1">"only use a provided full answer.\n\nQUESTION:\n{question}\n"</span></span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1215.1">"FULL ANSWER:\n{full_answer}\n\nCONCISE ANSWER:\n"</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1216.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1217.1">parse_prompt = PromptTemplate.from_template(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1218.1">   parse_prompt_template</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1219.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1220.1">final_chain = (</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1221.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1222.1">"full_answer"</span></span><span class="koboSpan" id="kobo.1223.1">: itemgetter(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1224.1">"question"</span></span><span class="koboSpan" id="kobo.1225.1">) | cot_chain,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1226.1">"question"</span></span><span class="koboSpan" id="kobo.1227.1">: itemgetter(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1228.1">"question"</span></span><span class="koboSpan" id="kobo.1229.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1230.1"> }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1231.1"> | parse_prompt</span></p>
<div aria-label="92" epub:type="pagebreak" id="page26-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.1232.1"> | llm</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1233.1"> | StrOutputParser()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1234.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1235.1">print</span></span><span class="koboSpan" id="kobo.1236.1">(final_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1237.1">"question"</span></span><span class="koboSpan" id="kobo.1238.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1239.1">"Solve equation 2*x+5=15"</span></span><span class="koboSpan" id="kobo.1240.1">}))</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1241.1">&gt;&gt; 5</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1242.1">Although a CoT prompt seems to be relatively simple, it’s extremely powerful since, as we’ve mentioned, it has been demonstrated multiple times that it significantly increases performance in many cases. </span><span class="koboSpan" id="kobo.1242.2">We will see its evolution and expansion when we discuss agents in </span><em class="italic"><span class="koboSpan" id="kobo.1243.1">Chapters 5</span></em><span class="koboSpan" id="kobo.1244.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1245.1">6</span></em><span class="koboSpan" id="kobo.1246.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1247.1">These days, we can observe how the CoT pattern gets more and more application with so-called reasoning models </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.1248.1">such as o3-mini or gemini-flash-thinking. </span><span class="koboSpan" id="kobo.1248.2">To a certain extent, these models do exactly the same (but often in a more advanced manner) – they think </span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.1249.1">before they answer, and this is achieved not only by changing the </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.1250.1">prompt but also by preparing training data (sometimes synthetic) that follows a CoT format.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1251.1">Please note that alternatively to using reasoning models, we can use CoT modification with additional instructions by asking an LLM to first generate output tokens that represent a reasoning process:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1252.1">template = ChatPromptTemplate.from_messages([</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1253.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1254.1">"system"</span></span><span class="koboSpan" id="kobo.1255.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1256.1">"""You are a problem-solving assistant that shows its reasoning process. </span><span class="koboSpan" id="kobo.1256.2">First, walk through your thought process step by step, labeling this section as 'THINKING:'. </span><span class="koboSpan" id="kobo.1256.3">After completing your analysis, provide your final answer labeled as 'ANSWER:'."""</span></span><span class="koboSpan" id="kobo.1257.1">),</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1258.1">    (</span><span class="hljs-string"><span class="koboSpan" id="kobo.1259.1">"user"</span></span><span class="koboSpan" id="kobo.1260.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1261.1">"{problem}"</span></span><span class="koboSpan" id="kobo.1262.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1263.1">])</span></p>
<h2 class="heading-2" id="_idParaDest-78"><a id="_idTextAnchor134"/><span class="koboSpan" id="kobo.1264.1">Self-consistency</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1265.1">The idea </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.1266.1">behind self-consistency is simple: let’s increase an </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.1267.1">LLM’s temperature, sample the answer multiple times, and then take the most frequent answer from the distribution. </span><span class="koboSpan" id="kobo.1267.2">This has been demonstrated to improve the performance of LLM-based workflows on certain tasks, and it works especially well on tasks such as classification or entity extraction, where the output’s dimensionality is low.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1268.1">Let’s use a chain from a previous example and try a quadratic equation. </span><span class="koboSpan" id="kobo.1268.2">Even with CoT prompting, the first attempt might give us a wrong answer, but if we sample from a distribution, we will be more likely to get the right one:</span></p>
<div aria-label="93" epub:type="pagebreak" id="page27-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.1269.1">generations = []</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1270.1">for</span></span><span class="koboSpan" id="kobo.1271.1"> _ </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1272.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1273.1">range</span></span><span class="koboSpan" id="kobo.1274.1">(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1275.1">20</span></span><span class="koboSpan" id="kobo.1276.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1277.1"> generations.append(final_chain.invoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1278.1">"question"</span></span><span class="koboSpan" id="kobo.1279.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1280.1">"Solve equation 2*x**2-96*x+1152"</span></span><span class="koboSpan" id="kobo.1281.1">}, temperature=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1282.1">2.0</span></span><span class="koboSpan" id="kobo.1283.1">).strip())</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1284.1">from</span></span><span class="koboSpan" id="kobo.1285.1"> collections </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1286.1">import</span></span><span class="koboSpan" id="kobo.1287.1"> Counter</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1288.1">print</span></span><span class="koboSpan" id="kobo.1289.1">(Counter(generations).most_common(</span><span class="hljs-number"><span class="koboSpan" id="kobo.1290.1">1</span></span><span class="koboSpan" id="kobo.1291.1">)[</span><span class="hljs-number"><span class="koboSpan" id="kobo.1292.1">0</span></span><span class="koboSpan" id="kobo.1293.1">][</span><span class="hljs-number"><span class="koboSpan" id="kobo.1294.1">0</span></span><span class="koboSpan" id="kobo.1295.1">])</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1296.1">&gt;&gt; x = 24</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1297.1">As you can see, we first created a list containing multiple outputs generated by an LLM for the same input </span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.1298.1">and then created a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1299.1">Counter</span></code><span class="koboSpan" id="kobo.1300.1"> class that allowed us to easily find the </span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.1301.1">most common element in this list, and we took it as a final answer.</span></p>
<div>
<div class="note" id="_idContainer047">
<p class="normal"><strong class="keyWord"><span class="koboSpan" id="kobo.1302.1">Switching between model providers</span></strong></p>
<p class="normal"><span class="koboSpan" id="kobo.1303.1">Different providers might have slightly different guidance on how to construct the best working prompts. </span><span class="koboSpan" id="kobo.1303.2">Always check the documentation on the provider’s side – for example, Anthropic emphasizes the importance of XML tags to structure your prompts. </span><span class="koboSpan" id="kobo.1303.3">Reasoning models have different prompting guidelines (for example, typically, you should not use either CoT or few-shot prompting with such models).</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1304.1">Last but not least, if you’re changing the model provider, we highly recommend running an evaluation and estimating the quality of your end-to-end application.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.1305.1">Now that we have learned how to efficiently organize your prompt and use different prompt engineering approaches with LangChain, let’s talk about what can we do </span><a id="_idTextAnchor135"/><span class="koboSpan" id="kobo.1306.1">if prompts become too long and they don’t fit into the model’s context window.</span></p>
<h1 class="heading-1" id="_idParaDest-79"><a id="_idTextAnchor136"/><span class="koboSpan" id="kobo.1307.1">Working with short context windows</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1308.1">A context window of 1 or 2 million tokens seems to be enough for almost any task we could imagine. </span><span class="koboSpan" id="kobo.1308.2">With </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.1309.1">multimodal models, you can just ask the model questions </span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.1310.1">about one, two, or many PDFs, images, or even videos. </span><span class="koboSpan" id="kobo.1310.2">To process multiple documents (for summarization or question answering), you can use what’s known as the </span><strong class="keyWord"><span class="koboSpan" id="kobo.1311.1">stuff</span></strong><span class="koboSpan" id="kobo.1312.1"> approach. </span><span class="koboSpan" id="kobo.1312.2">This approach is straightforward: use prompt templates to combine all inputs into a single prompt. </span><span class="koboSpan" id="kobo.1312.3">Then, send this consolidated prompt to an LLM. </span><span class="koboSpan" id="kobo.1312.4">This works well when the combined content fits within your model’s context window. </span><span class="koboSpan" id="kobo.1312.5">In the coming chapter, we’ll discuss further ways of using external data to improve models’ responses.</span></p>
<div>
<div class="note" id="_idContainer048">
<div aria-label="94" epub:type="pagebreak" id="page28-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1313.1">Keep in mind that, typically, PDFs are treated as images by a multimodal LLM.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.1314.1">Compared to the context window length of 4096 input tokens that we were working with only 2 years ago, the current context window of 1 or 2 million tokens is tremendous progress. </span><span class="koboSpan" id="kobo.1314.2">But it is still </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.1315.1">relevant to discuss techniques of overcoming limitations of context window size for a few reasons:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.1316.1">Not all models have long context windows, especially open-sourced ones or the ones served on edge.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1317.1">Our knowledge bases and the complexity of tasks we’re handling with LLMs are also expanding since we might be facing limitations even with current context windows.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1318.1">Shorter inputs also help reduce costs and latency.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1319.1">Inputs like audio or video are used more and more, and there are additional limitations o</span><a id="_idTextAnchor137"/><span class="koboSpan" id="kobo.1320.1">n the input length (total size of PDF files, length of the video or audio, etc.).</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.1321.1">Hence, let’s take a close look at what we can do to work with a context that is larger than a context window that an LLM can handle – summarization is a good example of such a task. </span><span class="koboSpan" id="kobo.1321.2">Handling a long context is similar to a classical Map-Reduce (a technique that was actively developed in the 2000s to handle computations on large datasets in a distributed and parallel manner). </span><span class="koboSpan" id="kobo.1321.3">In general, we have two phases:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1322.1">Map</span></strong><span class="koboSpan" id="kobo.1323.1">: We split </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.1324.1">the incoming context into smaller pieces and apply the same task to every one of them in a parallel manner. </span><span class="koboSpan" id="kobo.1324.2">We can repeat this phase a few times if needed.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1325.1">Reduce</span></strong><span class="koboSpan" id="kobo.1326.1">: We </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.1327.1">combine outputs of previous tasks together.</span></li>
</ul>
<figure class="mediaobject"><span class="koboSpan" id="kobo.1328.1"><img alt="Figure 3.5: A Map-Reduce summarization pipeline" src="../Images/B32363_03_05.png"/></span></figure>
<p class="packt_figref"><span class="koboSpan" id="kobo.1329.1">Figure 3.5: A Map-Reduce summarization pipeline</span></p>
<div aria-label="95" epub:type="pagebreak" id="page29-1" role="doc-pagebreak"/>
<h2 class="heading-2" id="_idParaDest-80"><a id="_idTextAnchor138"/><span class="koboSpan" id="kobo.1330.1">Summarizing long video</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1331.1">Let’s build a LangGraph workflow that implements the Map-Reduce approach presented above. </span><span class="koboSpan" id="kobo.1331.2">First, let’s </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.1332.1">define the state of the graph that keeps track of the video in question, the intermediate summaries we produce during the phase step, and the final summary:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1333.1">from</span></span><span class="koboSpan" id="kobo.1334.1"> langgraph.constants </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1335.1">import</span></span><span class="koboSpan" id="kobo.1336.1"> Send</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1337.1">import</span></span><span class="koboSpan" id="kobo.1338.1"> operator</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1339.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1340.1">AgentState</span></span><span class="koboSpan" id="kobo.1341.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.1342.1">TypedDict</span></span><span class="koboSpan" id="kobo.1343.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1344.1">   video_uri: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1345.1">str</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1346.1">   chunks: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1347.1">int</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1348.1">   interval_secs: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1349.1">int</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1350.1">   summaries: Annotated[</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1351.1">list</span></span><span class="koboSpan" id="kobo.1352.1">, operator.add]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1353.1">   final_summary: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1354.1">str</span></span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1355.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1356.1">_ChunkState</span></span><span class="koboSpan" id="kobo.1357.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.1358.1">TypedDict</span></span><span class="koboSpan" id="kobo.1359.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1360.1">   video_uri: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1361.1">str</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1362.1">   start_offset: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1363.1">int</span></span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1364.1">   interval_secs: </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1365.1">int</span></span></p>
<p class="normal"><span class="koboSpan" id="kobo.1366.1">Our state schema now tracks all input arguments (so that they can be accessed by various nodes) and intermediate results so that we can pass them across nodes. </span><span class="koboSpan" id="kobo.1366.2">However, the Map-Reduce pattern presents another challenge: we need to schedule many similar tasks that process different parts of the original video in parallel. </span><span class="koboSpan" id="kobo.1366.3">LangGraph provides a special </span><code class="inlineCode"><span class="koboSpan" id="kobo.1367.1">Send</span></code><span class="koboSpan" id="kobo.1368.1"> node that enables dynamic scheduling of execution on a node with a specific state. </span><span class="koboSpan" id="kobo.1368.2">For this approach, we need an additional state schema called </span><code class="inlineCode"><span class="koboSpan" id="kobo.1369.1">_ChunkState</span></code><span class="koboSpan" id="kobo.1370.1"> to represent a map step. </span><span class="koboSpan" id="kobo.1370.2">It’s worth mentioning that ordering is guaranteed – results are collected (in other words, applied to the main state) in exactly the same order as nodes are scheduled.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1371.1">Let’s define two nodes:</span></p>
<ul>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.1372.1">summarize_video_chunk</span></code><span class="koboSpan" id="kobo.1373.1"> for the Map phase</span></li>
<li class="b lletList"><code class="inlineCode"><span class="koboSpan" id="kobo.1374.1">_generate_final_summary</span></code><span class="koboSpan" id="kobo.1375.1"> for the Reduce phase</span></li>
</ul>
<div aria-label="96" epub:type="pagebreak" id="page30-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1376.1">The first node operates on a state different from the main state, but its output is added to the main state. </span><span class="koboSpan" id="kobo.1376.2">We run this node multiple times and outputs are combined into a list within the main graph. </span><span class="koboSpan" id="kobo.1376.3">To schedule </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.1377.1">these map tasks, we will create a conditional edge connecting the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1378.1">START</span></code><span class="koboSpan" id="kobo.1379.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1380.1">_summarize_video_chunk</span></code><span class="koboSpan" id="kobo.1381.1"> nodes with an edge based on a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1382.1">_map_summaries</span></code><span class="koboSpan" id="kobo.1383.1"> function:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1384.1">human_part = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1385.1">"type"</span></span><span class="koboSpan" id="kobo.1386.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1387.1">"text"</span></span><span class="koboSpan" id="kobo.1388.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1389.1">"text"</span></span><span class="koboSpan" id="kobo.1390.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1391.1">"Provide a summary of the video."</span></span><span class="koboSpan" id="kobo.1392.1">}</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1393.1">async</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1394.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1395.1">_summarize_video_chunk</span></span><span class="koboSpan" id="kobo.1396.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1397.1">state:  _ChunkState</span></span><span class="koboSpan" id="kobo.1398.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1399.1">   start_offset = state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1400.1">"start_offset"</span></span><span class="koboSpan" id="kobo.1401.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1402.1">   interval_secs = state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1403.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1404.1">interval_secs"</span></span><span class="koboSpan" id="kobo.1405.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1406.1">   video_part = {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1407.1">"type"</span></span><span class="koboSpan" id="kobo.1408.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1409.1">"media"</span></span><span class="koboSpan" id="kobo.1410.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1411.1">"file_uri"</span></span><span class="koboSpan" id="kobo.1412.1">: state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1413.1">"video_uri"</span></span><span class="koboSpan" id="kobo.1414.1">], </span><span class="hljs-string"><span class="koboSpan" id="kobo.1415.1">"mime_type"</span></span><span class="koboSpan" id="kobo.1416.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1417.1">"video/mp4"</span></span><span class="koboSpan" id="kobo.1418.1">,</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1419.1">"video_metadata"</span></span><span class="koboSpan" id="kobo.1420.1">: {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1421.1">"start_offset"</span></span><span class="koboSpan" id="kobo.1422.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1423.1">"seconds"</span></span><span class="koboSpan" id="kobo.1424.1">: start_offset*interval_secs},</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1425.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1426.1">end_offset"</span></span><span class="koboSpan" id="kobo.1427.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1428.1">"seconds"</span></span><span class="koboSpan" id="kobo.1429.1">: (start_offset+</span><span class="hljs-number"><span class="koboSpan" id="kobo.1430.1">1</span></span><span class="koboSpan" id="kobo.1431.1">)*interval_secs}}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1432.1">   }</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1433.1">   response = </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1434.1">await</span></span><span class="koboSpan" id="kobo.1435.1"> llm.ainvoke(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1436.1">       [HumanMessage(content=[human_part, video_part])])</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1437.1">return</span></span><span class="koboSpan" id="kobo.1438.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1439.1">"summaries"</span></span><span class="koboSpan" id="kobo.1440.1">: [response.content]}</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1441.1">async</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1442.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1443.1">_generate_final_summary</span></span><span class="koboSpan" id="kobo.1444.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1445.1">state: AgentState</span></span><span class="koboSpan" id="kobo.1446.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1447.1">   summary = _merge_summaries(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1448.1">       summaries=state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1449.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1450.1">summaries"</span></span><span class="koboSpan" id="kobo.1451.1">], interval_secs=state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1452.1">"interval_secs"</span></span><span class="koboSpan" id="kobo.1453.1">])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1454.1">   final_summary = </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1455.1">await</span></span><span class="koboSpan" id="kobo.1456.1"> (reduce_prompt | llm | StrOutputParser()).ainvoke({</span><span class="hljs-string"><span class="koboSpan" id="kobo.1457.1">"summaries"</span></span><span class="koboSpan" id="kobo.1458.1">: summary})</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1459.1">return</span></span><span class="koboSpan" id="kobo.1460.1"> {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1461.1">"final_summary"</span></span><span class="koboSpan" id="kobo.1462.1">: final_summary}</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1463.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1464.1">_map_summaries</span></span><span class="koboSpan" id="kobo.1465.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1466.1">state: AgentState</span></span><span class="koboSpan" id="kobo.1467.1">):</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1468.1">   chunks = state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1469.1">"chunks"</span></span><span class="koboSpan" id="kobo.1470.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1471.1">   payloads = [</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1472.1">       {</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1473.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1474.1">video_uri"</span></span><span class="koboSpan" id="kobo.1475.1">: state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1476.1">"video_uri"</span></span><span class="koboSpan" id="kobo.1477.1">],</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1478.1">"interval_secs"</span></span><span class="koboSpan" id="kobo.1479.1">: state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1480.1">"interval_secs"</span></span><span class="koboSpan" id="kobo.1481.1">],</span></p>
<p class="snippet-code"> <span class="hljs-string"><span class="koboSpan" id="kobo.1482.1">"start_offset"</span></span><span class="koboSpan" id="kobo.1483.1">: i</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1484.1">       } </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1485.1">for</span></span><span class="koboSpan" id="kobo.1486.1"> i </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1487.1">in</span></span> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1488.1">range</span></span><span class="koboSpan" id="kobo.1489.1">(state[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1490.1">"chunks"</span></span><span class="koboSpan" id="kobo.1491.1">])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1492.1">   ] </span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1493.1">return</span></span><span class="koboSpan" id="kobo.1494.1"> [Send(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1495.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1496.1">summarize_video_chunk"</span></span><span class="koboSpan" id="kobo.1497.1">, payload) </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1498.1">for</span></span><span class="koboSpan" id="kobo.1499.1"> payload </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1500.1">in</span></span><span class="koboSpan" id="kobo.1501.1"> payloads]</span></p>
<div aria-label="97" epub:type="pagebreak" id="page31-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1502.1">Now, let’s put everything toget</span><a id="_idTextAnchor139"/><span class="koboSpan" id="kobo.1503.1">her and run our graph. </span><span class="koboSpan" id="kobo.1503.2">We can pass all arguments to the pipeline in a simple manner:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1504.1">graph = StateGraph(AgentState)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1505.1">graph.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1506.1">"summarize_video_chunk"</span></span><span class="koboSpan" id="kobo.1507.1">, _summarize_video_chunk)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1508.1">graph.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1509.1">"generate_final_summary"</span></span><span class="koboSpan" id="kobo.1510.1">, _generate_final_summary)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1511.1">graph.add_conditional_edges(START, _map_summaries, [</span><span class="hljs-string"><span class="koboSpan" id="kobo.1512.1">"summarize_video_chunk"</span></span><span class="koboSpan" id="kobo.1513.1">])</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1514.1">graph.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1515.1">"summarize_video_chunk"</span></span><span class="koboSpan" id="kobo.1516.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1517.1">"generate_final_summary"</span></span><span class="koboSpan" id="kobo.1518.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1519.1">graph.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1520.1">"generate_final_summary"</span></span><span class="koboSpan" id="kobo.1521.1">, END)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1522.1">app = graph.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1523.1">compile</span></span><span class="koboSpan" id="kobo.1524.1">()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1525.1">result = </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1526.1">await</span></span><span class="koboSpan" id="kobo.1527.1"> app.ainvoke(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1528.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1529.1">"video_uri"</span></span><span class="koboSpan" id="kobo.1530.1">: video_uri, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1531.1">"chunks"</span></span><span class="koboSpan" id="kobo.1532.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1533.1">5</span></span><span class="koboSpan" id="kobo.1534.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1535.1">"interval_secs"</span></span><span class="koboSpan" id="kobo.1536.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1537.1">600</span></span><span class="koboSpan" id="kobo.1538.1">},</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1539.1">   {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1540.1">"max_concurrency"</span></span><span class="koboSpan" id="kobo.1541.1">: </span><span class="hljs-number"><span class="koboSpan" id="kobo.1542.1">3</span></span><span class="koboSpan" id="kobo.1543.1">}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1544.1">)[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1545.1">"final_summary"</span></span><span class="koboSpan" id="kobo.1546.1">]</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1547.1">Now, as we’re </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.1548.1">prepared to build our first workflows with LangGraph, there’s one last important topic to discuss. </span><span class="koboSpan" id="kobo.1548.2">What if your history of conversations becomes too long and won’t fit into the context window or it would start distracting an LLM from the last input? </span><span class="koboSpan" id="kobo.1548.3">Let’s discuss the various memory mechanisms LangChain offers.</span></p>
<h1 class="heading-1" id="_idParaDest-81"><a id="_idTextAnchor140"/><span class="koboSpan" id="kobo.1549.1">Understanding memory mechanisms</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1550.1">LangChain chains and any code you wrap them with are stateless. </span><span class="koboSpan" id="kobo.1550.2">When you deploy LangChain applications </span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.1551.1">to production, they should also be kept stateless to allow horizontal scaling (more about this in </span><a href="E_Chapter_9.xhtml#_idTextAnchor448"><em class="italic"><span class="koboSpan" id="kobo.1552.1">Chapter 9</span></em></a><span class="koboSpan" id="kobo.1553.1">). </span><span class="koboSpan" id="kobo.1553.2">In this section, we’ll discuss how to organize memory to </span><a id="_idTextAnchor141"/><span class="koboSpan" id="kobo.1554.1">keep track of interactions between your generative AI application and a specific user.</span></p>
<h2 class="heading-2" id="_idParaDest-82"><a id="_idTextAnchor142"/><span class="koboSpan" id="kobo.1555.1">Trimming chat history</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1556.1">Every chat application should preserve a dialogue history. </span><span class="koboSpan" id="kobo.1556.2">In prototype applications, you can store it in </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.1557.1">a variable, though this won’t work for </span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.1558.1">production applications, which we’ll address in the next section.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1559.1">The chat history is essentially a list of messages, but there are situations where trimming this history becomes necessary. </span><span class="koboSpan" id="kobo.1559.2">While this was a very important design pattern when LLMs had a limited context window, these days, it’s not that relevant since most of the models (even small open-sourced models) now support 8192 tokens or even more. </span><span class="koboSpan" id="kobo.1559.3">Nevertheless, understanding trimming techniques remains valuable for specific use cases.</span></p>
<div aria-label="98" epub:type="pagebreak" id="page32-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1560.1">There are </span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.1561.1">five ways to trim the chat history:</span></p>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1562.1">Discard messages based on length</span></strong><span class="koboSpan" id="kobo.1563.1"> (like tokens or messages count): You keep only the most recent messages so their total length is shorter than a threshold. </span><span class="koboSpan" id="kobo.1563.2">The special LangChain function </span><code class="inlineCode"><span class="koboSpan" id="kobo.1564.1">from langchain_core.messages import trim_messages</span></code><span class="koboSpan" id="kobo.1565.1"> allows you to trim a sequence of messages. </span><span class="koboSpan" id="kobo.1565.2">You can provide a function or an LLM instance as a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1566.1">token_counter</span></code><span class="koboSpan" id="kobo.1567.1"> argument to this function (and a corresponding LLM integration should support a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1568.1">get_token_ids</span></code><span class="koboSpan" id="kobo.1569.1"> method; otherwise, a default tokenizer might be used and results might differ from token counts for this specific LLM provider). </span><span class="koboSpan" id="kobo.1569.2">This function also allows you to customize how to trim the messages – for example, whether to keep a system message and whether a human message should always come first since many model providers require that a chat always starts with a human message (or with a system message). </span><span class="koboSpan" id="kobo.1569.3">In that case, you should trim the original sequence of </span><code class="inlineCode"><span class="koboSpan" id="kobo.1570.1">human, ai, human, ai</span></code><span class="koboSpan" id="kobo.1571.1"> to a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1572.1">human, ai</span></code><span class="koboSpan" id="kobo.1573.1"> one and not </span><code class="inlineCode"><span class="koboSpan" id="kobo.1574.1">ai, human, ai</span></code><span class="koboSpan" id="kobo.1575.1"> even if all three messages do fit within the context window threshold.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1576.1">Summarize the previous conversation</span></strong><span class="koboSpan" id="kobo.1577.1">: On each turn, you can summarize the previous conversation to a single message that you prepend to the next user’s input. </span><span class="koboSpan" id="kobo.1577.2">LangChain offered some building blocks for a running memory implementation but, as of March 2025, the recommended way is to build your own summarization node with LangGraph.You can find a detailed guide in the LangChain documentation section: </span><a href="https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/"><span class="url"><span class="koboSpan" id="kobo.1578.1">https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/</span></span></a><span class="koboSpan" id="kobo.1579.1">).</span></li>
</ul>
<p class="normal-one"><span class="koboSpan" id="kobo.1580.1">When implementing summarization or trimming, think about whether you should keep both histories in your database for further debugging, analytics, etc. </span><span class="koboSpan" id="kobo.1580.2">You might want to keep the short-memory history of the latest summary and the message after that summary for the application itself, and you probably want to keep track of the whole history (all raw messages and all the summaries) for further analysis. </span><span class="koboSpan" id="kobo.1580.3">If yes, design your application carefully. </span><span class="koboSpan" id="kobo.1580.4">For example, you probably don’t need to load all the raw history and summary messages; it’s enough to dump new messages into the database keeping track of the raw history.</span></p>
<div aria-label="99" epub:type="pagebreak" id="page33-1" role="doc-pagebreak"/>
<ul>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1581.1">Combine both trimming and summarization</span></strong><span class="koboSpan" id="kobo.1582.1">: Instead of simply discarding old messages </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.1583.1">that make the context window too long, you could </span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.1584.1">summarize these messages and prepend the remaining history.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1585.1">Summarize long messages into a short one</span></strong><span class="koboSpan" id="kobo.1586.1">: You could also summarize long messages. </span><span class="koboSpan" id="kobo.1586.2">This might be especially relevant for RAG use cases, which we’re going to discuss in the next chapter, when your input to the model might include a lot of additional context added on top of the actual user’s input.</span></li>
<li class="b lletList"><strong class="keyWord"><span class="koboSpan" id="kobo.1587.1">Implement your own trimming logic</span></strong><span class="koboSpan" id="kobo.1588.1">: The recommended way is to implement your own tokenizer that can be passed to a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1589.1">trim_messages</span></code><span class="koboSpan" id="kobo.1590.1"> function since you can reuse a lot of logic that this function already cares for.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.1591.1">Of cou</span><a id="_idTextAnchor143"/><span class="koboSpan" id="kobo.1592.1">rse, the question remains on how you can persist the chat history. </span><span class="koboSpan" id="kobo.1592.2">Let’s examine that next.</span></p>
<h2 class="heading-2" id="_idParaDest-83"><a id="_idTextAnchor144"/><span class="koboSpan" id="kobo.1593.1">Saving history to a database</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1594.1">As mentioned above, an application deployed to production can’t store chat history in a local memory. </span><span class="koboSpan" id="kobo.1594.2">If you have your code running on more than one machine, there’s no guarantee that a request </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.1595.1">from the same user will hit the same server at the next turn. </span><span class="koboSpan" id="kobo.1595.2">Of course, you can store history on the frontend and send it back and forth each time, but that also makes sessions not sharable, increases the request size, etc.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1596.1">Various database providers might offer an implementation that inherits from the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1597.1">langchain_core.chat_history.BaseChatMessageHistory</span></code><span class="koboSpan" id="kobo.1598.1">, which allows you to store and retrieve a chat history by </span><code class="inlineCode"><span class="koboSpan" id="kobo.1599.1">session_id</span></code><span class="koboSpan" id="kobo.1600.1">. </span><span class="koboSpan" id="kobo.1600.2">If you’re saving a history to a local variable while prototyping, we recommend using </span><code class="inlineCode"><span class="koboSpan" id="kobo.1601.1">InMemoryChatMessageHistory</span></code><span class="koboSpan" id="kobo.1602.1"> instead of a list to be able to later switch to integration with a database.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1603.1">Let’s look at an example. </span><span class="koboSpan" id="kobo.1603.2">We create a fake chat model with a callback that prints out the amount of input messages each time it’s called. </span><span class="koboSpan" id="kobo.1603.3">Then we initialize the dictionary that keeps histories, and we create a separate function that returns a history given the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1604.1">session_id</span></code><span class="koboSpan" id="kobo.1605.1">:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1606.1">from</span></span><span class="koboSpan" id="kobo.1607.1"> langchain_core.chat_history </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1608.1">import</span></span><span class="koboSpan" id="kobo.1609.1"> InMemoryChatMessageHistory</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1610.1">from</span></span><span class="koboSpan" id="kobo.1611.1"> langchain_core.runnables.history </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1612.1">import</span></span><span class="koboSpan" id="kobo.1613.1"> RunnableWithMessageHistory</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1614.1">from</span></span><span class="koboSpan" id="kobo.1615.1"> langchain_core.language_models </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1616.1">import</span></span><span class="koboSpan" id="kobo.1617.1"> FakeListChatModel</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1618.1">from</span></span><span class="koboSpan" id="kobo.1619.1"> langchain.callbacks.base </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1620.1">import</span></span><span class="koboSpan" id="kobo.1621.1"> BaseCallbackHandler</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1622.1">class</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1623.1">PrintOutputCallback</span></span><span class="koboSpan" id="kobo.1624.1">(</span><span class="hljs-title"><span class="koboSpan" id="kobo.1625.1">BaseCallbackHandler</span></span><span class="koboSpan" id="kobo.1626.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1627.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1628.1">on_chat_model_start</span></span><span class="koboSpan" id="kobo.1629.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1630.1">self, serialized, messages, **kwargs</span></span><span class="koboSpan" id="kobo.1631.1">):</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1632.1">print</span></span><span class="koboSpan" id="kobo.1633.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1634.1">f"Amount of input messages: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1635.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1636.1">len</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1637.1">(messages)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1638.1">"</span></span><span class="koboSpan" id="kobo.1639.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1640.1">sessions = {}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1641.1">handler = PrintOutputCallback()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1642.1">llm = FakeListChatModel(responses=[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1643.1">"ai1"</span></span><span class="koboSpan" id="kobo.1644.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1645.1">"ai2"</span></span><span class="koboSpan" id="kobo.1646.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1647.1">"ai3"</span></span><span class="koboSpan" id="kobo.1648.1">])</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1649.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1650.1">get_session_history</span></span><span class="koboSpan" id="kobo.1651.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1652.1">session_id: </span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1653.1">str</span></span><span class="koboSpan" id="kobo.1654.1">):</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1655.1">if</span></span><span class="koboSpan" id="kobo.1656.1"> session_id </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1657.1">not</span></span> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1658.1">in</span></span><span class="koboSpan" id="kobo.1659.1"> sessions:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1660.1">       sessions[session_id] = InMemoryChatMessageHistory()</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1661.1">return</span></span><span class="koboSpan" id="kobo.1662.1"> sessions[session_id]</span></p>
<div aria-label="100" epub:type="pagebreak" id="page34-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1663.1">Now we </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.1664.1">create a trimmer that uses a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1665.1">len</span></code><span class="koboSpan" id="kobo.1666.1"> function and threshold </span><code class="inlineCode"><span class="koboSpan" id="kobo.1667.1">1</span></code><span class="koboSpan" id="kobo.1668.1"> – i.e., it always removes the entire history and keeps a system message only:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1669.1">trimmer = trim_messages(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1670.1">   max_tokens=</span><span class="hljs-number"><span class="koboSpan" id="kobo.1671.1">1</span></span><span class="koboSpan" id="kobo.1672.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1673.1">   strategy=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1674.1">"last"</span></span><span class="koboSpan" id="kobo.1675.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1676.1">   token_counter=</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1677.1">len</span></span><span class="koboSpan" id="kobo.1678.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1679.1">   include_system=</span><span class="hljs-literal"><span class="koboSpan" id="kobo.1680.1">True</span></span><span class="koboSpan" id="kobo.1681.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1682.1">   start_on=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1683.1">"human"</span></span><span class="koboSpan" id="kobo.1684.1">,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1685.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1686.1">raw_chain = trimmer | llm</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1687.1">chain = RunnableWithMessageHistory(raw_chain, get_session_history)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1688.1">Now let’s run it and make sure that our history keeps all the interactions with the user but a trimmed history is passed to the LLM:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1689.1">config = {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1690.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1691.1">callbacks"</span></span><span class="koboSpan" id="kobo.1692.1">: [PrintOutputCallback()], </span><span class="hljs-string"><span class="koboSpan" id="kobo.1693.1">"configurable"</span></span><span class="koboSpan" id="kobo.1694.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1695.1">"session_id"</span></span><span class="koboSpan" id="kobo.1696.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1697.1">"1"</span></span><span class="koboSpan" id="kobo.1698.1">}}</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1699.1">_ = chain.invoke(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1700.1">   [HumanMessage(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1701.1">"Hi!"</span></span><span class="koboSpan" id="kobo.1702.1">)],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1703.1">   config=config,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1704.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1705.1">print</span></span><span class="koboSpan" id="kobo.1706.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1707.1">f"History length: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1708.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1709.1">len</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1710.1">(sessions[</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1711.1">'1'</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1712.1">].messages)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1713.1">"</span></span><span class="koboSpan" id="kobo.1714.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1715.1">_ = chain.invoke(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1716.1">   [HumanMessage(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1717.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1718.1">How are you?"</span></span><span class="koboSpan" id="kobo.1719.1">)],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1720.1">   config=config,</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1721.1">)</span></p>
<p class="snippet-code"><span class="hljs-built_in"><span class="koboSpan" id="kobo.1722.1">print</span></span><span class="koboSpan" id="kobo.1723.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1724.1">f"History length: </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1725.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1726.1">len</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1727.1">(sessions[</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1728.1">'1'</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1729.1">].messages)}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1730.1">"</span></span><span class="koboSpan" id="kobo.1731.1">)</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1732.1">&gt;&gt; Amount of input messages: 1</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1733.1">History length: 2</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1734.1">Amount of input messages: 1</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1735.1">History length: 4</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1736.1">We used a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1737.1">RunnableWithMessageHistory</span></code><span class="koboSpan" id="kobo.1738.1"> that takes a chain and wraps it (like a decorator) with calls to history before executing the chain (to retrieve the history and pass it to the chain) and after finishing the chain (to add new messages to the history).</span></p>
<div aria-label="101" epub:type="pagebreak" id="page35-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1739.1">Database </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.1740.1">providers might have their integrations as part of the </span><code class="inlineCode"><span class="koboSpan" id="kobo.1741.1">langchain_commuity</span></code><span class="koboSpan" id="kobo.1742.1"> package or outside of it – for example, in libraries such as </span><code class="inlineCode"><span class="koboSpan" id="kobo.1743.1">langchain_postgres</span></code><span class="koboSpan" id="kobo.1744.1"> for a standalone PostgreSQL database or </span><code class="inlineCode"><span class="koboSpan" id="kobo.1745.1">langchain-google-cloud-sql-pg</span></code><span class="koboSpan" id="kobo.1746.1"> for a managed one.</span></p>
<div>
<div class="note" id="_idContainer050">
<p class="normal"><span class="koboSpan" id="kobo.1747.1">You can find the full list of integrations to store chat history on the documentation page: </span><a href="https://python.langchain.com/api_reference/community/chat_message_histories.html"><span class="url"><span class="koboSpan" id="kobo.1748.1">python.langchain.com/api_reference/community/chat_message_histories.html</span></span></a><span class="koboSpan" id="kobo.1749.1">.</span></p>
</div>
</div>
<p class="normal"><span class="koboSpan" id="kobo.1750.1">When designing a real application, you should be cautious about managing access to somebody’s sessions. </span><span class="koboSpan" id="kobo.1750.2">For example, if you use a sequential </span><code class="inlineCode"><span class="koboSpan" id="kobo.1751.1">session_id</span></code><span class="koboSpan" id="kobo.1752.1">, users might easily access sessions that don’t belong to them. </span><span class="koboSpan" id="kobo.1752.2">Practically, it might be enough to use a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1753.1">uuid</span></code><span class="koboSpan" id="kobo.1754.1"> (a uniquely generated long identifier) instead of a sequential </span><code class="inlineCode"><span class="koboSpan" id="kobo.1755.1">session_id</span></code><span class="koboSpan" id="kobo.1756.1">, </span><a id="_idTextAnchor145"/><span class="koboSpan" id="kobo.1757.1">or, depending on your security requirements, add other permissions validations during runtime.</span></p>
<h2 class="heading-2" id="_idParaDest-84"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.1758.1">LangGraph checkpoints</span></h2>
<p class="normal"><span class="koboSpan" id="kobo.1759.1">A checkpoint is a snapshot of the current state of the graph. </span><span class="koboSpan" id="kobo.1759.2">It keeps all the information to continue </span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.1760.1">running the workflow from the moment when the snapshot has </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.1761.1">been taken – including the full state, metadata, nodes that were planned to be executed, and tasks that failed. </span><span class="koboSpan" id="kobo.1761.2">This is a different mechanism from storing the chat history since you can store the workflow at any given point in time and later restore from the checkpoint to continue. </span><span class="koboSpan" id="kobo.1761.3">It is important for multiple reasons:</span></p>
<ul>
<li class="b lletList"><span class="koboSpan" id="kobo.1762.1">Checkpoints allow deep debugging and “time travel.”</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1763.1">Checkpoints allow you to experiment with different paths in your complex workflow without the need to rerun it each time.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1764.1">Checkpoints facilitate human-in-the-loop workflows by making it possible to implement human intervention at a given point and continue further.</span></li>
<li class="b lletList"><span class="koboSpan" id="kobo.1765.1">Checkpoints help to implement production-ready systems since they add a required level of persistence and fault tolerance.</span></li>
</ul>
<p class="normal"><span class="koboSpan" id="kobo.1766.1">Let’s build a simple example with a single node that prints the amount of messages in the state and returns a fake </span><code class="inlineCode"><span class="koboSpan" id="kobo.1767.1">AIMessage</span></code><span class="koboSpan" id="kobo.1768.1">. </span><span class="koboSpan" id="kobo.1768.2">We use a built-in </span><code class="inlineCode"><span class="koboSpan" id="kobo.1769.1">MessageGraph</span></code><span class="koboSpan" id="kobo.1770.1"> that represents a state with only a list of messages, and we initiate a </span><code class="inlineCode"><span class="koboSpan" id="kobo.1771.1">MemorySaver</span></code><span class="koboSpan" id="kobo.1772.1"> that will keep checkpoints in local memory and pass it to the graph during compilation:</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1773.1">from</span></span><span class="koboSpan" id="kobo.1774.1"> langgraph.graph </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1775.1">import</span></span><span class="koboSpan" id="kobo.1776.1"> MessageGraph</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1777.1">from</span></span><span class="koboSpan" id="kobo.1778.1"> langgraph.checkpoint.memory </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1779.1">import</span></span><span class="koboSpan" id="kobo.1780.1"> MemorySaver</span></p>
<div aria-label="102" epub:type="pagebreak" id="page36-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1781.1">def</span></span> <span class="hljs-title"><span class="koboSpan" id="kobo.1782.1">test_node</span></span><span class="koboSpan" id="kobo.1783.1">(</span><span class="hljs-params"><span class="koboSpan" id="kobo.1784.1">state</span></span><span class="koboSpan" id="kobo.1785.1">):</span></p>
<p class="snippet-code"> <span class="hljs-comment"><span class="koboSpan" id="kobo.1786.1"># ignore the last message since it's an input one</span></span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1787.1">print</span></span><span class="koboSpan" id="kobo.1788.1">(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1789.1">f"History length = </span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1790.1">{</span></span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1791.1">len</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1792.1">(state[:-</span></span><span class="hljs-number"><span class="koboSpan" id="kobo.1793.1">1</span></span><span class="hljs-subst"><span class="koboSpan" id="kobo.1794.1">])}</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1795.1">"</span></span><span class="koboSpan" id="kobo.1796.1">)</span></p>
<p class="snippet-code"> <span class="hljs-keyword"><span class="koboSpan" id="kobo.1797.1">return</span></span><span class="koboSpan" id="kobo.1798.1"> [AIMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1799.1">"Hello!"</span></span><span class="koboSpan" id="kobo.1800.1">)]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1801.1">builder = MessageGraph()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1802.1">builder.add_node(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1803.1">"test_node"</span></span><span class="koboSpan" id="kobo.1804.1">, test_node)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1805.1">builder.add_edge(START, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1806.1">"test_node"</span></span><span class="koboSpan" id="kobo.1807.1">)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1808.1">builder.add_edge(</span><span class="hljs-string"><span class="koboSpan" id="kobo.1809.1">"test_node"</span></span><span class="koboSpan" id="kobo.1810.1">, END)</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1811.1">memory = MemorySaver()</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1812.1">graph = builder.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1813.1">compile</span></span><span class="koboSpan" id="kobo.1814.1">(checkpointer=memory)</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1815.1">Now, each time </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.1816.1">we invoke the graph, we should provide </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.1817.1">either a specific checkpoint or a thread-id (a unique identifier of each run). </span><span class="koboSpan" id="kobo.1817.2">We invoke our graph two times with different </span><code class="inlineCode"><span class="koboSpan" id="kobo.1818.1">thread-id</span></code><span class="koboSpan" id="kobo.1819.1"> values, make sure they each start with an empty history, and then check that the first thread has a history when we invoke it for the second time:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1820.1">_ = graph.invoke([HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1821.1">"test"</span></span><span class="koboSpan" id="kobo.1822.1">)],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1823.1">  config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1824.1">"configurable"</span></span><span class="koboSpan" id="kobo.1825.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1826.1">"thread_id"</span></span><span class="koboSpan" id="kobo.1827.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1828.1">"thread-a"</span></span><span class="koboSpan" id="kobo.1829.1">}})</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1830.1">_ = graph.invoke([HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1831.1">"test"</span></span><span class="koboSpan" id="kobo.1832.1">)]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1833.1">  config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1834.1">"configurable"</span></span><span class="koboSpan" id="kobo.1835.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1836.1">"thread_id"</span></span><span class="koboSpan" id="kobo.1837.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1838.1">"thread-b"</span></span><span class="koboSpan" id="kobo.1839.1">}})</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1840.1">_ = graph.invoke([HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1841.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1842.1">test"</span></span><span class="koboSpan" id="kobo.1843.1">)]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1844.1">  config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1845.1">"configurable"</span></span><span class="koboSpan" id="kobo.1846.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1847.1">"thread_id"</span></span><span class="koboSpan" id="kobo.1848.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1849.1">"thread-a"</span></span><span class="koboSpan" id="kobo.1850.1">}})</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1851.1">&gt;&gt; History length = 0</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1852.1">History length = 0</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1853.1">History length = 2</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1854.1">We can inspect checkpoints for a given thread:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1855.1">checkpoints = </span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1856.1">list</span></span><span class="koboSpan" id="kobo.1857.1">(memory.</span><span class="hljs-built_in"><span class="koboSpan" id="kobo.1858.1">list</span></span><span class="koboSpan" id="kobo.1859.1">(config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1860.1">"configurable"</span></span><span class="koboSpan" id="kobo.1861.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1862.1">"thread_id"</span></span><span class="koboSpan" id="kobo.1863.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1864.1">"thread-a"</span></span><span class="koboSpan" id="kobo.1865.1">}}))</span></p>
<p class="snippet-code"><span class="hljs-keyword"><span class="koboSpan" id="kobo.1866.1">for</span></span><span class="koboSpan" id="kobo.1867.1"> check_point </span><span class="hljs-keyword"><span class="koboSpan" id="kobo.1868.1">in</span></span><span class="koboSpan" id="kobo.1869.1"> checkpoints:</span></p>
<p class="snippet-code"> <span class="hljs-built_in"><span class="koboSpan" id="kobo.1870.1">print</span></span><span class="koboSpan" id="kobo.1871.1">(check_point.config[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1872.1">"configurable"</span></span><span class="koboSpan" id="kobo.1873.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.1874.1">"checkpoint_id"</span></span><span class="koboSpan" id="kobo.1875.1">])</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1876.1">Let’s also restore from the initial checkpoint for </span><code class="inlineCode"><span class="koboSpan" id="kobo.1877.1">thread-a</span></code><span class="koboSpan" id="kobo.1878.1">. </span><span class="koboSpan" id="kobo.1878.2">We’ll see that we start with an empty history:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1879.1">checkpoint_id = checkpoints[-</span><span class="hljs-number"><span class="koboSpan" id="kobo.1880.1">1</span></span><span class="koboSpan" id="kobo.1881.1">].config[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1882.1">"configurable"</span></span><span class="koboSpan" id="kobo.1883.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.1884.1">"checkpoint_id"</span></span><span class="koboSpan" id="kobo.1885.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1886.1">_ = graph.invoke(</span></p>
<div aria-label="103" epub:type="pagebreak" id="page37-1" role="doc-pagebreak"/>
<p class="snippet-code"><span class="koboSpan" id="kobo.1887.1">   [HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1888.1">"test"</span></span><span class="koboSpan" id="kobo.1889.1">)],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1890.1">   config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1891.1">"configurable"</span></span><span class="koboSpan" id="kobo.1892.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1893.1">"thread_id"</span></span><span class="koboSpan" id="kobo.1894.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1895.1">"thread-a"</span></span><span class="koboSpan" id="kobo.1896.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1897.1">"checkpoint_id"</span></span><span class="koboSpan" id="kobo.1898.1">: checkpoint_id}})</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1899.1">&gt;&gt; History length = 0</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1900.1">We can </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.1901.1">also start from an intermediate checkpoint, as shown here:</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1902.1">checkpoint_id = checkpoints[-</span><span class="hljs-number"><span class="koboSpan" id="kobo.1903.1">3</span></span><span class="koboSpan" id="kobo.1904.1">].config[</span><span class="hljs-string"><span class="koboSpan" id="kobo.1905.1">"configurable"</span></span><span class="koboSpan" id="kobo.1906.1">][</span><span class="hljs-string"><span class="koboSpan" id="kobo.1907.1">"checkpoint_id"</span></span><span class="koboSpan" id="kobo.1908.1">]</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1909.1">_ = graph.invoke(</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1910.1">   [HumanMessage(content=</span><span class="hljs-string"><span class="koboSpan" id="kobo.1911.1">"</span></span><span class="hljs-string"><span class="koboSpan" id="kobo.1912.1">test"</span></span><span class="koboSpan" id="kobo.1913.1">)],</span></p>
<p class="snippet-code"><span class="koboSpan" id="kobo.1914.1">   config={</span><span class="hljs-string"><span class="koboSpan" id="kobo.1915.1">"configurable"</span></span><span class="koboSpan" id="kobo.1916.1">: {</span><span class="hljs-string"><span class="koboSpan" id="kobo.1917.1">"thread_id"</span></span><span class="koboSpan" id="kobo.1918.1">: </span><span class="hljs-string"><span class="koboSpan" id="kobo.1919.1">"thread-a"</span></span><span class="koboSpan" id="kobo.1920.1">, </span><span class="hljs-string"><span class="koboSpan" id="kobo.1921.1">"checkpoint_id"</span></span><span class="koboSpan" id="kobo.1922.1">: checkpoint_id}})</span></p>
<p class="snippet-con"><span class="koboSpan" id="kobo.1923.1">&gt;&gt; History length = 2</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1924.1">One obvious </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.1925.1">use case for checkpoints is implementing workflows that require additional input from the user. </span><span class="koboSpan" id="kobo.1925.2">We’ll run into exactly the same problem as above – when deploying our production to multiple instances, we can’t guarantee that the next request from the user hits the same server as before. </span><span class="koboSpan" id="kobo.1925.3">Our graph is stateful (during the execution), but the application that wraps it as a web service should remain stateless. </span><span class="koboSpan" id="kobo.1925.4">Hence, we can’t store checkpoints in local memory, and we should write them to the database instead. </span><span class="koboSpan" id="kobo.1925.5">LangGraph offers two integrations: </span><code class="inlineCode"><span class="koboSpan" id="kobo.1926.1">SqliteSaver</span></code><span class="koboSpan" id="kobo.1927.1"> and </span><code class="inlineCode"><span class="koboSpan" id="kobo.1928.1">PostgresSaver</span></code><span class="koboSpan" id="kobo.1929.1">. </span><span class="koboSpan" id="kobo.1929.2">You can always use them as a starting point and build your own integration if you’d like to use another database provider since all you need to implement is storing and retrieving dictionaries that represent a checkpoint.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1930.1">Now, you’ve learned the basics and are fully equipped to develop you</span><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.1931.1">r own workflows. </span><span class="koboSpan" id="kobo.1931.2">We’ll continue to look at more complex examples and techniques in the next chapter.</span></p>
<h1 class="heading-1" id="_idParaDest-85"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.1932.1">Summary</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1933.1">In this chapter, we dived into building complex workflows with LangChain and LangGraph, going beyond simple text generation. </span><span class="koboSpan" id="kobo.1933.2">We introduced LangGraph as an orchestration framework designed to handle agentic workflows and also created a basic workflow with nodes and edges, and conditional edges, that allow workflow to branch based on the current state. </span><span class="koboSpan" id="kobo.1933.3">Next, we shifted to output parsing and error handling, where we saw how to use built-in LangChain output parsers and emphasized the importance of graceful error handling.</span></p>
<div aria-label="104" epub:type="pagebreak" id="page38-1" role="doc-pagebreak"/>
<p class="normal"><span class="koboSpan" id="kobo.1934.1">We then looked into prompt engineering and discussed how to use zero-shot and dynamic few-shot prompting with LangChain, how to construct advanced prompts such as CoT prompting, and how to use substitution mechanisms. </span><span class="koboSpan" id="kobo.1934.2">Finally, we discussed how to work with long and short contexts, exploring techniques for managing large contexts by splitting the input into smaller pieces and combining the outputs in a Map-Reduce fashion, and worked on an example of processing a large video that doesn’t fit into a context.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1935.1">Finally, we covered memory mechanisms in LangChain, emphasized the need for statelessness in production deployments, and discussed methods for managing chat history, including trimming based on length and summarizing conversations.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1936.1">We will use what we l</span><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.1937.1">earned here to develop a RAG system in </span><a href="E_Chapter_4.xhtml#_idTextAnchor152"><em class="italic"><span class="koboSpan" id="kobo.1938.1">Chapter 4</span></em></a><span class="koboSpan" id="kobo.1939.1"> and more complex agentic workflows in </span><em class="italic"><span class="koboSpan" id="kobo.1940.1">Chapters 5</span></em><span class="koboSpan" id="kobo.1941.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1942.1">6</span></em><span class="koboSpan" id="kobo.1943.1">.</span></p>
<h1 class="heading-1" id="_idParaDest-86"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.1944.1">Questions</span></h1>
<ol>
<li class="numberedList" value="1"><span class="koboSpan" id="kobo.1945.1">What is LangGraph, and how does LangGraph workflow differ from LangChain’s vanilla chains?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1946.1">What is a “state” in LangGraph, and what are its main functions?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1947.1">Explain the purpose of </span><em class="italic"><span class="koboSpan" id="kobo.1948.1">add_node</span></em><span class="koboSpan" id="kobo.1949.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1950.1">add_edge</span></em><span class="koboSpan" id="kobo.1951.1"> in LangGraph.</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1952.1">What are “supersteps” in LangGraph, and how do they relate to parallel execution?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1953.1">How do conditional edges enhance LangGraph workflows compared to sequential chains?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1954.1">What is the purpose of the Literal type hint when defining conditional edges?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1955.1">What are reducers in LangGraph, and how do they allow modification of the state?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1956.1">Why is error handling crucial in LangChain workflows, and what are some strategies for achieving it?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1957.1">How can memory mechanisms be used to trim the history of a conversational bot?</span></li>
<li class="numberedList"><span class="koboSpan" id="kobo.1958.1">What is the use case of LangGraph checkpoints?</span></li>
</ol>
<div aria-label="105" epub:type="pagebreak" id="page39-1" role="doc-pagebreak"/>
<h1 class="heading-1" id="_idParaDest-87"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.1959.1">Subscribe to our weekly newsletter</span></h1>
<p class="normal"><span class="koboSpan" id="kobo.1960.1">Subscribe to AI_Distilled, the go-to newsletter for AI professionals, researchers, and innovators, at </span><a href="E_Chapter_3.xhtml"><span class="url"><span class="koboSpan" id="kobo.1961.1">https://packt.link/Q5UyU</span></span></a><span class="koboSpan" id="kobo.1962.1">.</span></p>
<p class="normal"><span class="koboSpan" id="kobo.1963.1"><img alt="" src="../Images/Newsletter_QRcode1.jpg"/></span></p>
</div>
</body></html>