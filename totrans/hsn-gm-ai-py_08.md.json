["```py\nconda create -n gameAI python=3.6\n```", "```py\nactivate gameAI\n```", "```py\nconda install pytorch torchvision cpuonly -c pytorch\n```", "```py\nimport torch\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n\nbatch_size, inputs, hidden, outputs = 64, 1000, 100, 10\nx = torch.randn(batch_size, inputs, device=device, dtype=dtype)\ny = torch.randn(batch_size, outputs, device=device, dtype=dtype)\n\nlayer1 = torch.randn(inputs, hidden, device=device, dtype=dtype)\nlayer2 = torch.randn(hidden, outputs, device=device, dtype=dtype)\nlearning_rate = 1e-6\n\nfor t in range(500):\n  h = x.mm(layer1)\n  h_relu = h.clamp(min=0)\n  y_pred = h_relu.mm(layer2)\n\n  loss = (y_pred - y).pow(2).sum().item()\n  if t % 100 == 99:\n    print(t, loss)\n\n  grad_y_pred = 2.0 * (y_pred - y)\n  grad_layer2 = h_relu.t().mm(grad_y_pred)\n  grad_h_relu = grad_y_pred.mm(layer2.t())\n  grad_h = grad_h_relu.clone()\n  grad_h[h < 0] = 0\n  grad_layer1 = x.t().mm(grad_h)\n\n  layer1 -= learning_rate * grad_layer1\n  layer2 -= learning_rate * grad_layer2\n```", "```py\nbatch_size, inputs, hidden, outputs = 64, 1000, 100, 10\n```", "```py\nx = torch.randn(batch_size, inputs, device=device, dtype=dtype)\ny = torch.randn(batch_size, outputs, device=device, dtype=dtype)\n```", "```py\nlayer1 = torch.randn(inputs, hidden, device=device, dtype=dtype)\nlayer2 = torch.randn(hidden, outputs, device=device, dtype=dtype)\n```", "```py\nlearning_rate = 1e-6\n\n```", "```py\nfor t in range(500):\n  h = x.mm(layer1)\n  h_relu = h.clamp(min=0)\n  y_pred = h_relu.mm(layer2)\n```", "```py\nloss = (y_pred - y).pow(2).sum().item()\nif t % 100 == 99:\n  print(t, loss)\n```", "```py\ngrad_y_pred = 2.0 * (y_pred - y)\ngrad_layer2 = h_relu.t().mm(grad_y_pred)\ngrad_h_relu = grad_y_pred.mm(layer2.t())\ngrad_h = grad_h_relu.clone()\ngrad_h[h < 0] = 0\ngrad_layer1 = x.t().mm(grad_h)\n```", "```py\nlayer1 -= learning_rate * grad_layer1\nlayer2 -= learning_rate * grad_layer2\n```", "```py\nimport torch\n\nbatch_size, inputs, hidden, outputs = 64, 1000, 100, 10\n\nx = torch.randn(batch_size, inputs)\ny = torch.randn(batch_size, outputs)\n\nmodel = torch.nn.Sequential(\n  torch.nn.Linear(inputs, hidden),\n  torch.nn.ReLU(),\n  torch.nn.Linear(hidden, outputs),\n)\n\nloss_fn = torch.nn.MSELoss(reduction='sum')\nlearning_rate = 1e-4\n\nfor t in range(500):   \n  y_pred = model(x)\n\n  loss = loss_fn(y_pred, y)\n\n  if t % 100 == 99:\n    print(t, loss.item())  \n\n  model.zero_grad()\n  loss.backward()  \n\n  with torch.no_grad():\n    for param in model.parameters():\n      param -= learning_rate * param.grad\n```", "```py\nmodel = torch.nn.Sequential(\n  torch.nn.Linear(inputs, hidden),\n  torch.nn.ReLU(),\n  torch.nn.Linear(hidden, outputs),\n)\n```", "```py\nloss_fn = torch.nn.MSELoss(reduction='sum')\n```", "```py\nfor t in range(500):   \n  y_pred = model(x)\n```", "```py\nloss = loss_fn(y_pred, y)\n```", "```py\nmodel.zero_grad()\nloss.backward()\n```", "```py\nwith torch.no_grad():\n    for param in model.parameters():\n      param -= learning_rate * param.grad\n```", "```py\nconda install swig\npip install box2d-py\n```", "```py\npip install gym[all]\n\n```", "```py\npip install matplotlib\npip install tqdm\n```", "```py\nimport math, random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.autograd as autograd \nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nimport gym\nimport numpy as np\nfrom collections import deque\nfrom tqdm import trange\n```", "```py\nenv_id = \"CartPole-v0\"\nenv = gym.make(env_id)\nepsilon_start = 1.0\n\nepsilon_final = 0.01\nepsilon_decay = 500\neps_by_episode = lambda epoch: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1\\. * epoch / epsilon_decay)\n\nplt.plot([eps_by_episode(i) for i in range(10000)])\nplt.show()\n```", "```py\nmodel = DQN(env.observation_space.shape[0], env.action_space.n)\noptimizer = optim.Adam(model.parameters())\nreplay_buffer = ReplayBuffer(1000)\n```", "```py\nepisodes = 10000\nbatch_size = 32\ngamma      = 0.99\n\nlosses = []\nall_rewards = []\nepisode_reward = 0\n\nstate = env.reset()\ntot_reward = 0\ntr = trange(episodes+1, desc='Agent training', leave=True)\n```", "```py\nfor episode in tr:\n  tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n  tr.refresh() \n  epsilon = eps_by_episode(episode)\n\n  action = model.act(state, epsilon)\n  next_state, reward, done, _ = env.step(action)\n\n  replay_buffer.push(state, action, reward, next_state, done)\n  tot_reward += reward\n\n  state = next_state\n  episode_reward += reward\n\n  if done:\n    state = env.reset()\n    all_rewards.append(episode_reward)\n    episode_reward = 0\n\n  if len(replay_buffer) > batch_size:\n    loss = compute_td_loss(batch_size)\n losses.append(loss.item())\n\n  if epoch % 2000 == 0:\n    plot(epoch, all_rewards, losses) \n```", "```py\nclass ReplayBuffer(object):\n  def __init__(self, capacity):\n    self.buffer = deque(maxlen=capacity)\n\n  def push(self, state, action, reward, next_state, done):\n    state      = np.expand_dims(state, 0)\n    next_state = np.expand_dims(next_state, 0)\n    self.buffer.append((state, action, reward, next_state, done))\n\n  def sample(self, batch_size): \n state, action, reward, next_state, done \n = zip(*random.sample(self.buffer, batch_size))\n    return np.concatenate(state), action,\n  reward, np.concatenate(next_state), done\n\n  def __len__(self):\n    return len(self.buffer)\n```", "```py\ndef push(self, state, action, reward, next_state, done):\n  state      = np.expand_dims(state, 0)\n  next_state = np.expand_dims(next_state, 0)\n  self.buffer.append((state, action, reward, next_state, done))\n```", "```py\ndef sample(self, batch_size): \n    state, action, reward, next_state, done \n      = zip(*random.sample(self.buffer, batch_size))\n    return np.concatenate(state), action,\n      reward, np.concatenate(next_state), done\n```", "```py\nreplay_buffer = ReplayBuffer(3000)\n```", "```py\nreplay_buffer = ReplayBuffer(333)\n```", "```py\nclass DQN(nn.Module):\n  def __init__(self, num_inputs, num_actions):\n    super(DQN, self).__init__()\n\n    self.layers = nn.Sequential(\n      nn.Linear(env.observation_space.shape[0], 128),\n      nn.ReLU(),\n      nn.Linear(128, 128),\n      nn.ReLU(),\n      nn.Linear(128, env.action_space.n))\n\n  def forward(self, x):\n    return self.layers(x)\n\n  def act(self, state, epsilon):\n    if random.random() > epsilon:\n      state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0),\n        volatile=True)\n      q_value = self.forward(state)\n      action  = q_value.max(1)[1].item()\n    else:\n      action = random.randrange(env.action_space.n)\n    return action\n```", "```py\nstate   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0),\n        volatile=True)\nq_value = self.forward(state)\naction  = q_value.max(1)[1].item()\n```", "```py\nself.layers = nn.Sequential(\n      nn.Linear(env.observation_space.shape[0], 64),\n      nn.ReLU(),\n      nn.Linear(64, 64),\n      nn.ReLU(),\n      nn.Linear(64, env.action_space.n))\n```", "```py\ndef compute_td_loss(batch_size):\n  state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n\n  state      = autograd.Variable(torch.FloatTensor(np.float32(state)))\n  next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)),\n    volatile=True)\n  action     = autograd.Variable(torch.LongTensor(action))\n  reward     = autograd.Variable(torch.FloatTensor(reward))\n  done       = autograd.Variable(torch.FloatTensor(done))\n\n  q_values      = model(state)\n  next_q_values = model(next_state)\n  q_value       = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n\n  next_q_value  = next_q_values.max(1)[0]\n  expected_q_value = reward + gamma * next_q_value * (1 - done)\n\n  loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n  optimizer.zero_grad()\n  loss.backward()\n  optimizer.step()\n\n  return loss\n```", "```py\nq_values      = model(state)\nnext_q_values = model(next_state)\nq_value       = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n```", "```py\nnext_q_value  = next_q_values.max(1)[0]\nexpected_q_value = reward + gamma * next_q_value * (1 - done)\n```", "```py\nloss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()  \n```", "```py\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```", "```py\nbuffer_size = 1000\nneurons = 128\n```", "```py\n if done:\n   if episode > buffer_size:\n play_game()\n   state = env.reset()\n   all_rewards.append(episode_reward)\n   episode_reward = 0  \n```", "```py\ndef play_game():\n  done = False\n  state = env.reset()\n  while(not done):\n    action = model.act(state, epsilon_final)\n    next_state, reward, done, _ = env.step(action)\n    env.render()\n    state = next_state\n```", "```py\nenv_id = 'LunarLander-v2'\nenv = gym.make(env_id)\n```", "```py\nepsilon_decay = 1000\nbuffer_size = 3000\nneurons = 192\n\n```"]