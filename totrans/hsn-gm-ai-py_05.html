<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Temporal Difference Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">In our previous discussion on the history of reinforcement learning, we covered the two main threads, trial and error and <strong>Dynamic Programming</strong> (<strong>DP</strong>), which came together to derive current modern <strong>Reinforcement Learning</strong> (<strong>RL</strong>). As we mentioned in earlier chapters, there is also a third thread that arrived late called <strong>Temporal Difference Learning</strong> (<strong>TDL</strong>). In this chapter, we will explore TDL and how it solves the <strong>Temporal Credit Assignment</strong> (<strong>TCA</strong>) problem. From there, we will explore how TD differs from <strong>Monte Carlo</strong> (<strong>MC</strong>) and how it evolves to full Q-learning. After that, we will explore the differences between on-policy and off-policy learning and then, finally, work on a new example RL environment.</p>
<p>For this chapter, we will introduce TDL and how it improves on the previous techniques we looked at in previous chapters. Here are the main topics we will cover in this chapter:</p>
<ul>
<li>Understanding the TCA problem</li>
<li>Introducing TDL</li>
<li>Applying TDL to Q-learning</li>
<li><span>Exploring TD(0) in Q-learning</span></li>
<li><span>Running off-policy versus on-policy</span></li>
</ul>
<p>This chapter introduces TDL and Q-learning in detail. Therefore, it is worth reviewing and understanding the material well. Knowing the foundational material well will only ease your learning later.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the TCA problem</h1>
                </header>
            
            <article>
                
<p>The credit assignment problem is described as the task of understanding what actions you need to take to receive the most credit or, in the case of RL, rewards. RL solves the credit assignment problem by allowing an algorithm or agent to find the optimum set of actions to maximize the rewards. In all of our previous chapters, we have seen how variations of this can be done with DP and MC methods. However, both of these previous methods are offline, so they cannot learn while performing a task.</p>
<p><span>The TCA problem is differentiated from the credit assignment CA problem in that it needs to be solved across time; that is, an algorithm needs to find the best policy across time steps instead of learning after an episode, in the case of MC, or needing to plan before, as DP does. This also means that an algorithm that solves the CA problem across time can also or should be able to learn in real time, that is, be able to make updates to a policy during the progression of the task, rather than before or after as we have seen in earlier chapters.</span></p>
<p>By introducing the concept of time or a progression of events, we also allow our agent to learn the importance of event timing. Previously, our agents would have no awareness of time-critical events such as hitting a moving target or timing a running jump just right. TDL, on the other hand, allows an agent to understand event timing and take appropriate action. We will introduce the concept and intuition of TDL in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing TDL</h1>
                </header>
            
            <article>
                
<p>TDL was introduced by the father of RL himself, Dr. Richard Sutton, in 1988. Sutton had developed the method as an improvement to MC/DP but, as we will see, the method itself led to the development of Q-learning by Chris Watkins in 1989. The method itself is model-free and does not require episode completion before an agent learns. This makes this method very powerful for exploring unknown environments in real time, as we will see.</p>
<p>Before we get into discovering the updated mathematics to this approach, it may be helpful to look at the backup diagrams of all of the methods covered so far in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bootstrapping and backup diagrams</h1>
                </header>
            
            <article>
                
<p>TDL can learn during an episode by approximating the updated value function given previous experience. This allows the algorithm to learn while it is in an episode and hence make corrections as needed. To understand the differences further, let's review a composite of the backup diagrams for DP, MC, and TD in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-489 image-border" src="assets/730cb028-e6f9-469a-9d2e-5fdb56020163.png" style="width:59.67em;height:36.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Backup diagrams for DP, MC, and TDL</div>
<p>The diagram was taken from <em>An Introduction to Reinforcement Learning</em> by Barto and Sutton (2018). In the diagram, you can see our previous two methods, DP and MC, as well as TDL. The shaded area (red or black) denotes the algorithm's learning space. That is the area the agent will need to explore before being able to update its value function and, hence, policy. Notice how, as the diagrams progress from DP to TDL, the shaded area becomes smallerâ€”that is, for each advancing algorithm, that agent needs to explore less and less area initially or during an episode before learning. As we will see, this allows an agent to learn before even finishing an episode.</p>
<p>Before we look at the code, we should take a look at how this new approach revises the math of our value function in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying TD prediction</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we will explore methods for allowing an algorithm to predict and control an agent to complete a task. Prediction and control are at the heart of RL, and previously we had both methods separate. That is, they either ran before (DP) or after (MC). Now, for an agent to learn in real time, we need an online update rule that will update the value function after a designated time step. In TDL, this is called the TD update rule.</p>
<p>The rule is shown here in equation form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/956df4fc-01db-4559-b254-7ad3413a6414.png" style="width:20.42em;height:1.25em;"/></p>
<p>In the previous equation, we have the following:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/7fc168a7-ace6-409c-97eb-86a90e1489e8.png" style="width:2.92em;height:1.33em;"/>: The value function for the current state</li>
<li><img class="fm-editor-equation" src="assets/023a6a65-0a6a-4a35-9459-bdc579e13a2e.png" style="width:1.00em;height:0.92em;"/>: <span>Alpha, the learning rate</span></li>
<li><img style="font-size: 1em;color: #333333;width:2.42em;height:1.25em;" class="fm-editor-equation" src="assets/373c9f31-ae6e-46bf-a01c-72958e5a7f97.png"/>: <span>The reward of the next state</span></li>
<li><span><img class="fm-editor-equation" src="assets/c76bca53-77df-4b0b-b907-7f32c82761b1.png" style="width:0.83em;height:1.25em;"/>: A discount factor</span></li>
<li><span><img class="fm-editor-equation" src="assets/14dc93fa-c8d1-42f7-83a1-cfc226215e99.png" style="width:4.08em;height:1.42em;"/>: The value of the next state</span></li>
</ul>
<p>Hence, we can say that the value of the current state is equal to the value of the current state plus alpha, times the summation of the next reward, plus a discount factor, gamma, times the difference between the next state value and the current state value.</p>
<p>To understand this better, let's look at a code example in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TD(0) or one-step TD</h1>
                </header>
            
            <article>
                
<p>One thing we should identify before getting too far ahead is that the method we look at here is for one-step TD or what we refer to as TD(0). Remember, as programmers, we start counting at 0, so TD(0) essentially means TD one-step. We will look at multiple-step TD in <a href="3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml">Chapter 5</a>, <em>Exploring SARSA</em>.</p>
<p>For now, though, we will look at an example of using one-step TD in the next exercise:</p>
<ol>
<li>Open the <kbd>Chapter_4_1.py</kbd> source code example, as seen in the following code:</li>
</ol>
<pre style="padding-left: 60px">import numpy as np<br/>from tqdm import tqdm<br/>import random<br/><br/>gamma = 0.5 <br/>rewardSize = -1<br/>gridSize = 4<br/>alpha = 0.5 <br/>terminations = [[0,0], [gridSize-1, gridSize-1]]<br/>actions = [[-1, 0], [1, 0], [0, 1], [0, -1]]<br/>episodes = 10000<br/><br/>V = np.zeros((gridSize, gridSize))<br/>returns = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}<br/>deltas = {(i, j):list() for i in range(gridSize) for j in range(gridSize)}<br/>states = [[i, j] for i in range(gridSize) for j in range(gridSize)]<br/><br/>def generateInitialState():<br/>    initState = random.choice(states[1:-1])<br/>    return initState<br/><br/>def generateNextAction():<br/>    return random.choice(actions)<br/><br/>def takeAction(state, action):<br/>  if list(state) in terminations:<br/>    return 0, None<br/>  finalState = np.array(state)+np.array(action)<br/>    <br/>  if -1 in list(finalState) or gridSize in list(finalState):<br/>    finalState = state<br/>  return rewardSize, list(finalState)<br/><br/>for it in tqdm(range(episodes)):    <br/>  state = generateInitialState()<br/>  while True:<br/>    action = generateNextAction()<br/>    reward, finalState = takeAction(state, action) <br/>    if finalState is None:<br/>      break<br/>        <br/>  before =  V[state[0], state[1]]<br/>  V[state[0], state[1]] += alpha*(reward + gamma*V[finalState[0], finalState[1]] - V[state[0], state[1]])<br/>  deltas[state[0], state[1]].append(float(np.abs(before-V[state[0], state[1]])))        <br/>  state = finalState<br/><br/>print(V)</pre>
<ol start="2">
<li>This is a straight code example demonstrating how value updates work and that uses no RL environment. The first section we will focus on is where we initialize our parameters just after the imports. Here, we initialize the learning rate, <kbd>alpha</kbd> (<kbd>0.5</kbd>); discount factor, <kbd>gamma</kbd> (<kbd>0.5</kbd>); the size of the environment, <kbd>gridSize</kbd> (<kbd>4</kbd>); a list of state <kbd>terminations</kbd>; list of <kbd>actions</kbd>; and finally, <kbd>episodes</kbd>. Actions represent the movement vector and terminations represent the grid square where an episode will terminate.</li>
<li>Next, we initialize the value function, <kbd>V</kbd>, with all zeros using the <kbd>numpy</kbd> function, <kbd>zeros</kbd>. We then create three lists, <kbd>returns</kbd>, <kbd>deltas</kbd>, and <kbd>states</kbd> using Python list comprehensions. These lists hold values for later retrieval and notice how this relates to a DP technique.</li>
<li>Then, we define some utility functions, <kbd>generateInitialState</kbd>, <kbd>generateNextAction</kbd>, and <kbd>takeAction</kbd>. The first two functions are self-explanatory but let's focus on the <kbd>takeAction</kbd> function:</li>
</ol>
<pre style="padding-left: 60px"><span>def takeAction(state, action):<br/></span><span> if list(state) in terminations:<br/></span><span>   return 0, None<br/></span><span> finalState = np.array(state)+np.array(action)<br/> </span><span>if -1 in list(finalState) or gridSize in list(finalState):<br/>   </span><span>f</span><span>inalState</span><span> = state<br/></span><span> return rewardSize, list(finalState)</span></pre>
<ol start="5">
<li>The preceding function takes the current <kbd>state</kbd> and <kbd>action</kbd> as inputs. It then determines whether the current state is terminal; if it is, it returns. Otherwise, it calculates the next state using simple vector math to get <kbd>finalState</kbd>.</li>
<li>Then, we enter the <kbd>for</kbd> loop that starts episodic training. Note that, although the agent explores the environment episodically, since the environment has a beginning and an end, it still learns temporally. That is, it learns after every time step. The <kbd>tqdm</kbd> library is a helper enumerator, which prints a status bar when we run a <kbd>for</kbd> loop.</li>
<li>The first thing that happens at the start of the <kbd>for</kbd> loop is the agent's state is initialized randomly. After that, it enters a <kbd>while</kbd> loop that runs one entire episode. Most of this code is self-explanatory, aside from the implementation of the value update equation, as shown here:</li>
</ol>
<pre style="padding-left: 60px"><span>V[state[0], state[1]] += alpha*(reward + gamma*V[finalState[0], finalState[1]] - V[state[0], state[1]])</span></pre>
<ol start="8">
<li>The block of code is an implementation of the previous value update function. Notice the use of the learning rate, <kbd>alpha</kbd>, and discount factor, <kbd>gamma</kbd>.</li>
</ol>
<ol start="9">
<li>Run the code as you normally would and notice the output of the <kbd>Value</kbd> function:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-490 image-border" src="assets/19cbf135-fb34-45ec-8d9c-867c47f23945.png" style="width:84.83em;height:20.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Running example in Chapter_4_1.py</div>
<p>Now, the function is less than optimal and that has more to do with the training or hyperparameters we used. We will look at the importance of tuning hyperparameters in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters</h1>
                </header>
            
            <article>
                
<p>The collection of training parameters that we investigated at the top of the last code example are called hyperparameters, so named to differentiate them from normal parameters or weights we use in deep learning. We have yet to look at deep learning in detail, but it is important to understand why they are called <strong>hyperparameters</strong>. Previously, we played around with the concept of a learning rate and discount factor but now we need to formalize them and understand their effect across methods and environments.</p>
<p>In our last example, both the learning rate (alpha) and discount factor (gamma) were set to .5. What we need to understand is what effect these parameters have on training. Let's open up sample code <kbd>Chapter_4_2.py</kbd> and follow the next exercise:</p>
<ol>
<li><kbd>Chapter_4_2.py</kbd> is almost identical to the previous example, aside from a few minor differences. The first of these is we <kbd>import matplotlib</kbd> here to be able to view some results later.</li>
<li>Recall, you can install <kbd>matplotlib</kbd> from a Python console with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip install matplotlib</strong></pre>
<ol start="3">
<li>We use <kbd>matplotlib</kbd> to render the results of our training efforts. As we progress through this book, we will look at more advanced methods later.</li>
<li>Next, we see that the hyperparameters, alpha and gamma, have been modified to values of <kbd>0.1</kbd>:</li>
</ol>
<pre style="padding-left: 60px">gamma = 0.1 <br/>rewardSize = -1<br/>gridSize = 4<br/>alpha = 0.1 </pre>
<ol start="5">
<li>Now, as you often refine training, you likely will only want to modify a single parameter at a time. This will give you more control and understanding of the effect a parameter may have.</li>
<li>Finally, at the end of the file, we see the code to output the training values or change in training values. Recall the list we created earlier called <kbd>deltas</kbd>. Captured in this list are all of the deltas or changes made during training. This can be extremely useful to visualize, as we'll see:</li>
</ol>
<pre style="padding-left: 60px">plt.figure(figsize=(20,10))<br/>all_series = [list(x)[:50] for x in deltas.values()]<br/>for series in all_series:<br/>    plt.plot(series)<br/><br/>plt.show()<br/><br/>print(V)</pre>
<ol start="7">
<li>This code just loops through the list of changes made during training for each episode. What we expect is that over training, the amount of change will reduce. Reducing the amount of change allows the agent to converge to some optimal value function and hence policy later.</li>
<li>Run the code as you normally would and notice how the output of the value function has changed substantially but we can also see how the agent's training progressed:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="alignnone size-full wp-image-1045 image-border" src="assets/734a9474-69fe-4ded-a419-51551aa206c1.png" style="width:82.67em;height:58.58em;"/><br/>
<br/>
Example output from Chapter_4_2.py</div>
<p>On the plot, you can now see how the training converges over time. The plot is represented over the number of steps in an episode. Notice how the amount of change or delta is greater to the left of the plot, with fewer steps in an episode, and then it decreases over time with more steps. This convergence assures us that the agent is indeed learning using the provided hyperparameters.</p>
<div class="packt_tip">Tuning hyperparameters is foundational to deep learning and deep reinforcement learning. Many consider the tuning practice to be where all of the work is done and, for the most part, that is true. You may often spend days, weeks, or months tuning hyperparameters of a single network model.</div>
<p>Feel free to explore playing with the hyperparameters on your own and see what effect each has on training convergence. In the next section, we look at how TD is combined with Q-learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying TDL to Q-learning</h1>
                </header>
            
            <article>
                
<p>Q-learning is considered one of the most popular and often <span>used</span> foundational RL methods . The method itself was developed by Chris Watkins in 1989 as part of his thesis, <em>Learning from Delayed Rewards</em>. Q-learning or rather Deep Q-learning, which we will cover in <a href="a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml">Chapter 6</a>, <em>Going Deep with DQN</em>, became so popular because of its use by DeepMind (Google) to play classic Atari games better than a human. What Watkins did was show how an update could be applied across state-action pairs using a learning rate and discount factor gamma.</p>
<p>This improved the update equation into a Q or quality of state-action update equation, as shown in the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4cc882f2-cb7a-4fb0-b25b-d2e7fd76b4d3.png" style="width:30.92em;height:1.83em;"/></p>
<p>In the previous equation, we have the following:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/8b4a101d-da73-40ed-adae-292972e81996.png" style="width:5.67em;height:1.33em;"/>The current state-action quality being updated</li>
<li><img style="font-size: 1em;color: #333333;width:5.83em;height:1.08em;" class="fm-editor-equation" src="assets/a570960a-0217-47dc-9587-489391546791.png"/>The <span>learning rate</span></li>
<li><img style="font-size: 1em;color: #333333;width:3.25em;height:1.08em;" class="fm-editor-equation" src="assets/54152573-c230-41cb-bbe8-ba63f0480176.png"/><span>The reward for the next state</span></li>
<li><img style="font-size: 1em;color: #333333;width:7.33em;height:1.00em;" class="fm-editor-equation" src="assets/04ee51c9-7562-452e-ad22-3a08624e7d44.png"/><span>Gamma, the discount factor</span></li>
<li><img style="font-size: 1em;color: #333333;width:8.42em;height:1.83em;" class="fm-editor-equation" src="assets/2334cd52-d080-486f-b3e4-82e33576e28b.png"/><span>Take the max best or greedy action</span></li>
</ul>
<p>This equation allows us to update state-action pairs based on learned future state-action pairs. It also does not require a model as the algorithm explores by trial and error and can learn during an episode since updates are run during the episode.</p>
<p>This method can <span>now</span> <span>solve the temporal credit assignment problem and we will look at a code example in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring TD(0) in Q-learning</h1>
                </header>
            
            <article>
                
<p>TDL for first step or TD(0) then essentially simplifies to Q-learning. To do a full comparison of this method against DP and MC, we will first revisit the FrozenLake environment from Gym. Open up example code <kbd>Chapter_4_4.py</kbd> and follow the exercise:</p>
<ol>
<li>The full listing of code is too large to show. Instead, we will review the code in sections starting with the imports:</li>
</ol>
<pre style="padding-left: 60px">from os import system, name<br/>from time import sleep<br/>import numpy as np<br/>import gym<br/>import random<br/>from tqdm import tqdm</pre>
<ol start="2">
<li>We have seen all of these imports before, so there is nothing new here. Next, we cover the initialization of the environment and outputting some initial environment variables:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make("FrozenLake-v0")<br/>env.render()<br/>action_size = env.action_space.n<br/>print("Action size ", action_size)<br/>state_size = env.observation_space.n<br/>print("State size ", state_size)</pre>
<ol start="3">
<li>There's nothing new here either. Next, we introduce the concept of a Q table or quality table that now defines our policy in terms of <kbd>state-action</kbd> pairs. We set this to an equal quality for each <kbd>state-action</kbd> pair by dividing one by the total number of actions in a state (<kbd>action-size</kbd>):</li>
</ol>
<pre style="padding-left: 60px">qtable = np.ones((state_size, action_size))/action_size <br/>print(qtable)</pre>
<ol start="4">
<li>Next, we can see a section of hyperparameters:</li>
</ol>
<pre style="padding-left: 60px">total_episodes = 50000 <br/>total_test_episodes = 100<br/>play_game_test_episode = 1000<br/>max_steps = 99 <br/>learning_rate = 0.7 <br/>gamma = 0.618 </pre>
<ol start="5">
<li>There are two new parameters here called <kbd>play_game_test_episode</kbd> and <kbd>max_steps</kbd>. <kbd>max_steps</kbd> determine the number of maximum steps our algorithm may run in an episode. We do this to limit the agent from getting into possible endless loops. <kbd>play_game_test_episode</kbd> sets the episode number to show a preview of the agent playing based on the current best Q table.</li>
<li>Next, we introduce an entirely new set of parameters that have to deal with exploration and exploitation:</li>
</ol>
<pre style="padding-left: 60px">epsilon = 1.0 <br/>max_epsilon = 1.0<br/>min_epsilon = 0.01<br/>decay_rate = 0.01 </pre>
<ol start="7">
<li>Recall that we discussed the exploration versus exploitation dilemma in RL in <a href="5553d896-c079-4404-a41b-c25293c745bb.xhtml">Chapter 1</a>, <em>Understanding Rewards-Based Learning</em>. In this section, we introduce <kbd>epsilon</kbd>, <kbd>max_epsilon</kbd>, <kbd>min_epsilon</kbd>, and <kbd>decay_rate</kbd>. These hyperparameters control the exploration rate of the agent while it explores, where <kbd>epsilon</kbd> is the current probability of an agent exploring during a time step. Maximum and minimum epsilon represent the limits of how much or little the agent explores, with <kbd>decay_rate</kbd> controlling how much the <kbd>epsilon</kbd> value decays from time step to step.</li>
<li>Understanding the exploration versus exploitation dilemma is essential to RL so we will pause here and let you run the example. The following is an example of the agent playing on the FrozenLake environment:</li>
</ol>
<div class="CDPAlignCenter CDPAlign packt_figref"><img class="aligncenter size-full wp-image-492 image-border" src="assets/8ef78387-241b-43d5-87a5-0d066bee52dd.png" style="width:34.00em;height:13.33em;"/><br/>
Example output from Chapter_4_4.py</div>
<p>Watch how the agent plays the game in the later episodes, after 40,000, and you will see that the agent can move around the holes and find the goal in short order, usually. The reason it may not do this has to do with exploration/exploitation and something we will review again in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploration versus exploitation revisited</h1>
                </header>
            
            <article>
                
<p>For several chapters now, we have always assumed our agent to be greedy. That is, it always chooses the best action given a choice in policy. However, as we have seen, this does not always provide the best path to optimum reward. Instead, what we find is that by allowing an agent to randomly explore early and then over time reduce the chance of exploration, there is a substantial improvement in learning. Except, if the environment is too large or complex, an agent may need more exploration time compared to a much smaller environment. If we maintained high exploration in a small environment, our agent would just waste time exploring. This is the trade-off you need to balance and it is often tied to the task required to perform.</p>
<p>The method of exploration we are using in this example is called e-greedy or epsilon-greedy. It's so named because we start out greedy with high epsilon and then decrease it over time. There are several exploration methods we may use and a list of the more common versions and a description of each is shown here:</p>
<ul>
<li><strong>Random</strong>: This is the always random method, which can be an effective baseline test to perform on a new environment.</li>
<li><strong>Greedy</strong>: This is always taking the greedy or best action in a state. As we have seen, this can have bad consequences.</li>
<li><strong>E-greedy</strong>: Epsilon greedy allows for a method to balance exploration rate over time by decreasing epsilon (exploration rate) by a factor during each time step or episode.</li>
<li><strong>Bayesian or Thompson sampling</strong>: This uses statistics and probability to best choose an action over a random distribution of sampled actions. Essentially, the action is chosen from across action distributions. For example, if a state has a bag of actions to choose from, each bag then also stores previous rewards for each action. A new action is chosen by taking a random choice from each action bag and comparing it to all of the other choices. The best action, the one giving the highest value, is selected. We don't actually store all of the rewards for all previous actions but, rather, we determine the sample distribution that describes these returned rewards.</li>
</ul>
<div class="packt_tip">If the concepts of statistics and probability we just discussed are foreign to you, then you should engage in some free online learning of these topics. These concepts will be covered over and over again in-depth in later chapters.</div>
<p>There are other methods that provide additional options for strategies on how to pick the best action for a Q-learner. For now, though, we will stick to e-greedy as it is relatively simple to implement and is quite effective.</p>
<p>Now, if you go back to example <kbd>Chapter_4_4.py</kbd> and watch closely, you will see the agent may reach the goal, or it may not. In fact, the FrozenLake environment is more treacherous than we give it credit for. What that means is that rewards in that environment are more sparse and it often takes considerably longer to train with trial and error techniques. Instead, this type of learning method performs better in environments with continuous rewards. That is, when an agent receives a reward during an episode and not just at termination.</p>
<p>Fortunately, Gym has plenty of environments we can play with that will allow us more continuous rewards and we will explore a fun example in the next section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Teaching an agent to drive a taxi</h1>
                </header>
            
            <article>
                
<p>OpenAI Gym has plenty of fun environments that allow us to switch out and test new environments very easily. This, as we have seen, allows us to compare results of algorithms far easier. However, as we have also seen, there are limitations to various algorithms, and the new environment we explore in this section introduces the limitation of time. That is, it places a time limit on the agent as part of the goal. In doing this, our previous algorithms, DP and MC, become unable to solve such a problem, which makes this a good example to also introduce time-based or time-critical rewards.</p>
<p>What better way to introduce time-dependent learning than to think of a time-dependent task? There are plenty of tasks, but one that works well is an example of a taxi. That is, the agent is a taxi driver and must pick up passengers and drop them off at their correct destinations but promptly. In the Gym Taxi-v2 environment, it is the goal of the agent to pick up a passenger at a location and then drop them off at a goal. The agent gets a reward of +20 points for a successful drop off and -1 point for every time step it takes. Hence, the agent needs to pick up and drop off passengers as quickly as possible.</p>
<p>An example of an agent playing this environment is shown here:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-943 image-border" src="assets/f7020394-2df8-466f-bbac-28cb0be8e588.png" style="width:27.92em;height:14.92em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Example output from the Taxi-v2 Gym environment</div>
<p>In the screenshot, the car is green, meaning it has picked up a passenger from one of the symbols. The current goal is for the agent to drop off the passenger at their designated goal (symbol) highlighted. Let's now go back to the code, this time, to <kbd>Chapter_4_5.py</kbd>, which is an updated version of our last example now using the Taxi-v2 environment.</p>
<p>After you have the code opened, follow the exercise:</p>
<ol>
<li>This code example is virtually identical to <kbd>Chapter_4_4.py</kbd>, aside from the initialization of the environment shown:</li>
</ol>
<pre style="padding-left: 60px">env = gym.make("Taxi-v2")</pre>
<ol start="2">
<li>We will use all of the same hyperparameters as before, so there's no need to look at those again. Instead, skip down to the <kbd>play_game</kbd> function, as shown in the following code block:</li>
</ol>
<pre style="padding-left: 60px">def play_game(render_game):<br/>  state = env.reset()<br/>  step = 0<br/>  done = False<br/>  total_rewards = 0 <br/>  for step in range(max_steps):<br/>    if render_game:<br/>      env.render()<br/>      print("**...*****************")<br/>      print("EPISODE ", episode)<br/>      sleep(.5)<br/>      clear()<br/>   <strong>action = np.argmax(qtable[state,:])</strong><br/>   new_state, reward, done, info = env.step(action)<br/>   total_rewards += reward<br/>   if done:<br/>     rewards.append(total_rewards)<br/>     if render_game:<br/>       print ("Score", total_rewards)<br/>     break<br/>   state = new_state<br/> return done, state, step, total_rewards</pre>
<ol start="3">
<li>The <kbd>play_game</kbd> function essentially uses the <kbd>qtable</kbd> list, which is essentially the generated policy of qualities for state-action pairs. The code should be comfortable now and one detail to notice is how the agent selects an action from the <kbd>qtable</kbd> list using the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>action = np.argmax(qtable[state,:])</span></pre>
<ol start="4">
<li>The <kbd>play_game</kbd> function here takes the role of our agent test function previously. This function will allow you to see the agent play the game as it progresses through training. This is accomplished by setting <kbd>render_game</kbd> to <kbd>play_game</kbd> to <kbd>True</kbd>. Doing this allows you to visualize the agent playing an episode of the game.</li>
</ol>
<div class="packt_tip">You will often want to watch how your agent is training, at least initially. This can provide you with clues as to possible errors in your implementation or the agent finding possible cheats in new environments. We have found agents to be very good cheaters or at least be able to find cheats quite readily.</div>
<ol start="5">
<li>Next, we jump down to the next for loop that iterates through the training episodes and trains the <kbd>qtable</kbd>. When a threshold set by <kbd>play_game_test_episode</kbd> of episodes has elapsed, we allow the agent to play a visible game. Doing this allows us to visualize the overall training progress. However, it is important to remember that this is only a single episode and the agent could be doing extensive exploration. So, it is important to remember that, when watching the agent, they may just occasionally randomly explore. The code shows how we loop through the episodes:</li>
</ol>
<pre style="padding-left: 60px">for episode in tqdm(range(total_episodes)):<br/>  state = env.reset()<br/>  step = 0<br/>  done = False<br/>  if episode % play_game_test_episode == 0:<br/>    play_game(True)<br/>  for step in range(max_steps):<br/>    exp_exp_tradeoff = random.uniform(0,1)<br/>    if exp_exp_tradeoff &gt; epsilon:<br/>      action = np.argmax(qtable[state,:])<br/>    else:<br/>      action = env.action_space.sample()<br/>   new_state, reward, done, info = env.step(action)<br/>   qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])<br/>   state = new_state<br/>   if done == True:<br/>     break<br/>  epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)</pre>
<ol start="6">
<li>First, inside the episode loop, we handle the exploration-exploitation dilemma by sampling a random value and then comparing it to epsilon. If it is greater than the greedy action, it is selected; otherwise, a random exploratory action is selected, as shown in the code:</li>
</ol>
<pre style="padding-left: 60px"><span>exp_exp_tradeoff = random.uniform(0,1)<br/></span><span>if exp_exp_tradeoff &gt; epsilon:<br/>  </span><span>action = np.argmax(qtable[state,:])<br/></span><span>else:<br/>  </span><span>action = env.action_space.sample()</span></pre>
<ol start="7">
<li>Then, the next line is where the step is taken with the selected action. After that, <kbd>qtable</kbd> is updated based on the previous Q-learning equation. There is a lot going on in this single line of code, so make sure you understand it:</li>
</ol>
<pre style="padding-left: 60px">qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])</pre>
<ol start="8">
<li>After that, we check whether the episode is done with the <kbd>done</kbd> flag. If it is, we terminate and continue with another episode. Otherwise, we update the value of <kbd>epsilon</kbd> with the following code:</li>
</ol>
<pre style="padding-left: 60px">epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)</pre>
<ol start="9">
<li>Finally, the remainder of the code is shown:</li>
</ol>
<pre style="padding-left: 60px">env.reset()<br/>print(qtable)<br/><br/>for episode in range(total_test_episodes):<br/>  done, state, step, total_rewards = play_game(False)<br/><br/>env.close()<br/>print ("Score over time: " + str(sum(rewards)/total_test_episodes))</pre>
<ol start="10">
<li>The last piece of code resets and then tests the environment with the trained <kbd>qtable</kbd> for <kbd>total_test_episodes</kbd> and then outputs the average score or reward for an episode.</li>
</ol>
<ol start="11">
<li>Finally, run the code as you normally would and observe the output carefully. Pay particular attention to how the taxi picks up and drops off passengers in later episodes. Sample output from the training is shown here:</li>
</ol>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-494 image-border" src="assets/7ca331b0-d030-4496-a285-75ec1f378d60.png" style="width:84.83em;height:39.50em;"/><br/>
<br/>
The final output from sample Chapter_4_5.py</div>
<p>In this example, you will clearly see how the agent progresses in training from doing nothing to picking up and dropping off passengers in short order. The agent will actually perform remarkably better in this environment than in other apparently simpler environments such as the FrozenLake. This has more to do with the method of learning and the related task and suggests that we need to be careful as to what methods we use for which problems. You may find, in some cases, that certain advanced algorithms perform poorly on simple problems and that the converse may happen. That is, simpler algorithms such as Q-learning when paired with other technologies can become far more powerful as we will see in <a href="42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml">Chapter 6</a>, <em>Going Deeper with DQN</em>.</p>
<p>In the last section of this chapter, we look at how the previous Q-learning method could be improved.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Running off- versus on-policy</h1>
                </header>
            
            <article>
                
<p>We covered the terms on- and off-policy previously when we looked at MC training in <a href="5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml">Chapter 2</a>, <em>Monte Carlo Methods</em>. Recall that the agent didn't update its policy until after an episode. Hence, this defines the TD(0) <span>method of learning</span> <span>in the last example as an off-policy learner. In our last example, it may seem that the agent is learning online but it still, in fact, trains a policy or Q table externally. That is, the agent needs to build up a policy before it can learn to make decisions and play the game. Ideally, we want our agent to learn or improve its policy as it plays through an episode. After all, we don't learn</span> <span>offline</span> <span>nor does any other biological animal. Instead, our goal will be to understand how an agent can learn using on-policy learning. On-policy learning will be covered in</span> <a href="3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml"/><a href="3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml">Chapter 5</a><span>,</span> <em>Exploring SARSA</em><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>As we progress through this book, the exercises at the end of each chapter will be more directed toward providing you with agent training experience. Training RL agents not only requires a fair amount of patience but also intuition on how to spot whether something is wrong or right. That only comes with training experience, so use the following exercises to learn that:</p>
<ol>
<li>Open example <kbd>Chapter_4_2.py</kbd> and change the <kbd>gridSize</kbd> variable to see what effect this has on convergence.</li>
<li>Open example <kbd>Chapter_4_2.py</kbd> and tune the hyperparameters for alpha and gamma. Try to find the optimum values for both. This will require you to run the example multiple times.</li>
<li>Open example <kbd>Chapter_4_2.py</kbd> and change the number of episodes, up or down. See what effect a large number of episodes, such as 100,000 or 1,000,000, has on training.</li>
<li>Tune the <kbd>learning_rate</kbd> and <kbd>gamma</kbd> hyperparameters in example <kbd>Chapter_4_4.py</kbd>. Can they be improved upon?</li>
<li>Adjust the exploration (<kbd>epsilon</kbd>, <kbd>max_epsilon</kbd>, <kbd>min_epsilon</kbd>, and <kbd>decay_rate</kbd>) hyperparameters from example <kbd>Chapter_4_4.py</kbd>. How does changing these values affect training performance or lack thereof?</li>
<li>Tune the <kbd>learning_rate</kbd> and <kbd>gamma</kbd> hyperparameters in example <kbd>Chapter_4_5.py</kbd>. Can they be improved upon?</li>
</ol>
<ol start="7">
<li>Adjust the exploration (<kbd>epsilon</kbd>, <kbd>max_epsilon</kbd>, <kbd>min_epsilon</kbd>, and <kbd>decay_rate</kbd>) hyperparameters from example <kbd>Chapter_4_5.py</kbd>. <span>How does changing these values affect training performance or lack thereof?</span></li>
<li>Add the ability to track the deltas or change in Q values during training to examples <kbd>Chapter_4_4.py</kbd> or <kbd>Chapter_4_5.py</kbd>. Recall how we tracked and output the deltas to a plot in example <kbd>Chapter_4_2.py</kbd>.</li>
<li>Add the ability to render plots of the training performance to example <kbd>Chapter_4_4.py</kbd> or <kbd>Chapter_4_5.py</kbd>. This will require you to complete the previous exercise as well.</li>
<li>Use the code in example <kbd>Chapter_4_5.py</kbd> and try the Q-learner on other environments. The Mountain Car or Cart Pole environments from Gym are interesting and ones we will be exploring soon.</li>
</ol>
<p>At this stage in your RL training career, it is important to get how hyperparameters work. The right or wrong hyperparameters can make or truly break an experiment. This leaves you with two options: read a lot of boring math-induced papers or just do it. Since this is a hands-on book, we are expecting you prefer the latter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how temporal difference learning, the third thread of RL, combined to develop TD(0) and Q-learning. We did that by first exploring the temporal credit assignment problem and how it differed from the credit assignment problem. From that, we learned how TD learning works and how TD(0) or first step TD can be reduced to Q-learning.</p>
<p>After that, we again played on the FrozenLake environment to understand how the new algorithm compared to our past efforts. Using model-free off-policy Q-learning allowed us to tackle the more difficult Taxi environment problem. This is where we learned how to tune hyperparameters and finally looked at the difference between off- and on-policy learning. In the next chapter, we continue where we left off with on- versus off-policy as we explore SARSA.</p>


            </article>

            
        </section>
    </body></html>