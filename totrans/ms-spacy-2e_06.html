<html><head></head><body>
  <div id="_idContainer082">
   <h1 class="chapter-number" id="_idParaDest-88">
    <a id="_idTextAnchor087">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     6
    </span>
   </h1>
   <h1 id="_idParaDest-89">
    <a id="_idTextAnchor088">
    </a>
    <span class="koboSpan" id="kobo.2.1">
     Utilizing spaCy with Transformers
    </span>
   </h1>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.3.1">
      Transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.4.1">
     are the
    </span>
    <a id="_idIndexMarker275">
    </a>
    <span class="koboSpan" id="kobo.5.1">
     latest hot topic in NLP.
    </span>
    <span class="koboSpan" id="kobo.5.2">
     The goal of this chapter is to learn how to use transformers to improve the performance of trainable components
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.6.1">
      in spaCy.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.7.1">
     First, you will learn about transformers and transfer learning.
    </span>
    <span class="koboSpan" id="kobo.7.2">
     Next, you’ll learn more about spaCy trainable components and how to train a component, introducing spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.8.1">
      config.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.9.1">
     files
    </span>
    <a id="_idIndexMarker276">
    </a>
    <span class="koboSpan" id="kobo.10.1">
     and spaCy’s CLI.
    </span>
    <span class="koboSpan" id="kobo.10.2">
     Then, you will learn about the architectural details of the
    </span>
    <a id="_idIndexMarker277">
    </a>
    <span class="koboSpan" id="kobo.11.1">
     commonly used Transformer architecture –
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.12.1">
      Bidirectional Encoder
     </span>
    </strong>
    <strong class="bold">
     <span class="koboSpan" id="kobo.13.1">
      Representations from Transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.14.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.15.1">
      BERT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.16.1">
     ) and its successor, RoBERTa.
    </span>
    <span class="koboSpan" id="kobo.16.2">
     Finally, you’ll train the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.17.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.18.1">
     component
    </span>
    <a id="_idIndexMarker278">
    </a>
    <span class="koboSpan" id="kobo.19.1">
     to classify texts using a transformer layer to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.20.1">
      improve accuracy.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.21.1">
     By the end of this chapter, you will be able to prepare data for training and fine-tune your own spaCy components.
    </span>
    <span class="koboSpan" id="kobo.21.2">
     Because of the way spaCy is designed; while doing that, you’ll be following software engineering best practices.
    </span>
    <span class="koboSpan" id="kobo.21.3">
     You’ll also have a solid basis of how transformers
    </span>
    <a id="_idIndexMarker279">
    </a>
    <span class="koboSpan" id="kobo.22.1">
     work, which will be useful when we work with
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.23.1">
      large
     </span>
    </strong>
    <strong class="bold">
     <span class="koboSpan" id="kobo.24.1">
      language models
     </span>
    </strong>
    <span class="koboSpan" id="kobo.25.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.26.1">
      LLMs
     </span>
    </strong>
    <span class="koboSpan" id="kobo.27.1">
     ) in
    </span>
    <a href="B22441_07.xhtml#_idTextAnchor102">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.28.1">
        Chapter 7
       </span>
      </em>
     </span>
    </a>
    <span class="koboSpan" id="kobo.29.1">
     .
    </span>
    <span class="koboSpan" id="kobo.29.2">
     You’ll be able to build state-of-the-art NLP pipelines with just a few lines of code with the power of Transformer models and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.30.1">
      transfer learning.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.31.1">
     In this chapter, we’re going to cover the following
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.32.1">
      main topics:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.33.1">
      Model training and transfer learning
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.34.1">
       with spaCy
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.35.1">
      Classifying text with
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.36.1">
       spaCy pipelines
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.37.1">
      Working with spaCy
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.38.1">
        config.cfg
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.39.1">
       files
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.40.1">
      Preparing training data to fine-tune models
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.41.1">
       with spaCy
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.42.1">
      Using Hugging Face’s Transformer for downstream tasks
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.43.1">
       with spaCy
      </span>
     </span>
    </li>
   </ul>
   <h1 id="_idParaDest-90">
    <a id="_idTextAnchor089">
    </a>
    <span class="koboSpan" id="kobo.44.1">
     Technical requirements
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.45.1">
     The dataset and the chapter code can be found
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.46.1">
      at
     </span>
    </span>
    <a href="https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.47.1">
       https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.48.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.49.1">
     We will use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.50.1">
      pandas
     </span>
    </strong>
    <span class="koboSpan" id="kobo.51.1">
     library to manipulate the datasets and also install the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.52.1">
      spacy-transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.53.1">
     library to work with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.54.1">
      transformer
     </span>
    </strong>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.55.1">
      spaCy component.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-91">
    <a id="_idTextAnchor090">
    </a>
    <span class="koboSpan" id="kobo.56.1">
     Transformers and transfer learning
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.57.1">
     A milestone in NLP happened in 2017 with the release of the research paper
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.58.1">
      Attention Is All You Need
     </span>
    </em>
    <span class="koboSpan" id="kobo.59.1">
     , by Vaswani et al.
    </span>
    <span class="koboSpan" id="kobo.59.2">
     (
    </span>
    <a href="https://arxiv.org/abs/1706.03762">
     <span class="koboSpan" id="kobo.60.1">
      https://arxiv.org/abs/1706.03762
     </span>
    </a>
    <span class="koboSpan" id="kobo.61.1">
     ), which introduced a brand-new machine learning idea and architecture –
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.62.1">
      transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.63.1">
     .
    </span>
    <span class="koboSpan" id="kobo.63.2">
     Transformers
    </span>
    <a id="_idIndexMarker280">
    </a>
    <span class="koboSpan" id="kobo.64.1">
     in NLP is a fresh idea that aims to solve sequential modeling tasks and
    </span>
    <a id="_idIndexMarker281">
    </a>
    <span class="koboSpan" id="kobo.65.1">
     targets some problems introduced by
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.66.1">
      Long Short-Term Memory
     </span>
    </strong>
    <span class="koboSpan" id="kobo.67.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.68.1">
      LSTM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.69.1">
     ) architecture.
    </span>
    <span class="koboSpan" id="kobo.69.2">
     Here’s how the paper explains how
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.70.1">
      transformers work:
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.71.1">
     “
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.72.1">
      The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence aligned RNNs
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.73.1">
       or convolution.
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.74.1">
      ”
     </span>
    </span>
   </p>
   <p>
    <strong class="bold">
     <span class="koboSpan" id="kobo.75.1">
      Transduction
     </span>
    </strong>
    <strong class="bold">
     <a id="_idIndexMarker282">
     </a>
    </strong>
    <span class="koboSpan" id="kobo.76.1">
     in this context means transforming input to output by transforming input words and sentences into vectors.
    </span>
    <span class="koboSpan" id="kobo.76.2">
     Typically, a transformer is trained on a huge corpus.
    </span>
    <span class="koboSpan" id="kobo.76.3">
     Then, in our downstream tasks, we use these vectors as they carry information regarding the word semantics, sentence structure, and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.77.1">
      sentence semantics.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.78.1">
     Before transformers, the cool kid in the NLP world was the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.79.1">
      word vector
     </span>
    </strong>
    <span class="koboSpan" id="kobo.80.1">
     technique.
    </span>
    <span class="koboSpan" id="kobo.80.2">
     A word vector is
    </span>
    <a id="_idIndexMarker283">
    </a>
    <span class="koboSpan" id="kobo.81.1">
     basically a dense number representation of a word.
    </span>
    <span class="koboSpan" id="kobo.81.2">
     What’s surprising about these vectors is that semantically similar words have similar word vectors.
    </span>
    <span class="koboSpan" id="kobo.81.3">
     Word vectors
    </span>
    <a id="_idIndexMarker284">
    </a>
    <span class="koboSpan" id="kobo.82.1">
     such
    </span>
    <a id="_idIndexMarker285">
    </a>
    <span class="koboSpan" id="kobo.83.1">
     as
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.84.1">
      GloVe
     </span>
    </strong>
    <span class="koboSpan" id="kobo.85.1">
     and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.86.1">
      FastText
     </span>
    </strong>
    <span class="koboSpan" id="kobo.87.1">
     vectors are
    </span>
    <a id="_idIndexMarker286">
    </a>
    <span class="koboSpan" id="kobo.88.1">
     already trained on the Wikipedia corpus and can be
    </span>
    <a id="_idIndexMarker287">
    </a>
    <span class="koboSpan" id="kobo.89.1">
     used for semantic similarity calculations.
    </span>
    <span class="koboSpan" id="kobo.89.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.90.1">
      Token.similarity()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.91.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.92.1">
      Span.similarity()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.93.1">
     , and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.94.1">
      Doc.similarity()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.95.1">
     methods all use word vectors to predict how similar these containers are.
    </span>
    <span class="koboSpan" id="kobo.95.2">
     This is a simple example usage
    </span>
    <a id="_idIndexMarker288">
    </a>
    <span class="koboSpan" id="kobo.96.1">
     of
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.97.1">
      transfer learning
     </span>
    </strong>
    <span class="koboSpan" id="kobo.98.1">
     , where we are using the knowledge from the texts (the knowledge extracted from words in the word vectors training) to solve a new problem (the similarity problem).
    </span>
    <span class="koboSpan" id="kobo.98.2">
     Transformers are more powerful because they are designed to understand language in context, something that word vectors can’t do.
    </span>
    <span class="koboSpan" id="kobo.98.3">
     We’ll learn more about this in the
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.99.1">
       BERT
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.100.1">
      section.
     </span>
    </span>
   </p>
   <p>
    <em class="italic">
     <span class="koboSpan" id="kobo.101.1">
      Transformer
     </span>
    </em>
    <span class="koboSpan" id="kobo.102.1">
     is the
    </span>
    <a id="_idIndexMarker289">
    </a>
    <span class="koboSpan" id="kobo.103.1">
     name of the model architecture, but Hugging Face Transformers
    </span>
    <a id="_idIndexMarker290">
    </a>
    <span class="koboSpan" id="kobo.104.1">
     is also the name of the Hugging Face set of APIs and tools to easily download and train state-of-the-art pretrained models.
    </span>
    <span class="koboSpan" id="kobo.104.2">
     Hugging Face Transformers offer thousands of pre-trained models to perform NLP tasks, such as text classification, text summarization, question answering, machine translation, and natural language generation in more than 100 languages.
    </span>
    <span class="koboSpan" id="kobo.104.3">
     The goal is to make state-of-the-art NLP accessible
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.105.1">
      to everyone.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.106.1">
     In this chapter, we are going to use the transformers models to apply a form of transfer learning to improve the accuracy of a downstream task – in our case, the text classification task.
    </span>
    <span class="koboSpan" id="kobo.106.2">
     We’ll do that using spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.107.1">
      transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.108.1">
     component from the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.109.1">
      spacy-transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.110.1">
     package in conjunction with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.111.1">
      textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.112.1">
     component to increase the accuracy of the pipeline.
    </span>
    <span class="koboSpan" id="kobo.112.2">
     With spaCy, there is also the option to use predictions directly from an existing Hugging Face model.
    </span>
    <span class="koboSpan" id="kobo.112.3">
     To do that, you can use the wrappers from the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.113.1">
      spacy-huggingface-pipelines
     </span>
    </strong>
    <span class="koboSpan" id="kobo.114.1">
     package.
    </span>
    <span class="koboSpan" id="kobo.114.2">
     We will see how to do that in
    </span>
    <a href="B22441_11.xhtml#_idTextAnchor143">
     <span class="No-Break">
      <em class="italic">
       <span class="koboSpan" id="kobo.115.1">
        Chapter 11
       </span>
      </em>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.116.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.117.1">
     Alright, now that you know what transformers are, let’s go ahead and learn more about the machine learning concepts behind
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.118.1">
      the technique.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-92">
    <a id="_idTextAnchor091">
    </a>
    <span class="koboSpan" id="kobo.119.1">
     From LSTMs to Transformers
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.120.1">
     Before transformers, LSTM
    </span>
    <a id="_idIndexMarker291">
    </a>
    <span class="koboSpan" id="kobo.121.1">
     neural network cells were the go-to solution for modeling text.
    </span>
    <span class="koboSpan" id="kobo.121.2">
     LSTMs are a variation of a
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.122.1">
      recurrent neural network
     </span>
    </strong>
    <span class="koboSpan" id="kobo.123.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.124.1">
      RNN
     </span>
    </strong>
    <span class="koboSpan" id="kobo.125.1">
     ) cell.
    </span>
    <span class="koboSpan" id="kobo.125.2">
     RNN is a special neural network architecture that
    </span>
    <a id="_idIndexMarker292">
    </a>
    <span class="koboSpan" id="kobo.126.1">
     can process sequential data in steps.
    </span>
    <span class="koboSpan" id="kobo.126.2">
     In usual neural networks, we assume that all the inputs and outputs are independent of each other.
    </span>
    <span class="koboSpan" id="kobo.126.3">
     The problem is that this way of modeling is not true for text data.
    </span>
    <span class="koboSpan" id="kobo.126.4">
     Every word’s presence depends on the neighbor’s words.
    </span>
    <span class="koboSpan" id="kobo.126.5">
     For example, during a machine translation task, we predict a word by considering all the words we predicted before.
    </span>
    <span class="koboSpan" id="kobo.126.6">
     RNNs address this situation by capturing information about the past
    </span>
    <a id="_idIndexMarker293">
    </a>
    <span class="koboSpan" id="kobo.127.1">
     sequence elements and holding them in memory (called
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.128.1">
       hidden state
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.129.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.130.1">
     LSTMs
    </span>
    <a id="_idIndexMarker294">
    </a>
    <span class="koboSpan" id="kobo.131.1">
     were created to fix some computational problems of RNNs.
    </span>
    <span class="koboSpan" id="kobo.131.2">
     RNNs have the problem of forgetting some data back in the sequence, as well as some numerical stability issues due to chain multiplications called vanishing and exploding gradients.
    </span>
    <span class="koboSpan" id="kobo.131.3">
     An LSTM cell is slightly more complicated than an RNN cell, but the logic of computation is the same: we feed one input word at each time step and LSTM outputs a value at each
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.132.1">
      time step.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.133.1">
     LSTMs are better than vanilla RNNs, but they have some shortcomings too.
    </span>
    <span class="koboSpan" id="kobo.133.2">
     The LSTM architecture sometimes has difficulties with learning long text.
    </span>
    <span class="koboSpan" id="kobo.133.3">
     Statistical dependencies in a long text can be difficult to represent by an LSTM because, as the time steps pass, the LSTM can forget some of the words that were processed at earlier time steps.
    </span>
    <span class="koboSpan" id="kobo.133.4">
     Also, the nature of LSTMs is sequential.
    </span>
    <span class="koboSpan" id="kobo.133.5">
     We process one word at each time step.
    </span>
    <span class="koboSpan" id="kobo.133.6">
     This means parallelizing the learning process is impossible; we must process it sequentially.
    </span>
    <span class="koboSpan" id="kobo.133.7">
     Not allowing parallelization creates a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.134.1">
      performance bottleneck.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.135.1">
     Transformers
    </span>
    <a id="_idIndexMarker295">
    </a>
    <span class="koboSpan" id="kobo.136.1">
     address these problems by not using recurrent layers at all.
    </span>
    <span class="koboSpan" id="kobo.136.2">
     The Transformer architecture consists of two parts – an input encoder (called
    </span>
    <a id="_idIndexMarker296">
    </a>
    <span class="koboSpan" id="kobo.137.1">
     the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.138.1">
      Encoder
     </span>
    </strong>
    <span class="koboSpan" id="kobo.139.1">
     ) block on the left, and the output decoder (called
    </span>
    <a id="_idIndexMarker297">
    </a>
    <span class="koboSpan" id="kobo.140.1">
     the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.141.1">
      Decoder
     </span>
    </strong>
    <span class="koboSpan" id="kobo.142.1">
     ) block on the right.
    </span>
    <span class="koboSpan" id="kobo.142.2">
     The following diagram is taken from the original paper,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.143.1">
      Attention Is All You Need
     </span>
    </em>
    <span class="koboSpan" id="kobo.144.1">
     (
    </span>
    <a href="https://arxiv.org/abs/1706.03762">
     <span class="koboSpan" id="kobo.145.1">
      https://arxiv.org/abs/1706.03762
     </span>
    </a>
    <span class="koboSpan" id="kobo.146.1">
     ), and exhibits the
    </span>
    <a id="_idIndexMarker298">
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.147.1">
      transformer architecture:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer069">
     <span class="koboSpan" id="kobo.148.1">
      <img alt="Figure 6.1 – Transformer architecture from the paper “Attention is All You Need”" src="image/B22441_06_01.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.149.1">
     Figure 6.1 – Transformer architecture from the paper “Attention is All You Need”
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.150.1">
     The preceding
    </span>
    <a id="_idIndexMarker299">
    </a>
    <span class="koboSpan" id="kobo.151.1">
     architecture is used for a machine translation task; hence, the input is a sequence of words from the source language, and the output is a sequence of words in the target language.
    </span>
    <span class="koboSpan" id="kobo.151.2">
     The encoder generates a vector representation of the input words and passes them to the decoder (the word vector transfer is represented by the arrow from the encoder block in the direction of the decoder block).
    </span>
    <span class="koboSpan" id="kobo.151.3">
     The decoder takes these input word vectors, transforms the output words into word vectors, and finally, generates the probability of each output word (labeled in
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.152.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.153.1">
      .1
     </span>
    </em>
    <span class="koboSpan" id="kobo.154.1">
     as
    </span>
    <span class="No-Break">
     <strong class="bold">
      <span class="koboSpan" id="kobo.155.1">
       Output Probabilities
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.156.1">
      ).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.157.1">
     The innovation
    </span>
    <a id="_idIndexMarker300">
    </a>
    <span class="koboSpan" id="kobo.158.1">
     transformers bring lies in the
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.159.1">
      Multi-Head Attention
     </span>
    </strong>
    <span class="koboSpan" id="kobo.160.1">
     block.
    </span>
    <span class="koboSpan" id="kobo.160.2">
     This block creates a dense representation for each word by using a self-attention mechanism.
    </span>
    <span class="koboSpan" id="kobo.160.3">
     The self-attention mechanism relates each word in the input sentence to the other words in the input sentence.
    </span>
    <span class="koboSpan" id="kobo.160.4">
     The word embedding of each word is calculated by taking a weighted average of the other words’ embeddings.
    </span>
    <span class="koboSpan" id="kobo.160.5">
     This way, the importance of each word in the input sentence is calculated, so the architecture focuses its attention on each input word
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.161.1">
      in turn.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.162.1">
     The following diagram illustrates the mechanism of self-attention in a Transformer model.
    </span>
    <span class="koboSpan" id="kobo.162.2">
     It shows how the input words on the left-hand side attend to the input word “it” on the right-hand side.
    </span>
    <span class="koboSpan" id="kobo.162.3">
     The color gradient in the diagram represents the level of relevance each word has with respect to “it.”
    </span>
    <span class="koboSpan" id="kobo.162.4">
     Words with darker, more intense colors, such as “The animal,” have higher relevance, while words with lighter shades, like “didn't” or “too tired,” have
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.163.1">
      lower relevance.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.164.1">
     This visualization demonstrates that the Transformer can precisely determine that the pronoun “it” refers to “The animal” in this sentence.
    </span>
    <span class="koboSpan" id="kobo.164.2">
     Such a capability allows Transformers to resolve complex semantic dependencies and relationships within a sentence, showcasing their powerful ability to understand context
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.165.1">
      and meaning.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer070">
     <span class="koboSpan" id="kobo.166.1">
      <img alt="Figure 6.2 – Illustration of the self-attention mechanism" src="image/B22441_06_02.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.167.1">
     Figure 6.2 – Illustration of the self-attention mechanism
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.168.1">
     Later on in the chapter, we are going to learn about a famous transformer model called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.169.1">
      BERT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.170.1">
     , so
    </span>
    <a id="_idIndexMarker301">
    </a>
    <span class="koboSpan" id="kobo.171.1">
     don’t worry if all this content seems too abstract now.
    </span>
    <span class="koboSpan" id="kobo.171.2">
     We will learn how to use transformers through a text classification use case, but before using transformers, we need to see how to tackle the text classification problem with spaCy.
    </span>
    <span class="koboSpan" id="kobo.171.3">
     Let’s do that in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.172.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-93">
    <a id="_idTextAnchor092">
    </a>
    <span class="koboSpan" id="kobo.173.1">
     Text classification with spaCy
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.174.1">
     spaCy
    </span>
    <a id="_idIndexMarker302">
    </a>
    <span class="koboSpan" id="kobo.175.1">
     models are very successful for general NLP purposes, such as understanding a sentence’s syntax, splitting a paragraph into sentences, and extracting entities.
    </span>
    <span class="koboSpan" id="kobo.175.2">
     However, sometimes, we work on very specific domains that spaCy pre-trained models didn’t learn how
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.176.1">
      to handle.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.177.1">
     For example, X (formerly Twitter) text
    </span>
    <a id="_idIndexMarker303">
    </a>
    <span class="koboSpan" id="kobo.178.1">
     contains many non-regular words, such as hashtags, emoticons, and mentions.
    </span>
    <span class="koboSpan" id="kobo.178.2">
     Also, X sentences are usually just phrases, not full
    </span>
    <a id="_idIndexMarker304">
    </a>
    <span class="koboSpan" id="kobo.179.1">
     sentences.
    </span>
    <span class="koboSpan" id="kobo.179.2">
     Here, it’s entirely reasonable that spaCy’s POS tagger performs in a substandard manner as the POS tagger is trained on full, grammatically correct
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.180.1">
      English sentences.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.181.1">
     Another example is the medical domain.
    </span>
    <span class="koboSpan" id="kobo.181.2">
     It contains many entities, such as drug, disease, and chemical compound names.
    </span>
    <span class="koboSpan" id="kobo.181.3">
     These entities are not expected to be recognized by spaCy’s NER model because it has no disease or drug entity labels.
    </span>
    <span class="koboSpan" id="kobo.181.4">
     NER does not know anything about the medical domain
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.182.1">
      at all.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.183.1">
     In this chapter, we’ll be working with the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.184.1">
      Amazon Fine Food Reviews
     </span>
    </em>
    <span class="koboSpan" id="kobo.185.1">
     dataset (
    </span>
    <a href="https://www.kaggle.com/snap/amazon-fine-food-reviews">
     <span class="koboSpan" id="kobo.186.1">
      https://www.kaggle.com/snap/amazon-fine-food-reviews
     </span>
    </a>
    <span class="koboSpan" id="kobo.187.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.187.2">
     This
    </span>
    <a id="_idIndexMarker305">
    </a>
    <span class="koboSpan" id="kobo.188.1">
     dataset contains customer reviews about fine food sold on Amazon  (
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.189.1">
      J.
     </span>
     <span class="koboSpan" id="kobo.189.2">
      McAuley and J.
     </span>
     <span class="koboSpan" id="kobo.189.3">
      Leskovec.
     </span>
     <span class="koboSpan" id="kobo.189.4">
      Hidden factors and hidden topics: understanding rating dimensions with review text.
     </span>
     <span class="koboSpan" id="kobo.189.5">
      RecSys, 2013
     </span>
    </em>
    <span class="koboSpan" id="kobo.190.1">
     ,
    </span>
    <a href="https://dl.acm.org/doi/abs/10.1145/2507157.2507163">
     <span class="koboSpan" id="kobo.191.1">
      https://dl.acm.org/doi/abs/10.1145/2507157.2507163
     </span>
    </a>
    <span class="koboSpan" id="kobo.192.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.192.2">
     Reviews include user and product information, user ratings, and text.
    </span>
    <span class="koboSpan" id="kobo.192.3">
     We want to classify these reviews as positive or negative.
    </span>
    <span class="koboSpan" id="kobo.192.4">
     As this is a specific domain problem, spaCy doesn’t know how to classify it (yet).
    </span>
    <span class="koboSpan" id="kobo.192.5">
     To teach the pipeline how to do that, we will use
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.193.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.194.1">
     , a trainable component to classify text.
    </span>
    <span class="koboSpan" id="kobo.194.2">
     We’ll do that in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.195.1">
      next section.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-94">
    <a id="_idTextAnchor093">
    </a>
    <span class="koboSpan" id="kobo.196.1">
     Training the TextCategorizer component
    </span>
   </h2>
   <p>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.197.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.198.1">
     is an optional
    </span>
    <a id="_idIndexMarker306">
    </a>
    <span class="koboSpan" id="kobo.199.1">
     and trainable pipeline component to predict categories over a whole document.
    </span>
    <span class="koboSpan" id="kobo.199.2">
     To train it, we need to provide examples and their class labels.
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.200.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.201.1">
      .3
     </span>
    </em>
    <span class="koboSpan" id="kobo.202.1">
     shows exactly where the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.203.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.204.1">
     component lies in the NLP pipeline; this component comes after the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.205.1">
      essential components.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer071">
     <span class="koboSpan" id="kobo.206.1">
      <img alt="Figure 6.3 – TextCategorizer in the NLP pipeline" src="image/B22441_06_03.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.207.1">
     Figure 6.3 – TextCategorizer in the NLP pipeline
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.208.1">
     A neural network architecture lies behind spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.209.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.210.1">
     component, which provides us with user-friendly and end-to-end approaches to training the classifier.
    </span>
    <span class="koboSpan" id="kobo.210.2">
     This means we don’t have to deal directly with the neural network architecture.
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.211.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.212.1">
     is available in two flavors: single-label classifier (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.213.1">
      textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.214.1">
     ) and multilabel classifier (
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.215.1">
      textcat_multilabel
     </span>
    </strong>
    <span class="koboSpan" id="kobo.216.1">
     ).
    </span>
    <span class="koboSpan" id="kobo.216.2">
     As the name suggests, a multilabel classifier can predict more than one class.
    </span>
    <span class="koboSpan" id="kobo.216.3">
     A single-label classifier predicts only one class for each example and classes are mutually exclusive.
    </span>
    <span class="koboSpan" id="kobo.216.4">
     The predictions of the component are saved in
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.217.1">
      doc.cats
     </span>
    </strong>
    <span class="koboSpan" id="kobo.218.1">
     as a dictionary, where the key is the name of the category, and the value
    </span>
    <a id="_idIndexMarker307">
    </a>
    <span class="koboSpan" id="kobo.219.1">
     is a score between 0 and
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.220.1">
      1 (inclusive).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.221.1">
     To understand how to use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.222.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.223.1">
     component, it is helpful to learn how to train a deep model in general.
    </span>
    <span class="koboSpan" id="kobo.223.2">
     Let’s do that in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.224.1">
      next section.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.225.1">
     Training a deep learning model
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.226.1">
     To train a
    </span>
    <a id="_idIndexMarker308">
    </a>
    <span class="koboSpan" id="kobo.227.1">
     neural network, we need to configure the model parameters and provide the training examples.
    </span>
    <span class="koboSpan" id="kobo.227.2">
     Each prediction of the neural network is a sum of its weight values; hence, the training procedure adjusts the weights of the neural network with our examples.
    </span>
    <span class="koboSpan" id="kobo.227.3">
     If you want to learn more about how neural networks function, you can read the excellent guide
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.228.1">
      at
     </span>
    </span>
    <a href="http://neuralnetworksanddeeplearning.com/">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.229.1">
       http://neuralnetworksanddeeplearning.com/
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.230.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.231.1">
     In the training procedure, we’ll go over the training set several times and show each example several times.
    </span>
    <span class="koboSpan" id="kobo.231.2">
     Each iteration is called
    </span>
    <a id="_idIndexMarker309">
    </a>
    <span class="koboSpan" id="kobo.232.1">
     an
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.233.1">
      epoch
     </span>
    </strong>
    <span class="koboSpan" id="kobo.234.1">
     .
    </span>
    <span class="koboSpan" id="kobo.234.2">
     At each iteration, we also shuffle the training data to prevent the model from learning patterns specific to the order of the examples.
    </span>
    <span class="koboSpan" id="kobo.234.3">
     This shuffling of training data helps to ensure that the model generalizes well to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.235.1">
      unseen data.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.236.1">
     In each epoch, the training code updates the weights of the neural network through incremental updates.
    </span>
    <span class="koboSpan" id="kobo.236.2">
     These
    </span>
    <a id="_idIndexMarker310">
    </a>
    <span class="koboSpan" id="kobo.237.1">
     incremental updates are usually applied by dividing the data of each epoch into
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.238.1">
      mini-batches
     </span>
    </strong>
    <span class="koboSpan" id="kobo.239.1">
     .
    </span>
    <span class="koboSpan" id="kobo.239.2">
     A
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.240.1">
      loss
     </span>
    </strong>
    <span class="koboSpan" id="kobo.241.1">
     is calculated by comparing the actual label with the
    </span>
    <a id="_idIndexMarker311">
    </a>
    <span class="koboSpan" id="kobo.242.1">
     neural network’s current output.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.243.1">
      Optimizers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.244.1">
     are
    </span>
    <a id="_idIndexMarker312">
    </a>
    <span class="koboSpan" id="kobo.245.1">
     functions that update the neural network weights subject to that loss.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.246.1">
      Gradient descent
     </span>
    </strong>
    <span class="koboSpan" id="kobo.247.1">
     is the
    </span>
    <a id="_idIndexMarker313">
    </a>
    <span class="koboSpan" id="kobo.248.1">
     name of the algorithm used to find the direction and the rate to update the network parameters.
    </span>
    <span class="koboSpan" id="kobo.248.2">
     The optimizers work by iteratively updating the model parameters in the direction of the gradient that reduces the loss.
    </span>
    <span class="koboSpan" id="kobo.248.3">
     That is the training process in
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.249.1">
      a nutshell.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.250.1">
     If you ever had to train a deep learning model using PyTorch or TensorFlow, you’re likely familiar with the often-challenging nature of the process.
    </span>
    <span class="koboSpan" id="kobo.250.2">
     spaCy uses Thinc, a lightweight deep learning library with a functional programming API for composing models.
    </span>
    <span class="koboSpan" id="kobo.250.3">
     With Thinc, we can switch between PyTorch, TensorFlow, and MXNet models without changing the code (and without having to code with these
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.251.1">
      libraries directly).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.252.1">
     The Thinc conceptual model
    </span>
    <a id="_idIndexMarker314">
    </a>
    <span class="koboSpan" id="kobo.253.1">
     is a little different from the other neural network libraries.
    </span>
    <span class="koboSpan" id="kobo.253.2">
     To train spaCy models, we’ll have to learn about Thinc’s configuration system.
    </span>
    <span class="koboSpan" id="kobo.253.3">
     We will do that in the next sections of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.254.1">
      this chapter.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.255.1">
     Summarizing the training process, we need to gather and prepare the data, define the optimizer to update the weights for each mini-batch, split the data into mini-batches, and shuffle each mini-batch
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.256.1">
      for training.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.257.1">
     We haven’t touched on the phase of gathering and preparing the data yet.
    </span>
    <span class="koboSpan" id="kobo.257.2">
     spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.258.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.259.1">
     container holds the information for one training instance.
    </span>
    <span class="koboSpan" id="kobo.259.2">
     It stores two
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.260.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.261.1">
     objects: one for holding the reference label and one for holding the predictions of the pipeline.
    </span>
    <span class="koboSpan" id="kobo.261.2">
     Let's learn how to build the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.262.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.263.1">
     objects from our training data in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.264.1">
      next section.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.265.1">
     Preparing the data for spaCy trainable components
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.266.1">
     To create a dataset
    </span>
    <a id="_idIndexMarker315">
    </a>
    <span class="koboSpan" id="kobo.267.1">
     for training, we need to construct Example objects.
    </span>
    <span class="koboSpan" id="kobo.267.2">
     Example objects can be created using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.268.1">
      Example.from_dict()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.269.1">
     method with a Doc reference and a dictionary of gold-standard annotations.
    </span>
    <span class="koboSpan" id="kobo.269.2">
     For the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.270.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.271.1">
     component, the annotation name for
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.272.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.273.1">
     should be a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.274.1">
      cat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.275.1">
     dictionary of label/value pairs indicating how relevant the category is for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.276.1">
      the text.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.277.1">
     Each review of the dataset we’ll be working on can be either positive or negative.
    </span>
    <span class="koboSpan" id="kobo.277.2">
     Here is an example of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.278.1">
      a review:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.279.1">
review_text = 'This Hot chocolate is very good. </span><span class="koboSpan" id="kobo.279.2">It has just the right amount of milk chocolate flavor. </span><span class="koboSpan" id="kobo.279.3">The price is a very good deal and more than worth it!'
</span><span class="koboSpan" id="kobo.279.4">category = 'positive'</span></pre>
   <p>
    <span class="koboSpan" id="kobo.280.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.281.1">
      Example.from_dict()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.282.1">
     method takes
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.283.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.284.1">
     as the first parameter and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.285.1">
      Dict[str, Any]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.286.1">
     as the second parameter.
    </span>
    <span class="koboSpan" id="kobo.286.2">
     For our classification use case, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.287.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.288.1">
     will be the review text and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.289.1">
      Dict[str, Any]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.290.1">
     will be
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.291.1">
      cat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.292.1">
     dictionary with the labels and the correct classification.
    </span>
    <span class="koboSpan" id="kobo.292.2">
     Let's build
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.293.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.294.1">
     for the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.295.1">
      previous review:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.296.1">
      First, let’s load a blank
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.297.1">
       English pipeline:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.298.1">
import spacy
from spacy.training import Example
nlp = spacy.blank("en")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.299.1">
      Now, let’s create a
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.300.1">
       Doc
      </span>
     </strong>
     <span class="koboSpan" id="kobo.301.1">
      to wrap the review text and create the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.302.1">
       cats
      </span>
     </strong>
     <span class="koboSpan" id="kobo.303.1">
      dictionary with the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.304.1">
       correct labels:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.305.1">
review_text = 'This Hot chocolate is very good. </span><span class="koboSpan" id="kobo.305.2">It has just the right amount of milk chocolate flavor. </span><span class="koboSpan" id="kobo.305.3">The price is a very good deal and more than worth it!'
</span><span class="koboSpan" id="kobo.305.4">doc = nlp(review_text)
annotation = {"cats": {"positive": 1, "negative": 0}}</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.306.1">
      Finally, let’s
     </span>
     <a id="_idIndexMarker316">
     </a>
     <span class="koboSpan" id="kobo.307.1">
      create an
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.308.1">
        Example
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.309.1">
       object:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.310.1">
example = Example.from_dict(doc, annotation)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.311.1">
     In this chapter, we are only fine-tuning the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.312.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.313.1">
     component, but with spaCy, you can also train other trainable components such as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.314.1">
      Tagger
     </span>
    </strong>
    <span class="koboSpan" id="kobo.315.1">
     or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.316.1">
      DependencyParser
     </span>
    </strong>
    <span class="koboSpan" id="kobo.317.1">
     .
    </span>
    <span class="koboSpan" id="kobo.317.2">
     The process of creating the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.318.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.319.1">
     objects is the same; the only thing that differs is the type of annotation for each.
    </span>
    <span class="koboSpan" id="kobo.319.2">
     Here are some examples of different annotations (you can find the full list
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.320.1">
      at
     </span>
    </span>
    <a href="https://spacy.io/api/data-formats#dict-input">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.321.1">
       https://spacy.io/api/data-formats#dict-input
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.322.1">
      ):
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.323.1">
       text
      </span>
     </strong>
     <span class="koboSpan" id="kobo.324.1">
      :
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.325.1">
       Raw text
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.326.1">
       cats
      </span>
     </strong>
     <span class="koboSpan" id="kobo.327.1">
      : Dictionary of
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.328.1">
       label
      </span>
     </strong>
     <span class="koboSpan" id="kobo.329.1">
      /
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.330.1">
       value
      </span>
     </strong>
     <span class="koboSpan" id="kobo.331.1">
      pairs indicating how relevant a certain text category is for
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.332.1">
       the text
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.333.1">
       tags
      </span>
     </strong>
     <span class="koboSpan" id="kobo.334.1">
      : List of fine-grained
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.335.1">
       POS tags
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.336.1">
       deps
      </span>
     </strong>
     <span class="koboSpan" id="kobo.337.1">
      : List of string values indicating the dependency relation of a token to
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.338.1">
       its head
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.339.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.340.1">
      amazon_food_reviews.csv
     </span>
    </strong>
    <span class="koboSpan" id="kobo.341.1">
     file has a sample of 4,000 rows of the original
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.342.1">
      Amazon Fine Food Reviews
     </span>
    </em>
    <span class="koboSpan" id="kobo.343.1">
     dataset.
    </span>
    <span class="koboSpan" id="kobo.343.2">
     We will take 80% of these rows for training and use the other 20% for testing.
    </span>
    <span class="koboSpan" id="kobo.343.3">
     Let’s create the array with all the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.344.1">
      training examples:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.345.1">
      First, let’s download
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.346.1">
       the dataset:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.347.1">
mkdir data
wget -P data https://github.com/PacktPublishing/Mastering-spaCy-Second-Edition/blob/main/chapter_06/data/amazon_food_reviews.csv</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.348.1">
      Now, let’s
     </span>
     <a id="_idIndexMarker317">
     </a>
     <span class="koboSpan" id="kobo.349.1">
      load and split 80% of the dataset
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.350.1">
       for training:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.351.1">
import pandas as pd
import spacy
from spacy.training import Example
df = pd.read_csv("data/amazon_food_reviews.csv")
df_train = df.sample(frac=0.8,random_state=200)
df_test = df.drop(df_train.index)
df_test.to_json("data/df_dev.json")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.352.1">
      Finally, let’s create the train examples and store them in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.353.1">
       a list:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.354.1">
nlp = spacy.blank("en")
TRAIN_EXAMPLES = []
for _,row in df_train.iterrows():
    if row["positive_review"] == 1:
        annotation = {"cats": {"positive": 1, "negative": 0}}
    else:
        annotation = {"cats": {"negative": 1, "positive": 0}}
    example = Example.from_dict(nlp(row["text"]), annotation)
    TRAIN_EXAMPLES.append(example)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.355.1">
     Now that you know how to create the examples to feed the training data, we can go ahead and write the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.356.1">
      training script.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.357.1">
     Creating the training script
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.358.1">
     The recommended way of training the
    </span>
    <a id="_idIndexMarker318">
    </a>
    <span class="koboSpan" id="kobo.359.1">
     models is using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.360.1">
      spacy train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.361.1">
     command with spaCy’s CLI; we
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.362.1">
      shouldn’t
     </span>
    </em>
    <span class="koboSpan" id="kobo.363.1">
     be writing our own training scripts.
    </span>
    <span class="koboSpan" id="kobo.363.2">
     In this section, we will write our own training script for learning purposes.
    </span>
    <span class="koboSpan" id="kobo.363.3">
     We’ll learn how to properly train the models using the CLI in the next sections of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.364.1">
      this chapter.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.365.1">
     Let’s review the steps to train a deep learning model.
    </span>
    <span class="koboSpan" id="kobo.365.2">
     At each epoch, we shuffle the training data and
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.366.1">
      update the weights of the neural network
     </span>
    </em>
    <span class="koboSpan" id="kobo.367.1">
     through i
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.368.1">
      ncremental updates
     </span>
    </em>
    <span class="koboSpan" id="kobo.369.1">
     using
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.370.1">
      optimizer functions
     </span>
    </em>
    <span class="koboSpan" id="kobo.371.1">
     .
    </span>
    <span class="koboSpan" id="kobo.371.2">
     spaCy offers methods to create all of these steps of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.372.1">
      training loop.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.373.1">
     Our goal is to train the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.374.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.375.1">
     component, so the first step is to create it and add it to the pipeline.
    </span>
    <span class="koboSpan" id="kobo.375.2">
     Since this is a trainable component, we need to initialize it providing the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.376.1">
      Examples
     </span>
    </strong>
    <span class="koboSpan" id="kobo.377.1">
     .
    </span>
    <span class="koboSpan" id="kobo.377.2">
     We also need to provide the current
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.378.1">
      nlp
     </span>
    </strong>
    <span class="koboSpan" id="kobo.379.1">
     object.
    </span>
    <span class="koboSpan" id="kobo.379.2">
     Here is the code to create and initialize the component, using the list we’ve created in the previous
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.380.1">
      code block:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.381.1">
import spacy
from spacy.training import Example
nlp = spacy.blank("en")
textcat = nlp.add_pipe("textcat")
textcat.initialize(lambda: TRAIN_EXAMPLES, nlp=nlp)</span></pre>
   <p>
    <span class="koboSpan" id="kobo.382.1">
     We pass the whole
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.383.1">
      TRAIN_EXAMPLES
     </span>
    </strong>
    <span class="koboSpan" id="kobo.384.1">
     list as a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.385.1">
      lambda
     </span>
    </strong>
    <span class="koboSpan" id="kobo.386.1">
     function.
    </span>
    <span class="koboSpan" id="kobo.386.2">
     The next step is to define the optimizer to update the model weights.
    </span>
    <span class="koboSpan" id="kobo.386.3">
     spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.387.1">
      Language
     </span>
    </strong>
    <span class="koboSpan" id="kobo.388.1">
     class has a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.389.1">
      resume_training()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.390.1">
     method that creates and returns an optimizer.
    </span>
    <span class="koboSpan" id="kobo.390.2">
     By default, it returns the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.391.1">
      Adam
     </span>
    </strong>
    <span class="koboSpan" id="kobo.392.1">
     optimizer, and we will stick to
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.393.1">
      it here.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.394.1">
     We’re ready to define the training loop.
    </span>
    <span class="koboSpan" id="kobo.394.2">
     For each epoch, we go over training examples one by one and update the weights of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.395.1">
      textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.396.1">
     .
    </span>
    <span class="koboSpan" id="kobo.396.2">
     We go over the data for 40 epochs.
    </span>
    <span class="koboSpan" id="kobo.396.3">
     spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.397.1">
      util.minibatch
     </span>
    </strong>
    <span class="koboSpan" id="kobo.398.1">
     function iterates over batches of items.
    </span>
    <span class="koboSpan" id="kobo.398.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.399.1">
      size
     </span>
    </strong>
    <span class="koboSpan" id="kobo.400.1">
     parameter defines the batch size.
    </span>
    <span class="koboSpan" id="kobo.400.2">
     I have a GPU with enough memory so I'm dividing the data into groups of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.401.1">
      200 rows.
     </span>
    </span>
   </p>
   <p class="callout-heading">
    <span class="koboSpan" id="kobo.402.1">
     Important note
    </span>
   </p>
   <p class="callout">
    <span class="koboSpan" id="kobo.403.1">
     If you’re running the code and run into a “GPU out of memory” error, you may try to decrease the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.404.1">
       size
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.405.1">
      parameter.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.406.1">
     With the loop through
    </span>
    <a id="_idIndexMarker319">
    </a>
    <span class="koboSpan" id="kobo.407.1">
     the training data in place, the next step is to finally compute the difference between the model prediction and the correct labels and update the weights accordingly.
    </span>
    <span class="koboSpan" id="kobo.407.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.408.1">
      update
     </span>
    </strong>
    <span class="koboSpan" id="kobo.409.1">
     method of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.410.1">
      Language
     </span>
    </strong>
    <span class="koboSpan" id="kobo.411.1">
     class handles that.
    </span>
    <span class="koboSpan" id="kobo.411.2">
     We will provide the data and a dictionary to update the loss so we can keep track of it and the optimizer we’ve created previously.
    </span>
    <span class="koboSpan" id="kobo.411.3">
     The following code defines the full
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.412.1">
      training loop:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.413.1">
      Initialize the pipeline and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.414.1">
       the component:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.415.1">
import spacy
from spacy.util import minibatch
import random
nlp = spacy.blank("en")
textcat = nlp.add_pipe("textcat")
textcat.initialize(lambda: TRAIN_EXAMPLES, nlp=nlp)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.416.1">
      Create
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.417.1">
       the optimizer:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.418.1">
optimizer = nlp.resume_training()</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.419.1">
      Define the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.420.1">
       training loop:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.421.1">
for epoch in range(40):
    random.shuffle(TRAIN_EXAMPLES)
    batches = minibatch(TRAIN_EXAMPLES, size=200)
    losses = {}
    for batch in batches:
        nlp.update(
            batch,
            losses=losses,
            sgd=optimizer,
        ) 
    if epoch % 10 == 0: 
        print(epoch, "Losses", losses) 
    print(epoch, "Losses", losses)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.422.1">
     And there you have it, your first trained spaCy component.
    </span>
    <span class="koboSpan" id="kobo.422.2">
     If everything is okay, the model is learning and the losses should be decreasing.
    </span>
    <span class="koboSpan" id="kobo.422.3">
     At every 10 epochs, we print the losses to check if this is happening.
    </span>
    <span class="koboSpan" id="kobo.422.4">
     Let’s predict the categories of some unseen reviews to have a quick glance at how
    </span>
    <a id="_idIndexMarker320">
    </a>
    <span class="koboSpan" id="kobo.423.1">
     the model
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.424.1">
      is behaving:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.425.1">
text = "Smoke Paprika My mother uses it for allot of dishes, but this particular one, doesn't compare to anything she had.  It is now being used for a decoration on the spice shelf and I will never use it and ruin a dish again. </span><span class="koboSpan" id="kobo.425.2">I have tried using just a little bit, thinking it was stronger than her's. </span><span class="koboSpan" id="kobo.425.3">And I am a decent cook. </span><span class="koboSpan" id="kobo.425.4">But this does not taste like the smoke paprika that I have had in the past.  Sorry I don't recommend this product at all."
</span><span class="koboSpan" id="kobo.425.5">doc = nlp(text)
print("Example 1", doc.cats)
text = "Terrible Tasting for me The Teechino Caffeine-Free Herbal Coffee, Mediterranean Vanilla Nut tasted undrinkable to me. </span><span class="koboSpan" id="kobo.425.6">It lacked a deep, full-bodied flavor, which Cafix and Pero coffee-like substitute products have. </span><span class="koboSpan" id="kobo.425.7">I wanted to try something new, and for me, this substitute coffee drink wasn't my favorite."
</span><span class="koboSpan" id="kobo.425.8">doc = nlp(text)
print("Example 2", doc.cats)
text = "Dishwater If I had a choice of THIS or nothing, I'd go with nothing. </span><span class="koboSpan" id="kobo.425.9">Of all the K-cups I've tasted - this is the worst. </span><span class="koboSpan" id="kobo.425.10">Very weak and if you close your eyes and think really hard about it, maybe you can almost taste cinnamon. </span><span class="koboSpan" id="kobo.425.11">Blech."
</span><span class="koboSpan" id="kobo.425.12">doc = nlp(text)
print("Example 3", doc.cats)</span></pre>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.426.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.427.1">
      .4
     </span>
    </em>
    <span class="koboSpan" id="kobo.428.1">
     shows
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.429.1">
      the results:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer072">
     <span class="koboSpan" id="kobo.430.1">
      <img alt="Figure 6.4 – Categories of the review examples" src="image/B22441_06_04.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.431.1">
     Figure 6.4 – Categories of the review examples
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.432.1">
     The model is
    </span>
    <a id="_idIndexMarker321">
    </a>
    <span class="koboSpan" id="kobo.433.1">
     right for the first two examples, but the last one is clearly a negative review and the model classifies it to be positive.
    </span>
    <span class="koboSpan" id="kobo.433.2">
     We can see that the review has some very objective indicators of a bad review such as the passage
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.434.1">
      this is the worst
     </span>
    </strong>
    <span class="koboSpan" id="kobo.435.1">
     .
    </span>
    <span class="koboSpan" id="kobo.435.2">
     Maybe if we add more information about the context of words like transformers do, we can increase the performance of the model.
    </span>
    <span class="koboSpan" id="kobo.435.3">
     Let’s try this in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.436.1">
      next section.
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-95">
    <a id="_idTextAnchor094">
    </a>
    <span class="koboSpan" id="kobo.437.1">
     Using Hugging Face transformers in spaCy
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.438.1">
     In this chapter, we are going to use spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.439.1">
      transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.440.1">
     component from
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.441.1">
      spacy-transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.442.1">
     in conjunction with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.443.1">
      textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.444.1">
     component to increase the accuracy of the pipeline.
    </span>
    <span class="koboSpan" id="kobo.444.2">
     This time, we will create the pipeline using spaCy’s
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.445.1">
      config.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.446.1">
     system, which is the recommended way to train the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.447.1">
      spaCy components.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.448.1">
     Let’s first get to know the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.449.1">
       Transformer
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.450.1">
      component.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-96">
    <a id="_idTextAnchor095">
    </a>
    <span class="koboSpan" id="kobo.451.1">
     The Transformer component
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.452.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.453.1">
      Transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.454.1">
     component is
    </span>
    <a id="_idIndexMarker322">
    </a>
    <span class="koboSpan" id="kobo.455.1">
     provided by the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.456.1">
      spacy-transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.457.1">
     package.
    </span>
    <span class="koboSpan" id="kobo.457.2">
     With the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.458.1">
      Transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.459.1">
     component, we can use transformer models to improve the accuracy of our tasks.
    </span>
    <span class="koboSpan" id="kobo.459.2">
     The component supports all models that are available via the Hugging Face
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.460.1">
      transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.461.1">
     library.
    </span>
    <span class="koboSpan" id="kobo.461.2">
     In this chapter, we are going to use the RoBERTa model.
    </span>
    <span class="koboSpan" id="kobo.461.3">
     We'll learn more about this model in the next sections of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.462.1">
      this chapter.
     </span>
    </span>
   </p>
   <p>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.463.1">
      Transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.464.1">
     adds a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.465.1">
      Doc._.trf_data
     </span>
    </strong>
    <span class="koboSpan" id="kobo.466.1">
     attribute to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.467.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.468.1">
     objects.
    </span>
    <span class="koboSpan" id="kobo.468.2">
     These transformer tokens can be shared with the other pipeline components.
    </span>
    <span class="koboSpan" id="kobo.468.3">
     In this chapter, we are going to use the tokens of the RoBERTa model as part of the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.469.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.470.1">
     component.
    </span>
    <span class="koboSpan" id="kobo.470.2">
     But first, let’s use the RoBERTa model without
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.471.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.472.1">
     to see how it works.
    </span>
    <span class="koboSpan" id="kobo.472.2">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.473.1">
      Transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.474.1">
     component allows us to use a lot of different architectures.
    </span>
    <span class="koboSpan" id="kobo.474.2">
     To use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.475.1">
      roberta-base
     </span>
    </strong>
    <span class="koboSpan" id="kobo.476.1">
     model from Hugging Face, we need to use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.477.1">
      spacy-transformers.TransformerModel.v3
     </span>
    </strong>
    <span class="koboSpan" id="kobo.478.1">
     architecture.
    </span>
    <span class="koboSpan" id="kobo.478.2">
     This is how we
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.479.1">
      do it:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.480.1">
      Import the
     </span>
     <a id="_idIndexMarker323">
     </a>
     <span class="koboSpan" id="kobo.481.1">
      libraries and load a
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.482.1">
       blank model:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.483.1">
import spacy
nlp = spacy.blank("en")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.484.1">
      Define the architecture we want to use with the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.485.1">
       Transformer
      </span>
     </strong>
     <span class="koboSpan" id="kobo.486.1">
      component.
     </span>
     <span class="koboSpan" id="kobo.486.2">
      The
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.487.1">
       Transformer
      </span>
     </strong>
     <span class="koboSpan" id="kobo.488.1">
      component accepts a
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.489.1">
       model
      </span>
     </strong>
     <span class="koboSpan" id="kobo.490.1">
      config to set the Thinc model wrapping the transformer.
     </span>
     <span class="koboSpan" id="kobo.490.2">
      We set the architecture to
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.491.1">
       spacy-transformers.TransformerModel.v3
      </span>
     </strong>
     <span class="koboSpan" id="kobo.492.1">
      and the model
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.493.1">
       to
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.494.1">
        roberta-base
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.495.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.496.1">
config = {
    "model": {
        "@architectures": "spacy-transformers.TransformerModel.v3",
        "name": "roberta-base"
    }
}</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.497.1">
      Add the component to the pipeline, initialize it, and print
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.498.1">
       the vectors:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.499.1">
nlp.add_pipe("transformer", config=config)
nlp.initialize()
doc = nlp("Dishwater If I had a choice of THIS or nothing, I'd go with nothing. </span><span class="koboSpan" id="kobo.499.2">Of all the K-cups I've tasted - this is the worst. </span><span class="koboSpan" id="kobo.499.3">Very weak and if you close your eyes and think really hard about it, maybe you can almost taste cinnamon. </span><span class="koboSpan" id="kobo.499.4">Blech.")
print(doc._.trf_data)</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.500.1">
     The result is
    </span>
    <a id="_idIndexMarker324">
    </a>
    <span class="koboSpan" id="kobo.501.1">
     a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.502.1">
      FullTransformerBatch
     </span>
    </strong>
    <span class="koboSpan" id="kobo.503.1">
     object that holds a batch of input and output objects for the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.504.1">
      transformer model.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.505.1">
     Cool, now we need to use this model output together with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.506.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.507.1">
     component.
    </span>
    <span class="koboSpan" id="kobo.507.2">
     We'll do that using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.508.1">
      config.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.509.1">
     file, so first we need to learn how to work with this
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.510.1">
      configuration system.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-97">
    <a id="_idTextAnchor096">
    </a>
    <span class="koboSpan" id="kobo.511.1">
     spaCy’s configuration system
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.512.1">
     spaCy v3.0 introduces
    </span>
    <a id="_idIndexMarker325">
    </a>
    <span class="koboSpan" id="kobo.513.1">
     configuration files.
    </span>
    <span class="koboSpan" id="kobo.513.2">
     These files are used to include all settings and hyperparameters for training pipelines.
    </span>
    <span class="koboSpan" id="kobo.513.3">
     Under the hood, the training config uses the configuration system provided by the Thinc library.
    </span>
    <span class="koboSpan" id="kobo.513.4">
     As the spaCy documentation points out, some of the main advantages and features of spaCy’s training config are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.514.1">
      as
     </span>
    </span>
    <span class="No-Break">
     <a id="_idIndexMarker326">
     </a>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.515.1">
      follows:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.516.1">
       Structured sections
      </span>
     </strong>
     <span class="koboSpan" id="kobo.517.1">
      : The config is grouped into sections, and nested sections are defined using the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.518.1">
       .
      </span>
     </strong>
     <span class="koboSpan" id="kobo.519.1">
      notation.
     </span>
     <span class="koboSpan" id="kobo.519.2">
      For example,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.520.1">
       [components.textcat]
      </span>
     </strong>
     <span class="koboSpan" id="kobo.521.1">
      defines the settings for the pipeline’s
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.522.1">
        TextCategorizer
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.523.1">
       component.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.524.1">
       Interpolation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.525.1">
      : If you have hyperparameters or other settings used by multiple components, define them once and reference them
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.526.1">
       as variables.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.527.1">
       Reproducibility with no hidden defaults
      </span>
     </strong>
     <span class="koboSpan" id="kobo.528.1">
      : The config file is the “single source of truth” and includes
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.529.1">
       all settings.
      </span>
     </span>
    </li>
    <li>
     <strong class="bold">
      <span class="koboSpan" id="kobo.530.1">
       Automated checks and validation
      </span>
     </strong>
     <span class="koboSpan" id="kobo.531.1">
      : When you load a config, spaCy checks whether the settings are complete and whether all values have the correct types.
     </span>
     <span class="koboSpan" id="kobo.531.2">
      This lets you catch potential mistakes early.
     </span>
     <span class="koboSpan" id="kobo.531.3">
      In your custom architectures, you can use Python type hints to tell the config which types of data
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.532.1">
       to expect.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.533.1">
     The config is divided into sections and subsections, indicated by the square brackets and dot notation.
    </span>
    <span class="koboSpan" id="kobo.533.2">
     For example,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.534.1">
      [components]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.535.1">
     is a section, and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.536.1">
      [components.textcat]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.537.1">
     is a subsection.
    </span>
    <span class="koboSpan" id="kobo.537.2">
     The main top-level sections of a config file are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.538.1">
      as follows:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.539.1">
       paths
      </span>
     </strong>
     <span class="koboSpan" id="kobo.540.1">
      : Paths to data and other assets.
     </span>
     <span class="koboSpan" id="kobo.540.2">
      Reused across the config as variables (e.g.,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.541.1">
       ${paths.train}
      </span>
     </strong>
     <span class="koboSpan" id="kobo.542.1">
      ) and can be overwritten on
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.543.1">
       the CLI.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.544.1">
       system
      </span>
     </strong>
     <span class="koboSpan" id="kobo.545.1">
      : Settings related to system and hardware.
     </span>
     <span class="koboSpan" id="kobo.545.2">
      Reused across the config as variables (e.g.,
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.546.1">
       ${system.seed}
      </span>
     </strong>
     <span class="koboSpan" id="kobo.547.1">
      ) and can be overwritten on
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.548.1">
       the CLI.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.549.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.550.1">
      : Definition of the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.551.1">
       nlp
      </span>
     </strong>
     <span class="koboSpan" id="kobo.552.1">
      object, its tokenizer, and processing pipeline
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.553.1">
       component names.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.554.1">
       components
      </span>
     </strong>
     <span class="koboSpan" id="kobo.555.1">
      : Definitions of the pipeline components and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.556.1">
       their models.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.557.1">
       training
      </span>
     </strong>
     <span class="koboSpan" id="kobo.558.1">
      : Settings and controls for the training and
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.559.1">
       evaluation process.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.560.1">
       pretraining
      </span>
     </strong>
     <span class="koboSpan" id="kobo.561.1">
      : Optional settings and controls for the language
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.562.1">
       model pretraining.
      </span>
     </span>
    </li>
    <li>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.563.1">
       initialize
      </span>
     </strong>
     <span class="koboSpan" id="kobo.564.1">
      : Data
     </span>
     <a id="_idIndexMarker327">
     </a>
     <span class="koboSpan" id="kobo.565.1">
      resources and arguments passed to components when
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.566.1">
       nlp.initialize ()
      </span>
     </strong>
     <span class="koboSpan" id="kobo.567.1">
      is called before training (but not
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.568.1">
       at runtime).
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.569.1">
     We now know how to train a deep learning model and how to define the configuration for this training using Thinc as part of the spaCy training process.
    </span>
    <span class="koboSpan" id="kobo.569.2">
     This configuration system is very handy for maintaining and reproducing NLP pipelines, and they are not only for training but also for building the pipelines when we don’t need to train the components.
    </span>
    <span class="koboSpan" id="kobo.569.3">
     The spaCy config system really shines when combined with the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.570.1">
      spaCy CLI.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-98">
    <a id="_idTextAnchor097">
    </a>
    <span class="koboSpan" id="kobo.571.1">
     Training the TextCategorizer with a config file
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.572.1">
     In this section, we will use spaCy’s
    </span>
    <a id="_idIndexMarker328">
    </a>
    <span class="koboSpan" id="kobo.573.1">
     CLI to fine-tune the classification pipeline.
    </span>
    <span class="koboSpan" id="kobo.573.2">
     As usual, the first thing you need to do to train the model is prepare the data.
    </span>
    <span class="koboSpan" id="kobo.573.3">
     The recommended way of preparing data for training with spaCy is using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.574.1">
      DocBin
     </span>
    </strong>
    <span class="koboSpan" id="kobo.575.1">
     container instead of creating the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.576.1">
      Example
     </span>
    </strong>
    <span class="koboSpan" id="kobo.577.1">
     objects as we did earlier.
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.578.1">
      DocBin
     </span>
    </strong>
    <span class="koboSpan" id="kobo.579.1">
     container packs a collection of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.580.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.581.1">
     objects for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.582.1">
      binary serialization.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.583.1">
     To create the training data with
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.584.1">
      DocBin
     </span>
    </strong>
    <span class="koboSpan" id="kobo.585.1">
     , we will create the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.586.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.587.1">
     objects using the text of the reviews and add the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.588.1">
      doc.cats
     </span>
    </strong>
    <span class="koboSpan" id="kobo.589.1">
     attribute accordingly.
    </span>
    <span class="koboSpan" id="kobo.589.2">
     The process is pretty straightforward as we just need to use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.590.1">
      DocBin.add()
     </span>
    </strong>
    <span class="koboSpan" id="kobo.591.1">
     method to add a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.592.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.593.1">
     annotation
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.594.1">
      for serialization:
     </span>
    </span>
   </p>
   <ol>
    <li>
     <span class="koboSpan" id="kobo.595.1">
      First, we load and split the data as we
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.596.1">
       did previously:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.597.1">
import pandas as pd
import spacy
from spacy.tokens import DocBin
df = pd.read_csv("data/amazon_food_reviews.csv")
df_train = df.sample(frac=0.8,random_state=200)
nlp = spacy.blank("en")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.598.1">
      Now, we create a
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.599.1">
       DocBin
      </span>
     </strong>
     <span class="koboSpan" id="kobo.600.1">
      object and, inside the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.601.1">
       for
      </span>
     </strong>
     <span class="koboSpan" id="kobo.602.1">
      loop, we create the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.603.1">
       Doc
      </span>
     </strong>
     <span class="koboSpan" id="kobo.604.1">
      objects
     </span>
     <a id="_idIndexMarker329">
     </a>
     <span class="koboSpan" id="kobo.605.1">
      and add them
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.606.1">
       to
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.607.1">
        DocBin
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.608.1">
       :
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.609.1">
db = DocBin()
for _,row in df_train.iterrows():
    doc = nlp(row["text"])
    if row["positive_review"] == 1:
        doc.cats = {"positive": 1, "negative": 0}
    else:
        doc.cats = {"positive": 0, "negative": 1}
    db.add(doc)</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.610.1">
      Finally, we save the
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.611.1">
       DocBin
      </span>
     </strong>
     <span class="koboSpan" id="kobo.612.1">
      object
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.613.1">
       to disk:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.614.1">
db.to_disk("data/train.spacy")</span></pre>
    </li>
    <li>
     <span class="koboSpan" id="kobo.615.1">
      We will also need to create a
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.616.1">
       dev
      </span>
     </strong>
     <span class="koboSpan" id="kobo.617.1">
      test set (it will be used in the training), so let’s create a function to convert
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.618.1">
       the datasets:
      </span>
     </span>
     <pre class="source-code"><span class="koboSpan" id="kobo.619.1">
from pathlib import Path
def convert_dataset(lang: str, input_path: Path, 
                    output_path: Path):
    nlp = spacy.blank(lang)
    db = DocBin()
    df = pd.read_json(input_path)
    for _,row in df.iterrows():
        doc = nlp.make_doc(row["Text"])
        if row["positive_review"] == 1:
            doc.cats = {"positive": 1, "negative": 0}
        else:
            doc.cats = {"negative": 1, "positive": 0}
        db.add(doc)
    db.to_disk(output_path)
convert_dataset("en", "data/df_dev.json", "data/dev.spacy")</span></pre>
    </li>
   </ol>
   <p>
    <span class="koboSpan" id="kobo.620.1">
     Now, we have all the data prepared in the recommended way to train the models.
    </span>
    <span class="koboSpan" id="kobo.620.2">
     We will use these
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.621.1">
      .spacy
     </span>
    </strong>
    <span class="koboSpan" id="kobo.622.1">
     files in a minute; let’s first learn how to use the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.623.1">
      spaCy CLI.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.624.1">
     spaCy’s CLI
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.625.1">
     With spaCy’s CLI, you
    </span>
    <a id="_idIndexMarker330">
    </a>
    <span class="koboSpan" id="kobo.626.1">
     can perform spaCy operations using the command line.
    </span>
    <span class="koboSpan" id="kobo.626.2">
     Using the command line is important because we can create and automate the execution of the pipelines, making sure that every time we run the pipeline, it will follow the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.627.1">
      same steps.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.628.1">
     spaCy’s CLI provides commands for training pipelines, converting data, debugging your config files, evaluating the models, and so on.
    </span>
    <span class="koboSpan" id="kobo.628.2">
     You can type
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.629.1">
      python -m spacy --help
     </span>
    </strong>
    <span class="koboSpan" id="kobo.630.1">
     to see the list of all
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.631.1">
      CLI commands.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.632.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.633.1">
      spacy train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.634.1">
     command
    </span>
    <a id="_idIndexMarker331">
    </a>
    <span class="koboSpan" id="kobo.635.1">
     trains or updates a spaCy pipeline.
    </span>
    <span class="koboSpan" id="kobo.635.2">
     It requires data in spaCy’s binary format but you can also convert data from other formats using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.636.1">
      spacy convert
     </span>
    </strong>
    <span class="koboSpan" id="kobo.637.1">
     command.
    </span>
    <span class="koboSpan" id="kobo.637.2">
     The config file should include all settings and
    </span>
    <a id="_idIndexMarker332">
    </a>
    <span class="koboSpan" id="kobo.638.1">
     hyperparameters used during training.
    </span>
    <span class="koboSpan" id="kobo.638.2">
     We can override settings using command-line options.
    </span>
    <span class="koboSpan" id="kobo.638.3">
     For instance,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.639.1">
      --training.batch_size 128
     </span>
    </strong>
    <span class="koboSpan" id="kobo.640.1">
     overrides the value of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.641.1">
      "batch_size"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.642.1">
     in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.643.1">
      "[
     </span>
    </strong>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.644.1">
       training]"
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.645.1">
      block.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.646.1">
     We will use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.647.1">
      spacy init config
     </span>
    </strong>
    <span class="koboSpan" id="kobo.648.1">
     CLI command to create the configuration file.
    </span>
    <span class="koboSpan" id="kobo.648.2">
     The information in the
    </span>
    <a id="_idIndexMarker333">
    </a>
    <span class="koboSpan" id="kobo.649.1">
     configuration file includes
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.650.1">
      the following:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.651.1">
      The paths for the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.652.1">
       converted datasets
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.653.1">
      A seed number and the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.654.1">
       GPU config
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.655.1">
      How to create the
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.656.1">
        nlp
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.657.1">
       object
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.658.1">
      How to build the components we
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.659.1">
       will use
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.660.1">
      How to do the
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.661.1">
       training itself
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.662.1">
     For training, it is important to
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.663.1">
      make all the settings explicit
     </span>
    </em>
    <span class="koboSpan" id="kobo.664.1">
     .
    </span>
    <span class="koboSpan" id="kobo.664.2">
     We don´t want hidden defaults because they can make the pipelines hard to reproduce.
    </span>
    <span class="koboSpan" id="kobo.664.3">
     That’s part of the design of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.665.1">
      configuration files.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.666.1">
     Let’s create a training configuration that doesn’t use the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.667.1">
      Transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.668.1">
     component to see the proper way of training
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.669.1">
      a model:
     </span>
    </span>
   </p>
   <pre class="console">
<strong class="bold"><span class="koboSpan" id="kobo.670.1">python3 -m spacy init config config_without_transformer.cfg --lang “en” --pipeline “textcat”</span></strong></pre>
   <p>
    <span class="koboSpan" id="kobo.671.1">
     This command creates a configuration file named
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.672.1">
      config_without_transformer.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.673.1">
     using an English model and with a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.674.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.675.1">
     component, with all the other settings defined
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.676.1">
      by default.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.677.1">
     Inside the file, in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.678.1">
      paths
     </span>
    </strong>
    <span class="koboSpan" id="kobo.679.1">
     section, we should point to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.680.1">
      train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.681.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.682.1">
      dev
     </span>
    </strong>
    <span class="koboSpan" id="kobo.683.1">
     data paths.
    </span>
    <span class="koboSpan" id="kobo.683.2">
     Then, in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.684.1">
      system
     </span>
    </strong>
    <span class="koboSpan" id="kobo.685.1">
     section, we set the random seed.
    </span>
    <span class="koboSpan" id="kobo.685.2">
     spaCy uses CuPy for GPU support.
    </span>
    <span class="koboSpan" id="kobo.685.3">
     CuPy provides a NumPy-compatible interface for GPU arrays.
    </span>
    <span class="koboSpan" id="kobo.685.4">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.686.1">
      gpu_allocator
     </span>
    </strong>
    <span class="koboSpan" id="kobo.687.1">
     parameter sets the library for CuPy to route GPU memory allocation to and the values can be either
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.688.1">
      pytorch
     </span>
    </strong>
    <span class="koboSpan" id="kobo.689.1">
     or
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.690.1">
      tensorflow
     </span>
    </strong>
    <span class="koboSpan" id="kobo.691.1">
     .
    </span>
    <span class="koboSpan" id="kobo.691.2">
     This avoids the memory problems when using CuPy together with one of these libraries, but since it's now our case, here we can leave it set
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.692.1">
      to
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.693.1">
       null
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.694.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.695.1">
     In the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.696.1">
      nlp
     </span>
    </strong>
    <span class="koboSpan" id="kobo.697.1">
     section, we specify the model we’ll use and define the components of the pipeline, which is just
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.698.1">
      textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.699.1">
     for now.
    </span>
    <span class="koboSpan" id="kobo.699.2">
     In the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.700.1">
      components
     </span>
    </strong>
    <span class="koboSpan" id="kobo.701.1">
     section, we need to specify how to initialize the component, so we set the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.702.1">
      factory = "textcat"
     </span>
    </strong>
    <span class="koboSpan" id="kobo.703.1">
     parameter in the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.704.1">
      component.textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.705.1">
     subsection.
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.706.1">
      textcat
     </span>
    </strong>
    <span class="koboSpan" id="kobo.707.1">
     is the name of the registered function to create the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.708.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.709.1">
     component.
    </span>
    <span class="koboSpan" id="kobo.709.2">
     You can see all the available config parameters
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.710.1">
      at
     </span>
    </span>
    <a href="https://spacy.io/api/data-formats#config">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.711.1">
       https://spacy.io/api/data-formats#config
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.712.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.713.1">
     With the config all set, we
    </span>
    <a id="_idIndexMarker334">
    </a>
    <span class="koboSpan" id="kobo.714.1">
     can run the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.715.1">
      spacy train
     </span>
    </strong>
    <span class="koboSpan" id="kobo.716.1">
     command.
    </span>
    <span class="koboSpan" id="kobo.716.2">
     The result of this run is a new pipeline, so you need to specify a path to save it.
    </span>
    <span class="koboSpan" id="kobo.716.3">
     Here is the full command to run the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.717.1">
      training process:
     </span>
    </span>
   </p>
   <pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.718.1">python3 -m spacy train config_without_transformer.cfg --paths.train "data/train.spacy" --paths.dev "data/dev.spacy" --output pipeline_without_transformer/</span></strong></pre>
   <p>
    <span class="koboSpan" id="kobo.719.1">
     This command trains the pipeline using the configuration file we’ve created and points to the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.720.1">
      train.spacy
     </span>
    </strong>
    <span class="koboSpan" id="kobo.721.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.722.1">
      dev.spacy
     </span>
    </strong>
    <span class="koboSpan" id="kobo.723.1">
     data.
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.724.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.725.1">
      .5
     </span>
    </em>
    <span class="koboSpan" id="kobo.726.1">
     shows the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.727.1">
      training output.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer073">
     <span class="koboSpan" id="kobo.728.1">
      <img alt="Figure 6.5 – Training output" src="image/B22441_06_05.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.729.1">
     Figure 6.5 – Training output
    </span>
   </p>
   <p>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.730.1">
      E
     </span>
    </strong>
    <span class="koboSpan" id="kobo.731.1">
     indicates the epoch, and
    </span>
    <a id="_idIndexMarker335">
    </a>
    <span class="koboSpan" id="kobo.732.1">
     you can also see the loss and the score for each optimization step.
    </span>
    <span class="koboSpan" id="kobo.732.2">
     The best model is saved at
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.733.1">
      pipeline_without_transformer/model-last
     </span>
    </strong>
    <span class="koboSpan" id="kobo.734.1">
     .
    </span>
    <span class="koboSpan" id="kobo.734.2">
     Let’s load it and check the results of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.735.1">
      previous examples:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.736.1">
import spacy
nlp = spacy.load("pipeline_without_transformer/model-best")
text = "Smoke Paprika My mother uses it for allot of dishes, but this particular one, doesn't compare to anything she had.  It is now being used for a decoration on the spice shelf and I will never use it and ruin a dish again. </span><span class="koboSpan" id="kobo.736.2">I have tried using just a little bit, thinking it was stronger than her's. </span><span class="koboSpan" id="kobo.736.3">And I am a decent cook. </span><span class="koboSpan" id="kobo.736.4">But this does not taste like the smoke paprika that I have had in the past.  Sorry I don't recommend this product at all."
</span><span class="koboSpan" id="kobo.736.5">doc = nlp(text)
print("Example 1", doc.cats)
text = "Terrible Tasting for me The Teechino Caffeine-Free Herbal Coffee, Mediterranean Vanilla Nut tasted undrinkable to me. </span><span class="koboSpan" id="kobo.736.6">It lacked a deep, full-bodied flavor, which Cafix and Pero coffee-like substitute products have. </span><span class="koboSpan" id="kobo.736.7">I wanted to try something new, and for me, this substitute coffee drink wasn't my favorite."
</span><span class="koboSpan" id="kobo.736.8">doc = nlp(text)
print("Example 2", doc.cats)
text = "Dishwater If I had a choice of THIS or nothing, I'd go with nothing. </span><span class="koboSpan" id="kobo.736.9">Of all the K-cups I've tasted - this is the worst. </span><span class="koboSpan" id="kobo.736.10">Very weak and if you close your eyes and think really hard about it, maybe you can almost taste cinnamon. </span><span class="koboSpan" id="kobo.736.11">Blech."
</span><span class="koboSpan" id="kobo.736.12">doc = nlp(text)
print("Example 3", doc.cats)</span></pre>
   <p>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.737.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.738.1">
      .6
     </span>
    </em>
    <span class="koboSpan" id="kobo.739.1">
     shows
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.740.1">
      the results.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer074">
     <span class="koboSpan" id="kobo.741.1">
      <img alt="Figure 6.6 – Categories of the review examples using the new pipeline" src="image/B22441_06_06.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.742.1">
     Figure 6.6 – Categories of the review examples using the new pipeline
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.743.1">
     Now, the model is incorrect for the first two examples and correct for the third one.
    </span>
    <span class="koboSpan" id="kobo.743.2">
     Let’s see whether we can improve that using the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.744.1">
      transformers
     </span>
    </strong>
    <span class="koboSpan" id="kobo.745.1">
     component.
    </span>
    <span class="koboSpan" id="kobo.745.2">
     Before doing that, now is a good time to learn the internals of one of the most influential transformer
    </span>
    <a id="_idIndexMarker336">
    </a>
    <span class="koboSpan" id="kobo.746.1">
     models,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.747.1">
      BERT
     </span>
    </strong>
    <span class="koboSpan" id="kobo.748.1">
     .
    </span>
    <span class="koboSpan" id="kobo.748.2">
     Then, we’ll learn more about its successor,
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.749.1">
      RoBERTa
     </span>
    </strong>
    <span class="koboSpan" id="kobo.750.1">
     , the model we’ll use in this chapter’s
    </span>
    <a id="_idIndexMarker337">
    </a>
    <span class="koboSpan" id="kobo.751.1">
     classification
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.752.1">
      use case.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-99">
    <a id="_idTextAnchor098">
    </a>
    <span class="koboSpan" id="kobo.753.1">
     BERT and RoBERTa
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.754.1">
     In this section, we’ll explore the most influential and commonly used Transformer model, BERT.
    </span>
    <span class="koboSpan" id="kobo.754.2">
     BERT is
    </span>
    <a id="_idIndexMarker338">
    </a>
    <span class="koboSpan" id="kobo.755.1">
     introduced in Google’s 2018 research paper; you can read it
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.756.1">
      here:
     </span>
    </span>
    <a href="https://arxiv.org/pdf/1810.04805.pdf">
     <span class="No-Break">
      <span class="koboSpan" id="kobo.757.1">
       https://arxiv.org/pdf/1810.04805.pdf
      </span>
     </span>
    </a>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.758.1">
      .
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.759.1">
     What does BERT do exactly?
    </span>
    <span class="koboSpan" id="kobo.759.2">
     To understand what BERT outputs, let’s dissect
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.760.1">
      the name:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.761.1">
Bidirectional: Training on the text data is bi-directional, which means each input sentence is processed from left to right as well as from right to left.
</span><span class="koboSpan" id="kobo.761.2">Encoder: An encoder encodes the input sentence.
</span><span class="koboSpan" id="kobo.761.3">Representations: A representation is a word vector.
</span><span class="koboSpan" id="kobo.761.4">Transformers: The architecture is transformer-based.</span></pre>
   <p>
    <span class="koboSpan" id="kobo.762.1">
     BERT is essentially a trained transformer encoder stack.
    </span>
    <span class="koboSpan" id="kobo.762.2">
     Input into BERT is a sentence, and the output is a sequence of word vectors.
    </span>
    <span class="koboSpan" id="kobo.762.3">
     The difference between BERT and previous word vector techniques is that BERT’s word vectors are contextual, which means that a vector is assigned to a word based on the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.763.1">
      input sentence.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.764.1">
     Word vectors such as GloVe are context-free, meaning that the word vector for a word is always the same independent of the sentence it is used in.
    </span>
    <span class="koboSpan" id="kobo.764.2">
     The following diagram explains
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.765.1">
      this problem:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer075">
     <span class="koboSpan" id="kobo.766.1">
      <img alt="Figure 6.7 – Word vector for the word “bank”" src="image/B22441_06_07.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.767.1">
     Figure 6.7 – Word vector for the word “bank”
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.768.1">
     Here, even though the word
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.769.1">
      bank
     </span>
    </em>
    <span class="koboSpan" id="kobo.770.1">
     has two completely different meanings in these two sentences, the word vectors are the same.
    </span>
    <span class="koboSpan" id="kobo.770.2">
     Each word has only one vector and vectors are saved to a file
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.771.1">
      following training.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.772.1">
     On the contrary, BERT
    </span>
    <a id="_idIndexMarker339">
    </a>
    <span class="koboSpan" id="kobo.773.1">
     word vectors are dynamic.
    </span>
    <span class="koboSpan" id="kobo.773.2">
     BERT can generate different word vectors for the same word depending on the input sentence.
    </span>
    <span class="koboSpan" id="kobo.773.3">
     The following diagram shows the word vectors generated
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.774.1">
      by BERT:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer076">
     <span class="koboSpan" id="kobo.775.1">
      <img alt="Figure 6.8 – Two distinct word vectors generated by BERT for the same word, “bank,” in two different contexts" src="image/B22441_06_08.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.776.1">
     Figure 6.8 – Two distinct word vectors generated by BERT for the same word, “bank,” in two different contexts
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.777.1">
     How does BERT generate these word vectors?
    </span>
    <span class="koboSpan" id="kobo.777.2">
     In the next section, we’ll explore the details of the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.778.1">
      BERT architecture.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.779.1">
     BERT architecture
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.780.1">
     BERT is a
    </span>
    <a id="_idIndexMarker340">
    </a>
    <span class="koboSpan" id="kobo.781.1">
     transformer encoder stack, which means that several encoder layers are stacked on top of each other.
    </span>
    <span class="koboSpan" id="kobo.781.2">
     The first layer initializes the word vectors randomly, and then each encoder layer transforms the output of the previous encoder layer.
    </span>
    <span class="koboSpan" id="kobo.781.3">
     The paper introduces two model sizes for BERT: BERT Base and BERT Large.
    </span>
    <span class="koboSpan" id="kobo.781.4">
     The following diagram shows the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.782.1">
      BERT architecture:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer077">
     <span class="koboSpan" id="kobo.783.1">
      <img alt="Figure 6.9 – BERT Base and Large architectures, having 12 and 24 encoder layers, respectively" src="image/B22441_06_09.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.784.1">
     Figure 6.9 – BERT Base and Large architectures, having 12 and 24 encoder layers, respectively
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.785.1">
     Both BERT
    </span>
    <a id="_idIndexMarker341">
    </a>
    <span class="koboSpan" id="kobo.786.1">
     models have a huge number of encoder layers.
    </span>
    <span class="koboSpan" id="kobo.786.2">
     BERT Base has 12 encoder layers and BERT Large has 24 encoder layers.
    </span>
    <span class="koboSpan" id="kobo.786.3">
     The dimensions of the resulting word vectors are different too; BERT Base generates word vectors of size 768 and BERT Large generates word vectors of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.787.1">
      size 1024.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.788.1">
     The following diagram exhibits a high-level overview of BERT inputs and outputs (ignore the CLS token for now; you’ll learn about it in the
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.789.1">
      BERT input
     </span>
    </em>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.790.1">
       format
      </span>
     </em>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.791.1">
      section):
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer078">
     <span class="koboSpan" id="kobo.792.1">
      <img alt="Figure 6.10 – BERT model input word and output word vectors" src="image/B22441_06_10.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.793.1">
     Figure 6.10 – BERT model input word and output word vectors
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.794.1">
     In the
    </span>
    <a id="_idIndexMarker342">
    </a>
    <span class="koboSpan" id="kobo.795.1">
     preceding diagram, we can see a high-level overview of BERT inputs and outputs.
    </span>
    <span class="koboSpan" id="kobo.795.2">
     BERT input must be in a special format and include some special tokens, such as CLS in
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.796.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.797.1">
      .10
     </span>
    </em>
    <span class="koboSpan" id="kobo.798.1">
     .
    </span>
    <span class="koboSpan" id="kobo.798.2">
     In the next section, you’ll learn about the details of the BERT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.799.1">
      input format.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.800.1">
     BERT input format
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.801.1">
     To understand
    </span>
    <a id="_idIndexMarker343">
    </a>
    <span class="koboSpan" id="kobo.802.1">
     how BERT generates the output vectors, we need to know the BERT input data format.
    </span>
    <span class="koboSpan" id="kobo.802.2">
     BERT input format can represent a single sentence, as well as a pair of sentences.
    </span>
    <span class="koboSpan" id="kobo.802.3">
     For tasks such as question answering and semantic similarity, we input two sentences to the model in a single sequence
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.803.1">
      of tokens.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.804.1">
     BERT works with a class of special tokens and a special tokenization algorithm called
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.805.1">
      WordPiece
     </span>
    </strong>
    <span class="koboSpan" id="kobo.806.1">
     .
    </span>
    <span class="koboSpan" id="kobo.806.2">
     The main
    </span>
    <a id="_idIndexMarker344">
    </a>
    <span class="koboSpan" id="kobo.807.1">
     special tokens are
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.808.1">
      [CLS]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.809.1">
     ,
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.810.1">
      [SEP]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.811.1">
     ,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.812.1">
      and
     </span>
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.813.1">
       [PAD]
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.814.1">
      :
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.815.1">
      The first special token of BERT is [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.816.1">
       CLS
      </span>
     </strong>
     <span class="koboSpan" id="kobo.817.1">
      ].
     </span>
     <span class="koboSpan" id="kobo.817.2">
      The first token of every input sequence has to be [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.818.1">
       CLS
      </span>
     </strong>
     <span class="koboSpan" id="kobo.819.1">
      ].
     </span>
     <span class="koboSpan" id="kobo.819.2">
      We use this token in classification tasks as an aggregate of the input sentence.
     </span>
     <span class="koboSpan" id="kobo.819.3">
      We ignore this token in
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.820.1">
       non-classification tasks.
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.821.1">
      [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.822.1">
       SEP
      </span>
     </strong>
     <span class="koboSpan" id="kobo.823.1">
      ] means a sentence separator.
     </span>
     <span class="koboSpan" id="kobo.823.2">
      If the input is a single sentence, we place this token at the end of the sentence.
     </span>
     <span class="koboSpan" id="kobo.823.3">
      If the input is two sentences, then we use this token to separate two sentences.
     </span>
     <span class="koboSpan" id="kobo.823.4">
      Hence, for a single sentence, the input looks like [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.824.1">
       CLS
      </span>
     </strong>
     <span class="koboSpan" id="kobo.825.1">
      ] sentence [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.826.1">
       SEP
      </span>
     </strong>
     <span class="koboSpan" id="kobo.827.1">
      ], and for two sentences, the input looks like [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.828.1">
       CLS
      </span>
     </strong>
     <span class="koboSpan" id="kobo.829.1">
      ] sentence1 [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.830.1">
       SEP
      </span>
     </strong>
     <span class="koboSpan" id="kobo.831.1">
      ]
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.832.1">
       sentence2 [
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.833.1">
        SEP
       </span>
      </strong>
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.834.1">
       ].
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.835.1">
      [
     </span>
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.836.1">
       PAD
      </span>
     </strong>
     <span class="koboSpan" id="kobo.837.1">
      ] is a special token meaning padding.
     </span>
     <span class="koboSpan" id="kobo.837.2">
      BERT receives sentences of a fixed length; hence, we pad the short sentences before feeding them to BERT.
     </span>
     <span class="koboSpan" id="kobo.837.3">
      The maximum length of tokens we can feed to BERT
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.838.1">
       is 512.
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.839.1">
     BERT tokenizes the words using WordPiece tokenization.
    </span>
    <span class="koboSpan" id="kobo.839.2">
     A “word piece” is literally a piece of a word.
    </span>
    <span class="koboSpan" id="kobo.839.3">
     The WordPiece algorithm breaks words down into several sub-words.
    </span>
    <span class="koboSpan" id="kobo.839.4">
     The idea is to break down complex/long tokens into simpler tokens.
    </span>
    <span class="koboSpan" id="kobo.839.5">
     For example, the word
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.840.1">
      playing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.841.1">
     is tokenized as
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.842.1">
      play
     </span>
    </strong>
    <span class="koboSpan" id="kobo.843.1">
     and
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.844.1">
      ##ing
     </span>
    </strong>
    <span class="koboSpan" id="kobo.845.1">
     .
    </span>
    <span class="koboSpan" id="kobo.845.2">
     A
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.846.1">
      ##
     </span>
    </strong>
    <span class="koboSpan" id="kobo.847.1">
     character is placed before every word piece to indicate that this token is not a word from the language’s vocabulary but that it’s a
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.848.1">
      word piece.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.849.1">
     Let’s look at some
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.850.1">
      more examples:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.851.1">
playing  play, ##ing
played   play, ##ed
going    go, ##ing
vocabulary = [play, go, ##ing, ##ed]</span></pre>
   <p>
    <span class="koboSpan" id="kobo.852.1">
     By doing that, we represent the language vocabulary more compactly, grouping common sub-words.
    </span>
    <span class="koboSpan" id="kobo.852.2">
     WordPiece tokenization creates wonders on rare/unseen words, as these words are broken down into
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.853.1">
      their sub-words.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.854.1">
     After tokenizing the input sentence and adding the special tokens, each token is converted to its ID.
    </span>
    <span class="koboSpan" id="kobo.854.2">
     After that, we feed the sequence of token IDs
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.855.1">
      to BERT.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.856.1">
     To summarize, this is
    </span>
    <a id="_idIndexMarker345">
    </a>
    <span class="koboSpan" id="kobo.857.1">
     how we transform a sentence into a BERT
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.858.1">
      input format:
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer079">
     <span class="koboSpan" id="kobo.859.1">
      <img alt="Figure 6.11 – Transforming an input sentence into BERT input format" src="image/B22441_06_11.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.860.1">
     Figure 6.11 – Transforming an input sentence into BERT input format
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.861.1">
     This tokenization process is crucial for transformer models because it allows the model to handle out-of-vocabulary words and helps in generalization.
    </span>
    <span class="koboSpan" id="kobo.861.2">
     For example, the model can learn that the suffix
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.862.1">
      ness
     </span>
    </em>
    <span class="koboSpan" id="kobo.863.1">
     in words such as
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.864.1">
      happiness
     </span>
    </em>
    <span class="koboSpan" id="kobo.865.1">
     and
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.866.1">
      sadness
     </span>
    </em>
    <span class="koboSpan" id="kobo.867.1">
     has a specific meaning and can use this knowledge for new words that also have
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.868.1">
      this suffix.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.869.1">
     BERT is
    </span>
    <a id="_idIndexMarker346">
    </a>
    <span class="koboSpan" id="kobo.870.1">
     trained on a large unlabeled Wiki corpus and a huge book corpus.
    </span>
    <span class="koboSpan" id="kobo.870.2">
     As stated in Google Research’s BERT GitHub repository,
    </span>
    <a href="https://github.com/google-research/bert">
     <span class="koboSpan" id="kobo.871.1">
      https://github.com/google-research/bert
     </span>
    </a>
    <span class="koboSpan" id="kobo.872.1">
     , they trained a large model (12-layer to 24-layer Transformer) on a large corpus (Wikipedia + BookCorpus) for a long time (1M
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.873.1">
      update steps).
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.874.1">
     BERT is trained with two training
    </span>
    <a id="_idIndexMarker347">
    </a>
    <span class="koboSpan" id="kobo.875.1">
     methods:
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.876.1">
      masked language modeling
     </span>
    </strong>
    <span class="koboSpan" id="kobo.877.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.878.1">
      MLM
     </span>
    </strong>
    <span class="koboSpan" id="kobo.879.1">
     ) and
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.880.1">
      next sentence prediction
     </span>
    </strong>
    <span class="koboSpan" id="kobo.881.1">
     (
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.882.1">
      NSP
     </span>
    </strong>
    <span class="koboSpan" id="kobo.883.1">
     ).
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.884.1">
      Language modeling
     </span>
    </strong>
    <span class="koboSpan" id="kobo.885.1">
     is the task of
    </span>
    <a id="_idIndexMarker348">
    </a>
    <span class="koboSpan" id="kobo.886.1">
     predicting the next token
    </span>
    <a id="_idIndexMarker349">
    </a>
    <span class="koboSpan" id="kobo.887.1">
     given the sequence of previous tokens.
    </span>
    <span class="koboSpan" id="kobo.887.2">
     For example, given the sequence of the words
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.888.1">
      Yesterday I visited a
     </span>
    </em>
    <span class="koboSpan" id="kobo.889.1">
     , a language model can predict the next token as one of the tokens such as
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.890.1">
      church
     </span>
    </em>
    <span class="koboSpan" id="kobo.891.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.892.1">
      hospital
     </span>
    </em>
    <span class="koboSpan" id="kobo.893.1">
     ,
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.894.1">
      school
     </span>
    </em>
    <span class="koboSpan" id="kobo.895.1">
     , and so on.
    </span>
    <strong class="bold">
     <span class="koboSpan" id="kobo.896.1">
      Masked language modeling
     </span>
    </strong>
    <span class="koboSpan" id="kobo.897.1">
     is a kind of language modeling where we mask a percentage of the tokens randomly by replacing them with a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.898.1">
      [MASK]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.899.1">
     token.
    </span>
    <span class="koboSpan" id="kobo.899.2">
     We expect MLM to predict the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.900.1">
      masked words.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.901.1">
     The masked language model data preparation in BERT is implemented as follows.
    </span>
    <span class="koboSpan" id="kobo.901.2">
     First, 15 of the input tokens are chosen at random.
    </span>
    <span class="koboSpan" id="kobo.901.3">
     Then, the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.902.1">
      following happens:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <span class="koboSpan" id="kobo.903.1">
      80% of the tokens chosen are replaced
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.904.1">
       with
      </span>
     </span>
     <span class="No-Break">
      <strong class="source-inline">
       <span class="koboSpan" id="kobo.905.1">
        [MASK]
       </span>
      </strong>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.906.1">
      10% of the tokens chosen are replaced with another token from
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.907.1">
       the vocabulary
      </span>
     </span>
    </li>
    <li>
     <span class="koboSpan" id="kobo.908.1">
      The remaining 10% are
     </span>
     <span class="No-Break">
      <span class="koboSpan" id="kobo.909.1">
       left unchanged
      </span>
     </span>
    </li>
   </ul>
   <p>
    <span class="koboSpan" id="kobo.910.1">
     A training example sentence for MLM looks like
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.911.1">
      the following:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.912.1">
[CLS] Yesterday I [MASK] my friend at [MASK] house [SEP]</span></pre>
   <p>
    <span class="koboSpan" id="kobo.913.1">
     NSP is the task of predicting the next sentence given an input sentence.
    </span>
    <span class="koboSpan" id="kobo.913.2">
     In this approach, we feed two sentences to BERT and expect BERT to predict the order of the sentences, more specifically, whether the second sentence is the sentence that comes after the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.914.1">
      first sentence.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.915.1">
     Let’s make an example input to NSP.
    </span>
    <span class="koboSpan" id="kobo.915.2">
     We’ll feed two sentences separated by the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.916.1">
      [SEP]
     </span>
    </strong>
    <span class="koboSpan" id="kobo.917.1">
     token
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.918.1">
      as input:
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.919.1">
[CLS] A man robbed a [MASK] yesterday [MASK] 8 o'clock [SEP]
He [MASK] the bank with 6 million dollars [SEP]
Label = IsNext</span></pre>
   <p>
    <span class="koboSpan" id="kobo.920.1">
     In this example, the second sentence can follow the first sentence; hence, the predicted label is
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.921.1">
      IsNext
     </span>
    </strong>
    <span class="koboSpan" id="kobo.922.1">
     .
    </span>
    <span class="koboSpan" id="kobo.922.2">
     How about
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.923.1">
      this example?
     </span>
    </span>
   </p>
   <pre class="source-code"><span class="koboSpan" id="kobo.924.1">
[CLS] Rabbits like to [MASK] carrots and [MASK] leaves [SEP]
[MASK] Schwarzenegger is elected as the governor of [MASK] [SEP]
Label= NotNext</span></pre>
   <p>
    <span class="koboSpan" id="kobo.925.1">
     This example pair of sentences generates the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.926.1">
      NotNext
     </span>
    </strong>
    <span class="koboSpan" id="kobo.927.1">
     label, as they are not contextually or
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.928.1">
      semantically related.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.929.1">
     Both these
    </span>
    <a id="_idIndexMarker350">
    </a>
    <span class="koboSpan" id="kobo.930.1">
     training techniques allow the model to learn complex concepts about the language.
    </span>
    <span class="koboSpan" id="kobo.930.2">
     Transformers are the basis of LLMs.
    </span>
    <span class="koboSpan" id="kobo.930.3">
     LLMs are revolutionizing the NLP world, mostly because of their capacity for
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.931.1">
      understanding context.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.932.1">
     Now that you know the BERT architecture, the details of the input format, and training data preparation, you have a solid basis to understand how LLMs work.
    </span>
    <span class="koboSpan" id="kobo.932.2">
     Getting back to our classification use case, we’ll work with a successor of BERT, a model called RoBERTa.
    </span>
    <span class="koboSpan" id="kobo.932.3">
     Let’s learn about RoBERTa in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.933.1">
      next section.
     </span>
    </span>
   </p>
   <h3>
    <span class="koboSpan" id="kobo.934.1">
     RoBERTa
    </span>
   </h3>
   <p>
    <span class="koboSpan" id="kobo.935.1">
     The RoBERTa model
    </span>
    <a id="_idIndexMarker351">
    </a>
    <span class="koboSpan" id="kobo.936.1">
     was proposed at
    </span>
    <a href="https://arxiv.org/abs/1907.11692">
     <span class="koboSpan" id="kobo.937.1">
      https://arxiv.org/abs/1907.11692
     </span>
    </a>
    <span class="koboSpan" id="kobo.938.1">
     .
    </span>
    <span class="koboSpan" id="kobo.938.2">
     It builds on BERT and the key difference between them is in the data preparation
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.939.1">
      and training.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.940.1">
     BERT does the token masking once during data preprocessing, which results in the same mask for each training instance in every epoch.
    </span>
    <span class="koboSpan" id="kobo.940.2">
     RoBERTa uses
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.941.1">
      dynamic masking
     </span>
    </em>
    <span class="koboSpan" id="kobo.942.1">
     , where they generate the masking patterns every time we feed a sequence to the model.
    </span>
    <span class="koboSpan" id="kobo.942.2">
     They also removed the NSP because they found that it matches or slightly improves downstream
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.943.1">
      task performance.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.944.1">
     RoBERTa also uses larger batch sizes than BERT and a larger vocabulary size, from a vocabulary size of 30K to a vocabulary containing 50K sub-word units.
    </span>
    <span class="koboSpan" id="kobo.944.2">
     The paper is a very good read to understand design decisions that impact
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.945.1">
      transformer models.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.946.1">
     Now that we know how BERT and RoBERTa work, it’s time to finally use RoBERTa in our text
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.947.1">
      classification pipeline.
     </span>
    </span>
   </p>
   <h2 id="_idParaDest-100">
    <a id="_idTextAnchor099">
    </a>
    <span class="koboSpan" id="kobo.948.1">
     Training the TextCategorizer with a transformer
    </span>
   </h2>
   <p>
    <span class="koboSpan" id="kobo.949.1">
     To work with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.950.1">
      transformer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.951.1">
     component
    </span>
    <a id="_idIndexMarker352">
    </a>
    <span class="koboSpan" id="kobo.952.1">
     for downstream tasks, we need to tell spaCy how to connect the component output with the other pipeline components.
    </span>
    <span class="koboSpan" id="kobo.952.2">
     We’ll do that with the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.953.1">
      spacy-transformers.TransformerModel.v3
     </span>
    </strong>
    <span class="koboSpan" id="kobo.954.1">
     and the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.955.1">
       spacy-transformers.TransformerListener.v1
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.956.1">
      layers.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.957.1">
     In spaCy, we have different model architectures, which are functions that wire up Thinc
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.958.1">
      Model
     </span>
    </strong>
    <span class="koboSpan" id="kobo.959.1">
     instances.
    </span>
    <span class="koboSpan" id="kobo.959.2">
     Both
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.960.1">
      TransformerModel
     </span>
    </strong>
    <span class="koboSpan" id="kobo.961.1">
     and the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.962.1">
      TransformerListener
     </span>
    </strong>
    <span class="koboSpan" id="kobo.963.1">
     model are
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.964.1">
      Transformer layers.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.965.1">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.966.1">
      spacy-transformers.TransformerModel.v3
     </span>
    </strong>
    <span class="koboSpan" id="kobo.967.1">
     layer loads and wraps the transformer models from the Hugging Face Transformers library.
    </span>
    <span class="koboSpan" id="kobo.967.2">
     It works with any transformer that has pretrained weights and has a PyTorch implementation.
    </span>
    <span class="koboSpan" id="kobo.967.3">
     The
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.968.1">
      spacy-transformers.TransformerListener.v1
     </span>
    </strong>
    <span class="koboSpan" id="kobo.969.1">
     layer takes a list of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.970.1">
      Doc
     </span>
    </strong>
    <span class="koboSpan" id="kobo.971.1">
     objects as input and uses the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.972.1">
      TransformerModel
     </span>
    </strong>
    <span class="koboSpan" id="kobo.973.1">
     layer to produce a list of two-dimensional arrays
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.974.1">
      as output.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.975.1">
     Now that you know the spaCy layer concept, it is time to revisit the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.976.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.977.1">
     component.
    </span>
    <span class="koboSpan" id="kobo.977.2">
     In spaCy, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.978.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.979.1">
     component has different model architecture layers.
    </span>
    <span class="koboSpan" id="kobo.979.2">
     Typically, each architecture accepts sublayers as arguments.
    </span>
    <span class="koboSpan" id="kobo.979.3">
     By default, the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.980.1">
      TextCategorizer
     </span>
    </strong>
    <span class="koboSpan" id="kobo.981.1">
     component uses the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.982.1">
      spacy.TextCatEnsemble.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.983.1">
     layer, which is a stacked ensemble of a linear bag-of-words model and a neural network model.
    </span>
    <span class="koboSpan" id="kobo.983.2">
     We used this layer when we trained the pipeline using the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.984.1">
      configuration file.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.985.1">
     To finish our journey of this chapter, we will change the neural network layer of
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.986.1">
      TextCatEnsemble
     </span>
    </strong>
    <span class="koboSpan" id="kobo.987.1">
     from the default
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.988.1">
      spacy.Tok2Vec.v2
     </span>
    </strong>
    <span class="koboSpan" id="kobo.989.1">
     layer and use the RoBERTa transformer model.
    </span>
    <span class="koboSpan" id="kobo.989.2">
     We will do this by creating a new
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.990.1">
      configuration file:
     </span>
    </span>
   </p>
   <pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.991.1">python3 -m spacy init config config_transformer.cfg --lang "en" --pipeline "textcat" --optimize "accuracy" --gpu</span></strong></pre>
   <p>
    <span class="koboSpan" id="kobo.992.1">
     This command created a
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.993.1">
      config_transformer.cfg
     </span>
    </strong>
    <span class="koboSpan" id="kobo.994.1">
     file optimized for accuracy and for training on a GPU.
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.995.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.996.1">
      .12
     </span>
    </em>
    <span class="koboSpan" id="kobo.997.1">
     shows the output of
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.998.1">
      the command.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer080">
     <span class="koboSpan" id="kobo.999.1">
      <img alt="Figure 6.12 – Creating the new training configuration that uses RoBERTa" src="image/B22441_06_12.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.1000.1">
     Figure 6.12 – Creating the new training configuration that uses RoBERTa
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.1001.1">
     Now, we can train the pipeline pointing to this configuration file, train the model, and make the predictions, just as we did in the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.1002.1">
      previous section:
     </span>
    </span>
   </p>
   <pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.1003.1">python3 -m spacy train config_transformer.cfg --paths.train "data/train.spacy" --paths.dev "data/dev.spacy" --output pipeline_transformer/ --gpu-id 0</span></strong></pre>
   <p>
    <span class="koboSpan" id="kobo.1004.1">
     This time, we do the
    </span>
    <a id="_idIndexMarker353">
    </a>
    <span class="koboSpan" id="kobo.1005.1">
     training using a GPU, so we set the
    </span>
    <strong class="source-inline">
     <span class="koboSpan" id="kobo.1006.1">
      gpu_id
     </span>
    </strong>
    <span class="koboSpan" id="kobo.1007.1">
     parameter.
    </span>
    <span class="No-Break">
     <em class="italic">
      <span class="koboSpan" id="kobo.1008.1">
       Figure 6
      </span>
     </em>
    </span>
    <em class="italic">
     <span class="koboSpan" id="kobo.1009.1">
      .13
     </span>
    </em>
    <span class="koboSpan" id="kobo.1010.1">
     shows the results using this new
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.1011.1">
      trained model.
     </span>
    </span>
   </p>
   <div>
    <div class="IMG---Figure" id="_idContainer081">
     <span class="koboSpan" id="kobo.1012.1">
      <img alt="Figure 6.13 – Categories of the review examples using the pipeline with transformer" src="image/B22441_06_13.jpg"/>
     </span>
    </div>
   </div>
   <p class="IMG---Caption" lang="en-US" xml:lang="en-US">
    <span class="koboSpan" id="kobo.1013.1">
     Figure 6.13 – Categories of the review examples using the pipeline with transformer
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.1014.1">
     Now, the model classifies reviews correctly.
    </span>
    <span class="koboSpan" id="kobo.1014.2">
     Nice,
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.1015.1">
      isn’t it?
     </span>
    </span>
   </p>
   <h1 id="_idParaDest-101">
    <a id="_idTextAnchor100">
    </a>
    <span class="koboSpan" id="kobo.1016.1">
     Summary
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.1017.1">
     It’s fair to say that this chapter is one of the most important chapters of the book.
    </span>
    <span class="koboSpan" id="kobo.1017.2">
     Here, we learned about transfer learning and transformers, and how to use the spaCy configuration system to train the
    </span>
    <span class="No-Break">
     <strong class="source-inline">
      <span class="koboSpan" id="kobo.1018.1">
       TextCategorizer
      </span>
     </strong>
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.1019.1">
      component.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.1020.1">
     With the knowledge of how to prepare the data to train spaCy components and the knowledge of how to use the configuration files to define the training settings, you are now able to fine-tune any spaCy trainable component.
    </span>
    <span class="koboSpan" id="kobo.1020.2">
     That´s a huge
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.1021.1">
      step, congratulations!
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.1022.1">
     In this chapter, you learned about language models.
    </span>
    <span class="koboSpan" id="kobo.1022.2">
     In the next chapter, you will learn how to use LLMs, which are the most powerful NLP
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.1023.1">
      models today.
     </span>
    </span>
   </p>
  </div>
 

  <div class="Content" epub:type="part" id="_idContainer083">
   <h1 id="_idParaDest-102" lang="en-US" xml:lang="en-US">
    <a id="_idTextAnchor101">
    </a>
    <span class="koboSpan" id="kobo.1.1">
     Part 3: Customizing and Integrating NLP Workflows
    </span>
   </h1>
   <p>
    <span class="koboSpan" id="kobo.2.1">
     This final section focuses on creating tailored NLP solutions and integrating spaCy with other tools and platforms.
    </span>
    <span class="koboSpan" id="kobo.2.2">
     You’ll explore how to leverage LLMs, train custom models, and integrate spaCy projects with web applications to build
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.3.1">
      end-to-end solutions.
     </span>
    </span>
   </p>
   <p>
    <span class="koboSpan" id="kobo.4.1">
     This part has the
    </span>
    <span class="No-Break">
     <span class="koboSpan" id="kobo.5.1">
      following chapters:
     </span>
    </span>
   </p>
   <ul>
    <li>
     <a href="B22441_07.xhtml#_idTextAnchor102">
      <em class="italic">
       <span class="koboSpan" id="kobo.6.1">
        Chapter 7
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.7.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.8.1">
       Enhancing NLP Tasks Using LLMs with spacy-llm
      </span>
     </em>
    </li>
    <li>
     <a href="B22441_08.xhtml#_idTextAnchor109">
      <em class="italic">
       <span class="koboSpan" id="kobo.9.1">
        Chapter 8
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.10.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.11.1">
       Training an NER Component with Your Own Data
      </span>
     </em>
    </li>
    <li>
     <a href="B22441_09.xhtml#_idTextAnchor123">
      <em class="italic">
       <span class="koboSpan" id="kobo.12.1">
        Chapter 9
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.13.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.14.1">
       Creating End-to-End spaCy Workflows with Weasel
      </span>
     </em>
    </li>
    <li>
     <a href="B22441_10.xhtml#_idTextAnchor134">
      <em class="italic">
       <span class="koboSpan" id="kobo.15.1">
        Chapter 10
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.16.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.17.1">
       Training an Entity Linker Model with spaCy
      </span>
     </em>
    </li>
    <li>
     <a href="B22441_11.xhtml#_idTextAnchor143">
      <em class="italic">
       <span class="koboSpan" id="kobo.18.1">
        Chapter 11
       </span>
      </em>
     </a>
     <span class="koboSpan" id="kobo.19.1">
      ,
     </span>
     <em class="italic">
      <span class="koboSpan" id="kobo.20.1">
       Integrating spaCy with Third-Party Libraries
      </span>
     </em>
    </li>
   </ul>
  </div>
  <div>
   <div id="_idContainer084">
   </div>
  </div>
  <div>
   <div class="Basic-Graphics-Frame" id="_idContainer085">
   </div>
  </div>
 </body></html>