<html><head></head><body>
<div id="_idContainer019">
<h1 class="chapter-number" id="_idParaDest-32"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-33"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.2.1">Surveying GenAI Types and Modes: An Overview of GANs, Diffusers, and Transformers</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapter, we established the key distinction between generative and discriminative models. </span><span class="koboSpan" id="kobo.3.2">Discriminative models focus on predicting outputs by learning </span><strong class="source-inline"><span class="koboSpan" id="kobo.4.1">p(output</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.5.1">∣</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.6.1">input)</span></strong><span class="koboSpan" id="kobo.7.1">, or the conditional probability of some expected output given an input or set of inputs. </span><span class="koboSpan" id="kobo.7.2">In contrast, generative models, such as </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">Generative Pretrained Transformer</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">GPT</span></strong><span class="koboSpan" id="kobo.11.1">), generate </span><a id="_idIndexMarker050"/><span class="koboSpan" id="kobo.12.1">text by predicting the next token (a partial word, whole word, or punctuation) using </span><strong class="source-inline"><span class="koboSpan" id="kobo.13.1">p(next token</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.14.1">∣</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.15.1">previous tokens)</span></strong><span class="koboSpan" id="kobo.16.1">, based on the probabilities of possible continuations given the current context. </span><span class="koboSpan" id="kobo.16.2">Tokens are represented as vectors containing embeddings that capture latent features and rich semantic dependencies learned through </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">extensive training.</span></span></p>
<p><span class="koboSpan" id="kobo.18.1">We briefly surveyed leading generative approaches, including </span><strong class="bold"><span class="koboSpan" id="kobo.19.1">Generative Adversarial Networks</span></strong><span class="koboSpan" id="kobo.20.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.21.1">GANs</span></strong><span class="koboSpan" id="kobo.22.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.23.1">Variational Autoencoders</span></strong><span class="koboSpan" id="kobo.24.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.25.1">VAEs</span></strong><span class="koboSpan" id="kobo.26.1">), diffusion models, and </span><a id="_idIndexMarker051"/><span class="koboSpan" id="kobo.27.1">autoregressive transformers. </span><span class="koboSpan" id="kobo.27.2">Each </span><a id="_idIndexMarker052"/><span class="koboSpan" id="kobo.28.1">methodology possesses unique strengths suitable for different data types and tasks. </span><span class="koboSpan" id="kobo.28.2">For example, GANs are adept at generating high-fidelity photographic images through an adversarial process. </span><span class="koboSpan" id="kobo.28.3">Diffusion models take a probabilistic approach, iteratively adding and removing noise from data to learn robust generative representations. </span><span class="koboSpan" id="kobo.28.4">Autoregressive transformers leverage self-attention and massive scale to achieve remarkable controlled </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">text generation.</span></span></p>
<p><span class="koboSpan" id="kobo.30.1">In this chapter, we will explore the theoretical foundations and real-world applications of these techniques in greater depth. </span><span class="koboSpan" id="kobo.30.2">We will make direct comparisons, elucidating architectural innovations and enhancements that improve training stability and output quality over time. </span><span class="koboSpan" id="kobo.30.3">Through practical examples, we will see how researchers have adapted these models to produce art, music, videos, stories, and </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.32.1">To enable an unbiased comparison, we will concentrate primarily on image synthesis tasks. </span><span class="koboSpan" id="kobo.32.2">GANs and diffusion models are specifically architected for image data, harnessing advances in convolutional processing and computer vision. </span><span class="koboSpan" id="kobo.32.3">Transformers, powered by self-attention, excel at language modeling but can also generate images. </span><span class="koboSpan" id="kobo.32.4">This will allow us to benchmark performance on a </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">common task.</span></span></p>
<p><span class="koboSpan" id="kobo.34.1">By the end of this chapter, we will have implemented state-of-the-art image generation models and explored how these core methods enhance and complement </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">each other</span><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.36.1">.</span></span></p>
<h1 id="_idParaDest-34"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.37.1">Understanding General Artificial Intelligence (GAI) Types – distinguishing features of GANs, diffusers, and transformers</span></h1>
<p><span class="koboSpan" id="kobo.38.1">The often-stunning </span><a id="_idIndexMarker053"/><span class="koboSpan" id="kobo.39.1">human-like quality we experience from GAI can be attributed to deep-generative machine learning advances. </span><span class="koboSpan" id="kobo.39.2">In particular, three fundamental methods have inspired many derivative innovations – GANs, diffusion models, and transformers. </span><span class="koboSpan" id="kobo.39.3">Each has its distinct strengths and is particularly well-suited for </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">specific applications.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">We briefly described GANs, a groundbreaking approach that exploits the adversarial interplay between two competing neural networks – a generator and a discriminator – to generate hyper-realistic synthetic data. </span><span class="koboSpan" id="kobo.41.2">Over time, GANs have seen substantial advancements, achieving greater control in data generation, higher image fidelity, and enhanced training stability. </span><span class="koboSpan" id="kobo.41.3">For instance, NVIDIA’s StyleGAN has created highly detailed and realistic human faces. </span><span class="koboSpan" id="kobo.41.4">The adversarial training process of GANs, where one network generates data and the other evaluates it, allows you to create highly refined and detailed synthetic images, enhancing realism with each training iteration. </span><span class="koboSpan" id="kobo.41.5">The synthetic images generated can be utilized in a plethora of domains. </span><span class="koboSpan" id="kobo.41.6">In the entertainment industry, they can be used to create realistic characters for video games or films. </span><span class="koboSpan" id="kobo.41.7">In research, they provide a means to augment datasets, especially in scenarios where real data is scarce or sensitive. </span><span class="koboSpan" id="kobo.41.8">Moreover, in computer vision, these synthetic images aid in training and fine-tuning other machine-learning models, advancing applications like </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">facial recognition.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Diffusion models, an innovative generative modeling alternative, explicitly address some GAN limitations. </span><span class="koboSpan" id="kobo.43.2">As discussed briefly in </span><a href="B21773_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.44.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.45.1">, diffusion models adopt a unique approach to introducing and systematically removing noise, enabling high-quality image synthesis with less training complexity. </span><span class="koboSpan" id="kobo.45.2">In medical imaging, diffusion models can significantly enhance image clarity by generating high-resolution synthetic examples to train other machine-learning models. </span><span class="koboSpan" id="kobo.45.3">Introducing and then iteratively removing noise can help reconstruct high-fidelity images from lower-quality inputs, which is invaluable in scenarios where obtaining high-resolution medical images </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">is challenging.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Simultaneously, generative transformers, initially designed for language modeling, have been adopted for multimodal synthesis. </span><span class="koboSpan" id="kobo.47.2">Today, transformers are not confined to language and have permeated into audio, images, and video applications. </span><span class="koboSpan" id="kobo.47.3">For instance, OpenAI’s GPT-4 excels in processing and generating text, while DALL-E creates images from textual descriptions, a perfect example of the interplay between methods. </span><span class="koboSpan" id="kobo.47.4">When integrated, GPT-4 and DALL-E form a robust multimodal system. </span><span class="koboSpan" id="kobo.47.5">GPT-4 processes and understands textual instructions, while DALL-E takes the interpreted instructions to generate corresponding visual representations. </span><span class="koboSpan" id="kobo.47.6">A practical application of this combination could be automated digital advertisement creation. </span><span class="koboSpan" id="kobo.47.7">For example, given textual </span><a id="_idIndexMarker054"/><span class="koboSpan" id="kobo.48.1">descriptions of a product and the desired aesthetic, GPT-4 could interpret these instructions, and DALL-E could generate visually compelling </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">advertisements according</span><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.50.1">ly.</span></span></p>
<h1 id="_idParaDest-35"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.51.1">Deconstructing GAI methods – exploring GANs, diffusers, and transformers</span></h1>
<p><span class="koboSpan" id="kobo.52.1">Let’s deconstruct</span><a id="_idIndexMarker055"/><span class="koboSpan" id="kobo.53.1"> these core approaches to understand their distinct characteristics and illustrate their transformative role in advancing generative machine learning. </span><span class="koboSpan" id="kobo.53.2">As GAI continues to move forward, it’s crucial to understand how these approaches </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">drive innovat</span><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.55.1">ion.</span></span></p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.56.1">A closer look at GANs</span></h2>
<p><span class="koboSpan" id="kobo.57.1">GANs, introduced</span><a id="_idIndexMarker056"/><span class="koboSpan" id="kobo.58.1"> by Goodfellow et al. </span><span class="koboSpan" id="kobo.58.2">in 2014, primarily consist of two neural </span><a id="_idIndexMarker057"/><span class="koboSpan" id="kobo.59.1">networks – the </span><strong class="bold"><span class="koboSpan" id="kobo.60.1">Generator</span></strong><span class="koboSpan" id="kobo.61.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.62.1">G</span></strong><span class="koboSpan" id="kobo.63.1">) and</span><a id="_idIndexMarker058"/><span class="koboSpan" id="kobo.64.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.65.1">Discriminator</span></strong><span class="koboSpan" id="kobo.66.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.67.1">D</span></strong><span class="koboSpan" id="kobo.68.1">). </span><span class="koboSpan" id="kobo.68.2">G aims to create synthetic data resembling real data, while D strives to distinguish real from </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">synthetic data.</span></span></p>
<p><span class="koboSpan" id="kobo.70.1">In this setup, the </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">following occurs:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.72.1">G receives input from a “latent space,” a high-dimensional space representing structured randomness. </span><span class="koboSpan" id="kobo.72.2">This structured randomness serves as a seed to generate synthetic data, transforming it into </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">meaningful information.</span></span></li>
<li><span class="koboSpan" id="kobo.74.1">D evaluates the generated data, attempting to differentiate between real (or reference) and </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">synthetic data.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.76.1">In short, the process begins with G deriving random noise from the latent space to create data. </span><span class="koboSpan" id="kobo.76.2">This synthetic data, along with real data, is supplied to D, which then tries to discern between the two. </span><span class="koboSpan" id="kobo.76.3">Feedback from D informs the parameters of G to refine its data generation process. </span><span class="koboSpan" id="kobo.76.4">The adversarial interaction continues until an equilibrium </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">is reached.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Equilibrium</span></strong><span class="koboSpan" id="kobo.79.1"> in GANs occurs when D can no longer differentiate between real and synthetic data, assigning an equal probability of 0.5 to both. </span><span class="koboSpan" id="kobo.79.2">Arriving at this state signals that the synthetic data produced by G is indistinguishable from real data, which is the core objective of the </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">synthesis process.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.81.1">Ultimately, the success of GANs has had meaningful implications for various sectors. </span><span class="koboSpan" id="kobo.81.2">In the automotive industry, GANs have been used to simulate real-world scenarios for autonomous vehicle testing. </span><span class="koboSpan" id="kobo.81.3">In the entertainment sector, GANs are deployed to generate digital </span><a id="_idIndexMarker059"/><span class="koboSpan" id="kobo.82.1">characters and realistic environments for filmmaking and game design. </span><span class="koboSpan" id="kobo.82.2">In the art world, GANs can literally craft new words. </span><span class="koboSpan" id="kobo.82.3">Moreover, the development of GANs has continued to move forward over the years with significant improvements in quality, control, and </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">overall per</span><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.84.1">formance.</span></span></p>
<h3><span class="koboSpan" id="kobo.85.1">Advancement of GANs</span></h3>
<p><span class="koboSpan" id="kobo.86.1">Since its</span><a id="_idIndexMarker060"/><span class="koboSpan" id="kobo.87.1"> inception, GAN technology has evolved significantly with several </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">notable advancements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.89.1">Conditional GANs (cGANs)</span></strong><span class="koboSpan" id="kobo.90.1">: Introduced </span><a id="_idIndexMarker061"/><span class="koboSpan" id="kobo.91.1">by Mirza and Osindero in 2014, conditional GANs incorporated specific conditions during data generation, enabling more controlled outputs. </span><span class="koboSpan" id="kobo.91.2">cGANs have been used in tasks such </span><a id="_idIndexMarker062"/><span class="koboSpan" id="kobo.92.1">as image-to-image translation (e.g., converting photos </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">into paintings).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.94.1">Deep Convolutional GANs (DCGANs)</span></strong><span class="koboSpan" id="kobo.95.1">: In 2015, Radford et al. </span><span class="koboSpan" id="kobo.95.2">enhanced GANs by</span><a id="_idIndexMarker063"/><span class="koboSpan" id="kobo.96.1"> integrating convolutional layers, which help to analyze image data in small, overlapping regions to capture fine granularity, substantially improving the visual quality of the synthetic output. </span><span class="koboSpan" id="kobo.96.2">DCGANs can generate realistic images for applications such as fashion design, where the model evolves new designs from </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">existing trends.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.98.1">Wasserstein GANs (WGANs)</span></strong><span class="koboSpan" id="kobo.99.1">: Introduced by Arjovsky et al. </span><span class="koboSpan" id="kobo.99.2">in 2017, Wasserstein </span><a id="_idIndexMarker064"/><span class="koboSpan" id="kobo.100.1">GANs applied the Wasserstein distance metric to GANs’ objective function, facilitating a more accurate measurement of differences between real and synthetic data. </span><span class="koboSpan" id="kobo.100.2">Specifically, the metric helps you find the most efficient way to make the generated data distribution resemble the real data distribution. </span><span class="koboSpan" id="kobo.100.3">This small adjustment leads to a more stable learning process, minimizing volatility during training. </span><span class="koboSpan" id="kobo.100.4">WGANs have helped generate realistic medical imagery to aid in training diagnostic AI algorithms, improving a model’s ability to generalize from synthetic to </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">actual data.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.102.1">Following the advent of Wasserstein GANs, the landscape experienced a surge of inventive expansions, each</span><a id="_idIndexMarker065"/><span class="koboSpan" id="kobo.103.1"> tailor-made to address specific challenges or open new avenues in synthetic </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">data generation:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.105.1">Progressively growing GANs</span></strong><span class="koboSpan" id="kobo.106.1"> incrementally increase the resolution during training, starting with lower-resolution images and gradually moving to higher resolution. </span><span class="koboSpan" id="kobo.106.2">This approach allows the model to learn coarse-to-fine details effectively, making training more manageable and generating high-quality images (Karras et al. </span><span class="koboSpan" id="kobo.106.3">2017). </span><span class="koboSpan" id="kobo.106.4">These high-resolution images can enhance the realism and immersion of virtual </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">reality environments.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">CycleGANs</span></strong><span class="koboSpan" id="kobo.109.1"> facilitates</span><a id="_idIndexMarker066"/><span class="koboSpan" id="kobo.110.1"> image-to-image translations, bridging domain adaptation tasks (Zhu et al., 2017). </span><span class="koboSpan" id="kobo.110.2">For example, a CycleGAN could transform a summer scene into a winter scene without requiring example pairs (e.g., summer-winter) during training. </span><span class="koboSpan" id="kobo.110.3">CycleGANs have been used to simulate weather conditions in autonomous vehicle testing, evaluating system performance</span><a id="_idIndexMarker067"/><span class="koboSpan" id="kobo.111.1"> under varying </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">environmental conditions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.113.1">BigGANs</span></strong><span class="koboSpan" id="kobo.114.1"> push the</span><a id="_idIndexMarker068"/><span class="koboSpan" id="kobo.115.1"> boundaries in high-resolution image generation, showcasing the versatility of GANs in complex generation tasks. </span><span class="koboSpan" id="kobo.115.2">They achieve this by scaling up the size of the model (more layers and units per layer) and the batch size during training, alongside other architectural and training innovations (Brock et al., 2018). </span><span class="koboSpan" id="kobo.115.3">BigGANs have been used to generate realistic textures for video games, enhancing gaming </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">environments’ realism.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.117.1">These developments significantly broadened what GANs could achieve, ranging from high-resolution image synthesis to domain adaptation and cross-modal generation tasks. </span><span class="koboSpan" id="kobo.117.2">However, despite </span><a id="_idIndexMarker069"/><span class="koboSpan" id="kobo.118.1">these incredible advancements, GANs have suffered from some continual limitations, which inspired alternative approaches</span><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.119.1"> such </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">as diffusion.</span></span></p>
<h3><span class="koboSpan" id="kobo.121.1">Limitations and challenges of GANs</span></h3>
<p><span class="koboSpan" id="kobo.122.1">The training process</span><a id="_idIndexMarker070"/><span class="koboSpan" id="kobo.123.1"> of GANs requires a careful balance between the G and D networks. </span><span class="koboSpan" id="kobo.123.2">It requires substantial computational resources, often demanding powerful GPUs and enormous datasets to achieve desirable outcomes. </span><span class="koboSpan" id="kobo.123.3">Moreover, there are complexities in training GANs that arise from challenges such as vanishing gradients and mode collapse. </span><span class="koboSpan" id="kobo.123.4">While the vanishing gradient problem is a problem broadly affecting deep neural networks, mode collapse is a challenge that is particularly unique to the training of GANs. </span><span class="koboSpan" id="kobo.123.5">Let’s explore these a </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">bit further:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.125.1">Vanishing gradients</span></strong><span class="koboSpan" id="kobo.126.1">: This</span><a id="_idIndexMarker071"/><span class="koboSpan" id="kobo.127.1"> issue arises during the neural network training phase when the gradient of the loss function diminishes to a point where the learning either drastically slows or halts. </span><span class="koboSpan" id="kobo.127.2">The crux of GANs lies in the delicate balance of learning between the G and D models. </span><span class="koboSpan" id="kobo.127.3">Disproportionate learning can hinder the overall training process. </span><span class="koboSpan" id="kobo.127.4">In practical terms, the issue of vanishing gradients can lead to longer training times and increased computational costs, which might render GANs impractical for time-sensitive or </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">resource-constrained applications.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Mode collapse</span></strong><span class="koboSpan" id="kobo.130.1">: Inherent to GANs, mode collapse occurs when the G starts producing a narrow variety of samples, thereby stifling output diversity and undermining a network’s effectiveness. </span><span class="koboSpan" id="kobo.130.2">Techniques such as a gradient penalty and spectral normalization have alleviated these issues. </span><span class="koboSpan" id="kobo.130.3">This phenomenon can significantly degrade the quality of generated data, limiting the use of GANs in applications that require diverse outputs, such as data augmentation for machine learning</span><a id="_idIndexMarker072"/><span class="koboSpan" id="kobo.131.1"> or generating diverse design alternatives in </span><span class="No-Break"><span class="koboSpan" id="kobo.132.1">creative industries.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.133.1">Of course, GANs carry the same ethical considerations as any state-of-the-art generative synthesis. </span><span class="koboSpan" id="kobo.133.2">For instance, they can be used to create deepfakes or generate biased outputs that reinforce societal prejudices. </span><span class="koboSpan" id="kobo.133.3">For example, when GANs, often used to generate synthetic data (e.g., faces), underrepresent certain groups, downstream applications may exhibit gender or racial bias (Kenfack et </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">al., 2021).</span></span></p>
<p><span class="koboSpan" id="kobo.135.1">Even with the advent of other generative models such as diffusion models and Transformer-based image generators, GANs have played a seminal role in shaping the trajectory of generative image synthesis, showcasing both the potential and some of the challenges inherent in </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">this domain.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">Now that </span><a id="_idIndexMarker073"/><span class="koboSpan" id="kobo.138.1">we better understand GANs in the context of deep generative models, let’s shift our focus to a successor in image generatio</span><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.139.1">n, the </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">diffusion model.</span></span></p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.141.1">A closer look at diffusion models</span></h2>
<p><span class="koboSpan" id="kobo.142.1">Having explored the </span><a id="_idIndexMarker074"/><span class="koboSpan" id="kobo.143.1">dynamics of GANs, let’s transition our attention to a subsequent innovation in image generation – the diffusion model. </span><span class="koboSpan" id="kobo.143.2">Initially proposed by Sohl-Dickstein et al. </span><span class="koboSpan" id="kobo.143.3">in 2015, diffusion models present a novel approach, where a neural network iteratively introduces and subsequently removes noise from data to generate highly refined images. </span><span class="koboSpan" id="kobo.143.4">Unlike GANs, which leverage an adversarial mechanism involving two contrasting models, diffusion models apply a more gradual, iterative process of noise manipulation within </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.145.1">In practical terms, GANs have shown substantial merit in art and design, creating realistic faces or generating sharp, high-fidelity images from descriptions. </span><span class="koboSpan" id="kobo.145.2">They are also used in data augmentation, expanding datasets by generating realistic synthetic data to augment the training of machine </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.147.1">Conversely, diffusion models excel in tasks requiring a structured approach to image generation, such as in medical imaging. </span><span class="koboSpan" id="kobo.147.2">Their iterative process can enhance the quality of medical images, such as MRI or CT scans, where noise reduction and clarity are paramount. </span><span class="koboSpan" id="kobo.147.3">This makes diffusion models invaluable in clinical settings, aiding in better diagnostics and analysis. </span><span class="koboSpan" id="kobo.147.4">Moreover, their controlled and gradual process offers a more predictable or stable</span><a id="_idIndexMarker075"/><span class="koboSpan" id="kobo.148.1"> training process compared to the adversarial and dynamic training </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">of GANs.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">The foundation of diffusion models is anchored in two </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">primary processes:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.152.1">A forward diffusion process</span></strong><span class="koboSpan" id="kobo.153.1">: This process begins with clean data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">x</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.155.1">₀</span></strong><span class="koboSpan" id="kobo.156.1">) and iteratively introduces Gaussian noise, akin to progressively applying a fog-like filter, transforming the data into indistinguishable </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">noise (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.158.1">x</span></strong></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.159.1">ₜ</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.161.1">A learned reverse model</span></strong><span class="koboSpan" id="kobo.162.1">: Following the forward diffusion, the “reverse model” (</span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">p</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.164.1">θ</span></strong><span class="koboSpan" id="kobo.165.1">) attempts to eliminate (or de-fog) the noise from the noisy data (</span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">x</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">ₜ</span></strong><span class="koboSpan" id="kobo.168.1">), aiming to revert to the original clean state (</span><strong class="source-inline"><span class="koboSpan" id="kobo.169.1">x</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">ₜ₋₁</span></strong><span class="koboSpan" id="kobo.171.1">). </span><span class="koboSpan" id="kobo.171.2">Specifically, this reversion is orchestrated by estimating the probability of transitioning from the noisy state back to the clear state, using a conditional distribution denoted as </span><strong class="source-inline"><span class="koboSpan" id="kobo.172.1">p</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.173.1">θ</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.174.1">(x</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">ₜ₋₁</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">|x</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">ₜ</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">)</span></strong><span class="koboSpan" id="kobo.179.1">. </span><span class="koboSpan" id="kobo.179.2">A </span><strong class="bold"><span class="koboSpan" id="kobo.180.1">conditional distribution</span></strong><span class="koboSpan" id="kobo.181.1"> tells </span><a id="_idIndexMarker076"/><span class="koboSpan" id="kobo.182.1">us the likelihood of one event happening when we know another related event has occurred. </span><span class="koboSpan" id="kobo.182.2">In this case, the reversion estimates the likelihood of reverting to the original state, given some amount </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">of noise.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.184.1">In the pivotal work </span><em class="italic"><span class="koboSpan" id="kobo.185.1">Score-Based Generative Modeling through Stochastic Differential Equations</span></em><span class="koboSpan" id="kobo.186.1">, the authors propose a novel framework that unifies score-based generative models and diffusion probabilistic modeling by employing </span><strong class="bold"><span class="koboSpan" id="kobo.187.1">Stochastic Differential Equations</span></strong><span class="koboSpan" id="kobo.188.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.189.1">SDEs</span></strong><span class="koboSpan" id="kobo.190.1">). </span><span class="koboSpan" id="kobo.190.2">This</span><a id="_idIndexMarker077"/><span class="koboSpan" id="kobo.191.1"> framework involves the transformation of data distributions to a known prior distribution through the gradual addition and then removal of noise, guided by SDEs. </span><span class="koboSpan" id="kobo.191.2">Optimizing the reverse-time SDE – dependent only on the score of the perturbed data distribution – allows you to generate new samples. </span><strong class="bold"><span class="koboSpan" id="kobo.192.1">Stochastic Gradient Descent</span></strong><span class="koboSpan" id="kobo.193.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.194.1">SGD</span></strong><span class="koboSpan" id="kobo.195.1">) is then</span><a id="_idIndexMarker078"/><span class="koboSpan" id="kobo.196.1"> applied to fine-tune the model parameters until arriving at an </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">improved </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">p</span></strong></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">θ</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">The reverse model (</span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">p</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.203.1">θ</span></strong><span class="koboSpan" id="kobo.204.1">) was implemented using convolutional networks to predict variations in the Gaussian noise distribution – a critical component of the noise-introduction process within the forward diffusion. </span><span class="koboSpan" id="kobo.204.2">Initially, the efficacy of this approach was validated on more straightforward datasets. </span><span class="koboSpan" id="kobo.204.3">However, the methodology’s applicability was later significantly improved to handle more complex images (Ho et al., 2020). </span><span class="koboSpan" id="kobo.204.4">This</span><a id="_idIndexMarker079"/><span class="koboSpan" id="kobo.205.1"> expansion demonstrated the practical potential of diffusion models in generating highly refined images across a broa</span><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.206.1">der spectrum </span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">of complexities.</span></span></p>
<h3><span class="koboSpan" id="kobo.208.1">Advancement of diffusion models</span></h3>
<p><span class="koboSpan" id="kobo.209.1">Since its inception, diffusion</span><a id="_idIndexMarker080"/><span class="koboSpan" id="kobo.210.1"> model technology has witnessed key advancements, propelling its capabilities in </span><span class="No-Break"><span class="koboSpan" id="kobo.211.1">image generation:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.212.1">Simplified training objectives</span></strong><span class="koboSpan" id="kobo.213.1">: Ho et al. </span><span class="koboSpan" id="kobo.213.2">proposed simplified training objectives that predict Gaussian noise directly, eliminating the need for conditional means and facilitating the application to more complex datasets (Ho et al., 2020). </span><span class="koboSpan" id="kobo.213.3">This advancement facilitated handling more complex datasets, potentially aiding in tasks such as anomaly detection or complex data synthesis, which could be resource-intensive with </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">traditional models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.215.1">UNet modules with self-attention</span></strong><span class="koboSpan" id="kobo.216.1">: Ho et al. </span><span class="koboSpan" id="kobo.216.2">also incorporated UNet modules with self-attention into the diffusion model architecture, inspired by PixelCNN++ by Salimans et al. </span><span class="koboSpan" id="kobo.216.3">(2017), enhancing a model’s performance on complex datasets (Ho et al., 2020). </span><span class="koboSpan" id="kobo.216.4">Again, enhancing performance on complex datasets facilitates better image restoration, which is particularly beneficial in fields such as medical imaging or satellite imagery analysis, where high-fidelity image reconstruction </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">is crucial.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.218.1">Synchronization with SDEs</span></strong><span class="koboSpan" id="kobo.219.1">: Song et al. </span><span class="koboSpan" id="kobo.219.2">defined diffusion models as solutions to SDEs, linking score learning with denoising score-matching losses and expanding model usage for image generation, editing, in-painting, and colorization (Song et </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">al., 2020).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.221.1">Following these foundational advancements, diffusion models witnessed a wave of innovative enhancements as researchers introduced novel methodologies to address existing challenges and broaden a model’s applicability in generative modeling tasks. </span><span class="koboSpan" id="kobo.221.2">These advancements include </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.223.1">Noise conditioning and annealing strategies</span></strong><span class="koboSpan" id="kobo.224.1">: Song et al. </span><span class="koboSpan" id="kobo.224.2">improved score-based models by including noise conditioning and annealing strategies, achieving performance comparable to GANs on benchmark datasets like the Flickr-Faces-HQ dataset  (Song et al., 2021), which is a high-quality image dataset of human faces designed to measure GAN performance. </span><span class="koboSpan" id="kobo.224.3">Achieving performance comparable to GANs could make diffusion models a viable alternative for high-fidelity image generation tasks in areas where GANs are </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">traditionally used.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.226.1">Latent Diffusion Models</span></strong><span class="koboSpan" id="kobo.227.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.228.1">LDMs</span></strong><span class="koboSpan" id="kobo.229.1">): Rombach et al. </span><span class="koboSpan" id="kobo.229.2">addressed computational inefficiency by</span><a id="_idIndexMarker081"/><span class="koboSpan" id="kobo.230.1"> proposing LDMs, which operate in a compressed latent space learned by autoencoders, employing perceptual losses to create a visually equivalent, reduced latent space (Rombach et al., 2021). </span><span class="koboSpan" id="kobo.230.2">By addressing computational inefficiency, LDMs could expedite the image generation process, making them suitable for real-time applications or scenarios where computational resources </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">are limited.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.232.1">Classifier-free guidance</span></strong><span class="koboSpan" id="kobo.233.1">: Ho &amp; Salimans introduced classifier-free guidance for controlled generation without relying on pre-trained networks, marking a step toward</span><a id="_idIndexMarker082"/><span class="koboSpan" id="kobo.234.1"> more flexible generation techniques (Ho &amp; Salimans, 2022). </span><span class="koboSpan" id="kobo.234.2">This advancement led to more flexible generation techniques, enabling more controlled and customized image generation in applications such as design, advertising, or content creation without relying on </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">pre-trained networks.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.236.1">Subsequent explorations in the diffusion model domain extended its applications, </span><span class="No-Break"><span class="koboSpan" id="kobo.237.1">showcasing versatility:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.238.1">Video generation</span></strong><span class="koboSpan" id="kobo.239.1">: Ho et al. </span><span class="koboSpan" id="kobo.239.2">adapted diffusion models for video generation, demonstrating their utility beyond static image generation (Ho et </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">al., 2022)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.241.1">3D data processing</span></strong><span class="koboSpan" id="kobo.242.1">: Luo &amp; Hu extended the application to 3D data processing, showcasing the flexibility of diffusion models (Luo &amp; </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">Hu, 2021)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.244.1">The evolution of diffusion models has led to enhanced image generation and expanded applications in video, 3D data processing, and rapid learning methodologies. </span><span class="koboSpan" id="kobo.244.2">However, the methodology</span><a id="_idIndexMarker083"/><span class="koboSpan" id="kobo.245.1"> does have its  challenges and limitations, outlined in some det</span><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.246.1">ail in the section </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">that follows..</span></span></p>
<h3><span class="koboSpan" id="kobo.248.1">Limitations and challenges of diffusion models</span></h3>
<p><span class="koboSpan" id="kobo.249.1">Despite their evident</span><a id="_idIndexMarker084"/><span class="koboSpan" id="kobo.250.1"> benefits and notable progress, diffusion models have some unique limitations, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.252.1">Sampling speed</span></strong><span class="koboSpan" id="kobo.253.1">: A notable limitation of diffusion models is the slow sampling process, particularly when compared to GANs. </span><span class="koboSpan" id="kobo.253.2">Sampling, in this context, refers to the process of generating new data points from the learned distribution of a model. </span><span class="koboSpan" id="kobo.253.3">The speed at which new samples can be generated is crucial for many real-time or near-real-time applications, and the slower sampling speed of diffusion models can be a </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">significant drawback.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.255.1">Stability during large-scale training</span></strong><span class="koboSpan" id="kobo.256.1">: The stability of diffusion models during large-scale training is another area requiring further exploration. </span><span class="koboSpan" id="kobo.256.2">Large-scale training refers to training a model on a substantial amount of data, sometimes leading to instability in the model’s learning process. </span><span class="koboSpan" id="kobo.256.3">Ensuring stability during this phase is crucial to achieve reliable and consistent performance from </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">the model.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.258.1">A close examination of the societal impact of the media generated by these models is crucial, especially given the level of fine control now possible over the generated content. </span><span class="koboSpan" id="kobo.258.2">However, diffusion models’ inherent simplicity, versatility, and positive inductive biases </span><a id="_idIndexMarker085"/><span class="koboSpan" id="kobo.259.1">signify a bright future. </span><span class="koboSpan" id="kobo.259.2">These attributes suggest a trajectory of rapid development within generative modeling, potentially integrating diffusion models as pivotal components in various disciplines, su</span><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.260.1">ch as computer vision </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">and graphics.</span></span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.262.1">A closer look at generative transformers</span></h2>
<p><span class="koboSpan" id="kobo.263.1">The revolutionary </span><a id="_idIndexMarker086"/><span class="koboSpan" id="kobo.264.1">advent of transformer models has significantly impacted the task of generating high-fidelity images from text descriptions. </span><span class="koboSpan" id="kobo.264.2">Notable models such as </span><strong class="bold"><span class="koboSpan" id="kobo.265.1">CLIP </span></strong><span class="koboSpan" id="kobo.266.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.267.1">Contrastive Language-Image Pretraining</span></strong><span class="koboSpan" id="kobo.268.1">) and DALL-E utilized transformers in unique ways to</span><a id="_idIndexMarker087"/><span class="koboSpan" id="kobo.269.1"> create images based on natural language captions. </span><span class="koboSpan" id="kobo.269.2">This section will discuss the transformer-based approach for text-to-image generation, its foundations, the key techniques, the res</span><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.270.1">ulting benefits, and </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">some challenges.</span></span></p>
<h3><span class="koboSpan" id="kobo.272.1">A brief</span><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.273.1"> overview of transformer architecture</span></h3>
<p><span class="koboSpan" id="kobo.274.1">The original transformer </span><a id="_idIndexMarker088"/><span class="koboSpan" id="kobo.275.1">architecture, introduced by Vaswani et al. </span><span class="koboSpan" id="kobo.275.2">in 2017, is a cornerstone of many modern language-processing systems. </span><span class="koboSpan" id="kobo.275.3">In fact, the transformer may be considered the most important architecture in the area of GAI, as it is foundational to the GPT series of models and many other state-of-the-art generative methods. </span><span class="koboSpan" id="kobo.275.4">As such, we’ll cover the architecture briefly in our survey of generative approaches but will have a dedicated chapter, where we will have the opportunity to deconstruct and implement the transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">from scratch.</span></span></p>
<p><span class="koboSpan" id="kobo.277.1">At the core of the transformer architecture lies</span><a id="_idIndexMarker089"/><span class="koboSpan" id="kobo.278.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.279.1">self-attention mechanism</span></strong><span class="koboSpan" id="kobo.280.1">, a unique approach that captures complex relationships among different elements within an ordered data sequence. </span><span class="koboSpan" id="kobo.280.2">These elements, known as </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">tokens</span></strong><span class="koboSpan" id="kobo.282.1">, represent</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.283.1"> words in a sentence or characters in a word based on the level of granularity chosen</span><a id="_idIndexMarker091"/> <span class="No-Break"><span class="koboSpan" id="kobo.284.1">for </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.285.1">tokenization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">The principle of </span><strong class="bold"><span class="koboSpan" id="kobo.288.1">attention</span></strong><span class="koboSpan" id="kobo.289.1"> in this architecture enables a model to focus on certain pivotal aspects of the input data while potentially disregarding less significant parts. </span><span class="koboSpan" id="kobo.289.2">This mechanism augments the model’s understanding of the context and the relative importance of words in </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">a sentence.</span></span></p>
<p><span class="koboSpan" id="kobo.291.1">The transformer bifurcates into two main segments, the </span><strong class="bold"><span class="koboSpan" id="kobo.292.1">encoder</span></strong><span class="koboSpan" id="kobo.293.1"> and the </span><strong class="bold"><span class="koboSpan" id="kobo.294.1">decoder</span></strong><span class="koboSpan" id="kobo.295.1">, each comprising multiple layers of self-attention mechanisms. </span><span class="koboSpan" id="kobo.295.2">While the encoder discerns relationships between different positions in the input sequence, the decoder focuses on the outputs from the encoder, employing a variant of self-attention termed </span><strong class="bold"><span class="koboSpan" id="kobo.296.1">masked self-attention</span></strong><span class="koboSpan" id="kobo.297.1"> to </span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.298.1">prevent consideration of future outputs it hasn’t </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">generated yet.</span></span></p>
<p><span class="koboSpan" id="kobo.300.1">The calculation of </span><strong class="bold"><span class="koboSpan" id="kobo.301.1">attention weights</span></strong><span class="koboSpan" id="kobo.302.1"> through the scaled dot-product of query and key vectors plays a</span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.303.1"> crucial role in determining the level of focus on different parts of the input. </span><span class="koboSpan" id="kobo.303.2">Additionally, </span><strong class="bold"><span class="koboSpan" id="kobo.304.1">multi-head attention</span></strong><span class="koboSpan" id="kobo.305.1"> allows</span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.306.1"> the model to channel attention toward multiple data </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">points simultaneously.</span></span></p>
<p><span class="koboSpan" id="kobo.308.1">Lastly, to retain the sequence order of data, the model adopts a strategy known as </span><strong class="bold"><span class="koboSpan" id="kobo.309.1">positional encoding</span></strong><span class="koboSpan" id="kobo.310.1">. </span><span class="koboSpan" id="kobo.310.2">This </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.311.1">mechanism is vital for tasks requiring an understanding of sequence or temporal dynamics, ensuring the model preserves the initial order of data throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">its processing.</span></span></p>
<p><span class="koboSpan" id="kobo.313.1">Again, we will revisit the transformer architecture in </span><a href="B21773_03.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.314.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.315.1"> to further reinforce our understanding, as it is foundational to the continued research and evolution of generative AI. </span><span class="koboSpan" id="kobo.315.2">Nevertheless, with at least a fundamental grasp of the Transformer architecture, we are better positioned to dissect transformer-driven generative modeling paradigms </span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.316.1">across a spectrum </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">of ap</span><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.318.1">plications.</span></span></p>
<h3><span class="koboSpan" id="kobo.319.1">Generative modeling paradigms with transformers</span></h3>
<p><span class="koboSpan" id="kobo.320.1">In tackling various </span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.321.1">tasks, transformers </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.322.1">adopt distinct training paradigms aligning with the task at hand. </span><span class="koboSpan" id="kobo.322.2">For example, discriminative tasks such as classification might use a </span><span class="No-Break"><span class="koboSpan" id="kobo.323.1">masking paradigm:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.324.1">Masked Language Modeling</span></strong><span class="koboSpan" id="kobo.325.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.326.1">MLM</span></strong><span class="koboSpan" id="kobo.327.1">): MLM is</span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.328.1"> a discriminative pretraining technique used by models such as </span><strong class="bold"><span class="koboSpan" id="kobo.329.1">BERT</span></strong><span class="koboSpan" id="kobo.330.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.331.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.332.1">). </span><span class="koboSpan" id="kobo.332.2">During </span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.333.1">training, some percentage of input tokens are randomly masked out. </span><span class="koboSpan" id="kobo.333.2">The model must then predict the original masked words based on the context of the surrounding unmasked words. </span><span class="koboSpan" id="kobo.333.3">This teaches the model to build robust context-based representations, facilitating many </span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.334.1">downstream </span><strong class="bold"><span class="koboSpan" id="kobo.335.1">natural language processing </span></strong><span class="koboSpan" id="kobo.336.1">(</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.337.1">NLP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">) tasks.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.339.1">MLM, as utilized in BERT, has been instrumental in enhancing the performance of NLP systems across various domains. </span><span class="koboSpan" id="kobo.339.2">For instance, it can power medical coding systems in healthcare by accurately identifying and categorizing medical terms within clinical notes. </span><span class="koboSpan" id="kobo.339.3">This automatic coding can save significant time and reduce errors in medical documentation, thereby improving the efficiency and accuracy of healthcare </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">data management.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">For generative tasks, the focus shifts to creating new data sequences, requiring different </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">training paradigms:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.343.1">Sequence-to-sequence modeling</span></strong><span class="koboSpan" id="kobo.344.1">: Sequence-to-sequence models employ both an encoder</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.345.1"> and a decoder. </span><span class="koboSpan" id="kobo.345.2">The encoder maps the input sequence to a latent representation. </span><span class="koboSpan" id="kobo.345.3">The decoder then generates the target sequence token by token from that representation. </span><span class="koboSpan" id="kobo.345.4">This paradigm is useful for tasks such as translation, summarization, </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">and question-answering.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.347.1">Autoregressive modeling</span></strong><span class="koboSpan" id="kobo.348.1">: Autoregressive modeling generates sequences by predicting the </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.349.1">next token conditioned only on previous tokens. </span><span class="koboSpan" id="kobo.349.2">The model produces outputs one step at a time, with each new token depending on those preceding it. </span><span class="koboSpan" id="kobo.349.3">Autoregressive transformers such as GPT leverage this technique for controlled text generation. </span></li>
</ul>
<p><span class="koboSpan" id="kobo.350.1">Transformers </span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.351.1">combine self-attention</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.352.1"> for long-range dependencies, pre-trained representations, and autoregressive decoding to adapt to discriminative and </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">generative tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.354.1">Advanced generative synthesis can be achieved with different architectures that make trade-offs between complexity, scalability, and specialization. </span><span class="koboSpan" id="kobo.354.2">For example, instead of using both the encoder and decoder, many state-of-the-art generative models employ a decoder-only or encoder-only approach. </span><span class="koboSpan" id="kobo.354.3">The encoder-decoder framework is often the most computationally intensive learning to specialize in, as it increases model size. </span><span class="koboSpan" id="kobo.354.4">Decoder-only architectures leverage powerful pre-trained language models such as GPT as the decoder, reducing parameters through weight sharing. </span><span class="koboSpan" id="kobo.354.5">Encoder-only methods forego decoding, instead, they encode inputs and perform regression or search on the resulting embeddings. </span><span class="koboSpan" id="kobo.354.6">Each approach has advantages that suit certain use cases, datasets, and computational budgets. </span><span class="koboSpan" id="kobo.354.7">In the following sections, we explore examples of models that </span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.355.1">employ these</span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.356.1"> derivative transformer architectures for creative applications, such as image generation </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">and captioning.</span></span></p>
<h3><span class="koboSpan" id="kobo.358.1">Encoder-only approach</span></h3>
<p><span class="koboSpan" id="kobo.359.1">In certain models, only </span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.360.1">the encoder network </span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.361.1">maps the input to an embedding space. </span><span class="koboSpan" id="kobo.361.2">The output is then generated directly from this embedding, eliminating the need for a decoder. </span><span class="koboSpan" id="kobo.361.3">While this straightforward architecture has typically found its place in classification or regression tasks, recent advancements have broadened its application to more complex tasks. </span><span class="koboSpan" id="kobo.361.4">In particular, models developed for tasks such as image synthesis leverage the encoder-only setup to process both text and visual inputs, creating a multimodal relationship that facilitates the generation of high-fidelity images from natural </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">language instruction.</span></span></p>
<h3><span class="koboSpan" id="kobo.363.1">Decoder-only approach</span></h3>
<p><span class="koboSpan" id="kobo.364.1">Similarly, some</span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.365.1"> models operate using a </span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.366.1">decoder-only strategy, where a singular decoder network is tasked with both encoding the input and generating output. </span><span class="koboSpan" id="kobo.366.2">This mechanism starts by joining the input and output sequences, which the decoder processes. </span><span class="koboSpan" id="kobo.366.3">Despite its simplicity and the characteristic sharing of parameters between input and output stages, the effectiveness of this architecture relies heavily on the pretraining of robust decoders. </span><span class="koboSpan" id="kobo.366.4">Recently, even more complex tasks such as text-to-image synthesis have seen the successful deployment of the decoder-only architecture, illustrating its versatility and adaptabi</span><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.367.1">lity to </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">diverse applications.</span></span></p>
<h3><span class="koboSpan" id="kobo.369.1">Advancement of transformers</span></h3>
<p><span class="koboSpan" id="kobo.370.1">Transformer </span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.371.1">mechanisms with other novel techniques to tackle generative tasks. </span><span class="koboSpan" id="kobo.371.2">This evolution led to distinct approaches to handling text and image generation. </span><span class="koboSpan" id="kobo.371.3">In this section, we will explore some of these innovative models and their unique methodologies in </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">advancing GAI.</span></span></p>
<h3><span class="koboSpan" id="kobo.373.1">Encoder-decoder image generation with DALL-E</span></h3>
<p><span class="koboSpan" id="kobo.374.1">Introduced by</span><a id="_idIndexMarker113"/><span class="koboSpan" id="kobo.375.1"> Ramesh </span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.376.1">et al. </span><span class="koboSpan" id="kobo.376.2">in 2021, DALL-E employs an </span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.377.1">encoder-decoder framework to facilitate text-to-image generation. </span><span class="koboSpan" id="kobo.377.2">This model comprises two </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">primary components:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.379.1">Text encoder</span></strong><span class="koboSpan" id="kobo.380.1">: Applies the transformer’s encoder, processing plain text to derive a semantic embedding that serves as the context for the </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">image decoder.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.382.1">Image decoder</span></strong><span class="koboSpan" id="kobo.383.1">: Applies the transformer’s decoder to generate the image autoregressively, predicting each pixel based on the text embedding and previously </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">predicted pixels.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.385.1">By training on image-caption datasets, DALL-E refines the transition from text to detailed image renderings. </span><span class="koboSpan" id="kobo.385.2">This setup underscores the capability of dedicated encoder and decoder modules for conditional </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">image generation.</span></span></p>
<h3><span class="koboSpan" id="kobo.387.1">Encoder-only image captioning with CLIP</span></h3>
<p><span class="koboSpan" id="kobo.388.1">CLIP, conceptualized</span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.389.1"> by Radford</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.390.1"> et al. </span><span class="koboSpan" id="kobo.390.2">in 2021, adopts an encoder-only approach for image-text tasks. </span><span class="koboSpan" id="kobo.390.3">Key components include a visual encoder and a </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">text encoder.</span></span></p>
<p><span class="koboSpan" id="kobo.392.1">Visual Encoder and Text Encoder process the image and candidate captions, respectively, determining the matching caption based on </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">encoded representations.</span></span></p>
<p><span class="koboSpan" id="kobo.394.1">Pretraining on extensive image-text datasets enables CLIP to establish a shared embedding space, facilitating efficient inference for </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">retrieval-based captioning.</span></span></p>
<h3><span class="koboSpan" id="kobo.396.1">Improving image fidelity with scaled transformers (DALL-E 2)</span></h3>
<p><span class="koboSpan" id="kobo.397.1">Ramesh et al. </span><span class="koboSpan" id="kobo.397.2">in 2022 </span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.398.1">extended </span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.399.1">DALL-E to DALL-E 2, showcasing techniques to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.400.1">visual quality:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.401.1">A scaled-up decoder</span></strong><span class="koboSpan" id="kobo.402.1">: By expanding the decoder to 3.5 billion parameters and applying classifier-free guidance during sampling, visual quality in complex image distributions such as human faces is </span><span class="No-Break"><span class="koboSpan" id="kobo.403.1">significantly improved.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.404.1">Hierarchical decoding for high-resolution images (GLIDE)</span></strong><span class="koboSpan" id="kobo.405.1">: Proposed by Nichol et al. </span><span class="koboSpan" id="kobo.405.2">in 2021, GLIDE employs a hierarchical </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">generation strategy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.407.1">A coarse-to-fine approach</span></strong><span class="koboSpan" id="kobo.408.1">: This entails an initial low-resolution image prediction followed by progressive detailing through up-sampling and refining, capturing </span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.409.1">global structure and </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">high-frequency textures.</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.411.1">Multimodal image generation with GPT-4</span></h3>
<p><span class="koboSpan" id="kobo.412.1">GPT-4 developed</span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.413.1"> by OpenAI, is </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.414.1">a powerful multimodal model </span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.415.1">based on the Transformer architecture. </span><span class="koboSpan" id="kobo.415.2">GPT-4 demonstrates a capability for conditional image generation without requiring continued training </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">or fine-tuning:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.417.1">Pretraining and fine-tuning</span></strong><span class="koboSpan" id="kobo.418.1">: The massive scale of GPT-4 and its pretraining on diverse datasets enable a robust understanding of relationships between textual and </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">visual data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.420.1">Multimodal generation:</span></strong><span class="koboSpan" id="kobo.421.1"> GPT-4 can generate images based on text descriptions. </span><span class="koboSpan" id="kobo.421.2">The model uses a deep neural network to encode the semantic meaning of the text into a visual representation. </span><span class="koboSpan" id="kobo.421.3">Given a text prompt, GPT-4 generates an image by predicting the visual content consistent with the provided text. </span><span class="koboSpan" id="kobo.421.4">This involves taking high-dimensional text embeddings and processing them through successive neural network layers to generate a corresponding </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">visual representation.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.423.1">Using a pretrained multimodal model eliminates the need for a separate encoder module for image inputs, facilitating rapid adaptation for image generation tasks. </span><span class="koboSpan" id="kobo.423.2">This approach underscores the versatility and power of Transformer architectures in generative tasks, providing a streamlined methodology to translate text into </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">high-quality images.</span></span></p>
<p><span class="koboSpan" id="kobo.425.1">Transformer architectures offer many benefits for controlled image generation when compared to GANs. </span><span class="koboSpan" id="kobo.425.2">Their autoregressive nature ensures precise control over image construction while allowing you to adapt to varying computational needs and diverse downstream</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.426.1"> applica</span><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.427.1">tions. </span><span class="koboSpan" id="kobo.427.2">However, transformers </span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.428.1">also</span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.429.1"> introduce new challenges in </span><span class="No-Break"><span class="koboSpan" id="kobo.430.1">this domain.</span></span></p>
<h3><span class="koboSpan" id="kobo.431.1">Limitations and challenges of transformer-based approaches</span></h3>
<p><span class="koboSpan" id="kobo.432.1">Some early </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.433.1">transformers-based approaches demonstrated slower sampling speed and restricted fidelity compared to GANs. </span><span class="koboSpan" id="kobo.433.2">Generating or manipulating images while maintaining precise control over specific attributes or characteristics of the objects within those images remains challenging. </span><span class="koboSpan" id="kobo.433.3">Additionally, training large-scale transformers that can overcome these challenges demands extensive computing resources. </span><span class="koboSpan" id="kobo.433.4">Notwithstanding, current multimodal results demonstrate a rapidly evolving and </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">promising landscape.</span></span></p>
<p><span class="koboSpan" id="kobo.435.1">We must also remember that alongside technical</span><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.436.1"> challenges there are broader sociotechnical implications </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">and considerations.</span></span></p>
<h3><span class="koboSpan" id="kobo.438.1">Bias and ethics in generative models</span></h3>
<p><span class="koboSpan" id="kobo.439.1">Significant </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.440.1">advancements in generative models such as GANs, diffusers, and transformers necessitate serious contemplation of potential bias and </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">ethical implications.</span></span></p>
<p><span class="koboSpan" id="kobo.442.1">We need to remain alert to the risk of reinforcing prejudices and stereotypes that reflect skewed training data. </span><span class="koboSpan" id="kobo.442.2">For instance, diffusion models trained on data that over-represents specific demographics might propagate these biases in their output. </span><span class="koboSpan" id="kobo.442.3">Analogously, language models exposed to toxic or violent content during training might generate </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">similar content.</span></span></p>
<p><span class="koboSpan" id="kobo.444.1">The directive nature of prompt-based generation also, unfortunately, opens doors to misuse if deployed carelessly. </span><span class="koboSpan" id="kobo.444.2">Transformers risk facilitating impersonation, misinformation, and the creation of deceptive content. </span><span class="koboSpan" id="kobo.444.3">Image synthesis models such as GANs could potentially be exploited to generate non-consensual deepfakes or </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">artificial media.</span></span></p>
<p><span class="koboSpan" id="kobo.446.1">Additionally, the potential for ultra-realistic output prompts ethical dilemmas regarding consent, privacy, identity, and copyright. </span><span class="koboSpan" id="kobo.446.2">The ability to create convincingly real yet fictional faces or voices complicates the distinction between real and synthetic, necessitating careful examination of training data sources and </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">generative capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.448.1">Further, as these technologies become ubiquitous, their societal impact must be considered. </span><span class="koboSpan" id="kobo.448.2">Defining clear policies will be crucial as the distinction between authentic and AI-generated content becomes increasingly ambiguous. </span><span class="koboSpan" id="kobo.448.3">Upholding principles of integrity, attribution, and consent </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">remains vital.</span></span></p>
<p><span class="koboSpan" id="kobo.450.1">Despite these risks, the potential benefits of generative models are substantial. </span><span class="koboSpan" id="kobo.450.2">Addressing bias proactively, advocating transparency, auditing data and models, and implementing safeguards become increasingly critical as technologies evolve. </span><span class="koboSpan" id="kobo.450.3">Ultimately, the </span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.451.1">responsibility to ensure fairness, </span><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.452.1">accountability, and ethical practice falls on all developers </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">and practitioners.</span></span></p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.454.1">Applying GAI models – image generation using GANs, diffusers, and transformers</span></h1>
<p><span class="koboSpan" id="kobo.455.1">In this hands-on </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.456.1">section, we’ll reinforce</span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.457.1"> the </span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.458.1">concepts </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.459.1">discussed throughout the </span><a id="_idIndexMarker134"/><span class="koboSpan" id="kobo.460.1">chapter</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.461.1"> by</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.462.1"> putting them into practice. </span><span class="koboSpan" id="kobo.462.2">You’ll get a first-hand experience and deep dive into the actual implementation of generative models, specifically GANs, diffusion models, </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">and transformers.</span></span></p>
<p><span class="koboSpan" id="kobo.464.1">The Python code provided will guide you through this process. </span><span class="koboSpan" id="kobo.464.2">Manipulating and observing the code in action will build your understanding of the intricate workings and potential applications of these models. </span><span class="koboSpan" id="kobo.464.3">This exercise will provide insight into model capabilities for tasks like generating art from prompts and synthesizing </span><span class="No-Break"><span class="koboSpan" id="kobo.465.1">hyper-realistic images.</span></span></p>
<p><span class="koboSpan" id="kobo.466.1">We’ll be utilizing the highly versatile </span><strong class="source-inline"><span class="koboSpan" id="kobo.467.1">PyTorch</span></strong><span class="koboSpan" id="kobo.468.1"> library, a popular choice among machine learning practitioners, to facilitate our operations. </span><strong class="source-inline"><span class="koboSpan" id="kobo.469.1">PyTorch</span></strong><span class="koboSpan" id="kobo.470.1"> provides a powerful and dynamic toolset to define and compute gradients, which is central to training </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">these models.</span></span></p>
<p><span class="koboSpan" id="kobo.472.1">In addition, we’ll also use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.473.1">diffusers</span></strong><span class="koboSpan" id="kobo.474.1"> library. </span><span class="koboSpan" id="kobo.474.2">It’s a specialized library that provides functionality to implement diffusion models. </span><span class="koboSpan" id="kobo.474.3">This library enables us to reproduce state-of-the-art diffusion models directly from our workspace. </span><span class="koboSpan" id="kobo.474.4">It underpins the creation, training, and usage of denoising diffusion probabilistic models at an unprecedented level of simplicity, without compromising the </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">models’ complexity.</span></span></p>
<p><span class="koboSpan" id="kobo.476.1">Through this practical session, we’ll explore how to operate and integrate these libraries and implement and manipulate GANs, diffusers, and transformers using the Python programming language. </span><span class="koboSpan" id="kobo.476.2">This hands-on experience will complement the theoretical knowledge we have gained in the chapter, enabling us to see these models in action in the </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">real world.</span></span></p>
<p><span class="koboSpan" id="kobo.478.1">By the end of this section, you will not only have a conceptual understanding of these generative models but also understand how they are implemented, trained, and used for several innovative applications in data science and machine learning. </span><span class="koboSpan" id="kobo.478.2">You’ll have a much</span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.479.1"> deeper </span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.480.1">understanding</span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.481.1"> of how these </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.482.1">models </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.483.1">work </span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.484.1">and </span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.485.1">the experience of implementing </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">them yourself.</span></span></p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.487.1">Working with Jupyter Notebook and Google Colab</span></h2>
<p><span class="koboSpan" id="kobo.488.1">Jupyter notebooks </span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.489.1">enable live code execution, visualization, and</span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.490.1"> explanatory text, suitable for prototyping and data analysis. </span><span class="koboSpan" id="kobo.490.2">Google Colab, conversely, is a cloud-based version of Jupyter Notebook, designed for machine learning prototyping. </span><span class="koboSpan" id="kobo.490.3">It provides free GPU resources and integrates with Google Drive f</span><a id="_idTextAnchor070"/><span class="koboSpan" id="kobo.491.1">or file storage and sharing. </span><span class="koboSpan" id="kobo.491.2">We’ll leverage Colab as our prototyping environment </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">going forward.</span></span></p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor071"/><span class="koboSpan" id="kobo.493.1">Stable diffusion transformer</span></h2>
<p><span class="koboSpan" id="kobo.494.1">We begin with a </span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.495.1">pre-trained stable diffusion model, a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION (Patil et al., 2022). </span><span class="koboSpan" id="kobo.495.2">The diffusion process is used to draw samples from complex, high-dimensional distributions, and when it interacts with the text embeddings, it creates a powerful conditional image </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">synthesis model.</span></span></p>
<p><span class="koboSpan" id="kobo.497.1">The term “stable” in this context refers to the fact that during training, a model maintains certain properties that stabilize the learning process. </span><span class="koboSpan" id="kobo.497.2">Stable diffusion models offer rich potential to create entirely new samples from a given data distribution, based on </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">text prompts.</span></span></p>
<p><span class="koboSpan" id="kobo.499.1">Again, for our practical example, we will Google Colab to alleviate a lot of initial setups. </span><span class="koboSpan" id="kobo.499.2">Colab also provides all of the computational resources needed to begin experimenting right away.  </span><span class="koboSpan" id="kobo.499.3">We start by installing some libraries, and with three simple functions, we will build out a minimal </span><strong class="source-inline"><span class="koboSpan" id="kobo.500.1">StableDiffusionPipeline</span></strong><span class="koboSpan" id="kobo.501.1"> using a well-established open-source implementation of the stable </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">diffusion method.</span></span></p>
<p><span class="koboSpan" id="kobo.503.1">First, let’s navigate to our pre-configured Python environment, Google Colab, and install the </span><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">diffusers</span></strong><span class="koboSpan" id="kobo.505.1"> open-source library, which will provide most of the key underlying components we need for </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">our experiment.</span></span></p>
<p><span class="koboSpan" id="kobo.507.1">In the first cell, we install all dependencies using the following </span><strong class="source-inline"><span class="koboSpan" id="kobo.508.1">bash</span></strong><span class="koboSpan" id="kobo.509.1"> command. </span><span class="koboSpan" id="kobo.509.2">Note the exclamation point at the beginning of the line, which tells our environment to reach down to its underlying</span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.510.1"> process and install the packages </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">we need:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.512.1">
!pip install pytorch-fid torch diffusers clip transformers accelerate</span></pre>
<p><span class="koboSpan" id="kobo.513.1">Next, we import the libraries we’ve just installed to make them avai</span><a id="_idTextAnchor072"/><span class="koboSpan" id="kobo.514.1">lable to our </span><span class="No-Break"><span class="koboSpan" id="kobo.515.1">Python program:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.516.1">
from typing import List
import torch
import matplotlib.pyplot as plt
from diffusers import StableDiffusionPipeline, DDPMScheduler</span></pre>
<p><span class="koboSpan" id="kobo.517.1">Now, we’re ready for our three functions, which will execute the three tasks – loading the pre-trained model, generating the images based on prompting, and rendering </span><span class="No-Break"><span class="koboSpan" id="kobo.518.1">the images:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.519.1">
def load_model(model_id: str) -&gt; StableDiffusionPipeline:
    """Load model with provided model_id."""
</span><span class="koboSpan" id="kobo.519.2">    return StableDiffusionPipeline.from_pretrained(
        model_id, 
        torch_dtype=torch.float16, 
        revision="fp16", 
        use_auth_token=False
    ).to("cuda")
def generate_images(
    pipe: StableDiffusionPipeline, 
    prompts: List[str]
) -&gt; torch.Tensor:
    """Generate images based on provided prompts."""
</span><span class="koboSpan" id="kobo.519.3">    with torch.autocast("cuda"):
        images = pipe(prompts).images
    return images
def render_images(images: torch.Tensor):
    """Plot the generated images."""
</span><span class="koboSpan" id="kobo.519.4">    plt.figure(figsize=(10, 5))
    for i, img in enumerate(images):
        plt.subplot(1, 2, i + 1)
        plt.imshow(img)
        plt.axis("off")
    plt.show()</span></pre>
<p><span class="koboSpan" id="kobo.520.1">In summary, </span><strong class="source-inline"><span class="koboSpan" id="kobo.521.1">load_model</span></strong><span class="koboSpan" id="kobo.522.1"> loads a machine learning model identified by </span><strong class="source-inline"><span class="koboSpan" id="kobo.523.1">model_id</span></strong><span class="koboSpan" id="kobo.524.1"> onto a GPU for faster </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.525.1">processing. </span><span class="koboSpan" id="kobo.525.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.526.1">generate_images</span></strong><span class="koboSpan" id="kobo.527.1"> function takes this model and a list of prompts to create our images. </span><span class="koboSpan" id="kobo.527.2">Within this function, you will notice </span><strong class="source-inline"><span class="koboSpan" id="kobo.528.1">torch.autocast("cuda")</span></strong><span class="koboSpan" id="kobo.529.1">, which is a special command that allows PyTorch (our underlying machine learning library) to perform operations faster while maintaining accuracy. </span><span class="koboSpan" id="kobo.529.2">Lastly, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">render_images</span></strong><span class="koboSpan" id="kobo.531.1"> function displays these images in a simple grid format, making use of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.532.1">matplotlib</span></strong><span class="koboSpan" id="kobo.533.1"> visualization library to render </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">our output.</span></span></p>
<p><span class="koboSpan" id="kobo.535.1">With our functions defined, we select our mod</span><a id="_idTextAnchor073"/><span class="koboSpan" id="kobo.536.1">el version, define our pipeline, and execute our image </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">generation process:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.538.1">
# Execution
model_id = "CompVis/stable-diffusion-v1-4"
prompts = [
    "A hyper-realistic photo of a friendly lion",
    "A stylized oil painting of a NYC Brownstone"
]
pipe = load_model(model_id)
images = generate_images(pipe, prompts)
render_images(images)</span></pre>
<p><span class="koboSpan" id="kobo.539.1">The output in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.540.1">Figure 2</span></em></span><em class="italic"><span class="koboSpan" id="kobo.541.1">.1</span></em><span class="koboSpan" id="kobo.542.1"> is a vivid example of the imaginativeness and creativity we typically expect</span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.543.1"> from human art, generated entirely by the diffusion process. </span><span class="koboSpan" id="kobo.543.2">Except, how do we measure whether the model was faithful to the </span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">text provided?</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<span class="koboSpan" id="kobo.545.1"><img alt="Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion” (left) and “A stylized oil painting of a NYC Brownstone” (right)" src="image/B21773_02_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.546.1">Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion” (left) and “A stylized oil painting of a NYC Brownstone” (right)</span></p>
<p><span class="koboSpan" id="kobo.547.1">The next step is to evaluate the quality and relevance of our generated images in relation to the prompts. </span><span class="koboSpan" id="kobo.547.2">This is where CLIP comes into play. </span><span class="koboSpan" id="kobo.547.3">CLIP is designed to measure the alignment</span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.548.1"> between text and images by analyzing their semantic similar</span><a id="_idTextAnchor074"/><span class="koboSpan" id="kobo.549.1">i</span><a id="_idTextAnchor075"/><span class="koboSpan" id="kobo.550.1">ties, giving us a true quantitative measure of the fidelity of our synthetic images to </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">the prompts.</span></span></p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor076"/><span class="koboSpan" id="kobo.552.1">Scoring with the CLIP model</span></h2>
<p><span class="koboSpan" id="kobo.553.1">CLIP is trained to </span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.554.1">understand the relationship between text and images by learning to place similar images and text near each other in a shared space. </span><span class="koboSpan" id="kobo.554.2">When evaluating a generated image, CLIP checks how closely the image aligns with the textual description provided. </span><span class="koboSpan" id="kobo.554.3">A higher score indicates a better match, meaning the image accurately represents the text. </span><span class="koboSpan" id="kobo.554.4">Conversely, a lower score suggests a deviation from the text, indicating a lesser quality or fidelity to the prompt, providing a quantitative measure of how well the generated image adheres to the </span><span class="No-Break"><span class="koboSpan" id="kobo.555.1">intended description.</span></span></p>
<p><span class="koboSpan" id="kobo.556.1">Again, we will import the </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">necessary libraries:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.558.1">
from typing import List, Tuple
from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel
import torch</span></pre>
<p><span class="koboSpan" id="kobo.559.1">We begin by loading the CLIP model, processor, and </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">necessary parameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.561.1">
# Constants
CLIP_REPO = "openai/clip-vit-base-patch32"
def load_model_and_processor(
    model_name: str
) -&gt; Tuple[CLIPModel, CLIPProcessor]:
    """
    Loads the CLIP model and processor.
    </span><span class="koboSpan" id="kobo.561.2">"""
    model = CLIPModel.from_pretrained(model_name)
    processor = CLIPProcessor.from_pretrained(model_name)
    return model, processor</span></pre>
<p><span class="koboSpan" id="kobo.562.1">Next, we define a processing function to adjust the textual prompts and images, ensuring that they are in the correct format for </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">CLIP inference:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.564.1">
def process_inputs(
    processor: CLIPProcessor, prompts: List[str],
    images: List[Image.Image]) -&gt; dict:
"""
Processes the inputs using the CLIP processor.
</span><span class="koboSpan" id="kobo.564.2">"""
    return processor(text=prompts, images=images,
        return_tensors="pt", padding=True)</span></pre>
<p><span class="koboSpan" id="kobo.565.1">In this step, we</span><a id="_idIndexMarker152"/><span class="koboSpan" id="kobo.566.1"> initiate the evaluation process by inputting the images and textual prompts into the CLIP model. </span><span class="koboSpan" id="kobo.566.2">This is done in parallel across multiple devices to optimize performance. </span><span class="koboSpan" id="kobo.566.3">The model then computes similarity scores, known as logits, for each image-text pair. </span><span class="koboSpan" id="kobo.566.4">These scores indicate how well each image corresponds to the text prompts. </span><span class="koboSpan" id="kobo.566.5">To interpret these scores more intuitively, we convert them into probabilities, which indicate the likelihood that an image aligns with any of the </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">given prompts:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.568.1">
def get_probabilities(
    model: CLIPModel, inputs: dict) -&gt; torch.Tensor:
"""
Computes the probabilities using the CLIP model.
</span><span class="koboSpan" id="kobo.568.2">"""
    outputs = model(**inputs)
    logits = outputs.logits_per_image
    # Define temperature - higher temperature will make the distribution more uniform.
</span><span class="koboSpan" id="kobo.568.3">    T = 10
    # Apply temperature to the logits
    temp_adjusted_logits = logits / T
    probs = torch.nn.functional.softmax(
        temp_adjusted_logits, dim=1)
    return probs</span></pre>
<p><span class="koboSpan" id="kobo.569.1">Lastly, we display the images along with their scores, visually representing how well each image </span><a id="_idIndexMarker153"/><span class="koboSpan" id="kobo.570.1">adheres to the </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">provided prompts:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.572.1">
def display_images_with_scores(
    images: List[Image.Image], scores: torch.Tensor) -&gt; None:
"""
Displays the images alongside their scores.
</span><span class="koboSpan" id="kobo.572.2">"""
    # Set print options for readability
    torch.set_printoptions(precision=2, sci_mode=False)
    for i, image in enumerate(images):
        print(f"Image {i + 1}:")
        display(image)
        print(f"Scores: {scores[i, :]}")
        print()</span></pre>
<p><span class="koboSpan" id="kobo.573.1">With everything detailed, let’s execute the pipeline </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.575.1">
# Load CLIP model
model, processor = load_model_and_processor(CLIP_REPO)
# Process image and text inputs together
inputs = process_inputs(processor, prompts, images)
# Extract the probabilities
probs = get_probabilities(model, inputs)
# Display each image with corresponding scores
display_images_with_scores(images, probs)</span></pre>
<p><span class="koboSpan" id="kobo.576.1">We now have scores for each of our synthetic images that quantify the fidelity of the synthetic image to the text provided, based on the CLIP model, which interprets both image and text</span><a id="_idIndexMarker154"/><span class="koboSpan" id="kobo.577.1"> data as one combined mathematical representation (or geometric space) and can measure </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">their similarity.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<span class="koboSpan" id="kobo.579.1"><img alt="Figure 2.2: CLIP scores" src="image/B21773_02_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.580.1">Figure 2.2: CLIP scores</span></p>
<p><span class="koboSpan" id="kobo.581.1">For our “friendly lion,” we computed scores of 83% and 17% for each prompt, which we can interpret as an 83% likelihood that the image aligns with the </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">first prompt.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">In practical scenarios, this metric can be applied across </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">various domains:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.585.1">Content moderation</span></strong><span class="koboSpan" id="kobo.586.1">: Automatically moderating or flagging inappropriate content by comparing images to a set of predefined </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">descriptive prompts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.588.1">Image retrieval</span></strong><span class="koboSpan" id="kobo.589.1">: Facilitating refined image searches by matching textual queries to a vast database of images, hence narrowing down the search to the most </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">relevant visuals</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.591.1">Image captioning</span></strong><span class="koboSpan" id="kobo.592.1">: Assisting in generating accurate captions for images by identifying the most relevant </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">descriptive prompts</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.594.1">Advertising</span></strong><span class="koboSpan" id="kobo.595.1">: Tailoring advertisements based on the content of images on a web page to enhance </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">user engagement</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.597.1">Accessibility</span></strong><span class="koboSpan" id="kobo.598.1">: Enhancing accessibility features by providing accurate descriptions of images for individuals with </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">visual impairments</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.600.1">This evaluation method not only speeds up processes that would otherwise require manual inspection but also lends itself to many applications that could benefit from a deeper understanding and contextual analysis of visual data. </span><span class="koboSpan" id="kobo.600.2">We will revisit the CLIP evaluation in </span><a href="B21773_04.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.601.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.602.1">, where we simulate a real-world scenario to determine the quality and </span><a id="_idIndexMarker155"/><span class="koboSpan" id="kobo.603.1">appropriateness of automatically generated captions for a set of </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">product images.</span></span></p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor077"/><span class="koboSpan" id="kobo.605.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.606.1">This chapter explored the theoretical underpinnings and real-world applications of leading GAI techniques, including GANs, diffusion models, and transformers. </span><span class="koboSpan" id="kobo.606.2">We examined their unique strengths, including GANs’ ability to synthesize highly realistic images, diffusion models’ elegant image generation process, and transformers’ exceptional language </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">generation capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.608.1">Using a cloud-based Python environment, we implemented these models to generate compelling images and evaluated their output quality using CLIP. </span><span class="koboSpan" id="kobo.608.2">We analyzed how techniques such as progressive growing and classifier guidance enhanced output fidelity over time. </span><span class="koboSpan" id="kobo.608.3">We also considered societal impacts, urging developers to address potential harm through transparency and </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">ethical practices.</span></span></p>
<p><span class="koboSpan" id="kobo.610.1">Generative methods have unlocked remarkable creative potential, but thoughtful oversight is critical as capabilities advance. </span><span class="koboSpan" id="kobo.610.2">We can guide these technologies toward broadly beneficial outcomes by grounding ourselves in core methodologies, scrutinizing their limitations, and considering downstream uses. </span><span class="koboSpan" id="kobo.610.3">The path ahead will re</span><a id="_idTextAnchor078"/><span class="koboSpan" id="kobo.611.1">quire continued research and ethical reflection to unlock AI’s creative promise while </span><span class="No-Break"><span class="koboSpan" id="kobo.612.1">mitigating risks.</span></span></p>
<h1 id="_idParaDest-44"><a id="_idTextAnchor079"/><span class="koboSpan" id="kobo.613.1">References</span></h1>
<p><span class="koboSpan" id="kobo.614.1">This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">subject matter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.616.1">Kenfack, P. </span><span class="koboSpan" id="kobo.616.2">J., Arapov, D. </span><span class="koboSpan" id="kobo.616.3">D., Hussain, R., Ahsan Kazmi, S. </span><span class="koboSpan" id="kobo.616.4">M., &amp; Khan, A. </span><span class="koboSpan" id="kobo.616.5">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.617.1">On the fairness of generative adversarial networks (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.618.1">GANs)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">. </span></span><a href="https://Arxiv.org"><span class="No-Break"><span class="koboSpan" id="kobo.620.1">Arxiv.org</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.621.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.622.1">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. </span><span class="koboSpan" id="kobo.622.2">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.623.1">Generative adversarial nets. </span><span class="koboSpan" id="kobo.623.2">Advances in neural information processing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.624.1">systems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">, 27.</span></span></li>
<li><span class="koboSpan" id="kobo.626.1">Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., &amp; Chen, M. </span><span class="koboSpan" id="kobo.626.2">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.627.1">GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models</span></em><span class="koboSpan" id="kobo.628.1">. </span><span class="koboSpan" id="kobo.628.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">preprint arXiv:2112.10741.</span></span></li>
<li><span class="koboSpan" id="kobo.630.1">Radford, A., Kim, J. </span><span class="koboSpan" id="kobo.630.2">W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. </span><span class="koboSpan" id="kobo.630.3">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.631.1">Learning Transferable Visual Models From Natural Language Supervision.</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.632.1">ArXiv. </span><span class="koboSpan" id="kobo.632.2">/abs/2103.00020.</span></span></li>
<li><span class="koboSpan" id="kobo.633.1">Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. </span><span class="koboSpan" id="kobo.633.2">(2022).</span><em class="italic"><span class="koboSpan" id="kobo.634.1"> Hierarchical text-conditional image generation with clip latents</span></em><span class="koboSpan" id="kobo.635.1">. </span><span class="koboSpan" id="kobo.635.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">preprint arXiv:2204.06125.</span></span></li>
<li><span class="koboSpan" id="kobo.637.1">Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. </span><span class="koboSpan" id="kobo.637.2">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.638.1">Zero-shot text-to-image generation. </span><span class="koboSpan" id="kobo.638.2">In International Conference on Machine Learning</span></em><span class="koboSpan" id="kobo.639.1"> (pp. </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">8821–8831). </span><span class="koboSpan" id="kobo.640.2">PMLR.</span></span></li>
<li><span class="koboSpan" id="kobo.641.1">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. </span><span class="koboSpan" id="kobo.641.2">N., Kaiser, L., &amp; Polosukhin, I. </span><span class="koboSpan" id="kobo.641.3">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.642.1">Attention is all you need. </span><span class="koboSpan" id="kobo.642.2">Advances in neural information processing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.643.1">systems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.644.1">, 30.</span></span></li>
<li><span class="koboSpan" id="kobo.645.1">Arjovsky, M., Chintala, S. </span><span class="koboSpan" id="kobo.645.2">&amp; Bottou, L. </span><span class="koboSpan" id="kobo.645.3">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.646.1">Wasserstein GAN. </span><span class="koboSpan" id="kobo.646.2">In Proceedings of the 31st International Conference on Neural Information Processing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.647.1">System (NIPS).</span></em></span></li>
<li><span class="koboSpan" id="kobo.648.1">Brock, A., Donahue, J., &amp; Simonyan, K. </span><span class="koboSpan" id="kobo.648.2">(2018). </span><em class="italic"><span class="koboSpan" id="kobo.649.1">BigGANs: Large Scale GAN Training for High Fidelity Natural Image </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.650.1">Synthesis.</span></em></span><span class="No-Break"> </span><a href="https://arxiv.org/abs/1809.11096"><span class="No-Break"><span class="koboSpan" id="kobo.651.1">https://arxiv.org/abs/1809.11096</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.652.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.653.1">Karras, T., Aila, T., Laine, S., &amp; Lehtinen, J. </span><span class="koboSpan" id="kobo.653.2">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.654.1">Progressive Growing of GANs for Improved Quality, Stability, and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.655.1">Variation.</span></em></span><span class="No-Break"> </span><a href="https://arxiv.org/abs/1710.10196"><span class="No-Break"><span class="koboSpan" id="kobo.656.1">https://arxiv.org/abs/1710.10196</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.657.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.658.1">Mirza, M., &amp; Osindero, S. </span><span class="koboSpan" id="kobo.658.2">(2014). </span><em class="italic"><span class="koboSpan" id="kobo.659.1">Conditional Generative Adversarial </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.660.1">Nets</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">. </span></span><a href="https://arxiv.org/abs/1411.1784"><span class="No-Break"><span class="koboSpan" id="kobo.662.1">https://arxiv.org/abs/1411.1784</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.663.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.664.1">Radford, A., Metz, L., &amp; Chintala, S. </span><span class="koboSpan" id="kobo.664.2">(2015). </span><em class="italic"><span class="koboSpan" id="kobo.665.1">Unsupervised representation learning with deep convolutional generative adversarial networks. </span><span class="koboSpan" id="kobo.665.2">3rd International Conference for </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.666.1">Learning Representations.</span></em></span></li>
<li><span class="koboSpan" id="kobo.667.1">Zhu, J.-Y., Park, T., Isola, P., &amp; Efros, A. </span><span class="koboSpan" id="kobo.667.2">A. </span><span class="koboSpan" id="kobo.667.3">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.668.1">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. </span><span class="koboSpan" id="kobo.668.2">In Proceedings of the IEEE International Conference on Computer </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.669.1">Vision (ICCV).</span></em></span></li>
<li><span class="koboSpan" id="kobo.670.1">Ho, J., &amp; Salimans, T. </span><span class="koboSpan" id="kobo.670.2">(2022). </span><em class="italic"><span class="koboSpan" id="kobo.671.1">Classifier-Free Diffusion Guidance. </span><span class="koboSpan" id="kobo.671.2">Advances in Neural Information Processing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.672.1">Systems</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">, 34.</span></span></li>
<li><span class="koboSpan" id="kobo.674.1">Ho, J., Salimans, T., Gritsenko, A. </span><span class="koboSpan" id="kobo.674.2">A., Chan, W., Norouzi, M., &amp; Fleet, D. </span><span class="koboSpan" id="kobo.674.3">J. </span><span class="koboSpan" id="kobo.674.4">(2022). </span><em class="italic"><span class="koboSpan" id="kobo.675.1">Video diffusion models</span></em><span class="koboSpan" id="kobo.676.1">. </span><span class="koboSpan" id="kobo.676.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">preprint arXiv:2205.10477.</span></span></li>
<li><span class="koboSpan" id="kobo.678.1">Ho, J., Jain, A., &amp; Abbeel, P. </span><span class="koboSpan" id="kobo.678.2">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.679.1">Denoising diffusion probabilistic models. </span><span class="koboSpan" id="kobo.679.2">Advances in Neural Information Processing Systems</span></em><span class="koboSpan" id="kobo.680.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">33, 6840–6851.</span></span></li>
<li><span class="koboSpan" id="kobo.682.1">Luo, S., &amp; Hu, W. </span><span class="koboSpan" id="kobo.682.2">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.683.1">Diffusion probabilistic models for 3d point cloud generation. </span><span class="koboSpan" id="kobo.683.2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.684.1">Recognition</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">, 2837–2845.</span></span></li>
<li><span class="koboSpan" id="kobo.686.1">Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. </span><span class="koboSpan" id="kobo.686.2">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.687.1">High-resolution image synthesis with latent diffusion models. </span><span class="koboSpan" id="kobo.687.2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.688.1">Recognition</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">, 10684–10695.</span></span></li>
<li><span class="koboSpan" id="kobo.690.1">Salimans, T., Karpathy, A., Chen, X., &amp; Kingma, D. </span><span class="koboSpan" id="kobo.690.2">P. </span><span class="koboSpan" id="kobo.690.3">(2017). </span><em class="italic"><span class="koboSpan" id="kobo.691.1">PixelCNN++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</span></em><span class="koboSpan" id="kobo.692.1">. </span><span class="koboSpan" id="kobo.692.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">preprint arXiv:1701.05517.</span></span></li>
<li><span class="koboSpan" id="kobo.694.1">Song, Y., Meng, C., &amp; Ermon, S. </span><span class="koboSpan" id="kobo.694.2">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.695.1">Denoising diffusion implicit models</span></em><span class="koboSpan" id="kobo.696.1">. </span><span class="koboSpan" id="kobo.696.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">preprint arXiv:2010.02502.</span></span></li>
<li><span class="koboSpan" id="kobo.698.1">Song, Y., &amp; Ermon, S. </span><span class="koboSpan" id="kobo.698.2">(2021). </span><em class="italic"><span class="koboSpan" id="kobo.699.1">Improved techniques for training score-based generative models. </span><span class="koboSpan" id="kobo.699.2">Advances in Neural Information Processing Systems</span></em><span class="koboSpan" id="kobo.700.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">33, 12438–12448.</span></span></li>
<li><span class="koboSpan" id="kobo.702.1">Sohl-Dickstein, J., Weiss, E. </span><span class="koboSpan" id="kobo.702.2">A., Maheswaranathan, N., &amp; Ganguli, S. </span><span class="koboSpan" id="kobo.702.3">(2015). </span><em class="italic"><span class="koboSpan" id="kobo.703.1">Deep unsupervised learning using nonequilibrium thermodynamics</span></em><span class="koboSpan" id="kobo.704.1">. </span><span class="koboSpan" id="kobo.704.2">arXiv </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">preprint arXiv:1503.03585.</span></span></li>
<li><span class="koboSpan" id="kobo.706.1">Ho, J., Jain, A., &amp; Abbeel, P. </span><span class="koboSpan" id="kobo.706.2">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.707.1">Denoising diffusion probabilistic models. </span><span class="koboSpan" id="kobo.707.2">Advances in Neural Information Processing Systems</span></em><span class="koboSpan" id="kobo.708.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">33, 6840–6851.</span></span></li>
<li><span class="koboSpan" id="kobo.710.1">Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... </span><span class="koboSpan" id="kobo.710.2">&amp; Sutskever, I. </span><span class="koboSpan" id="kobo.710.3">(2022). </span><em class="italic"><span class="koboSpan" id="kobo.711.1">Zero-shot text-to-image generation. </span><span class="koboSpan" id="kobo.711.2">International Conference on Machine </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.712.1">Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">, 8821-8831.</span></span></li>
<li><span class="koboSpan" id="kobo.714.1">Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. </span><span class="koboSpan" id="kobo.714.2">D., Dhariwal, P., ... </span><span class="koboSpan" id="kobo.714.3">&amp; Amodei, D. </span><span class="koboSpan" id="kobo.714.4">(2020). </span><em class="italic"><span class="koboSpan" id="kobo.715.1">Language models are few-shot learners. </span><span class="koboSpan" id="kobo.715.2">Advances in neural information processing systems</span></em><span class="koboSpan" id="kobo.716.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">33, 1877–1901.</span></span></li>
<li><span class="koboSpan" id="kobo.718.1">Patil, S., Cuenca, P., Lambert, N., &amp; von Platen, P. </span><span class="koboSpan" id="kobo.718.2">(2022). </span><em class="italic"><span class="koboSpan" id="kobo.719.1">Stable diffusion with diffusers</span></em><span class="koboSpan" id="kobo.720.1">. </span><span class="koboSpan" id="kobo.720.2">Hugging Face </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">Blog. </span></span><a href="https://huggingface.co/blog/stable_diffusion"><span class="No-Break"><span class="koboSpan" id="kobo.722.1">https://huggingface.co/blog/stable_diffusion</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.723.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.724.1">Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê, Luke, Ritobrata Ghosh. </span><span class="koboSpan" id="kobo.724.2">(2022, June 4). </span><em class="italic"><span class="koboSpan" id="kobo.725.1">DALL-E Mini Explained</span></em><span class="koboSpan" id="kobo.726.1">. </span><span class="koboSpan" id="kobo.726.2">W&amp;B; Weights &amp; Biases, </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">Inc. </span></span><a href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA"><span class="No-Break"><span class="koboSpan" id="kobo.728.1">https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.729.1">.</span></span></li>
</ul>
</div>
</body></html>