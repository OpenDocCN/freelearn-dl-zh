<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-32"><a id="_idTextAnchor045"/>2</h1>
<h1 id="_idParaDest-33"><a id="_idTextAnchor046"/>Surveying GenAI Types and Modes: An Overview of GANs, Diffusers, and Transformers</h1>
<p>In the previous chapter, we established the key distinction between generative and discriminative models. Discriminative models focus on predicting outputs by learning <code>p(output</code><code>∣</code><code>input)</code>, or the conditional probability of some expected output given an input or set of inputs. In contrast, generative models, such as <code>p(next token</code><code>∣</code><code>previous tokens)</code>, based on the probabilities of possible continuations given the current context. Tokens are represented as vectors containing embeddings that capture latent features and rich semantic dependencies learned through extensive training.</p>
<p>We briefly surveyed leading generative approaches, including <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>), <strong class="bold">Variational Autoencoders</strong> (<strong class="bold">VAEs</strong>), diffusion models, and <a id="_idIndexMarker051"/>autoregressive transformers. Each <a id="_idIndexMarker052"/>methodology possesses unique strengths suitable for different data types and tasks. For example, GANs are adept at generating high-fidelity photographic images through an adversarial process. Diffusion models take a probabilistic approach, iteratively adding and removing noise from data to learn robust generative representations. Autoregressive transformers leverage self-attention and massive scale to achieve remarkable controlled text generation.</p>
<p>In this chapter, we will explore the theoretical foundations and real-world applications of these techniques in greater depth. We will make direct comparisons, elucidating architectural innovations and enhancements that improve training stability and output quality over time. Through practical examples, we will see how researchers have adapted these models to produce art, music, videos, stories, and so on.</p>
<p>To enable an unbiased comparison, we will concentrate primarily on image synthesis tasks. GANs and diffusion models are specifically architected for image data, harnessing advances in convolutional processing and computer vision. Transformers, powered by self-attention, excel at language modeling but can also generate images. This will allow us to benchmark performance on a common task.</p>
<p>By the end of this chapter, we will have implemented state-of-the-art image generation models and explored how these core methods enhance and complement each other<a id="_idTextAnchor047"/>.</p>
<h1 id="_idParaDest-34"><a id="_idTextAnchor048"/>Understanding General Artificial Intelligence (GAI) Types – distinguishing features of GANs, diffusers, and transformers</h1>
<p>The often-stunning <a id="_idIndexMarker053"/>human-like quality we experience from GAI can be attributed to deep-generative machine learning advances. In particular, three fundamental methods have inspired many derivative innovations – GANs, diffusion models, and transformers. Each has its distinct strengths and is particularly well-suited for specific applications.</p>
<p>We briefly described GANs, a groundbreaking approach that exploits the adversarial interplay between two competing neural networks – a generator and a discriminator – to generate hyper-realistic synthetic data. Over time, GANs have seen substantial advancements, achieving greater control in data generation, higher image fidelity, and enhanced training stability. For instance, NVIDIA’s StyleGAN has created highly detailed and realistic human faces. The adversarial training process of GANs, where one network generates data and the other evaluates it, allows you to create highly refined and detailed synthetic images, enhancing realism with each training iteration. The synthetic images generated can be utilized in a plethora of domains. In the entertainment industry, they can be used to create realistic characters for video games or films. In research, they provide a means to augment datasets, especially in scenarios where real data is scarce or sensitive. Moreover, in computer vision, these synthetic images aid in training and fine-tuning other machine-learning models, advancing applications like facial recognition.</p>
<p>Diffusion models, an innovative generative modeling alternative, explicitly address some GAN limitations. As discussed briefly in <a href="B21773_01.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, diffusion models adopt a unique approach to introducing and systematically removing noise, enabling high-quality image synthesis with less training complexity. In medical imaging, diffusion models can significantly enhance image clarity by generating high-resolution synthetic examples to train other machine-learning models. Introducing and then iteratively removing noise can help reconstruct high-fidelity images from lower-quality inputs, which is invaluable in scenarios where obtaining high-resolution medical images is challenging.</p>
<p>Simultaneously, generative transformers, initially designed for language modeling, have been adopted for multimodal synthesis. Today, transformers are not confined to language and have permeated into audio, images, and video applications. For instance, OpenAI’s GPT-4 excels in processing and generating text, while DALL-E creates images from textual descriptions, a perfect example of the interplay between methods. When integrated, GPT-4 and DALL-E form a robust multimodal system. GPT-4 processes and understands textual instructions, while DALL-E takes the interpreted instructions to generate corresponding visual representations. A practical application of this combination could be automated digital advertisement creation. For example, given textual <a id="_idIndexMarker054"/>descriptions of a product and the desired aesthetic, GPT-4 could interpret these instructions, and DALL-E could generate visually compelling advertisements according<a id="_idTextAnchor049"/>ly.</p>
<h1 id="_idParaDest-35"><a id="_idTextAnchor050"/>Deconstructing GAI methods – exploring GANs, diffusers, and transformers</h1>
<p>Let’s deconstruct<a id="_idIndexMarker055"/> these core approaches to understand their distinct characteristics and illustrate their transformative role in advancing generative machine learning. As GAI continues to move forward, it’s crucial to understand how these approaches drive innovat<a id="_idTextAnchor051"/>ion.</p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor052"/>A closer look at GANs</h2>
<p>GANs, introduced<a id="_idIndexMarker056"/> by Goodfellow et al. in 2014, primarily consist of two neural <a id="_idIndexMarker057"/>networks – the <strong class="bold">Generator</strong> (<strong class="bold">G</strong>) and<a id="_idIndexMarker058"/> the <strong class="bold">Discriminator</strong> (<strong class="bold">D</strong>). G aims to create synthetic data resembling real data, while D strives to distinguish real from synthetic data.</p>
<p>In this setup, the following occurs:</p>
<ol>
<li>G receives input from a “latent space,” a high-dimensional space representing structured randomness. This structured randomness serves as a seed to generate synthetic data, transforming it into meaningful information.</li>
<li>D evaluates the generated data, attempting to differentiate between real (or reference) and synthetic data.<p class="list-inset">In short, the process begins with G deriving random noise from the latent space to create data. This synthetic data, along with real data, is supplied to D, which then tries to discern between the two. Feedback from D informs the parameters of G to refine its data generation process. The adversarial interaction continues until an equilibrium is reached.</p></li>
<li><strong class="bold">Equilibrium</strong> in GANs occurs when D can no longer differentiate between real and synthetic data, assigning an equal probability of 0.5 to both. Arriving at this state signals that the synthetic data produced by G is indistinguishable from real data, which is the core objective of the synthesis process.</li>
</ol>
<p>Ultimately, the success of GANs has had meaningful implications for various sectors. In the automotive industry, GANs have been used to simulate real-world scenarios for autonomous vehicle testing. In the entertainment sector, GANs are deployed to generate digital <a id="_idIndexMarker059"/>characters and realistic environments for filmmaking and game design. In the art world, GANs can literally craft new words. Moreover, the development of GANs has continued to move forward over the years with significant improvements in quality, control, and overall per<a id="_idTextAnchor053"/>formance.</p>
<h3>Advancement of GANs</h3>
<p>Since its<a id="_idIndexMarker060"/> inception, GAN technology has evolved significantly with several notable advancements:</p>
<ul>
<li><strong class="bold">Conditional GANs (cGANs)</strong>: Introduced <a id="_idIndexMarker061"/>by Mirza and Osindero in 2014, conditional GANs incorporated specific conditions during data generation, enabling more controlled outputs. cGANs have been used in tasks such <a id="_idIndexMarker062"/>as image-to-image translation (e.g., converting photos into paintings).</li>
<li><strong class="bold">Deep Convolutional GANs (DCGANs)</strong>: In 2015, Radford et al. enhanced GANs by<a id="_idIndexMarker063"/> integrating convolutional layers, which help to analyze image data in small, overlapping regions to capture fine granularity, substantially improving the visual quality of the synthetic output. DCGANs can generate realistic images for applications such as fashion design, where the model evolves new designs from existing trends.</li>
<li><strong class="bold">Wasserstein GANs (WGANs)</strong>: Introduced by Arjovsky et al. in 2017, Wasserstein <a id="_idIndexMarker064"/>GANs applied the Wasserstein distance metric to GANs’ objective function, facilitating a more accurate measurement of differences between real and synthetic data. Specifically, the metric helps you find the most efficient way to make the generated data distribution resemble the real data distribution. This small adjustment leads to a more stable learning process, minimizing volatility during training. WGANs have helped generate realistic medical imagery to aid in training diagnostic AI algorithms, improving a model’s ability to generalize from synthetic to actual data.</li>
</ul>
<p>Following the advent of Wasserstein GANs, the landscape experienced a surge of inventive expansions, each<a id="_idIndexMarker065"/> tailor-made to address specific challenges or open new avenues in synthetic data generation:</p>
<ul>
<li><strong class="bold">Progressively growing GANs</strong> incrementally increase the resolution during training, starting with lower-resolution images and gradually moving to higher resolution. This approach allows the model to learn coarse-to-fine details effectively, making training more manageable and generating high-quality images (Karras et al. 2017). These high-resolution images can enhance the realism and immersion of virtual reality environments.</li>
<li><strong class="bold">CycleGANs</strong> facilitates<a id="_idIndexMarker066"/> image-to-image translations, bridging domain adaptation tasks (Zhu et al., 2017). For example, a CycleGAN could transform a summer scene into a winter scene without requiring example pairs (e.g., summer-winter) during training. CycleGANs have been used to simulate weather conditions in autonomous vehicle testing, evaluating system performance<a id="_idIndexMarker067"/> under varying environmental conditions.</li>
<li><strong class="bold">BigGANs</strong> push the<a id="_idIndexMarker068"/> boundaries in high-resolution image generation, showcasing the versatility of GANs in complex generation tasks. They achieve this by scaling up the size of the model (more layers and units per layer) and the batch size during training, alongside other architectural and training innovations (Brock et al., 2018). BigGANs have been used to generate realistic textures for video games, enhancing gaming environments’ realism.</li>
</ul>
<p>These developments significantly broadened what GANs could achieve, ranging from high-resolution image synthesis to domain adaptation and cross-modal generation tasks. However, despite <a id="_idIndexMarker069"/>these incredible advancements, GANs have suffered from some continual limitations, which inspired alternative approaches<a id="_idTextAnchor054"/> such as diffusion.</p>
<h3>Limitations and challenges of GANs</h3>
<p>The training process<a id="_idIndexMarker070"/> of GANs requires a careful balance between the G and D networks. It requires substantial computational resources, often demanding powerful GPUs and enormous datasets to achieve desirable outcomes. Moreover, there are complexities in training GANs that arise from challenges such as vanishing gradients and mode collapse. While the vanishing gradient problem is a problem broadly affecting deep neural networks, mode collapse is a challenge that is particularly unique to the training of GANs. Let’s explore these a bit further:</p>
<ul>
<li><strong class="bold">Vanishing gradients</strong>: This<a id="_idIndexMarker071"/> issue arises during the neural network training phase when the gradient of the loss function diminishes to a point where the learning either drastically slows or halts. The crux of GANs lies in the delicate balance of learning between the G and D models. Disproportionate learning can hinder the overall training process. In practical terms, the issue of vanishing gradients can lead to longer training times and increased computational costs, which might render GANs impractical for time-sensitive or resource-constrained applications.</li>
<li><strong class="bold">Mode collapse</strong>: Inherent to GANs, mode collapse occurs when the G starts producing a narrow variety of samples, thereby stifling output diversity and undermining a network’s effectiveness. Techniques such as a gradient penalty and spectral normalization have alleviated these issues. This phenomenon can significantly degrade the quality of generated data, limiting the use of GANs in applications that require diverse outputs, such as data augmentation for machine learning<a id="_idIndexMarker072"/> or generating diverse design alternatives in creative industries.</li>
</ul>
<p>Of course, GANs carry the same ethical considerations as any state-of-the-art generative synthesis. For instance, they can be used to create deepfakes or generate biased outputs that reinforce societal prejudices. For example, when GANs, often used to generate synthetic data (e.g., faces), underrepresent certain groups, downstream applications may exhibit gender or racial bias (Kenfack et al., 2021).</p>
<p>Even with the advent of other generative models such as diffusion models and Transformer-based image generators, GANs have played a seminal role in shaping the trajectory of generative image synthesis, showcasing both the potential and some of the challenges inherent in this domain.</p>
<p>Now that <a id="_idIndexMarker073"/>we better understand GANs in the context of deep generative models, let’s shift our focus to a successor in image generatio<a id="_idTextAnchor055"/>n, the diffusion model.</p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor056"/>A closer look at diffusion models</h2>
<p>Having explored the <a id="_idIndexMarker074"/>dynamics of GANs, let’s transition our attention to a subsequent innovation in image generation – the diffusion model. Initially proposed by Sohl-Dickstein et al. in 2015, diffusion models present a novel approach, where a neural network iteratively introduces and subsequently removes noise from data to generate highly refined images. Unlike GANs, which leverage an adversarial mechanism involving two contrasting models, diffusion models apply a more gradual, iterative process of noise manipulation within the data.</p>
<p>In practical terms, GANs have shown substantial merit in art and design, creating realistic faces or generating sharp, high-fidelity images from descriptions. They are also used in data augmentation, expanding datasets by generating realistic synthetic data to augment the training of machine learning models.</p>
<p>Conversely, diffusion models excel in tasks requiring a structured approach to image generation, such as in medical imaging. Their iterative process can enhance the quality of medical images, such as MRI or CT scans, where noise reduction and clarity are paramount. This makes diffusion models invaluable in clinical settings, aiding in better diagnostics and analysis. Moreover, their controlled and gradual process offers a more predictable or stable<a id="_idIndexMarker075"/> training process compared to the adversarial and dynamic training of GANs.</p>
<p>The foundation of diffusion models is anchored in two primary processes:</p>
<ul>
<li><code>x</code><code>₀</code>) and iteratively introduces Gaussian noise, akin to progressively applying a fog-like filter, transforming the data into indistinguishable noise (<code>x</code><code>ₜ</code>).</li>
<li><code>p</code><code>θ</code>) attempts to eliminate (or de-fog) the noise from the noisy data (<code>x</code><code>ₜ</code>), aiming to revert to the original clean state (<code>x</code><code>ₜ₋₁</code>). Specifically, this reversion is orchestrated by estimating the probability of transitioning from the noisy state back to the clear state, using a conditional distribution denoted as <code>p</code><code>θ</code><code>(x</code><code>ₜ₋₁</code><code>|x</code><code>ₜ</code><code>)</code>. A <strong class="bold">conditional distribution</strong> tells <a id="_idIndexMarker076"/>us the likelihood of one event happening when we know another related event has occurred. In this case, the reversion estimates the likelihood of reverting to the original state, given some amount of noise.</li>
</ul>
<p>In the pivotal work <em class="italic">Score-Based Generative Modeling through Stochastic Differential Equations</em>, the authors propose a novel framework that unifies score-based generative models and diffusion probabilistic modeling by employing <code>p</code><code>θ</code>.</p>
<p>The reverse model (<code>p</code><code>θ</code>) was implemented using convolutional networks to predict variations in the Gaussian noise distribution – a critical component of the noise-introduction process within the forward diffusion. Initially, the efficacy of this approach was validated on more straightforward datasets. However, the methodology’s applicability was later significantly improved to handle more complex images (Ho et al., 2020). This<a id="_idIndexMarker079"/> expansion demonstrated the practical potential of diffusion models in generating highly refined images across a broa<a id="_idTextAnchor057"/>der spectrum of complexities.</p>
<h3>Advancement of diffusion models</h3>
<p>Since its inception, diffusion<a id="_idIndexMarker080"/> model technology has witnessed key advancements, propelling its capabilities in image generation:</p>
<ul>
<li><strong class="bold">Simplified training objectives</strong>: Ho et al. proposed simplified training objectives that predict Gaussian noise directly, eliminating the need for conditional means and facilitating the application to more complex datasets (Ho et al., 2020). This advancement facilitated handling more complex datasets, potentially aiding in tasks such as anomaly detection or complex data synthesis, which could be resource-intensive with traditional models.</li>
<li><strong class="bold">UNet modules with self-attention</strong>: Ho et al. also incorporated UNet modules with self-attention into the diffusion model architecture, inspired by PixelCNN++ by Salimans et al. (2017), enhancing a model’s performance on complex datasets (Ho et al., 2020). Again, enhancing performance on complex datasets facilitates better image restoration, which is particularly beneficial in fields such as medical imaging or satellite imagery analysis, where high-fidelity image reconstruction is crucial.</li>
<li><strong class="bold">Synchronization with SDEs</strong>: Song et al. defined diffusion models as solutions to SDEs, linking score learning with denoising score-matching losses and expanding model usage for image generation, editing, in-painting, and colorization (Song et al., 2020).</li>
</ul>
<p>Following these foundational advancements, diffusion models witnessed a wave of innovative enhancements as researchers introduced novel methodologies to address existing challenges and broaden a model’s applicability in generative modeling tasks. These advancements include the following:</p>
<ul>
<li><strong class="bold">Noise conditioning and annealing strategies</strong>: Song et al. improved score-based models by including noise conditioning and annealing strategies, achieving performance comparable to GANs on benchmark datasets like the Flickr-Faces-HQ dataset  (Song et al., 2021), which is a high-quality image dataset of human faces designed to measure GAN performance. Achieving performance comparable to GANs could make diffusion models a viable alternative for high-fidelity image generation tasks in areas where GANs are traditionally used.</li>
<li><strong class="bold">Latent Diffusion Models</strong> (<strong class="bold">LDMs</strong>): Rombach et al. addressed computational inefficiency by<a id="_idIndexMarker081"/> proposing LDMs, which operate in a compressed latent space learned by autoencoders, employing perceptual losses to create a visually equivalent, reduced latent space (Rombach et al., 2021). By addressing computational inefficiency, LDMs could expedite the image generation process, making them suitable for real-time applications or scenarios where computational resources are limited.</li>
<li><strong class="bold">Classifier-free guidance</strong>: Ho &amp; Salimans introduced classifier-free guidance for controlled generation without relying on pre-trained networks, marking a step toward<a id="_idIndexMarker082"/> more flexible generation techniques (Ho &amp; Salimans, 2022). This advancement led to more flexible generation techniques, enabling more controlled and customized image generation in applications such as design, advertising, or content creation without relying on pre-trained networks.</li>
</ul>
<p>Subsequent explorations in the diffusion model domain extended its applications, showcasing versatility:</p>
<ul>
<li><strong class="bold">Video generation</strong>: Ho et al. adapted diffusion models for video generation, demonstrating their utility beyond static image generation (Ho et al., 2022)</li>
<li><strong class="bold">3D data processing</strong>: Luo &amp; Hu extended the application to 3D data processing, showcasing the flexibility of diffusion models (Luo &amp; Hu, 2021)</li>
</ul>
<p>The evolution of diffusion models has led to enhanced image generation and expanded applications in video, 3D data processing, and rapid learning methodologies. However, the methodology<a id="_idIndexMarker083"/> does have its  challenges and limitations, outlined in some det<a id="_idTextAnchor058"/>ail in the section that follows..</p>
<h3>Limitations and challenges of diffusion models</h3>
<p>Despite their evident<a id="_idIndexMarker084"/> benefits and notable progress, diffusion models have some unique limitations, such as the following:</p>
<ul>
<li><strong class="bold">Sampling speed</strong>: A notable limitation of diffusion models is the slow sampling process, particularly when compared to GANs. Sampling, in this context, refers to the process of generating new data points from the learned distribution of a model. The speed at which new samples can be generated is crucial for many real-time or near-real-time applications, and the slower sampling speed of diffusion models can be a significant drawback.</li>
<li><strong class="bold">Stability during large-scale training</strong>: The stability of diffusion models during large-scale training is another area requiring further exploration. Large-scale training refers to training a model on a substantial amount of data, sometimes leading to instability in the model’s learning process. Ensuring stability during this phase is crucial to achieve reliable and consistent performance from the model.</li>
</ul>
<p>A close examination of the societal impact of the media generated by these models is crucial, especially given the level of fine control now possible over the generated content. However, diffusion models’ inherent simplicity, versatility, and positive inductive biases <a id="_idIndexMarker085"/>signify a bright future. These attributes suggest a trajectory of rapid development within generative modeling, potentially integrating diffusion models as pivotal components in various disciplines, su<a id="_idTextAnchor059"/>ch as computer vision and graphics.</p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor060"/>A closer look at generative transformers</h2>
<p>The revolutionary <a id="_idIndexMarker086"/>advent of transformer models has significantly impacted the task of generating high-fidelity images from text descriptions. Notable models such as <strong class="bold">CLIP </strong>(<strong class="bold">Contrastive Language-Image Pretraining</strong>) and DALL-E utilized transformers in unique ways to<a id="_idIndexMarker087"/> create images based on natural language captions. This section will discuss the transformer-based approach for text-to-image generation, its foundations, the key techniques, the res<a id="_idTextAnchor061"/>ulting benefits, and some challenges.</p>
<h3>A brief<a id="_idTextAnchor062"/> overview of transformer architecture</h3>
<p>The original transformer <a id="_idIndexMarker088"/>architecture, introduced by Vaswani et al. in 2017, is a cornerstone of many modern language-processing systems. In fact, the transformer may be considered the most important architecture in the area of GAI, as it is foundational to the GPT series of models and many other state-of-the-art generative methods. As such, we’ll cover the architecture briefly in our survey of generative approaches but will have a dedicated chapter, where we will have the opportunity to deconstruct and implement the transformer from scratch.</p>
<p>At the core of the transformer architecture lies<a id="_idIndexMarker089"/> the <strong class="bold">self-attention mechanism</strong>, a unique approach that captures complex relationships among different elements within an ordered data sequence. These elements, known as <strong class="bold">tokens</strong>, represent<a id="_idIndexMarker090"/> words in a sentence or characters in a word based on the level of granularity chosen<a id="_idIndexMarker091"/> for <strong class="bold">tokenization</strong>.</p>
<p>The principle of <strong class="bold">attention</strong> in this architecture enables a model to focus on certain pivotal aspects of the input data while potentially disregarding less significant parts. This mechanism augments the model’s understanding of the context and the relative importance of words in a sentence.</p>
<p>The transformer bifurcates into two main segments, the <strong class="bold">encoder</strong> and the <strong class="bold">decoder</strong>, each comprising multiple layers of self-attention mechanisms. While the encoder discerns relationships between different positions in the input sequence, the decoder focuses on the outputs from the encoder, employing a variant of self-attention termed <strong class="bold">masked self-attention</strong> to <a id="_idIndexMarker092"/>prevent consideration of future outputs it hasn’t generated yet.</p>
<p>The calculation of <strong class="bold">attention weights</strong> through the scaled dot-product of query and key vectors plays a<a id="_idIndexMarker093"/> crucial role in determining the level of focus on different parts of the input. Additionally, <strong class="bold">multi-head attention</strong> allows<a id="_idIndexMarker094"/> the model to channel attention toward multiple data points simultaneously.</p>
<p>Lastly, to retain the sequence order of data, the model adopts a strategy known as <strong class="bold">positional encoding</strong>. This <a id="_idIndexMarker095"/>mechanism is vital for tasks requiring an understanding of sequence or temporal dynamics, ensuring the model preserves the initial order of data throughout its processing.</p>
<p>Again, we will revisit the transformer architecture in <a href="B21773_03.xhtml#_idTextAnchor081"><em class="italic">Chapter 3</em></a> to further reinforce our understanding, as it is foundational to the continued research and evolution of generative AI. Nevertheless, with at least a fundamental grasp of the Transformer architecture, we are better positioned to dissect transformer-driven generative modeling paradigms <a id="_idIndexMarker096"/>across a spectrum of ap<a id="_idTextAnchor063"/>plications.</p>
<h3>Generative modeling paradigms with transformers</h3>
<p>In tackling various <a id="_idIndexMarker097"/>tasks, transformers <a id="_idIndexMarker098"/>adopt distinct training paradigms aligning with the task at hand. For example, discriminative tasks such as classification might use a masking paradigm:</p>
<ul>
<li><strong class="bold">Masked Language Modeling</strong> (<strong class="bold">MLM</strong>): MLM is<a id="_idIndexMarker099"/> a discriminative pretraining technique used by models such as <strong class="bold">BERT</strong> (<strong class="bold">Bidirectional Encoder Representations from Transformers</strong>). During <a id="_idIndexMarker100"/>training, some percentage of input tokens are randomly masked out. The model must then predict the original masked words based on the context of the surrounding unmasked words. This teaches the model to build robust context-based representations, facilitating many <a id="_idIndexMarker101"/>downstream <strong class="bold">natural language processing </strong>(<strong class="bold">NLP</strong>) tasks.</li>
</ul>
<p>MLM, as utilized in BERT, has been instrumental in enhancing the performance of NLP systems across various domains. For instance, it can power medical coding systems in healthcare by accurately identifying and categorizing medical terms within clinical notes. This automatic coding can save significant time and reduce errors in medical documentation, thereby improving the efficiency and accuracy of healthcare data management.</p>
<p>For generative tasks, the focus shifts to creating new data sequences, requiring different training paradigms:</p>
<ul>
<li><strong class="bold">Sequence-to-sequence modeling</strong>: Sequence-to-sequence models employ both an encoder<a id="_idIndexMarker102"/> and a decoder. The encoder maps the input sequence to a latent representation. The decoder then generates the target sequence token by token from that representation. This paradigm is useful for tasks such as translation, summarization, and question-answering.</li>
<li><strong class="bold">Autoregressive modeling</strong>: Autoregressive modeling generates sequences by predicting the <a id="_idIndexMarker103"/>next token conditioned only on previous tokens. The model produces outputs one step at a time, with each new token depending on those preceding it. Autoregressive transformers such as GPT leverage this technique for controlled text generation. </li>
</ul>
<p>Transformers <a id="_idIndexMarker104"/>combine self-attention<a id="_idIndexMarker105"/> for long-range dependencies, pre-trained representations, and autoregressive decoding to adapt to discriminative and generative tasks.</p>
<p>Advanced generative synthesis can be achieved with different architectures that make trade-offs between complexity, scalability, and specialization. For example, instead of using both the encoder and decoder, many state-of-the-art generative models employ a decoder-only or encoder-only approach. The encoder-decoder framework is often the most computationally intensive learning to specialize in, as it increases model size. Decoder-only architectures leverage powerful pre-trained language models such as GPT as the decoder, reducing parameters through weight sharing. Encoder-only methods forego decoding, instead, they encode inputs and perform regression or search on the resulting embeddings. Each approach has advantages that suit certain use cases, datasets, and computational budgets. In the following sections, we explore examples of models that <a id="_idIndexMarker106"/>employ these<a id="_idIndexMarker107"/> derivative transformer architectures for creative applications, such as image generation and captioning.</p>
<h3>Encoder-only approach</h3>
<p>In certain models, only <a id="_idIndexMarker108"/>the encoder network <a id="_idIndexMarker109"/>maps the input to an embedding space. The output is then generated directly from this embedding, eliminating the need for a decoder. While this straightforward architecture has typically found its place in classification or regression tasks, recent advancements have broadened its application to more complex tasks. In particular, models developed for tasks such as image synthesis leverage the encoder-only setup to process both text and visual inputs, creating a multimodal relationship that facilitates the generation of high-fidelity images from natural language instruction.</p>
<h3>Decoder-only approach</h3>
<p>Similarly, some<a id="_idIndexMarker110"/> models operate using a <a id="_idIndexMarker111"/>decoder-only strategy, where a singular decoder network is tasked with both encoding the input and generating output. This mechanism starts by joining the input and output sequences, which the decoder processes. Despite its simplicity and the characteristic sharing of parameters between input and output stages, the effectiveness of this architecture relies heavily on the pretraining of robust decoders. Recently, even more complex tasks such as text-to-image synthesis have seen the successful deployment of the decoder-only architecture, illustrating its versatility and adaptabi<a id="_idTextAnchor064"/>lity to diverse applications.</p>
<h3>Advancement of transformers</h3>
<p>Transformer <a id="_idIndexMarker112"/>mechanisms with other novel techniques to tackle generative tasks. This evolution led to distinct approaches to handling text and image generation. In this section, we will explore some of these innovative models and their unique methodologies in advancing GAI.</p>
<h3>Encoder-decoder image generation with DALL-E</h3>
<p>Introduced by<a id="_idIndexMarker113"/> Ramesh <a id="_idIndexMarker114"/>et al. in 2021, DALL-E employs an <a id="_idIndexMarker115"/>encoder-decoder framework to facilitate text-to-image generation. This model comprises two primary components:</p>
<ul>
<li><strong class="bold">Text encoder</strong>: Applies the transformer’s encoder, processing plain text to derive a semantic embedding that serves as the context for the image decoder.</li>
<li><strong class="bold">Image decoder</strong>: Applies the transformer’s decoder to generate the image autoregressively, predicting each pixel based on the text embedding and previously predicted pixels.</li>
</ul>
<p>By training on image-caption datasets, DALL-E refines the transition from text to detailed image renderings. This setup underscores the capability of dedicated encoder and decoder modules for conditional image generation.</p>
<h3>Encoder-only image captioning with CLIP</h3>
<p>CLIP, conceptualized<a id="_idIndexMarker116"/> by Radford<a id="_idIndexMarker117"/> et al. in 2021, adopts an encoder-only approach for image-text tasks. Key components include a visual encoder and a text encoder.</p>
<p>Visual Encoder and Text Encoder process the image and candidate captions, respectively, determining the matching caption based on encoded representations.</p>
<p>Pretraining on extensive image-text datasets enables CLIP to establish a shared embedding space, facilitating efficient inference for retrieval-based captioning.</p>
<h3>Improving image fidelity with scaled transformers (DALL-E 2)</h3>
<p>Ramesh et al. in 2022 <a id="_idIndexMarker118"/>extended <a id="_idIndexMarker119"/>DALL-E to DALL-E 2, showcasing techniques to enhance visual quality:</p>
<ul>
<li><strong class="bold">A scaled-up decoder</strong>: By expanding the decoder to 3.5 billion parameters and applying classifier-free guidance during sampling, visual quality in complex image distributions such as human faces is significantly improved.</li>
<li><strong class="bold">Hierarchical decoding for high-resolution images (GLIDE)</strong>: Proposed by Nichol et al. in 2021, GLIDE employs a hierarchical generation strategy.</li>
<li><strong class="bold">A coarse-to-fine approach</strong>: This entails an initial low-resolution image prediction followed by progressive detailing through up-sampling and refining, capturing <a id="_idIndexMarker120"/>global structure and high-frequency textures.</li>
</ul>
<h3>Multimodal image generation with GPT-4</h3>
<p>GPT-4 developed<a id="_idIndexMarker121"/> by OpenAI, is <a id="_idIndexMarker122"/>a powerful multimodal model <a id="_idIndexMarker123"/>based on the Transformer architecture. GPT-4 demonstrates a capability for conditional image generation without requiring continued training or fine-tuning:</p>
<ul>
<li><strong class="bold">Pretraining and fine-tuning</strong>: The massive scale of GPT-4 and its pretraining on diverse datasets enable a robust understanding of relationships between textual and visual data.</li>
<li><strong class="bold">Multimodal generation:</strong> GPT-4 can generate images based on text descriptions. The model uses a deep neural network to encode the semantic meaning of the text into a visual representation. Given a text prompt, GPT-4 generates an image by predicting the visual content consistent with the provided text. This involves taking high-dimensional text embeddings and processing them through successive neural network layers to generate a corresponding visual representation.</li>
</ul>
<p>Using a pretrained multimodal model eliminates the need for a separate encoder module for image inputs, facilitating rapid adaptation for image generation tasks. This approach underscores the versatility and power of Transformer architectures in generative tasks, providing a streamlined methodology to translate text into high-quality images.</p>
<p>Transformer architectures offer many benefits for controlled image generation when compared to GANs. Their autoregressive nature ensures precise control over image construction while allowing you to adapt to varying computational needs and diverse downstream<a id="_idIndexMarker124"/> applica<a id="_idTextAnchor065"/>tions. However, transformers <a id="_idIndexMarker125"/>also<a id="_idIndexMarker126"/> introduce new challenges in this domain.</p>
<h3>Limitations and challenges of transformer-based approaches</h3>
<p>Some early <a id="_idIndexMarker127"/>transformers-based approaches demonstrated slower sampling speed and restricted fidelity compared to GANs. Generating or manipulating images while maintaining precise control over specific attributes or characteristics of the objects within those images remains challenging. Additionally, training large-scale transformers that can overcome these challenges demands extensive computing resources. Notwithstanding, current multimodal results demonstrate a rapidly evolving and promising landscape.</p>
<p>We must also remember that alongside technical<a id="_idTextAnchor066"/> challenges there are broader sociotechnical implications and considerations.</p>
<h3>Bias and ethics in generative models</h3>
<p>Significant <a id="_idIndexMarker128"/>advancements in generative models such as GANs, diffusers, and transformers necessitate serious contemplation of potential bias and ethical implications.</p>
<p>We need to remain alert to the risk of reinforcing prejudices and stereotypes that reflect skewed training data. For instance, diffusion models trained on data that over-represents specific demographics might propagate these biases in their output. Analogously, language models exposed to toxic or violent content during training might generate similar content.</p>
<p>The directive nature of prompt-based generation also, unfortunately, opens doors to misuse if deployed carelessly. Transformers risk facilitating impersonation, misinformation, and the creation of deceptive content. Image synthesis models such as GANs could potentially be exploited to generate non-consensual deepfakes or artificial media.</p>
<p>Additionally, the potential for ultra-realistic output prompts ethical dilemmas regarding consent, privacy, identity, and copyright. The ability to create convincingly real yet fictional faces or voices complicates the distinction between real and synthetic, necessitating careful examination of training data sources and generative capabilities.</p>
<p>Further, as these technologies become ubiquitous, their societal impact must be considered. Defining clear policies will be crucial as the distinction between authentic and AI-generated content becomes increasingly ambiguous. Upholding principles of integrity, attribution, and consent remains vital.</p>
<p>Despite these risks, the potential benefits of generative models are substantial. Addressing bias proactively, advocating transparency, auditing data and models, and implementing safeguards become increasingly critical as technologies evolve. Ultimately, the <a id="_idIndexMarker129"/>responsibility to ensure fairness, <a id="_idTextAnchor067"/>accountability, and ethical practice falls on all developers and practitioners.</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor068"/>Applying GAI models – image generation using GANs, diffusers, and transformers</h1>
<p>In this hands-on <a id="_idIndexMarker130"/>section, we’ll reinforce<a id="_idIndexMarker131"/> the <a id="_idIndexMarker132"/>concepts <a id="_idIndexMarker133"/>discussed throughout the <a id="_idIndexMarker134"/>chapter<a id="_idIndexMarker135"/> by<a id="_idIndexMarker136"/> putting them into practice. You’ll get a first-hand experience and deep dive into the actual implementation of generative models, specifically GANs, diffusion models, and transformers.</p>
<p>The Python code provided will guide you through this process. Manipulating and observing the code in action will build your understanding of the intricate workings and potential applications of these models. This exercise will provide insight into model capabilities for tasks like generating art from prompts and synthesizing hyper-realistic images.</p>
<p>We’ll be utilizing the highly versatile <code>PyTorch</code> library, a popular choice among machine learning practitioners, to facilitate our operations. <code>PyTorch</code> provides a powerful and dynamic toolset to define and compute gradients, which is central to training these models.</p>
<p>In addition, we’ll also use the <code>diffusers</code> library. It’s a specialized library that provides functionality to implement diffusion models. This library enables us to reproduce state-of-the-art diffusion models directly from our workspace. It underpins the creation, training, and usage of denoising diffusion probabilistic models at an unprecedented level of simplicity, without compromising the models’ complexity.</p>
<p>Through this practical session, we’ll explore how to operate and integrate these libraries and implement and manipulate GANs, diffusers, and transformers using the Python programming language. This hands-on experience will complement the theoretical knowledge we have gained in the chapter, enabling us to see these models in action in the real world.</p>
<p>By the end of this section, you will not only have a conceptual understanding of these generative models but also understand how they are implemented, trained, and used for several innovative applications in data science and machine learning. You’ll have a much<a id="_idIndexMarker137"/> deeper <a id="_idIndexMarker138"/>understanding<a id="_idIndexMarker139"/> of how these <a id="_idIndexMarker140"/>models <a id="_idIndexMarker141"/>work <a id="_idIndexMarker142"/>and <a id="_idIndexMarker143"/>the experience of implementing them yourself.</p>
<h2 id="_idParaDest-40"><a id="_idTextAnchor069"/>Working with Jupyter Notebook and Google Colab</h2>
<p>Jupyter notebooks <a id="_idIndexMarker144"/>enable live code execution, visualization, and<a id="_idIndexMarker145"/> explanatory text, suitable for prototyping and data analysis. Google Colab, conversely, is a cloud-based version of Jupyter Notebook, designed for machine learning prototyping. It provides free GPU resources and integrates with Google Drive f<a id="_idTextAnchor070"/>or file storage and sharing. We’ll leverage Colab as our prototyping environment going forward.</p>
<h2 id="_idParaDest-41"><a id="_idTextAnchor071"/>Stable diffusion transformer</h2>
<p>We begin with a <a id="_idIndexMarker146"/>pre-trained stable diffusion model, a text-to-image latent diffusion model created by researchers and engineers from CompVis, Stability AI, and LAION (Patil et al., 2022). The diffusion process is used to draw samples from complex, high-dimensional distributions, and when it interacts with the text embeddings, it creates a powerful conditional image synthesis model.</p>
<p>The term “stable” in this context refers to the fact that during training, a model maintains certain properties that stabilize the learning process. Stable diffusion models offer rich potential to create entirely new samples from a given data distribution, based on text prompts.</p>
<p>Again, for our practical example, we will Google Colab to alleviate a lot of initial setups. Colab also provides all of the computational resources needed to begin experimenting right away.  We start by installing some libraries, and with three simple functions, we will build out a minimal <code>StableDiffusionPipeline</code> using a well-established open-source implementation of the stable diffusion method.</p>
<p>First, let’s navigate to our pre-configured Python environment, Google Colab, and install the <code>diffusers</code> open-source library, which will provide most of the key underlying components we need for our experiment.</p>
<p>In the first cell, we install all dependencies using the following <code>bash</code> command. Note the exclamation point at the beginning of the line, which tells our environment to reach down to its underlying<a id="_idIndexMarker147"/> process and install the packages we need:</p>
<pre class="source-code">
!pip install pytorch-fid torch diffusers clip transformers accelerate</pre>
<p>Next, we import the libraries we’ve just installed to make them avai<a id="_idTextAnchor072"/>lable to our Python program:</p>
<pre class="source-code">
from typing import List
import torch
import matplotlib.pyplot as plt
from diffusers import StableDiffusionPipeline, DDPMScheduler</pre>
<p>Now, we’re ready for our three functions, which will execute the three tasks – loading the pre-trained model, generating the images based on prompting, and rendering the images:</p>
<pre class="source-code">
def load_model(model_id: str) -&gt; StableDiffusionPipeline:
    """Load model with provided model_id."""
    return StableDiffusionPipeline.from_pretrained(
        model_id, 
        torch_dtype=torch.float16, 
        revision="fp16", 
        use_auth_token=False
    ).to("cuda")
def generate_images(
    pipe: StableDiffusionPipeline, 
    prompts: List[str]
) -&gt; torch.Tensor:
    """Generate images based on provided prompts."""
    with torch.autocast("cuda"):
        images = pipe(prompts).images
    return images
def render_images(images: torch.Tensor):
    """Plot the generated images."""
    plt.figure(figsize=(10, 5))
    for i, img in enumerate(images):
        plt.subplot(1, 2, i + 1)
        plt.imshow(img)
        plt.axis("off")
    plt.show()</pre>
<p>In summary, <code>load_model</code> loads a machine learning model identified by <code>model_id</code> onto a GPU for faster <a id="_idIndexMarker148"/>processing. The <code>generate_images</code> function takes this model and a list of prompts to create our images. Within this function, you will notice <code>torch.autocast("cuda")</code>, which is a special command that allows PyTorch (our underlying machine learning library) to perform operations faster while maintaining accuracy. Lastly, the <code>render_images</code> function displays these images in a simple grid format, making use of the <code>matplotlib</code> visualization library to render our output.</p>
<p>With our functions defined, we select our mod<a id="_idTextAnchor073"/>el version, define our pipeline, and execute our image generation process:</p>
<pre class="source-code">
# Execution
model_id = "CompVis/stable-diffusion-v1-4"
prompts = [
    "A hyper-realistic photo of a friendly lion",
    "A stylized oil painting of a NYC Brownstone"
]
pipe = load_model(model_id)
images = generate_images(pipe, prompts)
render_images(images)</pre>
<p>The output in <em class="italic">Figure 2</em><em class="italic">.1</em> is a vivid example of the imaginativeness and creativity we typically expect<a id="_idIndexMarker149"/> from human art, generated entirely by the diffusion process. Except, how do we measure whether the model was faithful to the text provided?</p>
<div><div><img alt="Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion” (left) and “A stylized oil painting of a NYC Brownstone” (right)" src="img/B21773_02_01.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: Output for the prompts “A hyper-realistic photo of a friendly lion” (left) and “A stylized oil painting of a NYC Brownstone” (right)</p>
<p>The next step is to evaluate the quality and relevance of our generated images in relation to the prompts. This is where CLIP comes into play. CLIP is designed to measure the alignment<a id="_idIndexMarker150"/> between text and images by analyzing their semantic similar<a id="_idTextAnchor074"/>i<a id="_idTextAnchor075"/>ties, giving us a true quantitative measure of the fidelity of our synthetic images to the prompts.</p>
<h2 id="_idParaDest-42"><a id="_idTextAnchor076"/>Scoring with the CLIP model</h2>
<p>CLIP is trained to <a id="_idIndexMarker151"/>understand the relationship between text and images by learning to place similar images and text near each other in a shared space. When evaluating a generated image, CLIP checks how closely the image aligns with the textual description provided. A higher score indicates a better match, meaning the image accurately represents the text. Conversely, a lower score suggests a deviation from the text, indicating a lesser quality or fidelity to the prompt, providing a quantitative measure of how well the generated image adheres to the intended description.</p>
<p>Again, we will import the necessary libraries:</p>
<pre class="source-code">
from typing import List, Tuple
from PIL import Image
import requests
from transformers import CLIPProcessor, CLIPModel
import torch</pre>
<p>We begin by loading the CLIP model, processor, and necessary parameters:</p>
<pre class="source-code">
# Constants
CLIP_REPO = "openai/clip-vit-base-patch32"
def load_model_and_processor(
    model_name: str
) -&gt; Tuple[CLIPModel, CLIPProcessor]:
    """
    Loads the CLIP model and processor.
    """
    model = CLIPModel.from_pretrained(model_name)
    processor = CLIPProcessor.from_pretrained(model_name)
    return model, processor</pre>
<p>Next, we define a processing function to adjust the textual prompts and images, ensuring that they are in the correct format for CLIP inference:</p>
<pre class="source-code">
def process_inputs(
    processor: CLIPProcessor, prompts: List[str],
    images: List[Image.Image]) -&gt; dict:
"""
Processes the inputs using the CLIP processor.
"""
    return processor(text=prompts, images=images,
        return_tensors="pt", padding=True)</pre>
<p>In this step, we<a id="_idIndexMarker152"/> initiate the evaluation process by inputting the images and textual prompts into the CLIP model. This is done in parallel across multiple devices to optimize performance. The model then computes similarity scores, known as logits, for each image-text pair. These scores indicate how well each image corresponds to the text prompts. To interpret these scores more intuitively, we convert them into probabilities, which indicate the likelihood that an image aligns with any of the given prompts:</p>
<pre class="source-code">
def get_probabilities(
    model: CLIPModel, inputs: dict) -&gt; torch.Tensor:
"""
Computes the probabilities using the CLIP model.
"""
    outputs = model(**inputs)
    logits = outputs.logits_per_image
    # Define temperature - higher temperature will make the distribution more uniform.
    T = 10
    # Apply temperature to the logits
    temp_adjusted_logits = logits / T
    probs = torch.nn.functional.softmax(
        temp_adjusted_logits, dim=1)
    return probs</pre>
<p>Lastly, we display the images along with their scores, visually representing how well each image <a id="_idIndexMarker153"/>adheres to the provided prompts:</p>
<pre class="source-code">
def display_images_with_scores(
    images: List[Image.Image], scores: torch.Tensor) -&gt; None:
"""
Displays the images alongside their scores.
"""
    # Set print options for readability
    torch.set_printoptions(precision=2, sci_mode=False)
    for i, image in enumerate(images):
        print(f"Image {i + 1}:")
        display(image)
        print(f"Scores: {scores[i, :]}")
        print()</pre>
<p>With everything detailed, let’s execute the pipeline as follows:</p>
<pre class="source-code">
# Load CLIP model
model, processor = load_model_and_processor(CLIP_REPO)
# Process image and text inputs together
inputs = process_inputs(processor, prompts, images)
# Extract the probabilities
probs = get_probabilities(model, inputs)
# Display each image with corresponding scores
display_images_with_scores(images, probs)</pre>
<p>We now have scores for each of our synthetic images that quantify the fidelity of the synthetic image to the text provided, based on the CLIP model, which interprets both image and text<a id="_idIndexMarker154"/> data as one combined mathematical representation (or geometric space) and can measure their similarity.</p>
<div><div><img alt="Figure 2.2: CLIP scores" src="img/B21773_02_02.jpg"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: CLIP scores</p>
<p>For our “friendly lion,” we computed scores of 83% and 17% for each prompt, which we can interpret as an 83% likelihood that the image aligns with the first prompt.</p>
<p>In practical scenarios, this metric can be applied across various domains:</p>
<ul>
<li><strong class="bold">Content moderation</strong>: Automatically moderating or flagging inappropriate content by comparing images to a set of predefined descriptive prompts</li>
<li><strong class="bold">Image retrieval</strong>: Facilitating refined image searches by matching textual queries to a vast database of images, hence narrowing down the search to the most relevant visuals</li>
<li><strong class="bold">Image captioning</strong>: Assisting in generating accurate captions for images by identifying the most relevant descriptive prompts</li>
<li><strong class="bold">Advertising</strong>: Tailoring advertisements based on the content of images on a web page to enhance user engagement</li>
<li><strong class="bold">Accessibility</strong>: Enhancing accessibility features by providing accurate descriptions of images for individuals with visual impairments</li>
</ul>
<p>This evaluation method not only speeds up processes that would otherwise require manual inspection but also lends itself to many applications that could benefit from a deeper understanding and contextual analysis of visual data. We will revisit the CLIP evaluation in <a href="B21773_04.xhtml#_idTextAnchor123"><em class="italic">Chapter 4</em></a>, where we simulate a real-world scenario to determine the quality and <a id="_idIndexMarker155"/>appropriateness of automatically generated captions for a set of product images.</p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor077"/>Summary</h1>
<p>This chapter explored the theoretical underpinnings and real-world applications of leading GAI techniques, including GANs, diffusion models, and transformers. We examined their unique strengths, including GANs’ ability to synthesize highly realistic images, diffusion models’ elegant image generation process, and transformers’ exceptional language generation capabilities.</p>
<p>Using a cloud-based Python environment, we implemented these models to generate compelling images and evaluated their output quality using CLIP. We analyzed how techniques such as progressive growing and classifier guidance enhanced output fidelity over time. We also considered societal impacts, urging developers to address potential harm through transparency and ethical practices.</p>
<p>Generative methods have unlocked remarkable creative potential, but thoughtful oversight is critical as capabilities advance. We can guide these technologies toward broadly beneficial outcomes by grounding ourselves in core methodologies, scrutinizing their limitations, and considering downstream uses. The path ahead will re<a id="_idTextAnchor078"/>quire continued research and ethical reflection to unlock AI’s creative promise while mitigating risks.</p>
<h1 id="_idParaDest-44"><a id="_idTextAnchor079"/>References</h1>
<p>This reference section serves as a repository of sources referenced within this book; you can explore these resources to further enhance your understanding and knowledge of the subject matter:</p>
<ul>
<li>Kenfack, P. J., Arapov, D. D., Hussain, R., Ahsan Kazmi, S. M., &amp; Khan, A. (2021). <em class="italic">On the fairness of generative adversarial networks (</em><em class="italic">GANs)</em>. <a href="https://Arxiv.org">Arxiv.org</a>.</li>
<li>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. (2014). <em class="italic">Generative adversarial nets. Advances in neural information processing </em><em class="italic">systems</em>, 27.</li>
<li>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., &amp; Chen, M. (2021). <em class="italic">GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models</em>. arXiv preprint arXiv:2112.10741.</li>
<li>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., &amp; Sutskever, I. (2021). <em class="italic">Learning Transferable Visual Models From Natural Language Supervision.</em> ArXiv. /abs/2103.00020.</li>
<li>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. (2022).<em class="italic"> Hierarchical text-conditional image generation with clip latents</em>. arXiv preprint arXiv:2204.06125.</li>
<li>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., &amp; Sutskever, I. (2021). <em class="italic">Zero-shot text-to-image generation. In International Conference on Machine Learning</em> (pp. 8821–8831). PMLR.</li>
<li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). <em class="italic">Attention is all you need. Advances in neural information processing </em><em class="italic">systems</em>, 30.</li>
<li>Arjovsky, M., Chintala, S. &amp; Bottou, L. (2017). <em class="italic">Wasserstein GAN. In Proceedings of the 31st International Conference on Neural Information Processing </em><em class="italic">System (NIPS).</em></li>
<li>Brock, A., Donahue, J., &amp; Simonyan, K. (2018). <em class="italic">BigGANs: Large Scale GAN Training for High Fidelity Natural Image </em><em class="italic">Synthesis.</em> <a href="https://arxiv.org/abs/1809.11096">https://arxiv.org/abs/1809.11096</a>.</li>
<li>Karras, T., Aila, T., Laine, S., &amp; Lehtinen, J. (2017). <em class="italic">Progressive Growing of GANs for Improved Quality, Stability, and </em><em class="italic">Variation.</em> <a href="https://arxiv.org/abs/1710.10196">https://arxiv.org/abs/1710.10196</a>.</li>
<li>Mirza, M., &amp; Osindero, S. (2014). <em class="italic">Conditional Generative Adversarial </em><em class="italic">Nets</em>. <a href="https://arxiv.org/abs/1411.1784">https://arxiv.org/abs/1411.1784</a>.</li>
<li>Radford, A., Metz, L., &amp; Chintala, S. (2015). <em class="italic">Unsupervised representation learning with deep convolutional generative adversarial networks. 3rd International Conference for </em><em class="italic">Learning Representations.</em></li>
<li>Zhu, J.-Y., Park, T., Isola, P., &amp; Efros, A. A. (2017). <em class="italic">Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks. In Proceedings of the IEEE International Conference on Computer </em><em class="italic">Vision (ICCV).</em></li>
<li>Ho, J., &amp; Salimans, T. (2022). <em class="italic">Classifier-Free Diffusion Guidance. Advances in Neural Information Processing </em><em class="italic">Systems</em>, 34.</li>
<li>Ho, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi, M., &amp; Fleet, D. J. (2022). <em class="italic">Video diffusion models</em>. arXiv preprint arXiv:2205.10477.</li>
<li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). <em class="italic">Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems</em>, 33, 6840–6851.</li>
<li>Luo, S., &amp; Hu, W. (2021). <em class="italic">Diffusion probabilistic models for 3d point cloud generation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern </em><em class="italic">Recognition</em>, 2837–2845.</li>
<li>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2021). <em class="italic">High-resolution image synthesis with latent diffusion models. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern </em><em class="italic">Recognition</em>, 10684–10695.</li>
<li>Salimans, T., Karpathy, A., Chen, X., &amp; Kingma, D. P. (2017). <em class="italic">PixelCNN++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</em>. arXiv preprint arXiv:1701.05517.</li>
<li>Song, Y., Meng, C., &amp; Ermon, S. (2021). <em class="italic">Denoising diffusion implicit models</em>. arXiv preprint arXiv:2010.02502.</li>
<li>Song, Y., &amp; Ermon, S. (2021). <em class="italic">Improved techniques for training score-based generative models. Advances in Neural Information Processing Systems</em>, 33, 12438–12448.</li>
<li>Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., &amp; Ganguli, S. (2015). <em class="italic">Deep unsupervised learning using nonequilibrium thermodynamics</em>. arXiv preprint arXiv:1503.03585.</li>
<li>Ho, J., Jain, A., &amp; Abbeel, P. (2020). <em class="italic">Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems</em>, 33, 6840–6851.</li>
<li>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., ... &amp; Sutskever, I. (2022). <em class="italic">Zero-shot text-to-image generation. International Conference on Machine </em><em class="italic">Learning</em>, 8821-8831.</li>
<li>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... &amp; Amodei, D. (2020). <em class="italic">Language models are few-shot learners. Advances in neural information processing systems</em>, 33, 1877–1901.</li>
<li>Patil, S., Cuenca, P., Lambert, N., &amp; von Platen, P. (2022). <em class="italic">Stable diffusion with diffusers</em>. Hugging Face Blog. <a href="https://huggingface.co/blog/stable_diffusion">https://huggingface.co/blog/stable_diffusion</a>.</li>
<li>Boris Dayma, Suraj Patil, Pedro Cuenca, Khalid Saifullah, Tanishq Abraham, Phúc Lê, Luke, Ritobrata Ghosh. (2022, June 4). <em class="italic">DALL-E Mini Explained</em>. W&amp;B; Weights &amp; Biases, Inc. <a href="https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA">https://wandb.ai/dalle-mini/dalle-mini/reports/DALL-E-Mini-Explained-with-Demo--Vmlldzo4NjIxODA</a>.</li>
</ul>
</div>
</body></html>