["```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    ```", "```py\n    from langdetect import detect\n    from nltk import word_tokenize\n    from nltk.probability import FreqDist\n    from nltk.corpus import stopwords\n    from string import punctuation\n    ```", "```py\n    (train_df, test_df) = load_train_test_dataset_pd(\"train\", \n        \"test\")\n    print(train_df)\n    print(test_df)\n    ```", "```py\n                                                       text  label\n    0     the rock is destined to be the 21st century's ...      1\n    1     the gorgeously elaborate continuation of \" the...      1\n    ...                                                 ...    ...\n    8525  any enjoyment will be hinge from a personal th...      0\n    8526  if legendary shlockmeister ed wood had ever ma...      0\n    [8530 rows x 2 columns]\n                                                       text  label\n    0     lovingly photographed in the manner of a golde...      1\n    1                 consistently clever and suspenseful .      1\n    ...                                                 ...    ...\n    1061  a terrible movie that some people will neverth...      0\n    1062  there are many definitions of 'time waster' bu...      0\n    [1066 rows x 2 columns]\n    ```", "```py\n    train_df[\"lang\"] = train_df[\"text\"].apply(detect)\n    train_df = train_df[train_df['lang'] == 'en']\n    print(train_df)\n    ```", "```py\n                                                       text  label lang\n    0     the rock is destined to be the 21st century's ...      1   en\n    1     the gorgeously elaborate continuation of \" the...      1   en\n    ...                                                 ...\n        ...  ...\n    8528    interminably bleak , to say nothing of boring .      0   en\n    8529  things really get weird , though not particula...      0   en\n    [8364 rows x 3 columns]\n    ```", "```py\n    test_df[\"lang\"] = test_df[\"text\"].apply(detect)\n    test_df = test_df[test_df['lang'] == 'en']\n    ```", "```py\n    train_df[\"tokenized_text\"] = train_df[\"text\"].apply(\n        word_tokenize)\n    print(train_df)\n    test_df[\"tokenized_text\"] = test_df[\"text\"].apply(word_tokenize)\n    print(test_df)\n    ```", "```py\n                                                       text  label lang  \\\n    0     the rock is destined to be the 21st century's ...\n          1   en\n    1     the gorgeously elaborate continuation of \" the...      \n    1   en\n    ...                                                 ...    ...\n      ...\n    8528    interminably bleak , to say nothing of boring .      \n    0   en\n    8529  things really get weird , though not particula...      \n    0   en\n                                             tokenized_text\n    0     [the, rock, is, destined, to, be, the, 21st, c...\n    1     [the, gorgeously, elaborate, continuation, of,...\n    ...                                                 ...\n    8528  [interminably, bleak, ,, to, say, nothing, of,...\n    8529  [things, really, get, weird, ,, though, not, p...\n    [8352 rows x 4 columns]\n    ```", "```py\n    stop_words = list(stopwords.words('english'))\n    stop_words.append(\"``\")\n    stop_words.append(\"'s\")\n    def remove_stopwords_and_punct(x):\n        new_list = [w for w in x if w not in stop_words and w not in punctuation]\n        return new_list\n    train_df[\"tokenized_text\"] = train_df[\"tokenized_text\"].apply(\n        remove_stopwords_and_punct)\n    print(train_df)\n    test_df[\"tokenized_text\"] = test_df[\"tokenized_text\"].apply(\n        remove_stopwords_and_punct)\n    print(test_df)\n    ```", "```py\n                                                       text  label lang  \\\n    0     the rock is destined to be the 21st century's ...\n          1   en\n    1     the gorgeously elaborate continuation of \" the...\n          1   en\n    ...                                                 ...\n        ...  ...\n    8528    interminably bleak , to say nothing of boring .\n          0   en\n    8529  things really get weird , though not particula...\n          0   en\n                                             tokenized_text\n    0     [rock, destined, 21st, century, new, conan, go...\n    1     [gorgeously, elaborate, continuation, lord, ri...\n    ...                                                 ...\n    8528        [interminably, bleak, say, nothing, boring]\n    8529  [things, really, get, weird, though, particula...\n    [8352 rows x 4 columns]\n    ```", "```py\n    print(train_df.groupby('label').count())\n    print(test_df.groupby('label').count())\n    ```", "```py\n    text  lang  tokenized_text\n    label\n    0      4185  4185            4185\n    1      4167  4167            4167\n           text  lang  tokenized_text\n    label\n    0       523   523             523\n    1       522   522             522\n    ```", "```py\n    train_df.to_json(\"../data/rotten_tomatoes_train.json\")\n    test_df.to_json(\"../data/rotten_tomatoes_test.json\")\n    ```", "```py\n    def get_stats(word_list, num_words=200):\n        freq_dist = FreqDist(word_list)\n        print(freq_dist.most_common(num_words))\n        return freq_dist\n    ```", "```py\n    positive_train_words = train_df[\n        train_df[\"label\"] == 1].tokenized_text.sum()\n    negative_train_words = train_df[\n        train_df[\"label\"] == 0].tokenized_text.sum()\n    positive_fd = get_stats(positive_train_words)\n    negative_fd = get_stats(negative_train_words)\n    ```", "```py\n    [('film', 683), ('movie', 429), (\"n't\", 286), ('one', 280), ('--', 271), ('like', 209), ('story', 194), ('comedy', 160), ('good', 150), ('even', 144), ('funny', 137), ('way', 135), ('time', 127), ('best', 126), ('characters', 125), ('make', 124), ('life', 124), ('much', 122), ('us', 122), ('love', 118), ...]\n    [('movie', 641), ('film', 557), (\"n't\", 450), ('like', 354), ('one', 293), ('--', 264), ('story', 189), ('much', 177), ('bad', 173), ('even', 160), ('time', 146), ('good', 143), ('characters', 138), ('little', 137), ('would', 130), ('never', 122), ('comedy', 121), ('enough', 107), ('really', 105), ('nothing', 103), ('way', 102), ('make', 101), ...]\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    ```", "```py\n    from nltk import word_tokenize\n    from sklearn.feature_extraction.text import CountVectorizer\n    from sklearn.metrics import classification_report\n    ```", "```py\n    train_df = pd.read_json(\"../data/rotten_tomatoes_train.json\")\n    test_df = pd.read_json(\"../data/rotten_tomatoes_test.json\")\n    ```", "```py\n    positive_train_words = train_df[train_df[\"label\"] \n        == 1].text.sum()\n    negative_train_words = train_df[train_df[\"label\"] \n        == 0].text.sum()\n    word_intersection = set(positive_train_words) \\ \n        & set(negative_train_words)\n    positive_filtered = list(set(positive_train_words) \n        - word_intersection)\n    negative_filtered = list(set(negative_train_words) \n        - word_intersection)\n    ```", "```py\n    def create_vectorizers(word_lists):\n        vectorizers = []\n        for word_list in word_lists:\n            vectorizer = CountVectorizer(vocabulary=word_list)\n            vectorizers.append(vectorizer)\n        return vectorizers\n    ```", "```py\n    vectorizers = create_vectorizers([negative_filtered,\n        positive_filtered])\n    ```", "```py\n    def vectorize(text_list, vectorizers):\n        text = \" \".join(text_list)\n        scores = []\n        for vectorizer in vectorizers:\n            output = vectorizer.transform([text])\n            output_sum = sum(output.todense().tolist()[0])\n            scores.append(output_sum)\n        return scores\n    ```", "```py\n    def classify(score_list):\n        return max(enumerate(score_list),key=lambda x: x[1])[0]\n    ```", "```py\n    train_df[\"prediction\"] = train_df[\"text\"].apply(\n        lambda x: classify(vectorize(x, vectorizers)))\n    print(train_df)\n    ```", "```py\n                                                       text  label lang  \\\n    0     [rock, destined, 21st, century, new, conan, go...      \n    1   en\n    1     [gorgeously, elaborate, continuation, lord, ri...      \n    1   en\n    ...                                                 ...    ...\n      ...\n    8528        [interminably, bleak, say, nothing, boring]      \n    0   en\n    8529  [things, really, get, weird, though, particula...      \n    0   en\n          prediction\n    0              1\n    1              1\n    ...          ...\n    8528           0\n    8529           0\n    [8364 rows x 4 columns]\n    ```", "```py\n    print(classification_report(train_df['label'], \n        train_df['prediction']))\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.79      0.99      0.88      4194\n               1       0.99      0.74      0.85      4170\n        accuracy                           0.87      8364\n       macro avg       0.89      0.87      0.86      8364\n    weighted avg       0.89      0.87      0.86      8364\n    ```", "```py\n    test_df[\"prediction\"] = test_df[\"text\"].apply(\n        lambda x: classify(vectorize(x, vectorizers)))\n    print(classification_report(test_df['label'], \n        test_df['prediction']))\n    ```", "```py\n                  precision    recall  f1-score   support\n               0       0.59      0.81      0.68       523\n               1       0.70      0.43      0.53       524\n        accuracy                           0.62      1047\n       macro avg       0.64      0.62      0.61      1047\n    weighted avg       0.64      0.62      0.61      1047\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    from nltk import word_tokenize\n    from sklearn.cluster import KMeans\n    from nltk.probability import FreqDist\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from sklearn.model_selection import StratifiedShuffleSplit\n    ```", "```py\n    train_dataset = load_dataset(\"SetFit/bbc-news\", split=\"train\")\n    test_dataset = load_dataset(\"SetFit/bbc-news\", split=\"test\")\n    train_df = train_dataset.to_pandas()\n    test_df = test_dataset.to_pandas()\n    print(train_df)\n    print(test_df)\n    ```", "```py\n                                                       text  label\n         label_text\n    0     wales want rugby league training wales could f...      2\n              sport\n    1     china aviation seeks rescue deal scandal-hit j...      1\n           business\n    ...                                                 ...    ...\n                ...\n    1223  why few targets are better than many the econo...      1\n           business\n    1224  boothroyd calls for lords speaker betty boothr...      4\n           politics\n    [1225 rows x 3 columns]\n                                                      text  label\n         label_text\n    0    carry on star patsy rowlands dies actress pats...      3\n      entertainment\n    1    sydney to host north v south game sydney will ...      2\n              sport\n    ..                                                 ...    ...\n                ...\n    998  stormy year for property insurers a string of ...      1\n           business\n    999  what the election should really be about  a ge...      4\n           politics\n    [1000 rows x 3 columns]\n    ```", "```py\n    print(train_df.groupby('label_text').count())\n    print(test_df.groupby('label_text').count())\n    ```", "```py\n                   text  label\n    label_text\n    business        286    286\n    entertainment   210    210\n    politics        242    242\n    sport           275    275\n    tech            212    212\n                   text  label\n    label_text\n    business        224    224\n    entertainment   176    176\n    politics        175    175\n    sport           236    236\n    tech            189    189\n    ```", "```py\n    combined_df = pd.concat([train_df, test_df],\n        ignore_index=True, sort=False)\n    print(combined_df)\n    sss = StratifiedShuffleSplit(n_splits=1,\n        test_size=0.2, random_state=0)\n    train_index, test_index = next(\n        sss.split(combined_df[\"text\"], combined_df[\"label\"]))\n    train_df = combined_df[combined_df.index.isin(\n        train_index)].copy()\n    test_df = combined_df[combined_df.index.isin(test_index)].copy()\n    print(train_df.groupby('label_text').count())\n    print(test_df.groupby('label_text').count())\n    ```", "```py\n                   text  label  text_tokenized  text_clean  cluster\n    label_text\n    business        408    408             408         408      330\n    entertainment   309    309             309         309      253\n    politics        333    333             333         333      263\n    sport           409    409             409         409      327\n    tech            321    321             321         321      262\n                   text  label  text_tokenized  text_clean  cluster\n    label_text\n    business        102    102             102         102       78\n    entertainment    77     77              77          77       56\n    politics         84     84              84          84       70\n    sport           102    102             102         102       82\n    tech             80     80              80          80       59\n    ```", "```py\n    train_df = tokenize(train_df, \"text\")\n    train_df = remove_stopword_punct(train_df, \"text_tokenized\")\n    test_df = tokenize(test_df, \"text\")\n    test_df = remove_stopword_punct(test_df, \"text_tokenized\")\n    ```", "```py\n    train_df[\"text_clean\"] = train_df[\"text_tokenized\"].apply(\n        lambda x: \" \".join(list(x)))\n    test_df[\"text_clean\"] = test_df[\"text_tokenized\"].apply(\n        lambda x: \" \".join(list(x)))\n    train_df.to_json(\"../data/bbc_train.json\")\n    test_df.to_json(\"../data/bbc_test.json\")\n    vec = TfidfVectorizer(ngram_range=(1,3))\n    matrix = vec.fit_transform(train_df[\"text_clean\"])\n    ```", "```py\nkm = KMeans(n_clusters=5, n_init=10)\nkm.fit(matrix)\n```", "```py\n    def get_most_frequent_words(text, num_words):\n        word_list = word_tokenize(text)\n        freq_dist = FreqDist(word_list)\n        top_words = freq_dist.most_common(num_words)\n        top_words = [word[0] for word in top_words]\n        return top_words\n    ```", "```py\n    def print_most_common_words_by_cluster(input_df, km, \n        num_clusters):\n        clusters = km.labels_.tolist()\n        input_df[\"cluster\"] = clusters\n        for cluster in range(0, num_clusters):\n            this_cluster_text = input_df[\n                input_df['cluster'] == cluster]\n            all_text = \" \".join(\n                this_cluster_text['text_clean'].astype(str))\n            top_200 = get_most_frequent_words(all_text, 200)\n            print(cluster)\n            print(top_200)\n        return input_df\n    ```", "```py\n    print_most_common_words_by_cluster(train_df, km, 5)\n    ```", "```py\n    0\n    ['mr', 'said', 'would', 'labour', 'party', 'election', 'blair', 'government', ...]\n    1\n    ['film', 'said', 'best', 'also', 'year', 'one', 'us', 'awards', 'music', 'new', 'number', 'award', 'show', ...]\n    2\n    ['said', 'game', 'england', 'first', 'win', 'world', 'last', 'one', 'two', 'would', 'time', 'play', 'back', 'cup', 'players', ...]\n    3\n    ['said', 'mr', 'us', 'year', 'people', 'also', 'would', 'new', 'one', 'could', 'uk', 'sales', 'firm', 'growth', ...]\n    4\n    ['said', 'people', 'software', 'would', 'users', 'mr', 'could', 'new', 'microsoft', 'security', 'net', 'search', 'also', ...]\n    ```", "```py\n    test_example = test_df.iloc[1, test_df.columns.get_loc('text')]\n    print(test_example)\n    vectorized = vec.transform([test_example])\n    prediction = km.predict(vectorized)\n    print(prediction)\n    ```", "```py\n    lib dems  new election pr chief the lib dems have appointed a senior figure from bt to be the party s new communications chief for their next general election effort.  sandy walkington will now work with senior figures such as matthew taylor on completing the party manifesto. party chief executive lord rennard said the appointment was a  significant strengthening of the lib dem team . mr walkington said he wanted the party to be ready for any  mischief  rivals or the media tried to throw at it.   my role will be to ensure this new public profile is effectively communicated at all levels   he said.  i also know the party will be put under scrutiny in the media and from the other parties as never before - and we will need to show ourselves ready and prepared to counter the mischief and misrepresentation that all too often comes from the party s opponents.  the party is already demonstrating on every issue that it is the effective opposition.  mr walkington s new job title is director of general election communications.\n    [0]\n    ```", "```py\n    dump(km, '../data/kmeans.joblib')\n    km_ = load('../data/kmeans.joblib')\n    prediction = km_.predict(vectorized)\n    print(prediction)\n    ```", "```py\n    [0]\n    ```", "```py\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    ```", "```py\n    from sklearn.svm import SVC\n    from sentence_transformers import SentenceTransformer\n    from sklearn.metrics import confusion_matrix\n    ```", "```py\n    train_df = pd.read_json(\"../data/bbc_train.json\")\n    test_df = pd.read_json(\"../data/bbc_test.json\")\n    train_df.sample(frac=1)\n    print(train_df.groupby('label_text').count())\n    print(test_df.groupby('label_text').count())\n    ```", "```py\n                   text  label  text_tokenized  text_clean  cluster\n    label_text\n    business        231    231             231         231      231\n    entertainment   181    181             181         181      181\n    politics        182    182             182         182      182\n    sport           243    243             243         243      243\n    tech            194    194             194         194      194\n                   text  label  text_tokenized  text_clean\n    label_text\n    business         58     58              58          58\n    entertainment    45     45              45          45\n    politics         45     45              45          45\n    sport            61     61              61          61\n    tech             49     49              49          49\n    ```", "```py\n    model = SentenceTransformer('all-MiniLM-L6-v2')\n    def get_sentence_vector(text, model):\n        sentence_embeddings = model.encode([text])\n        return sentence_embeddings[0]\n    ```", "```py\n    def train_classifier(X_train, y_train):\n        clf = SVC(C=0.1, kernel='rbf')\n        clf = clf.fit(X_train, y_train)\n        return clf\n    ```", "```py\n    target_names=[\"tech\", \"business\", \"sport\", \n        \"entertainment\", \"politics\"]\n    vectorize = lambda x: get_sentence_vector(x, model)\n    (X_train, X_test, y_train, y_test) = create_train_test_data(\n        train_df, test_df, vectorize, column_name=\"text_clean\")\n    clf = train_classifier(X_train, y_train)\n    print(classification_report(train_df[\"label\"],\n            y_train, target_names=target_names))\n    test_classifier(test_df, clf, target_names=target_names)\n    ```", "```py\n                   precision    recall  f1-score   support\n             tech       1.00      1.00      1.00       194\n         business       1.00      1.00      1.00       231\n            sport       1.00      1.00      1.00       243\n    entertainment       1.00      1.00      1.00       181\n         politics       1.00      1.00      1.00       182\n         accuracy                           1.00      1031\n        macro avg       1.00      1.00      1.00      1031\n     weighted avg       1.00      1.00      1.00      1031\n                   precision    recall  f1-score   support\n             tech       0.92      0.98      0.95        49\n         business       0.95      0.90      0.92        58\n            sport       1.00      1.00      1.00        61\n    entertainment       1.00      0.98      0.99        45\n         politics       0.96      0.98      0.97        45\n         accuracy                           0.97       258\n        macro avg       0.97      0.97      0.97       258\n     weighted avg       0.97      0.97      0.96       258\n    ```", "```py\n    print(confusion_matrix(test_df[\"label\"], test_df[\"prediction\"]))\n    [[48  1  0  0  0]\n     [ 4 52  0  0  2]\n     [ 0  0 61  0  0]\n     [ 0  1  0 44  0]\n     [ 0  1  0  0 44]]\n    ```", "```py\n    new_example = \"\"\"iPhone 12: Apple makes jump to 5G\n    Apple has confirmed its iPhone 12 handsets will be its first to work on faster 5G networks.\n    The company has also extended the range to include a new \"Mini\" model that has a smaller 5.4in screen.\n    The US firm bucked a wider industry downturn by increasing its handset sales over the past year.\n    But some experts say the new features give Apple its best opportunity for growth since 2014, when it revamped its line-up with the iPhone 6.\n    \"5G will bring a new level of performance for downloads and uploads, higher quality video streaming, more responsive gaming, real-time interactivity and so much more,\" said chief executive Tim Cook.\n    …\"\"\"\n    vector = vectorize(new_example)\n    prediction = clf.predict([vector])\n    print(prediction))\n    ```", "```py\n    [0]\n    ```", "```py\n    %run -i \"../util/lang_utils.ipynb\"\n    ```", "```py\n    import pandas as pd\n    from spacy.cli.train import train\n    from spacy.cli.evaluate import evaluate\n    from spacy.cli.debug_data import debug_data\n    from spacy.tokens import DocBin\n    ```", "```py\n    def preprocess_data_entry(input_text, label, label_list):\n        doc = small_model(input_text)\n        cats = [0] * len(label_list)\n        cats[label] = 1\n        final_cats = {}\n        for i, label in enumerate(label_list):\n            final_cats[label] = cats[i]\n        doc.cats = final_cats\n        return doc\n    ```", "```py\n    train_db = DocBin()\n    test_db = DocBin()\n    label_list = [\"tech\", \"business\", \"sport\", \n        \"entertainment\", \"politics\"]\n    train_df = pd.read_json(\"../data/bbc_train.json\")\n    test_df = pd.read_json(\"../data/bbc_test.json\")\n    train_df.sample(frac=1)\n    for idx, row in train_df.iterrows():\n        text = row[\"text\"]\n        label = row[\"label\"]\n        doc = preprocess_data_entry(text, label, label_list)\n        train_db.add(doc)\n    for idx, row in test_df.iterrows():\n        text = row[\"text\"]\n        label = row[\"label\"]\n        doc = preprocess_data_entry(text, label, label_list)\n        test_db.add(doc)\n    train_db.to_disk('../data/bbc_train.spacy')\n    test_db.to_disk('../data/bbc_test.spacy')\n    ```", "```py\n    train(\"../data/spacy_config.cfg\", output_path=\"../models/spacy_textcat_bbc\")\n    ```", "```py\n    ℹ Saving to output directory: ../models/spacy_textcat_bbc\n    ℹ Using CPU\n    =========================== Initializing pipeline ===========================\n    ✔ Initialized pipeline\n    4.5-spacy_textcat.ipynb\n    ============================= Training pipeline =============================\n    ℹ Pipeline: ['tok2vec', 'textcat']\n    ℹ Initial learn rate: 0.001\n    E    #       LOSS TOK2VEC  LOSS TEXTCAT  CATS_SCORE  SCORE\n    ---  ------  ------------  ------------  ----------  ------\n      0       0          0.00          0.16        8.48    0.08\n      0     200         20.77         37.26       35.58    0.36\n      0     400         98.56         35.96       26.90    0.27\n      0     600         49.83         37.31       36.60    0.37\n    … (truncated)\n      4    4800       7571.47          9.64       80.25    0.80\n      4    5000      16164.99         10.58       87.71    0.88\n      5    5200       8604.43          8.20       84.98    0.85\n    ✔ Saved pipeline to output directory\n    ../models/spacy_textcat_bbc/model-last\n    ```", "```py\n    nlp = spacy.load(\"../models/spacy_textcat_bbc/model-last\")\n    input_text = test_df.iloc[1, test_df.columns.get_loc('text')]\n    print(input_text)\n    print(test_df[\"label_text\"].iloc[[1]])\n    doc = nlp(input_text)\n    print(\"Predicted probabilities: \", doc.cats)\n    ```", "```py\n    lib dems  new election pr chief the lib dems have appointed a senior figure from bt to be the party s new communications chief for their next general election effort.  sandy walkington will now work with senior figures such as matthew taylor on completing the party manifesto. party chief executive lord rennard said the appointment was a  significant strengthening of the lib dem team . mr walkington said he wanted the party to be ready for any  mischief  rivals or the media tried to throw at it.   my role will be to ensure this new public profile is effectively communicated at all levels   he said.  i also know the party will be put under scrutiny in the media and from the other parties as never before - and we will need to show ourselves ready and prepared to counter the mischief and misrepresentation that all too often comes from the party s opponents.  the party is already demonstrating on every issue that it is the effective opposition.  mr walkington s new job title is director of general election communications.\n    8    politics\n    Name: label_text, dtype: object\n    Predicted probabilities:  {'tech': 3.531841841208916e-08, 'business': 0.000641813559923321, 'sport': 0.00033847044687718153, 'entertainment': 0.00016174423217307776, 'politics': 0.9988579750061035}\n    ```", "```py\n    def get_prediction(input_text, nlp_model, target_names):\n        doc = nlp_model(input_text)\n        category = max(doc.cats, key = doc.cats.get)\n        return target_names.index(category)\n    test_df[\"prediction\"] = test_df[\"text\"].apply(\n        lambda x: get_prediction(x, nlp, label_list))\n    ```", "```py\n    print(classification_report(test_df[\"label\"],\n        test_df[\"prediction\"], target_names=target_names))\n    ```", "```py\n                   precision    recall  f1-score   support\n             tech       0.82      0.94      0.87        80\n         business       0.94      0.83      0.89       102\n            sport       0.89      0.89      0.89       102\n    entertainment       0.94      0.87      0.91        77\n         politics       0.78      0.83      0.80        84\n         accuracy                           0.87       445\n        macro avg       0.87      0.87      0.87       445\n     weighted avg       0.88      0.87      0.87       445\n    ```", "```py\n    evaluate('../models/spacy_textcat_bbc/model-last', '../data/bbc_test.spacy')\n    ```", "```py\n    {'token_acc': 1.0,\n     'token_p': 1.0,\n     'token_r': 1.0,\n     'token_f': 1.0,\n     'cats_score': 0.8719339318444819,\n     'cats_score_desc': 'macro F',\n     'cats_micro_p': 0.8719101123595505,\n     'cats_micro_r': 0.8719101123595505,\n     'cats_micro_f': 0.8719101123595505,\n     'cats_macro_p': 0.8746516896205309,\n     'cats_macro_r': 0.8732906799083269,\n     'cats_macro_f': 0.8719339318444819,\n     'cats_macro_auc': 0.9800144873453936,\n     'cats_f_per_type': {'tech': {'p': 0.8152173913043478,\n       'r': 0.9375,\n       'f': 0.872093023255814},\n      'business': {'p': 0.9444444444444444,\n       'r': 0.8333333333333334,\n       'f': 0.8854166666666667},\n      'sport': {'p': 0.8921568627450981,\n       'r': 0.8921568627450981,\n       'f': 0.8921568627450981},\n      'entertainment': {'p': 0.9436619718309859,\n       'r': 0.8701298701298701,\n       'f': 0.9054054054054054},\n      'politics': {'p': 0.7777777777777778,\n       'r': 0.8333333333333334,\n       'f': 0.8045977011494253}},\n     'cats_auc_per_type': {'tech': 0.9842808219178081,\n      'business': 0.9824501229063054,\n      'sport': 0.9933544846510032,\n      'entertainment': 0.9834839073969509,\n      'politics': 0.9565030998549005},\n     'speed': 6894.989948433934}\n    ```", "```py\n    %run -i \"../util/file_utils.ipynb\"\n    %run -i \"../util/util_simple_classifier.ipynb\"\n    ```", "```py\n    import re\n    from sklearn.metrics import classification_report\n    from openai import OpenAI\n    client = OpenAI(api_key=OPEN_AI_KEY)\n    ```", "```py\n    train_dataset = load_dataset(\"SetFit/bbc-news\", split=\"train\")\n    test_dataset = load_dataset(\"SetFit/bbc-news\", split=\"test\")\n    ```", "```py\n    example = test_dataset[0][\"text\"]\n    category = test_dataset[0][\"label_text\"]\n    print(example)\n    print(category)\n    ```", "```py\n    carry on star patsy rowlands dies actress patsy rowlands  known to millions for her roles in the carry on films  has died at the age of 71.  rowlands starred in nine of the popular carry on films  alongside fellow regulars sid james  kenneth williams and barbara windsor. she also carved out a successful television career  appearing for many years in itv s well-loved comedy bless this house....\n    entertainment\n    ```", "```py\n    prompt=\"\"\"You are classifying texts by topics. There are 5 topics: tech, entertainment, business, politics and sport.\n    Output the topic and nothing else. For example, if the topic is business, your output should be \"business\".\n    Give the following text, what is its topic from the above list without any additional explanations: \"\"\" + example\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo\",\n        temperature=0,\n        max_tokens=256,\n        top_p=1.0,\n        frequency_penalty=0,\n        presence_penalty=0,\n        messages=[\n            {\"role\": \"system\", \"content\": \n                \"You are a helpful assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ],\n    )\n    print(response.choices[0].message.content)\n    ```", "```py\n    entertainment\n    ```", "```py\n    def get_gpt_classification(input_text):\n        prompt=\"\"\"You are classifying texts by topics. There are 5 topics: tech, entertainment, business, politics and sport.\n    Output the topic and nothing else. For example, if the topic is business, your output should be \"business\".\n    Give the following text, what is its topic from the above list without any additional explanations: \"\"\" + input_text\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            temperature=0,\n            max_tokens=256,\n            top_p=1.0,\n            frequency_penalty=0,\n            presence_penalty=0,\n            messages=[\n                {\"role\": \"system\", \"content\": \n                    \"You are a helpful assistant.\"},\n                {\"role\": \"user\", \"content\": prompt}\n            ],\n        )\n        classification = response.choices[0].message.content\n        classification = classification.lower().strip()\n        return classification\n    ```", "```py\n    test_df = test_dataset.to_pandas()\n    test_df.sample(frac=1)\n    test_data = test_df[0:200].copy()\n    ```", "```py\n    test_data[\"gpt_prediction\"] = test_data[\"text\"].apply(\n        lambda x: get_gpt_classification(x))\n    ```", "```py\n    def get_one_word_match(input_text):\n        loc = re.search(\n            r'tech|entertainment|business|sport|politics',\n            input_text).span()\n        return input_text[loc[0]:loc[1]]\n    test_data[\"gpt_prediction\"] = test_data[\"gpt_prediction\"].apply(\n        lambda x: get_one_word_match(x))\n    ```", "```py\n    label_list = [\"tech\", \"business\", \"sport\", \n        \"entertainment\", \"politics\"]\n    test_data[\"gpt_label\"] = test_data[\"gpt_prediction\"].apply(\n        lambda x: label_list.index(x))\n    ```", "```py\n    print(test_data)\n    ```", "```py\n                                                      text  label\n         label_text  \\\n    0    carry on star patsy rowlands dies actress pats...      3\n      entertainment\n    1    sydney to host north v south game sydney will ...      2\n              sport\n    ..                                                 ...    ...\n                ...\n    198  xbox power cable  fire fear  microsoft has sai...      0\n               tech\n    199  prop jones ready for hard graft adam jones say...      2\n              sport\n        gpt_prediction  gpt_label\n    0    entertainment          3\n    1            sport          2\n    ..             ...        ...\n    198           tech          0\n    199          sport          2\n    ```", "```py\n    print(classification_report(test_data[\"label\"],\n            test_data[\"gpt_label\"], target_names=label_list))\n    ```", "```py\n                   precision    recall  f1-score   support\n             tech       0.97      0.80      0.88        41\n         business       0.87      0.89      0.88        44\n            sport       1.00      0.96      0.98        48\n    entertainment       0.88      0.90      0.89        40\n         politics       0.76      0.96      0.85        27\n         accuracy                           0.90       200\n        macro avg       0.90      0.90      0.90       200\n     weighted avg       0.91      0.90      0.90       200\n    ```"]