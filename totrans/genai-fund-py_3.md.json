["```py\nimport pandas as pd\nimport numpy as np\n# Load demo data\ndata = pd.read_csv(\"./Chapter_3/data/en-fr_mini.csv\")\n# Separate English and French lexicons\nEN_TEXT = data.en.to_numpy().tolist()\nFR_TEXT = data.fr.to_numpy().tolist()\n# Arbitrarily cap at 100 characters for demonstration to avoid long training times\ndef demo_limit(vocab, limit=100):\n    return [i[:limit] for i in vocab]\nEN_TEXT = demo_limit(EN_TEXT)\nFR_TEXT = demo_limit(FR_TEXT)\n# Establish the maximum length of a given sequence\nMAX_LEN = 100\n```", "```py\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordPiece\nfrom tokenizers.trainers import WordPieceTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\ndef train_tokenizer(texts):\n    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n    tokenizer.pre_tokenizer = Whitespace()\n    trainer = WordPieceTrainer(\n        vocab_size=5000,\n        special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \n            \"<sos>\", \"<eos>\"],\n    )\n    tokenizer.train_from_iterator(texts, trainer)\n    return tokenizer\nen_tokenizer = train_tokenizer(EN_TEXT)\nfr_tokenizer = train_tokenizer(FR_TEXT)\n```", "```py\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\ndef tensorize_data(text_data, tokenizer):\n    numericalized_data = [\n        torch.tensor(tokenizer.encode(text).ids) for text in text_data\n    ]\n    padded_data = pad_sequence(numericalized_data,\n        batch_first=True)\n    return padded_data\nsrc_tensor = tensorize_data(EN_TEXT, en_tokenizer)\ntgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)\n```", "```py\nfrom torch.utils.data import Dataset, DataLoader\nclass TextDataset(Dataset):\n    def __init__(self, src_data, tgt_data):\n        self.src_data = src_data\n        self.tgt_data = tgt_data\n    def __len__(self):\n        return len(self.src_data)\n    def __getitem__(self, idx):\n        return self.src_data[idx], self.tgt_data[idx]\ndataset = TextDataset(src_tensor, tgt_tensor)\n```", "```py\nimport torch.nn as nn\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab_size):\n        super(Embeddings, self).__init__()\n        self.embed = nn.Embedding(vocab_size, d_model)\n    def forward(self, x):\n        return self.embed(x)\n```", "```py\nimport math\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1,\n                 max_len=MAX_LEN\n    ):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0.0, max_len).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0.0, d_model, 2) * - \\\n                (math.log(10000.0) / d_model)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer(\"pe\", pe)\n    def forward(self, x):\n        x = x + self.pe[:, : x.size(1)]\n        return self.dropout(x)\n```", "```py\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, d_model, nhead):\n        super(MultiHeadSelfAttention, self).__init__()\n        self.attention = nn.MultiheadAttention(d_model, nhead)\n    def forward(self, x):\n        return self.attention(x, x, x)\n```", "```py\nclass FeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(FeedForward, self).__init__()\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.dropout = nn.Dropout(0.1)\n        self.linear2 = nn.Linear(d_ff, d_model)\n    def forward(self, x):\n        return self.linear2(self.dropout(torch.relu(self.linear1(x))))\n```", "```py\nclass EncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, d_ff):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadSelfAttention(d_model, nhead)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n    def forward(self, x):\n        x = x.transpose(0, 1)\n        attn_output, _ = self.self_attn(x)\n        x = x + self.dropout(attn_output)\n        x = self.norm1(x)\n        ff_output = self.feed_forward(x)\n        x = x + self.dropout(ff_output)\n        return self.norm2(x).transpose(0, 1)\n```", "```py\nclass Encoder(nn.Module):\n    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):\n        super(Encoder, self).__init__()\n        self.embedding = Embeddings(d_model, vocab_size)\n        self.pos_encoding = PositionalEncoding(d_model)\n        self.encoder_layers = nn.ModuleList(\n            [EncoderLayer(d_model, nhead, d_ff) for _ in range(\n                num_layers)]\n        )\n        self.feed_forward = FeedForward(d_model, d_ff)\n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        for layer in self.encoder_layers:\n            x = layer(x)\n        return x\n```", "```py\nclass DecoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, d_ff):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadSelfAttention(d_model, nhead)\n        self.cross_attn = nn.MultiheadAttention(d_model, nhead)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n    def forward(self, x, memory):\n        x = x.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        attn_output, _ = self.self_attn(x)\n        x = x + self.dropout(attn_output)\n        x = self.norm1(x)\n        attn_output, _ = self.cross_attn(x, memory, memory)\n        x = x + self.dropout(attn_output)\n        x = self.norm2(x)\n        ff_output = self.feed_forward(x)\n        x = x + self.dropout(ff_output)\n        return self.norm3(x).transpose(0, 1)\n```", "```py\nclass Decoder(nn.Module):\n    def __init__(self, d_model, nhead, d_ff, num_layers, vocab_size):\n        super(Decoder, self).__init__()\n        self.embedding = Embeddings(d_model, vocab_size)\n        self.pos_encoding = PositionalEncoding(d_model)\n        self.decoder_layers = nn.ModuleList(\n            [DecoderLayer(d_model, nhead, d_ff) for _ in range(\n                num_layers)]\n        )\n        self.linear = nn.Linear(d_model, vocab_size)\n        self.softmax = nn.Softmax(dim=2)\n    def forward(self, x, memory):\n        x = self.embedding(x)\n        x = self.pos_encoding(x)\n        for layer in self.decoder_layers:\n            x = layer(x, memory)\n        x = self.linear(x)\n        return self.softmax(x)\n```", "```py\nclass Transformer(nn.Module):\n    def __init__(\n        self,\n        d_model,\n        nhead,\n        d_ff,\n        num_encoder_layers,\n        num_decoder_layers,\n        src_vocab_size,\n        tgt_vocab_size,\n    ):\n        super(Transformer, self).__init__()\n        self.encoder = Encoder(d_model, nhead, d_ff, \\\n            num_encoder_layers, src_vocab_size)\n        self.decoder = Decoder(d_model, nhead, d_ff, \\\n            num_decoder_layers, tgt_vocab_size)\n    def forward(self, src, tgt):\n        memory = self.encoder(src)\n        output = self.decoder(tgt, memory)\n        return output\n```", "```py\ndef train(model, loss_fn, optimizer, NUM_EPOCHS=10):\n    for epoch in range(NUM_EPOCHS):\n        model.train()\n        total_loss = 0\n        for batch in batch_iterator:\n            src, tgt = batch\n            optimizer.zero_grad()\n            output = model(src, tgt)\n            loss = loss_fn(output.view(-1, TGT_VOCAB_SIZE),\n                tgt.view(-1))\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        print(f\"Epoch {epoch}, \n            Loss {total_loss / len(batch_iterator)}\")\n```", "```py\ndef translate(model, src_text, src_tokenizer,\n              tgt_tokenizer, max_target_length=50\n):\n    model.eval()\n    src_tokens = src_tokenizer.encode(src_text).ids\n    src_tensor = torch.LongTensor(src_tokens).unsqueeze(0)\n    tgt_sos_idx = tgt_tokenizer.token_to_id(\"<sos>\")\n    tgt_eos_idx = tgt_tokenizer.token_to_id(\"<eos>\")\n    tgt_tensor = torch.LongTensor([tgt_sos_idx]).unsqueeze(0)\n    for i in range(max_target_length):\n        with torch.no_grad():\n            output = model(src_tensor, tgt_tensor)\n        predicted_token_idx = output.argmax(dim=2)[0, -1].item()\n        if predicted_token_idx == tgt_eos_idx:\n            break\n        tgt_tensor = torch.cat((tgt_tensor,\n            torch.LongTensor([[predicted_token_idx]])),\n            dim=1)\n    translated_token_ids = tgt_tensor[0, 1:].tolist()\n    translated_text = tgt_tokenizer.decode(translated_token_ids)\n    return translated_text\n```", "```py\nif __name__ == \"__main__\":\n    NUM_ENCODER_LAYERS = 2\n    NUM_DECODER_LAYERS = 2\n    DROPOUT_RATE = 0.1\n    EMBEDDING_DIM = 512\n    NHEAD = 8\n    FFN_HID_DIM = 2048\n    BATCH_SIZE = 31\n    LEARNING_RATE = 0.001\n    en_tokenizer = train_tokenizer(EN_TEXT)\n    fr_tokenizer = train_tokenizer(FR_TEXT)\n    SRC_VOCAB_SIZE = len(en_tokenizer.get_vocab())\n    TGT_VOCAB_SIZE = len(fr_tokenizer.get_vocab())\n    src_tensor = tensorize_data(EN_TEXT, en_tokenizer)\n    tgt_tensor = tensorize_data(FR_TEXT, fr_tokenizer)\n    dataset = TextDataset(src_tensor, tgt_tensor)\n    model = Transformer(\n        EMBEDDING_DIM,\n        NHEAD,\n        FFN_HID_DIM,\n        NUM_ENCODER_LAYERS,\n        NUM_DECODER_LAYERS,\n        SRC_VOCAB_SIZE,\n        TGT_VOCAB_SIZE,\n    )\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    batch_iterator = DataLoader(\n        dataset, batch_size=BATCH_SIZE,\n        shuffle=True, drop_last=True\n    )\n    train(model, loss_fn, optimizer, NUM_EPOCHS=10)\n    src_text = \"hello, how are you?\"\n    translated_text = translate(\n        model, src_text, en_tokenizer, fr_tokenizer)\n    print(translated_text)\n```"]