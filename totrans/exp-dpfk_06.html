<html><head></head><body>
		<div><h1 id="_idParaDest-107" class="chapter-number"><a id="_idTextAnchor107"/>6</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor108"/>Training a Deepfake Model</h1>
			<p>Training a deepfake model is the most important part of creating a deepfake. It is where the AI actually <a id="_idIndexMarker264"/>learns about the faces from your data and where the most interesting neural network operations take place.</p>
			<p>In this chapter, we’ll look into the training code and the code that actually creates the AI models. We’ll look at the submodules of the neural network and how they’re put together to create a complete neural network. Then we’ll go over everything needed to train the network and end up with a model ready to swap two faces.</p>
			<p>We’ll cover the following topics in this chapter:</p>
			<ul>
				<li>Understanding convolutional layers</li>
				<li>Getting hands-on with AI</li>
				<li>Exploring the training code</li>
			</ul>
			<p>By the end of this chapter, we’ll have designed our neural networks and built a training pipeline capable of teaching them to swap faces.</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor109"/>Technical requirements</h1>
			<p>To run any of the code in this chapter, we recommend downloading our official repository at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a> and following the instructions for setting up an Anaconda environment with all of the required libraries.</p>
			<p>In order to train, you must have two sets of extracted faces. You’ll feed both sets into the model and it will learn both faces separately. It’s important that you get sufficient data for both faces and that there be a good variety. If you’re in doubt, please check <a href="B17535_03.xhtml#_idTextAnchor054"><em class="italic">Chapter 3</em></a>, <em class="italic">Mastering Data</em>, for advice on getting the best data.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor110"/>Understanding convolutional layers</h1>
			<p>In this chapter, we’ll finally get into the meat of the neural networks behind deepfakes. A big part of how networks <a id="_idIndexMarker265"/>such as these work is a technique called convolutional layers. These layers are extremely important in effectively working with image data and form an important cornerstone of most neural networks.</p>
			<p>A <strong class="bold">convolution</strong> is an operation <a id="_idIndexMarker266"/>that changes the shape of an object. In the case of neural networks, we use <strong class="bold">convolutional layers</strong>, which iterate a convolution over a matrix and create a new (generally smaller) output matrix. Convolutions are a way to reduce an image in size while simultaneously searching for patterns. The more convolutional layers you stack, the more complicated the patterns that can be encoded from the original image.</p>
			<div><div><img src="img/B17535_06_001.jpg" alt="Figure 6.1 – An example of a convolution downscaling a full image"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – An example of a convolution downscaling a full image</p>
			<p>There are several details that define a convolutional layer. The first is dimensionality. In our case, we’re using 2D convolutions, which work in 2D space. This means that the convolution works on the x and y axes for each of the channels.  This means for the first convolution, each color channel is processed separately.</p>
			<p>Next is the <strong class="bold">kernel</strong>, which defines <a id="_idIndexMarker267"/>how big an area each convolution takes into account. The amount of kernels going across affects the output as well.  For example, if you had a matrix of 3x9 and kernel size of 3x3, you’d get a 1x3 matrix output.</p>
			<div><div><img src="img/B17535_06_002.jpg" alt="Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into a 1x3 matrix output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into a 1x3 matrix output</p>
			<p>Next is <strong class="bold">stride</strong> which defines <a id="_idIndexMarker268"/>how big a step each iteration of the convolution takes as it travels the matrix. A stride of 2, for example, would make our 3x3 kernel overlap by a single <strong class="bold">entry</strong> of the matrix. Stride is duplicated in every dimension, so if you extended the example input matrix to the left or right, you’d also get overlap in that direction.</p>
			<div><div><img src="img/B17535_06_003.jpg" alt="Figure 6.3 – An example of a stride smaller than the kernel size causing the elements in between to be shared on the output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – An example of a stride smaller than the kernel size causing the elements in between to be shared on the output</p>
			<p>Last is <code>padding_mode</code> you can specify different types of padding, such as reflect, which will make the padding equal to the entry that it mirrors along the padding axis like a mirror at the edge of the input matrix, reflecting each entry back.</p>
			<div><div><img src="img/B17535_06_004.jpg" alt="Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into a 1x3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into a 1x3</p>
			<p>We stack multiple convolutional layers because we’re looking for bigger patterns. To find all the appropriate patterns, we increase the depth of the convolutional layers as we add them to <a id="_idIndexMarker270"/>the tower. We start with a convolution 128 kernels deep, then double them to 256, 512, and finally 1,024. Each layer also has a kernel size of 5, a stride of 2, and a padding of 2. This effectively shrinks the width and height by half of each layer. So, the first layer takes in a 3x64x64 image and outputs a 128x32x32 matrix. The next layers turn that to 256x16x16, then 512x8x8, and finally 1024x4x4.</p>
			<p>Next, we’ll finally get into the code at the heart of deepfakes – the neural network itself.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It can be confusing to track how a convolution layer will change a matrix’s size. The equation to calculate the output matrix’s size is actually quite simple but non-intuitive: <code>(input_size+2*padding-stride+1)/2</code>. If you have square matrices, this calculation will match for either dimension, but if you have a non-square matrix, you’ll have to calculate this for both dimensions separately.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Getting hands-on with AI</h1>
			<p>The first code we’ll examine here is the actual model itself. This code defines the neural network <a id="_idIndexMarker271"/>and how it’s structured, as well as how it’s called. All of this is stored in the <code>lib/models.py</code> library file.</p>
			<p>First, we load any libraries we’re using:</p>
			<pre class="source-code">
import torch
from torch import nn</pre>
			<p>In this case, we only import PyTorch and its <code>nn</code> submodule. This is because we only include the model code in this file and any other libraries will be called in the file that uses those functions.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Defining our upscaler</h2>
			<p>One of the most important parts of our model is the upscaling layers. Because this is used multiple <a id="_idIndexMarker272"/>times in both the encoder and decoder, we’ve broken <a id="_idIndexMarker273"/>it out into its own definition, and we’ll cover that here:</p>
			<ol>
				<li>First, we define our class:<pre class="source-code">
class Upscale(nn.Module):
  """ Upscale block to double the width/height from depth. """
  def __init__(self, size):
    super().__init__()</pre></li>
			</ol>
			<p>Note that, like our encoder, this inherits from <code>nn.Module</code>. This means we have to make a call to the initialization from the parent class in this class’s initialization. This gives our class a lot of useful abilities from PyTorch, including the backpropagation algorithms that make neural networks work.</p>
			<ol>
				<li value="2">Next, we define our layers:<pre class="source-code">
self.conv = nn.Conv2d(size * 2, size * 2 * 2, kernel_size=3,
  padding="same")
self.shuffle = nn.PixelShuffle(2)</pre></li>
			</ol>
			<p>The upscaler only uses two layers. The first is a convolutional layer, which has an input size double that of the initialization function. We do this because the upscale class <a id="_idIndexMarker274"/>takes in an output size, and since it <a id="_idIndexMarker275"/>increases the width and height by halving the depth, it needs the input depth to be twice the output. In this case, padding is <code>same</code> instead of a number. This is a special way to make the <code>nn.Conv2d</code> layer output a matrix with the same width and height as the input. For a kernel size of <code>3</code> this creates a padding of <code>1</code>.</p>
			<p>The <code>nn.PixelShuffle</code> is a layer that takes an input matrix and, by moving the entries around, takes depth layers and converts them into width and height. Together with the earlier convolutional layer, this effectively “upscales” the image in a learnable and efficient way. We pass <code>2</code> since we want it to double the width and height. Other numbers can be used for different scaling factors but would require adjustments of the convolutional layers and the models that call the class.</p>
			<ol>
				<li value="3">Finally, we have our forward function:<pre class="source-code">
def forward(self, x):
  """ Upscale forward pass """
  x = self.conv(x)
  x = self.shuffle(x)
  return x</pre></li>
			</ol>
			<p>This forward function simply takes the input, then runs it through the convolutional and <code>PixelShuffle</code> layers and returns the result.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>Creating the encoder</h2>
			<p>Let’s <a id="_idIndexMarker276"/>create the encoder next:</p>
			<ol>
				<li>First, we declare <a id="_idIndexMarker277"/>the encoder class:<pre class="source-code">
class OriginalEncoder(nn.Module):
    """ Face swapping encoder
    Shared to create encodings for both the faces
    """</pre></li>
			</ol>
			<p>Here we’ve defined and provided a short comment on the encoder. We declare it as a child class of the <code>nn.Module</code> class. This gives our class a lot of useful abilities from PyTorch, including the backpropagation algorithms that make neural networks work.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">In this book, we’ve included only the basic, original model. This was the first deepfake model and has been surpassed in pretty much every way, but it’s easy to understand, so it works well for this book. If you’d like to explore other models, we recommend that you check <a id="_idIndexMarker278"/>out Faceswap at <a href="https://Faceswap.dev">https://Faceswap.dev</a>, which is constantly updated with the newest models.</p>
			<ol>
				<li value="2">Next, we’ll define the initialization function:<pre class="source-code">
def __init__(self):
  super().__init__()</pre></li>
			</ol>
			<p>This function is the one that actually builds the neural network's layers. Each layer is defined in this function so that PyTorch can automatically handle the details of the weights. We also call the <code>__init__</code> function from the parent class to prepare any variables or functionality that is necessary.</p>
			<ol>
				<li value="3">Next, we’ll start defining our activation function:<pre class="source-code">
self.activation = nn.LeakyReLU(.1)</pre></li>
			</ol>
			<p>We use <code>LeakyReLU</code> or <strong class="bold">Leaky Rectified Linear Unit</strong> as an <strong class="bold">activation function</strong> for our <a id="_idIndexMarker279"/>model. An activation function takes <a id="_idIndexMarker280"/>the output of a layer and brings it into a standardized range.</p>
			<p>What a Leaky <a id="_idIndexMarker281"/>Rectified Linear Unit <em class="italic">is</em> is pretty easy to understand <a id="_idIndexMarker282"/>if you break down the words from last to first. <em class="italic">Unit</em>, in this case, means the same as function; it takes an input and provides an output. <em class="italic">Linear</em> means a line, one that doesn’t change directions as it moves; in this case, it’s a 1:1, where the output matches the input (an input of 1 leads to an output of 1, an input of 2 leads to an output of 2, and so on). <em class="italic">Rectified</em> just means it has been made positive, so negative numbers become 0. <em class="italic">Leaky</em> actually makes that last sentence a bit of a lie. It’s been found that neural networks really don’t work very well when the entire negative space becomes 0. So leaky here means that negative numbers get scaled to a range barely below 0.</p>
			<p>We use 0.1 here so that any numbers below 0 get multiplied by 0.1, scaling them smaller by 10 times. Many different values can be used here, and various projects make different decisions. Standard values typically sit somewhere in the range of 0.005 to 0.2.</p>
			<ol>
				<li value="4">Next, we’ll define our convolution tower:<pre class="source-code">
self.conv_tower = nn.Sequential(
  nn.Conv2d(3, 128, kernel_size=5, stride=2, padding=2),
  self.activation,
  nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),
  self.activation,
  nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),
  self.activation,
  nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),
  self.activation)</pre></li>
			</ol>
			<p>The convolution tower is exactly what it sounds like, a stack of convolution layers. After each <a id="_idIndexMarker283"/>of the convolution layers, we include an activation <a id="_idIndexMarker284"/>function. This is helpful to ensure that the model stays on track and makes the convolutions more effective. The activation is identical in each case and doesn’t do any “learning” but just works like a function, so we don’t need to make separate layers for each one and can use the same activation function we already initialized.</p>
			<p>We use <code>nn.Sequential</code> here to combine the stack of layers into a single layer. The sequential layer is actually a very powerful tool in PyTorch, allowing you to make simple neural networks without having to write a whole class for the model. We use it here to combine all the convolutional layers since the input in one end goes all the way through in every case. This makes it easier to use later in our <code>forward</code> function. But a sequential model runs each of its constituent layers in sequence and can’t handle conditional <code>if</code> statements or functions that aren’t written for PyTorch.</p>
			<ol>
				<li value="5">Next, we’ll define a <code>flatten</code> layer:<pre class="source-code">
self.flatten = nn.Flatten()</pre></li>
			</ol>
			<p>A <code>flatten</code> layer does exactly what it sounds like; it flattens a previous layer to just one axis. This is used in the forward pass to turn the 1024x4x4 matrix that comes out of the convolution tower into a 4,096-element wide single-dimension layer.</p>
			<ol>
				<li value="6">Next, we’ll define our dense layers:<pre class="source-code">
self.dense1 = nn.Linear(4 * 4 * 1024, 1024)
self.dense2 = nn.Linear(1024, 4 * 4 * 1024)</pre></li>
			</ol>
			<p>Dense layers are called dense because they’re fully connected. Unlike convolutional layers, every single entry in the matrix is connected to every single input of the previous layer. Dense layers were the original neural network layer types and are very powerful, but they’re also very memory intensive. In fact, these two layers account for most of the memory of the entire deepfake model!</p>
			<p>We generate <a id="_idIndexMarker285"/>two separate dense layers. The first layer takes in <a id="_idIndexMarker286"/>an input of 4,096 entries wide and outputs a 1,024-wide output. This <a id="_idIndexMarker287"/>is the <strong class="bold">bottleneck</strong> of the model: the part of the model that has the least amount of data, which then needs to be rebuilt. The second layer takes a 1024 one-dimensional matrix input and outputs a matrix with one dimension of 4,096. This is the first layer that starts rebuilding a face from encoded details.</p>
			<ol>
				<li value="7">The last initialization step is to define our first upscale layer:<pre class="source-code">
self.upscale = Upscale(512)</pre></li>
			</ol>
			<p>This layer is our first upscaler. This layer will take a 1024x4x4 matrix and upscale it back to a 512x8x8 matrix. All other upscalers will exist in the decoder. This one was originally put in the encoder, probably as a memory-saving attempt since the first upscale was unlikely to need to match a particular person at all, as it only had the most general of face patterns.</p>
			<p>The upscale layer is given an output size of 512. This means that the output will be 512 deep but does not define the width or height. These come naturally from the input, with each call to upscale doubling the width and height.</p>
			<ol>
				<li value="8">Next, we’ll go over our forward function:<pre class="source-code">
def forward(self, x):
  """ Encoder forward pass """</pre></li>
			</ol>
			<p>The forward function is what actually applies the network to a given matrix. This is used both for training and for inference of the trained model.</p>
			<ol>
				<li value="9">First, we get the batch size:<pre class="source-code">
batch_size = x.shape[0]</pre></li>
			</ol>
			<p>We need the <a id="_idIndexMarker288"/>batch size that we started with later in the process, so we <a id="_idIndexMarker289"/>save it here immediately.</p>
			<ol>
				<li value="10">Finally, we run the data through the whole model:<pre class="source-code">
x = self.conv_tower(x)
x = self.flatten(x)
x = self.dense1(x)
x = self.dense2(x)
x = torch.reshape(x, [batch_size, 1024, 4, 4])
x = self.upscale(x)
x = self.activation(x)
return x</pre></li>
			</ol>
			<p>In this code, we run the input matrix through each layer in turn. The only new surprise here is the <code>torch.reshape</code> call after the final <code>dense</code>, which is effectively the opposite of the <code>flatten</code> call from right before the first <code>dense</code>. It takes the 4096-wide matrix and changes the shape so that it’s a 1024x4x4 matrix again.</p>
			<p>We then run the data through the upscale layer and then the activation function before we return the result.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>Building the decoders</h2>
			<p>The decoder <a id="_idIndexMarker290"/>is responsible for taking the encoded face data <a id="_idIndexMarker291"/>and re-creating a face as accurately as it can. To do this, it will iterate over thousands or even millions of faces to get better at turning encodings into faces. At the same time, the encoder will be getting better at encoding faces.</p>
			<p>We used the plural <em class="italic">decoders</em> here, but this code only actually defines a single decoder. That’s because the training code creates two copies of this decoder class.</p>
			<ol>
				<li>First, we define and initialize our model:<pre class="source-code">
class OriginalDecoder(nn.Module):
  """ Face swapping decoder
  An instance for each face to decode the shared encodings.
  """
  def __init__(self):
    super().__init__()</pre></li>
			</ol>
			<p>This code, just like the encoder and upscaler, is an instance of <code>nn.Module</code> and needs an initialization function that also calls the parent’s initializer.</p>
			<ol>
				<li value="2">Next, we define our activation function:<pre class="source-code">
self.activation = nn.LeakyReLU(.1)</pre></li>
			</ol>
			<p>Just like our encoder’s activation, we use <code>LeakeReLu</code> with a negative scaling of <code>0.1</code>.</p>
			<ol>
				<li value="3">Next, we define our upscaling tower:<pre class="source-code">
self.upscale_tower = nn.Sequential(Upscale(256),
  self.activation,
  Upscale(128),
  self.activation,
  Upscale(64),
  self.activation)</pre></li>
			</ol>
			<p>The upscale tower is much like the convolution tower of the encoder but uses upscale blocks instead of shrinking convolutions. Because there was one upscaler in the encoder, we actually have one fewer upscales in this decoder. Just like the convolution tower, there are also activation functions after each upscale to keep the range trending positive.</p>
			<ol>
				<li value="4">Next, we define our output layer:<pre class="source-code">
self.output = nn.Conv2d(64, 3, 5, padding="same")</pre></li>
			</ol>
			<p>The output layer is special. While each of the previous layers’ outputs was half the depth <a id="_idIndexMarker292"/>of the previous layer’s, this one takes <a id="_idIndexMarker293"/>the 64-deep output from the convolution layer and converts it back to a three-channel image. There is nothing special about the three-channel dimension, but due to how the training process works, each is correlated to one of the color channels of the training image.</p>
			<ol>
				<li value="5">Now, we define the forward function of the decoder:<pre class="source-code">
def forward(self, x):
  """ Decoder forward pass """
  x = self.upscale_tower(x)
  x = self.output(x)
  x = torch.sigmoid(x)
  return x</pre></li>
			</ol>
			<p>This forward function is familiar, being very similar to those in the encoder and the upscale layer. The major difference here is that after we pass the input through the upscale tower and the output layer, we use a <code>torch.sigmoid</code> layer. This is another type of activation layer.</p>
			<p>Sigmoid works differently from LeakyReLu in that it is not linear. Instead, it computes the logistic sigmoid of the input. This is an s-shaped output where negative inputs approach <code>0</code>, and positive inputs approach <code>1</code> with a <code>0</code> input coming out as <code>0.5</code>. The precise equation is <code>1/(1*e^-input)</code>. This basically puts the results between <code>0</code> and <code>1</code> with extremes being more compressed, which matches how <a id="_idIndexMarker294"/>the multiplication of high numbers leads to higher <a id="_idIndexMarker295"/>numbers faster. This effectively turns the output of the model into a range of <code>0-1</code>, which we can easily turn into an image.</p>
			<div><div><img src="img/B17535_06_005.jpg" alt="Figure 6.5 – An example of the sigmoid curve"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – An example of the sigmoid curve</p>
			<p>Next, we’ll examine the training code.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>Exploring the training code</h1>
			<p>Now that we have defined our models, we can go ahead with the process of training a neural <a id="_idIndexMarker296"/>network on our data. This is the part where we actually have AI learn the different faces so that it can later swap between them.</p>
			<ol>
				<li>First, we import our libraries:<pre class="source-code">
from glob import glob
import os
import random
from argparse import ArgumentParser
import cv2
import numpy as np
from tqdm import tqdm
import torch
from lib.models import OriginalEncoder, OriginalDecoder</pre></li>
			</ol>
			<p>Like all Python programs, we import our libraries. We also import our encoder and decoders from our model file. This loads the AI model code from earlier in this chapter and lets us use those to define our models in this code. Python really makes it easy to import code we’ve already written, as every Python file can be called directly or imported into another file.</p>
			<p>Note that Python uses a strange syntax for folder paths. Python treats this syntax exactly the same as a module, so you use a period to tell it to look in a folder and then give it the file you want. In this case, we’re pulling the <code>OriginalEncoder</code> and <code>OriginalDecoder</code> classes from the <code>models.py</code> file located in the <code>lib</code> folder.</p>
			<ol>
				<li value="2">Next, we define our arguments and call our main function:<pre class="source-code">
If __name__ == "__main__":
  # Train a deepfake model from two folders of face images.
  #    Example CLI:
  #    ------------
  #    python c6-train.py "C:/media/face1"
                          "C:/media/face2"</pre></li>
				<li>Next, we <a id="_idIndexMarker297"/>define our arguments:<pre class="source-code">
parser = ArgumentParser()
parser.add_argument("patha",
  help="folder of images of face a")
parser.add_argument("pathb",
  help="folder of images of face b")
parser.add_argument("--cpu",
  action="store_true",
  help="Force CPU usage")
parser.add_argument("--batchsize",
  type=int, default=16,
  help="Number of images to include in a batch")
parser.add_argument("--iterations", type=int, default=100000,
  help="Number of iterations to process before stopping")
parser.add_argument("--learning-rate",
  type=float, default=.000001,
  help="Number of images to include in a batch")
parser.add_argument("--save_freq",
  type=int, default=1000,
  help="Number of iterations to save between")
parser.add_argument("--out_path",
  default="model/",
  help="folder to place models")</pre></li>
			</ol>
			<p>Here we define our arguments. These give us the ability to change our settings, files, or details without having to modify the source code directly.</p>
			<ol>
				<li value="4">Then, we parse <a id="_idIndexMarker298"/>all the arguments and call our main function:<pre class="source-code">
opt = parser.parse_args()
main(opt)</pre></li>
			</ol>
			<p>We parse our arguments and pass them into our main function. The main function will handle all the training processes, and we need to give it all the arguments.</p>
			<ol>
				<li value="5">Next, we start our main function:<pre class="source-code">
def main(opt):
  """ Train a deepfake model from two folders of face images.
  """
  device = "cuda" if torch.cuda.is_available() and not opt.cpu else "cpu"
os.makedirs(opt.out_path, exist_ok=True)</pre></li>
			</ol>
			<p>Here we start our main function and check whether we’re supposed to use <code>cuda</code>. If so, we <a id="_idIndexMarker299"/>enable <code>cuda</code> so that we can use the <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) to accelerate training. Then we create our export folder if that isn’t already created. This is where we’ll save copies of our models and any training previews we generate later.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">While it’s <a id="_idIndexMarker300"/>possible to run the other parts of the process without a GPU, training is far more intensive, and running <a id="_idIndexMarker301"/>a training session on a <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) will take a very large amount of time. Because of this, it’s recommended that at least this part be run with a GPU. If you don’t have one locally, you can rent one at any number of online services.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Creating our models</h2>
			<p>Here we’ll <a id="_idIndexMarker302"/>create our neural models and fill them with weights:</p>
			<ol>
				<li>First, we’ll create instances of our previous models:<pre class="source-code">
encoder = OriginalEncoder()
decodera = OriginalDecoder()
decoderb = OriginalDecoder()</pre></li>
			</ol>
			<p>In this chunk of code, we create our AI models. We create one instance of the encoder and two separate decoders. We call them <code>a</code> and <code>b</code> here, but that’s entirely an arbitrary choice with no effect on the results. By default, we assume that you want to put the second face onto the first so in the case of this code, we’d be putting the face from <code>b</code> onto the frame from <code>a</code>.</p>
			<ol>
				<li value="2">Next, we load any previously saved models:<pre class="source-code">
if os.path.exists(os.path.join(opt.out_path,"encoder.pth")):
  encoder.load_state_dict( torch.load(
    os.path.join(opt.out_path, "encoder.pth")).state_dict())
  decodera.load_state_dict( torch.load(
    os.path.join(opt.out_path,"decodera.pth")).state_dict())
  decoderb.load_state_dict( torch.load(
    os.path.join(opt.out_path,"decoderb.pth")).state_dict())</pre></li>
			</ol>
			<p>Here we <a id="_idIndexMarker303"/>check whether any models already exist in the given output folder. If they do, we load those model weights into the models we instantiated in the last section. To do this, we have PyTorch load the weights from the disk and then assign the weights to the model’s state dictionary. This lets PyTorch load the weights into the model and get it ready for training.</p>
			<p>If there are no weights, then we skip this step. This means that the models will be initialized with random weights, ready to start a new training session. This lets you get started easily without having to generate any random weights yourself.</p>
			<ol>
				<li value="3">Next, we get a list of the images to train with:<pre class="source-code">
imagesa = glob(os.path.join(opt.patha, "face_aligned_*.png"))
imagesb = glob(os.path.join(opt.pathb, "face_aligned_*.png"))</pre></li>
			</ol>
			<p>This chunk gets a list of all the images from the folders to train. We load only the aligned face images from the folders since we can create the filenames for the other images from the filenames for the aligned images.</p>
			<ol>
				<li value="4">Next, we create the tensors for the images and the masks:<pre class="source-code">
img_tensora = torch.zeros([opt.batchsize, 3, 64, 64])
img_tensorb = torch.zeros([opt.batchsize, 3, 64, 64])
mask_tensora = torch.zeros([opt.batchsize, 1, 64, 64])
mask_tensorb = torch.zeros([opt.batchsize, 1, 64, 64])</pre></li>
			</ol>
			<p>Here we <a id="_idIndexMarker304"/>create the tensors that will hold the images we will use for training. For the image tensors, we create a tensor that is 64x64 pixels wide with 3 channels to handle the red, green, and blue color channels. We also add a <strong class="bold">batch size</strong> dimension to the tensor so we can store that many images at once. A batch is simply how many images we’ll process at the same time. Larger batch sizes help the training process run more efficiently as we’re able to benefit from hardware that can do multiple tasks simultaneously as well as benefit from PyTorch grouping the tasks in the best order.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">Larger batch sizes lead to more efficiencies in the training, so why don’t we set the batch size to 256 or 1024 instead of defaulting to 16? It’s because batch size is not a magic bullet. First, larger batches take more memory as the system must store every item of the batch at the same time. With large models, this can be prohibitive. Additionally, there is a side effect to large batch sizes. It’s the classic “forest for the trees,” meaning that larger batch sizes can help generalize over large sets of data but perform worse at learning specific details. So, picking the ideal batch size can be as important a question as anything else. A good rule of thumb for deepfakes is to keep batch size in double digits with 100+ tending to be too big and &lt;10 to be avoided unless you have specific plans.</p>
			<ol>
				<li value="5">Next, we define and set up our optimizers and loss function:<pre class="source-code">
encoder_optimizer = torch.optim.Adam(
  encoder.parameters(), lr=opt.learning_rate/2)
decodera_optimizer = torch.optim.Adam(
  decodera.parameters(), lr=opt.learning_rate)
decoderb_optimizer = torch.optim.Adam(
  decoderb.parameters(), lr=opt.learning_rate)
loss_function = torch.nn.MSELoss()</pre></li>
			</ol>
			<p>Optimizers are the part of PyTorch responsible for the most important part of training: backpropagation. This process is what changes the weights of the model and <a id="_idIndexMarker305"/>allows AI to “learn” and get better at re-creating the images we’re using for training. It’s responsible for more than just changing the weights but actually calculates how much to change them as well.</p>
			<p>In this case, we’re using the <code>torch.optim.Adam</code> optimizer. This is part of a family of optimizers proven to be very effective and flexible. We use it here because it’s what the original deepfake model used, but it’s still one of the most reliable and useful optimizers even today.</p>
			<p>We pass each <a id="_idIndexMarker306"/>model the <strong class="bold">learning rate</strong> from our options. The learning rate is basically a scaling value of how much the optimizer should change the weights. Higher numbers change the weights more, which can lead to faster training at the cost of difficulty in fine-tuning since the changes being made are large. Lower learning rates can get better accuracy but cause training to take longer by being slower. We cut the learning rate of the encoder by half because we will actually be training it twice as often since it is being used to encode both faces.</p>
			<p>The last thing we do <a id="_idIndexMarker307"/>here is to define our <code>torch.nn.MSEloss</code> provided by PyTorch. This is a <a id="_idIndexMarker308"/>loss called <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). Let’s look at this word by word again.</p>
			<p><em class="italic">Error</em> in mathematics is how far off the function is from a perfectly correct result. In our case, we are re-creating a face, so the loss function will compare the generated face to the original face and count how far off each pixel is from the correct answer. This gives a nice easy number for each pixel. Looking at each pixel is a bit too difficult, so next, our loss will take the average (mean) of all the pixels together. This gives a single number of how far off the AI was as a whole. Finally, that number is squared. This makes large differences stand out even more and <a id="_idIndexMarker309"/>has been shown to help the model reach a good result faster.</p>
			<p>There are other <a id="_idIndexMarker310"/>loss functions such as <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>), which gets rid of the squaring in the MSE, or <strong class="bold">structural similarity</strong>, which uses <a id="_idIndexMarker311"/>the similarity of the structure as a measure. In fact, <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>), which are <a id="_idIndexMarker312"/>a buzzword of the machine learning field, simply replace the static loss of an auto-encoder with another model that provides a trainable loss function and pits the two models against each other in a competition of which model can do their job better.</p>
			<ol>
				<li value="6">Next, we move everything to the GPU if enabled:<pre class="source-code">
if device == "cuda":
  encoder = encoder.cuda()
  decodera = decodera.cuda()
  decoderb = decoderb.cuda()
  img_tensora = img_tensora.cuda()
  img_tensorb = img_tensorb.cuda()
  mask_tensora = mask_tensora.cuda()
  mask_tensorb = mask_tensorb.cuda()</pre></li>
			</ol>
			<p>If <code>cuda</code> was enabled earlier, we need to move the models and the variables to the GPU so we can process them. So here, we check whether <code>cuda</code> was enabled, and if so, we move each of them to the GPU.</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>Looping over the training</h2>
			<p>In order to <a id="_idIndexMarker313"/>train the model, we need to loop over all the data. We call this the training loop.</p>
			<ol>
				<li>First, we create a progress bar and start the training loop:<pre class="source-code">
pbar = tqdm(range(opt.iterations))
for iteration in pbar:</pre></li>
			</ol>
			<p>We use <code>tqdm</code> again for a progress bar. Here we pass a range of how many iterations we want to <code>tqdm</code> so it can update our progress bar automatically and assign the progress bar to a variable. We then start our loop from that variable to provide more information in the progress bar by calling functions that <code>tqdm</code> exposes in the variable.</p>
			<ol>
				<li value="2">Next, we load a random set of images:<pre class="source-code">
images = random.sample(imagesa, opt.batchsize)
for imgnum, imagefile in enumerate(images):
  img = cv2.imread(imagefile)
  img = cv2.resize(image, (64, 64))
  mask = cv2.imread(imagefile.replace("aligned", "mask"), 0)
  mask = cv2.resize(mask, (64, 64))
  if np.random.rand() &gt; .5:
    image = cv2.flip(img, 1)
    mask = cv2.flip(mask, 1)
  img_tensor = torch.tensor(img[...,::-1]/255).permute(2,0,1)
  mask_tensor = torch.where(torch.tensor(mask) &gt; 200, 1, 0)
  if device == "cuda":
    img_tensor = img_tensor.cuda()
    mask_tensor = mask_tensor.cuda()
  img_tensora[imgnum] = img_tensor
  mask_tensora[imgnum] = mask_tensor</pre></li>
			</ol>
			<p>This chunk loads a set of images from the <code>a</code> set for the model to train with. To do this, we get a random sample the same size as our batch size from the list of files we generated earlier.</p>
			<p>Next, we go over <a id="_idIndexMarker314"/>a loop for each of those images. The loop first reads in the face image and resizes it down to 64x64. Then it does the same for the mask image by replacing the <code>"aligned"</code> word in the filename with <code>"mask"</code>, which matches the mask filenames. The mask is also resized to match the training image.</p>
			<p>Next, we randomly get a 50% chance to flip the images horizontally. This is an extremely common way to get more variety out of the dataset. Since faces are generally pretty symmetrical, we can usually flip them. We use a 50% chance here, which gives us an equal chance of the image being flipped or not. Since we have a mask, we have to flip it, too, if we flip the image.</p>
			<p>Next, we convert <a id="_idIndexMarker315"/>both the image and masks from image arrays <a id="_idIndexMarker316"/>into tensors. To do this to the image, we convert from <code>[...,::-1]</code>. This can also be done again to get it back to the BGR order (which we’ll do later). The mask is simpler since we don’t care about color data for it, so we just see if the pixel data is greater than 200; if it is, we put <code>1</code> into the tensor; if not, we put <code>0</code>.</p>
			<p>Next, we check whether <code>cuda</code> is enabled; if it is, we move the tensors we just created to the GPU. This puts everything onto the same device.</p>
			<p>Finally, we move the image into the tensor we’ll be using to train the model. This lets us batch the images together for efficiency.</p>
			<ol>
				<li value="3">Then, we do <a id="_idIndexMarker317"/>the same for the other set of images:<pre class="source-code">
images = random.sample(imagesb, opt.batchsize)
for imgnum, imagefile in enumerate(images):
  img = cv2.imread(imagefile)
  img = cv2.resize(image, (64, 64))
  mask = cv2.imread(imagefile.replace("aligned", "mask"), 0)
  mask = cv2.resize(mask, (64, 64))
  if np.random.rand() &gt; .5:
    image = cv2.flip(img, 1)
    mask = cv2.flip(mask, 1)
  img_tensor = torch.tensor(img[...,::-1]/255).permute(2,0,1)
  mask_tensor = torch.where(torch.tensor(mask) &gt; 200, 1, 0)
  if device == "cuda":
    img_tensor = img_tensor.cuda()
    mask_tensor = mask_tensor.cuda()
  img_tensorb[imgnum] = img_tensor
  mask_tensorb[imgnum] = mask_tensor</pre></li>
			</ol>
			<p>This code is identical to the last code but is repeated for the <code>b</code> set of images. This creates our second set of images ready to train.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>Teaching the network</h2>
			<p>Now it’s <a id="_idIndexMarker318"/>time to actually perform the steps that train the network:</p>
			<ol>
				<li>First, we clear the optimizers:<pre class="source-code">
Encoder_optimizer.zero_grad()
decodera_optimizer.zero_grad()</pre></li>
			</ol>
			<p>PyTorch is very flexible in how it lets you build your models and training process. Because of this, we need to tell PyTorch to clear the <code>a</code> side decoder.</p>
			<ol>
				<li value="2">Then, we pass the images through the encoder and decoder:<pre class="source-code">
Outa = decodera(encoder(img_tensora))</pre></li>
			</ol>
			<p>This chunk sends the image tensors we created earlier through the encoder and then through the decoder and stores the results for comparison. This is actually the AI model, and we give it a tensor of images and get a tensor of images back out.</p>
			<ol>
				<li value="3">Next, we calculate our loss and send it through the optimizers:<pre class="source-code">
Lossa = loss_function(
  outa * mask_tensora, img_tensora * mask_tensora)
lossa.backward()
encoder_optimizer.step()
decodera_optimizer.step()</pre></li>
			</ol>
			<p>This chunk does the rest of the training, calculating the loss and then having the optimizers perform backpropagation on the models to update the weights. We start by passing the output images and the original images to the loss function.</p>
			<p>To apply the masks, we multiply the images by the masks. We do this here instead of before we pass the images to the model because we might not have the best masks, and it’s better to train the neural network on the whole image and apply the mask later.</p>
			<p>Next, we call <code>backward</code> on the loss variable. We can do this because the variable is actually still a tensor, and tensors keep track of all actions that happened <a id="_idIndexMarker319"/>to them while in training mode. This lets the loss be carried back over all the steps back to the original image.</p>
			<p>The last step is to call the <code>step</code> function of our optimizers. This goes back over the model weights, updating them so that the next iteration should be closer to the correct results.</p>
			<ol>
				<li value="4">Next, we do the same thing, but for the <code>b</code> decoder instead:<pre class="source-code">
encoder_optimizer.zero_grad()
decoderb_optimizer.zero_grad()
outb = decoderb(encoder(img_tensorb))
lossb = loss_function(
  outb * mask_tensorb, img_tensorb * mask_tensorb)
lossb.backward()
encoder_optimizer.step()
decoderb_optimizer.step()</pre></li>
			</ol>
			<p>We go through the same process again with the <code>b</code> images and decoder. Remember that we’re using the same encoder for both models, so it actually gets trained a second time along with the <code>b</code> decoder. This is a key part of how deepfakes can swap faces. The two decoders share a single encoder, which eventually gives both the decoders the information to re-create their individual faces.</p>
			<ol>
				<li value="5">Next, we update <a id="_idIndexMarker320"/>the progress bar with information about this iteration’s loss of data:<pre class="source-code">
pbar.set_description(f"A: {lossa.detach().cpu().numpy():.6f} "
  f"B: {lossb.detach().cpu().numpy():.6f}")</pre></li>
			</ol>
			<p>Since the loss function outputs a number for the optimizers, we can also display this number for the user. Sometimes loss is used by deepfakers as an estimate of how finished the model is training. Unfortunately, this cannot actually measure how good a model is at converting one face into another; it only scores how good it is at re-creating the same face it was given. For this reason, it’s an imperfect measure and shouldn’t be relied on. Instead, we recommend that the previews we’ll be generating later be used for this purpose.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor119"/>Saving results</h2>
			<p>Finally, we will <a id="_idIndexMarker321"/>save our results:</p>
			<ol>
				<li>First, we’ll check to see whether we should trigger a save:<pre class="source-code">
if iteration % opt.save_freq == 0:
  with torch.no_grad():
    outa = decodera(encoder(img_tensora[:1]))
    outb = decoderb(encoder(img_tensorb[:1]))
    swapa = decoderb(encoder(img_tensora[:1]))
    swapb = decodera(encoder(img_tensorb[:1]))</pre></li>
			</ol>
			<p>We want to save regularly – an iteration may take less than a second, but a save could take several seconds of writing to disk. Because of this, we don’t want to save every iteration; instead, we want to trigger a save on a regular basis after a set number of iterations. Different computers will run at different speeds, so we let you set the save frequency with an argument.</p>
			<p>One thing we want to save along with the current weights is a preview image so we can get a good idea of how the model is doing at each save state. For this reason, we’ll be <a id="_idIndexMarker322"/>using the neural networks, but we don’t want to train while we’re doing this step. That’s the exact reason that <code>torch</code> has the <code>torch.no_grad</code> context. By calling our model from inside this context, we won’t be training and just getting the results from the network.</p>
			<p>We call each decoder with samples of images from both faces. This lets us compare the re-created faces along with the generated swaps. Since we only want a preview image, we can throw out all but the first image to be used as a sample of the current stage of training.</p>
			<ol>
				<li value="2">Next, we create the <a id="_idTextAnchor120"/>sample image:<pre class="source-code">
example = np.concatenate([
  img_tensora[0].permute(1, 2, 0).detach().cpu().numpy(),
  outa[0].permute(1, 2, 0).detach().cpu().numpy(),
  swapa[0].permute(1, 2, 0).detach().cpu().numpy(),
  img_tensorb[0].permute(1, 2, 0).detach().cpu().numpy(),
  outb[0].permute(1, 2, 0).detach().cpu().numpy(),
  swapb[0].permute(1, 2, 0).detach().cpu().numpy()
  ],axis=1)</pre></li>
			</ol>
			<p>We need to create our sample image from all the parts. To do this, we need to convert all the image tensors into a single image. We use <code>np.concatenate</code> to join them all into a single array along the width axis. To do this, we need to get them all into image order and convert them to NumPy arrays. The first thing we do is drop the batch dimension by selecting the first one. Then we use <code>permute</code> to reorder each tensor, so the channels are last. Then we use <code>detach</code> to remove any gradients from the tensors. We can then use <code>cpu</code> to bring the weights <a id="_idIndexMarker323"/>back onto the CPU. Finally, we use <code>numpy</code> to finish converting them into NumPy arrays.</p>
			<ol>
				<li value="3">Next, we write the preview image:<pre class="source-code">
cv2.imwrite(
  os.path.join(opt.out_path, f"preview_{iteration}.png"),
  example[...,::-1]*255)</pre></li>
			</ol>
			<p>This chunk uses <code>cv2.imwrite</code> from OpenCV to write out the preview image as a PNG file. We put it in the output path and give it a name based on what iteration this is. This lets us save each iteration’s preview together and track the progress of the network over time. To actually write out a usable image, we have to convert the color space back to the BGR that OpenCV expects, and then we multiply by <code>255</code> to get a result that fits into the integer space.</p>
			<ol>
				<li value="4">Next, we save the weights to a file:<pre class="source-code">
torch.save(encoder,
  os.path.join(opt.out_path, "encoder.pth"))
torch.save(decodera,
  os.path.join(opt.out_path, "decodera.pth"))
torch.save(decoderb,
  os.path.join(opt.out_path, "decoderb.pth"))</pre></li>
			</ol>
			<p>Here we save the weights for our encoder and both decoders by calling <code>torch.save</code> with the output path and the filenames we want to use to save the weights. PyTorch automatically saves them to the file in its native <code>pth</code> format.</p>
			<ol>
				<li value="5">Finally, we save again:<pre class="source-code">
torch.save(encoder,
  os.path.join(opt.out_path, "encoder.pth"))
torch.save(decodera,
  os.path.join(opt.out_path, "decodera.pth"))
torch.save(decoderb,
  os.path.join(opt.out_path, "decoderb.pth"))</pre></li>
			</ol>
			<p>For our last step, we repeat the save code. But, actually, this is done outside of the training loop. This is here just in case the training loop ends on an iteration number that <a id="_idIndexMarker324"/>doesn’t trigger the save there. This way, the model is definitely saved at least once, no matter what arguments are chosen by the user.</p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">This may seem obvious to anyone who has spent any time coding, but it’s worth repeating. You should always consider how choices made in the development process might lead to bad or unexpected outcomes and account for those in your design. It’s obviously impossible to consider everything, but sometimes even something as simple as duplicating a save right before exiting, “just in case,” can save someone’s day (or in the case of a very long training session, even more time).</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor121"/>Summary</h1>
			<p>In this chapter, we trained a neural network to swap faces. To do this, we had to explore what convolutional layers are and then build a foundational upscaler layer. Then we built the three networks. We built the encoder, then two decoders. Finally, we trained the model itself, including loading and preparing images, and made sure we saved previews and the final weights.</p>
			<p>First, we built the models of the neural networks that we were going to train to perform the face-swapping process. This was broken down into the upscaler, the shared encoder, and the two decoders. The upscaler is used to increase the size of the image by turning depth into a larger image. The encoder is used to encode the face image down into a smaller encoded space that we then pass to the decoders, which are responsible for re-creating the original image. We also looked at activation layers to understand why they’re helpful.</p>
			<p>Next, we covered the training code. We created instances of the network models, loaded weights into the models, and put them on the GPU if one was available. We explored optimizers and loss functions to understand the roles that they play in the training process. We loaded and processed images so that they were ready to go through the model to assist in training. Then we covered the training itself, including how to get the loss and apply it to the model using the optimizers. Finally, we saved preview images and the model weights themselves so we could load them again.</p>
			<p>In the next chapter, we’ll take our trained model and use it to “convert” a video, swapping the faces.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Exercises</h1>
			<ol>
				<li>Choosing learning rate is not a solved problem. There is no one “right” learning rate. What happens if you make the learning rate 10 times bigger? 10 times smaller? What if you start with a large learning rate and then reduce it after some training?</li>
				<li>We used the same loss function and optimizer as the original code, which was first released back in 2018, but there are a lot of options now that weren’t available then. Try replacing the loss function with others from PyTorch’s extensive collection (<a href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a>). Some of them will work without any change, but some won’t work for our situation at all. Try different ones, or even try combinations of loss functions!</li>
				<li>We defined a model that downscaled from a 64x64 pixel image and re-created that same image. But with some tweaks, this same architecture can instead create a 128x128 or 256x256 pixel image. How would you make changes to the model to do this? Should you increase the number (and size) of layers in the convolutional tower and keep the bottleneck the same, increase the size of the bottleneck but keep the layers the same, or change the convolutional layer’s kernel sizes and strides? It’s even possible to send in a 64x64 pixel image and get out a 128x128 pixel image. All of these techniques have their advantages and drawbacks. Try each out and see how they differ.</li>
				<li>We’re training on just two faces, but you could potentially do more. Try modifying the training code to use three different faces instead of just two. What changes would you need to make? What modifications would you make so that you can train an arbitrary number of faces at once?</li>
				<li>In the years since deepfakes were first released, there have been a lot of different models created. Faceswap has implementations for many newer and more advanced models, but they’re written in Keras for Tensorflow and can’t work in this PyTorch fork without being ported. Check the Faceswap models at the GitHub repo (<a href="https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model">https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model</a>). Compare this model to the one in <code>Original.py</code>, which implements the same model. Now use that to see how <code>dfaker.py</code> differs. The residual layer works by adding an extra convolution layer and an <code>add</code> layer, which just adds two layers together. Can you duplicate the <code>dfaker</code> model in this code base?  What about the others?</li>
			</ol>
		</div>
		<div><div></div>
		</div>
	<div><p>EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></p></div>
</body></html>