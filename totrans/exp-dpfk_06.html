<html><head></head><body>
		<div id="_idContainer080">
			<h1 id="_idParaDest-107" class="chapter-number"><a id="_idTextAnchor107"/>6</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor108"/>Training a Deepfake Model</h1>
			<p>Training a deepfake model is the most important part of creating a deepfake. It is where the AI actually <a id="_idIndexMarker264"/>learns about the faces from your data and where the most interesting neural network operations <span class="No-Break">take place.</span></p>
			<p>In this chapter, we’ll look into the training code and the code that actually creates the AI models. We’ll look at the submodules of the neural network and how they’re put together to create a complete neural network. Then we’ll go over everything needed to train the network and end up with a model ready to swap <span class="No-Break">two faces.</span></p>
			<p>We’ll cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Understanding <span class="No-Break">convolutional layers</span></li>
				<li>Getting hands-on <span class="No-Break">with AI</span></li>
				<li>Exploring the <span class="No-Break">training code</span></li>
			</ul>
			<p>By the end of this chapter, we’ll have designed our neural networks and built a training pipeline capable of teaching them to <span class="No-Break">swap faces.</span></p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor109"/>Technical requirements</h1>
			<p>To run any of the code in this chapter, we recommend downloading our official repository at <a href="https://github.com/PacktPublishing/Exploring-Deepfakes">https://github.com/PacktPublishing/Exploring-Deepfakes</a> and following the instructions for setting up an Anaconda environment with all of the <span class="No-Break">required libraries.</span></p>
			<p>In order to train, you must have two sets of extracted faces. You’ll feed both sets into the model and it will learn both faces separately. It’s important that you get sufficient data for both faces and that there be a good variety. If you’re in doubt, please check <a href="B17535_03.xhtml#_idTextAnchor054"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Mastering Data</em>, for advice on getting the <span class="No-Break">best data.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor110"/>Understanding convolutional layers</h1>
			<p>In this chapter, we’ll finally get into the meat of the neural networks behind deepfakes. A big part of how networks <a id="_idIndexMarker265"/>such as these work is a technique called convolutional layers. These layers are extremely important in effectively working with image data and form an important cornerstone of most <span class="No-Break">neural networks.</span></p>
			<p>A <strong class="bold">convolution</strong> is an operation <a id="_idIndexMarker266"/>that changes the shape of an object. In the case of neural networks, we use <strong class="bold">convolutional layers</strong>, which iterate a convolution over a matrix and create a new (generally smaller) output matrix. Convolutions are a way to reduce an image in size while simultaneously searching for patterns. The more convolutional layers you stack, the more complicated the patterns that can be encoded from the <span class="No-Break">original image.</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B17535_06_001.jpg" alt="Figure 6.1 – An example of a convolution downscaling a full image"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – An example of a convolution downscaling a full image</p>
			<p>There are several details that define a convolutional layer. The first is dimensionality. In our case, we’re using 2D convolutions, which work in 2D space. This means that the convolution works on the x and y axes for each of the channels.  This means for the first convolution, each color channel is <span class="No-Break">processed separately.</span></p>
			<p>Next is the <strong class="bold">kernel</strong>, which defines <a id="_idIndexMarker267"/>how big an area each convolution takes into account. The amount of kernels going across affects the output as well.  For example, if you had a matrix of 3x9 and kernel size of 3x3, you’d get a 1x3 <span class="No-Break">matrix output.</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B17535_06_002.jpg" alt="Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into a 1x3 matrix output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – An example of a 3x3 convolutional process turning a 3x9 into a 1x3 matrix output</p>
			<p>Next is <strong class="bold">stride</strong> which defines <a id="_idIndexMarker268"/>how big a step each iteration of the convolution takes as it travels the matrix. A stride of 2, for example, would make our 3x3 kernel overlap by a single <strong class="bold">entry</strong> of the matrix. Stride is duplicated in every dimension, so if you extended the example input matrix to the left or right, you’d also get overlap in <span class="No-Break">that direction.</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B17535_06_003.jpg" alt="Figure 6.3 – An example of a stride smaller than the kernel size causing the elements in between to be shared on the output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – An example of a stride smaller than the kernel size causing the elements in between to be shared on the output</p>
			<p>Last is <strong class="bold">padding</strong> which is added <a id="_idIndexMarker269"/>to the input and defines how much outside the original image the convolution should pretend is there. If we added a padding of 1 to an input matrix of 1x7, it would act like a 3x9, and we’d still get a 1x3 output. This is useful for controlling the precise size of your output matrix or for ensuring that every pixel gets covered by the center of a kernel. It’s also necessary for any situation where the kernel is larger than the input matrix in any direction – though in practice, this situation is extremely rare. In our example, the empty entries are filled with 0s. However, by using <strong class="source-inline">padding_mode</strong> you can specify different types of padding, such as reflect, which will make the padding equal to the entry that it mirrors along the padding axis like a mirror at the edge of the input matrix, reflecting each <span class="No-Break">entry back.</span></p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B17535_06_004.jpg" alt="Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into a 1x3"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Example of padding a 1x7 into a 3x9 before convolution into a 1x3</p>
			<p>We stack multiple convolutional layers because we’re looking for bigger patterns. To find all the appropriate patterns, we increase the depth of the convolutional layers as we add them to <a id="_idIndexMarker270"/>the tower. We start with a convolution 128 kernels deep, then double them to 256, 512, and finally 1,024. Each layer also has a kernel size of 5, a stride of 2, and a padding of 2. This effectively shrinks the width and height by half of each layer. So, the first layer takes in a 3x64x64 image and outputs a 128x32x32 matrix. The next layers turn that to 256x16x16, then 512x8x8, and <span class="No-Break">finally 1024x4x4.</span></p>
			<p>Next, we’ll finally get into the code at the heart of deepfakes – the neural <span class="No-Break">network itself.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It can be confusing to track how a convolution layer will change a matrix’s size. The equation to calculate the output matrix’s size is actually quite simple but non-intuitive: <strong class="source-inline">(input_size+2*padding-stride+1)/2</strong>. If you have square matrices, this calculation will match for either dimension, but if you have a non-square matrix, you’ll have to calculate this for both <span class="No-Break">dimensions separately.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Getting hands-on with AI</h1>
			<p>The first code we’ll examine here is the actual model itself. This code defines the neural network <a id="_idIndexMarker271"/>and how it’s structured, as well as how it’s called. All of this is stored in the <strong class="source-inline">lib/models.py</strong> <span class="No-Break">library file.</span></p>
			<p>First, we load any libraries <span class="No-Break">we’re using:</span></p>
			<pre class="source-code">
import torch
from torch import nn</pre>
			<p>In this case, we only import PyTorch and its <strong class="source-inline">nn</strong> submodule. This is because we only include the model code in this file and any other libraries will be called in the file that uses <span class="No-Break">those functions.</span></p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor112"/>Defining our upscaler</h2>
			<p>One of the most important parts of our model is the upscaling layers. Because this is used multiple <a id="_idIndexMarker272"/>times in both the encoder and decoder, we’ve broken <a id="_idIndexMarker273"/>it out into its own definition, and we’ll cover <span class="No-Break">that here:</span></p>
			<ol>
				<li>First, we define <span class="No-Break">our class:</span><pre class="source-code">
class Upscale(nn.Module):
  """ Upscale block to double the width/height from depth. """
  def __init__(self, size):
    super().__init__()</pre></li>
			</ol>
			<p>Note that, like our encoder, this inherits from <strong class="source-inline">nn.Module</strong>. This means we have to make a call to the initialization from the parent class in this class’s initialization. This gives our class a lot of useful abilities from PyTorch, including the backpropagation algorithms that make neural <span class="No-Break">networks work.</span></p>
			<ol>
				<li value="2">Next, we define <span class="No-Break">our layers:</span><pre class="source-code">
self.conv = nn.Conv2d(size * 2, size * 2 * 2, kernel_size=3,
  padding="same")
self.shuffle = nn.PixelShuffle(2)</pre></li>
			</ol>
			<p>The upscaler only uses two layers. The first is a convolutional layer, which has an input size double that of the initialization function. We do this because the upscale class <a id="_idIndexMarker274"/>takes in an output size, and since it <a id="_idIndexMarker275"/>increases the width and height by halving the depth, it needs the input depth to be twice the output. In this case, padding is <strong class="source-inline">same</strong> instead of a number. This is a special way to make the <strong class="source-inline">nn.Conv2d</strong> layer output a matrix with the same width and height as the input. For a kernel size of <strong class="source-inline">3</strong> this creates a padding <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">nn.PixelShuffle</strong> is a layer that takes an input matrix and, by moving the entries around, takes depth layers and converts them into width and height. Together with the earlier convolutional layer, this effectively “upscales” the image in a learnable and efficient way. We pass <strong class="source-inline">2</strong> since we want it to double the width and height. Other numbers can be used for different scaling factors but would require adjustments of the convolutional layers and the models that call <span class="No-Break">the class.</span></p>
			<ol>
				<li value="3">Finally, we have our <span class="No-Break">forward function:</span><pre class="source-code">
def forward(self, x):
  """ Upscale forward pass """
  x = self.conv(x)
  x = self.shuffle(x)
  return x</pre></li>
			</ol>
			<p>This forward function simply takes the input, then runs it through the convolutional and <strong class="source-inline">PixelShuffle</strong> layers and returns <span class="No-Break">the result.</span></p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor113"/>Creating the encoder</h2>
			<p>Let’s <a id="_idIndexMarker276"/>create the <span class="No-Break">encoder next:</span></p>
			<ol>
				<li>First, we declare <a id="_idIndexMarker277"/>the <span class="No-Break">encoder class:</span><pre class="source-code">
class OriginalEncoder(nn.Module):
    """ Face swapping encoder
    Shared to create encodings for both the faces
    """</pre></li>
			</ol>
			<p>Here we’ve defined and provided a short comment on the encoder. We declare it as a child class of the <strong class="source-inline">nn.Module</strong> class. This gives our class a lot of useful abilities from PyTorch, including the backpropagation algorithms that make neural <span class="No-Break">networks work.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">In this book, we’ve included only the basic, original model. This was the first deepfake model and has been surpassed in pretty much every way, but it’s easy to understand, so it works well for this book. If you’d like to explore other models, we recommend that you check <a id="_idIndexMarker278"/>out Faceswap at <a href="https://Faceswap.dev">https://Faceswap.dev</a>, which is constantly updated with the <span class="No-Break">newest models.</span></p>
			<ol>
				<li value="2">Next, we’ll define the <span class="No-Break">initialization function:</span><pre class="source-code">
def __init__(self):
  super().__init__()</pre></li>
			</ol>
			<p>This function is the one that actually builds the neural network's layers. Each layer is defined in this function so that PyTorch can automatically handle the details of the weights. We also call the <strong class="source-inline">__init__</strong> function from the parent class to prepare any variables or functionality that <span class="No-Break">is necessary.</span></p>
			<ol>
				<li value="3">Next, we’ll start defining our <span class="No-Break">activation function:</span><pre class="source-code">
self.activation = nn.LeakyReLU(.1)</pre></li>
			</ol>
			<p>We use <strong class="source-inline">LeakyReLU</strong> or <strong class="bold">Leaky Rectified Linear Unit</strong> as an <strong class="bold">activation function</strong> for our <a id="_idIndexMarker279"/>model. An activation function takes <a id="_idIndexMarker280"/>the output of a layer and brings it into a <span class="No-Break">standardized range.</span></p>
			<p>What a Leaky <a id="_idIndexMarker281"/>Rectified Linear Unit <em class="italic">is</em> is pretty easy to understand <a id="_idIndexMarker282"/>if you break down the words from last to first. <em class="italic">Unit</em>, in this case, means the same as function; it takes an input and provides an output. <em class="italic">Linear</em> means a line, one that doesn’t change directions as it moves; in this case, it’s a 1:1, where the output matches the input (an input of 1 leads to an output of 1, an input of 2 leads to an output of 2, and so on). <em class="italic">Rectified</em> just means it has been made positive, so negative numbers become 0. <em class="italic">Leaky</em> actually makes that last sentence a bit of a lie. It’s been found that neural networks really don’t work very well when the entire negative space becomes 0. So leaky here means that negative numbers get scaled to a range barely <span class="No-Break">below 0.</span></p>
			<p>We use 0.1 here so that any numbers below 0 get multiplied by 0.1, scaling them smaller by 10 times. Many different values can be used here, and various projects make different decisions. Standard values typically sit somewhere in the range of 0.005 <span class="No-Break">to 0.2.</span></p>
			<ol>
				<li value="4">Next, we’ll define our <span class="No-Break">convolution tower:</span><pre class="source-code">
self.conv_tower = nn.Sequential(
  nn.Conv2d(3, 128, kernel_size=5, stride=2, padding=2),
  self.activation,
  nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2),
  self.activation,
  nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2),
  self.activation,
  nn.Conv2d(512, 1024, kernel_size=5, stride=2, padding=2),
  self.activation)</pre></li>
			</ol>
			<p>The convolution tower is exactly what it sounds like, a stack of convolution layers. After each <a id="_idIndexMarker283"/>of the convolution layers, we include an activation <a id="_idIndexMarker284"/>function. This is helpful to ensure that the model stays on track and makes the convolutions more effective. The activation is identical in each case and doesn’t do any “learning” but just works like a function, so we don’t need to make separate layers for each one and can use the same activation function we <span class="No-Break">already initialized.</span></p>
			<p>We use <strong class="source-inline">nn.Sequential</strong> here to combine the stack of layers into a single layer. The sequential layer is actually a very powerful tool in PyTorch, allowing you to make simple neural networks without having to write a whole class for the model. We use it here to combine all the convolutional layers since the input in one end goes all the way through in every case. This makes it easier to use later in our <strong class="source-inline">forward</strong> function. But a sequential model runs each of its constituent layers in sequence and can’t handle conditional <strong class="source-inline">if</strong> statements or functions that aren’t written <span class="No-Break">for PyTorch.</span></p>
			<ol>
				<li value="5">Next, we’ll define a <span class="No-Break"><strong class="source-inline">flatten</strong></span><span class="No-Break"> layer:</span><pre class="source-code">
self.flatten = nn.Flatten()</pre></li>
			</ol>
			<p>A <strong class="source-inline">flatten</strong> layer does exactly what it sounds like; it flattens a previous layer to just one axis. This is used in the forward pass to turn the 1024x4x4 matrix that comes out of the convolution tower into a 4,096-element wide <span class="No-Break">single-dimension layer.</span></p>
			<ol>
				<li value="6">Next, we’ll define our <span class="No-Break">dense layers:</span><pre class="source-code">
self.dense1 = nn.Linear(4 * 4 * 1024, 1024)
self.dense2 = nn.Linear(1024, 4 * 4 * 1024)</pre></li>
			</ol>
			<p>Dense layers are called dense because they’re fully connected. Unlike convolutional layers, every single entry in the matrix is connected to every single input of the previous layer. Dense layers were the original neural network layer types and are very powerful, but they’re also very memory intensive. In fact, these two layers account for most of the memory of the entire <span class="No-Break">deepfake model!</span></p>
			<p>We generate <a id="_idIndexMarker285"/>two separate dense layers. The first layer takes in <a id="_idIndexMarker286"/>an input of 4,096 entries wide and outputs a 1,024-wide output. This <a id="_idIndexMarker287"/>is the <strong class="bold">bottleneck</strong> of the model: the part of the model that has the least amount of data, which then needs to be rebuilt. The second layer takes a 1024 one-dimensional matrix input and outputs a matrix with one dimension of 4,096. This is the first layer that starts rebuilding a face from <span class="No-Break">encoded details.</span></p>
			<ol>
				<li value="7">The last initialization step is to define our first <span class="No-Break">upscale layer:</span><pre class="source-code">
self.upscale = Upscale(512)</pre></li>
			</ol>
			<p>This layer is our first upscaler. This layer will take a 1024x4x4 matrix and upscale it back to a 512x8x8 matrix. All other upscalers will exist in the decoder. This one was originally put in the encoder, probably as a memory-saving attempt since the first upscale was unlikely to need to match a particular person at all, as it only had the most general of <span class="No-Break">face patterns.</span></p>
			<p>The upscale layer is given an output size of 512. This means that the output will be 512 deep but does not define the width or height. These come naturally from the input, with each call to upscale doubling the width <span class="No-Break">and height.</span></p>
			<ol>
				<li value="8">Next, we’ll go over our <span class="No-Break">forward function:</span><pre class="source-code">
def forward(self, x):
  """ Encoder forward pass """</pre></li>
			</ol>
			<p>The forward function is what actually applies the network to a given matrix. This is used both for training and for inference of the <span class="No-Break">trained model.</span></p>
			<ol>
				<li value="9">First, we get the <span class="No-Break">batch size:</span><pre class="source-code">
batch_size = x.shape[0]</pre></li>
			</ol>
			<p>We need the <a id="_idIndexMarker288"/>batch size that we started with later in the process, so we <a id="_idIndexMarker289"/>save it <span class="No-Break">here immediately.</span></p>
			<ol>
				<li value="10">Finally, we run the data through the <span class="No-Break">whole model:</span><pre class="source-code">
x = self.conv_tower(x)
x = self.flatten(x)
x = self.dense1(x)
x = self.dense2(x)
x = torch.reshape(x, [batch_size, 1024, 4, 4])
x = self.upscale(x)
x = self.activation(x)
return x</pre></li>
			</ol>
			<p>In this code, we run the input matrix through each layer in turn. The only new surprise here is the <strong class="source-inline">torch.reshape</strong> call after the final <strong class="source-inline">dense</strong>, which is effectively the opposite of the <strong class="source-inline">flatten</strong> call from right before the first <strong class="source-inline">dense</strong>. It takes the 4096-wide matrix and changes the shape so that it’s a 1024x4x4 <span class="No-Break">matrix again.</span></p>
			<p>We then run the data through the upscale layer and then the activation function before we return <span class="No-Break">the result.</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor114"/>Building the decoders</h2>
			<p>The decoder <a id="_idIndexMarker290"/>is responsible for taking the encoded face data <a id="_idIndexMarker291"/>and re-creating a face as accurately as it can. To do this, it will iterate over thousands or even millions of faces to get better at turning encodings into faces. At the same time, the encoder will be getting better at <span class="No-Break">encoding faces.</span></p>
			<p>We used the plural <em class="italic">decoders</em> here, but this code only actually defines a single decoder. That’s because the training code creates two copies of this <span class="No-Break">decoder class.</span></p>
			<ol>
				<li>First, we define and initialize <span class="No-Break">our model:</span><pre class="source-code">
class OriginalDecoder(nn.Module):
  """ Face swapping decoder
  An instance for each face to decode the shared encodings.
  """
  def __init__(self):
    super().__init__()</pre></li>
			</ol>
			<p>This code, just like the encoder and upscaler, is an instance of <strong class="source-inline">nn.Module</strong> and needs an initialization function that also calls the <span class="No-Break">parent’s initializer.</span></p>
			<ol>
				<li value="2">Next, we define our <span class="No-Break">activation function:</span><pre class="source-code">
self.activation = nn.LeakyReLU(.1)</pre></li>
			</ol>
			<p>Just like our encoder’s activation, we use <strong class="source-inline">LeakeReLu</strong> with a negative scaling <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.1</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Next, we define our <span class="No-Break">upscaling tower:</span><pre class="source-code">
self.upscale_tower = nn.Sequential(Upscale(256),
  self.activation,
  Upscale(128),
  self.activation,
  Upscale(64),
  self.activation)</pre></li>
			</ol>
			<p>The upscale tower is much like the convolution tower of the encoder but uses upscale blocks instead of shrinking convolutions. Because there was one upscaler in the encoder, we actually have one fewer upscales in this decoder. Just like the convolution tower, there are also activation functions after each upscale to keep the range <span class="No-Break">trending positive.</span></p>
			<ol>
				<li value="4">Next, we define our <span class="No-Break">output layer:</span><pre class="source-code">
self.output = nn.Conv2d(64, 3, 5, padding="same")</pre></li>
			</ol>
			<p>The output layer is special. While each of the previous layers’ outputs was half the depth <a id="_idIndexMarker292"/>of the previous layer’s, this one takes <a id="_idIndexMarker293"/>the 64-deep output from the convolution layer and converts it back to a three-channel image. There is nothing special about the three-channel dimension, but due to how the training process works, each is correlated to one of the color channels of the <span class="No-Break">training image.</span></p>
			<ol>
				<li value="5">Now, we define the forward function of <span class="No-Break">the decoder:</span><pre class="source-code">
def forward(self, x):
  """ Decoder forward pass """
  x = self.upscale_tower(x)
  x = self.output(x)
  x = torch.sigmoid(x)
  return x</pre></li>
			</ol>
			<p>This forward function is familiar, being very similar to those in the encoder and the upscale layer. The major difference here is that after we pass the input through the upscale tower and the output layer, we use a <strong class="source-inline">torch.sigmoid</strong> layer. This is another type of <span class="No-Break">activation layer.</span></p>
			<p>Sigmoid works differently from LeakyReLu in that it is not linear. Instead, it computes the logistic sigmoid of the input. This is an s-shaped output where negative inputs approach <strong class="source-inline">0</strong>, and positive inputs approach <strong class="source-inline">1</strong> with a <strong class="source-inline">0</strong> input coming out as <strong class="source-inline">0.5</strong>. The precise equation is <strong class="source-inline">1/(1*e^-input)</strong>. This basically puts the results between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> with extremes being more compressed, which matches how <a id="_idIndexMarker294"/>the multiplication of high numbers leads to higher <a id="_idIndexMarker295"/>numbers faster. This effectively turns the output of the model into a range of <strong class="source-inline">0-1</strong>, which we can easily turn into <span class="No-Break">an image.</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B17535_06_005.jpg" alt="Figure 6.5 – An example of the sigmoid curve"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – An example of the sigmoid curve</p>
			<p>Next, we’ll examine the <span class="No-Break">training code.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>Exploring the training code</h1>
			<p>Now that we have defined our models, we can go ahead with the process of training a neural <a id="_idIndexMarker296"/>network on our data. This is the part where we actually have AI learn the different faces so that it can later swap <span class="No-Break">between them.</span></p>
			<ol>
				<li>First, we import <span class="No-Break">our libraries:</span><pre class="source-code">
from glob import glob
import os
import random
from argparse import ArgumentParser
import cv2
import numpy as np
from tqdm import tqdm
import torch
from lib.models import OriginalEncoder, OriginalDecoder</pre></li>
			</ol>
			<p>Like all Python programs, we import our libraries. We also import our encoder and decoders from our model file. This loads the AI model code from earlier in this chapter and lets us use those to define our models in this code. Python really makes it easy to import code we’ve already written, as every Python file can be called directly or imported into <span class="No-Break">another file.</span></p>
			<p>Note that Python uses a strange syntax for folder paths. Python treats this syntax exactly the same as a module, so you use a period to tell it to look in a folder and then give it the file you want. In this case, we’re pulling the <strong class="source-inline">OriginalEncoder</strong> and <strong class="source-inline">OriginalDecoder</strong> classes from the <strong class="source-inline">models.py</strong> file located in the <span class="No-Break"><strong class="source-inline">lib</strong></span><span class="No-Break"> folder.</span></p>
			<ol>
				<li value="2">Next, we define our arguments and call our <span class="No-Break">main function:</span><pre class="source-code">
If __name__ == "__main__":
  # Train a deepfake model from two folders of face images.
  #    Example CLI:
  #    ------------
  #    python c6-train.py "C:/media/face1"
                          "C:/media/face2"</pre></li>
				<li>Next, we <a id="_idIndexMarker297"/>define <span class="No-Break">our arguments:</span><pre class="source-code">
parser = ArgumentParser()
parser.add_argument("patha",
  help="folder of images of face a")
parser.add_argument("pathb",
  help="folder of images of face b")
parser.add_argument("--cpu",
  action="store_true",
  help="Force CPU usage")
parser.add_argument("--batchsize",
  type=int, default=16,
  help="Number of images to include in a batch")
parser.add_argument("--iterations", type=int, default=100000,
  help="Number of iterations to process before stopping")
parser.add_argument("--learning-rate",
  type=float, default=.000001,
  help="Number of images to include in a batch")
parser.add_argument("--save_freq",
  type=int, default=1000,
  help="Number of iterations to save between")
parser.add_argument("--out_path",
  default="model/",
  help="folder to place models")</pre></li>
			</ol>
			<p>Here we define our arguments. These give us the ability to change our settings, files, or details without having to modify the source <span class="No-Break">code directly.</span></p>
			<ol>
				<li value="4">Then, we parse <a id="_idIndexMarker298"/>all the arguments and call our <span class="No-Break">main function:</span><pre class="source-code">
opt = parser.parse_args()
main(opt)</pre></li>
			</ol>
			<p>We parse our arguments and pass them into our main function. The main function will handle all the training processes, and we need to give it all <span class="No-Break">the arguments.</span></p>
			<ol>
				<li value="5">Next, we start our <span class="No-Break">main function:</span><pre class="source-code">
def main(opt):
  """ Train a deepfake model from two folders of face images.
  """
  device = "cuda" if torch.cuda.is_available() and not opt.cpu else "cpu"
os.makedirs(opt.out_path, exist_ok=True)</pre></li>
			</ol>
			<p>Here we start our main function and check whether we’re supposed to use <strong class="source-inline">cuda</strong>. If so, we <a id="_idIndexMarker299"/>enable <strong class="source-inline">cuda</strong> so that we can use the <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) to accelerate training. Then we create our export folder if that isn’t already created. This is where we’ll save copies of our models and any training previews we <span class="No-Break">generate later.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">While it’s <a id="_idIndexMarker300"/>possible to run the other parts of the process without a GPU, training is far more intensive, and running <a id="_idIndexMarker301"/>a training session on a <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>) will take a very large amount of time. Because of this, it’s recommended that at least this part be run with a GPU. If you don’t have one locally, you can rent one at any number of <span class="No-Break">online services.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Creating our models</h2>
			<p>Here we’ll <a id="_idIndexMarker302"/>create our neural models and fill them <span class="No-Break">with weights:</span></p>
			<ol>
				<li>First, we’ll create instances of our <span class="No-Break">previous models:</span><pre class="source-code">
encoder = OriginalEncoder()
decodera = OriginalDecoder()
decoderb = OriginalDecoder()</pre></li>
			</ol>
			<p>In this chunk of code, we create our AI models. We create one instance of the encoder and two separate decoders. We call them <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> here, but that’s entirely an arbitrary choice with no effect on the results. By default, we assume that you want to put the second face onto the first so in the case of this code, we’d be putting the face from <strong class="source-inline">b</strong> onto the frame <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">a</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">Next, we load any previously <span class="No-Break">saved models:</span><pre class="source-code">
if os.path.exists(os.path.join(opt.out_path,"encoder.pth")):
  encoder.load_state_dict( torch.load(
    os.path.join(opt.out_path, "encoder.pth")).state_dict())
  decodera.load_state_dict( torch.load(
    os.path.join(opt.out_path,"decodera.pth")).state_dict())
  decoderb.load_state_dict( torch.load(
    os.path.join(opt.out_path,"decoderb.pth")).state_dict())</pre></li>
			</ol>
			<p>Here we <a id="_idIndexMarker303"/>check whether any models already exist in the given output folder. If they do, we load those model weights into the models we instantiated in the last section. To do this, we have PyTorch load the weights from the disk and then assign the weights to the model’s state dictionary. This lets PyTorch load the weights into the model and get it ready <span class="No-Break">for training.</span></p>
			<p>If there are no weights, then we skip this step. This means that the models will be initialized with random weights, ready to start a new training session. This lets you get started easily without having to generate any random <span class="No-Break">weights yourself.</span></p>
			<ol>
				<li value="3">Next, we get a list of the images to <span class="No-Break">train with:</span><pre class="source-code">
imagesa = glob(os.path.join(opt.patha, "face_aligned_*.png"))
imagesb = glob(os.path.join(opt.pathb, "face_aligned_*.png"))</pre></li>
			</ol>
			<p>This chunk gets a list of all the images from the folders to train. We load only the aligned face images from the folders since we can create the filenames for the other images from the filenames for the <span class="No-Break">aligned images.</span></p>
			<ol>
				<li value="4">Next, we create the tensors for the images and <span class="No-Break">the masks:</span><pre class="source-code">
img_tensora = torch.zeros([opt.batchsize, 3, 64, 64])
img_tensorb = torch.zeros([opt.batchsize, 3, 64, 64])
mask_tensora = torch.zeros([opt.batchsize, 1, 64, 64])
mask_tensorb = torch.zeros([opt.batchsize, 1, 64, 64])</pre></li>
			</ol>
			<p>Here we <a id="_idIndexMarker304"/>create the tensors that will hold the images we will use for training. For the image tensors, we create a tensor that is 64x64 pixels wide with 3 channels to handle the red, green, and blue color channels. We also add a <strong class="bold">batch size</strong> dimension to the tensor so we can store that many images at once. A batch is simply how many images we’ll process at the same time. Larger batch sizes help the training process run more efficiently as we’re able to benefit from hardware that can do multiple tasks simultaneously as well as benefit from PyTorch grouping the tasks in the <span class="No-Break">best order.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">Larger batch sizes lead to more efficiencies in the training, so why don’t we set the batch size to 256 or 1024 instead of defaulting to 16? It’s because batch size is not a magic bullet. First, larger batches take more memory as the system must store every item of the batch at the same time. With large models, this can be prohibitive. Additionally, there is a side effect to large batch sizes. It’s the classic “forest for the trees,” meaning that larger batch sizes can help generalize over large sets of data but perform worse at learning specific details. So, picking the ideal batch size can be as important a question as anything else. A good rule of thumb for deepfakes is to keep batch size in double digits with 100+ tending to be too big and &lt;10 to be avoided unless you have <span class="No-Break">specific plans.</span></p>
			<ol>
				<li value="5">Next, we define and set up our optimizers and <span class="No-Break">loss function:</span><pre class="source-code">
encoder_optimizer = torch.optim.Adam(
  encoder.parameters(), lr=opt.learning_rate/2)
decodera_optimizer = torch.optim.Adam(
  decodera.parameters(), lr=opt.learning_rate)
decoderb_optimizer = torch.optim.Adam(
  decoderb.parameters(), lr=opt.learning_rate)
loss_function = torch.nn.MSELoss()</pre></li>
			</ol>
			<p>Optimizers are the part of PyTorch responsible for the most important part of training: backpropagation. This process is what changes the weights of the model and <a id="_idIndexMarker305"/>allows AI to “learn” and get better at re-creating the images we’re using for training. It’s responsible for more than just changing the weights but actually calculates how much to change them <span class="No-Break">as well.</span></p>
			<p>In this case, we’re using the <strong class="source-inline">torch.optim.Adam</strong> optimizer. This is part of a family of optimizers proven to be very effective and flexible. We use it here because it’s what the original deepfake model used, but it’s still one of the most reliable and useful optimizers <span class="No-Break">even today.</span></p>
			<p>We pass each <a id="_idIndexMarker306"/>model the <strong class="bold">learning rate</strong> from our options. The learning rate is basically a scaling value of how much the optimizer should change the weights. Higher numbers change the weights more, which can lead to faster training at the cost of difficulty in fine-tuning since the changes being made are large. Lower learning rates can get better accuracy but cause training to take longer by being slower. We cut the learning rate of the encoder by half because we will actually be training it twice as often since it is being used to encode <span class="No-Break">both faces.</span></p>
			<p>The last thing we do <a id="_idIndexMarker307"/>here is to define our <strong class="bold">loss function</strong>. This is the function that is responsible for providing a “score” of how well AI did its job. In this case, we use the <strong class="source-inline">torch.nn.MSEloss</strong> provided by PyTorch. This is a <a id="_idIndexMarker308"/>loss called <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). Let’s look at this word by <span class="No-Break">word again.</span></p>
			<p><em class="italic">Error</em> in mathematics is how far off the function is from a perfectly correct result. In our case, we are re-creating a face, so the loss function will compare the generated face to the original face and count how far off each pixel is from the correct answer. This gives a nice easy number for each pixel. Looking at each pixel is a bit too difficult, so next, our loss will take the average (mean) of all the pixels together. This gives a single number of how far off the AI was as a whole. Finally, that number is squared. This makes large differences stand out even more and <a id="_idIndexMarker309"/>has been shown to help the model reach a good <span class="No-Break">result faster.</span></p>
			<p>There are other <a id="_idIndexMarker310"/>loss functions such as <strong class="bold">mean absolute error</strong> (<strong class="bold">MAE</strong>), which gets rid of the squaring in the MSE, or <strong class="bold">structural similarity</strong>, which uses <a id="_idIndexMarker311"/>the similarity of the structure as a measure. In fact, <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>), which are <a id="_idIndexMarker312"/>a buzzword of the machine learning field, simply replace the static loss of an auto-encoder with another model that provides a trainable loss function and pits the two models against each other in a competition of which model can do their <span class="No-Break">job better.</span></p>
			<ol>
				<li value="6">Next, we move everything to the GPU <span class="No-Break">if enabled:</span><pre class="source-code">
if device == "cuda":
  encoder = encoder.cuda()
  decodera = decodera.cuda()
  decoderb = decoderb.cuda()
  img_tensora = img_tensora.cuda()
  img_tensorb = img_tensorb.cuda()
  mask_tensora = mask_tensora.cuda()
  mask_tensorb = mask_tensorb.cuda()</pre></li>
			</ol>
			<p>If <strong class="source-inline">cuda</strong> was enabled earlier, we need to move the models and the variables to the GPU so we can process them. So here, we check whether <strong class="source-inline">cuda</strong> was enabled, and if so, we move each of them to <span class="No-Break">the GPU.</span></p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>Looping over the training</h2>
			<p>In order to <a id="_idIndexMarker313"/>train the model, we need to loop over all the data. We call this the <span class="No-Break">training loop.</span></p>
			<ol>
				<li>First, we create a progress bar and start the <span class="No-Break">training loop:</span><pre class="source-code">
pbar = tqdm(range(opt.iterations))
for iteration in pbar:</pre></li>
			</ol>
			<p>We use <strong class="source-inline">tqdm</strong> again for a progress bar. Here we pass a range of how many iterations we want to <strong class="source-inline">tqdm</strong> so it can update our progress bar automatically and assign the progress bar to a variable. We then start our loop from that variable to provide more information in the progress bar by calling functions that <strong class="source-inline">tqdm</strong> exposes in <span class="No-Break">the variable.</span></p>
			<ol>
				<li value="2">Next, we load a random set <span class="No-Break">of images:</span><pre class="source-code">
images = random.sample(imagesa, opt.batchsize)
for imgnum, imagefile in enumerate(images):
  img = cv2.imread(imagefile)
  img = cv2.resize(image, (64, 64))
  mask = cv2.imread(imagefile.replace("aligned", "mask"), 0)
  mask = cv2.resize(mask, (64, 64))
  if np.random.rand() &gt; .5:
    image = cv2.flip(img, 1)
    mask = cv2.flip(mask, 1)
  img_tensor = torch.tensor(img[...,::-1]/255).permute(2,0,1)
  mask_tensor = torch.where(torch.tensor(mask) &gt; 200, 1, 0)
  if device == "cuda":
    img_tensor = img_tensor.cuda()
    mask_tensor = mask_tensor.cuda()
  img_tensora[imgnum] = img_tensor
  mask_tensora[imgnum] = mask_tensor</pre></li>
			</ol>
			<p>This chunk loads a set of images from the <strong class="source-inline">a</strong> set for the model to train with. To do this, we get a random sample the same size as our batch size from the list of files we <span class="No-Break">generated earlier.</span></p>
			<p>Next, we go over <a id="_idIndexMarker314"/>a loop for each of those images. The loop first reads in the face image and resizes it down to 64x64. Then it does the same for the mask image by replacing the <strong class="source-inline">"aligned"</strong> word in the filename with <strong class="source-inline">"mask"</strong>, which matches the mask filenames. The mask is also resized to match the <span class="No-Break">training image.</span></p>
			<p>Next, we randomly get a 50% chance to flip the images horizontally. This is an extremely common way to get more variety out of the dataset. Since faces are generally pretty symmetrical, we can usually flip them. We use a 50% chance here, which gives us an equal chance of the image being flipped or not. Since we have a mask, we have to flip it, too, if we flip <span class="No-Break">the image.</span></p>
			<p>Next, we convert <a id="_idIndexMarker315"/>both the image and masks from image arrays <a id="_idIndexMarker316"/>into tensors. To do this to the image, we convert from <strong class="bold">blue, green, red</strong> (<strong class="bold">BGR</strong>) to <strong class="bold">red, green, blue</strong> (<strong class="bold">RGB</strong>), divide by 255, and move the channels to the first dimension. This is brought into a single-liner, but it is the same process as we used in the last chapter, where we were finding faces. Since BGR and RGB are just reversed, we can change BGR to RGB by just reversing the channels by indexing with <strong class="source-inline">[...,::-1]</strong>. This can also be done again to get it back to the BGR order (which we’ll do later). The mask is simpler since we don’t care about color data for it, so we just see if the pixel data is greater than 200; if it is, we put <strong class="source-inline">1</strong> into the tensor; if not, we <span class="No-Break">put </span><span class="No-Break"><strong class="source-inline">0</strong></span><span class="No-Break">.</span></p>
			<p>Next, we check whether <strong class="source-inline">cuda</strong> is enabled; if it is, we move the tensors we just created to the GPU. This puts everything onto the <span class="No-Break">same device.</span></p>
			<p>Finally, we move the image into the tensor we’ll be using to train the model. This lets us batch the images together <span class="No-Break">for efficiency.</span></p>
			<ol>
				<li value="3">Then, we do <a id="_idIndexMarker317"/>the same for the other set <span class="No-Break">of images:</span><pre class="source-code">
images = random.sample(imagesb, opt.batchsize)
for imgnum, imagefile in enumerate(images):
  img = cv2.imread(imagefile)
  img = cv2.resize(image, (64, 64))
  mask = cv2.imread(imagefile.replace("aligned", "mask"), 0)
  mask = cv2.resize(mask, (64, 64))
  if np.random.rand() &gt; .5:
    image = cv2.flip(img, 1)
    mask = cv2.flip(mask, 1)
  img_tensor = torch.tensor(img[...,::-1]/255).permute(2,0,1)
  mask_tensor = torch.where(torch.tensor(mask) &gt; 200, 1, 0)
  if device == "cuda":
    img_tensor = img_tensor.cuda()
    mask_tensor = mask_tensor.cuda()
  img_tensorb[imgnum] = img_tensor
  mask_tensorb[imgnum] = mask_tensor</pre></li>
			</ol>
			<p>This code is identical to the last code but is repeated for the <strong class="source-inline">b</strong> set of images. This creates our second set of images ready <span class="No-Break">to train.</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>Teaching the network</h2>
			<p>Now it’s <a id="_idIndexMarker318"/>time to actually perform the steps that train <span class="No-Break">the network:</span></p>
			<ol>
				<li>First, we clear <span class="No-Break">the optimizers:</span><pre class="source-code">
Encoder_optimizer.zero_grad()
decodera_optimizer.zero_grad()</pre></li>
			</ol>
			<p>PyTorch is very flexible in how it lets you build your models and training process. Because of this, we need to tell PyTorch to clear the <strong class="bold">gradients</strong> each time we start a training iteration. Gradients are a log of the results of each image going through each layer. These are used to know how much each weight needs to be modified during the backpropagation step. In this chunk we clear these gradients for the encoder and the <strong class="source-inline">a</strong> <span class="No-Break">side decoder.</span></p>
			<ol>
				<li value="2">Then, we pass the images through the encoder <span class="No-Break">and decoder:</span><pre class="source-code">
Outa = decodera(encoder(img_tensora))</pre></li>
			</ol>
			<p>This chunk sends the image tensors we created earlier through the encoder and then through the decoder and stores the results for comparison. This is actually the AI model, and we give it a tensor of images and get a tensor of images <span class="No-Break">back out.</span></p>
			<ol>
				<li value="3">Next, we calculate our loss and send it through <span class="No-Break">the optimizers:</span><pre class="source-code">
Lossa = loss_function(
  outa * mask_tensora, img_tensora * mask_tensora)
lossa.backward()
encoder_optimizer.step()
decodera_optimizer.step()</pre></li>
			</ol>
			<p>This chunk does the rest of the training, calculating the loss and then having the optimizers perform backpropagation on the models to update the weights. We start by passing the output images and the original images to the <span class="No-Break">loss function.</span></p>
			<p>To apply the masks, we multiply the images by the masks. We do this here instead of before we pass the images to the model because we might not have the best masks, and it’s better to train the neural network on the whole image and apply the <span class="No-Break">mask later.</span></p>
			<p>Next, we call <strong class="source-inline">backward</strong> on the loss variable. We can do this because the variable is actually still a tensor, and tensors keep track of all actions that happened <a id="_idIndexMarker319"/>to them while in training mode. This lets the loss be carried back over all the steps back to the <span class="No-Break">original image.</span></p>
			<p>The last step is to call the <strong class="source-inline">step</strong> function of our optimizers. This goes back over the model weights, updating them so that the next iteration should be closer to the <span class="No-Break">correct results.</span></p>
			<ol>
				<li value="4">Next, we do the same thing, but for the <strong class="source-inline">b</strong> <span class="No-Break">decoder instead:</span><pre class="source-code">
encoder_optimizer.zero_grad()
decoderb_optimizer.zero_grad()
outb = decoderb(encoder(img_tensorb))
lossb = loss_function(
  outb * mask_tensorb, img_tensorb * mask_tensorb)
lossb.backward()
encoder_optimizer.step()
decoderb_optimizer.step()</pre></li>
			</ol>
			<p>We go through the same process again with the <strong class="source-inline">b</strong> images and decoder. Remember that we’re using the same encoder for both models, so it actually gets trained a second time along with the <strong class="source-inline">b</strong> decoder. This is a key part of how deepfakes can swap faces. The two decoders share a single encoder, which eventually gives both the decoders the information to re-create their <span class="No-Break">individual faces.</span></p>
			<ol>
				<li value="5">Next, we update <a id="_idIndexMarker320"/>the progress bar with information about this iteration’s loss <span class="No-Break">of data:</span><pre class="source-code">
pbar.set_description(f"A: {lossa.detach().cpu().numpy():.6f} "
  f"B: {lossb.detach().cpu().numpy():.6f}")</pre></li>
			</ol>
			<p>Since the loss function outputs a number for the optimizers, we can also display this number for the user. Sometimes loss is used by deepfakers as an estimate of how finished the model is training. Unfortunately, this cannot actually measure how good a model is at converting one face into another; it only scores how good it is at re-creating the same face it was given. For this reason, it’s an imperfect measure and shouldn’t be relied on. Instead, we recommend that the previews we’ll be generating later be used for <span class="No-Break">this purpose.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor119"/>Saving results</h2>
			<p>Finally, we will <a id="_idIndexMarker321"/>save <span class="No-Break">our results:</span></p>
			<ol>
				<li>First, we’ll check to see whether we should trigger <span class="No-Break">a save:</span><pre class="source-code">
if iteration % opt.save_freq == 0:
  with torch.no_grad():
    outa = decodera(encoder(img_tensora[:1]))
    outb = decoderb(encoder(img_tensorb[:1]))
    swapa = decoderb(encoder(img_tensora[:1]))
    swapb = decodera(encoder(img_tensorb[:1]))</pre></li>
			</ol>
			<p>We want to save regularly – an iteration may take less than a second, but a save could take several seconds of writing to disk. Because of this, we don’t want to save every iteration; instead, we want to trigger a save on a regular basis after a set number of iterations. Different computers will run at different speeds, so we let you set the save frequency with <span class="No-Break">an argument.</span></p>
			<p>One thing we want to save along with the current weights is a preview image so we can get a good idea of how the model is doing at each save state. For this reason, we’ll be <a id="_idIndexMarker322"/>using the neural networks, but we don’t want to train while we’re doing this step. That’s the exact reason that <strong class="source-inline">torch</strong> has the <strong class="source-inline">torch.no_grad</strong> context. By calling our model from inside this context, we won’t be training and just getting the results from <span class="No-Break">the network.</span></p>
			<p>We call each decoder with samples of images from both faces. This lets us compare the re-created faces along with the generated swaps. Since we only want a preview image, we can throw out all but the first image to be used as a sample of the current stage <span class="No-Break">of training.</span></p>
			<ol>
				<li value="2">Next, we create the <a id="_idTextAnchor120"/><span class="No-Break">sample image:</span><pre class="source-code">
example = np.concatenate([
  img_tensora[0].permute(1, 2, 0).detach().cpu().numpy(),
  outa[0].permute(1, 2, 0).detach().cpu().numpy(),
  swapa[0].permute(1, 2, 0).detach().cpu().numpy(),
  img_tensorb[0].permute(1, 2, 0).detach().cpu().numpy(),
  outb[0].permute(1, 2, 0).detach().cpu().numpy(),
  swapb[0].permute(1, 2, 0).detach().cpu().numpy()
  ],axis=1)</pre></li>
			</ol>
			<p>We need to create our sample image from all the parts. To do this, we need to convert all the image tensors into a single image. We use <strong class="source-inline">np.concatenate</strong> to join them all into a single array along the width axis. To do this, we need to get them all into image order and convert them to NumPy arrays. The first thing we do is drop the batch dimension by selecting the first one. Then we use <strong class="source-inline">permute</strong> to reorder each tensor, so the channels are last. Then we use <strong class="source-inline">detach</strong> to remove any gradients from the tensors. We can then use <strong class="source-inline">cpu</strong> to bring the weights <a id="_idIndexMarker323"/>back onto the CPU. Finally, we use <strong class="source-inline">numpy</strong> to finish converting them into <span class="No-Break">NumPy arrays.</span></p>
			<ol>
				<li value="3">Next, we write the <span class="No-Break">preview image:</span><pre class="source-code">
cv2.imwrite(
  os.path.join(opt.out_path, f"preview_{iteration}.png"),
  example[...,::-1]*255)</pre></li>
			</ol>
			<p>This chunk uses <strong class="source-inline">cv2.imwrite</strong> from OpenCV to write out the preview image as a PNG file. We put it in the output path and give it a name based on what iteration this is. This lets us save each iteration’s preview together and track the progress of the network over time. To actually write out a usable image, we have to convert the color space back to the BGR that OpenCV expects, and then we multiply by <strong class="source-inline">255</strong> to get a result that fits into the <span class="No-Break">integer space.</span></p>
			<ol>
				<li value="4">Next, we save the weights to <span class="No-Break">a file:</span><pre class="source-code">
torch.save(encoder,
  os.path.join(opt.out_path, "encoder.pth"))
torch.save(decodera,
  os.path.join(opt.out_path, "decodera.pth"))
torch.save(decoderb,
  os.path.join(opt.out_path, "decoderb.pth"))</pre></li>
			</ol>
			<p>Here we save the weights for our encoder and both decoders by calling <strong class="source-inline">torch.save</strong> with the output path and the filenames we want to use to save the weights. PyTorch automatically saves them to the file in its native <span class="No-Break"><strong class="source-inline">pth</strong></span><span class="No-Break"> format.</span></p>
			<ol>
				<li value="5">Finally, we <span class="No-Break">save again:</span><pre class="source-code">
torch.save(encoder,
  os.path.join(opt.out_path, "encoder.pth"))
torch.save(decodera,
  os.path.join(opt.out_path, "decodera.pth"))
torch.save(decoderb,
  os.path.join(opt.out_path, "decoderb.pth"))</pre></li>
			</ol>
			<p>For our last step, we repeat the save code. But, actually, this is done outside of the training loop. This is here just in case the training loop ends on an iteration number that <a id="_idIndexMarker324"/>doesn’t trigger the save there. This way, the model is definitely saved at least once, no matter what arguments are chosen by <span class="No-Break">the user.</span></p>
			<p class="callout-heading">Author’s note</p>
			<p class="callout">This may seem obvious to anyone who has spent any time coding, but it’s worth repeating. You should always consider how choices made in the development process might lead to bad or unexpected outcomes and account for those in your design. It’s obviously impossible to consider everything, but sometimes even something as simple as duplicating a save right before exiting, “just in case,” can save someone’s day (or in the case of a very long training session, even <span class="No-Break">more time).</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor121"/>Summary</h1>
			<p>In this chapter, we trained a neural network to swap faces. To do this, we had to explore what convolutional layers are and then build a foundational upscaler layer. Then we built the three networks. We built the encoder, then two decoders. Finally, we trained the model itself, including loading and preparing images, and made sure we saved previews and the <span class="No-Break">final weights.</span></p>
			<p>First, we built the models of the neural networks that we were going to train to perform the face-swapping process. This was broken down into the upscaler, the shared encoder, and the two decoders. The upscaler is used to increase the size of the image by turning depth into a larger image. The encoder is used to encode the face image down into a smaller encoded space that we then pass to the decoders, which are responsible for re-creating the original image. We also looked at activation layers to understand why <span class="No-Break">they’re helpful.</span></p>
			<p>Next, we covered the training code. We created instances of the network models, loaded weights into the models, and put them on the GPU if one was available. We explored optimizers and loss functions to understand the roles that they play in the training process. We loaded and processed images so that they were ready to go through the model to assist in training. Then we covered the training itself, including how to get the loss and apply it to the model using the optimizers. Finally, we saved preview images and the model weights themselves so we could load <span class="No-Break">them again.</span></p>
			<p>In the next chapter, we’ll take our trained model and use it to “convert” a video, swapping <span class="No-Break">the faces.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor122"/>Exercises</h1>
			<ol>
				<li>Choosing learning rate is not a solved problem. There is no one “right” learning rate. What happens if you make the learning rate 10 times bigger? 10 times smaller? What if you start with a large learning rate and then reduce it after <span class="No-Break">some training?</span></li>
				<li>We used the same loss function and optimizer as the original code, which was first released back in 2018, but there are a lot of options now that weren’t available then. Try replacing the loss function with others from PyTorch’s extensive collection (<a href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a>). Some of them will work without any change, but some won’t work for our situation at all. Try different ones, or even try combinations of <span class="No-Break">loss functions!</span></li>
				<li>We defined a model that downscaled from a 64x64 pixel image and re-created that same image. But with some tweaks, this same architecture can instead create a 128x128 or 256x256 pixel image. How would you make changes to the model to do this? Should you increase the number (and size) of layers in the convolutional tower and keep the bottleneck the same, increase the size of the bottleneck but keep the layers the same, or change the convolutional layer’s kernel sizes and strides? It’s even possible to send in a 64x64 pixel image and get out a 128x128 pixel image. All of these techniques have their advantages and drawbacks. Try each out and see how <span class="No-Break">they differ.</span></li>
				<li>We’re training on just two faces, but you could potentially do more. Try modifying the training code to use three different faces instead of just two. What changes would you need to make? What modifications would you make so that you can train an arbitrary number of faces <span class="No-Break">at once?</span></li>
				<li>In the years since deepfakes were first released, there have been a lot of different models created. Faceswap has implementations for many newer and more advanced models, but they’re written in Keras for Tensorflow and can’t work in this PyTorch fork without being ported. Check the Faceswap models at the GitHub repo (<a href="https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model">https://github.com/deepfakes/Faceswap/tree/master/plugins/train/model</a>). Compare this model to the one in <strong class="source-inline">Original.py</strong>, which implements the same model. Now use that to see how <strong class="source-inline">dfaker.py</strong> differs. The residual layer works by adding an extra convolution layer and an <strong class="source-inline">add</strong> layer, which just adds two layers together. Can you duplicate the <strong class="source-inline">dfaker</strong> model in this code base?  What about <span class="No-Break">the others?</span></li>
			</ol>
		</div>
		<div>
			<div id="_idContainer081" class="IMG---Figure">
			</div>
		</div>
	<div style="width:100%; margin-top:20px; "><div style="text-align:left; padding:10px" aria-hidden="true"><span style="font-size: 0.75em">EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to <a target="_blank" href="https://www.ebsco.com/terms-of-use">https://www.ebsco.com/terms-of-use</a></span></div></div>
</body></html>