<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Sensors</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll learn to implement AI behavior using the concept of a sensory system similar to what living entities have. As we discussed earlier, a character AI system needs to have awareness of its environment such as where the obstacles are, where the enemy it is looking for is, whether the enemy is visible in the player's sight, and so on. The quality of our NPC's AI completely depends on the information it can get from the environment. Nothing breaks the level of immersion in a game like an NPC getting stuck behind a wall. Based on the information the NPC can collect, the AI system can decide which logic to execute in response to that data. If the sensory systems do not provide enough data, or the AI system is unable to properly take action on that data, the agent can begin to glitch, or behave in a way contrary to what the developer, or more importantly the player, would expect. Some games have become infamous for their comically bad AI glitches, and it's worth a quick internet search to find some videos of AI glitches for a good laugh.</p>
<p>We can detect all the environment parameters and check them against our predetermined values if we want. But using a proper design pattern will help us maintain code and thus will be easy to extend. This chapter will introduce a design pattern that we can use to implement sensory systems. We will be covering:</p>
<ul>
<li>What sensory systems are</li>
<li>Some of the different sensory systems that exist</li>
<li>How to set up a sample tank with sensing</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic sensory systems</h1>
                </header>
            
            <article>
                
<p>Our agent's sensory systems should believably emulate real-world senses such as vision, sound, and so on, to build a model of its environment, much like we do as humans. Have you ever tried to navigate a room in the dark after shutting off the lights? It gets more and more difficult as you move from your initial position when you turned the lights off because your perspective shifts and you have to rely more and more on your fuzzy memory of the room's layout. While our senses rely on and take in a constant stream of data to navigate their environment, our agent's AI is a lot more forgiving, giving us the freedom to examine the environment at predetermined intervals. This allows us to build a more efficient system in which we can focus only on the parts of the environment that are relevant to the agent.</p>
<p>The concept of a basic sensory system is that there will be two components, <kbd>Aspect</kbd> and <kbd>Sense</kbd>. Our AI characters will have senses, such as perception, smell, and touch. These senses will look out for specific aspects such as enemies and bandits. For example, you could have a patrol guard AI with a perception sense that's looking for other game objects with an enemy aspect, or it could be a zombie entity with a smell sense looking for other entities with an aspect defined as a brain.</p>
<p>For our demo, this is basically what we are going to implement—a base interface called <kbd>Sense</kbd> that will be implemented by other custom senses. In this chapter, we'll implement perspective and touch senses. Perspective is what animals use to see the world around them. If our AI character sees an enemy, we want to be notified so that we can take some action. Likewise with touch, when an enemy gets too close, we want to be able to sense that, almost as if our AI character can hear that the enemy is nearby. Then we'll write a minimal <kbd>Aspect</kbd> class that our senses will be looking for.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cone of sight</h1>
                </header>
            
            <article>
                
<p>In the example provided in <a href="9e338555-162c-4ed0-a519-035cfcea94ce.xhtml">Chapter 2</a><em>,</em> <em>Finite State Machines and You</em>, we set up our agent to detect the player tank using line of sight, which is literally a line in the form of a raycast. A <strong>raycast</strong> is a feature in Unity that allows you to determine which objects are intersected by a line cast from a point toward a given direction. While this is a fairly efficient way to handle visual detection in a simple way, it doesn't accurately model the way vision works for most entities. An alternative to using line of sight is using a cone-shaped field of vision. As the following figure illustrates, the field of vision is literally modeled using a cone shape. This can be in 2D or 3D, as appropriate for your type of game:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="250" width="334" src="assets/db1843c4-9f08-43a7-bb52-1b9718725497.png"/></div>
<p>The preceding figure illustrates the concept of a cone of sight. In this case, beginning with the source, that is, the agent's eyes, the cone grows, but becomes less accurate with distance, as represented by the fading color of the cone.</p>
<p>The actual implementation of the cone can vary from a basic overlap test to a more complex realistic model, mimicking eyesight. In a simple implementation, it is only necessary to test whether an object overlaps with the cone of sight, ignoring distance or periphery. A complex implementation mimics eyesight more closely; as the cone widens away from the source, the field of vision grows, but the chance of getting to see things toward the edges of the cone diminishes compared to those near the center of the source.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hearing, feeling, and smelling using spheres</h1>
                </header>
            
            <article>
                
<p>One very simple yet effective way of modeling sounds, touch, and smell is via the use of spheres. For sounds, for example, we can imagine the center as being the source and the loudness dissipating the farther from the center the listener is. Inversely, the listener can be modeled instead of, or in addition to, the source of the sound. The listener's hearing is represented by a sphere, and the sounds closest to the listener are more likely to be "heard." We can modify the size and position of the sphere relative to our agent to accommodate feeling and smelling.</p>
<p>The following figure represents our sphere and how our agent fits into the setup:</p>
<div class="CDPAlignCenter CDPAlign"><img height="214" width="286" src="assets/ec1eded7-23cd-4bfd-94ba-cc68c8fc5769.png"/></div>
<p>As with sight, the probability of an agent registering the sensory event can be modified, based on the distance from the sensor or as a simple overlap event, where the sensory event is always detected as long as the source overlaps the sphere.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Expanding AI through omniscience</h1>
                </header>
            
            <article>
                
<p>In a nutshell, omniscience is really just a way to make your AI cheat. While your agent doesn't necessarily know everything, it simply means that they <em>can</em> know anything. In some ways, this can seem like the antithesis to realism, but often the simplest solution is the best solution. Allowing our agent access to seemingly hidden information about its surroundings or other entities in the game world can be a powerful tool to provide an extra layer of complexity.</p>
<p>In games, we tend to model abstract concepts using concrete values. For example, we may represent a player's health with a numeric value ranging from 0 to 100. Giving our agent access to this type of information allows it to make realistic decisions, even though having access to that information is not realistic. You can also think of omniscience as your agent being able to <em>use the force</em> or sense events in your game world without having to <em>physically</em> experience them.</p>
<p>While omniscience is not necessarily a specific pattern or technique, it's another tool in your toolbox as a game developer to cheat a bit and make your game more interesting by, in essence, bending the rules of AI, and giving your agent data that they may not otherwise have had access to through <em>physical</em> means.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting creative with sensing</h1>
                </header>
            
            <article>
                
<p>While <span>cones, spheres, and lines</span> are among the most basic ways an agent can see, hear, and perceive their environment, they are by no means the only ways to implement these senses. If your game calls for other types of sensing, feel free to combine these patterns. Want to use a cylinder or a sphere to represent a field of vision? Go for it. Want to use boxes to represent the sense of smell? Sniff away!</p>
<p>Using the tools at your disposal, come up with creative ways to model sensing in terms relative to your player. Combine different approaches to create unique gameplay mechanics for your games by mixing and matching these concepts. For example, a magic-sensitive but blind creature could completely ignore a character right in front of them until they cast or receive the effect of a magic spell. Maybe certain NPCs can track the player using smell, and walking through a collider marked <em>water</em> can clear the scent from the player so that the NPC can no longer track him. As you progress through the book, you'll be given all the tools to pull these and many other mechanics off—sensing, decision-making, pathfinding, and so on. As we cover some of these techniques, start thinking about creative twists for your game.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the scene</h1>
                </header>
            
            <article>
                
<p>In order to get started with implementing the sensing system, you can jump right into the example provided for this chapter, or set up the scene yourself, by following these steps:</p>
<ol>
<li>Let's create a few barriers to block the line of sight from our AI character to the tank. These will be short but wide cubes grouped under an empty game object called <kbd>Obstacles</kbd>.</li>
<li>Add a plane to be used as a floor.</li>
<li>Then, we add a directional light so that we can see what is going on in our scene.</li>
</ol>
<p>As you can see in the example, there is a target 3D model, which we use for our player, and we represent our AI agent using a simple cube. We will also have a <kbd>Target</kbd> object to show us where the tank will move to in our scene.</p>
<p>For simplicity, our example provides a point light as a child of the <kbd>Target</kbd> so that we can easily see our target destination in the game view. Our scene hierarchy will look similar to the following screenshot after you've set everything up correctly:</p>
<div class="CDPAlignCenter CDPAlign"><img height="209" width="309" src="assets/ec3ecb5f-098e-4579-ac8e-10722262f860.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">The scene hierarchy</div>
<p>Now we will position the tank, the AI character, and walls randomly in our scene. Increase the size of the plane to something that looks good. Fortunately, in this demo, our objects float, so nothing will fall off the plane. Also, be sure to adjust the camera so that we can have a clear view of the following scene:</p>
<div class="CDPAlignCenter CDPAlign"><img height="217" width="366" class="aligncenter size-full wp-image-538 image-border" src="assets/58a5d229-ccbe-48d9-b22a-5d1f383dddf2.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Our game scene</div>
<p>With the essential setup out of the way, we can begin tackling the code for driving the various systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the player tank and aspect</h1>
                </header>
            
            <article>
                
<p>Our <kbd>Target</kbd> object is a simple sphere game object with the mesh render removed, so that we end up with only the <strong>Sphere Collider</strong>. </p>
<p>Look at the following code in the <kbd>Target.cs</kbd> file:</p>
<pre>using UnityEngine;<br/><br/>public class Target : MonoBehaviour<br/>{<br/>    public Transform targetMarker;<br/>    <br/>    void Start (){}<br/><br/>    void Update ()<br/>    {<br/>        int button = 0;<br/><br/>        //Get the point of the hit position when the mouse is being clicked<br/>        if(Input.GetMouseButtonDown(button)) <br/>        {<br/>            Ray ray = Camera.main.ScreenPointToRay(Input.mousePosition);<br/>            RaycastHit hitInfo;<br/><br/>            if (Physics.Raycast(ray.origin, ray.direction, out hitInfo)) <br/>            {<br/>                Vector3 targetPosition = hitInfo.point;<br/>                targetMarker.position = targetPosition;<br/>            }<br/>        }<br/>    }<br/>}</pre>
<div class="packt_tip">You'll notice we left in an empty <kbd>Start</kbd> method in the code. While there is a cost in having empty <kbd>Start</kbd>, <kbd>Update</kbd>, and other <kbd>MonoBehaviour</kbd> events that don't do anything, we can sometimes choose to leave the <kbd>Start</kbd> method in during development, so that the component shows an enable/disable toggle in the inspector.</div>
<p>Attach this script to our <kbd>Target</kbd> object, which is what we assigned in the inspector to the <kbd>targetMarker</kbd> variable. The script detects the mouse click event and then, using a raycast, it detects the mouse click point on the plane in the 3D space. After that, it updates the <kbd>Target</kbd> object to that position in the world space in the scene.</p>
<div class="packt_infobox">A raycast is a feature of the Unity Physics API that shoots a virtual ray from a given origin towards a given direction, and returns data on any colliders hit along the way.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the player tank</h1>
                </header>
            
            <article>
                
<p>Our player tank is the simple tank model we used in <a href="9e338555-162c-4ed0-a519-035cfcea94ce.xhtml">Chapter 2</a>, <em>Finite State Machines and You</em>, with a kinematic rigid body component attached. The rigid body component is needed in order to generate trigger events whenever we do collision detection with any AI characters. The first thing we need to do is to assign the tag <kbd>Player</kbd> to our tank.</p>
<div class="packt_infobox">The <kbd>isKinematic</kbd> flag in Unity's Rigidbody component makes it so that external forces are ignored, so that you can control the Rigidbody entirely from code or from an animation, while still having access to the Rigidbody API.</div>
<p>The tank is controlled by the <kbd>PlayerTank</kbd> script, which we will create in a moment. This script retrieves the target position on the map and updates its destination point and the direction accordingly.</p>
<p>The code in the <kbd>PlayerTank.cs</kbd> file is as follows:</p>
<pre>using UnityEngine;<br/><br/>public class PlayerTank : MonoBehaviour <br/>{<br/>    public Transform targetTransform;<br/>    public float targetDistanceTolerance = 3.0f;<br/><br/>    private float movementSpeed;<br/>    private float rotationSpeed;<br/><br/>  // Use this for initialization<br/>  void Start () <br/>    {<br/>        movementSpeed = 10.0f;<br/>        rotationSpeed = 2.0f;<br/>  }<br/>  <br/>  // Update is called once per frame<br/>  void Update () <br/>    {<br/>        if (Vector3.Distance(transform.position, targetTransform.position) &lt; targetDistanceTolerance) <br/>        {<br/>            return;<br/>        }<br/><br/>        Vector3 targetPosition = targetTransform.position;<br/>        targetPosition.y = transform.position.y;<br/>        Vector3 direction = targetPosition - transform.position;<br/><br/>        Quaternion tarRot = Quaternion.LookRotation(direction);<br/>        transform.rotation = Quaternion.Slerp(transform.rotation, tarRot, rotationSpeed * Time.deltaTime);<br/><br/>        transform.Translate(new Vector3(0, 0, movementSpeed * Time.deltaTime));<br/>  }<br/>}</pre>
<div class="mce-root CDPAlignCenter CDPAlign"><img height="322" width="295" class="aligncenter size-full wp-image-540 image-border" src="assets/9d470de6-cccd-4cb1-b95d-38034bd5bff7.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Properties of our tank object</div>
<p>The preceding screenshot shows us a snapshot of our script in the inspector once applied to our tank.</p>
<p>This script queries the position of the <kbd>Target</kbd> object on the map and updates its destination point and the direction accordingly. After we assign this script to our tank, be sure to assign our <kbd>Target</kbd> object to the <kbd>targetTransform</kbd> variable.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing the Aspect class</h1>
                </header>
            
            <article>
                
<p>Next, let's take a look at the <kbd>Aspect.cs</kbd> class. <kbd>Aspect</kbd> is a very simple class with just one public enum of type <kbd>AspectTypes</kbd> called <kbd>aspectType</kbd>. That's all of the variables we need in this component. Whenever our AI character senses something, we'll check the  <kbd>aspectType</kbd> to see whether it's the aspect that the AI has been looking for.</p>
<p>The code in the <kbd>Aspect.cs</kbd> file looks like this:</p>
<pre>using UnityEngine;<br/><br/>public class Aspect : MonoBehaviour {<br/>  public enum AspectTypes {<br/>    PLAYER,<br/>    ENEMY,<br/>  }<br/>  public AspectTypes aspectType;<br/>}</pre>
<p>Attach this aspect script to our player tank and set the <kbd>aspectType</kbd> to <kbd>PLAYER</kbd>, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="49" width="341" src="assets/4b549b44-5340-4e0b-8e0a-770184fa4f69.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Setting the Aspect Type of the tank</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an AI character</h1>
                </header>
            
            <article>
                
<p>Our NPC will be roaming around the scene in a random direction. It'll have the following two senses:</p>
<ul>
<li>The perspective sense will check whether the tank aspect is within a set visible range and distance</li>
<li>The touch sense will detect if the enemy aspect has collided with its box collider, which we'll be adding to the tank in a later step</li>
</ul>
<p>Because our player tank will have the <kbd>PLAYER</kbd> aspect type, the NPC will be looking for any <kbd>aspectType</kbd> not equal to its own.</p>
<p>The code in the <kbd>Wander.cs</kbd> file is as follows:</p>
<pre>using UnityEngine;<br/><br/>public class Wander : MonoBehaviour {<br/>    private Vector3 targetPosition;<br/><br/>    private float movementSpeed = 5.0f;<br/>    private float rotationSpeed = 2.0f;<br/>    private float targetPositionTolerance = 3.0f;<br/>    private float minX;<br/>    private float maxX;<br/>    private float minZ;<br/>    private float maxZ;<br/><br/>    void Start() {<br/>        minX = -45.0f;<br/>        maxX = 45.0f;<br/><br/>        minZ = -45.0f;<br/>        maxZ = 45.0f;<br/><br/>        //Get Wander Position<br/>        GetNextPosition();<br/>    }<br/><br/>    void Update() {<br/>        if (Vector3.Distance(targetPosition, transform.position) &lt;= targetPositionTolerance) {<br/>            GetNextPosition();<br/>        }<br/><br/>        Quaternion targetRotation = Quaternion.LookRotation(targetPosition - transform.position);<br/>        transform.rotation = Quaternion.Slerp(transform.rotation, targetRotation, rotationSpeed * Time.deltaTime);<br/><br/>        transform.Translate(new Vector3(0, 0, movementSpeed * Time.deltaTime));<br/>    }<br/><br/>    void GetNextPosition() {<br/>        targetPosition = new Vector3(Random.Range(minX, maxX), 0.5f, Random.Range(minZ, maxZ));<br/>    }<br/>}<br/><br/></pre>
<p>The <kbd>Wander</kbd> script generates a new random position in a specified range whenever the AI character reaches its current destination point. The <kbd>Update</kbd> method will then rotate our enemy and move it toward this new destination. Attach this script to our AI character so that it can move around in the scene. The <kbd>Wander</kbd> script is rather simplistic, but we will cover more advanced locomotion approaches in later chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Sense class</h1>
                </header>
            
            <article>
                
<p>The <kbd>Sense</kbd> class is the interface of our sensory system that the other custom senses can implement. It defines two virtual methods, <kbd>Initialize</kbd> and <kbd>UpdateSense</kbd>, which will be implemented in custom senses, and are executed from the <kbd>Start</kbd> and <kbd>Update</kbd> methods, respectively.</p>
<div class="packt_infobox">Virtual methods are methods that can be overridden using the <kbd>override</kbd> modifier in derived classes. Unlike <kbd>abstract</kbd> classes, virtual classes do not require that you override them.</div>
<p>The code in the <kbd>Sense.cs</kbd> file looks like this:</p>
<pre>using UnityEngine;<br/><br/>public class Sense : MonoBehaviour {<br/>  public bool enableDebug = true;<br/>  public Aspect.AspectTypes aspectName = Aspect.AspectTypes.ENEMY;<br/>  public float detectionRate = 1.0f;<br/><br/>  protected float elapsedTime = 0.0f;<br/><br/>  protected virtual void Initialize() { }<br/>  protected virtual void UpdateSense() { }<br/><br/>  // Use this for initialization<br/>  void Start () <br/>    {<br/>    elapsedTime = 0.0f;<br/>    Initialize();<br/>  }<br/>  <br/>  // Update is called once per frame<br/>  void Update () <br/>    {<br/>    UpdateSense();<br/>  }<br/>}</pre>
<p>The basic properties include its detection rate to execute the sensing operation, as well as the name of the aspect it should look for. This script will not be attached to any of our objects since we'll be deriving from it for our actual senses.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Giving a little perspective</h1>
                </header>
            
            <article>
                
<p>The perspective sense will detect whether a specific aspect is within its field of view and visible distance. If it sees anything, it will take the specified action, which in this case is to print a message to the console.</p>
<p>The code in the <kbd>Perspective.cs</kbd> file looks like this:</p>
<pre>using UnityEngine;<br/><br/>public class Perspective : Sense<br/>{<br/>    public int fieldOfView = 45;<br/>    public int viewDistance = 100;<br/><br/>    private Transform playerTransform;<br/>    private Vector3 rayDirection;<br/><br/>    protected override void Initialize() <br/>    {<br/>        playerTransform = GameObject.FindGameObjectWithTag("Player").transform;<br/>    }<br/><br/>    protected override void UpdateSense() <br/>    {<br/>        elapsedTime += Time.deltaTime;<br/><br/>        if (elapsedTime &gt;= detectionRate) <br/>        {<br/>            DetectAspect();<br/>        }<br/>  }<br/><br/>    //Detect perspective field of view for the AI Character<br/>    void DetectAspect()<br/>    {<br/>        RaycastHit hit;<br/>        rayDirection = playerTransform.position - transform.position;<br/><br/>        if ((Vector3.Angle(rayDirection, transform.forward)) &lt; fieldOfView)<br/>        {<br/>            // Detect if player is within the field of view<br/>            if (Physics.Raycast(transform.position, rayDirection, out hit, viewDistance))<br/>            {<br/>                Aspect aspect = hit.collider.GetComponent&lt;Aspect&gt;();<br/>                if (aspect != null)<br/>                {<br/>                    //Check the aspect<br/>                    if (aspect.aspectType != aspectName)<br/>                    {<br/>                        print("Enemy Detected");<br/>                    }<br/>                }<br/>            }<br/>        }<br/>    }</pre>
<p>We need to implement the <kbd>Initialize</kbd> and <kbd>UpdateSense</kbd> methods that will be called from the <kbd>Start</kbd> and <kbd>Update</kbd> methods of the parent <kbd>Sense</kbd> class, respectively. In the <kbd>DetectAspect</kbd> method, we first check the angle between the player and the AI's current direction. If it's in the field of view range, we shoot a ray in the direction that the player tank is located. The ray length is the value of the visible distance property.</p>
<p>The <kbd>Raycast</kbd> method will return when it first hits another object. <span>This way, even if the player is in the visible range, the AI character will not be able to see if it's hidden behind the wall.</span> We then check for an <kbd>Aspect</kbd> component, and it will return true only if the object that was hit has an <kbd>Aspect</kbd> component and its <kbd>aspectType</kbd> is different from its own.</p>
<p>The <kbd>OnDrawGizmos</kbd> method draws lines based on the perspective field of view angle and viewing distance so that we can see the AI character's line of sight in the editor window during play testing. Attach this script to our AI character and be sure that the aspect type is set to <kbd>ENEMY</kbd>.</p>
<p>This method can be illustrated as follows:</p>
<pre>  void OnDrawGizmos()<br/>    {<br/>        if (playerTransform == null) <br/>        {<br/>            return;<br/>        }<br/><br/>        Debug.DrawLine(transform.position, playerTransform.position, Color.red);<br/><br/>        Vector3 frontRayPoint = transform.position + (transform.forward * viewDistance);<br/><br/>        //Approximate perspective visualization<br/>        Vector3 leftRayPoint = frontRayPoint;<br/>        leftRayPoint.x += fieldOfView * 0.5f;<br/><br/>        Vector3 rightRayPoint = frontRayPoint;<br/>        rightRayPoint.x -= fieldOfView * 0.5f;<br/><br/>        Debug.DrawLine(transform.position, frontRayPoint, Color.green);<br/>        Debug.DrawLine(transform.position, leftRayPoint, Color.green);<br/>        Debug.DrawLine(transform.position, rightRayPoint, Color.green);<br/>    }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Touching is believing</h1>
                </header>
            
            <article>
                
<p>The next sense we'll be implementing is <kbd>Touch.cs</kbd>, which triggers when the player tank entity is within a certain area near the AI entity. Our AI character has a box collider component and its <kbd>IsTrigger</kbd> flag is on.</p>
<p>We need to implement the <kbd>OnTriggerEnter</kbd> event, which will be called whenever another collider enters the collision area of this game object's collider. Since our tank entity also has a collider and rigid body components, collision events will be raised as soon as the colliders of the AI character and player tank collide.</p>
<div class="packt_tip">Unity provides two other trigger events besides <kbd>OnTriggerEnter</kbd>: <kbd>OnTriggerExit</kbd> and <kbd>OnTriggerStay</kbd>. Use these to detect when a collider leaves a trigger, and to fire off every frame that a collider is inside the trigger, respectively.</div>
<p>The code in the <kbd>Touch.cs</kbd> file is as follows:</p>
<pre>using UnityEngine;<br/><br/>public class Touch : Sense<br/>{<br/>    void OnTriggerEnter(Collider other)<br/>    {<br/>        Aspect aspect = other.GetComponent&lt;Aspect&gt;();<br/>        if (aspect != null)<br/>        {<br/>            //Check the aspect<br/>            if (aspect.aspectType != aspectName)<br/>            {<br/>                print("Enemy Touch Detected");<br/>            }<br/>        }<br/>    }<br/>}</pre>
<p>Our sample NPC and tank have  <kbd>BoxCollider</kbd> components on them already. The NPC has its sensor collider set to <kbd>IsTrigger = true</kbd> . If you're setting up the scene on your own, make sure you add the <kbd>BoxCollider</kbd> component yourself, and that it covers a wide enough area to trigger easily for testing purposes. Our trigger can be seen in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img height="152" width="261" src="assets/79522f53-2057-45db-a581-acae504d9d2f.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The collider around our player</div>
<p>The previous screenshot shows the box collider on our enemy AI that we'll use to trigger the touch sense event. In the following screenshot, we can see how our AI character is set up:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img height="449" width="370" src="assets/dbb978b2-789e-49da-b789-ab409d25467b.png"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The properties of our NPC</div>
<p>For demo purposes, we just print out that the enemy aspect has been detected by the touch sense, but in your own games, you can implement any events and logic that you want. This system ties in really nicely with other concepts covered in this book, such as states, which we learned about in <a href="9e338555-162c-4ed0-a519-035cfcea94ce.xhtml">Chapter 2</a><em>, Finite State Machines and You</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the results</h1>
                </header>
            
            <article>
                
<p>Hit play in the Unity editor and move the player tank near the wandering AI NPC by clicking on the ground to direct the tank to move to the clicked location. You should see the <span class="packt_screen">Enemy touch detected</span> message in the console log window whenever our AI character gets close to our player tank:</p>
<div class="CDPAlignCenter CDPAlign"><img height="277" width="447" src="assets/888ce9b1-b216-4115-9ede-f83fbdcdf7a2.png"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Our NPC and tank in action</div>
<p>The previous screenshot shows an AI agent with touch and perspective senses looking for another aspect. Move the player tank in front of the NPC, and you'll get the <span class="packt_screen">Enemy detected</span> message. If you go to the editor view while running the game, you should see the debug lines being rendered. This is because of the <kbd>OnDrawGizmos</kbd> method implemented in the perspective <kbd>Sense</kbd> class.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter introduced the concept of using sensors and implemented two distinct senses—perspective and touch—for our AI character. The sensory system is one component of the whole decision-making system. We can use the sensory system in combination with a behavior system to execute certain behaviors for certain senses. For example, we can use an FSM to change to <span class="packt_screen">Chase</span> and <span class="packt_screen">Attack</span> states from the <span class="packt_screen">Patrol</span> state once we have detected that there's an enemy within line of sight. We'll also cover how to apply behavior tree systems in <a href="8db41b31-be4b-432f-a68e-ef13e1f7e03b.xhtml">Chapter 6</a>, <em>Behavior Trees</em>.</p>
<p>In the next chapter, we'll be looking at popular pathfinding algorithms. We'll learn how to make our AI agent navigate complex environments using the ever-popular A* pathfinding algorithm, and even Unity's own <kbd>NavMesh</kbd> system.</p>


            </article>

            
        </section>
    </body></html>