<html><head></head><body>
        

                            
                    <h1 class="header-title">Finding Optimal Parameters</h1>
                
            
            
                
<p>In this chapter, we will use the open source package SwarmOps, version 4.0, to help you better understand how you can use this tool to find optimal parameters for your functions. You can get the latest version of SwarmOps from the following location: <a href="https://github.com/mattcolefla/SwarmOps">https://github.com/mattcolefla/SwarmOps</a>.</p>
<p>Once again, we must spend a little time on theory, where we will take you back to your academic days and lay a foundation so that we are all speaking the same language. It should be noted that SwarmOps is a highly research-oriented tool and should be used as such. We have worked hard to make this product open source, and the latest version has over 60 different optimization functions for you to use.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Fitness function</li>
<li>Constraints</li>
<li>Meta-optimization</li>
<li>Optimization methods</li>
<li>Parallelism</li>
</ul>
<p>Ready? Here we go!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>You will be required to have a basic knowledge of .NET development using Microsoft Visual Studio and C#. You will need to download the code for this chapter from the book's website: SwarmOps (<a href="https://github.com/mattcolefla/SwarmOps">https://github.com/mattcolefla/SwarmOps</a>).</p>
<p>Check out the following video to see Code in Action: <a href="http://bit.ly/2QPddLO">http://bit.ly/2QPddLO</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimization</h1>
                
            
            
                
<p>Solutions to some problems are not as cut and dry as <em>correct</em> or <em>incorrect</em>, but are rated in terms of quality. Such problems are known as <strong>optimization problems</strong> because the goal is to find the candidate solution with the best, that is, <em>optimal</em> quality.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">What is a fitness function?</h1>
                
            
            
                
<p>SwarmOps works for real-valued and single-objective optimization problems, that is, optimization problems that map candidate solutions from <img class="fm-editor-equation" src="img/27d5e4e0-0b6f-4c24-a070-c1926d03298c.png" style="width:0.92em;height:1.00em;"/>-dimensional real-valued spaces to one-dimensional real-valued spaces. Mathematically speaking, we consider optimization problems to be functions <img class="fm-editor-equation" src="img/ff972492-0e7b-4a6b-8c36-0f7d57f0b201.png" style="width:0.67em;height:1.33em;"/> of the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/f66bee59-ca43-4b53-b430-1122beddd22b.png" style="width:5.83em;height:1.25em;"/></p>
<p>In SwarmOps, it is assumed that <img class="fm-editor-equation" src="img/d71ae3ff-ae43-421d-a951-cb3ba8c1300f.png" style="width:0.83em;height:1.67em;"/> is a minimization problem, meaning that we are searching for the candidate solution <img class="fm-editor-equation" src="img/03c78311-96e5-484d-8851-42ab9d21e4d1.png" style="width:3.92em;height:1.17em;"/>with the smallest value <img class="fm-editor-equation" src="img/51747946-9f40-4185-9253-1cddfb223c83.png" style="width:2.17em;height:1.42em;"/>. Mathematically, this may be written as the following:</p>
<p>Find <img class="fm-editor-equation" src="img/154be667-72da-4d84-8f6f-cfd2742596be.png" style="width:0.92em;height:1.42em;"/>, such that <img class="fm-editor-equation" src="img/b9292a90-ca0f-42ca-bc9f-c02828a84941.png" style="width:12.08em;height:1.50em;"/>.</p>
<p>Typically, however, it is not possible to locate the exact optimum; we must be satisfied with a candidate solution of sufficient quality that is perhaps not quite optimal. In this chapter, we refer to the optimization problem <img class="fm-editor-equation" src="img/2a5bcabc-0829-4cc1-ad19-482a4789e4b3.png" style="width:0.67em;height:1.33em;"/> as the <kbd>fitness</kbd> function, but it is can also be known as the cost function, objective function, error function, quality measure, and so on. We may also refer to candidate solutions as positions, agents, or particles, and to all possible candidate solutions as the search-space.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Maximization</h1>
                
            
            
                
<p>SwarmOps can also be used with maximization problems. If <img class="fm-editor-equation" src="img/be3529dc-49fc-4781-8e46-82823aeddb0b.png" style="width:5.92em;height:1.08em;"/> is a maximization problem, then the equivalent minimization problem is as follows: <img class="fm-editor-equation" src="img/93fe06a1-e72d-413d-b9e4-2036f0e02bcc.png" style="width:7.08em;height:1.42em;"/>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Gradient-based optimization</h1>
                
            
            
                
<p>The classic way of optimizing a fitness function <img class="fm-editor-equation" src="img/f5b3e719-abbc-4f04-a45a-8b09794acaa6.png" style="width:0.75em;height:1.50em;"/> is to first deduce its gradient, that is, <img class="fm-editor-equation" src="img/cddb2fb9-9dab-4bfc-a4e3-a6bbdd529ff0.png" style="width:7.33em;height:1.25em;"/>, which consists of the partial differentials of <img class="fm-editor-equation" src="img/5e346118-7080-4de5-afc1-761fde7db44d.png" style="width:0.67em;height:1.33em;"/>, that is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/e951e83d-2148-426c-89ff-0a6dec619e63.png" style="width:13.58em;height:2.92em;"/></p>
<p>The gradient is then followed iteratively in the direction of the steepest descent; a quasi-Newton optimizer can also be used if necessary. This optimizer requires that not only for the fitness function <img class="fm-editor-equation" src="img/ddc2311e-3af9-4850-bf5d-4228aea941f6.png" style="width:0.67em;height:1.33em;"/> be differentiable, but time and patience as well. This is because the gradient can be very laborious to derive, and the execution can be very time-consuming.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Heuristic optimization</h1>
                
            
            
                
<p>An alternative to gradient-based optimization methods is to let the optimization be guided solely by fitness values. This kind of optimization has no explicit knowledge of how the fitness landscape looks, but merely considers the fitness function to be a black box that takes candidate solutions as input and produces a fitness value as output. This is known in this chapter as Derivate-free optimization, direct search, heuristic optimization, meta-heuristics, black-box optimization, and so on. We will use these terms a lot!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Constraints</h1>
                
            
            
                
<p>Constraints split the search-space into regions of feasible and infeasible candidate solutions. For instance, an engineering problem could have a mathematical model that should be optimized, but producing the solution in the real world puts some constraints on what is feasible. There are different ways of supporting and handling constraints in heuristic optimization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Boundaries</h1>
                
            
            
                
<p>A simple form of constraint is search-space boundaries. Instead of letting <img class="fm-editor-equation" src="img/4b827c98-c3b5-4f2a-a2e3-22a8f3cd3d14.png" style="width:0.58em;height:1.17em;"/> map from the entire <img class="fm-editor-equation" src="img/19fe84b2-bee4-4df9-9731-e4c5979c7344.png" style="width:0.92em;height:1.00em;"/>-dimensional real-valued space, it is often practical to use only a part of this vast search-space. The lower and upper boundaries that constitute the search-space are denoted as <img class="fm-editor-equation" src="img/fd613b81-7202-46fa-a065-f4a9e7d46c66.png" style="width:1.25em;height:1.92em;"/> and <img class="fm-editor-equation" src="img/bc508a18-3f8e-460f-994a-a9a844344e7e.png" style="width:1.50em;height:1.58em;"/>, so the fitness function is of the following form:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a9c3f595-e542-4b73-a111-d269ef1cfee6.png" style="width:8.83em;height:2.25em;"/></p>
<p>Such boundaries are typically enforced in optimization methods by moving candidate solutions back to the boundary value if they have exceeded the boundaries. This is the default type of constraint available in SwarmOps.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Penalty functions</h1>
                
            
            
                
<p>More complicated constraints are supported transparently by any heuristic optimizer by penalizing infeasible candidate solutions, that is, by adding a penalty function to the fitness function. Examples can be found in the penalized benchmark problems section of the SwarmOps source code.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">General constraints</h1>
                
            
            
                
<p>SwarmOps supports general constraints by taking feasibility (constraint satisfaction) into account when comparing candidate solutions. Normally, we determine whether candidate solution <img class="fm-editor-equation" src="img/b7d1afed-2b91-4ede-92ad-7d844093c95c.png" style="width:0.83em;height:1.25em;"/> is better than <img class="fm-editor-equation" src="img/6815653e-4aad-427b-bc3c-42cd1411241f.png" style="width:0.75em;height:1.50em;"/> by comparing their fitness with <img class="fm-editor-equation" src="img/0c872fc8-9935-4e18-bc4e-251e8c91cece.png" style="width:5.75em;height:1.33em;"/>, but it is also possible to take feasibility into account. Feasibility is a Boolean; either a candidate solution is feasible or it is infeasible. The comparison operator is as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-705 image-border" src="img/06b8c937-53e5-41e6-98f1-721e4ec3c124.png" style="width:28.92em;height:11.00em;"/></p>
<p>Note in the preceding diagram that the actual implementation of this comparison is simplified somewhat. Also note that when <img class="fm-editor-equation" src="img/0dd53fa0-41cc-4d49-bfe8-cb86d4e72111.png" style="width:0.83em;height:1.67em;"/> is feasible and <img class="fm-editor-equation" src="img/606727bc-e8d5-4fcc-8c0f-e8b40205a94f.png" style="width:0.92em;height:1.42em;"/> is infeasible, their fitness need not be computed. This is because <img class="fm-editor-equation" src="img/04942206-ca8b-4c51-893f-41b045d9c844.png" style="width:0.92em;height:1.42em;"/> is worse than <img class="fm-editor-equation" src="img/7dd24064-e485-4ab1-a789-218fe581c80c.png" style="width:0.83em;height:1.67em;"/> due to their mutual feasibility. This is used in the implementation to avoid fitness computations when possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Constrained optimization phases</h1>
                
            
            
                
<p>Using the earlier comparison operator means that optimization has two phases. First, the optimizer will likely only find infeasible candidate solutions, so it optimizes the fitness of infeasible solutions. Then, at some point, the optimizer hopefully discovers a feasible candidate solution; regardless of its fitness, it will then become the best-found solution of the optimizer and will form the basis of the further search. This is essentially the optimization of a feasible solution's fitness.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Constrained optimization difficulties</h1>
                
            
            
                
<p>While SwarmOps gives you the ability to implement any constraint imaginable, constraints themselves will make it increasingly difficult for the optimizer to find feasibly optimal solutions because constraints narrow the feasible regions of the search-space. You should therefore also narrow the initialization and search-space boundaries to be as close to the feasible region as possible.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Implementation</h1>
                
            
            
                
<p>There are two methods in the <kbd>problem</kbd> class where you can implement constraints; they are as follows:</p>
<ul>
<li><kbd>EnforceConstraints()</kbd> allows you to make repairs to a candidate solution before its feasibility and fitness are evaluated. For example, when search-space boundaries are used as constraints then the repairing would consist of moving candidate solutions back between boundaries if they were overstepped. This is done by default.</li>
<li><kbd>Feasible()</kbd> evaluates and returns the feasibility of a candidate solution without altering it.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Meta-optimization</h1>
                
            
            
                
<p>Optimization methods usually have several user-defined parameters that govern the behavior and efficacy of the optimization method. These are called the optimizer's behavioral or control parameters. Finding a good choice of these behavioral parameters has previously been done manually by hand-tuning, and sometimes even by using coarse mathematical analysis. It has also become a common belief among researchers that behavioral parameters can be adapted during optimization to improve overall optimization performance; however, this has been demonstrated to be mostly unlikely. Tuning behavioral parameters can be considered an optimization problem and hence can be solved by an overlaid optimization method. This is known here as meta-optimization, but is also known in the chapter as meta-evolution, super-optimization, parameter calibration, and so on. The success of SwarmOps when doing meta-optimization relies mainly on the following three factors:</p>
<ol>
<li>SwarmOps features an optimization method that is particularly suitable as the overlaid meta-optimizer because it quickly discovers well-performing behavioral parameters (this is the LUS method described in this chapter).</li>
<li>SwarmOps employs a simple technique for reducing computational time called pre-emptive fitness evaluation.</li>
<li>SwarmOps uses the same function-interface for both optimization problems and optimization methods. Several scientific publications use SwarmOps for meta-optimization and have more elaborate descriptions than those given here, as well as having literature surveys and experimental results. The concept of meta-optimization can be illustrated schematically as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-703 image-border" src="img/728cfbba-e247-40f4-949f-9c6f2ca0b3fe.png" style="width:19.50em;height:19.75em;"/></p>
<p>In the preceding diagram, the optimizer whose behavioral parameters are to be tuned is taken to the DE method, which we will look at later on in this chapter. The SwarmOps framework allows for parameters to be tuned regarding multiple optimization problems, which is sometimes necessary to make the performance of the behavioral parameters respond better to more general problems.</p>
<p>In the preceding example, the DE parameters are tuned for two specific problems.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Fitness normalization</h1>
                
            
            
                
<p>Fitness functions must be non-negative to work properly with meta-optimization in SwarmOps. This is because pre-emptive fitness evaluation works by summing fitness values for several optimization runs and aborting the summation when the fitness sum becomes worse than that needed for the new candidate solution to be considered an improvement. This means that fitness values must be non-negative, so the fitness sum is only able to grow worse and the evaluation can thus be aborted safely. SwarmOps for C# does this normalization automatically, provided you accurately implement the <strong>MinFitness</strong> field of the <kbd>problem</kbd> class. For example, you may have a fitness function <img class="fm-editor-equation" src="img/e06c6075-b237-4bd1-9991-78e53f0daf3f.png" style="width:0.75em;height:1.50em;"/> which maps to, for example, <img class="fm-editor-equation" src="img/6395813e-55ae-4fe8-8fad-868fc3b68984.png" style="width:3.67em;height:1.33em;"/>. In this case, you would have to set <strong>MinFitness</strong> to <img class="fm-editor-equation" src="img/c1279116-97cc-4e9c-b33d-18fd0264cc45.png" style="width:1.58em;height:1.08em;"/>. It is best to make <strong>MinFitness</strong> accurate so that <img class="fm-editor-equation" src="img/b5f6c6b2-1db6-4603-b18f-d789f786eadc.png" style="width:11.92em;height:1.33em;"/> for the optimum <img class="fm-editor-equation" src="img/4c08bc18-d933-418f-bcfe-9e672b621b2d.png" style="width:0.83em;height:1.25em;"/>, that is, <strong>MinFitness</strong> should be the fitness of the optimum. You should be able to estimate a lower fitness boundary for most real-world problems, and if you are unsure what the theoretical boundary value is, you may choose some boundary fitness value of ample—but not extreme—magnitude.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Fitness weights for multiple problems</h1>
                
            
            
                
<p>If you are using multiple problems in meta-optimization, you may need to experiment with weights on each problem to make their influence on the meta-optimization process more equal.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Advice</h1>
                
            
            
                
<p>The <kbd>LUS</kbd> method is generally recommended as the overlaid meta-optimizer. The tutorial source code contains suggestions for experimental settings that have been found to work well. It is best if you perform meta-optimization regarding the problems you are ultimately going to use the optimization method for. However, if your fitness function is very expensive to evaluate, then you may try and resort to using benchmark problems as a temporary replacement when meta-optimizing the behavioral parameters of your optimizer—provided you use multiple benchmark problems and the optimization settings are the same as those used in a real-world application. In other words, you should use benchmark problems of similar dimensionality and with a similar number of optimization iterations to what you would use for the actual problem you will ultimately optimize.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Constraints and meta-optimization</h1>
                
            
            
                
<p>Two issues regarding constraints in meta-optimization should be mentioned; they are as follows:</p>
<ul>
<li>Constraints can be made on an optimizer's control parameters in the same manner as for an optimization problem by implementing the <kbd>EnforceConstraints()</kbd> and <kbd>Feasible()</kbd> methods in the optimizer's class. This means the meta-optimizer will search for control parameters that are feasibly optimal, allowing you to search for control parameters that meet certain criteria; for example, they have certain relationships with each other, such as one parameter being smaller than the other, and so on. See the source code of the MOL optimizer for an example of this.</li>
<li>Constraint satisfaction is ignored when determining how well an optimizer performs in making up the meta-fitness measure. This is an open research topic, but experiments suggest that an optimizer's control parameters should be meta-optimized for unconstrained problems. This will also yield good performance on constrained problems.</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Meta-meta-optimization</h1>
                
            
            
                
<p>When using meta-optimization to find the best performing parameters of an optimizer, one may naturally wonder what the best performing parameters for the meta-optimizer itself are. It makes good sense to find the best meta-optimizer if one is going to use it often. The best parameters for the meta-optimizer can be found by employing yet another layer of optimization, which may be termed meta-meta-optimization.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimization methods</h1>
                
            
            
                
<p>This section will give you a brief description of the optimization methods that are supplied with SwarmOps and some recommendations for their use.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Choosing an optimizer</h1>
                
            
            
                
<p>When faced with a new optimization problem, the first optimizer you may want to try is the <kbd>PS</kbd> method, which is often sufficient and has the advantage of converging (or stagnating) very quickly. In addition, <kbd>PS</kbd> does not have any behavioral parameters that need tuning, so it either works or it doesn't. If the <kbd>PS</kbd> method fails at optimizing your problem, you may want to try the <kbd>LUS</kbd> method. You may need to run both <kbd>PS</kbd> and <kbd>LUS</kbd> several times as they may converge to sub-optimal solutions. If <kbd>PS</kbd> and <kbd>LUS</kbd> both fail, you may try the <kbd>DE</kbd>, <kbd>MOL</kbd>, or <kbd>PSO</kbd> methods and experiment with their behavioral parameters.</p>
<p>As a rule of thumb, the <kbd>PS</kbd> and <kbd>LUS</kbd> methods stagnate rather quickly, say, after <img class="fm-editor-equation" src="img/9467ab0f-45fe-4278-bd10-7cfb6072cdae.png" style="width:2.92em;height:1.33em;"/> iterations, where <img class="fm-editor-equation" src="img/47a20a43-ff5e-4135-9011-d2742eb4ef34.png" style="width:0.92em;height:1.00em;"/> is the dimensionality of the search-space. On the other hand, the <kbd>DE</kbd>, <kbd>MOL</kbd>, and <kbd>PSO</kbd> methods require substantially more iterations, say, <img class="fm-editor-equation" src="img/b449793d-94f2-4c94-baea-87d7a69b9c6b.png" style="width:3.67em;height:1.33em;"/> or <img class="fm-editor-equation" src="img/a0a7fe71-44bf-4320-b818-da90e75e0091.png" style="width:4.50em;height:1.33em;"/>, and sometimes even more than that.</p>
<p>If these optimizers fail, you either need to tune their behavioral parameters using meta-optimization or use another optimizer altogether.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Gradient descent (GD)</h1>
                
            
            
                
<p>A classic way of minimizing a fitness function (<img class="fm-editor-equation" src="img/94019cf0-3050-4743-9987-820903b35c25.png" style="width:5.00em;height:1.08em;"/>) is to repeatedly follow the gradient in the direction of steepest descent. The gradient function <img class="fm-editor-equation" src="img/6097ce3d-ecec-44d0-88ad-d1a08f6e8028.png" style="width:6.92em;height:1.17em;"/> is defined as the vector of the partial differentials of <img class="fm-editor-equation" src="img/91947301-c391-4a50-b1b1-d3b790d461f8.png" style="width:0.83em;height:1.67em;"/>, which is denoted as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1d146b75-a3cf-410f-98ef-707dab8373d4.png" style="width:14.00em;height:3.00em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works</h1>
                
            
            
                
<p>The position <img class="fm-editor-equation" src="img/a4e702b4-42d1-441f-bb95-a84afbda5653.png" style="width:0.92em;height:1.42em;"/> is first chosen randomly from the search-space and then updated iteratively according to the following formula, regardless of fitness improvement:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/1f01f96e-a274-489c-90ad-2b3f1e17f5d8.png" style="width:12.00em;height:3.42em;"/></p>
<p>As shown in the preceding formula, <img class="fm-editor-equation" src="img/93217723-328b-470a-8e96-76d9ecc19067.png" style="width:2.83em;height:1.08em;"/> is the step-size. When <img class="fm-editor-equation" src="img/4425b020-40c2-4b94-b042-ffca9b82ce33.png" style="width:0.75em;height:1.50em;"/> is a minimization problem, the descent direction is followed, that is, we subtract the gradient from the current position instead of adding it—as we would have done for ascending a maximization problem.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Drawbacks</h1>
                
            
            
                
<p>The <kbd>GD</kbd> method has some drawbacks, namely that it requires the gradient <img class="fm-editor-equation" src="img/a272cd38-a583-4582-9587-029852000720.png" style="width:1.50em;height:1.17em;"/> to be defined. The gradient may also be expensive to compute, and <kbd>GD</kbd> may approach the optimum too slowly.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Pattern Search (PS)</h1>
                
            
            
                
<p>The optimization method known as <strong>Pattern Search</strong> (<strong>PS</strong>) was originally by Fermi and Metropolis, as described in [6], and is a similar method used by Hooke and Jeeves [7]. The implementation presented here is the variant from [4].</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works</h1>
                
            
            
                
<p><kbd>PS</kbd> uses one agent or position in the search-space that is being moved around. Let the position be denoted as <img class="fm-editor-equation" src="img/1c97efff-4502-4505-a525-9ef682e9cf8c.png" style="width:3.92em;height:1.17em;"/>, which is initially picked at random from the entire search-space. The initial sampling range is the entire search-space: <img class="fm-editor-equation" src="img/97452731-07d3-4a41-be58-f9e5253e99b4.png" style="width:7.17em;height:1.75em;"/>. The potential new position is denoted as <img class="fm-editor-equation" src="img/383a60a2-670d-4ea2-8a1d-3085a8cbdf0a.png" style="width:0.75em;height:1.50em;"/> and is sampled as follows.</p>
<p>First, pick an index (<img class="fm-editor-equation" src="img/1f82aee5-8112-4944-bac9-1ae0cbe6ee1e.png" style="width:6.92em;height:1.25em;"/>) at random and let <img class="fm-editor-equation" src="img/30e356f6-bda7-42da-84bb-84956944d5ae.png" style="width:7.42em;height:1.17em;"/> and <img class="fm-editor-equation" src="img/a629f2c3-776e-4e6c-98a4-0c6093e7f4bf.png" style="width:3.83em;height:1.08em;"/> for all <img class="fm-editor-equation" src="img/b04cd7cf-0359-4a25-8dae-87061030f16a.png" style="width:2.75em;height:1.17em;"/>. If <img class="fm-editor-equation" src="img/f2ba7f3f-82db-476d-a385-cca9ae94133e.png" style="width:0.67em;height:1.33em;"/> improves on the fitness of <img class="fm-editor-equation" src="img/3b3a310c-23de-49fe-8f2d-824b19f06300.png" style="width:0.83em;height:1.25em;"/> then move to <img class="fm-editor-equation" src="img/9a444349-8dc2-4be1-afc1-15b5c7eb9d11.png" style="width:0.58em;height:1.17em;"/>. Otherwise, halve and reverse the sampling range for the <img class="fm-editor-equation" src="img/75aab672-6214-4521-95a8-92e4894092ff.png" style="width:1.67em;height:1.17em;"/> dimension with <img class="fm-editor-equation" src="img/3e7a1210-2df4-4af2-ae36-e8886b578f52.png" style="width:6.83em;height:1.42em;"/>. Repeat this several times.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Local Unimodal Sampling (LUS)</h1>
                
            
            
                
<p>The LUS optimization method performs local sampling by moving a single agent around the search-space to decrease the sampling range during optimization. The <kbd>LUS</kbd> method was presented in [4] [8].</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works</h1>
                
            
            
                
<p>The agent's current position is denoted as <img class="fm-editor-equation" src="img/3a07dd2f-9346-48e2-aa18-8b07f1c4c4c4.png" style="width:3.92em;height:1.17em;"/> and is initially picked at random from the entire search-space. The potential new position is denoted as <img class="fm-editor-equation" src="img/b60d3ad7-a646-4019-800c-d326b0a99c87.png" style="width:0.67em;height:1.33em;"/> and is sampled from the neighborhood of <img class="fm-editor-equation" src="img/11ba00fb-61bf-4c7c-a242-8ea43cca6fbf.png" style="width:0.92em;height:1.42em;"/> by letting <img class="fm-editor-equation" src="img/adb1fcb8-215c-4c9a-8398-9967987c6665.png" style="width:5.17em;height:1.33em;"/>, where <img class="fm-editor-equation" src="img/44b6af3f-efba-4a88-9092-635930f642b0.png" style="width:6.08em;height:1.42em;"/> is a random vector picked uniformly from the range <img class="fm-editor-equation" src="img/f6d7b8de-7c30-490c-82e0-fcf9ee1faaa6.png" style="width:3.50em;height:1.50em;"/>, which is initially <img class="fm-editor-equation" src="img/f72307cb-493c-4fa1-97fe-4d16e53a2e6a.png" style="width:6.58em;height:1.58em;"/>. In other words, the full range of the entire search-space is defined by its upper boundaries, <img class="fm-editor-equation" src="img/e9850d5a-6c90-4d4a-ace5-f9ea998266c1.png" style="width:1.50em;height:1.58em;"/>, and its lower boundaries, <img class="fm-editor-equation" src="img/e4df128a-35c5-4686-83be-90ce4e6573b4.png" style="width:1.25em;height:1.42em;"/>. <kbd>LUS</kbd> moves from position <img class="fm-editor-equation" src="img/48b50b61-ffce-4b26-b429-e92ef7bb129a.png" style="width:0.83em;height:1.25em;"/> to position <img class="fm-editor-equation" src="img/703decc9-3c80-4ccb-91d7-dae8b3ceff5f.png" style="width:0.58em;height:1.17em;"/> in the event of any improvement in the fitness. Upon each failure for <img class="fm-editor-equation" src="img/44ccd04b-5c5e-4684-8ba7-aae3fe01ab0f.png" style="width:0.75em;height:1.50em;"/> to improve on the fitness of <img class="fm-editor-equation" src="img/fd3d5809-f440-46e1-9203-1826d1aa23b2.png" style="width:0.75em;height:1.17em;"/>, the sampling range is decreased by multiplication with a factor of <img class="fm-editor-equation" src="img/b654a3b2-a26e-40b6-b22b-ede4bdbc8741.png" style="width:0.67em;height:1.00em;"/>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/83931e90-c652-4733-804c-0ac21eb7fd09.png" style="width:3.67em;height:1.25em;"/></p>
<p>Here, the decrease factor <img class="fm-editor-equation" src="img/c3aede74-9165-4ab2-9e02-3d275b9b7276.png" style="width:0.75em;height:1.17em;"/> is then defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/b05528ec-893c-422a-88e2-0ac036b8e206.png" style="width:9.58em;height:2.83em;"/></p>
<p>The preceding formula denotes <img class="fm-editor-equation" src="img/2ca25f34-c60d-46ab-9506-6dd0085e3443.png" style="width:0.92em;height:1.00em;"/> as the dimensionality of the search-space and <img class="fm-editor-equation" src="img/e8595bdc-ea0d-4e6c-a28e-34893412347d.png" style="width:0.58em;height:0.92em;"/> as a user-defined parameter used to adjust the rate of sampling-range decrease. A value of <img class="fm-editor-equation" src="img/77cb6838-49db-4845-8b94-de4dd8e6926d.png" style="width:2.08em;height:0.92em;"/> has been found to work well for many optimization problems.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Differential Evolution (DE)</h1>
                
            
            
                
<p>The multi-agent optimization method known as <strong>Differential Evolution</strong> (DE) was originally devised by Storn and Price [9]. Many DE variants exist and a simple one is implemented in the DE class. Several different DE variants are available through the DE Suite and JDE classes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works</h1>
                
            
            
                
<p><kbd>DE</kbd> uses a population of agents. Let <img class="fm-editor-equation" src="img/84c40d14-17d8-4b04-8b86-6b56739a2e33.png" style="width:0.92em;height:1.42em;"/> denote the position of an agent being updated and which has been picked at random from the entire population. Let <img class="fm-editor-equation" src="img/c48c1a1d-9f92-4b3b-ba0e-69da34ad03fa.png" style="width:7.33em;height:1.25em;"/> be its new potential position computed as follows (this is the so-called <strong>DE/rand/1/bin variant</strong>):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6812e471-12ab-4405-98c7-e78ee13f3c5f.png" style="width:19.58em;height:2.92em;"/></p>
<p>Here, the vectors <img class="fm-editor-equation" src="img/ae2baaf5-4b25-4f9f-9d02-e386675886bb.png" style="width:0.75em;height:1.25em;"/>, <img class="fm-editor-equation" src="img/f558b5bd-2d7a-4c81-96eb-ffb62f909916.png" style="width:0.58em;height:1.33em;"/>, and <img class="fm-editor-equation" src="img/a488252c-4041-4699-abc4-82172389fb1e.png" style="width:0.67em;height:1.17em;"/> are the positions of distinct and randomly-picked agents from the population. The index <img class="fm-editor-equation" src="img/1f764f95-e451-412a-b593-6550ec827609.png" style="width:6.50em;height:1.17em;"/> is randomly-picked and <img class="fm-editor-equation" src="img/aa14b88e-bc36-43e5-941f-9d007c9b0f12.png" style="width:5.42em;height:1.25em;"/> is also picked randomly for each dimension, <img class="fm-editor-equation" src="img/804bffd2-51f3-4ecd-b6a9-b3830df85450.png" style="width:0.50em;height:1.33em;"/>. A move is made to the new position <img class="fm-editor-equation" src="img/3bd72ce6-3032-434e-9ad5-c209d3ef88d9.png" style="width:0.83em;height:1.67em;"/> if it improves on the fitness of <img class="fm-editor-equation" src="img/75b48837-90f8-4865-961c-060377b7e5a8.png" style="width:0.92em;height:1.42em;"/>. The user-defined parameters consist of the differential weight <img class="fm-editor-equation" src="img/836c3c67-bddf-4a17-aab4-b3bceea3bee9.png" style="width:0.92em;height:1.08em;"/>, the crossover probability <img class="fm-editor-equation" src="img/c376bc2b-a065-4165-b678-3ced4ae106bb.png" style="width:1.75em;height:1.08em;"/>, and the population-size <img class="fm-editor-equation" src="img/57e01e44-1266-402d-a06f-0ddf27caddd4.png" style="width:1.75em;height:0.92em;"/>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Particle Swarm Optimization (PSO)</h1>
                
            
            
                
<p>The optimization method known as <strong>Particle Swarm Optimization</strong> (<strong>PSO</strong>) was originally devised by Kennedy, Eberhart, and Shi [10] [11]. It works by having a swarm of candidate solutions called particles, with each particle having a velocity that is updated recurrently and added to the particle's current position to move it to a new one.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works</h1>
                
            
            
                
<p>Let <img class="fm-editor-equation" src="img/e5496bcc-ae8f-41b5-aaa1-58182c96dc4e.png" style="width:0.92em;height:1.42em;"/> denote the current position of a particle from the swarm. The particle's velocity <img class="fm-editor-equation" src="img/418d63a9-09b6-4d2c-b685-b27fa76ef6ad.png" style="width:0.75em;height:1.42em;"/> is then updated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/6b429bb7-491d-4aac-a063-d2f865b17f73.png" style="width:21.25em;height:1.58em;"/></p>
<p>Here, the user-defined parameter <img class="fm-editor-equation" src="img/d9fc81cb-cc09-4278-862c-a4aa77557643.png" style="width:1.00em;height:1.00em;"/> is called the inertia weight and the user-defined parameters <img class="fm-editor-equation" src="img/4e2c67e8-3aa3-44c5-8017-65b1661b373b.png" style="width:1.67em;height:1.75em;"/> and <img class="fm-editor-equation" src="img/5999e90e-9f04-4f64-893f-e8d47324ecbe.png" style="width:1.58em;height:1.75em;"/> are weights on the attraction toward the particle's own best-known position, <img class="fm-editor-equation" src="img/3f94cc2b-8fe8-4891-8569-23ea42d20bde.png" style="width:0.67em;height:1.17em;"/>, and the swarm's best-known position, <img class="fm-editor-equation" src="img/fd105a0a-c96b-4f8d-9f4d-8ee65e2edfec.png" style="width:0.58em;height:1.33em;"/>. These are also weighted by the random numbers <img class="fm-editor-equation" src="img/7236df76-a421-4502-9307-bb9b42bbb7cd.png" style="width:8.58em;height:1.50em;"/>. In addition to this, the user also determines the swarm-size, <img class="fm-editor-equation" src="img/1ccff208-31e0-4de6-a54d-9d1b8dac1f3d.png" style="width:0.83em;height:1.25em;"/>. In the SwarmOps implementation, the velocity is bounded to the full range of the search-space, so an agent cannot move further than one search space boundary to the other in a single move.</p>
<p>Once the agent's velocity has been computed it is added to the agent's position, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="img/a0293a8a-57f7-4f10-8f0e-1aad6e37eb51.png" style="width:6.33em;height:1.33em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Many Optimizing Liaisons (MOL)</h1>
                
            
            
                
<p>A simplification of PSO is called <strong>Many Optimizing Liaisons</strong> (<strong>MOL</strong>) and was originally suggested by Kennedy [12], who called it the <em>Social Only</em> PSO. The name MOL is used in [5], where more thorough studies were made. MOL differs from PSO in that it eliminates the particle's best-known position, <img class="fm-editor-equation" src="img/91413f97-6cc3-4be2-9d50-6f384ac4cfe2.png" style="width:0.75em;height:1.33em;"/>. This has been found to improve performance on some problems and makes it easier to tune behavioral parameters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Mesh (MESH)</h1>
                
            
            
                
<p>Fitness can be computed at regular intervals of the search-space using the <kbd>MESH</kbd> method. For increasing search-space dimensionality, this incurs an exponentially increasing number of mesh-points to retain a similar interval size. This phenomenon is what is known as the Curse of Dimensionality. The <kbd>MESH</kbd> method is used as any other optimization method in SwarmOps is and will indeed return the mesh-point found to have the best fitness as its solution. The quality of this solution will depend on how coarse or fine the mesh is. The <kbd>MESH</kbd> method is mostly used to make plots of the fitness landscape for simpler optimization problems, or for studying how different choices of behavioral parameters influence an optimization method's performance, that is, how the meta-fitness landscape looks.</p>
<p>The <kbd>MESH</kbd> method is not intended to be used as an optimizer.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parallelism</h1>
                
            
            
                
<p>Computers with multiple processing units are becoming increasingly popular and there are different ways of using this parallelism.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parallelizing the optimization problem </h1>
                
            
            
                
<p>Some optimization problems can be parallelized internally. The advantage of this is that all optimization methods in SwarmOps can be used without modification. The disadvantage is that each optimization problem must be parallelized, and this process does not take advantage of the natural parallel structure of population-based optimizers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Parallel optimization methods</h1>
                
            
            
                
<p>SwarmOps provides parallelized versions of the <kbd>DE</kbd>, <kbd>PSO</kbd>, and <kbd>MOL</kbd> methods, all of which merely assume that the implementation of the fitness function is thread-safe. These parallelized optimizers are best suited for fitness functions that are time-consuming to compute, otherwise the parallelization overhead cancels out the gain.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Necessary parameter tuning</h1>
                
            
            
                
<p>Parallel optimizer variants are implemented somewhat differently from their sequential versions. The typical way of parallelizing a multi-agent optimizer is to maintain and update the population of agents on one execution thread and then distribute only the computation of the fitness to multiple execution threads. This makes it easier to synchronize access to the data. However, this also means the entire population must be processed before improvements can become effective and be used in the computation of new candidate solutions. This changes the dynamic behavior of the optimizer and means it requires different behavioral parameters to work well, which may not necessarily work as well as the optimizer's sequential version.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">And finally, the code</h1>
                
            
            
                
<p>Assuming you have already downloaded the code we described at the beginning of the chapter, let's now take a look at what's happening. To start, let's open the <kbd>TestParallelMetaBenchmarks</kbd> project and open the <kbd>main.cs</kbd> file. This is the file we will be working with for the following code.</p>
<p>First, we need to create some very important variables which will become settings for the optimization layer. We have commented each so that you know what they are for, shown as follows:</p>
<pre>// Set this close to 50 and a multiple of the number of processors, e.g. 8.<br/>static readonly int NumRuns = 64;<br/>// The total dimensions.<br/>static readonly int Dim = 5;<br/>// The dimension factor.<br/>static readonly int DimFactor = 2000;<br/>// The total number of times we will loop to determine optimal parameters.<br/>static readonly int NumIterations = DimFactor * Dim;</pre>
<p>Next, we are going to create our optimizer. There are several optimizers included with SwarmOps, but for our purposes we will use the MOL optimizer. <strong>MOL</strong> stands for <strong>Many Optimizing Liaisons</strong>, which is devised as a simplification to the original Particle Swarm Optimization method from Eberhart et al [1][2]. The Many Optimizing Liaisons method does not have any attraction to the particles' own best-known position, and the algorithm also randomly selects which particle to update instead of iterating over the entire swarm. It is similar to the Social Only Particle Swarm Optimization suggested by Kennedy [3] and was studied more thoroughly by Pedersen et al [4], who found that it can outperform the standard Particle Swarm Optimization approach and has more easily-tunable control parameters. Whew, that was a mouthful, wasn't it?</p>
<pre>// The optimizer whose control parameters are to be tuned.<br/>static Optimizer Optimizer = new MOL();</pre>
<p>Next is the problem(s) that we want to optimize. You can choose to have one or multiple problems solved at the same time, but it is often easier to solve one optimization tuning problem at a time.</p>
<p>The optimizer is having its control parameters tuned to work well on the included problem(s), shown as follows. The numbers are the weights that signify the mutual importance of the problems in tuning. The higher the weight, the more important it is, as shown in the following code:</p>
<pre>static WeightedProblem[] WeightedProblems = new WeightedProblem[]<br/>{<br/>new WeightedProblem(1.0, new Sphere(Dim, NumIterations)),<br/>};<br/>Next we have our settings for the meta-optimization layer.<br/>static readonly int MetaNumRuns = 5;<br/>static readonly int MetaDim = Optimizer.Dimensionality;<br/>static readonly int MetaDimFactor = 20;<br/>static readonly int MetaNumIterations = MetaDimFactor * MetaDim;</pre>
<p>The meta-fitness aspect consists of computing optimization performance for the problems we listed over several optimization runs and summing the results. For ease of use, we wrap the optimizer in a <kbd>MetaFitness</kbd> object which takes care of this for us, as follows:</p>
<pre>static SwarmOps.Optimizers.Parallel.MetaFitness MetaFitness = new SwarmOps.Optimizers.Parallel.MetaFitness(Optimizer, WeightedProblems, NumRuns, MetaNumIterations);</pre>
<p>Now we need to create out meta-optimizer object, as shown in the following snippet. For this, we will use the <strong>Local Unimodal Sampling</strong> (<strong>LUS</strong>) optimizer originally created by Pedersen 1. This object does local sampling with an exponential deduction of the sampling range. It works well for many optimization problems, especially when only short runs are used or allowed. It is particularly well-suited as the overlaying meta-optimizer when tuning parameters for another optimizer:</p>
<pre>static Optimizer MetaOptimizer = new LUS(LogSolutions);</pre>
<p>Finally, we will wrap the meta-optimizer in a <kbd>Statistics</kbd> object to log our results. We then repeat a number of meta-optimization runs using the <kbd>MetaRepeat</kbd> object, shown as follows:</p>
<pre>static readonly bool StatisticsOnlyFeasible = true;<br/>static Statistics Statistics = new Statistics(MetaOptimizer, StatisticsOnlyFeasible);<br/>static Repeat MetaRepeat = new RepeatMin(Statistics, MetaNumRuns);</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Performing meta-optimization</h1>
                
            
            
                
<p>If you look at the project, the main method in our optimizer appears to be a large method that performs the meta-optimization run, but instead it only takes the following line of code:</p>
<pre>double fitness = MetaRepeat.Fitness(MetaParameters);</pre>
<p>That's it! Everything else involves logging and printing results and information to the user.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Computing fitness</h1>
                
            
            
                
<p>The next block of code that we should look at is how we calculate our solution. Our main loop calls our fitness function as follows:</p>
<pre><strong>Statistics.Compute();</strong></pre>
<p>Now let's dive into the <kbd>Fitness</kbd> function. For ease, we have placed the entire function in the following snippet. We will dissect each line relative to its importance in the function. Our ultimate objective here is to compute the meta-fitness measure by passing the parameters to our optimizer. We perform optimization runs on the array of problem(s) until the fitness exceeds the <kbd>fitnessLimit</kbd> parameter:</p>
<pre>public override double Fitness(double[] parameters, double fitnessLimit)<br/>{<br/>double <strong>fitnessSum</strong> = 0;<br/>// Iterate over the problems.<br/>for (int <strong>i</strong> = 0; <strong>i</strong> &lt; ProblemIndex.Count &amp;&amp; <strong>fitnessSum</strong>&lt;fitnessLimit; <strong>i</strong>++)<br/>{<br/>// Assign the problem to the optimizer.<br/>Optimizer.Problem = ProblemIndex.GetProblem(<strong>i</strong>);<br/>// Get the weight associated with this problem.<br/>double weight = ProblemIndex.GetWeight(<strong>i</strong>);<br/>// Use another fitness summation because we need to keep<br/>// track of the performance on each problem.<br/>double <strong>fitnessSumInner</strong> = 0;<br/>// Perform a number of optimization runs.<br/>for (int <strong>j</strong> = 0; <strong>j</strong> &lt; NumRuns &amp;&amp; <strong>fitnessSum</strong> &lt; fitnessLimit; <strong>j</strong>++)<br/>{<br/>// Perform one optimization run on the problem.<br/>Result result = Optimizer.Optimize(parameters, fitnessLimit -<strong>fitnessSum</strong>);<br/>// Get the best fitness result from optimization and adjust it<br/>// by subtracting its minimum possible value.<br/>double fitness = result.Fitness;<br/>double <strong>fitnessAdjusted</strong> = fitness - Optimizer.MinFitness;<br/>// Ensure adjusted fitness is non-negative, otherwise Preemptive<br/>// Fitness Evaluation does not work.<br/>Debug.Assert(<strong>fitnessAdjusted</strong> &gt;= 0);<br/>// Apply weight to the adjusted fitness.<br/><strong>fitnessAdjusted</strong> *= weight;<br/>// Accumulate both fitness sums.<br/><strong>fitnessSumInner</strong> += <strong>fitnessAdjusted</strong>;<br/><strong>fitnessSum</strong> += <strong>fitnessAdjusted</strong>;<br/>}<br/>// Set the fitness result achieved on the problem.<br/>// This was why we needed an extra summation variable.<br/>ProblemIndex.SetFitness(<strong>i</strong>, <strong>fitnessSumInner</strong>);<br/>}<br/>// Sort the optimization problems so that the worst<br/>// performing will be attempted optimized first, when<br/>// this method is called again.<br/>ProblemIndex.Sort();<br/>return <strong>fitnessSum</strong>;<br/>}</pre>
<p>Now let's look at our code in action, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1263 image-border" src="img/d0ae8670-c850-492c-95da-bcc2b8167d02.png" style="width:36.83em;height:38.58em;"/></p>
<p>As you can see, the goal of the program is to output the most optimal parameters so that you can tune your network using the same function optimization.</p>
<p>But what can you do if you have a function that is not one of those included in SwarmOps? Luckily, you can define a custom problem of your own and use it. Let's take a look at how that's used. First, let's look at the <kbd>TestCustomProblem</kbd> project, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1265 image-border" src="img/ad4f4bca-c395-405a-976a-da0b20ccf2b2.png" style="width:20.58em;height:29.25em;"/></p>
<p>TestCustomProblem Project</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Testing custom problems</h1>
                
            
            
                
<p>Before we get into creating and testing our own custom problem, let's talk about a more general problem. We have already outlined what we define as a problem earlier in this chapter, but now is a good time to show you the code for our base object <kbd>Problem</kbd> before we design our own. So, let's move on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Base problem</h1>
                
            
            
                
<p>The following is the base class <kbd>Problem</kbd> that is used in every optimization:</p>
<pre>public abstract class Problem<br/>{<br/>public Problem() : this(0, true)<br/>{<br/>}<br/>public Problem(int maxIterations) : this(maxIterations, true)<br/>{<br/>}<br/>public Problem(int maxIterations, bool requireFeasible)<br/>{<br/>MaxIterations = maxIterations;<br/>RequireFeasible = requireFeasible;<br/>}</pre>
<p>The maximum number of optimization iterations to perform is as follows:</p>
<pre>public int MaxIterations</pre>
<p>The following command checks that the solution is feasible (that it satisfies constraints):</p>
<pre>public bool RequireFeasible</pre>
<p>Then, the name of the optimization problem is returned with the following command:</p>
<pre>public abstract string Name</pre>
<p>This includes an array with the names of the parameters, as follows:</p>
<pre>public virtual string[] ParameterName =&gt; null;</pre>
<p>To lower the search-space boundary, use the following command:</p>
<pre>public abstract double[] LowerBound</pre>
<p>To increase the upper search-space boundary, use the following command:</p>
<pre>public abstract double[] UpperBound</pre>
<p>The lower initialization boundary, if different from the search-space boundary, is denoted as follows:</p>
<pre>public virtual double[] LowerInit =&gt; LowerBound;</pre>
<p>The upper initialization boundary, if different from the search-space boundary, is denoted as follows:</p>
<pre>public virtual double[] UpperInit =&gt; UpperBound;</pre>
<p>The following command details the maximum (that is, the worst) fitness possible, as follows:</p>
<pre>public virtual double MaxFitness =&gt; double.<strong>MaxValue</strong>;</pre>
<p>The following command details the minimum (that is, the best) fitness possible. This is especially important if using meta-optimization where fitness is assumed to be non-negative; this should be roughly equivalent among all the problems we meta-optimize:</p>
<pre>public abstract double MinFitness</pre>
<p>The threshold for an acceptable fitness value is denoted as follows:</p>
<pre><br/>public virtual double AcceptableFitness =&gt; MinFitness;</pre>
<p>To return the dimensionality of the problem, that is, the number of parameters in a candidate solution, use the following command:</p>
<pre>public abstract int Dimensionality</pre>
<p>The following line checks if the gradient has been implemented:</p>
<pre>public virtual bool HasGradient =&gt; false;</pre>
<p>The following command computes and returns fitness for the given parameters:</p>
<pre>public virtual double Fitness(double[] parameters)<br/>{<br/>return Fitness(parameters, true);<br/>}</pre>
<p>The fitness evaluation is aborted preemptively if the fitness becomes higher (that is, worse) than <kbd>fitnessLimit()</kbd>, or if it is not possible for the fitness to improve, as follows:</p>
<pre>public virtual double Fitness(double[] parameters, double fitnessLimit){<br/>return Fitness(parameters);<br/>}</pre>
<p>We compute and return fitness for the given parameters. The fitness evaluation is aborted preemptively if feasibility of the new candidate solution is the same as or better than that of the old candidate solution—or if the fitness becomes higher (that is, worse) than <kbd>fitnessLimit()</kbd> and it is not possible for the fitness to improve, as follows:</p>
<pre>public virtual double Fitness(double[] parameters, double fitnessLimit, bool oldFeasible, bool newFeasible)<br/>{<br/>return Tools.BetterFeasible(oldFeasible, newFeasible)? Fitness(parameters, fitnessLimit) : Fitness(parameters);<br/>}</pre>
<p>Compute and return fitness for the given parameters as follows:</p>
<pre>public virtual double Fitness(double[] parameters, bool feasible)<br/>{<br/>return Fitness(parameters, MaxFitness, feasible, feasible);<br/>}</pre>
<p>Compute the gradient of the fitness-function with the following command relating to the computation time-complexity factor. For example, if fitness takes time O(n) to compute and gradient takes time O(n*n) to compute, then <kbd>return n. &lt;/returns&gt;</kbd>:</p>
<pre>public virtual int Gradient(double[] x, ref double[] v)<br/>{<br/>throw new <strong>NotImplementedException</strong>();<br/>}</pre>
<p>Enforce constraints and evaluate feasibility with the following command. If you do not wish to enforce constraints, you should make the call <kbd>Feasible()</kbd>:</p>
<pre>public virtual bool EnforceConstraints(ref double[] parameters)<br/>{</pre>
<p>By default, we bound the candidate solution to the search-space boundaries, as follows:</p>
<pre>Tools.Bound(ref parameters, LowerBound, UpperBound);</pre>
<p>Since we know that the candidate solution is now within bounds and this is all that is required for feasibility, we could just return <kbd>true</kbd> here. As shown in the following snippet, <kbd>Feasible</kbd> is called for educational purposes, as most optimizers call <kbd>EnforceConstraints()</kbd>.</p>
<pre>return Feasible(parameters);<br/>}</pre>
<p>Evaluate feasibility (constraint satisfaction) with the following code:</p>
<pre>public virtual bool Feasible(double[] parameters)<br/>{<br/>return Tools.BetweenBounds(parameters, LowerBound, UpperBound);<br/>}</pre>
<p>The following is called at the beginning of an optimization run:</p>
<pre>public virtual void BeginOptimizationRun()</pre>
<p>The following is called at the end of an optimization run:</p>
<pre>public virtual void EndOptimizationRun()</pre>
<p>To return whether optimization is allowed to continue, use the following code:</p>
<pre>public virtual bool Continue(int iterations, double fitness, bool feasible)<br/>{<br/>return (iterations &lt; MaxIterations &amp;&amp;!(fitness &lt;= AcceptableFitness &amp;&amp; (!RequireFeasible || feasible)));<br/>}<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a custom problem</h1>
                
            
            
                
<p>Now that we have that out of the way, let's create a custom problem based upon our base problem class. The code will look like the following example.</p>
<p>The following is the two-dimensional Rosenbrock problem with some example constraints; its optimal feasible solution seems to be as follows:</p>
<pre class="mce-root">&lt;summary&gt;<br/>a ~ 1.5937<br/>b ~ 2.5416<br/> &lt;/summary&gt;<br/>Class CustomProblem :Problem<br/>{<br/>public double GetA(double[] parameters)<br/>{<br/>return parameters[0];<br/>}<br/>public double GetB(double[] parameters)<br/>{<br/>return parameters[1];<br/>}</pre>
<p class="mce-root">Here, the base-class overrides the name of the optimizer, as follows:</p>
<pre>public override string Name =&gt; "CustomProblem";</pre>
<p>The dimensionality of the problem is as follows:</p>
<pre>public override int Dimensionality =&gt; 2;<br/>double[] _lowerBound = { -100, -100 };</pre>
<p class="packt_figref"> The following is the lower search-space boundary:</p>
<pre>public override double[] LowerBound =&gt; _lowerBound;<br/>double[] _upperBound = { 100, 100 };</pre>
<p class="mce-root">The following is the upper search-space boundary:</p>
<pre>public override double[] UpperBound =&gt; _upperBound;</pre>
<p>The lower initialization boundary is as follows:</p>
<pre>public override double[] LowerInit =&gt; LowerBound;</pre>
<p>The upper initialization boundary is as follows:</p>
<pre>public override double[] UpperInit =&gt; UpperBound;</pre>
<p>The minimum possible fitness for this problem is worked out using the following line:</p>
<pre>public override double MinFitness =&gt; 0;</pre>
<p> The acceptable fitness threshold is as follows:</p>
<pre>public override double AcceptableFitness =&gt; 0.4;<br/>string[] _parameterName = { "a", "b" };</pre>
<p>The names of the parameters for the problem are as follows:</p>
<pre>public override string[] ParameterName =&gt; _parameterName;</pre>
<p>To compute and return fitness for the given parameters, use the following code:</p>
<pre>public override double Fitness(double[] x)<br/>{<br/>Debug.Assert(x != null &amp;&amp; x.Length == Dimensionality);<br/>double a = GetA(x);<br/>double b = GetB(x);<br/>double t1 = 1 - a;<br/>double t2 = b - a * a;<br/>return t1 * t1 + 100 * t2 * t2;<br/>}</pre>
<p>To enforce and evaluate constraints, use the following code:</p>
<pre>public override bool EnforceConstraints(ref double[] x)<br/>{<br/>// Enforce boundaries.<br/>SwarmOps.Tools.Bound(ref x, LowerBound, UpperBound);<br/>return Feasible(x);<br/>}<br/>// Evaluate constraints.<br/>public override bool Feasible(double[] x)<br/>{<br/>Debug.Assert(x != null &amp;&amp; x.Length == Dimensionality);<br/>double a = GetA(x);<br/>double b = GetB(x);<br/>// Radius.<br/>double r = Math.Sqrt(a * a + b * b);<br/>return ((r &lt; 0.7) || ((r &gt; 3) &amp;&amp; (r &lt; 5))) &amp;&amp; (a &lt; b * b);<br/>}<br/>}<br/>}</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Our Custom Problem</h1>
                
            
            
                
<p>Now, create an object of the custom problem, as follows:</p>
<pre>static Problem Problem = new CustomProblem();</pre>
<p>The optimization settings should be as follows:</p>
<pre>static readonly int NumRuns = 50;<br/>static readonly int DimFactor = 4000;<br/>static readonly int Dim = Problem.Dimensionality;<br/>static readonly int NumIterations = DimFactor * Dim;</pre>
<p>Create the optimizer object as follows:</p>
<pre>static Optimizer Optimizer = new DE(Problem);</pre>
<p>The control parameters for the optimizer should be as follows:</p>
<pre>static readonly double[] Parameters = Optimizer.DefaultParameters;</pre>
<p>Wrap the optimizer in a logger of result-statistics, as follows:</p>
<pre>static readonly bool StatisticsOnlyFeasible = true;<br/>static Statistics Statistics = new Statistics(Optimizer, StatisticsOnlyFeasible); </pre>
<p>Wrap it again in the following repeater:</p>
<pre>static Repeat Repeat = new RepeatSum(Statistics, NumRuns);<br/>static void Main(string[] args)<br/>{</pre>
<p class="mce-root">Next, initialize the parallel random number generator, as follows:</p>
<pre>Globals.Random = new RandomOps.MersenneTwister();</pre>
<p>Then, set the maximum number of optimization iterations to perform with the following:</p>
<pre>Problem.MaxIterations = NumIterations;</pre>
<p> Create a fitness trace for tracing the progress of optimization with the following code:</p>
<pre>int NumMeanIntervals = 3000;<br/>FitnessTrace fitnessTrace = new FitnessTraceMean(NumIterations, NumMeanIntervals);<br/>FeasibleTrace feasibleTrace = new FeasibleTrace(NumIterations, NumMeanIntervals, fitnessTrace);</pre>
<p>Then, assign the fitness trace to the optimizer as follows:</p>
<pre>Optimizer.FitnessTrace = feasibleTrace;</pre>
<p>Perform the optimizations as follows:</p>
<pre>double fitness = Repeat.Fitness(Parameters);<br/>if (Statistics.FeasibleFraction &gt; 0)<br/>{</pre>
<p>Compute the result-statistics with the following line:</p>
<pre>Statistics.Compute();</pre>
<p>Output the best result, as well as result-statistics, with the following code:</p>
<pre>Console.WriteLine("Best feasible solution found:", Color.Yellow);<br/>Tools.PrintParameters(Problem, Statistics.BestParameters);<br/>Console.WriteLine();<br/>Console.WriteLine("Result Statistics:", Color.Yellow);<br/>Console.WriteLine("\tFeasible: \t{0} of solutions found.", Tools.FormatPercent(Statistics.FeasibleFraction), Color.Yellow);<br/>Console.WriteLine("\tBest Fitness: \t{0}", Tools.FormatNumber(Statistics.FitnessMin), Color.Yellow);<br/>Console.WriteLine("\tWorst: \t\t{0}", Tools.FormatNumber(Statistics.FitnessMax), Color.Yellow);<br/>Console.WriteLine("\tMean: \t\t{0}", Tools.FormatNumber(Statistics.FitnessMean), Color.Yellow);<br/>Console.WriteLine("\tStd.Dev.: \t{0}", Tools.FormatNumber(Statistics.FitnessStdDev), Color.Yellow);<br/>Console.WriteLine();<br/>Console.WriteLine("Iterations used per run:", Color.Yellow);<br/>Console.WriteLine("\tMean: {0}", Tools.FormatNumber(Statistics.IterationsMean), Color.Yellow);<br/>}<br/>else<br/>{<br/>Console.WriteLine("No feasible solutions found.", Color.Red);<br/>}<br/>}</pre>
<p>When we run our program, it should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1266 image-border" src="img/d844cd54-286c-4959-b6a8-14ae58f7bd46.png" style="width:28.92em;height:35.42em;"/></p>
<p>The output result of our problem</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned how to use SwarmOps to help us optimize parameters for our function optimization. We learned how to use the built-in functions of SwarmOps, as well as how to define our own. In the next chapter, we will move on to image detection and will use the great open source package, TensorFlowSharp.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">References</h1>
                
            
            
                
<ul>
<li>J. Kennedy and R. Eberhart. Particle swarm optimization in Proceedings of IEEE International Conference on Neural Networks, volume IV, pages 1942-1948, Pert, Australia, 1995</li>
<li>Y. Shi and R.C. Eberhart. A modified particle swarm optimizer. In Proceedings of the IEEE International Conference on Evolutionary Computation, pages 69-73, Anchorage, AK, USA, 1998.</li>
<li>J. Kennedy. The particle swarm: social adaptation of knowledge. In Proceedings of the IEEE International Conference on Evolutionary Computation, Indianapolis, USA, 1997.</li>
<li>M.E.H Pederson and A.J. Chipperfield. Simplified particle swarm optimization. Applied Soft Computing, 10, P. 618-628, 2010.</li>
<li>Simplifying Particle Swarm Optimization. Pedersen, M.E.H. and Chipperfield, A.J. s.l. : Applied Soft Computing, 2010, Vol. 10, pp. 618-628.</li>
<li>Variable metric method for minimization. Davidon, W.C. 1, s.l. : SIAM Journal on Optimization, 1991, Vol. 1, pp. 1-17.</li>
<li>"Direct Search" solution for numerical and statistical problems. Hooke, R. and Jeeves, T.A. 2, s.l. : Journal of the Association for Computing Machinery (ACM), 1961, Vol. 8, pp. 212-229.</li>
<li>Pedersen, M.E.H. and Chipperfield, A.J.Local Unimodal Sampling. s.l. : Hvass Laboratories, 2008. HL0801.</li>
<li>Differential evolution - a simple and efficient heuristic for global optimization over continuous space. Storn, R. and Price, K. s.l. : Journal of Global Optimization, 1997, Vol. 11, pp. 341-359.</li>
</ul>
<ul>
<li>Particle Swarm Optimization. Kennedy, J. and Eberhart, R. Perth, Australia : IEEE Internation Conference on Neural Networks, 1995.</li>
<li>A Modified Particle Swarm Optimizer. Shi, Y. and Eberhart, R. Anchorage, AK, USA : IEEE International Conference on Evolutionary Computation, 1998.</li>
<li>The particle swarm: social adaptation of knowledge. Kennedy, J. Indianapolis, USA : Proceedings of the IEEE International Conference on Evolutionary Computation, 1997.</li>
</ul>


            

            
        
    </body></html>