- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applying the Lessons of Deepfakes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The techniques in this book can be used for a lot more than face replacements.
    In this chapter, we’ll examine just a few examples of how you can apply the lessons
    and tools of this book in other fields. We’ll look at how to tweak and modify
    the techniques to use the results in new and unique ways.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we’ll look at just a few techniques from earlier in this book
    and see how they can be used in a new way. The examples in this chapter are not
    exhaustive, and there are always more ways that you could implement the abilities
    that deepfakes bring. In this chapter, we are more focused on the technique than
    the specifics, but in examining the technique, we’ll explore the following in
    new ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Aligning other types of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The power of masking images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting data under control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this chapter, there is one section with a small amount of code that demonstrates
    how to use a non-module Git repo for your own uses.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this isn’t part of the hands-on section of the book, we’ve included the
    code to interface with a library: `PeCLR`. This code is also included in the book’s
    code repo with some additional functionality, including visualizing the points,
    but is just an example and is not meant to be a complete API for using `PeCLR`
    in your own project:'
  prefs: []
  type: TYPE_NORMAL
- en: First, open Anaconda Command Prompt.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On Windows, hit *Start* and then type `anaconda`. This should bring up the
    following option:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Anaconda Prompt](img/B17535_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Anaconda Prompt
  prefs: []
  type: TYPE_NORMAL
- en: Click on this, and it will open an Anaconda prompt for the rest of the following
    commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to clone a copy of the `PeCLR` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Download the model data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The library includes a copy of all the pretrained models at [https://dataset.ait.ethz.ch/downloads/guSEovHBpR/](https://dataset.ait.ethz.ch/downloads/guSEovHBpR/).
    Open the link in a browser and download the files (if this URL fails, check the
    `PeCLR` library or book repository for any updated links).
  prefs: []
  type: TYPE_NORMAL
- en: Extract the files into the `data/models/` folder inside your local copy of the
    `PeCLR` repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `conda` environment with all the `PeCLR` requirements installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create an Anaconda environment with all the libraries that `PeCLR`
    needs to run.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, this will install a Jupyter notebook. Jupyter Notebook is a useful
    tool for real-time coding. To run a cell, click on it and then either hit *Shift
    + Enter* or click on the **Play** triangle button. Jupyter will run that one chunk
    of code and then stop, allowing you to change the code and rerun it at will.
  prefs: []
  type: TYPE_NORMAL
- en: Copy the `PeCLR.ipynb` file into the cloned repo folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you want to follow the Jupyter Notebook file, you can just copy the file
    from the book’s repo into the folder that you cloned `PeCLR` into earlier. This
    will save you from having to retype everything.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open Jupyter Notebook and access it with a browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will run Jupyter Notebook. If you’re running the command on the same computer
    that you’re using it on, it should also automatically open your browser to the
    running Jupyter Notebook instance, and you’ll be ready to go. If not, you can
    open your favorite browser and go to http://<jupyter server ip>:8888/tree to access
    it.
  prefs: []
  type: TYPE_NORMAL
- en: The usage of this code will be explained when we come to the *Writing our own
    interface* and *Using the library* parts of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning other types of images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aligning faces is a critical tool for getting deepfakes to work. Without the
    alignment of faces, we’d be doomed with extremely long training times and huge
    models to correct the faces. It’s not a stretch to say that without alignment,
    modern deepfakes would effectively be impossible today.
  prefs: []
  type: TYPE_NORMAL
- en: Alignment saves time and compute power by removing the need for the neural network
    to figure out where the face is in the image and adapt for the many different
    locations the face may be. By aligning in advance, the AI doesn’t even need to
    learn what a face *is* in order to do its job. This allows the AI to focus on
    learning the task at hand, such as generating realistic facial expressions or
    speech, rather than trying to locate and correct misaligned faces.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to improving the efficiency of the training process, aligning faces
    also helps to improve the quality and consistency of the final deepfake. Without
    proper alignment, the generated faces may appear distorted or unnatural, which
    can detract from the overall realism of the deepfake.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, alignment doesn’t just apply to faces. You could use it for hands,
    people, animals, or even cars and furniture. In fact, anything that you can detect
    with defined parts can be aligned. For this to work, you need to somehow find
    points to align. For example, with hands, this could be the individual fingers.
  prefs: []
  type: TYPE_NORMAL
- en: While this works with any object, we will focus on a single example case. Here
    is an example process on how you could align hands. Other objects could be aligned
    in the same way. You’ll just want to follow the same steps but replace the hands
    with whatever object you want to align.
  prefs: []
  type: TYPE_NORMAL
- en: Finding an aligner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to find a way to identify the points of the hand that we’re interested
    in aligning with. For this, we need to do something called pose estimation. We
    could develop this ourselves using YOLO ([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5))
    or another object detection tool that would identify some point, such as the tips
    of the fingers. You might have to do some heuristics to order them properly so
    that you can align with them.
  prefs: []
  type: TYPE_NORMAL
- en: However, better than developing this ourselves, we could use a library that
    does this for us. When I want to find a library or code that does a particular
    task, the first place I look is **Papers with Code** ([https://paperswithcode.com/](https://paperswithcode.com/)).
    This site has all sorts of software projects based on various AI tasks. In our
    example, they have a section specifically for hand pose estimation ([https://paperswithcode.com/task/hand-pose-estimation](https://paperswithcode.com/task/hand-pose-estimation)),
    which lists a variety of benchmarks. These are tests that the code has been tested
    against. This lets you see not only the libraries that will do what you want but
    even show you the “best” ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, the best result is **Virtual View Selection**, which is located
    at [https://github.com/iscas3dv/handpose-virtualview](https://github.com/iscas3dv/handpose-virtualview).
    Unfortunately, this one has a restrictive “no commercial use” license. So, we’ll
    actually skip it and go to the next one, **AWR: Adaptive Weighting Regression
    for 3D Hand Pose Estimation**, which can be found at [https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression](https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression).
    This one is MIT-licensed, which is an open license that lets you use the software
    even for commercial purposes, but only works on depth images. *Depth* refers to
    the distance between the camera and the object in the image. These images are
    useful for tasks such as hand detection, but unfortunately, they require special
    cameras or techniques to get right, so we will have to skip this one too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the others only work on depth images, too. However, if we keep looking
    through the posted options, we should come across **PeCLR: Self-Supervised 3D
    Hand Pose Estimation from monocular RGB via Equivariant Contrastive Learning**,
    which has an MIT license and works on standard RGB (color) photos. You can download
    it at [https://github.com/dahiyaaneesh/peclr](https://github.com/dahiyaaneesh/peclr).'
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: While in this section we’re just using the code and treating it like a library,
    in reality, the `PeCLR` code (and the other projects listed) was released as a
    part of an academic paper. It is not the intention of the authors to diminish
    that work, as academic work drives a lot of innovation in the AI field. However,
    this section of the book is about how to *implement* ideas, and that means using
    the code without necessarily paying attention to the innovations. If you’re interested
    in a deep dive into what exactly `PeCLR` is doing, we recommend that you read
    the paper, which is linked in the Git repo readme.
  prefs: []
  type: TYPE_NORMAL
- en: Using the library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `PeCLR` library has all the models and tools needed to do detection and
    pose estimation for the hands but not all the code to run on the external image.
    Unfortunately, this is very common in academic research-style projects that are
    often more interested in you being able to validate the results that they have
    already published instead of letting you run it on new data. Because of this,
    we’ll need to actually write some code to run our images through their model.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best place to interface with the existing code can be hard if they
    don’t provide an easy-to-use API. Since `PeCLR` was an academic project, there
    is no easy API, and we’ll need to find our own place to call their code with our
    own API substitute.
  prefs: []
  type: TYPE_NORMAL
- en: Writing our own interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code to run the model on the validation data is only partially usable for
    our situation since the dataset that they were using expects data to be in a certain
    format, which would be hard to recreate with our data. Because of this, we’ll
    start from scratch and call the model in our own code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get started with this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll want to import all the libraries we’re using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code imports all the libraries we’re going to need. Most of these
    are standard libraries that we’ve used before, but the last one is the model that
    `PeCLR` uses to do the actual detection. We’ve imported that one, so we can call
    it with the image to run the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll load the model from `PeCLR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code loads the model from the model data that `PeCLR` provides.
    To do this, first, we define the model path and type. Then, we pass the model
    type to generate an appropriate model. Next, we load the checkpoints and copy
    the weights into the model. Finally, we prepare the model for evaluation and set
    it to run on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll load the image and prepare it for the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code prepares the image. It does this by, first, loading it with the `SciKit`
    image loader, which, unlike `OpenCV`, can directly handle URLs or local files.
    It then calculates an adjustment for restoring the model’s coordinates to the
    ones that match the full image size. It does this by dividing 224 by the height
    and width of the image. Then, we convert the image data into a floating point
    with a range of 0–1\. We then normalize the images by dividing them by a standard
    deviation and subtracting a mean. This brings the images down to a range that
    the model expects. Then, we resize the image to 224 x 224, which is the image
    size that the model expects. We then convert the image into a tensor and get it
    in the order Pytorch uses, with the channels first. Finally, we add another dimension
    at the front to hold the batch and convert it into a 32-bit floating point on
    the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: This all prepares the image for the model to be run on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we run the model on the image and get 2D coordinates out of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code first runs the image through the model without generating training
    gradients. To do this, we pass the image and the `None` value, which will use
    a default camera intrinsics matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Author’s note
  prefs: []
  type: TYPE_NORMAL
- en: '*Camera intrinsics* is a fancy term, but it just means the details of your
    camera. In the case of `PeCLR`, it wants a matrix that details how large the pixel
    space is so that it can attempt to guess the depth information from the 2D image.
    We don’t need the depth information, so we can let `PeCLR` create a default matrix
    instead of giving it one.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, the code takes just the 2D alignment points. We don’t need 3D points since
    we’re aligning in 2D space. If we were working with a depth image, we may have
    wanted the third dimension, but we aren’t and we don’t need that for our scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Next, since the model was given a small 224 x 224 image, we’re going to adjust
    those coordinates to match the width and height of the original image. To do this,
    we divide the coordinates by 224 and multiply the result by the original image
    sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Using the landmarks to align
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this case, the library will mark the joints and tips of every finger and
    the thumb and one point near the “middle” of the hand. Unfortunately, the point
    in the middle of the hand is not well defined and could be anywhere from the actual
    middle to the wrist, so we wouldn’t want to use it for alignment. The joint and
    fingertip locations are going to be more consistent, so we can use those for the
    alignment process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – A hand detected and marked with detection from PeCLR (original
    photo by Kira auf der Heide via Unsplash)](img/B17535_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – A hand detected and marked with detection from PeCLR (original
    photo by Kira auf der Heide via Unsplash)
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Aligners are never perfectly accurate. Some can vary by a significant amount,
    but it’s not a matter of perfect results every time. Any alignment, even an imperfect
    one, has benefits for neural network training as it normalizes the data into a
    more reliable format. Any work done before the data is given to the AI means that
    is one less task the AI has to waste effort on doing.
  prefs: []
  type: TYPE_NORMAL
- en: Once we can get some known points on an image, we can scale, rotate, and crop
    the image so that it’s in the orientation that you want and run it through the
    detector to get a list of points. If you can, I recommend running several images
    through and averaging the points together. That way, you can reduce any variations
    in the hand images and get better alignments.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have the points that you want to align to, you can use them to generate
    aligned images using the **Umeyama** algorithm. The algorithm just needs two sets
    of points, a known “aligned” set and a second set that you can convert into an
    aligned set. Umeyama returns a matrix that you can feed into an Affine Warp function
    to get a final aligned image. See [*Chapter 5*](B17535_05.xhtml#_idTextAnchor090),
    *Extracting Faces*, for a hands-on code example of how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the aligned hand images, you can do whatever it was you were planning
    on doing with them, be that displaying them or using them for your AI task. It’s
    even possible that, once you have your aligned data, you can use it to train a
    pose detection model of your own to get even better alignment results. This process
    of using AI-processed data to feed into a model to make that model better is called
    **bootstrapping** and, with proper supervision, is an invaluable technique.
  prefs: []
  type: TYPE_NORMAL
- en: Alignments are a critical part of deepfakes, and now you can use them in other
    fields. Next, we’ll look at how to use masking to get clean cut-outs of an object.
  prefs: []
  type: TYPE_NORMAL
- en: The power of masking images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you take a photograph, you are capturing everything that the camera sees.
    However, the chances are that you’re not equally interested in every part of the
    image. If you’re on vacation, you might take a selfie of yourself in front of
    a waterfall, and while you value yourself and the waterfall, you care less about
    the cars or other people in the image. While you can’t remove the cars without
    adding something into the gaps for your vacation photos, sometimes, you’re only
    interested in the main subject and might want to cut it from the rest of the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'With deepfakes, we can use a mask to help us remove the face from the image
    so that we replace only the face and leave the rest of the image alone. In other
    AI tasks, you might have similar needs but different objects that you want to
    cut out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – An example of the mask used in the deepfake process](img/B17535_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – An example of the mask used in the deepfake process
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at other types of masking.
  prefs: []
  type: TYPE_NORMAL
- en: Types of masking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Masking out an image can be useful in a lot of tasks. We only used it as a part
    of the conversion process in a step called **composition**. This is only part
    of the power of masking. It can be used to guide **inpainting**, which is the
    process by which you fill in gaps in an image to erase an object. You could also
    do it to an image before it gets fed into an AI to make sure that the AI can focus
    on the important parts.
  prefs: []
  type: TYPE_NORMAL
- en: In order to mask an image, you need to get some sort of idea of what part of
    an image you want to be masked. This is called **segmentation**, and it has a
    lot of sub-domains. If you want to segment based on the type of object, it would
    be called **semantic segmentation**. If you wanted to segment the image based
    on the subject, it is called **instance segmentation**. You can even use **depth
    segmentation** if you have a solid depth map of the image. Unfortunately, deciding
    which type of segmentation you need requires special attention in order to find
    the right tool.
  prefs: []
  type: TYPE_NORMAL
- en: Finding a usable mask for your object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Libraries such as **PaddleSeg** ([https://github.com/PaddlePaddle/PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg))
    have special tools that let you do multiple types of segmentation. They even have
    an interactive segmentation system that lets you “mark” what you want to segment
    like Photoshop’s magic wand tool. Following that, you might need to use that data
    to train a segmentation model that is capable of masking that particular type
    of object in new contexts.
  prefs: []
  type: TYPE_NORMAL
- en: To find the best method of masking the given object you’re interested in, you
    should probably start with a search for the item that you want to mask and segment.
    For some objects such as faces, cars, people, and more, there are prebuilt models
    to segment those objects.
  prefs: []
  type: TYPE_NORMAL
- en: But if a segmentation model doesn’t exist for the particular object you’re interested
    in, there’s no need to despair. Newer models such as **CLIP** ([https://github.com/openai/CLIP](https://github.com/openai/CLIP))
    have opened up whole new opportunities. CLIP is made up of a pair of AI models
    that connect language and images together. Because of the shared nature of CLIP,
    it’s possible to learn the difference between objects based on their text descriptions.
    This means that libraries such as **CLIPseg** ([https://github.com/timojl/clipseg](https://github.com/timojl/clipseg))
    can use language prompts to segment objects in an image.
  prefs: []
  type: TYPE_NORMAL
- en: Examining an example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s look at an example. Let’s say that you wanted to count the cars parked
    in a parking lot and see whether any spaces are still available, but all you have
    is a webcam image of the parking lot from above. To do this, you need to know
    which parts of the image are cars and which are empty parking spots. This task
    mixes both semantic segmentation and instance segmentation but uses them together.
  prefs: []
  type: TYPE_NORMAL
- en: The first step would be to mark out each parking spot in the image to define
    which spots you want to look at. You could pick a single spot in each parking
    spot or define them by the whole area. Either way, you’ll probably want to do
    this manually since it is unlikely to change often enough to justify having the
    computer do it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know where the parking spots are in the image, you can start looking
    for cars. To do this, we’d want to look around for a good neural network trained
    to do this task. In our case, we can check this example from Kaggle user Tanishq
    Gautam: [https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch](https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch).
    This page gives pretrained models and a solid guide for how to segment cars.'
  prefs: []
  type: TYPE_NORMAL
- en: While this model does not do instance segmentation (giving each car a different
    “color” or tag), we can use it to count the cars anyway. We can do this because
    we’ve already established that we’re counting cars in a parking lot and that we
    have manually marked each parking spot.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we can simply use the segmentation model to detect any cars and see whether
    they overlap with the parking spots. If we only marked a single point for each
    parking spot, we could simply check each point and see whether it is segmented
    as a “car” or as “background” (a common term used for anything that we’re not
    segmenting).
  prefs: []
  type: TYPE_NORMAL
- en: If we marked the whole area of the parking spot, then we might want to calculate
    a minimum coverage for the spot to be considered “taken.” For example, we want
    to ensure that a motorcycle in a parking spot gets counted, but a car that is
    slightly over the line shouldn’t be counted. You can set a minimum threshold of
    the parking spot’s area, and if it’s filled beyond that with a “car,” then you
    mark the whole spot as taken.
  prefs: []
  type: TYPE_NORMAL
- en: Additional functionality is possible too. For example, you could even detect
    whether a car is in a prohibited area by checking whether an area segmented as
    a “car” is not inside a parking spot. This could be used to automatically alert
    a parking enforcement officer to check the lot.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve gotten a good hold on our masking, let’s look at how we can manage
    our data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting data under control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a common saying in the AI community that an ML scientist’s job is only
    10% ML and 90% data management. This, like many such sayings, is not far from
    the truth. While every ML task is focused on the actual training of the model,
    first, you must get your data into a manageable form before you can start the
    training. Hours of training can be completely wasted if your data isn’t properly
    prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Before you can start training a model, you have to decide what data it is that
    you’re going to train it with. That data must be gathered, cleaned, converted
    into the right format, and generally made ready to train. Often, this involves
    a lot of manual processes and verification.
  prefs: []
  type: TYPE_NORMAL
- en: Defining your rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most important thing in the manual process is to make sure that all your
    data meets your requirements and meets a consistent level of quality. To do this,
    you need to define exactly what “good” data means. Whether you’re annotating your
    data or gathering large amounts, you should have a standard way of doing whatever
    it is you’re doing.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say you’re annotating a dog versus cat dataset and you want
    to put all the pictures into one of two buckets, one bucket consisting of all
    the dog images and the other bucket consisting of all the cat images. What happens
    when an image contains both a cat and a dog? Do you put it in the bucket that
    is more prominent? Do you exclude it from your dataset? Do you edit the image
    to remove one of the animals? Do you crop it into two images so both animals end
    up in the dataset?
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to have a consistent set of rules for these situations. That
    ensures that your data is appropriate for the purposes. You don’t want to have
    these edge cases happen in different ways each time, or it will upset your training
    and raise confusion when you’re trying to fix issues.
  prefs: []
  type: TYPE_NORMAL
- en: Evolving your rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Also, it’s important to have a plan for what happens when you change your rules.
    As you go forward with your data management, it’s almost inevitable that you’ll
    find that you want to make some tweaks or changes to the rules based on what data
    you find, how well your training process goes, and whether your final use case
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at our example, let’s consider the case where you decided to exclude
    any images that contained both cats and dogs but other animals were fine as long
    as the image also contained a cat or dog. What happens when you decide that you
    want to add rabbits to your cat/dog detector? This means not just that you add
    a new bucket for rabbit images but also that you have to re-process all the existing
    images that you have gone through to make sure that any cat or dog images that
    also contain rabbits get removed. What about if you find out that guinea pigs
    in your cat and dog buckets are being flagged as rabbits? These processes need
    to be considered as you manage your data.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with errors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Errors happen, and small amounts of bad data getting through into your dataset
    is inevitable. However, there is a fine line between a few harmless errors and
    a large enough error to completely invalidate the training. Because of this, it’s
    often best to get a second (or third) set of eyes on the data as a part of the
    standard process. Sometimes, this isn’t possible, but in any situation where it
    is possible, it’s invaluable. A second set of eyes could find flaws in your data
    or even your methodology. What if you were tagging a particularly weird-looking
    animal as fine in your dog data but a second person identified it as a rare breed
    of cat?
  prefs: []
  type: TYPE_NORMAL
- en: I also recommend automating as much of your data gathering as possible. When
    you can cut a human out of the loop, you’ll not only save timebut you'll also
    prevent errors and mistakes. Time spent automating a data process will almost
    always pay back dividends in time down the line. Even if you think that there
    is no way that a process will take less time to automate than to do it manually,
    you should consider the errors that you will avoid. Well-written automation code
    can be reused later for future projects, too. Anything that can be reused should
    be automated so that the next time you need to get it done, there is a tool to
    handle it for you.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how you can apply the lessons and techniques of
    deepfakes to other environments. First, we examined how to align other types of
    images, using hands as an example. Then, we looked at the different types of masks
    and considered using them in a parking lot monitoring solution. Following this,
    we examined data management and considered how a dataset to detect different animals
    might be built.
  prefs: []
  type: TYPE_NORMAL
- en: This process of figuring out how to apply techniques in new environments used
    throughout this chapter is itself a valuable technique that can help you throughout
    your development career, especially if you’re going to work at the edge of your
    computer’s capabilities like AI does now. Sometimes, the only difference between
    a successful project and an impossible one is the technique you borrow from a
    previous project.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to look at the potential and future of deepfakes
    and other generative AIs.
  prefs: []
  type: TYPE_NORMAL
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  prefs: []
  type: TYPE_NORMAL
