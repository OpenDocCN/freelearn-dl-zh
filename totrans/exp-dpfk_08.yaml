- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Applying the Lessons of Deepfakes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用Deepfakes的教训
- en: The techniques in this book can be used for a lot more than face replacements.
    In this chapter, we’ll examine just a few examples of how you can apply the lessons
    and tools of this book in other fields. We’ll look at how to tweak and modify
    the techniques to use the results in new and unique ways.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书中的技术可以用于比面部替换更多的用途。在本章中，我们将探讨一些如何将本书的教训和工具应用于其他领域的例子。我们将探讨如何调整和修改这些技术，以便以新的和独特的方式使用结果。
- en: 'In particular, we’ll look at just a few techniques from earlier in this book
    and see how they can be used in a new way. The examples in this chapter are not
    exhaustive, and there are always more ways that you could implement the abilities
    that deepfakes bring. In this chapter, we are more focused on the technique than
    the specifics, but in examining the technique, we’ll explore the following in
    new ways:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们将探讨本书早期的一些技术，看看它们如何以新的方式使用。本章中的例子并不全面，而且总有更多的方式可以实现deepfakes带来的能力。在本章中，我们更关注技术而不是具体细节，但在研究技术时，我们将以新的方式探索以下内容：
- en: Aligning other types of images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对其他类型的图像进行对齐
- en: The power of masking images
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遮罩图像的力量
- en: Getting data under control
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制数据
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, there is one section with a small amount of code that demonstrates
    how to use a non-module Git repo for your own uses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，有一个包含少量代码的部分，演示了如何使用非模块Git仓库供您自己使用。
- en: 'While this isn’t part of the hands-on section of the book, we’ve included the
    code to interface with a library: `PeCLR`. This code is also included in the book’s
    code repo with some additional functionality, including visualizing the points,
    but is just an example and is not meant to be a complete API for using `PeCLR`
    in your own project:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不是本书的动手部分，但我们已包含与库`PeCLR`接口的代码：`PeCLR`。此代码也包含在本书的代码仓库中，并具有一些附加功能，包括可视化点，但仅作为示例，并不打算作为在您自己的项目中使用`PeCLR`的完整API：
- en: First, open Anaconda Command Prompt.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开Anaconda命令提示符。
- en: 'On Windows, hit *Start* and then type `anaconda`. This should bring up the
    following option:'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Windows上，点击*开始*并输入`anaconda`。这应该会显示以下选项：
- en: '![Figure 8.1 – Anaconda Prompt](img/B17535_08_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – Anaconda命令提示符](img/B17535_08_001.jpg)'
- en: Figure 8.1 – Anaconda Prompt
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – Anaconda命令提示符
- en: Click on this, and it will open an Anaconda prompt for the rest of the following
    commands.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 点击此处，它将打开一个Anaconda提示符，用于以下所有命令。
- en: 'Next, we need to clone a copy of the `PeCLR` library:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要克隆`PeCLR`库的副本：
- en: '[PRE0]'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Download the model data.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载模型数据。
- en: The library includes a copy of all the pretrained models at [https://dataset.ait.ethz.ch/downloads/guSEovHBpR/](https://dataset.ait.ethz.ch/downloads/guSEovHBpR/).
    Open the link in a browser and download the files (if this URL fails, check the
    `PeCLR` library or book repository for any updated links).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 该库包含所有预训练模型的副本，可在[https://dataset.ait.ethz.ch/downloads/guSEovHBpR/](https://dataset.ait.ethz.ch/downloads/guSEovHBpR/)找到。在浏览器中打开此链接并下载文件（如果此URL失败，请检查`PeCLR`库或书籍仓库中的任何更新链接）。
- en: Extract the files into the `data/models/` folder inside your local copy of the
    `PeCLR` repo.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将文件提取到您本地`PeCLR`仓库中的`data/models/`文件夹内。
- en: 'Create a `conda` environment with all the `PeCLR` requirements installed:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含所有`PeCLR`要求的`conda`环境：
- en: '[PRE1]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will create an Anaconda environment with all the libraries that `PeCLR`
    needs to run.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个包含`PeCLR`运行所需所有库的Anaconda环境。
- en: Additionally, this will install a Jupyter notebook. Jupyter Notebook is a useful
    tool for real-time coding. To run a cell, click on it and then either hit *Shift
    + Enter* or click on the **Play** triangle button. Jupyter will run that one chunk
    of code and then stop, allowing you to change the code and rerun it at will.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这将安装一个Jupyter笔记本。Jupyter笔记本是一个用于实时编码的有用工具。要运行一个单元格，点击它，然后按*Shift + Enter*或点击**播放**三角形按钮。Jupyter将运行那一块代码，然后停止，让您可以随意更改代码并重新运行。
- en: Copy the `PeCLR.ipynb` file into the cloned repo folder.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`PeCLR.ipynb`文件复制到已克隆的仓库文件夹中。
- en: If you want to follow the Jupyter Notebook file, you can just copy the file
    from the book’s repo into the folder that you cloned `PeCLR` into earlier. This
    will save you from having to retype everything.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想跟随Jupyter Notebook文件，只需将文件从本书的仓库复制到您之前克隆的`PeCLR`文件夹中即可。这将让您免于重新输入所有内容。
- en: 'Open Jupyter Notebook and access it with a browser:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开Jupyter Notebook并使用浏览器访问：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will run Jupyter Notebook. If you’re running the command on the same computer
    that you’re using it on, it should also automatically open your browser to the
    running Jupyter Notebook instance, and you’ll be ready to go. If not, you can
    open your favorite browser and go to http://<jupyter server ip>:8888/tree to access
    it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行 Jupyter Notebook。如果你在运行命令的同一台电脑上使用它，它也应该自动打开你的浏览器到正在运行的 Jupyter Notebook
    实例，你就可以开始了。如果没有，你可以打开你喜欢的浏览器，并访问 http://<jupyter server ip>:8888/tree 来访问它。
- en: The usage of this code will be explained when we come to the *Writing our own
    interface* and *Using the library* parts of the next section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进入下一节的“编写我们自己的界面”和“使用库”部分时，将解释此代码的使用方法。
- en: Aligning other types of images
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对其他类型图像的对齐
- en: Aligning faces is a critical tool for getting deepfakes to work. Without the
    alignment of faces, we’d be doomed with extremely long training times and huge
    models to correct the faces. It’s not a stretch to say that without alignment,
    modern deepfakes would effectively be impossible today.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐面部是使深度伪造工作的重要工具。没有面部的对齐，我们将面临极长的训练时间和巨大的模型来纠正面部。可以说，没有对齐，现代深度伪造在今天是无法实现的。
- en: Alignment saves time and compute power by removing the need for the neural network
    to figure out where the face is in the image and adapt for the many different
    locations the face may be. By aligning in advance, the AI doesn’t even need to
    learn what a face *is* in order to do its job. This allows the AI to focus on
    learning the task at hand, such as generating realistic facial expressions or
    speech, rather than trying to locate and correct misaligned faces.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐通过消除神经网络确定面部在图像中的位置并适应面部可能出现的许多不同位置的需要，从而节省了时间和计算能力。通过预先对齐，AI甚至不需要学习面部“是什么”就能完成其工作。这允许AI专注于学习手头的任务，例如生成逼真的面部表情或语音，而不是试图定位和纠正未对齐的面部。
- en: In addition to improving the efficiency of the training process, aligning faces
    also helps to improve the quality and consistency of the final deepfake. Without
    proper alignment, the generated faces may appear distorted or unnatural, which
    can detract from the overall realism of the deepfake.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提高训练过程的效率外，对齐面部还有助于提高最终深度伪造的质量和一致性。如果没有适当的对齐，生成的面部可能看起来扭曲或不自然，这可能会降低深度伪造的整体真实感。
- en: In fact, alignment doesn’t just apply to faces. You could use it for hands,
    people, animals, or even cars and furniture. In fact, anything that you can detect
    with defined parts can be aligned. For this to work, you need to somehow find
    points to align. For example, with hands, this could be the individual fingers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，对齐不仅适用于面部。你可以用它来对齐手、人、动物，甚至汽车和家具。实际上，任何你可以用定义的部分检测到的对象都可以进行对齐。为了使其工作，你需要以某种方式找到对齐的点。例如，对于手，这可能是指尖。
- en: While this works with any object, we will focus on a single example case. Here
    is an example process on how you could align hands. Other objects could be aligned
    in the same way. You’ll just want to follow the same steps but replace the hands
    with whatever object you want to align.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这适用于任何对象，但我们将专注于一个单一示例案例。以下是如何对齐手的示例过程。其他对象也可以以相同的方式进行对齐。你只需要遵循相同的步骤，但将手替换为你想要对齐的任何对象。
- en: Finding an aligner
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻找对齐器
- en: First, we need to find a way to identify the points of the hand that we’re interested
    in aligning with. For this, we need to do something called pose estimation. We
    could develop this ourselves using YOLO ([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5))
    or another object detection tool that would identify some point, such as the tips
    of the fingers. You might have to do some heuristics to order them properly so
    that you can align with them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要找到一种方法来识别我们想要对齐的手的点的位置。为此，我们需要做一些被称为姿态估计的事情。我们可以使用 YOLO ([https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5))
    或其他对象检测工具来识别某些点，例如手指的尖端。你可能需要做一些启发式的工作来正确地排列它们，以便可以对齐。
- en: However, better than developing this ourselves, we could use a library that
    does this for us. When I want to find a library or code that does a particular
    task, the first place I look is **Papers with Code** ([https://paperswithcode.com/](https://paperswithcode.com/)).
    This site has all sorts of software projects based on various AI tasks. In our
    example, they have a section specifically for hand pose estimation ([https://paperswithcode.com/task/hand-pose-estimation](https://paperswithcode.com/task/hand-pose-estimation)),
    which lists a variety of benchmarks. These are tests that the code has been tested
    against. This lets you see not only the libraries that will do what you want but
    even show you the “best” ones.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与其自己开发，我们可能可以使用一个为我们完成这项工作的库。当我想要找到一个执行特定任务的库或代码时，我首先会查看**Papers with Code**([https://paperswithcode.com/](https://paperswithcode.com/))。这个网站基于各种AI任务有各种各样的软件项目。在我们的例子中，他们有一个专门用于手部姿态估计的部分([https://paperswithcode.com/task/hand-pose-estimation](https://paperswithcode.com/task/hand-pose-estimation))，列出了各种基准。这些是代码已经测试过的测试。这让你不仅能看到会做你想要的事情的库，甚至还能看到“最佳”的。
- en: 'Right now, the best result is **Virtual View Selection**, which is located
    at [https://github.com/iscas3dv/handpose-virtualview](https://github.com/iscas3dv/handpose-virtualview).
    Unfortunately, this one has a restrictive “no commercial use” license. So, we’ll
    actually skip it and go to the next one, **AWR: Adaptive Weighting Regression
    for 3D Hand Pose Estimation**, which can be found at [https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression](https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression).
    This one is MIT-licensed, which is an open license that lets you use the software
    even for commercial purposes, but only works on depth images. *Depth* refers to
    the distance between the camera and the object in the image. These images are
    useful for tasks such as hand detection, but unfortunately, they require special
    cameras or techniques to get right, so we will have to skip this one too.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，最好的结果是**虚拟视图选择**，位于[https://github.com/iscas3dv/handpose-virtualview](https://github.com/iscas3dv/handpose-virtualview)。不幸的是，这个项目有一个限制性的“禁止商业用途”许可证。因此，我们将跳过它，转到下一个，**AWR：用于3D手部姿态估计的自适应加权回归**，可以在[https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression](https://github.com/Elody-07/AWR-Adaptive-Weighting-Regression)找到。这个项目是MIT许可的，这是一个开放许可，允许您即使用于商业目的也可以使用软件，但它仅适用于深度图像。“深度”指的是图像中相机与对象之间的距离。这些图像对于手部检测等任务很有用，但不幸的是，它们需要特殊的相机或技术才能正确获取，所以我们也将跳过这个。
- en: 'Many of the others only work on depth images, too. However, if we keep looking
    through the posted options, we should come across **PeCLR: Self-Supervised 3D
    Hand Pose Estimation from monocular RGB via Equivariant Contrastive Learning**,
    which has an MIT license and works on standard RGB (color) photos. You can download
    it at [https://github.com/dahiyaaneesh/peclr](https://github.com/dahiyaaneesh/peclr).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他方法也仅处理深度图像。然而，如果我们继续浏览发布的选项，我们应该会遇到**PeCLR：通过等变对比学习从单目RGB进行自监督3D手部姿态估计**，它拥有MIT许可证，并且可以在标准的RGB（彩色）照片上工作。您可以在[https://github.com/dahiyaaneesh/peclr](https://github.com/dahiyaaneesh/peclr)下载它。
- en: Author’s note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注记
- en: While in this section we’re just using the code and treating it like a library,
    in reality, the `PeCLR` code (and the other projects listed) was released as a
    part of an academic paper. It is not the intention of the authors to diminish
    that work, as academic work drives a lot of innovation in the AI field. However,
    this section of the book is about how to *implement* ideas, and that means using
    the code without necessarily paying attention to the innovations. If you’re interested
    in a deep dive into what exactly `PeCLR` is doing, we recommend that you read
    the paper, which is linked in the Git repo readme.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在这个部分我们只是使用代码，将其视为库，但实际上，`PeCLR`代码（以及其他列出的项目）是作为学术论文的一部分发布的。作者并不打算贬低这项工作，因为学术工作推动了AI领域的许多创新。然而，这本书的这一部分是关于如何*实现*想法的，这意味着使用代码而不必
    necessarily 关注创新。如果你对深入了解`PeCLR`具体做了什么感兴趣，我们建议你阅读论文，它链接在Git仓库的readme中。
- en: Using the library
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用库
- en: The `PeCLR` library has all the models and tools needed to do detection and
    pose estimation for the hands but not all the code to run on the external image.
    Unfortunately, this is very common in academic research-style projects that are
    often more interested in you being able to validate the results that they have
    already published instead of letting you run it on new data. Because of this,
    we’ll need to actually write some code to run our images through their model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`PeCLR`库包含进行手部检测和姿态估计所需的所有模型和工具，但不是所有在外部图像上运行的代码。不幸的是，这在学术研究风格的项目中很常见，这些项目通常更感兴趣的是你能验证他们已经发布的结果，而不是让你在新数据上运行它。因此，我们需要实际编写一些代码来将我们的图像通过他们的模型。'
- en: Finding the best place to interface with the existing code can be hard if they
    don’t provide an easy-to-use API. Since `PeCLR` was an academic project, there
    is no easy API, and we’ll need to find our own place to call their code with our
    own API substitute.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果他们没有提供易于使用的API，找到与现有代码接口的最佳位置可能很困难。由于`PeCLR`是一个学术项目，没有易于使用的API，我们需要找到我们自己的位置来用我们的API替代品调用他们的代码。
- en: Writing our own interface
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写我们自己的接口
- en: The code to run the model on the validation data is only partially usable for
    our situation since the dataset that they were using expects data to be in a certain
    format, which would be hard to recreate with our data. Because of this, we’ll
    start from scratch and call the model in our own code.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证数据上运行模型的代码对于我们的情况只有部分可用，因为他们使用的数据集期望数据以某种格式存在，这很难用我们的数据重新创建。因此，我们将从头开始，并在我们的代码中调用模型。
- en: 'Let’s get started with this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始做这件事：
- en: 'First, we’ll want to import all the libraries we’re using:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入我们使用的所有库：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preceding code imports all the libraries we’re going to need. Most of these
    are standard libraries that we’ve used before, but the last one is the model that
    `PeCLR` uses to do the actual detection. We’ve imported that one, so we can call
    it with the image to run the model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码导入了我们将需要的所有库。其中大部分是我们之前使用过的标准库，但最后一个是我们使用`PeCLR`进行实际检测所使用的模型。我们已经导入了它，所以我们可以用图像调用它来运行模型。
- en: 'Next, we’ll load the model from `PeCLR`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将从`PeCLR`加载模型：
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code loads the model from the model data that `PeCLR` provides.
    To do this, first, we define the model path and type. Then, we pass the model
    type to generate an appropriate model. Next, we load the checkpoints and copy
    the weights into the model. Finally, we prepare the model for evaluation and set
    it to run on the GPU.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码从`PeCLR`提供的模型数据中加载模型。为此，首先，我们定义模型路径和类型。然后，我们将模型类型传递给生成适当的模型。接下来，我们加载检查点并将权重复制到模型中。最后，我们准备模型以进行评估，并将其设置为在GPU上运行。
- en: 'Next, we’ll load the image and prepare it for the model:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载图像并为其准备模型：
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This code prepares the image. It does this by, first, loading it with the `SciKit`
    image loader, which, unlike `OpenCV`, can directly handle URLs or local files.
    It then calculates an adjustment for restoring the model’s coordinates to the
    ones that match the full image size. It does this by dividing 224 by the height
    and width of the image. Then, we convert the image data into a floating point
    with a range of 0–1\. We then normalize the images by dividing them by a standard
    deviation and subtracting a mean. This brings the images down to a range that
    the model expects. Then, we resize the image to 224 x 224, which is the image
    size that the model expects. We then convert the image into a tensor and get it
    in the order Pytorch uses, with the channels first. Finally, we add another dimension
    at the front to hold the batch and convert it into a 32-bit floating point on
    the GPU.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码准备图像。它通过首先使用`SciKit`图像加载器来加载图像，与`OpenCV`不同，它可以直接处理URL或本地文件。然后，它计算一个调整值，以将模型的坐标恢复到与完整图像大小匹配的坐标。它是通过将224除以图像的高度和宽度来做到这一点的。然后，我们将图像数据转换为范围在0–1之间的浮点数。然后，我们通过除以标准差并减去平均值来归一化图像。这使图像的范围降低到模型期望的范围。然后，我们将图像调整大小到224
    x 224，这是模型期望的图像大小。然后，我们将图像转换为张量，并按照Pytorch使用的顺序，以通道优先的方式获取它。最后，我们在前面添加一个维度来存储批次，并将其转换为GPU上的32位浮点数。
- en: This all prepares the image for the model to be run on it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是为了准备图像，以便模型可以在其上运行。
- en: 'Next, we run the model on the image and get 2D coordinates out of it:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在图像上运行模型，并从中获取2D坐标：
- en: '[PRE6]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code first runs the image through the model without generating training
    gradients. To do this, we pass the image and the `None` value, which will use
    a default camera intrinsics matrix.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码首先将图像通过模型，而不生成训练梯度。为此，我们传递图像和`None`值，这将使用默认的相机内参矩阵。
- en: Author’s note
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 作者注记
- en: '*Camera intrinsics* is a fancy term, but it just means the details of your
    camera. In the case of `PeCLR`, it wants a matrix that details how large the pixel
    space is so that it can attempt to guess the depth information from the 2D image.
    We don’t need the depth information, so we can let `PeCLR` create a default matrix
    instead of giving it one.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*相机内参*是一个术语，但它只是指你的相机的细节。在`PeCLR`的情况下，它需要一个矩阵来详细说明像素空间的大小，以便它可以从2D图像中尝试猜测深度信息。我们不需要深度信息，所以我们可以让`PeCLR`创建一个默认矩阵，而不是提供它。'
- en: Next, the code takes just the 2D alignment points. We don’t need 3D points since
    we’re aligning in 2D space. If we were working with a depth image, we may have
    wanted the third dimension, but we aren’t and we don’t need that for our scenario.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码只取了2D对齐点。因为我们是在2D空间中进行对齐，所以不需要3D点。如果我们处理的是深度图像，我们可能需要第三维度，但在这个场景中我们不需要，也不需要它。
- en: Next, since the model was given a small 224 x 224 image, we’re going to adjust
    those coordinates to match the width and height of the original image. To do this,
    we divide the coordinates by 224 and multiply the result by the original image
    sizes.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，由于模型被给了一个小的224 x 224图像，我们将调整这些坐标以匹配原始图像的宽度和高度。为此，我们将坐标除以224，然后将结果乘以原始图像的大小。
- en: Using the landmarks to align
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用地标进行对齐
- en: 'In this case, the library will mark the joints and tips of every finger and
    the thumb and one point near the “middle” of the hand. Unfortunately, the point
    in the middle of the hand is not well defined and could be anywhere from the actual
    middle to the wrist, so we wouldn’t want to use it for alignment. The joint and
    fingertip locations are going to be more consistent, so we can use those for the
    alignment process:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，库将标记每个手指和拇指的关节和尖端以及手“中间”附近的一个点。不幸的是，手中间的点定义得不好，它可以从实际中间到手腕的任何地方，所以我们不想用它来进行对齐。关节和指尖的位置将更一致，因此我们可以使用这些来进行对齐过程：
- en: '![Figure 8.2 – A hand detected and marked with detection from PeCLR (original
    photo by Kira auf der Heide via Unsplash)](img/B17535_08_002.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – 使用PeCLR检测并标记的手（原始照片由Kira auf der Heide通过Unsplash提供）](img/B17535_08_002.jpg)'
- en: Figure 8.2 – A hand detected and marked with detection from PeCLR (original
    photo by Kira auf der Heide via Unsplash)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – 使用PeCLR检测并标记的手（原始照片由Kira auf der Heide通过Unsplash提供）
- en: Tip
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Aligners are never perfectly accurate. Some can vary by a significant amount,
    but it’s not a matter of perfect results every time. Any alignment, even an imperfect
    one, has benefits for neural network training as it normalizes the data into a
    more reliable format. Any work done before the data is given to the AI means that
    is one less task the AI has to waste effort on doing.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐器永远不会完全准确。有些可能变化很大，但这并不是每次都得到完美结果的问题。任何对齐，即使是错误的，对神经网络训练都有好处，因为它将数据规范化为更可靠的形式。在将数据提供给AI之前所做的任何工作，意味着AI少了一个需要浪费精力去做的任务。
- en: Once we can get some known points on an image, we can scale, rotate, and crop
    the image so that it’s in the orientation that you want and run it through the
    detector to get a list of points. If you can, I recommend running several images
    through and averaging the points together. That way, you can reduce any variations
    in the hand images and get better alignments.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们可以在图像上获得一些已知点，我们就可以缩放、旋转和裁剪图像，使其处于你想要的朝向，并通过检测器运行它以获得点列表。如果你可以，我建议运行多张图像并平均点。这样，你可以减少手部图像中的任何变化，并获得更好的对齐。
- en: Once you have the points that you want to align to, you can use them to generate
    aligned images using the **Umeyama** algorithm. The algorithm just needs two sets
    of points, a known “aligned” set and a second set that you can convert into an
    aligned set. Umeyama returns a matrix that you can feed into an Affine Warp function
    to get a final aligned image. See [*Chapter 5*](B17535_05.xhtml#_idTextAnchor090),
    *Extracting Faces*, for a hands-on code example of how to do this.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了想要对齐的点，你可以使用它们通过**Umeyama**算法生成对齐的图像。该算法只需要两组点，一个已知的“对齐”集和一个你可以转换成对齐集的第二组。Umeyama返回一个矩阵，你可以将其输入到仿射变换函数中，以获得最终的对齐图像。参见[*第5章*](B17535_05.xhtml#_idTextAnchor090)，*提取人脸*，以获取如何进行此操作的动手代码示例。
- en: Once we have the aligned hand images, you can do whatever it was you were planning
    on doing with them, be that displaying them or using them for your AI task. It’s
    even possible that, once you have your aligned data, you can use it to train a
    pose detection model of your own to get even better alignment results. This process
    of using AI-processed data to feed into a model to make that model better is called
    **bootstrapping** and, with proper supervision, is an invaluable technique.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了对齐的手部图像，你就可以用它们做你计划要做的事情，无论是展示它们还是用于你的AI任务。甚至可能在你有了对齐数据后，你可以用它来训练你自己的姿态检测模型，以获得更好的对齐结果。这个过程，即使用AI处理的数据来喂养模型以使该模型变得更好，被称为**自举**，并且，在适当的监督下，是一种无价的技术。
- en: Alignments are a critical part of deepfakes, and now you can use them in other
    fields. Next, we’ll look at how to use masking to get clean cut-outs of an object.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐是深度伪造的关键部分，现在你可以在其他领域使用它们。接下来，我们将探讨如何使用遮罩来获取物体的干净剪影。
- en: The power of masking images
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 遮罩图像的力量
- en: When you take a photograph, you are capturing everything that the camera sees.
    However, the chances are that you’re not equally interested in every part of the
    image. If you’re on vacation, you might take a selfie of yourself in front of
    a waterfall, and while you value yourself and the waterfall, you care less about
    the cars or other people in the image. While you can’t remove the cars without
    adding something into the gaps for your vacation photos, sometimes, you’re only
    interested in the main subject and might want to cut it from the rest of the image.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当你拍照时，你正在捕捉相机看到的一切。然而，你很可能对图像的每一部分都不感兴趣。如果你在度假，你可能会在瀑布前拍一张自拍照，虽然你重视自己和瀑布，但你可能对图像中的汽车或其他人不那么关心。虽然你无法在不添加东西填补空缺的情况下从你的度假照片中移除汽车，但有时，你可能只对主要主题感兴趣，并可能想要将其从图像的其余部分中剪裁出来。
- en: 'With deepfakes, we can use a mask to help us remove the face from the image
    so that we replace only the face and leave the rest of the image alone. In other
    AI tasks, you might have similar needs but different objects that you want to
    cut out:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度伪造中，我们可以使用遮罩来帮助我们从图像中移除面部，以便只替换面部而留下图像的其余部分。在其他AI任务中，你可能会有类似的需求，但想要剪裁的对象不同：
- en: '![Figure 8.3 – An example of the mask used in the deepfake process](img/B17535_08_003.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 深度伪造过程中使用的遮罩示例](img/B17535_08_003.jpg)'
- en: Figure 8.3 – An example of the mask used in the deepfake process
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 深度伪造过程中使用的遮罩示例
- en: Next, let’s look at other types of masking.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看其他类型的遮罩。
- en: Types of masking
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 遮罩类型
- en: Masking out an image can be useful in a lot of tasks. We only used it as a part
    of the conversion process in a step called **composition**. This is only part
    of the power of masking. It can be used to guide **inpainting**, which is the
    process by which you fill in gaps in an image to erase an object. You could also
    do it to an image before it gets fed into an AI to make sure that the AI can focus
    on the important parts.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多任务中，遮罩图像可能很有用。我们只将其用作称为**合成**步骤的一部分。这仅仅是遮罩功能的一部分。它可以用来引导**修复**，这是通过填充图像中的空白来擦除物体的过程。你还可以在图像被喂入AI之前对其进行处理，以确保AI可以专注于重要的部分。
- en: In order to mask an image, you need to get some sort of idea of what part of
    an image you want to be masked. This is called **segmentation**, and it has a
    lot of sub-domains. If you want to segment based on the type of object, it would
    be called **semantic segmentation**. If you wanted to segment the image based
    on the subject, it is called **instance segmentation**. You can even use **depth
    segmentation** if you have a solid depth map of the image. Unfortunately, deciding
    which type of segmentation you need requires special attention in order to find
    the right tool.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对图像进行遮罩，你需要有一个想法，知道你想要遮罩图像的哪一部分。这被称为**分割**，并且有众多子领域。如果你想根据物体的类型进行分割，那么它被称为**语义分割**。如果你想要根据主题进行图像分割，那么它被称为**实例分割**。如果你有一个稳定的深度图，你甚至可以使用**深度分割**。不幸的是，决定你需要哪种类型的分割需要特别注意，以便找到正确的工具。
- en: Finding a usable mask for your object
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为你的物体找到可用的遮罩
- en: Libraries such as **PaddleSeg** ([https://github.com/PaddlePaddle/PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg))
    have special tools that let you do multiple types of segmentation. They even have
    an interactive segmentation system that lets you “mark” what you want to segment
    like Photoshop’s magic wand tool. Following that, you might need to use that data
    to train a segmentation model that is capable of masking that particular type
    of object in new contexts.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 像**PaddleSeg** ([https://github.com/PaddlePaddle/PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg))
    这样的库有特殊的工具，可以让您进行多种类型的分割。它们甚至有一个交互式分割系统，可以让您像Photoshop的魔术棒工具一样“标记”您想要分割的内容。之后，您可能需要使用这些数据来训练一个能够在新的环境中分割特定类型对象的分割模型。
- en: To find the best method of masking the given object you’re interested in, you
    should probably start with a search for the item that you want to mask and segment.
    For some objects such as faces, cars, people, and more, there are prebuilt models
    to segment those objects.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳的方法来分割你感兴趣的对象，你可能需要从搜索你想要分割和标记的项目开始。对于一些对象，如人脸、汽车、人物等，有现成的模型可以分割这些对象。
- en: But if a segmentation model doesn’t exist for the particular object you’re interested
    in, there’s no need to despair. Newer models such as **CLIP** ([https://github.com/openai/CLIP](https://github.com/openai/CLIP))
    have opened up whole new opportunities. CLIP is made up of a pair of AI models
    that connect language and images together. Because of the shared nature of CLIP,
    it’s possible to learn the difference between objects based on their text descriptions.
    This means that libraries such as **CLIPseg** ([https://github.com/timojl/clipseg](https://github.com/timojl/clipseg))
    can use language prompts to segment objects in an image.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你感兴趣的对象没有分割模型，你不必绝望。较新的模型如**CLIP** ([https://github.com/openai/CLIP](https://github.com/openai/CLIP))
    为我们开辟了全新的机会。CLIP由一对连接语言和图像的AI模型组成。由于CLIP的共享性质，我们可以根据对象的文本描述来学习它们之间的差异。这意味着像**CLIPseg**
    ([https://github.com/timojl/clipseg](https://github.com/timojl/clipseg)) 这样的库可以使用语言提示来分割图像中的对象。
- en: Examining an example
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查一个示例
- en: Let’s look at an example. Let’s say that you wanted to count the cars parked
    in a parking lot and see whether any spaces are still available, but all you have
    is a webcam image of the parking lot from above. To do this, you need to know
    which parts of the image are cars and which are empty parking spots. This task
    mixes both semantic segmentation and instance segmentation but uses them together.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。假设你想要计数停车场停放的汽车，并查看是否有空位，但你只有从上方拍摄的停车场网络摄像头的图像。为了做到这一点，你需要知道图像中哪些部分是汽车，哪些是空停车位。这个任务结合了语义分割和实例分割，但将它们一起使用。
- en: The first step would be to mark out each parking spot in the image to define
    which spots you want to look at. You could pick a single spot in each parking
    spot or define them by the whole area. Either way, you’ll probably want to do
    this manually since it is unlikely to change often enough to justify having the
    computer do it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在图像中标记出每个停车位，以定义您想要查看的位置。您可以在每个停车位中挑选一个单独的位置，或者通过整个区域来定义它们。无论如何，您可能需要手动完成这项工作，因为它们不太可能经常改变，从而证明让计算机来做是有必要的。
- en: 'Now that you know where the parking spots are in the image, you can start looking
    for cars. To do this, we’d want to look around for a good neural network trained
    to do this task. In our case, we can check this example from Kaggle user Tanishq
    Gautam: [https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch](https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch).
    This page gives pretrained models and a solid guide for how to segment cars.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道图像中停车位的所在位置，你可以开始寻找汽车了。为了做到这一点，我们可能需要寻找一个训练有素的神经网络来完成这个任务。在我们的例子中，我们可以查看Kaggle用户Tanishq
    Gautam提供的这个示例：[https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch](https://www.kaggle.com/code/ligtfeather/semantic-segmentation-is-easy-with-pytorch)。这个页面提供了预训练模型以及如何分割汽车的详细指南。
- en: While this model does not do instance segmentation (giving each car a different
    “color” or tag), we can use it to count the cars anyway. We can do this because
    we’ve already established that we’re counting cars in a parking lot and that we
    have manually marked each parking spot.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个模型不做实例分割（为每辆车分配不同的“颜色”或标签），但我们仍然可以用它来计数汽车。我们可以这样做，因为我们已经确定我们是在停车场计数汽车，并且我们已经手动标记了每个停车位。
- en: Then, we can simply use the segmentation model to detect any cars and see whether
    they overlap with the parking spots. If we only marked a single point for each
    parking spot, we could simply check each point and see whether it is segmented
    as a “car” or as “background” (a common term used for anything that we’re not
    segmenting).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以简单地使用分割模型来检测任何汽车，并查看它们是否与停车位重叠。如果我们只为每个停车位标记一个点，我们就可以简单地检查每个点，看看它是否被分割为“汽车”或“背景”（一个用于我们未分割的任何事物的常用术语）。
- en: If we marked the whole area of the parking spot, then we might want to calculate
    a minimum coverage for the spot to be considered “taken.” For example, we want
    to ensure that a motorcycle in a parking spot gets counted, but a car that is
    slightly over the line shouldn’t be counted. You can set a minimum threshold of
    the parking spot’s area, and if it’s filled beyond that with a “car,” then you
    mark the whole spot as taken.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们标记了整个停车位区域，那么我们可能希望计算一个最小覆盖面积，以便将停车位视为“占用”。例如，我们希望确保一个停放在停车位上的摩托车被计算在内，但稍微超过线的汽车不应该被计算。你可以设置停车位面积的最小阈值，如果它被一个“汽车”填满超过那个阈值，那么你就可以标记整个停车位为占用。
- en: Additional functionality is possible too. For example, you could even detect
    whether a car is in a prohibited area by checking whether an area segmented as
    a “car” is not inside a parking spot. This could be used to automatically alert
    a parking enforcement officer to check the lot.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以实现其他功能。例如，你甚至可以通过检查一个标记为“汽车”的区域是否不在停车位内来检测汽车是否在禁止区域。这可以用来自动提醒停车执法官员检查停车场。
- en: Now that we’ve gotten a good hold on our masking, let’s look at how we can manage
    our data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地掌握了我们的掩码，让我们看看我们如何管理我们的数据。
- en: Getting data under control
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制数据
- en: There’s a common saying in the AI community that an ML scientist’s job is only
    10% ML and 90% data management. This, like many such sayings, is not far from
    the truth. While every ML task is focused on the actual training of the model,
    first, you must get your data into a manageable form before you can start the
    training. Hours of training can be completely wasted if your data isn’t properly
    prepared.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI社区中有一个常见的说法，那就是机器学习科学家的工作只有10%是机器学习，90%是数据管理。这种说法，就像许多这样的说法一样，并不离真相太远。虽然每个机器学习任务都专注于模型的实际训练，但首先你必须将你的数据整理成可管理的形式，然后你才能开始训练。如果你的数据没有正确准备，数小时的训练可能会完全浪费。
- en: Before you can start training a model, you have to decide what data it is that
    you’re going to train it with. That data must be gathered, cleaned, converted
    into the right format, and generally made ready to train. Often, this involves
    a lot of manual processes and verification.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在你开始训练模型之前，你必须决定你要用哪些数据来训练它。这些数据必须被收集、清理、转换为正确的格式，并通常准备好用于训练。通常，这涉及到大量的手动过程和验证。
- en: Defining your rules
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义你的规则
- en: The most important thing in the manual process is to make sure that all your
    data meets your requirements and meets a consistent level of quality. To do this,
    you need to define exactly what “good” data means. Whether you’re annotating your
    data or gathering large amounts, you should have a standard way of doing whatever
    it is you’re doing.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动过程中最重要的事情是确保所有你的数据都符合你的要求，并达到一致的质量水平。为此，你需要确切地定义“好的”数据意味着什么。无论你是标注数据还是收集大量数据，你应该有一个标准的方式来完成你所做的一切。
- en: For example, let’s say you’re annotating a dog versus cat dataset and you want
    to put all the pictures into one of two buckets, one bucket consisting of all
    the dog images and the other bucket consisting of all the cat images. What happens
    when an image contains both a cat and a dog? Do you put it in the bucket that
    is more prominent? Do you exclude it from your dataset? Do you edit the image
    to remove one of the animals? Do you crop it into two images so both animals end
    up in the dataset?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你正在标注狗和猫的数据集，并且你希望将所有图片放入两个桶中的一个，一个桶包含所有狗的图片，另一个桶包含所有猫的图片。如果一个图片中同时包含猫和狗，你会怎么做？你会把它放入更显著的桶中吗？你会从你的数据集中排除它吗？你会编辑图片以移除其中一种动物吗？你会将其裁剪成两张图片，以便两种动物都进入数据集中吗？
- en: It’s important to have a consistent set of rules for these situations. That
    ensures that your data is appropriate for the purposes. You don’t want to have
    these edge cases happen in different ways each time, or it will upset your training
    and raise confusion when you’re trying to fix issues.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，拥有一套一致的规则非常重要。这确保了你的数据适用于这些目的。你不想每次都让这些边缘情况以不同的方式发生，否则当你试图解决问题时，会打乱你的训练并引起混淆。
- en: Evolving your rules
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规则的演变
- en: Also, it’s important to have a plan for what happens when you change your rules.
    As you go forward with your data management, it’s almost inevitable that you’ll
    find that you want to make some tweaks or changes to the rules based on what data
    you find, how well your training process goes, and whether your final use case
    changes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，制定一个计划，以应对你更改规则的情况是很重要的。随着你继续进行数据管理，你几乎不可避免地会发现，根据你发现的数据、你的训练过程进行得如何以及你的最终用例是否发生变化，你可能想要对规则进行一些调整或更改。
- en: Looking back at our example, let’s consider the case where you decided to exclude
    any images that contained both cats and dogs but other animals were fine as long
    as the image also contained a cat or dog. What happens when you decide that you
    want to add rabbits to your cat/dog detector? This means not just that you add
    a new bucket for rabbit images but also that you have to re-process all the existing
    images that you have gone through to make sure that any cat or dog images that
    also contain rabbits get removed. What about if you find out that guinea pigs
    in your cat and dog buckets are being flagged as rabbits? These processes need
    to be considered as you manage your data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的示例，让我们考虑这样一个情况：你决定排除包含猫和狗的任何图像，但只要图像中也包含猫或狗，其他动物都可以。当你决定想要将兔子添加到你的猫/狗检测器中时，会发生什么？这意味着不仅需要为兔子图像添加一个新的存储桶，而且还需要重新处理你已经检查过的所有现有图像，以确保任何包含兔子的猫或狗图像都被移除。如果你发现你的猫和狗存储桶中的豚鼠被标记为兔子怎么办？在管理你的数据时，这些过程都需要考虑。
- en: Dealing with errors
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理错误
- en: Errors happen, and small amounts of bad data getting through into your dataset
    is inevitable. However, there is a fine line between a few harmless errors and
    a large enough error to completely invalidate the training. Because of this, it’s
    often best to get a second (or third) set of eyes on the data as a part of the
    standard process. Sometimes, this isn’t possible, but in any situation where it
    is possible, it’s invaluable. A second set of eyes could find flaws in your data
    or even your methodology. What if you were tagging a particularly weird-looking
    animal as fine in your dog data but a second person identified it as a rare breed
    of cat?
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 错误是不可避免的，少量不良数据进入你的数据集是不可避免的。然而，在几处无害的错误和足以完全无效化训练的大量错误之间有一条细线。因此，通常最好在标准流程中将数据审查作为第二（或第三）套眼睛。有时，这可能是不可能的，但在任何可能的情况下，它都是无价的。第二套眼睛可能会发现你数据或方法中的缺陷。如果你在狗数据中将一个特别奇怪外观的动物标记为正常，但第二个人将其识别为罕见的猫品种怎么办？
- en: I also recommend automating as much of your data gathering as possible. When
    you can cut a human out of the loop, you’ll not only save timebut you'll also
    prevent errors and mistakes. Time spent automating a data process will almost
    always pay back dividends in time down the line. Even if you think that there
    is no way that a process will take less time to automate than to do it manually,
    you should consider the errors that you will avoid. Well-written automation code
    can be reused later for future projects, too. Anything that can be reused should
    be automated so that the next time you need to get it done, there is a tool to
    handle it for you.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我还建议尽可能自动化你的数据收集。当你能够从循环中去除人类时，你不仅会节省时间，还能防止错误和失误。花在自动化数据过程上的时间几乎总是会在未来带来回报。即使你认为没有一种方法可以使自动化过程比手动操作更快，你也应该考虑你将避免的错误。写得好的自动化代码可以后来用于未来的项目。任何可以重用的东西都应该自动化，这样下次你需要完成它时，就有工具为你处理。
- en: Summary
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how you can apply the lessons and techniques of
    deepfakes to other environments. First, we examined how to align other types of
    images, using hands as an example. Then, we looked at the different types of masks
    and considered using them in a parking lot monitoring solution. Following this,
    we examined data management and considered how a dataset to detect different animals
    might be built.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何将深度伪造的教训和技术应用于其他环境。首先，我们以手为例，探讨了如何对齐其他类型的图像。然后，我们考察了不同类型的面具，并考虑在停车场监控解决方案中使用它们。在此之后，我们研究了数据管理，并考虑了如何构建一个用于检测不同动物的数据集。
- en: This process of figuring out how to apply techniques in new environments used
    throughout this chapter is itself a valuable technique that can help you throughout
    your development career, especially if you’re going to work at the edge of your
    computer’s capabilities like AI does now. Sometimes, the only difference between
    a successful project and an impossible one is the technique you borrow from a
    previous project.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的新环境中应用技术的方法本身就是一个非常有价值的技巧，它可以帮助你在整个开发生涯中受益，尤其是如果你像现在的AI一样，打算在计算机能力的边缘工作。有时，成功项目和不可能的项目之间的唯一区别就是从以前的项目中借用的技术。
- en: In the next chapter, we’re going to look at the potential and future of deepfakes
    and other generative AIs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨深度伪造和其他生成式AI的潜力和未来。
- en: EBSCOhost - printed on 11/27/2023 6:20 AM via . All use subject to [https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: EBSCOhost - 2023年11月27日 6:20 AM 打印。所有使用均受[https://www.ebsco.com/terms-of-use](https://www.ebsco.com/terms-of-use)条款约束。
