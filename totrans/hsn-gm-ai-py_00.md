# 前言

本书是学习各种**强化学习**（**RL**）技术和算法如何在使用Python进行游戏开发中发挥重要作用的一站式商店。

本书将从基础知识开始，为你提供理解强化学习在游戏开发中扮演重要角色的必要基础。每一章都将帮助你实现各种强化学习技术，如马尔可夫决策过程、Q学习、演员-评论家方法、**状态-动作-奖励-状态-动作**（**SARSA**）和确定性策略梯度算法，以构建逻辑自学习智能体。你将使用这些技术来提高你的游戏开发技能，并添加各种功能以提高整体生产力。本书的后期，你将学习如何使用深度强化学习技术来制定策略，使智能体能够从自己的行动中学习，从而构建有趣且引人入胜的游戏。

到本书结束时，你将能够使用强化学习技术来构建各种项目，并为开源应用做出贡献。

# 本书面向的对象

本书面向希望通过实现强化学习技术从头开始构建游戏来增加知识的游戏开发者。本书也将吸引机器学习和深度学习从业者，以及希望了解自我学习智能体如何在游戏领域应用的强化学习研究者。本书假定读者具备游戏开发的基础知识以及Python编程的实际操作能力。

# 本书涵盖的内容

[第一章](5553d896-c079-4404-a41b-c25293c745bb.xhtml)，*理解基于奖励的学习*，探讨了学习的基础，学习的本质，以及强化学习与其他更经典的学习方法的不同之处。从那里，我们探讨了马尔可夫决策过程在代码中的工作原理以及它与学习的关系。这引导我们进入经典的带臂机和上下文赌博机问题。最后，我们将了解Q学习和基于质量的模型学习。

[第二章](8237fd36-1edf-4da0-b271-9a50c5b8deb3.xhtml)，*动态规划和贝尔曼方程*，深入探讨了动态规划，并研究了贝尔曼方程如何与强化学习交织在一起。在这里，你将学习贝尔曼方程是如何用来更新策略的。然后，我们进一步详细介绍了策略迭代或价值迭代方法，通过我们对Q学习的理解，通过在一个新的网格式环境中训练智能体来实现。

[第三章](5f6ea967-ae50-426e-ad18-c8dda835a950.xhtml)，*蒙特卡洛方法*，探讨了基于模型的方法以及它们如何被用来训练智能体在更经典的棋盘游戏中。

[第四章](bb05e528-e21b-4753-9e4c-372b8ed11e96.xhtml)，*时间差分学习*，探讨了强化学习的核心以及它如何解决学术界经常讨论的时间信用分配问题。我们将时间差分学习（**TDL**）应用于Q学习，并使用它来解决网格世界环境（如FrozenLake）。

[第五章](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml)，*探索SARSA*，深入探讨了在线策略方法如SARSA的基本原理。我们将通过理解部分可观察马尔可夫决策过程来探索基于策略的学习。然后，我们将探讨如何使用Q-learning实现SARSA。这将为我们在后续章节中探讨的更高级策略方法奠定基础，称为PPO和TRPO。

[第六章](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml)，*深入DQN*，将Q学习模型与深度学习相结合，创建了称为**深度Q学习网络**（**DQNs**）的高级智能体。从这一点出发，我们解释了基本的深度学习模型是如何用于回归的，或者在这种情况下，用于解决Q方程。我们将在CartPole环境中使用DQNs。

[第七章](42d53358-6f57-4f67-96ce-d8587cbe7cc5.xhtml)，*深入DDQNs*，探讨了深度学习扩展，称为**卷积神经网络**（**CNNs**），如何用于观察视觉状态。然后，我们将使用这些知识来玩Atari游戏，并探讨进一步的增强。

[第八章](42626cbd-87b8-428c-8f2a-ecc06f5e387c.xhtml)，*策略梯度方法*，深入探讨了更高级的策略方法以及它们如何集成到深度强化学习智能体中。这是一个高级章节，因为它涵盖了更高级的微积分和概率概念。你将在本章中获得MuJoCo动画强化学习环境作为你辛勤工作的回报。

[第九章](2f6812c0-fd1f-4eda-9df2-6c67c8077aec.xhtml)，*优化连续控制*，探讨了如何改进之前用于连续控制高级环境的策略方法。我们首先设置并安装MuJoCo环境。之后，我们研究了一种新颖的改进方法，称为循环网络，用于捕捉上下文，并了解循环网络是如何在PPO之上应用的。然后，我们回到演员-评论家方法，这次在几种不同的配置下研究异步演员-评论家，最后进展到带有经验回放的演员-评论家。

[第十章](1fbfb255-7fd9-44ea-8d02-f385e95d88d2.xhtml)，*彩虹DQN的全部内容*，告诉我们所有关于Rainbow的信息。Google DeepMind最近在一种称为Rainbow的算法中探索了将多个强化学习增强功能结合在一起。Rainbow是另一个你可以探索的工具包，你可以从中借用或使用它来与更高级的强化学习环境一起工作。

[第十一章](ab9a7f4f-60d8-4643-8627-199cf95bcf55.xhtml)，*利用ML-Agents*，探讨了如何在我们自己的智能体中使用ML-Agents工具包的元素，或者使用工具包来获得一个完全开发的智能体。

[第十二章](6d061d35-176a-421a-9b62-aed35f48a6b7.xhtml)，*DRL框架*，开启了在多种环境中与单独智能体玩耍的可能性。我们还将探索各种多智能体环境。

[第13章](e54c6adf-d238-4f1e-8e32-7ba3c5da0f46.xhtml)，*3D世界*，训练我们有效地使用RL代理来应对各种3D环境挑战。

[第14章](a171ddfa-e639-4b4e-9652-4279b5ac872a.xhtml)，*从DRL到AGI*，超越了DRL，进入了AGI的领域，或者至少是我们希望AGI能去往的地方。我们还将探讨各种可以在现实世界中应用的DRL算法。

# 为了充分利用本书

掌握Python和游戏开发的基本知识是必要的。拥有一台配备GPU的好PC将很有帮助。

# 下载示例代码文件

您可以从[www.packt.com](http://www.packt.com)上的账户下载本书的示例代码文件。如果您在其他地方购买了此书，您可以访问[www.packtpub.com/support](https://www.packtpub.com/support)并注册，以便将文件直接通过电子邮件发送给您。

您可以通过以下步骤下载代码文件：

1.  在[www.packt.com](http://www.packt.com)上登录或注册。

1.  选择“支持”选项卡。

1.  点击“代码下载”。

1.  在搜索框中输入书籍名称，并遵循屏幕上的说明。

下载文件后，请确保使用最新版本的以下软件解压缩或提取文件夹：

+   Windows上的WinRAR/7-Zip

+   Zipeg/iZip/UnRarX for Mac

+   7-Zip/PeaZip for Linux

本书代码包也托管在GitHub上，网址为[https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-for-Games)。如果代码有更新，它将在现有的GitHub仓库中更新。

我们还有其他来自我们丰富的图书和视频目录的代码包可供下载，网址为**[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**。查看它们！

# 下载彩色图像

我们还提供了一份包含本书中使用的截图/图表彩色图像的PDF文件。您可以从这里下载：[http://www.packtpub.com/sites/default/files/downloads/9781839214936_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781839214936_ColorImages.pdf)。

# 使用的约定

本书使用了多种文本约定。

`CodeInText`：表示文本中的代码单词、数据库表名、文件夹名、文件名、文件扩展名、路径名、虚拟URL、用户输入和Twitter昵称。以下是一个示例：“三个函数`make_atari`、`wrap_deepmind`和`wrap_pytorch`都位于我们之前导入的`wrappers.py`新文件中。”

代码块是这样设置的：

```py
env_id = 'PongNoFrameskip-v4'
env = make_atari(env_id)
env = wrap_deepmind(env)
env = wrap_pytorch(env)
```

当我们希望您注意代码块中的特定部分时，相关的行或项目将以粗体显示：

```py
epsilon_start = 1.0
epsilon_final = 0.01
epsilon_decay = 30000

epsilon_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1\. * episode / epsilon_decay)

plt.plot([epsilon_by_episode(i) for i in range(1000000)])
plt.show()
```

任何命令行输入或输出都应如下编写：

```py
pip install mujoco
```

**粗体**：表示新术语、重要单词或屏幕上看到的单词。例如，菜单或对话框中的单词在文本中显示如下。以下是一个示例：“在此基础上，我们将探讨DQN的一个变体，称为**DDQN**，或**双（对抗）DQN**。”

警告或重要提示看起来是这样的。

小贴士和技巧看起来是这样的。

# 联系我们

我们始终欢迎读者的反馈。

**一般反馈**：如果您对本书的任何方面有疑问，请在邮件主题中提及书名，并给我们发送邮件至 `customercare@packtpub.com`。

**勘误**：尽管我们已经尽一切努力确保内容的准确性，但错误仍然可能发生。如果您在这本书中发现了错误，我们将不胜感激，如果您能向我们报告这一错误。请访问 [www.packtpub.com/support/errata](https://www.packtpub.com/support/errata)，选择您的书籍，点击勘误提交表单链接，并输入详细信息。

**盗版**：如果您在互联网上发现我们作品的任何非法副本，我们将不胜感激，如果您能提供位置地址或网站名称。请通过 `copyright@packt.com` 联系我们，并提供材料的链接。

**如果您想成为一名作者**：如果您在某个领域有专业知识，并且对撰写或参与一本书籍感兴趣，请访问 [authors.packtpub.com](http://authors.packtpub.com/)。

# 评论

请留下评论。一旦您阅读并使用了这本书，为何不在您购买它的网站上留下评论？潜在读者可以查看并使用您的客观意见来做出购买决定，我们Packt可以了解您对我们产品的看法，我们的作者也可以看到他们对书籍的反馈。谢谢！

如需更多关于Packt的信息，请访问 [packt.com](http://www.packt.com/)。
