- en: Going Deeper with DDQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is the evolution of raw computational learning and it is quickly
    evolving and starting to dominate all areas of data science, **machine learning**
    (**ML**), and **artificial intelligence** (**AI**) in general. In turn, these
    enhancements have brought about incredible innovation in **deep reinforcement
    learning** (**DRL**) that have allowed it to play games, previously thought to
    be impossible. DRL is now able to tackle game environments such as the classic
    Atari 2600 series and play them better than a human. In this chapter, we'll look
    at what new features in DL allow DRL to play visual state games, such as Atari
    games. First, we'll look at how a game screen can be used as a visual state. Then,
    we'll understand how DL can consume a visual state with a new component called
    **convolutional neural networks** (**CNNs**). After, we'll use that knowledge
    to build a modified DQN agent to tackle the Atari environment. Building on that,
    we'll look at an enhancement of DQN called **DDQN**, or **double (dueling) DQN**.
    Finally, we'll finish the chapter by playing other visual environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, in this chapter we''ll look at how extensions to DL, called CNNs,
    can be used to observe visual states. Then, we''ll use that knowledge to play
    Atari games and implement further enhancements as we go. The following is what
    we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding visual state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with a DQN on Atari
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing DDQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending replay with prioritized experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will continue using the same virtual environment we constructed in [Chapter
    6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going Deep with DQN*, in this
    chapter. You will need that environment set up and configured properly in order
    to use the examples in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding visual state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Up until now, we have observed state as an encoded value or values. These values
    may have been the cell number in a grid or the x,y location in an area. Either
    way, these values have been encoded with respect to some reference. In the case
    of the grid environment, we may use a number to denote the square or a pair of
    numbers. For x,y coordinates, we still need to denote an origin, and examples
    of these three types of encoding mechanism are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b90d1fe2-5c07-428d-a721-5ba6484829e8.png)'
  prefs: []
  type: TYPE_IMG
- en: Three types of encoding state for an agent
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, there are three examples of encoding state for an
    environment. For the first example, which is on the left, we just use a number
    to represent that state. Moving right to the next grid, the state is now represented
    as a pair of digits, row by column. On the far right, we can see our old friend
    the Lunar Lander and how part of its state, the location, is taken with respect
    to the landing pad, which is the origin. With all these cases, the state is always
    represented as some form of encoding, whether a single-digit or eight like in
    the Lander environment. By encoding, we mean that we are using a value, that is,
    a number, to represent that state of the environment. In [Chapter 5](3e0c16c5-2145-498c-8ba1-b917745e0ef0.xhtml),
    *Exploring SARSA*, we learned how discretization of state is a type of transformation
    of that encoding into simpler forms but that transforming this encoding would
    need to be tweaked or learned and we realized there needed to be a better way
    to do this. Fortunately, we did devise a better way, but before we get to that,
    let's consider what state it is itself.
  prefs: []
  type: TYPE_NORMAL
- en: State is just a numeric representation or index of our policy that lets our
    agent determine its choice of next actions. The important thing to remember here
    is that state needs to be an index into the policy or, in the case of DRL, the
    model. Therefore, our agent will always need to transform that state into a numeric
    index in that model. This is made substantially simpler with DL, as we have already
    seen. What would be ideal is for the agent to be able to visually consume the
    same visible state – the game area – as we humans do and learn to encode the state
    on its own. 10 years ago, that statement would have sounded like science fiction.
    Today, it is a science fact, and we will learn how that is done in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding visual state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Fortunately for DRL, the concept of learning from an image has been the center
    of ongoing research into DL for over 30 years. DL has taken this concept from
    being able to recognize handwritten digits to being able to detect object position
    and rotation to understanding the human pose. All of this is done by feeding raw
    pixels into a deep learning network and it being taught (or teaching itself) how
    to encode those images to some answer. We will use these same tools in this chapter,
    but before we do, let''s understand the fundamentals of taking an image and feeding
    it into a network. An example of how you may do this is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a2d2581-3f16-4722-86ac-39237e1cdbd0.png)'
  prefs: []
  type: TYPE_IMG
- en: Dissecting an image for input into DL
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the image was split into four sections and each section
    was fed as a piece into the network. One thing to note is how each piece is fed
    into each neuron on the input layer. Now, we could use four pieces, like in the
    preceding diagram, or 100 pieces, perhaps breaking the image apart pixel by pixel.
    Either way, we are still blindly discretizing the space, that is, an image, and
    trying to make sense of it. Funnily enough, this problem that we recognized in
    RL with discretization is the same type of problem we encounter in deep learning.
    It is perhaps further compounded in DL because we would often just flatten the
    image, a 2D matrix of data, into a 1D vector of numbers. In the preceding example,
    for instance, we can see two eyes being entered into the network but no indication
    of a relationship, such as spacing and orientation, between them. This information
    is completely lost when we flatten an image and is more significant the more we
    discretize the input image. What we need, and what DL discovered, was a way to
    extract particular features from a set of data, such as an image, and preserve
    those features in order to classify the entire image in some manner. DL did, in
    fact, solve this problem very well and we will discover how in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In September 2012, a team supervised by Dr. Geoffrey Hinton from the University
    of Toronto, considered the godfather of deep learning, competed to build AlexNet.
    AlexNet was training against a behemoth image test set called ImageNet. ImageNet
    consisted of more than 14 million images in over 20,000 different classes. AlexNet
    handily beat its competition, a non-deep learning solution, by more than 10 points
    that year and achieved what many thought impossible – that is, the recognition
    of objects in images done as well or perhaps even better than humans. Since that
    time, the component that made this possible – CNN – has in some cases surpassed
    human cognition levels in image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: The component that made this possible, CNN, works by dissecting an image into
    features – features that it learns to detect by learning to detect those features.
    This sounds a bit recursive and it is, but it is also the reason it works so well.
    So, let's repeat that again. CNN works by detecting features in an image, except
    we don't specify those features – what we specify is whether the answer is right
    or wrong. By using that answer, we can then use backpropagation to push any errors
    back through the network and correct the way the network detects those features.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to detect features, we use filters, much the same way you may use
    a filter in Photoshop. These filters are the pieces that we now train and do so
    by introducing them in a new type of layer called CNN, convolution, or CONV. What
    we find is that we can then also stack those layers on top of each other to extract
    further features. These concepts likely still remain abstract. Fortunately, there
    are plenty of great tools that we can use to explore these concepts in the next
    exercise. Let''s take a look at one:'
  prefs: []
  type: TYPE_NORMAL
- en: Open and point a web browser to [tensorspace.org](https://tensorspace.org).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the link for the **Playground** and click on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the **TensorSpace Playground** page, note the various model names on the
    left-hand side. Click on the **AlexNet** example, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6a62226d-3d38-4446-98b7-f1c66579887a.png)'
  prefs: []
  type: TYPE_IMG
- en: TensorSpace Playground – AlexNet
  prefs: []
  type: TYPE_NORMAL
- en: Playground allows you to interactively explore the various deep learning models,
    such as AlexNet, right down to the layer.
  prefs: []
  type: TYPE_NORMAL
- en: Move through and click on the various layers in the diagram. You can zoom in
    and out and explore the model in 3D. You will be able to look at all the layers
    in the network model. Each layer type is color coded. This includes CNN layers
    (yellow), as well as special pooling layers (blue).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pooling layers, which are layers that collect the learned features from a CNN
    layer, allow a network to learn quicker since the layers essentially reduce the
    size of the learning space. However, that reduction eliminates any spatial relationship
    between features. As such, we typically avoid using pooling layers in DRL and
    games.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you zoom in, you can look at the way the image is broken by each color channel
    (red, green, and blue) and then fed into the network. The following screenshot
    shows this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/60a88ff3-b244-4d61-918f-a9ccda3f98c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Inspecting the image separation and filter extraction
  prefs: []
  type: TYPE_NORMAL
- en: From the way the image is separated, we can see how the first layer of CNN,
    the filters, are extracting features. By doing this, it is possible to recognize
    the entire dog, but as you go through the layer, the features get smaller and
    smaller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, there is a final pooling layer in blue, followed by a green layer,
    which is a single line. This single line layer represents the input data being
    flattened so that it can be fed into further layers of your typical deep learning
    network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, feel free to explore many of the other models in the Playground.
    Understanding how layers extract features is import to understanding how CNN works.
    In the next section, we'll look at upgrading our DQN agent so that it can play
    Atari games using CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Working with a DQN on Atari
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve looked at the output CNNs produce in terms of filters, the
    best way to understand how this works is to look at the code that constructs them.
    Before we get to that, though, let''s begin a new exercise where we use a new
    form of DQN to solve Atari:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open this chapter''s sample code, which can be found in the `Chapter_7_DQN_CNN.py` file.
    The code is fairly similar to `Chapter_6_lunar.py` but with some critical differences.
    We will just focus on the differences in this exercise. If you need a better explanation
    of the code, review [Chapter 6](a9e9aefb-40af-4886-9b4f-94e725dd2f92.xhtml), *Going
    Deep with DQN*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting at the top, the only change is a new import from a local file called
    `wrappers.py`. We will examine what this does by creating the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the environment quite differently here for a few reasons. The three
    functions, `make_atari`, `wrap_deepmind`, and `wrap_pytorch`, are all located
    in the new `wrappers.py` file we imported earlier. These wrappers are based on
    the OpenAI specification for creating wrappers around the Gym environment. We
    will spend more time on wrappers later but for now, the three functions do the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`make_atari`:This prepares the environment so that we can capture visual input
    in a form we can encode with CNN. We are setting this up so we can take screenshots
    of the environment at set intervals.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wrap_deepmind`: This is another wrapper that allows for some helper tools.
    We will look at this later.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wrap_pytorch`:This is a helper library that converts the visual input image
    we load into the CNN network into a special form for PyTorch. The various deep
    learning frameworks have different input styles for CNN layers, so until all the
    DL frameworks are standardized, you have to be aware of which way the channels
    appear in your input image. In PyTorch, image channels need to be first. For other
    frameworks, such as Keras, it is the exact opposite.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After that, we need to alter some of the other code that sets the hyperparameters,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The highlighted lines show the changes we made. The main thing we are changing
    is just increasing values – a lot. The Pong Atari environment is the simplest
    and still may require 1 million iterations to solve. On some systems, that may
    take days:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding block of code, we can see that we are constructing a new class
    called `CnnDQN`. We will get to that shortly. After that, the code is mostly the
    same except for a new variable, `replay_start`, and how large the replay buffer
    is now set to. Our buffer has increased in size 100 times from 1,000 to 100,000
    entries. However, we want to be able to train the agent before the entire buffer
    fills now. After all, that is a lot of entries. Due to this, we''re using `replay_start`
    to denote a training starting point for when the buffer will be used to train
    the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we update the episode count to a much higher number. This is because
    we can expect this environment requires at least a million episodes to train an
    agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: All of the other code remains the same aside from the last part of the training
    loop, which can be seen in the preceding code. This code shows that we plot iterations
    every 200,000 episodes. Previously, we did this every 2,000 episodes. You can,
    of course, increase this or remove it altogether if it gets annoying when training
    for long hours.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This environment and many of the others we will look at may now take hours or
    days to train. In fact, DeepMind recently estimated that it would take a regular
    desktop system somewhere near 45 years to train its top RL algorithms. And in
    case you are wondering, most of the other environments take 40 million iterations
    to converge. Pong is the easiest at 1 million iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the example as you normally do. Wait for a while and perhaps move on to
    the rest of this book. This sample will take hours to train, so we will continue
    exploring other sections of code while it runs. To confirm the sample is running
    correctly though, just confirm that the environment is rendering, as shown in
    the following image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/715cbba5-c487-4daa-9775-2fff2a6632a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the code example
  prefs: []
  type: TYPE_NORMAL
- en: Keep the sample running. In the next section, we will look at how the CNN layers
    are built into the new model.
  prefs: []
  type: TYPE_NORMAL
- en: Adding CNN layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we understand the basic premise behind CNN layers, it''s time to take
    an in-depth look at how they work. Open up code example, which can be found in
    the `Chapter_7_DQN_CNN.py` file, and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, the only code we need to focus on is for a new class called
    `CnnDQN`, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding class replaces our previous vanilla DQN version. A number of
    key differences exist between both, so let''s start with the network setup and
    building the first convolution layer, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first thing to notice is that we are constructing a new model and putting
    it in `self.features`. `features` will be our model for performing convolution
    and separating features. The first layer is constructed by passing in `input_shape`,
    the number of filters (32), `kernel_size` (8), and the `stride` (4). All of these
    inputs are described in more detail here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`input_shape[0]`:The input shape refers to the observation space. With the
    wrappers we looked at earlier, we transformed the input space to (1, 84,84). Remember
    that we needed to order the channels first. With 1 channel, we can see our image
    is grayscale (no RGB). 1 channel is also the number we input as the first value
    into `Conv2d.`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '** The **number of filters (`32`): The next input represents the number of
    filter patches we want to construct in this layer. Each filter is applied across
    the image and is determined by a window size (kernel size) and movement (stride).
    We observed the results of these patches earlier when we used TensorSpace Playground
    to view CNN models in detail.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size` (`8`):This represents the window size. In this case, since we
    are using a 2D convolution, Conv2d, that size actually represents a value of 8x8\.
    Passing the window or kernel over the image and applying the learned filter is
    the convolving operation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stride` (`4`):Stride indicates how much the window or kernel moves between
    operations. A stride of 4 means that the window is moved 4 pixels or units which,
    as it turns out, is half the window size of 8.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of how convolution works can be seen in the following image. The
    upper area is a single output patch. Each element in the kernel, that is, the
    3x3 patch in the following image, is the part that is being learned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ad2c12b1-37c5-42e7-ae9e-86b1633ea357.png)'
  prefs: []
  type: TYPE_IMG
- en: The strided convolution process explained
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of applying the kernel on the image is done by simply multiplying
    the values in the patch with each value in the image. All these values are summed
    and then output as a single element in the resulting output filter operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the code that constructs the model for the convolution layers, we build
    another Linear model, just like we constructed in our previous examples. This
    model will flatten the output from the convolution layers and use that flattened
    model to predict actions from. We end up with two models for the network in this
    case but note that we will pass the output from one to the other, as well as backpropagate
    errors back from one model to the other. The `feature_size` function is just a
    helper so that we can calculate the input from the CNN model to the `Linear` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Inside the `forward` function, we can see that the prediction of our model has
    changed. Now, we will break up the prediction by passing it to the `self.features`
    or the CNN part of our model. Then, we need to flatten the data and feed it into
    the Linear portion with `self.fc`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `action` function remains the same as our previous DQN implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the agent is still running, see if you can wait for it to finish. It can
    take a while but it can be both rewarding and interesting to see the final results.
    Like almost anything in RL, there have been various improvements to the DQN model
    and we will look at those in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing DDQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**DDQN** stands for **dueling DQN** and is different from the double DQN, although
    people often confuse them. Both variations assume some form of duality, but in
    the first case, the model is assumed to be split at the base, while in the second
    case, double DQN, the model is assumed to be split into two entirely different
    DQN models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the difference between DDQN and DQN, which is not
    to be confused with dueling DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ab231724-a4a6-41ea-ac48-ed0003d71459.png)'
  prefs: []
  type: TYPE_IMG
- en: The difference between DQN and DDQN
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, CNN layers are being used in both models but in the
    upcoming exercises, we will just use linear fully connected layers instead, just
    to simplify things.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how the DDQN network separates into two parts that then converge back
    to an answer. This is the dueling part of the DDQN model we will get to shortly.
    Before that, though, let's explore the double DQN model.
  prefs: []
  type: TYPE_NORMAL
- en: Double DQN or the fixed Q targets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to understand why we may use two networks in combination, or dueling,
    we first need to understand why we would need to do that. Let''s go back to how
    we calculated the TD loss and used that as our way to estimate actions. As you
    may recall, we calculated loss based on estimations of the target. However, in
    the case of our DQN model, that target is now continually changing. The analogy
    we can use here is that our agent may chase its own tail at times, trying to find
    a target. Those of you who have been very observant may have viewed this during
    previous training by seeing an oscillating reward. What we can do here is create
    another target network that we will aim for and update as we go along. This sounds
    way more complicated than it is, so let''s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the code example in the `Chapter_7_DoubleDQN.py` file. This example was
    built from the `Chapter_6_DQN_lunar.py` file that we looked at earlier. There
    are a number of subtle changes here, so we will review each of those in detail,
    starting with model construction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As its name suggests, we now construct two DQN models: one for online use and
    one as a target. We train the `current_model` value and then swap back to the
    target model every *x* number of iterations using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `update_target` function updates `target_model` so that it uses the `current_model`
    model. This assures us that the target Q values are always sufficiently enough
    ahead or behind since we are using skip traces and looking back.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Right after that is the `compute_td_loss` function, which needs to be updated
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The highlighted lines in the function show the lines that were changed. Notice
    how the new models, `current_model` and `target_model`, are used to predict the
    loss now and not just the individual model itself. Finally, in the training or
    trial and error loop, we can see a couple of final changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The first change is that we are now taking the action from the `current_model`
    model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The second change is updating `target_model` with the weights from `current_model`
    using `update_target`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We also need to update the `play_game` function so that we can take the action
    from `current_model`. It may be interesting to see what happens if you change
    that to the target model instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, run the code as you normally would and observe the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we understand why we may want to use a different model, we will move
    on and learn how we can use dueling DQN or DDQN to solve the same environment.
  prefs: []
  type: TYPE_NORMAL
- en: Dueling DQN or the real DDQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dueling DQN or DDQN extends the concept of a fixed target or fixed Q target
    and extends that to include a new concept called advantage. Advantage is a concept
    where we determine what additional value or advantage we may get by taking other
    actions. Ideally, we want to calculate advantage so that it includes all the other
    actions. We can do this with computational graphs by separating the layers into
    a calculation of state value and another that calculates the advantage from all
    the permutations of state and action.
  prefs: []
  type: TYPE_NORMAL
- en: 'This construction can be seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e94bf46b-2e18-484a-95d1-ac81d8b8330c.png)'
  prefs: []
  type: TYPE_IMG
- en: DDQN visualized in detail
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding diagram once again shows CNN layers, but our example will just
    start with the linear flattened model. What we can see is how the model is split
    into two parts after it is flattened. The first part calculates the state value
    or value and the second lower part calculates the advantage or action values.
    This is then aggregated to output the Q values. This setup works because we can
    push the loss back through the entire network using optimization, also known as
    backpropagation. Therefore, the network learns how to calculate the advantage
    of each action. Let''s look at how this comes together in a new code example.
    Open the same in the `Chapter_7_DDQN.py` file and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This example uses the previous example as a source but differs in terms of
    a number of important details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The DDQN class is almost entirely new aside from the `act` function. Inside
    the init function, we can see the construction of the three submodels: `self.feature`,
    `self.value`, and `self.advantage`. Then, inside the `forward` function, we can
    see how the input, **x** is transformed by the first **feature** submodel, then
    fed into the advantage and value submodels. The outputs, `advantage` and `value`,
    are then used to calculate the predicted value, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'What we can see is that the predicted value is the state value denoted by value.
    This is added to the advantage or combined state-action values and subtracted
    from the mean or average. The result is a prediction of the best advantage or
    what the agent learns may be an advantage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The next change is that we now construct two instances of the DDQN model instead
    of a DQN in our last double DQN example. This means that we also continue to use
    two models in order to evaluate our targets. After all, we don't want to go backward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next major change occurs in the `compute_td_loss` function. The updated
    lines are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This actually simplifies the preceding code. Now, we can clearly see that our
    next_q_values are being taken from the `target_model`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the code example as you always do and watch the agent play the Lander. Make
    sure you keep the agent training until it reaches some amount of positive reward.
    This may require you to increase the number of training iterations or episodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a reminder, we use the term episode to mean one training observation or iteration
    for one time step. Many examples will use the word frame and frames to denote
    the same thing. While frame can be appropriate in some contexts, it is less so
    in others, especially when we start to stack frames or input observations. If
    you find the name confusing, an alternative may be to use training iteration.
  prefs: []
  type: TYPE_NORMAL
- en: You will see that this algorithm does indeed converge faster, but as you may
    expect, there are improvements we can make to this algorithm as well. We will
    look at how we can improve on this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Extending replay with prioritized experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve seen how using a replay buffer or experience replay mechanism
    allows us to pull values back in batches at a later time in order to train the
    network graph. These batches of data were composed of random samples, which works
    well, but of course, we can do better. Therefore, instead of storing just everything,
    we can make two decisions: what data to store and what data is a priority to use.
    In order to simplify things, we will just look at prioritizing what data we extract
    from the experience replay. By prioritizing the data we extract, we can hope this
    will dramatically improve the information we do feed to the network for learning
    and thus the whole performance of the agent.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the idea behind prioritizing the replay buffer is quite simple
    to grasp but far more difficult in practice to derive and estimate. What we can
    do, though, is prioritize the return events by the TD error or loss from the prediction
    and the actual expected target of that event. Thus, we prioritize the values the
    agent predicts where the most amount of error is or where the agent is wrong the
    most. Another way to think of this is that we prioritize the events that surprised
    the agent the most. The replay buffer is structured so that it prioritizes those
    events by surprise level and then returns a sample of those, except it doesn't
    necessarily order the events by surprise. Here, it's better to randomly sample
    the events from a bucket or distribution ordered by surprise. This means the agent
    would then be more inclined to choose samples from the more average surprising
    events.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll use a Prioritized Experience Replay mechanism, which
    was first introduced in this paper: [https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf).
    It was then coded in PyTorch from this repository: [https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb).
    Our implementation has been modified to run outside a notebook and for Python
    3.6 ([https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb](https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will work with an entirely new sample. Open up `Chapter_7_DDQN_wprority.py`
    and follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first big change in this sample is an upgrade from the `ReplayBuffer` class
    to `NaivePrioritizedBuffer`, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This code naively assigns priorities based on observed error prediction. Then,
    it sorts those values based on priority order. It then randomly samples those
    events back. Again, since the sampling is random, but the samples are aligned
    by priority, random sampling will generally take the samples with an average error.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What happens is that by reordering the samples, we reorder to expected actual
    distribution of data. Therefore, to account for this, we introduce a new factor
    called **beta**, or **importance-sampling**. **Beta** allows us to control the
    distribution of events and essentially reset them to their original placement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will define a function to return an increasing beta over episodes using
    the preceding code. Then, the code plots beta much like we plot epsilon, as shown
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/ae38a330-3d0d-49bf-aeab-886984d7adda.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of beta and epsilon plots
  prefs: []
  type: TYPE_NORMAL
- en: 'After modifying the sample function in the replay buffer, we also need to update
    the `compute_td_loss` function, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Only the preceding highlighted lines are different from what we have seen already.
    The first difference is the return of two new values: `indices` and `weights`.
    Then, we can see that `replay_buffer` calls `update_priorities` based on the previously
    returned `indices`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, inside the training loop, we update the call to `play_game` and introduce
    a new `min_play_reward` threshold value. This allows us to set some minimum reward
    threshold before rendering the game. Rendering the game can be quite time-consuming
    and this will also speed up training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Continuing inside the training loop, we can see how we extract `beta` and use
    that in the `td_compute_loss` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the sample again. This time, you may have to wait to see the agent drive
    the Lander but when it does, it will do quite well, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/da8e4389-45ac-40c0-8f9d-04de5d01e475.png)'
  prefs: []
  type: TYPE_IMG
- en: The agent landing the Lander
  prefs: []
  type: TYPE_NORMAL
- en: Typically, in a reasonably short amount of time, the agent will be able to consistently
    land the Lander. The algorithm should converge to landing within 75,000 iterations.
    You can, of course, continue to tweak and play with the hyperparameters, but that
    is what our next section is for.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The further we progress in this book, the more valuable and expensive each
    of these exercises will become. By expensive, we mean the amount of time you need
    to invest in each will increase. That may mean you are inclined to do fewer exercises,
    but please continue to try and do two or three exercises on your own:'
  prefs: []
  type: TYPE_NORMAL
- en: Revisit TensorSpace Playground and see if you can understand the difference
    pooling makes in those models. Remember that we avoid the use of pooling in order
    to avoid losing spatial integrity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open `Chapter_7_DQN_CNN.py`and alter some of the convolutional layer inputs
    such as the kernel or stride size. See what effect this has on training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters or create new ones for `Chapter_7_DoubleDQN.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters or create new ones for `Chapter_7_DDQN.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the hyperparameters or create new ones for `Chapter_7_DoubleDQN_wprority.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert `Chapter_7_DoubleDQN.py` so that it uses convolutional layers and then
    upgrade the sample so that it works with an Atari environment such as Pong.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert `Chapter_7_DDQN.py`so that it uses convolutional layers and then upgrade
    the sample so that it works with an Atari environment such as Pong.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert `Chapter_7_DDQN_wprority.py`so that it uses convolutional layers and
    then upgrade the sample so that it works with an Atari environment such as Pong.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a Pooling layer in-between the convolutional layers in one of the examples.
    You will likely need to consult the PyTorch documentation to learn how to do this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How else could you improve the experience replay buffer in the preceding example?
    Are there other forms of replay buffers you could use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As always, have fun working through the samples. After all, if you are not happy
    watching your code play the Lunar Lander or an Atari game, when will you be?
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll wrap up this chapter and look at what we'll learn
    about next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Extending from where we left off with DQN, we looked at ways of extending this
    model with CNN and adding additional networks to create double DQN and dueling
    DQN, or DDQN. Before exploring CNN, we looked at what visual observation encoding
    is and why we need it. Then, we briefly introduced CNN and used the TensorSpace
    Playground to explore some well-known, state-of-the-art models. Next, we added
    CNN to a DQN model and used that to play the Atari game environment Pong. After,
    we took a closer look at how we could extend DQN by adding another network as
    the target and adding another network to duel against or to contradict the other
    network, also known as the dueling DQN or DDQN. This introduced the concept of
    advantage in choosing an action. Finally, we looked at extending the experience
    replay buffer so that we can prioritize events that get captured there. Using
    this framework, we were able to easily land the Lander with just a short amount
    of agent training.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at new ways of selecting policy methods and
    no longer look at global averages. Instead, we will sample distributions using
    policy gradient methods.
  prefs: []
  type: TYPE_NORMAL
